<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.93,116.95,295.50,12.62">UCD SIFT in the TREC 2011 Web Track</title>
				<funder ref="#_zTWcMq5">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.69,154.75,62.75,8.74"><forename type="first">David</forename><surname>Leonard</surname></persName>
							<email>david.leonard@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.56,154.75,73.63,8.74"><forename type="first">Doychin</forename><surname>Doychev</surname></persName>
							<email>doychin.doychev@ucdconnect.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.48,154.75,49.77,8.74"><forename type="first">David</forename><surname>Lillis</surname></persName>
							<email>david.lillis@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.82,166.70,59.76,8.74"><forename type="first">Fergus</forename><surname>Toolan</surname></persName>
							<email>fergus.toolan@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,263.57,166.70,67.34,8.74"><forename type="first">Rem</forename><forename type="middle">W</forename><surname>Collier</surname></persName>
							<email>rem.collier@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.55,166.70,61.99,8.74"><forename type="first">John</forename><surname>Dunnion</surname></persName>
							<email>john.dunnion@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.93,116.95,295.50,12.62">UCD SIFT in the TREC 2011 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74DDAEEA00140382370FA3CCE2917935</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The SIFT (Segmented Information Fusion Techniques) group in UCD is dedicated to researching Data Fusion in Information Retrieval. This area of research involves the merging of multiple sets of results into a single result set that is presented to the user. As a means of both evaluating the effectiveness of this work and comparing it against other retrieval systems, the group entered Category B of the TREC 2011 Web Track. This involved the use of freely-available Information Retrieval tools to provide inputs to the data fusion process. This paper outlines the strategies of the 3 candidate entries submitted to compete in the ad-hoc task, discusses the methodology employed by them and presents a preliminary analysis of the results issued by TREC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the third year of the SIFT (Segmented Information Fusion Techniques) project's participation in the TREC Web Track. In an effort to build on the experience gained in last year's competition, it was once again decided to enter Category B. The principal aim of the SIFT group is to develop data fusion algorithms that combine the outputs of multiple Information Retrieval (IR) systems or algorithms in order to produce a single result-set that is of a superior quality. It should therefore be emphasised that the motivation behind our entry was not to evaluate novel IR systems or algorithms, but rather to investigate methods that may be used to combine these. In order to achieve this, the method employed uses implementations of standard, off-the-shelf IR algorithms (available as open source software) as the base systems for fusion and subsequently layers the fusion algorithms on top of these. This year's entry comprised three runs submitted to the ad-hoc task. The result sets for the 3 runs were generated using the fusion technique, SlideFuse, which was our best performing entry at TREC 2010. The entries differ, however, in respect of the number and type component systems fused as well as the amount of training data used in the construction of the fusion model.</p><p>The paper is organised as follows: Section 2 gives a short introduction to the area of Data Fusion. Section 3 provides implementation details for the data fusion technique SlideFuse. The procedures used to tune the parameters of these algorithms for the submitted runs, in addition to details of the component IR systems, are described in Section 4. Preliminary results are presented in Section 5. Possible directions for future entries are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Fusion</head><p>Data Fusion is an IR technique for combining the ranked lists returned by different component IR systems in response to a query. The goal is to produce an aggregate ranked list with improved performance over each of the individual lists. An inherent assumption within the data fusion context (as distinct from the related concept of collection fusion) is that each system retrieves from the same document collection. Techniques for fusion may be decomposed into two broad categories based on the level at which they access information:</p><p>1. Rank-based: the fusion algorithm is restricted to accessing the linearly scaled ranked lists output by the component systems and is not privy to the degrees of confidence underpinning these rankings. Such algorithms include approaches based on interleaving <ref type="bibr" coords="2,298.54,352.69,10.52,8.74" target="#b0">[1]</ref> and voting-based techniques [2, 3] 2. Score-based: the fusion algorithm may also take into account the relevance scores of the documents in the ranked list. These are the internally generated real numbers used by each IR system as a basis for calculating the rankings.</p><p>Linear combination <ref type="bibr" coords="2,239.57,401.47,10.52,8.74" target="#b3">[4]</ref> and the popular CombMNZ algorithm <ref type="bibr" coords="2,423.85,401.47,10.52,8.74" target="#b4">[5]</ref> are examples of methods based on relevance scores. These categories may be further sub-divided in accordance with whether they require training data to tune the parameters of the algorithm.</p><p>The fusion algorithm which was used to generate the results sets for the runs submitted to the ad-hoc task is part of a family of rank-based fusion techniques that may be termed "probabilistic". They are probabilistic in the sense that they attempt to build a model of the ranking behaviour of each component system, which may subsequently be used to estimate the probability that a document returned by that system at a particular rank will be relevant. A training phase is utilised to gather statistics about the past performance of each system from which such a probability distribution may be approximated. At the fusion stage this probability information is used as a means to combine and re-rank the documents returned by each system in response to a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SlideFuse</head><p>SlideFuse is a rank-based probabilistic data fusion algorithm that attempts to model the characteristic ranking behaviour of each component system using a probability distribution <ref type="bibr" coords="2,240.03,657.11,9.96,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Phase</head><p>The input to the training phase is a dataset consisting of a collection of result sets for which relevance judgments are available and a set of component systems. For such a training set of topics and a component system, the objective of the training phase is to ascribe probabilities to each ranked position, in a results list, that will represent the likelihood that a document appearing at this position will be relevant to any given topic. These probabilities may be calculated using the following formula:</p><formula xml:id="formula_0" coords="3,258.02,235.32,222.57,23.80">P (d p |s) = q∈Q R dp,q Q (1)</formula><p>where, P (d p |s) is the probability that a document d returned by input system s in position p of a result set is relevant, R dp,q is the relevance of the document d, at position p, to the training topic q (1 if the document is relevant, 0 if not) and Q is the set of training topics.</p><p>In practice, however, a problem arises when using the above formula to calculate such probabilities, due to the presence of un-judged documents in the result sets i.e. documents for which no relevance information is available. During the training procedure, it is quite likely that there may be many positions at which only judged non-relevant or un-judged documents are returned. Unfortunately, this leads to a zero value for the probabilities of relevance calculated for these positions.</p><p>In order to address this problem and obtain a smoother, more representative, probability distribution the concept of a sliding window is introduced. Instead of focusing on individual positions, as above, the probability values for the surrounding positions are also taken into consideration and an average value calculated. The size of the window, or number of neighbouring positions that are taken into account on each side of a position, is fixed for each ranked list with a suitable value for this parameter being determined empirically. An illustration of the smoothing effect of the sliding window is shown in Figure <ref type="figure" coords="3,418.58,484.13,4.98,8.74" target="#fig_0">1</ref> for a sample input system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion Phase</head><p>The output of the training phase is, for each component system, a set of values estimating the probability that a document returned at each rank position in the result list is relevant. The next step of the process is fusion. In this phase, each document is examined and its position in each of the result sets to be fused is noted. Depending on the position at which the document is returned, each system may then contribute towards that document's final ranking score, with no contribution occurring from any system that fails to return the document. The ranking score R d for each document d is given by equation 2, with M representing the number of input systems and P (d p,w ) the probability value based on the positions in the surrounding window, w: </p><p>Once R d has been calculated for each document, the documents are then merged into the final result set, sorted in descending order of R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TREC 2011 Entries</head><p>In order to prepare for entry into the competition it was necessary to select both a suitable training dataset and also the input systems to be used during fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data</head><p>As discussed above, the fusion algorithm requires a training phase to tune the parameters of the models that are built of the input systems. Ideally, for fusion to be successful, the data on which this training occurs should provide a representative sample that will be sufficient to capture the ranking behaviour of the models on future queries. In an effort to fulfil this requirement, the training strategy adopted was to use the ClueWeb09 Category B document collection in conjunction with the topics and relevance judgments available from TREC Web Track 2009 and 2010 <ref type="bibr" coords="4,227.06,569.90,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="4,239.24,569.90,7.01,8.74" target="#b7">8]</ref>. The value for the window size parameter required by SlideFuse, 5, was chosen based on successful performance in previous empirical work <ref type="bibr" coords="4,159.14,593.81,9.96,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input Systems</head><p>In order to focus development work on the design of fusion techniques the philosophy of the group is to use freely available open source IR software as a means for generating inputs to the fusion process. Two such packages, Indri<ref type="foot" coords="5,476.12,118.42,3.97,6.12" target="#foot_0">1</ref> and Terrier <ref type="bibr" coords="5,187.15,131.95,9.96,8.74" target="#b8">[9]</ref>, provided the backbone for this year's entry.</p><p>It was required that a subset of the IR algorithms, available in these packages, be selected to generate the inputs to the fusion process. Last year's entry focused on fusing together the output of algorithms from Terrier with the output generated by submitting "free text" queries to Indri. This year it was decided to take a different approach.</p><p>For the first entry, R1, the inputs to SlideFuse were generated by formulating the query separately on each of 5 fields associated with a document and querying Indri to return a ranked list for each e.g. by querying only on the title field, Indri returns documents that are restricted to contain the query terms in the title of the document. The fields chosen were document, heading, inlinks, title and url. In effect, this constitutes one potentially strong information source, the document field, and 4 weaker but, possibly more focussed information sources. The 100 queries from TREC 2009 and 2010, along with their associated relevance judgements, were used as training data for SlideFuse in R1. The second entry, R2, is the same as R1 but the model generated by SlideFuse is based on only 50 training queries, those taken from TREC 2009. The third entry, R3, fuses 3 algorithms from Terrier, In expB2, PL2 and DFR BM25, and is based on the same training data as R1. The constant in this years entry is the fusion technique, SlideFuse, and the variables are 1.) number of input systems 2.) type of component system and 3.) amount of training data used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ad-hoc task</head><p>Baseline To put our results for the R1 and R2 entries on the ad-hoc task in context, we required a baseline. For this purpose, following the release of the relevance judgements for TREC 2011, we retrospectively ran this year's 50 TREC queries as "free text" queries on the Indri search engine. It is our understanding that this method ranks the documents by utilising information from all sources associated with a document. It therefore has access to the same information base that was used by R1 and R2. It should be noted, however, that due to the pooling of competition entries employed by TREC, direct comparison against a noncompetiton entry is not correct e.g. across the 50 queries, 10% of the documents in the top 20 of the result lists returned for the "free text" queries were unjudged. Bearing this in mind, the results presented next should be viewed with caution. Table <ref type="table" coords="5,202.21,568.45,4.98,8.74" target="#tab_0">1</ref> shows the percentage improvement of the R1 entry over the baseline referred to above. It may be seen from this table that on the traditional precision metric, the improvement, 62%, is greatest for P5. Moving further down the ranked list this advantage is lessened, culminating with a 15% performance difference at P20. One of the reasons underlying this performance difference relates to the presence of spam e.g. R1 reduces the number of judged spam documents in the top 20 by 33% over the baseline. Graded Relevance Metrics Table <ref type="table" coords="6,304.91,203.30,4.98,8.74" target="#tab_1">2</ref> gives a summary of how all 3 entries performed against each other on the nDCG@20 metric, alongside the averages of the scores for the best, median and worst results for all entries. Referring to table <ref type="table" coords="6,159.02,239.17,4.98,8.74" target="#tab_1">2</ref> it is observed that R1 and R2 achieve scores that are better than or equal to the median on 31 and 30 queries respectively (60% of the total). The average values for R1 and R2 are 0.2021 and 0.1953, both of which exceed the average value for the median, 0.1876. The difference in performance between R1 and R2 is not as great as expected, taking into account that the R1 fusion model is based on twice the amount of training data. R3 performed comparatively poorly, getting a score better than or equal to the median on 20 queries. The mean value of 0.1358 is also significantly lower than the median average. This is somewhat surprising given that the fusion was based on 2 years training data. The interpretation of this result will depend on an analysis of the performance of the component systems that were used as inputs to the fusion process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Diversity task</head><p>The primary evaluation metrics for the diversity task are the so-called cascade measures. There has been much recent debate about the effectiveness/behaviour of these metrics and it has been proposed that they achieve a balance between novelty and overall precision in result lists <ref type="bibr" coords="6,328.98,597.34,14.61,8.74" target="#b9">[10]</ref>. Because our entries were not optimised to compete in the diversity task, we feel that our results may be useful as a baseline with respect to this debate. The ERR-IA metric is taken as the representative cascade metric for evaluation of our performance in the diversity task (the Spearman correlation coefficient between the results for ERR-IA@20 and alpha-nDCG@20 on our entry R1 was 0.97).</p><p>ERR-IA metric It is noted that absolute values of this metric for ambiguous queries should be viewed with respect to how it is calculated using the ndeval tool i.e. the scheme adopted by ndeval rewards systems which return documents that capture many facets of a topic and are positioned in the very top positions of the ranked lists. For an ambiguous query such as e.g. "source of the nile", it is unlikely that such an ideal is achievable.</p><p>The breakdown of our results on the ERR-IA@20 metric for our best performing entry, the R1 system, is presented in tables 3 and 4. Table <ref type="table" coords="7,433.13,203.68,4.98,8.74" target="#tab_2">3</ref> segments the data along 3 lines: 1.) The number of subtopics for a query -taking into account the 4 queries for which subtopics were removed, 2.) the number of queries in that category, 3.) the average value of ERR-IA@20, for R1, in this context. The similarity of the average scores across the different number of subtopics is a feature of table <ref type="table" coords="7,205.48,263.45,3.87,8.74" target="#tab_2">3</ref>, although perhaps this should be conditioned on the number of queries available in each category. Table <ref type="table" coords="7,329.07,275.41,4.98,8.74" target="#tab_3">4</ref> shows the same data as table <ref type="table" coords="7,472.84,275.41,3.87,8.74" target="#tab_2">3</ref>, with the number of subtopics for a query replaced by an indicator of whether the query was faceted (F.) or ambiguous (A.). A failure of R1 to return a document relevant to more than one subtopic in the first 2 ranking positions is a contributory factor to the low average value, 0.35, for ERR-IA@20 on the ambiguous queries.  To study what happens in this region, figure <ref type="figure" coords="7,391.03,621.25,4.98,8.74" target="#fig_3">3</ref> presents a zoomed in view of the graph in figure <ref type="figure" coords="7,276.19,633.20,3.87,8.74" target="#fig_1">2</ref>. Additional context is provided by plotting results on the metric for 2 hypothetical systems Ideal and Worst. Starting with the value of ERR-IA at rank position 10, the Ideal system maximises the value of the metric attainable in the remaining positions, whereas the Worst system returns documents that are not relevant to any subtopics from ranks 11 to 20. It may be seen from this that there is still room for improvement up to rank 15, however improvements for subsequent positions are difficult to quantify.   <ref type="table" coords="8,177.20,633.20,4.98,8.74" target="#tab_4">5</ref> presents an overview of how all 3 entries performed relative to each other and the averages of the scores for the best, median and worst results on the ERR-IA@20 metric. R1 and R2 maintain their performance from the ad-hoc task, by achieving a score above or equal to the median on roughly 60% of the queries. Their average values across the 50 queries, 0.4546 and 0.44, are also above the median average of 0.4079. Similar to the results from the ad-hoc task, the difference between the performance of R1 and R2 is surprisingly small. The poor performance of R3 on the diversity task is more pronounced than on the ad-hoc task, scoring better than or equal to the median on just 12 queries and with an average value of 0.291. It should be stressed though, that the fusion technique is not optimised for this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P-IA metric</head><p>In order to gain a different perspective on the behaviour of our entries in the diversity task it is instructive to assess the results of a non-cascade measure. Figure <ref type="figure" coords="9,207.74,585.38,4.98,8.74">4</ref> plots the results for the R1 entry on the P-IA metric against rank position. With reference to this figure it may be seen that after the first 2 rank positions there is a noticeable drop, with a steadily decreasing trend in the values thereafter. Taken together with figure <ref type="figure" coords="9,346.89,621.25,3.87,8.74" target="#fig_1">2</ref>, above, this graph appears to support the intuition that our results in the diversity task are primarily based on the ability of the R1 and R2 entries to return documents in the first 2 rank positions that satisfy the diversity criteria encouraged by the cascade metrics.</p><p>To date, the analysis of this year's results is preliminary and definite conclusions will require a deeper analysis of the probability models generated by each entry. There are, however, a number of possible directions for future work to take. The models of the input systems, generated from the training data by SlideFuse, do not take graded relevance information into account i.e. an input system returning highly relevant or key documents to a query, receives the same credit as one returning documents that are relevant but perhaps not essential. Intuitively, this seems to be a sub-optimal approach. A step forward in this direction would be to model separately the probability of an input system returning highly relevant documents and incorporate this into the fusion process. A second area that may lead to improved performance, would be to learn to rank the input systems prior to the training phase of the fusion process. In particular, for the simpler input systems used by the R1 and R2 entries this would be expected to yield better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,211.14,262.80,193.07,7.89;4,202.88,116.83,209.60,131.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Probability Distribution using SlideFuse</figDesc><graphic coords="4,202.88,116.83,209.60,131.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,149.71,585.38,330.89,8.74;7,134.77,597.34,345.82,8.74;7,134.77,609.29,345.83,8.74;7,134.77,621.25,345.82,8.74;7,134.77,633.20,345.82,8.74;7,134.77,645.16,345.82,8.74;7,134.77,657.11,345.83,8.74"><head>Figure 2</head><label>2</label><figDesc>Figure 2 plots the value of the ERR-IA metric for the R1 entry at the first 20 rank positions. With reference to this figure it may be seen that the majority of the gain is attained in the first 5 positions. The curve flattens out after rank position 10. To study what happens in this region, figure 3 presents a zoomed in view of the graph in figure 2. Additional context is provided by plotting results on the metric for 2 hypothetical systems Ideal and Worst. Starting with the value of ERR-IA at rank position 10, the Ideal system maximises the value</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,180.07,329.92,255.22,7.89;8,234.67,187.35,146.03,127.80"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Plot of ERR-IA against Rank Position for the R1 entry</figDesc><graphic coords="8,234.67,187.35,146.03,127.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,578.42,345.83,7.89;8,134.77,589.40,100.64,7.86;8,210.33,393.25,194.70,170.40"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Plot of ERR-IA against Rank Position for the R1 entry vs the hypothetical Ideal and Worst systems</figDesc><graphic coords="8,210.33,393.25,194.70,170.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,149.72,116.91,315.93,47.47"><head>Table 1 .</head><label>1</label><figDesc>Percentage improvement of R1 over baseline for the precision metric</figDesc><table coords="6,223.22,140.77,168.92,23.60"><row><cell>Metric</cell><cell cols="4">P5 P10 P15 P20</cell></row><row><cell cols="2">% Improvement 62</cell><cell>34</cell><cell>24</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,182.44,390.15,250.48,94.69"><head>Table 2 .</head><label>2</label><figDesc>Results for R1, R2 and R3 on the nDCG@20 metric</figDesc><table coords="6,225.88,414.01,163.61,70.83"><row><cell>Entry</cell><cell>R3</cell><cell>R2</cell><cell>R1</cell></row><row><cell>&gt;= median</cell><cell>20</cell><cell>30</cell><cell>31</cell></row><row><cell>nDCG@20</cell><cell>0.1358</cell><cell>0.1953</cell><cell>0.2021</cell></row><row><cell>All entries</cell><cell>Best</cell><cell cols="2">Median Worst</cell></row><row><cell>nDCG@20</cell><cell>0.5370</cell><cell>0.1876</cell><cell>0.0106</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,177.65,365.31,260.06,63.21"><head>Table 3 .</head><label>3</label><figDesc>Breakdown of the average value of ERR-IA@20 for R1</figDesc><table coords="7,185.08,389.17,245.19,39.35"><row><cell>Number of subtopics</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>ERR-IA@20</cell><cell>0.45</cell><cell>0.45</cell><cell>0.44</cell><cell>0.51</cell><cell>0.47</cell></row><row><cell>Number of queries</cell><cell>6</cell><cell>29</cell><cell>11</cell><cell>3</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,177.65,484.43,260.06,63.21"><head>Table 4 .</head><label>4</label><figDesc>Breakdown of the average value of ERR-IA@20 for R1</figDesc><table coords="7,238.31,508.29,138.73,39.35"><row><cell>Query Type</cell><cell>F.</cell><cell>A.</cell></row><row><cell>ERR-IA@20</cell><cell>0.48</cell><cell>0.35</cell></row><row><cell>Number of queries</cell><cell>41</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,178.89,233.52,257.58,279.13"><head>Table 5 .</head><label>5</label><figDesc>Results for R1, R2 and R3 on the ERR-IA@20 metric</figDesc><table coords="9,224.41,257.38,166.54,70.83"><row><cell>Entry</cell><cell>R3</cell><cell>R2</cell><cell>R1</cell></row><row><cell>&gt;= median</cell><cell>12</cell><cell>27</cell><cell>30</cell></row><row><cell>ERR-IA@20</cell><cell>0.291</cell><cell>0.44</cell><cell>0.4546</cell></row><row><cell>All entries</cell><cell>Best</cell><cell cols="2">Median Worst</cell></row><row><cell cols="2">ERR-IA@20 0.7441</cell><cell>0.4079</cell><cell>0.0346</cell></row></table><note coords="9,186.85,504.75,241.65,7.89"><p>Fig. 4. Plot of P-IA against Rank Position for the R1 entry</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,657.79,95.55,7.86"><p>http://lemurproject.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This material is based upon works supported by <rs type="funder">Science Foundation Ireland</rs> under Grant No. <rs type="grantNumber">08/RFP/CMS1183</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zTWcMq5">
					<idno type="grant-number">08/RFP/CMS1183</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,382.92,337.64,7.86;10,151.52,393.87,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,352.79,382.92,123.24,7.86">The Collection Fusion Problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.38,393.87,251.41,7.86">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,404.03,337.64,7.86;10,151.52,414.99,329.07,7.86;10,151.52,425.94,326.31,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,266.69,404.03,88.38,7.86">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,376.71,404.03,103.89,7.86;10,151.52,414.99,329.07,7.86;10,151.52,425.94,119.68,7.86">SIGIR &apos;01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,436.10,337.63,7.86;10,151.52,447.06,329.07,7.86;10,151.52,458.01,256.25,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,264.32,436.10,154.56,7.86">Condorcet fusion for improved retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,438.42,436.10,42.17,7.86;10,151.52,447.06,329.07,7.86;10,151.52,458.01,48.64,7.86">CIKM &apos;02: Proceedings of the eleventh international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,468.17,337.63,7.86;10,151.52,479.10,139.34,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,265.47,468.17,172.25,7.86">Fusion Via a Linear Combination of Scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,445.75,468.17,34.83,7.86;10,151.52,479.12,55.19,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,489.25,337.63,7.89;10,151.52,500.24,32.25,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,193.29,489.28,167.98,7.86">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,368.92,489.28,54.64,7.86">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,510.39,337.64,7.86;10,151.52,521.35,329.07,7.86;10,151.52,532.31,297.71,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,335.08,510.39,145.51,7.86;10,151.52,521.35,94.05,7.86">Extending Probabilistic Data Fusion Using Sliding Windows</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.42,521.35,206.18,7.86;10,151.52,532.31,134.05,7.86">Proceedings of the 30th European Conference on Information Retrieval (ECIR &apos;08)</title>
		<meeting>the 30th European Conference on Information Retrieval (ECIR &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008-04-02">31st March -2nd April 2008</date>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,542.46,337.63,7.86;10,151.52,553.42,329.07,7.86;10,151.52,564.38,107.84,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,302.78,542.46,158.90,7.86">Overview of the TREC-2009 Web Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.67,553.42,249.76,7.86">TREC2009: Proceedings of the 18th Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,574.53,337.63,7.86;10,151.52,585.49,329.07,7.86;10,151.52,596.44,166.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,364.64,574.53,115.95,7.86;10,151.52,585.49,41.82,7.86">Overview of the TREC-2010 Web Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,226.12,585.49,250.21,7.86">TREC2010: Proceedings of the 19th Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,606.60,337.64,7.86;10,151.52,617.56,329.07,7.86;10,151.52,628.51,248.77,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,453.13,606.60,27.47,7.86;10,151.52,617.56,119.64,7.86">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,294.53,617.56,186.06,7.86;10,151.52,628.51,144.03,7.86">Proceedings of the 27th European Conference on Information Retrieval (ECIR 05)</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR 05)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,638.67,337.98,7.86;10,151.52,649.63,290.42,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,375.46,638.67,105.13,7.86;10,151.52,649.63,169.45,7.86">A comparative analysis of cascade measures for novelty and diversity</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,342.75,649.63,38.86,7.86">WSDM&apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
