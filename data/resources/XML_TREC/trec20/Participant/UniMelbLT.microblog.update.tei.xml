<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,104.72,81.18,402.55,12.64">Melbourne Language Technology Group Microblog Track Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,197.29,115.19,35.63,10.53"><forename type="first">Bo</forename><surname>Han</surname></persName>
							<email>hanb@student.unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Software Systems</orgName>
								<orgName type="laboratory">NICTA Victoria Research Laboratory</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.13,115.19,54.57,10.53"><forename type="first">Marco</forename><surname>Lui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Software Systems</orgName>
								<orgName type="laboratory">NICTA Victoria Research Laboratory</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.93,115.19,88.78,10.53"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Software Systems</orgName>
								<orgName type="laboratory">NICTA Victoria Research Laboratory</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,104.72,81.18,402.55,12.64">Melbourne Language Technology Group Microblog Track Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">788C88DC6CDC9ED9CE92F26934A8C74C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report outlines the TREC 2011 microblog track submission of the Language Technology Group at The University of Melbourne. The microblog track is an ad-hoc retrieval task over Twitter data with temporally-specified queries, and the requirement that all results must predate the query. Our objective is to establish baseline results for the task and study the relative impact of various factors on microblog retrieval. Twitter messages are authored in many different languages <ref type="bibr" coords="1,189.86,361.51,85.83,9.59" target="#b2">(Hong et al., 2011)</ref>, but the queries were all monolingual English and assessors where instructed to base their judgements on only the English content of tweets. As such, we first conduct language identification to filter out non-English tweets <ref type="bibr" coords="1,160.61,429.26,106.13,9.59" target="#b0">(Baldwin and Lui, 2010)</ref>. Next, we lexically-normalise tweets, to remove typos and phonetic substitutions, and deabbreviate common abbreviations <ref type="bibr" coords="1,133.25,469.90,108.99,9.59" target="#b1">(Han and Baldwin, 2011)</ref>. Finally, we index the language-filtered, normalised documents using Indri,<ref type="foot" coords="1,122.16,494.98,3.99,7.01" target="#foot_0">1</ref> apply dynamic lexical normalisation to the queries, and temporally filter the results relative to the query timestamp. Descriptions of each module in our system are presented in the following sections.</p><p>As we use language processing tools and a dictionary as part of the lexical normalisation, our submission is classified as making use of external evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Language Identification</head><p>For language identification, we used langid.py, a language identification toolkit developed at The University of Melbourne <ref type="bibr" coords="1,183.22,666.94,101.70,9.59" target="#b3">(Lui and Baldwin, 2011</ref>).<ref type="foot" coords="1,294.32,664.92,3.99,7.01" target="#foot_1">2</ref> langid.py combines a naive Bayes classifier with cross-domain feature selection to provide domainindependent language identification. It is available under a FOSS license as a stand-alone module pre-trained over 97 languages. In in-house evaluation over short text messages, we found that langid.py was much faster than competing automatic language identification systems without any loss in accuracy.</p><p>We apply langid.py to a combined crawl of 15,198,435 tweets based on the official crawling tool. langid.py returns a monolingual prediction of language content for a given document. All documents which are predicted to be non-English were removed from the dataset, resulting in 5,478,459 (putatively) English tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexical Normalisation</head><p>Lexical normalisation is potentially relevant to the retrieval task, since noisy tokens are prevalent in microblogs, and tend not to be picked up on by standard token normalisation techniques such as stemming. Types of noisy tokens which we target in lexical normalisation are typos (e.g. earthquak "earthquake"), abbreviations (e.g. lv "love"), phonetic substitutions (e.g. b4 "before") and vowel lengthening (e.g. goooood "good"). We suggest that lexical normalisation is particularly pertinent for recall, but note that the evaluation metric of choice for the microblog track is precision-based, meaning that its impact on our official results may be slight.</p><p>Ultimately, we are interested in performing context-sensitive lexical normalisation รก la <ref type="bibr" coords="1,503.09,645.03,36.90,9.59;1,313.20,658.58,70.69,9.59" target="#b1">Han and Baldwin (2011)</ref>. For the purposes of the microblog track, however, we chose to go for a high-precision, low-recall approach and avoid overnormalising correct unknown words. Therefore, we utilise the dictionary lookup method of <ref type="bibr" coords="1,500.86,712.78,39.14,9.59;2,72.00,75.74,68.86,9.59" target="#b1">Han and Baldwin (2011)</ref> to substitute noisy tokens with high confidence (e.g. u to you). For our pre-filtered English tweets, 1,279,169 tokens were normalised, and 868,993 tweets were influenced by normalisation, indicating the prevalence of noisy tokens in microblog data.</p><p>4 Data Processing, Indexing and Querying</p><p>We index the data with Indri, based on the TREC data format. Each document contains a single textbased tweet, with timestamp in the form of an unsigned integer. For instance, Jan 31 05:11:48 is mapped into 131051148, where month is placed in the first digit, followed by a direct conversion of the date to digits. We preserve case information in the queries, and used the Indri Retrieval Model <ref type="bibr" coords="2,72.00,300.15,102.38,9.59" target="#b4">(Strohman et al., 2005)</ref> with TREC-format outputs. The results are ordered by timestamp in decreasing order before the query time. Any results which a timestamp later than the query are removed from the result set according to the task guidelines.<ref type="foot" coords="2,254.70,352.32,3.99,7.01" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Precision@N was selected as the primary evaluation metric for the microblog task, using N = 30 to determine official results. We compare the precision of our method with and without both language identification and lexical normalisation in Figure <ref type="figure" coords="2,290.62,456.81,4.09,9.59">5</ref>, at different cutoff points in the result ranking.</p><p>The overall Precision@30 was 0.2565 (normalised) and 0.2571 (original) on English-only tweets. The four run settings yielded almost identical results for Precision@30. However, over fewer results, language identification improves precision by about 2% relative to the full document set. We also notice that lexical normalisation generally doesn't boost precision, and actually degrades precision slightly when using all tweets. <ref type="foot" coords="2,234.28,590.88,3.99,7.01" target="#foot_3">4</ref> The only exception is at Precision@20, when lexical normalisation delivers marginally better results both with and without language identification. We further investigate these observations from the viewpoints of the retrieval model, language identification and normalisation.</p><p>In our approach, we don't specifically tune the system parameters or perform query expansion, but instead consider the Indri Retrieval Model as a black box, combining factors such as a language model, term vectors and smoothing. As a result, we hypothesise that the setup is tweaked in favour of long documents, and that further improvements should be possible by tweaking the underlying IR engine. We confirm this hypothesis by manually checking the retrieval results. Over the English-only document collection, the highly-ranked documents are generally highly readable and well structured, regardless of relevance. For results on all tweets, some non-English tweets are present in the higher reaches of the document ranking, due to the occurrence of only one of the query terms. Language identification filters out most of these, but as N increases, more high-quality irrelevant tweets find their way into the results, pulling down precision.</p><p>We further manually compare results from the original and normalised English tweets at Precision@30 (hereafter denoted as en raw and en norm). We find that en raw returns two additional relevant tweets for both topic 8 (phone hacking British politicians) and topic 14 (release of The "Rite"), while en norm has four additional relevant tweets for topic 14 that are ranked in the top-20 results, which explains the prominence in the Figure <ref type="figure" coords="2,426.34,712.78,5.45,9.59">5</ref> at Precision@20. While we hypothesised that lexical normalisation would predominantly impact on recall, there were isolated instances of it being able to reduce false positives in the retrieval results. For instance, in en raw, some occurrences of rite are actually typos for right, but those tweets are selected as valid results. However, in en norm, the standalone word rite is normalised to right, while quoted occurrences of "rite" are preserved as valid results.</p><p>In the future, we plan to enhance our lexical normalisation method to boost the ranking of poorly structured but relevant tweets, so that the document ranking is not dominated by high quality (but potentially irrelevant) tweets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,313.20,222.05,226.80,8.76;2,313.20,233.80,226.80,9.30;2,313.20,245.96,226.80,8.76;2,313.20,257.91,226.80,8.76;2,313.20,269.87,135.16,8.76"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Average Precision@N across all topics for varying N โ {5, 10, 15, 20, 30} and with different settings (all raw = all tweets; all norm = lexical normalisation only; en raw = English tweets; en norm = English tweets with lexical normalisation)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.14,702.89,133.57,7.88"><p>http://sourceforge.net/projects/lemur/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,88.14,714.06,215.70,7.88"><p>http://www.csse.unimelb.edu.au/research/lt/resources/langid</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,88.14,670.01,180.82,7.88;2,72.00,680.97,36.86,7.88"><p>https://sites.google.com/site/microblogtrack/2011guidelines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,84.65,690.31,2.99,5.25;2,88.14,692.14,210.65,7.88;2,72.00,703.10,226.79,7.88;2,72.00,714.06,29.64,7.88"><p> 4  Although less than we might expect given that the method is applied to all documents, the majority of which are non-English.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,72.00,306.05,226.81,8.76;3,82.91,318.00,215.89,8.76;3,82.91,330.05,215.90,8.55;3,82.91,342.00,215.90,8.55;3,82.91,353.87,215.89,8.76;3,82.91,365.82,77.47,8.76" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,236.50,306.05,62.31,8.76;3,82.91,318.00,181.15,8.76">Language identification: the long and the short of the matter</title>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,283.31,318.09,15.49,8.55;3,82.91,330.05,215.90,8.55;3,82.91,342.00,215.90,8.55;3,82.91,353.96,144.75,8.55">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.00,378.53,226.81,8.76;3,82.91,390.48,215.91,8.76;3,82.91,402.44,215.89,8.76;3,82.91,414.48,215.90,8.55;3,82.91,426.35,215.89,8.76;3,82.91,438.30,44.54,8.76" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,234.10,378.53,64.71,8.76;3,82.91,390.48,212.25,8.76">Lexical normalisation of short text messages: makn sens a #twitter</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,94.09,402.53,204.71,8.55;3,82.91,414.48,215.90,8.55;3,82.91,426.44,78.92,8.55">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.00,451.00,226.81,8.76;3,82.91,462.96,215.88,8.76;3,82.91,475.00,215.89,8.55;3,82.91,486.87,125.89,8.76" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,259.52,451.00,39.28,8.76;3,82.91,462.96,151.03,8.76">Language matters in Twitter: a large scale study</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Convertino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,254.31,463.05,44.48,8.55;3,82.91,475.00,210.99,8.55">AAAI Conference on Weblogs and Social Media (ICWSM&apos;11)</title>
		<meeting><address><addrLine>Barcelona; Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.00,499.57,226.81,8.76;3,82.91,511.53,215.89,8.76;3,82.91,523.57,215.90,8.55;3,82.91,535.44,215.88,8.76;3,82.91,547.39,130.06,8.76" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,242.91,499.57,55.90,8.76;3,82.91,511.53,175.93,8.76">Cross-domain feature selection for language identification</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,280.99,511.62,17.81,8.55;3,82.91,523.57,215.90,8.55;3,82.91,535.53,185.30,8.55">Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011)</title>
		<meeting>the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011)<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="553" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,72.00,560.09,226.81,8.76;3,82.91,572.05,215.90,8.76;3,82.91,584.01,215.89,8.76;3,82.91,596.05,209.84,8.55" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,176.80,572.05,122.01,8.76;3,82.91,584.01,136.36,8.76">Indri: a language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,238.89,584.10,59.91,8.55;3,82.91,596.05,205.81,8.55">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
