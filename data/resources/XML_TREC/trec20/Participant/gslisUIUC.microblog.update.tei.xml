<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.34,137.98,401.31,15.12;1,184.40,159.90,243.20,15.12">The University of Illinois&apos; Graduate School of Library and Information Science at TREC 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.81,193.80,57.37,10.48"><forename type="first">Miles</forename><surname>Efron</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.37,193.80,64.32,10.48"><forename type="first">Adam</forename><surname>Kehoe</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.49,193.80,89.24,10.48"><forename type="first">Peter</forename><surname>Organisciak</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.92,193.80,55.28,10.48"><forename type="first">Sunah</forename><surname>Suh</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.79,207.75,48.13,10.48"><forename type="first">E</forename><surname>Daniel</surname></persName>
							<affiliation key="aff0">
								<address>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.34,137.98,401.31,15.12;1,184.40,159.90,243.20,15.12">The University of Illinois&apos; Graduate School of Library and Information Science at TREC 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A89BC8304E85D39B284AA795FFF7DCBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Illinois' Graduate School of Library and Information Science (GSLIS) participated in TREC's microblog track this year-the track's first iteration. In keeping with the foundational status of the track, our goals were chosen to emphasize the role of a single factor in microblog IR: time. As such, we adhered to the strictest guidelines of the task description, using only real-time information available in the microblog corpus to inform retrieval. Our innovation involved assessing the extent to which (and the way in which) temporal factors can improve microblog search effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Data</head><p>The TREC 2011 microblog collection consists of a corpus of microblog posts made available by the microblogging service Twitter 1 . Track organizers created 50 test topics and accumulated relevance judgments in a fashion similar to the standard TREC pooling method. Details of this process are available in the track overview paper.</p><p>Instead of distributing the microblog corpus via physical media or a direct download, the posts (known as "tweets") comprising the corpus were made available to participants in a two-stage process. Organizers defined the scope of the corpus by enumerating a set of approximately 16M unique tweet ID numbers. Participants downloaded these ID's, along with software that allowed them to fetch the tweets themselves directly from Twitter.</p><p>This process bears mentioning because the download process offered participants two methods of retrieving tweets. These methods yielded slightly different corpora. Twitter offers an API whose users can request a particular tweet by its ID number. Such an API call delivers not only the text of the tweet, but several pieces of metadata (mostly giving statistics regarding the tweet author) are also included. These data are delivered to the client in JSON format. However, the Twitter API enforces a rate limit that makes downloading the entire corpus prohibitively slow. Some participants had previous arrangements with Twitter giving them "whitelist" access to much API calls. We were among these groups. However, many participants did not have access to this resource, so we used the more restricted HTML mode of access to make our results comparable with a lowest common denominator among track participants. The HTML data lacked most metadata found in the JSON representation. Additionally some participants had difficulty acquiring particular tweets via the HTML method, yielding corpora with fewer documents than those obtained from the API. Our corpus contained 15,653,612 tweets, each containing: the user name of the author, the time at which the tweet was posted, and the tweet text itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Base System</head><p>For core indexing and retrieval we used the Indri search engine and API<ref type="foot" coords="2,431.33,263.60,4.23,6.99" target="#foot_0">2</ref> . Our baseline approach was the simple query likelihood retrieval model. We used Jelinek-Mercer smoothing with smoothing parameter λ = 0.4. Thus, given a query Q and a document D, we derive a score according to:</p><formula xml:id="formula_0" coords="2,210.09,328.62,310.11,34.74">P r(Q|D) • P r(D) = n(Q) i=1 P r(q i |D)P r(D)<label>(1)</label></formula><p>where the language model probabilities are estimated by: P r(</p><formula xml:id="formula_1" coords="2,229.24,395.79,290.96,24.43">q i |D) = λ n(q i , D) n(D) + (1 -λ) n(q i , C) n(C)<label>(2)</label></formula><p>where n(q i , D) is the number of times word q i occurs in D, n(D) is the length of D, n(q i , C) is the frequency of q i in the collection, and n(C) is the total number of word tokens in the collection. Typically we take the prior probability P r(D) to be uniform, though we describe one experiment below using non-uniform priors. Very little pre-processing was used in our experiments. We did not stem documents. We did create a stoplist of 133 terms. A few of these terms were familiar stopwords, which we included in the stoplist to improve proposed document expansion models. But most of these words were unique to the Twitter environment. For example, we removed words such as fb, ff, tinyurl and twitpic. Again, these were removed to reduce their influence during document expansion.</p><p>To improve retrieval we made an effort to remove non-English tweets from results. We excluded all tweets that contained more than 4 characters with encoding greater than 255 to remove non-western alphabets. We also applied a very crude filter for foreign language tweets by defining a set of 132 words that are very common in languages including French, Spanish and German. Any tweet containing one of these putatively foreign words was excluded during retrieval. It is important to stress that this filtering was accomplished via a list of words that we created based on our own knowledge. We believe that this does not constitute external evidence as described below (track organizers stated that stoplists were not external evidence, and we believe that our list of foreign words is analogous to a stoplist).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TREC 2011 Microblog Task</head><p>A full description of the task completed by participants in the microblog track is available in the track's overview paper. We offer only a brief synopsis here. The task involved finding tweets that were relevant to a keyword query issued at a particular time. The track corpus spanned a two-week period of Twitter activity and each test query had an associated time-stamp. Thus the scenario imagined a user issuing a keyword query at time t. Track participants only retrieved those documents written prior to t. Unlike many ad hoc IR scenarios, the task did not involve relevance-based ranking. Instead, systems retrieved a set of 30 documents that they deemed most relevant. The official performance metric was precision at 30 (P30).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">External and Future Evidence</head><p>Track organizers classified runs into categories based on the types of evidence they used. Evidence could consist of:</p><p>• Internal evidence: Data available within the corpus proper.</p><p>• External evidence: Information taken from sources not present in the downloaded corpus. For instance, many tweets contain links to URLs. Text acquired from linked URLs constitutes external evidence. Interestingly, many links on Twitter are shortened. Organizers defined the unshortened (i.e. resolved) text of these URLs as external evidence.</p><p>• Future evidence: The microblog retrieval task was inherently temporal, with each query "taking place" at a given time. Any evidence that would not have been available at query time was considered future evidence.</p><p>A surprising result of the track's evidence typology is that using corpus-level statistics requires care if we wish to avoid using future evidence. For example, smoothing language models with probabilities based on the entire corpus relies on evidence that accumulated after any of the supplied queries. To adhere to the track's strict guidelines, any smoothing must rely on word counts observed no later than query time.</p><p>All teams were required to submit at least one run that used only internal evidence. Among our stated goals was to adhere to the track's strictest guidelines in efforts to make the effect of our proposed innovations as clear as possible. Therefore, all of our runs used only internal evidence. To remove the influence of future evidence on document smoothing we employed the following approach. For a given query Q issued at time t we retrieved n = 2000 documents using Indri (admittedly, with corpus-level statistics informing smoothing via linear interpolation). We then calculated background probabilities for each query word at time t using the Indri query language. The 2000 retrieved documents were then re-ranked by smoothing based on the newly estimated background model. Only the top 30 of these re-ranked documents were submitted. While it is the case that word counts observed after time t influenced the set of 2000 initially retrieved documents, the brevity of the test topics makes it unlikely that the final 30 retrieved documents were influenced by this effect. Thus we believe that our runs did not rely on future evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted Runs: Temporally Informed Microblog Search</head><p>The temporal nature of this year's track description speaks to the strong role that time plays in microblog IR. We hypothesized that relevance in this context would have an inherently temporal dimension and that capitalizing on this would improve retrieval effectiveness. Specifically, we made use of that fact that each document D has a time-stamp t D indicating when it was posted to Twitter. Likewise, each query has a time-stamp t Q indicating when it was issued. Our working hypothesis was that this temporal information could improve effectiveness over the simple lexical model.</p><p>When accounting for time in retrieval, we calculated a document's temporal information t D as the amount of time that elapsed since the tweet was published and the time the query was issued, measured in fractions of days. Thus t D is how old D is.</p><p>We tested three methods of capitalizing on recency in retrieval:</p><p>• Temporal Priors: Following Li and Croft <ref type="bibr" coords="4,316.05,450.77,10.91,9.57" target="#b3">[4]</ref>, promote newer documents by assuming that P r(D) follows an exponential distribution on document age. [No official runs submitted.]</p><p>• Temporal Smoothing: Roughly following Efron and Golovchinsky <ref type="bibr" coords="4,442.53,500.39,10.91,9.57" target="#b0">[1]</ref>, smooth language models of older documents more aggressively than models for recently published documents. [Runs gus and gustc.]</p><p>• Temporal Profiles: This was to our knowledge a novel approach to temporal IR. We describe it in depth below. [Run gut.]</p><p>Of these three methods, two have been described in previous work, while a third is to our knowledge novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Temporal Document Priors</head><p>A standard approach to letting recency influence retrieval in the language modeling framework is to introduce document priors based on time <ref type="bibr" coords="5,351.08,132.69,10.91,9.57" target="#b3">[4]</ref>. The standard query likelihood model can be augmented as follows:</p><p>P r(D|Q) ∝ P r(Q|D)P r(D|t D )</p><p>where P r(D|t D ) can be understood as the probability that D is of general interest given that it was published at time t D . Li and Croft use the exponential distribution with rate parameter r for this prior, giving P r(D|t</p><formula xml:id="formula_3" coords="5,310.88,218.29,84.11,10.77">D ) = r exp[-r t d ].</formula><p>Based on prior published literature, we set r = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Temporal Document Smoothing</head><p>We submitted two runs that used temporally informed language model smoothing, as in <ref type="bibr" coords="5,105.50,294.87,10.91,9.57" target="#b0">[1]</ref>. These runs, gus and gustc smooth language models more aggressively for older documents, lending retrieval a document age penalty. We use the following quantity to smooth language models:</p><formula xml:id="formula_4" coords="5,228.10,340.72,292.11,25.50">λ D = 0.5 • (0.4 • t D t max ) + 0.5 * 0.4<label>(4)</label></formula><p>where t D is the age of document D as described above, and t max is the time-stamp of the oldest document in the collection. Eq. 4 mixes a temporal component with our standard smoothing value 0.4 using equal weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Tweets as Queries: Retrieval-based Evidence</head><p>Our most successful submitted run, gut, was based on a novel approach to considering time in IR. This approach improved retrieval effectiveness significantly. The approach uses a two-stage process. First we perform a standard ad hoc retrieval. The second phase involves re-ranking retrieved documents based on their observed characteristics. In the case of our submitted run gut the observed characteristic was temporal, but this need not be the case.</p><p>The evidence that we used for document re-ranking was obtained by submitting a given document D as a pseudo-query against the collection. This yielded a retrieved set of tweets R = r 1 , r 2 , . . . , R k where we set k = 50. We call the documents retrieved by using D as a pseudo-query the "retrieved set" for D. For instance, consider document 29983478363717633 from the microblog corpus:</p><p>[BBC News] Major cuts to BBC World Service: BBC World Service is to close five of its language services, with th... http://bbc.in/e2vlpX</p><p>This document was retrieved by the topic MB001, which we call Q. After processing this tweet according to the specifications described in Section 3 we have the pseudo-query: bbc news major cuts to bbc world service bbc world service is to close five of its language services with th bbc in e2vlpX</p><p>We ran this pseudo-query, obtaining a retrieved set of 50 documents that are putatively related to the original document 29983478363717633. During retrieval, we derive a retrieved set for each document among the top 100 tweets returned for the original query Q. We then re-ranked these 100 documents based on statistics calculated from their retrieved sets, finally returning the top 30 documents. We speculated that the retrieved set of each ranked document D lends additional information to the problem of estimating the relevance of D to Q.</p><p>The motivation for using pseudo-queries and their retrieved sets was twofold:</p><p>1. Tweets are typically about only one topic.</p><p>2. High-quality tweets are likely to evince strong topicality, while trivial, unexpressive tweets will have a less clear topic.</p><p>Points 1 and 2 suggest that the retrieved set of a relevant document is likely to have qualities that differ from non-relevant documents' retrieved sets. In particular, we focused on the time-stamps of documents. We hypothesized that the time-stamps of documents in the retrieved set for a relevant document would share a distribution similar to the distribution of time-stamps found among the documents retrieved for the initial query. On the other hand, non-relevant documents' retrieved sets will show a distribution of time-stamps that differs from Q's.</p><p>We refer to the time-stamps of document D's retrieved set as the "temporal profile" of D<ref type="foot" coords="6,113.81,417.00,4.23,6.99" target="#foot_1">3</ref> . Specifically, for a document D, the temporal profile T D is the times of documents retrieved by submitting D as a pseudo-query. In other words, T D consists of the ages of the first 50 documents retrieved by using D as a pseudo-query.</p><p>Figure <ref type="figure" coords="6,144.28,459.60,5.45,9.57" target="#fig_1">1</ref> schematizes the information that underlies our approach. The figure plots kernel density estimates obtained from the result sets from three retrievals: the original text of TREC topic MB001, a document that is relevant to MB001, and a non-relevant document that was retrieved by our system for MB001. The x-axis of the figure shows document age (measured from the end of the microblog corpus timespan). The black line plots the empirical density of the time-stamps retrieved by the original query. We can see that a large number of retrieved documents were written about two days "ago."</p><p>The blue and red lines show the densities of time-stamps observed when we submit a relevant and a non-relevant document as a pseudo-query, respectively-i.e. each document's temporal profile.</p><p>Two results are evident in Figure <ref type="figure" coords="6,266.06,595.09,4.24,9.57" target="#fig_1">1</ref>. First, the temporal profile of the relevant document allocates much of its probability mass in the same region as the query distribution's mode. On the other hand, the non-relevant document's temporal profile diverges widely from the  query's temporal profile. Putting it more concretely, the density of the relevant document's temporal profile has a smaller Kullback-Leibler divergence from the query profile than the non-relevant document's profile does. Secondly, both the query and the relevant document's densities show a stark mode near x = 2. In contrast, the non-relevant document's temporal profile allocates probability mass more uniformly over the domain of x.</p><p>These two observations speak to points 1 and 2 listed above. Each of them influenced our retrieval approach as described in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Use of Temporal Profiles</head><p>We submitted one run that made use of temporal profiles, gut. For this run, a document's final score is given by:</p><formula xml:id="formula_5" coords="8,220.58,269.90,299.61,10.68">s(Q, D) = log P r(Q|D) + φ(T Q , T D )<label>(5)</label></formula><p>where φ(T Q , T D ) is:</p><formula xml:id="formula_6" coords="8,251.70,301.74,268.50,27.57">φ(T Q , T D ) = log( mT Q mT D )<label>(6)</label></formula><p>where mT Q is the sample mean of the time-stamps of the documents retrieved by Q, mT D is the sample mean of the time-stamps retrieved by the pseudo-query for D, and σT Q and mT Q are the corresponding sample standard deviations. The first factor in Eq. 6 promotes newer documents. Older documents are penalized, though the penalty is tempered if the query itself shows weak preference for retrieving recent documents. The second factor addresses the extent to which a temporal profile evinces temporal coherence. If the query concerns a particular time, we expect that the standard deviation of its temporal profile will be small (centered around a mode as in Figure <ref type="figure" coords="8,126.00,444.13,4.24,9.57" target="#fig_1">1</ref>). Likewise, if a document relates to a particular, time-bound event, we expect its temporal profile to have a small standard deviation.</p><p>For the sake of expediency this run abandons the formalism of the language modeling approach. Initially our hope was to capture the relationship between the temporal profiles of a query and a document by assessing the probability that the document temporal profile was generated by the same distribution that generated the query's temporal profile. However, measuring this relationship based on estimated densities proved to be very noisy on training queries that we created, and so we opted for the less elegant but simpler approach shown here.</p><p>Also, the approach shown has an implicit assumption of normality among temporal profiles. Use of the mean and standard deviation in Eq. 6 suggests that we expect temporal profiles to be centered around their means. Figure <ref type="figure" coords="8,334.56,593.17,5.45,9.57" target="#fig_1">1</ref> suggests that this is wrong. In future work we will address this shortcoming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">An Aside: Semantic Profiles</head><p>The previous section introduced methods for bringing time to bear on IR using documents retrieved by using tweets as pseudo-queries. However, we could just as easily use pseudoquery results for other purposes. This section briefly proposes one such approach in which we rely on language models estimated from the a document's retrieved set to improve our assessment of query-document relation. For a given document D with a retrieved set R = r 1 , . . . , r k , we induce a smoothed language model for R using Eq. 2, substituting word counts in the document for word counts in R at large. This allows us to estimate P r(Q|R) for a query Q. In one approach which we did not submit, we multiply Eq. 1 by this quantity to achieve a final document score. This is similar to building a relevance model for each document <ref type="bibr" coords="9,187.16,244.84,10.91,9.57" target="#b2">[3]</ref>. We offer this description to demonstrate that evidence gleaned from pseudo-queries could have non-temporal applications, calling the induced model R a document's "semantic profile."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Empirical Results</head><p>For our official runs, we retrieved only 30 documents per query since the track's official effectiveness measure was P30. Median P30 (using all judged queries) was 0.2592. Our official results all exceeded the median: temporal smoothing (gus=0.2973 and gust=0.3027 <ref type="foot" coords="9,512.44,355.81,4.23,6.99" target="#foot_2">4</ref> , and temporal profiles (gut=0.3218).</p><p>For a more complete analysis, we computed a baseline using simple query likelihood retrieval (with no future evidence). We also re-ran each experimental condition, returning 100 tweets per query that were derived by re-ranking 300 retrieved documents. Results of these runs are shown in Table <ref type="table" coords="9,238.11,425.51,4.24,9.57" target="#tab_0">1</ref>.</p><p>While the more established methods of dealing with temporal factors in IR (exponential priors and temporal smoothing) have mixed success, the approach using temporal profiles gave statistically significant improvements on five effectiveness metrics over the query likelihood baseline. Additional information-semantic profiles-improved retrieval at statistically significant levels for several measures. These results suggest that supplementing lexical information in microblog IR holds promise for improving retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Directions</head><p>An important next step in the work reported here is to assess the reason that temporal profiles improved retrieval effectiveness in our results. The less successful temporal approaches are based strictly on recency, whereas temporal profiles are a more general way of treating time. This may explain their utility. However, it may also be the case that temporal profiles improved retrieval by acting as de facto document priors. The standard deviation of temporal profiles informed our ranking, and we hypothesize that tweets whose profiles have large variance are likely to be of little interest in general. In future work we will pursue this hypothesis.</p><p>In other work, we plan to integrate information obtained from documents' pseudoqueries into retrieval in a probabilistically sound fashion. This work used pseudo-queries in an ad hoc way, importing their temporal and semantic information without proper probabilistic motivation. However, using semantic profiles for document smoothing presents an obvious way to bring their information to bear on IR. Our current work addresses these challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,91.80,497.25,428.40,9.57;7,91.80,509.85,428.39,8.74;7,91.80,521.80,411.51,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of Time-Stamps of Retrieved Documents for MB001. Kernel density plots showing the distribution of time-stamps among documents retrieved by the topic MB001, the pseudo-query for a relevant document, and the pseudo-query for a non-relevant document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,91.80,105.13,461.24,164.31"><head>Table 1 :</head><label>1</label><figDesc>Summary Statistics for Retrieval Runs (non-official). Mean average precision, R-precision, normalized discounted cumulative gain, precision at 10 and precision at 30 observed using a baseline query likelihood model and four experimental conditions. A + indicates p &lt; 0.05 on a randomization test, and ++ indicates p &lt; 0.01. Numbers in parentheses are the percent improvement (or decline, indicated with a -sign) over the query likelihood baseline.</figDesc><table coords="10,97.78,189.33,455.26,80.10"><row><cell>Condition</cell><cell>MAP</cell><cell>Rprec</cell><cell>NDCG</cell><cell>P10</cell><cell>P30</cell></row><row><cell>Baseline</cell><cell>0.185</cell><cell>0.272</cell><cell>0.350</cell><cell>0.396</cell><cell>0.307</cell></row><row><cell>Exp. Priors</cell><cell cols="2">0.183 (-0.76) 0.271 (-0.66)</cell><cell>0.351 (0.40)</cell><cell>0.388 (-2.05)</cell><cell>0.310 (1.0)</cell></row><row><cell>TSQL</cell><cell>0.189 (2.00)</cell><cell>0.276 (1.18)</cell><cell>0.365 (4.29+)</cell><cell>0.371 (-6.19)</cell><cell>0.303 (-1.30)</cell></row><row><cell>Temp. Profiles</cell><cell cols="5">0.197 (6.27+) 0.286 (5.03+) 0.366 (4.55++) 0.449 (13.41++) 0.322 (5.0+)</cell></row><row><cell>Sem. Profiles</cell><cell cols="3">0.194 (5.08) 0.284 (4.37+) 0.365 (4.40+)</cell><cell>0.398 (0.53)</cell><cell>0.318 (3.58+)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.39,643.96,108.27,7.47"><p>http://lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,108.39,643.27,242.87,7.86"><p>The term temporal profile was coined by Jones and Diaz<ref type="bibr" coords="6,338.97,643.27,9.22,7.86" target="#b1">[2]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="9,108.39,640.81,337.60,8.12"><p>The run gust suffered from a bug that made it not substantively different from gus</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,108.77,471.73,411.43,9.57;10,108.77,485.28,411.42,9.57;10,108.77,498.83,408.58,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,260.26,471.73,240.78,9.57">Estimation methods for ranking recent information</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Golovchinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,108.77,485.28,411.42,9.57;10,108.77,498.83,162.73,9.57">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information, SIGIR &apos;11</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="495" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.77,520.31,411.42,9.57;10,108.77,533.86,114.89,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,215.88,520.31,129.76,9.57">Temporal profiles of queries</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,354.41,520.31,165.79,9.57;10,108.77,533.86,35.83,9.57">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.77,555.34,411.43,9.57;10,108.77,568.89,411.43,9.57;10,108.77,582.44,411.43,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,272.05,555.34,162.97,9.57">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,465.85,555.34,54.35,9.57;10,108.77,568.89,411.43,9.57;10,108.77,582.44,171.35,9.57">SIGIR &apos;01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.77,603.92,411.43,9.57;10,108.77,617.47,411.43,9.57;10,108.77,631.02,208.21,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,229.75,603.92,136.80,9.57">Time-based language models</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.24,603.92,126.96,9.57;10,108.77,617.47,376.15,9.57">CIKM &apos;03: Proceedings of the twelfth international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="469" to="475" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
