<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.53,71.79,418.48,12.90;1,82.56,87.73,432.44,12.90">LIA-iSmart at the TREC 2011 Entity track : Entity List Completion Using Contextual Unsupervised Scores for Candidate Entities Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.72,125.98,92.92,10.75"><forename type="first">Ludovic</forename><surname>Bonnefoy</surname></persName>
							<email>ludovic.bonnefoy@etd.univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIA -iSmart</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<settlement>Avignon -Aix en provence</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.73,125.98,69.27,10.75"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@lsis.org</email>
							<affiliation key="aff1">
								<orgName type="institution">LSIS Aix-Marseille University</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.53,71.79,418.48,12.90;1,82.56,87.73,432.44,12.90">LIA-iSmart at the TREC 2011 Entity track : Entity List Completion Using Contextual Unsupervised Scores for Candidate Entities Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB38E303AF772E76AF1342764C64AFFC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Entity List Completion (ELC) task at Entity track 2011. Our approach combined the work done for the Related Entity Finding 2010 task with some new criteria as the proximity or the similarity between a candidate answer with the correct answers given as examples or their cooccurrences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of the Entity track is to evaluate entityrelated searches on Web data. The third edition of the track features two main tasks (REF and ELC) and a pilot task (REF-LOD) <ref type="bibr" coords="1,197.14,475.47,82.57,9.46" target="#b0">(Balog et al., 2011)</ref>.</p><p>The Related Entity Finding task (REF) is formulated as follows : "Given an input entity, by its name and homepage, the type of the target entity, as well as the nature of their relation, described in free text, find related entities that are of target type, standing in the required relation to the input entity". As defining entities on the Web is still an unsolved problem, it was decided to represent entities by their homepage URL, used as unique identifier. These URLs have to be extract from the English portion of ClueWeb09<ref type="foot" coords="1,196.95,636.64,3.99,6.91" target="#foot_0">1</ref> which contains approximately 500 million pages. We participated to this task last year with results just under the median <ref type="bibr" coords="1,93.51,679.33,99.42,9.46" target="#b1">(Bonnefoy et al., 2010)</ref>.</p><p>The REF-LOD task has the same definition as the REF task but the unique identifiers are URIs from the Linked Open Data (LOD) sample provided by the Sindice's team.</p><p>The Entity List Completion task (ELC) was a pilot task last year and is now one of the two main tasks. There is two differences with REF-LOD:</p><p>• A number of relevant entities (homepages and the name if available) are given in the topic definition, as examples,</p><p>• In addition to a broad type of the target entities there is a more specific type from the DBPedia Ontology.</p><p>This year we decided to participate to the Entity List Completion task in order to explore the impact of the use of previous results that we are confident in (ie. the examples) to (re)rank other candidates.</p><p>As said above, in 2010 we participated to the REF task and we decided that for 2011 ELC task we were going to reuse the core of what we implemented and add new criteria, in order to use the information given by the examples, to rank candidate answers associated with their URI.</p><p>The paper is broken down as follows : we briefly describe how we extract candidate answers and in second time we describe all the criteria used and how we combined them for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Finding candidate answers</head><p>The first thing to do is to be able to find candidate answers (named entities) from the topic in input.</p><p>To do this we reused what we did last year and presented in Fig. <ref type="figure" coords="1,380.00,647.47,4.24,9.46">1</ref>.</p><p>First, we had to build a query Q in order to retrieve relevant documents. We build it by concatenating the source entity to the words of the narrative field (more sophisticated approaches, like only using the commons and proper nouns of the narrative, seem to be less effective).</p><p>For the Web runs, the query was used to retrieve a set of related web pages by querying the Fig. <ref type="figure" coords="2,114.94,258.25,4.09,9.46">1</ref>. Candidate answers finder (CAF) web search engine Bing. In 2010 we used Boss (Yahoo!'s API) but this service is now non free. The 100 top ranked web pages were downloaded, cleaned of HTML tags and parsed in sentences.</p><p>For the obligatory run using the ClueWeb09 collection we indexed the ClueWeb collection with Indri<ref type="foot" coords="2,93.21,370.25,3.99,6.91" target="#foot_1">2</ref> and used the embedded stoplist along with the standard Krovetz stemmer. We queried it and the 100 top ranked web pages were then extracted from the Warc files, cleaned of HTML and parsed in sentences too.</p><p>For all the runs, the sentences were then indexed with Indri. Finally, we queried Indri with Q and kept the 500 top ranked passages.</p><p>Lastly, candidate named entities were extracted from this set of passages by using the Stanford-NER<ref type="foot" coords="2,93.82,505.74,3.99,6.91" target="#foot_2">3</ref> and some homemade rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ranking candidate answers</head><p>The next step deals with candidate named entities ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compacity</head><p>The first criterion we used to rank the candidates named entities is the "Compacity" score <ref type="bibr" coords="2,255.73,618.32,34.54,9.46;2,72.00,631.87,53.41,9.46" target="#b4">(Gillard et al., 2006)</ref> and that we already used in 2010. It measures the density of the query words in a passage around a given candidate entity (a correct answer to a query tends to appear in the texts near of the query words). Compacity is defined as :</p><formula xml:id="formula_0" coords="2,73.93,705.25,216.34,29.64">Compacity(E, P ) = 1 |QW | w∈QW Z w R w + 1 (1)</formula><p>with QW being the set of query words (elements extracted from the topic to get the web pages), |QW | the cardinality of this set and w one of them.</p><p>Let E be a candidate named entity, R w the distance (in number of words) between w and the candidate named entity in the passage P. Let Z w be the number of query words between w and the E (both included).</p><p>3.2 An unsupervised measure of what extent a named entity is of a given type or is close to an other entity Last year we tried to find a way to determine to what extent candidate answers to a natural language question may be associated to a given type of entity and how we can use this information to rank them. Our goal was to be able to deal with any type of entities as broad as "person" or as specific as "scotch whiskey distilleries". Our idea, inspired by the distributional hypothesis <ref type="bibr" coords="2,321.48,336.11,71.33,9.46" target="#b5">(Sahlgren, 2008)</ref>, that seems to work relatively well <ref type="bibr" coords="2,328.66,349.65,99.04,9.46" target="#b2">(Bonnefoy et al., 2011)</ref>, is that we could do it by comparing the words distribution in web pages related to an entity to the one in web pages related to a given type :</p><p>• Obtain a first set of web pages related to the type, by querying a web search engine with the type (e.g.: "science-fiction writers"). This set is called "reference set". Obtain a second set, related to the entity, by querying the web search engine with it (e.g. : "Isaac Asimov").</p><p>• Compute, for each set, its words distribution (Dirichlet smoothing) :</p><formula xml:id="formula_1" coords="2,329.17,520.25,196.38,24.32">p (w|s) = p s (w|s) if w is in the set α d p(w|C) otherwise (2)</formula><p>where p (w|s) is the probability of word w in the set S, p s (w|s) is the smoothed probability of w, p(w|C) the Laplace smoothed probability of w in a collection C (consists of 10% of the ClueWeb09 corpus) and α d is a multiplier. p s (w|s) and α d are estimated as:</p><formula xml:id="formula_2" coords="2,344.08,639.93,144.68,18.01">p s (w|s) = tf (w, s) + µ.p(w|C)</formula><p>w ∈V tf (w , s) + µ</p><p>(3)</p><formula xml:id="formula_3" coords="2,372.36,667.21,153.18,27.09">α d = µ w∈V tf (w, s) + µ (4)</formula><p>where tf (w, s) is the term frequency of w in the set s, V is the set of all words w in s and µ is a multiplier with a value set to 2000 (according to <ref type="bibr" coords="2,389.20,742.31,121.13,9.46" target="#b3">(Chen and Goodman, 1996)</ref> for newspapers and largest collection).</p><p>Fig. <ref type="figure" coords="3,99.04,251.95,4.09,9.46">2</ref>. Measure of the membership of a named entity to a given type.</p><p>• Compare the words' probability p E , in documents associated to the entity, to the reference one p type , associated to the type. For this, we compute the Kullback-Leibler divergence between them :</p><formula xml:id="formula_4" coords="3,93.82,377.19,199.72,29.57">KLD(E, type) = i p E (i).log p E (i) p type (i)<label>(5)</label></formula><p>where KLD(E, type) is the Kullback-Leibler divergence for the given entity E and the type, p E (i) (resp. p type (i)) is the probability of the i th word in documents associated to the entity E (resp. to the type).</p><p>The degree of membership of a named entity to a type (see Fig2.) is then defined as :</p><formula xml:id="formula_5" coords="3,77.20,536.73,213.07,9.81">M embership(E, type) = KLD(E, type) (6)</formula><p>And the degree of similarity of an entity to an other one (see Fig. <ref type="figure" coords="3,153.49,575.85,4.39,9.46">3</ref>) is then defined as :</p><formula xml:id="formula_6" coords="3,90.85,600.54,199.42,9.81">Similarity(E, E ) = KLD(E, E )<label>(7)</label></formula><p>Fig. <ref type="figure" coords="3,95.93,741.19,4.09,9.46">3</ref>. Measure of what extent a named entity is similar to an other one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Does the candidate answers cooccur with examples?</head><p>We thought that if a candidate named entity occurs in documents in which examples occur too, it must be favored in comparison to candidate answers that do not. We propose three ways for doing this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Documents cooccurrences</head><p>The first one depends of how many times a candidate named entity is in a document where there is examples and how many of them are presents. This is formulated as :</p><formula xml:id="formula_7" coords="3,337.66,246.37,183.64,29.64">SD(E) = i∈D ln(x i + n i + 1) n i (<label>8</label></formula><formula xml:id="formula_8" coords="3,521.30,254.10,4.24,9.46">)</formula><p>where D is the set of the 100 web pages, x i is either 0 (if E does not occur in D i ) or equal to the number of unique examples in D i , n i is the number of unique entities (including examples) in D i .</p><p>We choose to use a logarithm in order to give an important advantage to a candidate answer which cooccurs with some examples compared to one that doesn't but also in order to give only a slight advantage to a candidate answer which cooccurs with a huge number of examples over than one that occurs with a few number of them. The n i + 1 in the logarithm allows to obtain results strictly positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Cooccurrences in homepage's website lists and tables</head><p>Our two others propositions to exploit cooccurrences of candidate answers and examples are using lists and tables in source entity homepage's website. As said above there is in the topic, in addition to the name of the source entity, the url of its homepage. By looking at the homepage of named entities from the 2009 topics, we noticed that some sub-pages contain all the correct answers, most of the time in lists or tables. Moreover, the url or the title of these web pages often contains either the type of named entity we are looking for or some words of the narrative. For example we can consider the 16th topic which has for source entity "Mancuso Quilt Festivals", for homepage "http://www.quiltfest.com/" and for target entity type "sponsors". One of the web pages of this website is "http://www.quiltfest.com/sponsor.asp" and it contains all the correct answers to the topic (their name and a link to their homepage) in a table. Here's how we proceeded (see Fig. <ref type="figure" coords="4,239.30,302.36,4.54,9.46" target="#fig_0">4</ref>) : First we crawled the homepage's website and kept all the web pages which have the same root (ie. which start with "http://www.quiltfest.com/" for the previous example) and which have some of the target type words in their url or title (we then refer to this way under the name "HLT") or some of the narrative (non-stop)words (this other way is called "HLN").</p><p>We then extracted all the lists and tables from the web pages kept and we discarded all the ones without one example at least. If for a topic, we did not have one list or table at least, this metric was not used for this one. In the other case, a score is associated to each candidate named entity (same formula for HLT and HLN) :</p><formula xml:id="formula_9" coords="4,102.02,538.74,184.01,29.64">HL(E) = i∈L ln(x i + n i + 1) n i (<label>9</label></formula><formula xml:id="formula_10" coords="4,286.03,546.47,4.24,9.46">)</formula><p>where L is the set of lists and tables kept, x i is either 0 (if E does not occur in L i ) or the number of unique examples in L i +1 (to count the entity), n i is the number of unique entities (including examples) in L i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Are the context of candidates and examples close?</head><p>We really think that if a candidate answer shares a same context with one or more examples then the candidate entity is probably a relevant answer too. We already propose in 3.2 a way to measure how much they share common vocabulary and with more or less the same distributions. Here we want to explore an other approach for doing this. We retrieved for each example and each candidate for a topic the 100 top ranked snippets returned by Bing (with for query an example or a candidate answer). Then, we used a KMeans algorithm (the one includes in Mallet<ref type="foot" coords="4,448.05,145.91,3.99,6.91" target="#foot_3">4</ref> ) to assign in two clusters only all the snippets. The hope is that all the snippets corresponding to the examples are assigned to the same cluster and that all the snippets corresponding to relevant answers are assigned to this cluster too. To each candidate named entity we gave a "context score" (CS) according to :</p><formula xml:id="formula_11" coords="4,307.28,250.99,223.21,26.10">CS(C) = (c 1 * i∈E e i,1 ) + (c 2 * i∈E e i,2 ) c 1 + c 2 (<label>10</label></formula><formula xml:id="formula_12" coords="4,530.49,259.33,4.54,9.46">)</formula><p>where C is a candidate answer, c 1 (resp. c 2 ) is the number of snippets corresponding to the candidate answer in the first cluster (resp. the second cluster), E the set of examples and e i,1 (resp. e i,2 ) is the number of snippets corresponding to the i th example in the first cluster (resp. the second cluster). The denominator is for normalization if there are less than 100 snippets for a candidate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Confidence in candidate answer's URI</head><p>To select correct URIs in the Sindice's LOD dump we looked for URIs which are subjects of RDF triples and satisfy the following constraints :</p><p>1. The name of the candidate answer occurs in one of the triples;</p><p>2. The name of the candidate answer occurs in a title triple;</p><p>3. The "specific words" for this answer occur in one triple at least. "Specific words" means here words which have the higher difference between their frequency in the 100 snippets corresponding to the entity and their frequency in a "real-world corpora" (here 10% of ClueWeb09);</p><p>4. The target type occurs in one of the triples.</p><p>If an URI satisfying all the constraints and not already associated for an other named entity was found, we associated it to the candidate answer with a confidence score of 1. If we did not find such URI we released the last constraint (the 4th one) and searched again with a confidence score of 0.9. We repeated this process until an URI was found or until all constraints were released in which case the candidate answer was discarded.</p><p>The confidence score of an URI for a named entity E is formulated as :</p><formula xml:id="formula_13" coords="5,112.60,143.00,173.13,9.81">U RI(E) = 0.6 + 0.1 * nc (<label>11</label></formula><formula xml:id="formula_14" coords="5,285.72,143.35,4.54,9.46">)</formula><p>where nc is the number of constraints used to find the URI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Runs</head><p>We have some criteria to estimate the relevance of a named entity and we can propose different ways to use combine them to rank candidate answers and URIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">LIAcwb and LIAwb</head><p>The LIAcwb run is the only one which used the ClueWeb09 to find relevant web pages at the beginning of the process (all the others were using Bing). Only these two runs used a small subset of the criteria presented and are our baselines. A score is associated to each candidate answer E :</p><formula xml:id="formula_15" coords="5,87.60,386.04,198.12,37.54">Score(E) = P i ∈P Compacity(E, P i ) * SD(E) * U RI(E) (<label>12</label></formula><formula xml:id="formula_16" coords="5,285.72,414.13,4.54,9.46">)</formula><p>where P is the set of 500 passages retrieved with Indri. We choose to combine all the scores by the mean of multiplications because it probably was the easiest and better way for doing this (all the scores have strictly positive values). If we made a linear combination of them, we should have to find a way to normalize all these scores and computed a weight for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">LIAwc</head><p>For this run we used all the criteria except the ones presented in 3.2 (how the distribution between snippets of the candidate answer and the ones of the target type or the examples are close). That gives for each candidate named entity :</p><formula xml:id="formula_17" coords="5,73.85,645.65,216.42,54.08">Score(E) = P i ∈P Compacity(E, P i ) * SD(E) * HLT (E) * HLN (E) * CS(E) * U RI(E)<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">LIAwd</head><p>For this run we do not used the cluster approach but instead used what we excluded from the previous run (the measure of membership of the answer to the DBPedia target type and the similarity with examples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score(E) =</head><formula xml:id="formula_18" coords="5,365.40,105.84,158.29,101.63">P i ∈P Compacity(E, P i ) * Ex i ∈EX Similarity(E, Ex i ) * M embership(E, type) * SD(E) * HLT (E) * HLN (E) * U RI(E) (<label>14</label></formula><formula xml:id="formula_19" coords="5,521.00,198.01,4.54,9.46">)</formula><p>where EX is the set of examples and type is the DBPedia target type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>As the official results are not released yet, it's a difficult task to analyze the performances of our methods. Anyway, we analyze a bunch of topics by ourselves in order to pointed out the main characteristics and difficulties that our methods could have. Of course the following results and analysis haves to be considered cautiously, because the observed phenomena may not be representatives.</p><p>The Table <ref type="table" coords="5,364.16,389.82,5.45,9.46">2</ref> shows some precision measures for two topics 5 (22 and 51) for each run. With this only two topics this is difficult to know if using the Web as resource to find candidate answers (instead of the ClueWeb09 collection) is interesting or not. For the topic 22 using the Web seems to bring noise but for the topic 51 it appears that the coverage of the ClueWeb09 is probably not important enough and using the Web is useful. An alternative of our approach (only one resource for a given run) could be to use both or to try to determine for each topic which one have to be used (i.e. does the ClueWeb09 cover this topic?). Moreover, information on the Web change with time and for some topics the good answers are not the same now that there was at the time of the ClueWeb09 was crawled. For the topic 24 for instance, we looked for members of the "Jazz at Lincoln Center Orchestra" and the examples given in the topic are not correct anymore (and don't appear on the official Web pages) that make them (almost) useless. So, for topics for which answers could change fast it is best to use the ClueWeb09.  is a crucial point. With topic 22 we can see that the Stanford-NER tool is not adapted. Indeed, we looked for named entities of type "Person" (in italic) but on the top ten the precision of the NER tool is only 50%. Some tools like DBPedia Spotlight 6 seems to be more adapted here. Top ten results for topic 62 show that to merge all the different spelling of one entity is really important.</p><p>The higher results for LIAwd on topic 51 than to the ones on topic 22 seem to be due to our cooccurrences measures (the correct answers are contain in tables in the homepage of the source entity for topic 51 7 ) and by our similarity measures (to the target type and to the examples). This last observation is comforted by Table <ref type="table" coords="6,203.99,683.11,5.45,9.46">1</ref> which shows that the top ten answers are all related to NIH and non topic related answers (eg. Maryland, Rockville, etc.) didn't appear here. However, this method 6 http://spotlight.dbpedia.org/demo/index.html 7 http://www.nih.gov/icd/ make examples ranked higher (due to our similarity function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented our work for the ELC task of the 2011 Entity track. We tried to evaluate the impact of using results that we are confident in to rank the other results. We started with what we did in 2010 for the REF task and added new criteria in order to use the information given by the examples in the topics to rank the candidate answers.</p><p>Our unofficial evaluations show that our approach seems to improve the obtained ranking but some difficulties remain to be overcome.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,76.11,256.90,210.05,9.46;4,125.98,270.45,110.30,9.46"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Measuring cooccurrences in homepage's website's lists and tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,307.28,701.52,218.27,63.38"><head></head><label></label><figDesc>Table 1 and 3 show the top ten results for three topics (but not necessary for all the runs). They show that the first selection of candidate answer</figDesc><table /><note coords="5,319.93,745.30,2.99,5.18;5,323.42,747.17,27.17,7.77;5,370.92,747.17,21.42,7.77;5,412.67,747.17,10.95,7.77;5,443.95,747.17,31.97,7.77;5,496.24,747.17,6.47,7.77;5,523.05,747.17,2.49,7.77;5,307.28,757.13,189.89,7.77"><p>5 Official topics are available at : http://trec.nist.gov/data/entity/11/11.entity elc topics</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,445.05,218.27,23.01"><head>Table 3 :</head><label>3</label><figDesc>Top ten results for topic 24 (run LIAcwb) and topic 62 (run LIAwd).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.14,757.13,160.67,7.77"><p>http://boston.lti.cs.cmu.edu/Data/clueweb09/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.14,746.25,124.28,7.77"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,88.14,757.13,138.10,7.77"><p>http://nlp.stanford.edu/ner/index.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,323.42,757.13,220.17,7.77"><p>http://mallet.cs.umass.edu/api/cc/mallet/cluster/KMeans.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic 22 Topic 51 Run P@5 P@10 P@R P@1 P@5 P@10 LIAcwb </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,307.28,449.90,218.27,8.64;6,318.19,460.69,207.36,8.81;6,318.19,471.65,122.09,8.58" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,318.19,460.86,149.03,8.64">Overview of the trec 2011 entity track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<idno>NIST Spe- cial Publication : TREC 2011</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,490.06,218.27,8.64;6,318.19,501.02,207.36,8.64;6,318.19,511.81,207.36,8.81;6,318.19,522.77,83.07,8.58" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,508.95,490.06,16.60,8.64;6,318.19,501.02,207.36,8.64;6,318.19,511.98,108.31,8.64">Liaismart at trec 2010: An unsupervised web-based approach for filtering answers</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Benoit</surname></persName>
		</author>
		<idno>cation : TREC 2010</idno>
	</analytic>
	<monogr>
		<title level="s" coord="6,445.64,511.81,75.76,8.58">NIST Special Publi</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,541.18,218.27,8.64;6,318.19,552.14,207.36,8.64;6,318.19,562.93,207.36,8.81;6,318.19,573.89,207.36,8.58;6,318.19,584.85,92.96,8.58" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,490.90,541.18,34.65,8.64;6,318.19,552.14,207.36,8.64;6,318.19,563.10,140.55,8.64">The web as a source of evidence for filtering candidate answers to natural language questions</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Benoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,392.98,573.89,132.56,8.58;6,318.19,584.85,64.30,8.58">ACM International Conference on Web intelligence</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>In 10th edition</note>
</biblStruct>

<biblStruct coords="6,307.28,603.26,218.27,8.64;6,318.19,614.22,207.36,8.64;6,318.19,625.01,207.36,8.58;6,318.19,635.97,35.70,8.58" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,449.01,603.26,76.53,8.64;6,318.19,614.22,190.10,8.64">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,318.19,625.01,126.14,8.58">34th Annual Meeting of the ACL</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,654.38,218.27,8.64;6,318.19,665.34,207.36,8.64;6,318.19,676.13,207.36,8.81;6,318.19,687.09,207.36,8.58;6,318.19,698.05,207.36,8.58;6,318.19,709.01,105.56,8.58" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,386.83,665.34,138.72,8.64;6,318.19,676.30,147.95,8.64">Relevance measures for question answering, the lia at qa@clef-2006</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sitbon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Blaudez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Beze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,495.48,676.13,30.07,8.58;6,318.19,687.09,109.77,8.58;6,481.82,687.09,43.73,8.58;6,318.19,698.05,207.36,8.58;6,318.19,709.01,25.86,8.58">Evaluation of Multilingual and Multi-modal Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="440" to="449" />
			<date type="published" when="2006">2006. 2007. 2007</date>
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct coords="6,307.28,727.42,218.27,8.64;6,318.19,738.21,207.36,8.58;6,318.19,749.17,59.22,8.58" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,392.47,727.42,115.42,8.64">The distributional hypothesis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,517.25,727.42,8.30,8.64;6,318.19,738.21,203.51,8.58">In Special issue of the Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
