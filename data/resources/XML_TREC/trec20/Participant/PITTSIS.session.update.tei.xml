<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.96,57.88,286.15,16.65">PITT at TREC 2011 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.14,80.18,57.66,11.10"><forename type="first">Jiepu</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>135 North Bellefield Avenue Pittsburgh</addrLine>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.20,80.18,75.87,11.10"><forename type="first">Shuguang</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>135 North Bellefield Avenue Pittsburgh</addrLine>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.46,80.18,32.34,11.10"><forename type="first">Jia</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>135 North Bellefield Avenue Pittsburgh</addrLine>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.00,80.18,55.87,11.10"><forename type="first">Daqing</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>135 North Bellefield Avenue Pittsburgh</addrLine>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.96,57.88,286.15,16.65">PITT at TREC 2011 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD7768D9EA71F223EDE720FB18A99B7B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC</term>
					<term>session</term>
					<term>query language model</term>
					<term>relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce our approaches for TREC 2011 session track. Our approaches focus on combining different query language models to model information needs in a search session. In RL1 stage, we build ad hoc retrieval system using sequential dependence model (SDM) on current query. In RL2 stage, we build query language models by combining SDM features (e.g. single term, ordered phrase, and unordered phrase) in both current query and previous queries in the session, which can significantly improve search performance. In RL3 and RL4, we combine query model in RL2 with two different pseudo-relevance feedback query models: in RL3, we use top ranked Wikipedia documents from RL2's results as pseudo-relevant documents; in RL4, snippets of the documents clicked by users in a search session are used. Our evaluation results indicate: texts of previous queries in a session are effective resources for estimating query models and improving search performance; mixing query model in RL2 with the query model estimated using click-through data (in RL4) can improve performance in evaluation setting that considers all subtopics, but no improvement is observed in evaluation setting that considers the only subtopic of current query; our methods of mixing query model in RL2 with query model in RL3 did not improve search performance over RL2 in any of the two evaluation settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In a recently study, Trotman and Keeler <ref type="bibr" coords="1,218.15,468.51,10.49,8.10" target="#b1">[1]</ref> find that search engines can provide ranked results comparable to those of human assessors (who are not those produced official judgments) on several TREC and INEX collections. They claim that there may be "not much room for improvement" for ad hoc information retrieval. Besides, it is also indicated from their results (in Fig. <ref type="figure" coords="1,289.60,520.29,4.50,8.10">2</ref> of <ref type="bibr" coords="1,65.87,530.61,10.08,8.10" target="#b1">[1]</ref>) that the judgments from human assessors are far from perfect (with the medium of average precision ranging from about 0.2 to 0.6 on different collections). This demonstrates that context information associated with a search query is important, without it, it is even difficult for human beings to correctly understand the underlying information needs of a query (from another person).</p><p>From this point of view, TREC session track provides a platform with open collections and evaluation environments for studying retrieval techniques with context information -different layers of historical user behaviors in a search session. In 2011 session track tasks, four layers of session information are provided: RL1: only current query (ad hoc retrieval setting) RL2: current query and past queries in the same session RL3: RL2 and top returned documents for each of the past queries RL4: RL3 and user's click-through for each of the past queries For each search topic, a few subtopics are identified and assigned to queries of the topic (which cannot be used for search). Because there may be transition of subtopics among queries in a search session, two different evaluation approaches are adopted in 2011 session track: one evaluates by documents relevant to any of the subtopics or the general topic ("all-subtopics" evaluation); the other evaluates by only documents relevant to the subtopic of current query ("current-only" evaluation). For details of session track task settings and evaluation methods, please refer to [2] and session track overview of the year.</p><p>Our retrieval approaches focus on using different mixture of query language models to incorporate into our retrieval process various session context information. In RL1 stage, we build ad hoc search system using sequential dependence model on current query. We further combine current query with two kinds of session context information and create correspondent query models for retrieval. One kind of session context information is the texts of users' past queries: we combine sequential dependence model query features (including single term, ordered phrase, and unordered phrase) in current query with those in past queries in RL2 stage. The other kind of session context is pseudo-relevance feedback information in the session, for which two different sets of documents are used as pseudo-relevance feedback documents set: in RL3, we use top ranked Wikipedia documents from RL2's results; in RL4, we use snippets of the documents clicked by users. The pseudo-relevance feedback models are combined with RL2's query model in search.</p><p>The rest of the article is organized as follows: section 2 introduces the methods we used for session track and some necessary details of implementation; in section 3, we analyze evaluation results; we finally discuss future works and draw a conclusion in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System and Collection</head><p>We use Indri 5.0 to build our retrieval system because lots of our methods can be quickly implemented using Indri query language. We use only Clueweb09 category B collection [3] in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ad hoc Retrieval (RL1)</head><p>In RL1 stage, we use sequential dependence model (SDM) <ref type="bibr" coords="1,534.56,584.01,10.49,8.10" target="#b2">[4]</ref> for ad hoc retrieval. We extract SDM features (including single term, ordered phrase, and unordered phrase) from current query and use Indri query language to incorporate SDM features into retrieval.</p><p>Besides, we also noticed from previous studies in web track that Clueweb09 collection involves lots of spam documents. Thus, we use Waterloo spam ranking score <ref type="bibr" coords="1,442.08,652.11,10.49,8.10">[5]</ref> to filter spam documents. In all our submitted runs, we first return top 5,000 documents and then filter out those with "fusion" spam score [6] less than 70%. This way of implementing filtering, however, does not guarantee to return 2,000 documents (the maximum number for evaluation) for each topic, which may lead to lower MAP evaluation results than it should be in all our runs (but nDCG@10 will be accurate).</p><p>We tune SDM feature weights and spam score threshold on 2010 session track RL2 data (in 2010, RL2 uses only current query) to maximize nDCG@10. Table <ref type="table" coords="2,162.84,103.42,4.50,8.10" target="#tab_0">1</ref> shows results of query likelihood and SDM on 2010 session track data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Session Historical Query Model (RL2)</head><p>In RL2 stage, except for current query, previous search queries in the same session can be used for ranking (which corresponds to the setting of RL3 in 2010 session track). We noticed a simple but effective method in 2010 session track for this setting: in <ref type="bibr" coords="2,267.71,174.94,10.49,8.10" target="#b4">[7]</ref> and <ref type="bibr" coords="2,54.00,185.31,9.52,8.10" target="#b5">[8]</ref>, current query and a previous query were combined as a new query for search, which improved nDCG@10 in both studies. We also employ the idea in our RL2 run and expand it to incorporate SDM features.</p><p>In language modeling approaches for information retrieval, query model was firstly introduced in risk minimization framework <ref type="bibr" coords="2,283.61,243.04,10.49,8.10" target="#b6">[9]</ref> to model user's information needs, which is defined as generative model for queries. In an ad hoc retrieval setting, we can only have current query, a very limited sample from the query model. Thus, alternative samples were usually adopted for estimation of query models, e.g. pseudo-relevant documents <ref type="bibr" coords="2,203.40,294.82,14.25,8.10" target="#b7">[10,</ref><ref type="bibr" coords="2,220.62,294.82,10.64,8.10" target="#b8">11]</ref>. Now in session track settings, we can have previous search queries as alternative samples for query model estimation (although the sample size is still small). However, it is common that user's information needs may evolve during a search session. As a result, we may need to discount past queries in query model estimation.</p><p>Our method for RL2 stage (will be referred to as session historical query model (SH-QM) in following texts) estimates query model based on texts of current query and historical queries in the search session. Let F be a specific type of query feature, in our method, single term (T), ordered phrase (O), or unordered phrase (U). For any type of feature F, the probability of generating f, a specific feature value, is estimated as (1): q m stands for current query and q 1 to q m-1 stand for previous queries; 𝑝 𝑀𝐿𝐸 (𝑓|𝑞 𝑚 ) is the maximum likelihood estimation of f from q m , which is calculated as (2); 𝑝 𝑀𝐿𝐸 (𝑓|𝑞 1 , … , 𝑞 𝑚-1 ) is the maximum likelihood estimation of f from past queries, as in (3); λ is the weight for previous queries. In (2) and (3), count(f, q) is the raw frequency of feature value f observed in query q. Finally, three types of features are combined (using the tuned feature weights from 2010 data) for search.</p><formula xml:id="formula_0" coords="2,69.66,514.39,224.48,9.18">𝑝̂𝑆 𝐻 (𝑓|𝑞) = (1 -𝜆) ⋅ 𝑝 𝑀𝐿𝐸 (𝑓|𝑞 𝑚 ) + 𝜆 ⋅ 𝑝 𝑀𝐿𝐸 (𝑓|𝑞 1 , … , 𝑞 𝑚-1 )<label>(1)</label></formula><formula xml:id="formula_1" coords="2,91.80,530.66,202.34,50.33">𝑝 𝑀𝐿𝐸 (𝑓|𝑞 𝑚 ) = 𝑐𝑜𝑢𝑛𝑡(𝑓, 𝑞 𝑚 ) ∑ 𝑐𝑜𝑢𝑛𝑡(𝑓 ′ , 𝑞 𝑚 ) 𝑓 ′ (2) 𝑝 𝑀𝐿𝐸 (𝑓|𝑞 1 , … , 𝑞 𝑚-1 ) = ∑ 𝑐𝑜𝑢𝑛𝑡(𝑓, 𝑞 𝑖 ) 𝑚-1 𝑖=1 ∑ ∑ 𝑐𝑜𝑢𝑛𝑡(𝑓 ′ , 𝑞 𝑖 ) 𝑚-1 𝑖=1 𝑓 ′ (3)</formula><p>When considering only feature T (single term) and setting λ to 0.5, (1) is similar to the approaches adopted by <ref type="bibr" coords="2,212.76,598.66,10.49,8.10" target="#b4">[7]</ref> and <ref type="bibr" coords="2,241.50,598.66,9.52,8.10" target="#b5">[8]</ref>. We tune λ using RL3 data of 2010 session track (in 2010, RL3 uses both q m and q m-1 for retrieval) to maximize nDCG@10. The tuned weight for λ is 0.3, which also indicates we should discount past queries. Table <ref type="table" coords="2,76.80,640.05,4.50,8.10" target="#tab_0">1</ref> shows nDCG@10 results of SH-QM in 2010 session track data and comparison with results using only current query. It is indicated from the results that SH-QM can largely improve the performance compared with using only current query for search, no matter only single term feature or all SDM features are used. Besides, table 1 also indicates feature O and U can further slightly improve the performance over methods using only T in SH-QM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relevance Feedback (RL3 and RL4)</head><p>Although SH-QM has a natural advantage of using real (although slightly outdated) user queries for estimation, the estimation may be rough because of the limited size of query texts. Thus, in RL3 and RL4 stage, we combine pseudo-relevance feedback query model (PRF-QM) with SH-QM with the expectation of improving SH-QM. We only combine single term feature from PRF-QM and SH-QM, and keep feature O and U unchanged, as shown in <ref type="bibr" coords="2,534.11,198.93,9.52,8.10" target="#b2">(4)</ref>.</p><p>In RL3, we use top 10 ranked Wikipedia documents from our RL2 results as pseudo-relevant documents and estimate query models using relevance model 1 (RM1) method <ref type="bibr" coords="2,467.63,235.95,13.75,8.10" target="#b7">[10]</ref>, which is calculated as ( <ref type="formula" coords="2,331.80,246.27,3.25,8.10">5</ref> In RL3, we use only top ranked Wikipedia documents for PRF with the expectation of improving reliability of PRF documents, which is proved to be important for the performance of PRF <ref type="bibr" coords="2,540.78,308.37,13.75,8.10" target="#b9">[12]</ref>. Also, there were studies in 2010 web track successfully improved results using Wikipedia articles for PRF <ref type="bibr" coords="2,464.34,329.07,13.75,8.10" target="#b10">[13]</ref>.</p><p>In RL4, we use the snippets (provided officially in session track topics) of the documents clicked by users as PRF documents (each clicked snippet is given equal weight in estimation), which can be calculated as (6): C is the set of clicked snippets and d refers to each of the snippets in C; p MLE (t|d) is the probability of t from the unsmoothed snippet model. </p><p>In ( <ref type="formula" coords="2,331.15,503.19,3.19,8.10">4</ref>), the mixture weight µ is not tuned and set to 0.3 intuitively. Also, we intuitively use top 20 query terms from T feature in final ranking, which is also not tuned. As a result, results reported for our RL3 and RL4 runs may not indicate optimized performance.</p><p>Although both methods for RL3 and RL4 try to combine PRF-QM with SH-QM, the different sets of documents used for relevance feedback may lead to certain preference of methods. Compared with RL3, the documents used for RL4 (those clicked by users) are intuitively more reliable because click-through data may more reliably indicate users' positive feedback. However, because only clicked documents for past queries were available for feedback, they may be misleading when current query is very different from previous queries at sub-topic level. On the other hand, although documents for RL3 may not involve users' feedback, considering we tuned RL2 methods to optimize performance on current query, the PRF documents used for RL3 may better model user's current information needs.</p><p>Because we are not aware of the performance of the systems used for generating user interaction data in 2011 topics, we did not use the snippets provided in official RL3 topic file, but directly used documents from our RL2 results for pseudo-relevance feedback. Thus the setting for our RL3 run is the same as that for RL2 run. This may also lead to difficulties for us to make fully comparison between RL3 and RL4 runs, because recent studies indicates PRF query models estimated from whole documents can be improved by considering positional information in PRF documents <ref type="bibr" coords="3,260.76,118.11,13.75,8.10" target="#b11">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION 3.1 SH-QM</head><p>We have partly evaluated results of SH-QM on 2010 session track data in 2.3. We find that SH-QM largely improved performance compared with methods using only current query. In 2011 session track, we find similar results (Table <ref type="table" coords="3,184.85,193.11,3.23,8.10" target="#tab_2">2</ref>). In both "all-subtopics" and "current-only" evaluation, SH-QM improved nDCG@10 by over 10% (improvements are significant at 0.05 levels in paired t-test). p( nDCG@10.RL1 for "↑" &gt; nDCG@10.RL1 for "-" ): 0.162 p( nDCG@10.RL1 for "↓" &gt; nDCG@10.RL1 for "↑" ): 0.072 p( nDCG@10.RL1 for "↓" &gt; nDCG@10.RL1 for "-" ): 0.023 tested by Welch's t-test Pearson correlation between nDCG@10.RL2 -nDCG@10.RL1 for two different evaluation methods on 76 different topics: 0.774.</p><p>In order to further investigate on the difference of performance for SH-QM on two evaluation settings, we compare results from two evaluation settings by per topic difference of nDCG@10 between RL1 and RL2. Figure <ref type="figure" coords="3,136.01,547.77,4.50,8.10" target="#fig_1">1</ref> shows per topic difference of nDCG@10 between RL1 and RL2 in two evaluation settings. We find similar trends: in both settings, no difference of nDCG@10 can be found for about half of the topics, and the number of topics improved by SH-QM is more than the number of topics hurt. In Figure <ref type="figure" coords="3,272.73,589.11,3.38,8.10">2</ref>, per topic difference of nDCG@10 between RL1 to RL2 is charted for both evaluation settings and compared. Still, similar trends can be found on most of the topics.</p><p>We further compare average nDCG@10 of topics at RL1 stage for topics that are improved, unchanged, and hurt in RL2, which may help us understand in which cases RL2 can help or hurt retrieval. In both evaluation settings, the group of topics hurt by RL2 has significant higher average nDCG@10 than other two groups.</p><p>In both evaluation settings, SH-QM seems more likely to hurt performance if current query is already very effective (with higher nDCG@10), and to improve (or do not hurt) performance when current search query is comparatively less effective (with lower nDCG@10). However, it does not indicate SH-QM will improve the most difficult queries, or hurt the most effective ones.</p><p>We identified 17 topics in "all-subtopics" evaluation and 26 topics in "current-only" evaluation with lower than 0.05 nDCG@10 as difficult queries. For the 17 difficult in "all-subtopics" evaluation, 13 topics with nDCG@10 equal to 0 have not been improved in RL2 stage, and only 2 topics are effectively improved, for which the improvements in nDCG@10 are greater than 0.1; for the 26 difficult in "current-only" evaluation, 21 topics with nDCG@10 equal to 0 have not been improved in RL2, and only 3 topics are effectively improved. Thus, it seems SH-QM is not likely to be able to improve the most difficult queries. Topics being improved are mostly those with nDCG@10 from 0.2 to 0.5.</p><p>Two typical queries that are identified difficult but improved in RL2 stage are topic No. 12 and No. 73. For both topics, user's previous queries are effective, but current query has some errors: there is a typo in current query for topic No. 12; for topic 73, user issued a over-specified query, while effective query exists among previous session histories. For both topics, instead of saying RL2 improved search, it may be more appropriate to say RL2 saved users' extremely ineffective queries. However, in such cases, RL2 does not necessarily perform better than previous queries, and it is thus arguable whether it really improved users' search experience.</p><p>We also identified certain "easy" topics and found SH-QM will not hurt the topics. We identified 20 topics and 13 topics with nDCG@10 larger than 0.5 in two evaluation settings. More than half of the topics (in both settings) are not hurt by RL2. No topic is greatly hurt (with nDCG@10 decrease by more than 0.2). Thus, it seems clear that SH-QM will also not hurt those most effective queries.</p><p>Finally, we select several typical topics in 2011 for discussion.</p><p>The three topics improved most by SH-QM are: No. 13, No. 59, and No. 67. Topics No. 13 and No. 67 are both cases similar to the case of "saving user from ineffective queries": in No. 13, user used an under-generalized word "job" in current query, but "employ" used in past queries are effective; in topic No. 67, user tried to over-specify results by connecting "joseph steffen" with those from Wikipedia, but such page does not exist and previous queries are effective by just using "joseph steffen". Topic 59 may indicate a typical case of ineffective search behaviors that can be improved by SH-QM: both current query and previous queries are over-generalized, while the overlapped documents are relevant. Figure <ref type="figure" coords="4,88.70,363.09,3.38,8.10">2</ref>. Comparison of nDCG@10.RL2 -nDCG@10.RL1 for two evaluation methods on different topics; only 42 topics which have difference between nDCG@10.RL2 and nDCG@10.RL1 are charted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RL3 and RL4</head><p>Compared with the significant improvements achieved by RL2 over RL1, we do not get much improvement by combining PRF with SH-QM. For both RL3 and RL4, evaluation results on topic No. 1 -No. 46 are reported in table <ref type="table" coords="4,184.28,432.64,4.50,8.10" target="#tab_3">3</ref> 1 .</p><p>Still, we can observe significant improvements on these 46 topics between RL1 and RL2 in both evaluation settings. Compared with RL2, only RL4 is significantly better in "all-subtopics" evaluation setting (significant at 0.1 level using one tail paired t-test). But in "current-only" evaluation setting, we did not find any significant difference between RL2, RL3, and RL4. It also seems from Table <ref type="table" coords="4,54.00,511.11,4.50,8.10" target="#tab_3">3</ref> that click-through data (RL4) can only improve RL2 in "allsubtopics" evaluation. Comparing RL3 and RL4, in either setting, no significant difference can be claimed.</p><p>We further calculate per topic difference of nDCG@10 between RL3/RL4 and RL2. We find Pearson correlation for differences of nDCG@10 from RL2 to RL3 and that from RL2 to RL4 is -0.178 and -0.046 in two evaluation settings, which can indicate RL3 and RL4 (and possibly the different resources used for PRF) will have different but not necessarily opposite behaviors in two evaluation settings.</p><p>1 made an error in generating our RL4 results submission by using our RL3 queries for topic No. 47 -No. 76 (30 topics). Thus, most results discussed in section 3.2 refer to those we only evaluated and compared for topic No. 1 -No. 46 (46 topics in total). As mentioned in section 2.4, however, because related parameters are not tuned for RL3 and RL4 in our runs, results reported in this section may not indicate the optimized results for each method. Also, we do not over emphasize any conclusion in this section. However, some of the observations are likely to be generalized: RL4 and click-through data may only help RL2 in "all-subtopics" evaluation. We did not observe any improvements of using RL3. However, considering PRF on top ranked documents is usually difficult to tune, we are not going to claim PRF is not useful for RL2. Besides, according to <ref type="bibr" coords="4,422.06,682.77,13.74,8.10" target="#b11">[14]</ref>, snippets used in RL4 may also contribute to the better performance of RL4 than RL3 in our runs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we introduce our methods performed in TREC 2011 session track. Our major contribution is combining different query language models: one kind of query model (SH-QM) is estimated from session historical queries; the other kind is estimated using pseudo-relevant documents. We noticed significant improvements of using SH-QM compared with ad hoc retrieval. We did not come to any solid conclusion for the benefits of combining SH-QM and PRF-QM because of some drawbacks in experiments. However, our results are most likely to support that click-through data can only significantly improve SH-QM in "all-subtopics" evaluation, but not in "current-only" evaluation. We need further experiment results to clearly find out the usefulness of two PRF resources and their benefits over SH-QM. Besides, our methods of combining different query models are extremely simple, which will be one of our major foci in future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,360.48,414.14,144.31,9.17;2,343.44,429.56,178.18,9.17;2,548.70,421.81,9.32,7.18;2,365.52,449.78,134.17,9.17;2,421.80,461.92,12.55,5.61;2,548.70,452.29,9.32,7.18;2,381.72,478.52,101.83,9.17;2,438.00,490.66,10.49,5.61"><head>𝑝(𝑡|𝑞) = ( 1 - 4 ) 5 )</head><label>145</label><figDesc>𝜇) ⋅ 𝑝 𝑆𝐻 (𝑡|𝑞) + 𝜇 ⋅ 𝑝 𝑃𝑅𝐹 (𝑡|𝑞) 𝑝 𝑅𝐿3 (𝑓|𝑞) = 𝑝 𝑅𝐿4 (𝑓|𝑞) = 𝑝 𝑆𝐻 (𝑓|𝑞), 𝑤ℎ𝑒𝑛 𝐹 𝑖𝑠 𝑂 𝑜𝑟 𝑈 (𝑝 𝑃𝑅𝐹-𝑅𝐿3 (𝑡|𝑞) ∝ � 𝑝 𝑀𝐿𝐸 (𝑡|𝑑)𝑝 𝑆𝐻 (𝑞|𝑑) 𝑑∈𝑊 (𝑝 𝑃𝑅𝐹-𝑅𝐿4 (𝑡|𝑞) ∝ � 𝑝 𝑀𝐿𝐸 (𝑡|𝑑) 𝑑∈𝐶</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,80.40,200.43,451.23,8.10;4,98.17,210.76,415.66,8.10;4,83.82,248.58,457.14,87.60"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Difference of nDCG@10 between RL1 and RL2 on different topics (sorted by nDCG@10.RL2 -nDCG@10.RL1). Left figure indicates evaluation on all subtopics; right figure indicates evaluation on only subtopic of current query.</figDesc><graphic coords="4,83.82,248.58,457.14,87.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,58.94,181.59,11.00,6.59;4,58.94,163.26,11.00,6.59;4,61.09,144.93,8.89,6.59;4,61.09,126.60,8.89,6.59;4,61.09,108.27,8.89,6.59;4,61.09,89.95,8.89,6.59;4,61.09,71.62,8.89,6.59;4,76.34,153.09,216.62,5.63;4,60.23,60.12,29.83,6.58;4,220.50,60.36,65.90,6.58;4,303.50,181.63,11.00,6.59;4,303.50,163.25,11.00,6.59;4,305.65,144.86,8.89,6.59;4,305.65,126.48,8.89,6.59;4,305.65,108.09,8.89,6.59;4,305.65,89.71,8.89,6.59;4,305.65,71.32,8.89,6.59;4,320.96,153.02,226.25,5.63;4,304.97,60.77,29.83,6.58;4,446.33,62.94,81.42,6.58;4,67.58,347.84,11.00,6.59;4,67.58,330.39,11.00,6.59;4,69.73,312.94,8.89,6.59;4,69.73,295.49,8.89,6.59;4,69.73,278.03,8.89,6.59;4,69.73,260.58,8.89,6.59;4,69.73,243.13,8.89,6.59;4,89.01,321.09,451.07,5.63;4,61.10,232.49,29.83,6.58;4,435.42,232.92,75.30,7.49"><head></head><label></label><figDesc>11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,321.95,56.20,231.95,52.88"><head>Table 1 . nDCG@10 of SH-QM using 2010 session track data Query Feature q m-1 (RL1) q m (RL2) SH-QM: 0.3 q m-1 + 0.7 q m</head><label>1</label><figDesc></figDesc><table coords="2,498.90,79.81,20.42,7.18"><row><cell>(RL3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,317.87,246.27,240.20,39.18"><head></head><label></label><figDesc>): W is the Wikipedia PRF document set and d can be each document in W; p MLE (t|d) is the probability of t from unsmoothed document model for d; p SH (q|d) is the probability of the weighted SDM features in SH-QM from smoothed document model for d.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,60.94,230.31,229.08,199.10"><head>Table 2 . nDCG@10 for RL1 and RL2 in 2011 session track and the number of topics that nDCG@10 has changed</head><label>2</label><figDesc></figDesc><table coords="3,61.02,256.09,229.00,173.32"><row><cell>Evaluation</cell><cell>RL1</cell><cell></cell><cell>RL2</cell></row><row><cell>Setting</cell><cell>(SDM+QL)</cell><cell cols="3">(SDM+SH-QM)</cell></row><row><cell>"all-subtopics"</cell><cell>0.3789</cell><cell cols="3">0.4281 (+12.98%, p = 0.002)</cell></row><row><cell cols="2">Trends of change from RL1 to RL2</cell><cell>↑</cell><cell>-</cell><cell>↓</cell></row><row><cell cols="2">Number of topics</cell><cell>27/76</cell><cell>36/76</cell><cell>13/76</cell></row><row><cell cols="2">Average nDCG@10.RL1</cell><cell>0.3386</cell><cell>0.3560</cell><cell>0.5259</cell></row><row><cell>Evaluation</cell><cell>RL1</cell><cell></cell><cell>RL2</cell></row><row><cell>Setting</cell><cell>(SDM+QL)</cell><cell cols="3">(SDM+SH-QM)</cell></row><row><cell>"current-only"</cell><cell>0.2679</cell><cell cols="3">0.2954 (+10.27%, p = 0.022)</cell></row><row><cell cols="2">Trends of change from RL1 to RL2</cell><cell>↑</cell><cell>-</cell><cell>↓</cell></row><row><cell cols="2">Number of topics</cell><cell>21/76</cell><cell>39/76</cell><cell>16/76</cell></row><row><cell cols="2">Average nDCG@10.RL1</cell><cell>0.2804</cell><cell>0.2136</cell><cell>0.3838</cell></row></table><note coords="3,76.80,326.17,197.38,7.18;3,77.10,335.35,196.78,7.18;3,76.81,344.52,197.37,7.18;3,137.82,353.76,75.30,7.18"><p>p( nDCG@10.RL1 for "↓" &gt; nDCG@10.RL1 for "-" ): 0.073 p( nDCG@10.RL1 for "↓" &gt; nDCG@10.RL1 for "↑" ): 0.043 p( nDCG@10.RL1 for "-" &gt; nDCG@10.RL1 for "↑" ): 0.413 tested by Welch's t-test</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,323.52,387.99,233.83,187.09"><head>Table 3 . nDCG@10 of RL1 to RL4 on 46 topics of 2011 session track data (topic No. 1 to No. 46) Methods "all-subtopics" Evaluation "current-only" Evaluation</head><label>3</label><figDesc></figDesc><table coords="4,323.52,437.11,233.83,137.98"><row><cell>RL1</cell><cell cols="2">0.3344</cell><cell cols="2">0.2080</cell></row><row><cell>RL2 SH-QM</cell><cell cols="2">0.3811 RL2:RL1 + 13.97% p = 0.019</cell><cell cols="2">0.2343 + 12.64% RL2:RL1 p = 0.044</cell></row><row><cell>RL3</cell><cell cols="2">0.3782</cell><cell cols="2">0.2371</cell></row><row><cell>SH-QM + PRF using top wiki doc</cell><cell>RL3:RL2</cell><cell>-0.76% p = 0.362</cell><cell>RL3:RL2</cell><cell>+ 1.20% p = 0.302</cell></row><row><cell></cell><cell cols="2">0.3993</cell><cell cols="2">0.2354</cell></row><row><cell>RL4 SH-QM + PRF</cell><cell>RL4:RL2</cell><cell>+ 4.78% p = 0.068</cell><cell>RL4:RL2</cell><cell>+ 0.47% p = 0.452</cell></row><row><cell>using clicked doc</cell><cell>RL4:RL3</cell><cell>+ 5.58% p = 0.072</cell><cell>RL4:RL3</cell><cell>-0.72% p = 0.441</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,54.00,56.01,240.10,18.42"><head>Table 4</head><label>4</label><figDesc>reports evaluation results for our submitted runs on all topics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,56.10,82.90,235.91,145.88"><head>Table 4 . nDCG@10 and MAP of submitted results (all topics)</head><label>4</label><figDesc></figDesc><table coords="5,67.86,99.01,205.18,129.76"><row><cell>Evaluation Settings</cell><cell>Stages</cell><cell>nDCG@10</cell><cell>MAP</cell></row><row><cell></cell><cell>RL1</cell><cell>0.3789</cell><cell>0.1206</cell></row><row><cell>All</cell><cell>RL2</cell><cell>0.4281</cell><cell>0.1446</cell></row><row><cell>Subtopics</cell><cell>RL3</cell><cell>0.4282</cell><cell>0.1453</cell></row><row><cell></cell><cell>RL4</cell><cell>0.4409</cell><cell>0.1508</cell></row><row><cell></cell><cell></cell><cell>nDCG@10</cell><cell>MAP</cell></row><row><cell></cell><cell>RL1</cell><cell>0.2679</cell><cell>0.1239</cell></row><row><cell>Current Subtopic Only</cell><cell>RL2 RL3</cell><cell>0.2954 0.2981</cell><cell>0.1391 0.1399</cell></row><row><cell></cell><cell>RL4</cell><cell>0.2971</cell><cell>0.1428</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,58.50,414.74,92.35,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,427.59,208.40,8.10;5,72.00,437.92,181.94,8.10;5,72.00,448.29,197.47,8.10;5,72.00,458.62,214.16,8.10;5,72.00,468.99,103.99,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,204.46,427.59,75.94,8.10;5,72.00,437.92,80.06,8.10">Ad hoc IR: not much room for improvement</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Keeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,168.27,437.92,85.67,8.10;5,72.00,448.29,197.47,8.10;5,72.00,458.62,143.72,8.10">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information (SIGIR &apos;11)</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information (SIGIR &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1095" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,512.00,220.19,8.10;5,72.00,522.38,204.71,8.10;5,335.88,56.00,208.99,8.10;5,335.88,66.32,220.36,8.10;5,335.88,76.70,137.24,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,206.51,512.00,85.67,8.10;5,72.00,522.38,103.05,8.10">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,190.92,522.38,85.80,8.10;5,335.88,56.00,208.99,8.10;5,335.88,66.32,192.14,8.10">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;05)</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;05)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.87,91.04,211.23,8.10;5,335.87,101.42,216.42,8.10;5,335.88,111.74,210.45,8.10;5,335.88,122.12,32.25,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,364.37,101.42,187.92,8.10;5,335.88,111.74,77.55,8.10">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,419.16,111.74,76.90,8.10">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.88,150.79,208.77,8.10;5,335.88,161.11,215.77,8.10;5,335.88,171.49,208.74,8.10;5,335.88,181.81,45.75,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,357.96,161.11,189.81,8.10">University of Essex at the TREC 2010 Session Track</title>
		<author>
			<persName coords=""><forename type="first">M-Dyaa</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinzhong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Fasli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,345.66,171.49,198.95,8.10;5,335.88,181.81,19.37,8.10">Proceedings of 19th Text REtrieval Conference (TREC 2010)</title>
		<meeting>19th Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.88,196.15,211.96,8.10;5,335.88,206.53,216.23,8.10;5,335.88,216.85,190.46,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,525.36,196.15,22.48,8.10;5,335.88,206.53,146.15,8.10">RMIT University at TREC 2010: Session Track</title>
		<author>
			<persName coords=""><forename type="first">Sadegh</forename><surname>Kharazmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingfang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,497.88,206.53,54.23,8.10;5,335.88,216.85,164.10,8.10">Proceedings of 19th Text REtrieval Conference (TREC 2010)</title>
		<meeting>19th Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.87,231.25,194.45,8.10;5,335.87,241.57,218.95,8.10;5,335.87,251.95,130.70,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,457.85,231.25,72.48,8.10;5,335.87,241.57,128.24,8.10">A risk minimization framework for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,469.61,241.57,85.21,8.10;5,335.87,251.95,53.98,8.10">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="55" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.86,266.29,197.71,8.10;5,335.86,276.61,187.95,8.10;5,335.86,286.99,197.47,8.10;5,335.86,297.31,204.57,8.10;5,335.86,307.69,137.24,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,473.37,266.29,60.20,8.10;5,335.86,276.61,58.95,8.10">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,410.85,276.61,112.96,8.10;5,335.86,286.99,197.47,8.10;5,335.86,297.31,176.37,8.10">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;01)</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;01)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.85,322.02,204.54,8.10;5,335.85,332.35,215.16,8.10;5,335.85,342.72,181.16,8.10;5,335.85,353.05,200.75,8.10;5,335.85,363.42,209.12,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,493.37,322.02,47.02,8.10;5,335.85,332.35,215.16,8.10;5,335.85,342.72,29.02,8.10">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,380.12,342.72,136.90,8.10;5,335.85,353.05,200.75,8.10;5,335.85,363.42,41.48,8.10">Proceedings of the tenth international conference on Information and knowledge management (CIKM &apos;01)</title>
		<meeting>the tenth international conference on Information and knowledge management (CIKM &apos;01)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.85,377.76,183.14,8.10;5,335.85,388.08,217.48,8.10;5,335.85,398.46,218.01,8.10;5,335.85,408.78,204.01,8.10;5,335.85,419.16,207.85,8.10;5,335.85,429.48,32.25,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,467.79,377.76,51.20,8.10;5,335.85,388.08,213.76,8.10">Improving the estimation of relevance models using large external corpora</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,345.64,398.46,208.22,8.10;5,335.85,408.78,204.01,8.10;5,335.85,419.16,74.72,8.10">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;06)</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;06)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.85,443.88,210.51,8.10;5,335.85,454.21,191.21,8.10;5,335.86,464.53,154.45,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,455.36,443.88,91.00,8.10;5,335.85,454.21,85.34,8.10">Combination of evidence for effective web search</title>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,436.84,454.21,90.23,8.10;5,335.86,464.53,128.09,8.10">Proceedings of 19th Text REtrieval Conference (TREC 2010)</title>
		<meeting>19th Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.86,478.93,203.74,8.10;5,335.86,489.25,213.45,8.10;5,335.86,499.63,216.72,8.10;5,335.86,509.95,204.57,8.10;5,335.86,520.33,137.24,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,466.89,478.93,72.71,8.10;5,335.86,489.25,133.63,8.10">Positional relevance model for pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,485.33,489.25,63.99,8.10;5,335.86,499.63,216.72,8.10;5,335.86,509.95,176.37,8.10">Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;10)</title>
		<meeting>eeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
