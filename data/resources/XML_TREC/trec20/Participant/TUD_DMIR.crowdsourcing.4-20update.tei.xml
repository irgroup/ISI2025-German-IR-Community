<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.98,57.46,448.15,16.88">Managing the Quality of Large-Scale Crowdsourcing</title>
				<funder>
					<orgName type="full">Amazon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,78.98,79.96,112.32,11.26"><forename type="first">Jeroen</forename><forename type="middle">B P</forename><surname>Vuurens</surname></persName>
							<email>j.b.p.vuurens@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.69,79.96,88.46,11.26"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centrum Wiskunde &amp; Informatica Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.90,79.96,86.50,11.26"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<email>c.eickhoff@tudelft.nl</email>
							<affiliation key="aff2">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.98,57.46,448.15,16.88">Managing the Quality of Large-Scale Crowdsourcing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD7DE0A58F97DA7D51E4C7D668CD8DE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crowdsourcing can be used to obtain relevance judgments needed for the evaluation of information retrieval systems. However, the quality of crowdsourced relevance judgments may be questionable; a substantial amount of workers appear to spam HITs in order to maximize their hourly wages, and workers may know less than expert annotators about the topic being queried. The task for the TREC 2011 Crowdsourcing track was to obtain high-quality relevance judgments. The quality of obtained annotations is improved by removing random judgments and aggregating multiple annotations per query-document pair. We conclude that crowdsourcing can be used as a feasible alternative to expert annotations, based on the estimated proportions of correctly judged query-document pairs in the crowdsourced relevance judgments and previous TREC qrels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Evaluation of IR-systems generally uses known ground truth for every query-document pair. Ground truth is commonly obtained from expert annotators who manually judge relevance for each pair. Obtaining ground truth through experts is an expensive and time-consuming process <ref type="bibr" coords="1,86.42,399.49,10.69,9.05" target="#b1">[1]</ref>.</p><p>Relevance judgments can be crowdsourced on the Internet by using anonymous web users (known as workers) as non-expert annotators <ref type="bibr" coords="1,175.10,437.89,10.71,9.05" target="#b1">[1]</ref>. Through the use of crowdsourcing services like Amazon's Mechanical Turk (AMT) or CrowdFlower, it is relatively inexpensive to obtain judgments from a large number of workers in a short amount of time. Typically, several judgments are obtained per query-document pair. A consensus algorithm is used to aggregate the judgments into a single outcome per pair <ref type="bibr" coords="1,275.75,507.01,10.58,9.05" target="#b2">[2]</ref>.</p><p>The use of crowdsourcing for relevance judgments comes with new challenges. There have been several reports of workers spamming questions <ref type="bibr" coords="1,223.61,545.32,10.69,9.05" target="#b3">[3]</ref>, <ref type="bibr" coords="1,242.33,545.32,10.60,9.05" target="#b4">[4]</ref>, <ref type="bibr" coords="1,260.93,545.32,10.60,9.05" target="#b5">[5]</ref>. The random votes these workers produce can seriously affect consensus, especially at increased spam rates. Attempts to suppress random votes in a consensus algorithm showed mediocre results <ref type="bibr" coords="1,125.09,591.40,10.66,9.05" target="#b6">[6]</ref>. Therefore, an elimination strategy is used to detect spam and take it out of the dataset before determining consensus.</p><p>Section 2 discusses the importance to remove random judgments, while leaving room for difference in opinion. Section 3 gives the design of the used HIT, spam detection and management tool for obtaining results for Task 1. In Section 4, an adapted approach is given for computing consensus over the data for Task 2. The results for both tasks are described and analyzed in Section 5. In <ref type="bibr" coords="1,520.50,135.67,37.34,9.06">Section 6</ref> we conclude that the results are comparable to those of expert annotators, at lower costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FRAMEWORK OF REFERENCE 2.1 Quality of relevance judgments</head><p>There is a distinct difference between random judgments and differences of opinion. Differences of opinion are inherent to subjective information needs. Voorhees compared the variance of relevance judgments created by different assessors and the intersection between assessors <ref type="bibr" coords="1,361.51,261.35,10.69,9.05" target="#b8">[8]</ref>. She concluded that different relevance assessments, created under disparate conditions, produce essentially comparative evaluation results. This study shows that differences of opinion do not affect the usefulness of qrels for evaluation. Random judgments on the other hand are useless for the evaluation of IR systems; a perfect IR system is expected to obtain the same score as a random machine, regardless of the measure used.</p><p>While there is no need to resolve differences in opinion amongst crowdsourcing workers, random judgments increase the variance of evaluation measures, and can render a test set useless if they are too abundant. We expect that the quality of relevance judgments can be increased by decreasing the proportion of random judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Consensus for relevance judgments</head><p>The relevance judgments obtained from anonymous crowdsourcing workers are of unknown quality. Only part of the workers may use ethical behavior, as they follow instructions and aim to produce meaningful results <ref type="bibr" coords="1,532.68,478.69,10.66,9.05" target="#b6">[6]</ref>. A common approach is to obtain several judgments for each query-document pair, and combine these with a consensus algorithm <ref type="bibr" coords="1,358.99,513.16,10.69,9.05" target="#b1">[1]</ref>. The redundant information helps to filter out errors in judgment and cheat attempts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Random judgments</head><p>Search results often contain duplicate documents, which contain the same content but have different URLs. In previous TREC datasets, duplicate documents that were retrieved for the same topic were judged by the same assessor. Scholer et al. found that 18% of the duplicate documents found in previous TREC datasets were judged inconsistently, when judgments are converted to a binary scale <ref type="bibr" coords="1,339.90,636.40,10.66,9.05">[7]</ref>. Every inconsistently judged duplicate can be seen as a random element within the set of relevance judgments, and will have the same value as random data when used in evaluation.</p><p>TREC 2011 Crowdsourcing track, team TUD_DMIR In crowdsourcing, when two workers submit inconsistent judgments for the same query-document pair, we also consider this to be a random element. If both judgments would be used as separate judgments in a set of qrels, they would have the same value as random judgments. We expect to find more inconsistently judged duplicate documents via crowdsourcing annotations than expert annotations, because not every worker can be trusted to work the task as required. Previous studies report different types of spammers <ref type="bibr" coords="2,170.62,378.13,10.92,9.05" target="#b4">[4]</ref>, <ref type="bibr" coords="2,188.33,378.13,10.88,9.05" target="#b6">[6]</ref>: random spammers try to randomize their responses hoping to stay undetected, uniform spammers repeat the same label over and over and semi-random spammers switch between ethical and spamming behavior <ref type="bibr" coords="2,137.66,424.09,10.69,9.05" target="#b6">[6]</ref>. We also suspect that workers that appear to submit random results, do not always have dishonest intentions. It is possible for workers to have a different understanding of the task, possibly caused by vague or ambiguous instructions, or workers having different frames of reference and abilities (for instance good understanding of English). There are countermeasures that can help to prevent these types of random results, such as clear instructions and the use of qualification tests to determine if a worker is capable of performing the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TASK 1: OBTAINING JUDGMENTS 3.1 Strategy</head><p>The dataset we were given for Task 1 is divided into sets of 5 documents each that are to be judged for a given topic. TREC 2011 requires that every worker has to judge all 5 documents within a set. Each HIT contains 2 sets, so every worker submits a minimum of 10 judgments. Every query-document pair is to be judged by a minimum of 5 workers.</p><p>The quality of obtained crowdsourcing results is increased by detecting workers that submit random results and replacing all of their judgments with judgments from another worker, until all accepted workers pass the spam detection filters. In this study, spammers are detected by comparing each worker's judgments to the judgments by other workers on the same query-document pairs, without additional gold set questions or pre-qualification tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HIT design</head><p>The HIT design as shown in Figure <ref type="figure" coords="2,489.14,317.39,3.76,9.05">1</ref>, is focused on clarity, simplicity and worker efficiency. Instructions were formulated as clear as possible to reduce inconsistencies due to unclear labels. The algorithm that is used for detection of random judgments requires judgments to be made on an ordinal scale. The 5 labels used were explained as follows:</p><p>o Totally relevant: the document completely answers the question. o Partly relevant: the information in the document is relevant to the question but not complete. o Related: the document mentions the subject or holds potentially good hyperlinks to relevant pages, but does not contain any actual information regarding the query itself. o Not relevant/Spam: the document is off topic or spam, not giving information about the subject. o Empty/Corrupt: a document that is corrupt, unreadable or empty.</p><p>On the screen, one query-document pair is displayed at a time for the worker to judge. The query is described by the query terms used and an additional query description that was supplied in the dataset, giving additional information on the meaning of relevant, partly relevant and not relevant. The document is represented by the title, which was extracted from the original HTML and a rendered image of the webpage. The rendered pages were cropped to a resolution of 1200x2400 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Removal of random results</head><p>Detecting random votes is more difficult at higher spam rates, because estimations are blurred by the noise that is present. In a previous study using simulations, we developed a measure for detecting random spammers that has a low false-positive rate in noisy environments <ref type="bibr" coords="2,525.37,713.22,10.66,9.05" target="#b6">[6]</ref>. The key idea here is to have relevance judged on an ordinal Figure <ref type="figure" coords="2,291.50,262.07,3.90,9.05">1</ref>: HIT design scale instead of a binary one, taking advantage of the fact that ethical annotators are more likely to vote closer to each other than random spammers. The random separator algorithm uses this characteristic to detect random spammers.</p><p>The uniform separator algorithm detects uniform spammers by counting the number of errors made in repeating voting patterns, complementing the random separator <ref type="bibr" coords="3,98.71,152.00,10.58,9.05" target="#b6">[6]</ref>. Simulations with a worker population containing more than 20% uniform spammers showed that the uniform separator avoids the situation where spammers are not detected by the random separator due to the consensus being affected by those uniform spammers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Crowdsourcing Management</head><p>The HIT was implemented on AMT as a frame without any static content. Within the frame of the HIT, an external question tag is used to show a page from our own webserver. When a worker views the HIT on AMT, an example of a question is shown, which is not the question the worker is going to answer. When a worker accepts the HIT, the webserver receives the worker ID from AMT and assigns 2 sets of 5 documents each to the HIT, guarding against any possible 'worker-training effects' by ensuring that the worker has not worked these query-document pairs before. The order of sets and documents within each set are shuffled to prevent workers from getting their assignments in the exact same order.</p><p>If a user accepts a HIT for the first time, the user is asked to enter his/her nickname, which is displayed in the top-right corner. To judge the assignment on the screen the user clicks a label (Figure <ref type="figure" coords="3,179.98,417.25,3.65,9.05">1</ref>). After judging the last document in the HIT, a thank you message with a submit button will appear, enabling the user to submit the HIT on AMT in order to get paid. The standard AMT interface then shows a screen that enables a user to continue by accepting another HIT from the same batch.</p><p>All judgments are registered on the webserver along with the start and end time. The AMT is only used to attract workers and to pay them afterwards. HITs that are not submitted within 15 minutes or HITs that are returned (i.e. not completed) by workers have their assignments reset so these can be assigned to another user.</p><p>The progress of the batch is monitored by an automated tool as follows: (1) The tool generates the assignments needed to obtain 5 judgments per query-document pair. <ref type="bibr" coords="3,283.03,586.36,11.69,9.05" target="#b2">(2)</ref> The assignments are uploaded to the webserver, waiting to be completed by workers. (3) Progress is monitored and if all assignments have been completed by workers, the relevance judgments are automatically downloaded and analyzed. (4) The worker(s) that submitted the most random judgments are removed along with all their judgments, but only up to the point where at least 4 judgments for every pair remain, because the replacing votes can affect the spam detection scores for the other workers working the same query-document pairs. ( <ref type="formula" coords="3,150.32,701.34,3.90,9.05">5</ref>) If there are query-document pairs with less than 5 judgments, the tool jumps back to <ref type="bibr" coords="3,280.66,712.86,10.58,9.05" target="#b1">(1)</ref>. Otherwise the batch is complete, having obtained all required judgments by workers that have passed spam detection.</p><p>The tool automatically puts the required number of HITs on AMT and monitors these, using the AMT API. Workers that are suspected of spamming are rejected, but the system does require manual confirmation before rejecting the corresponding HITs on AMT. When a batch is completely finished, all accepted work is paid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TASK 2: CONSENSUS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Strategy</head><p>The second task of the TREC 2011 Crowdsourcing track was to aggregate the binary labels obtained from crowdsourcing workers for several query-document pairs. The approach that was used for Task 1 requires that results that are rejected are replaced, that judgments are made on an ordinal scale, collecting 5 judgments per pair and at least 10 judgments per worker to reduce false detection of spammers. The dataset for Task 2 does however not meet these requirements, so while we use the same strategy as for Task 1, and our approach had to be adapted to this specific situation.</p><p>The dataset contains workers who exclusively use the exact same label for more than 100 pairs. If we assume the underlying distribution of relevance to be balanced, this indicates the presence of uniform spammers. Instead of the uniform separator applied in Task 1, that has not been tested on binary judgments, we used a simple rule: workers are removed if they cast over 80% of their votes on the same label.</p><p>The random separator algorithm does not work on binary labels. Instead the average percentage of inter worker agreement was used to estimate how ethically they work. This measure was applied using the same iteration scheme that is used for the random separator, based on the assumption that the workers below a certain threshold are producing random results and the worker with the lowest inter worker agreement is the most likely random spammer. Iteratively, the most likely random spammer is removed and inter worker agreement is recalculated. Because this estimate is less accurate than using the random separator, detection will be less precise, causing more false-positives. Still, this is expected to remove the majority of random spammers while leaving the majority of ethical workers in the pool. Workers are removed if they have less than 70% agreement with other ethical workers.</p><p>The most likely label for a query-document pair is calculated by multiplying the probabilities that accepted workers voting that label are correct and that accepted workers voting the opposite label are wrong. The probability that a worker makes a correct judgment is estimated by their average agreement with accepted workers. The documents were ranked according to the probability that the selected label is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task 1</head><p>In Task 1, relevance judgments had to be obtained for 2,165 query-document pairs, arranged into 433 sets of 5 pairs. The sets were split into a team-specific batch (510 pairs) and the shared batch (1,655 pairs). The size of all HITs was fixed for this experiment to 10 query-document pairs. One extra set of 5 query-document pairs from the team-specific batch was therefore added to the shared batch to make the batch size a multiple of 10.</p><p>No restrictions or qualifications of any kind were used to preselect workers. In total, 503 unique workers submitted judgments for our HITs. 40 of them worked on both batches. The submitted work was analyzed afterwards, and workers that worked both batches were evaluated for each batch separately. The 543 worker evaluations (503+40) resulted in 262 acceptances and 281 rejections.</p><p>In total, 20,840 judgments were obtained of which 55% (11,510) were accepted and 45% (9,330) were rejected. To determine consensus majority voting was used, and when votes tie an EM model, as described in <ref type="bibr" coords="4,211.50,304.55,10.66,9.05" target="#b6">[6]</ref>. On 20.5% of the cases votes tie because an ordinal scale is used instead of a binary scale. The ordinal labels where then converted to binary labels, after which 78.9% of the accepted judgments agree with consensus. In comparison, 55.4% of the rejected judgments agree with consensus. The higher than random percentage for rejected judgments can be explained by incidental rejection of ethical workers and by rejected semirandom spammers; workers who switch between ethical and random voting behavior <ref type="bibr" coords="4,169.46,408.25,10.71,9.05" target="#b6">[6]</ref>.</p><p>The payment was $0.055 per accepted HIT, each HIT consisting of 10 query-document pairs. The workers that were suspected of spamming were rejected along with all the HITs they submitted. The total expenses were $66.85.</p><p>On the team-specific batch, it appeared that the uniform separator algorithm did not scale well with the number of worked HITs, causing it to falsely detect 2 workers working more than 20 HITs. The algorithm as presented in <ref type="bibr" coords="4,263.94,509.77,11.69,9.05" target="#b6">[6]</ref> was re-normalized to Formula 1. is a collection of all possible label sequences with length | | = 2 or 3. We calculate the number of disagreements between the workers' judgments that occur within occurrences of label sequence , with judgments ̅ submitted by other workers for the same query-document pair as judgment . To reduce false detections of ethical workers = 0 if .</p><p>is the frequency of label sequence occurring within worker 's time ordered judgments .</p><formula xml:id="formula_0" coords="4,124.10,652.40,158.41,22.98">∑ | | ( ) (∑ ∑ ) ̅ ∑ ∑ | ̅ |</formula><p>Formula 1: re-normalized uniform separator.</p><p>Using simulation, we found that the worker with the highest UniformSpam score that is above the empirically determined threshold of 1.2 is likely to be a uniform spammer.</p><p>Due to technical problems in the crowdsourcing management system, we unintentionally over-obtained a total of 660 accepted judgments on Task 1, which were kept in the dataset. Also, by mistake, the dataset contains 5 judgments that were marked as rejected while the other judgments by the same worker were accepted. These 5 judgments should have been marked as accepted as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Time analysis</head><p>The start and end time of every judged query-document pair were registered. Figure <ref type="figure" coords="4,433.84,167.72,4.98,9.05" target="#fig_0">2</ref> shows the average time-perquestion vs. the questions in the order that the workers answered them (question sequence number). For Figure <ref type="figure" coords="4,550.45,190.76,3.76,9.05" target="#fig_0">2</ref>, only judgments with a time-per-question &lt; 80 seconds were used to suppress the noise from users taking long breaks not actually working the assignments. For the first assignment of every HIT, workers take more time on average to read the question. Every sixth question the worker is likely to get a different topic to match the next 5 documents to, increasing the average timeper-question as they have to read it. The time-per-question typically decreases after the first and sixth question of every HIT, which is presumably caused by some training effect, gaining a better understanding of the question and having seen other documents to compare to. The overall downward trend in Figure <ref type="figure" coords="4,383.85,488.77,4.98,9.05" target="#fig_0">2</ref> indicates that workers become faster as they work more HITs. Analysis showed that workers who submitted more HITs do not work faster on previous HITs than workers who submitted less HITs.</p><p>In Figure <ref type="figure" coords="4,371.41,538.84,3.80,9.05" target="#fig_1">3</ref>, a similar time-per-question analysis is done for rejected workers. On the first 20 questions the average time-per-question pattern for ethical and rejected workers is very similar. After working these 3 HITs, rejected workers take more time for the first question of each HIT, but the time-per-question pattern in between the first questions becomes more random compared to accepted workers. It seems that in this phase rejected workers are less likely to read the new topics that are presented on the screen.</p><p>66% of the workers worked only 1 HIT. The spammers amongst these show the same time-per-question pattern as the ethical workers. It seems unlikely that their random votes are caused by not reading the new topic that is presented halfway.</p><p>Several reports suggest time-on-task as a criterion to detect spam <ref type="bibr" coords="5,104.22,179.00,27.24,9.05">[3][10]</ref>. Although in this experiment the fastest 5 workers have an average time per judgment lower than 5 seconds and were all detected as spammers, 34% of the 56 workers with an average time per judgment lower than 10 seconds were found to be ethical. While time-on-task can reveal unrealistically fast workers as spammers, it appears to be a weak stand-alone measure for general spammer detection.</p><p>To get an indication of the volumes that can be processed at the used pay-rate, the pick-up rates were analyzed. The highest pick-up rate was observed during the first iteration of the shared batch, obtaining 8,300 relevance judgments in 12 hours. After the first iteration of a batch, workers were rejected in small amounts to minimize false rejections. The number of HITs in consecutive iterations were therefore small, often 1 to 6 HITs. A repost of 1 HIT on AMT took longer to get picked up than a task that is larger in volume. Presumably, a larger volume is more attractive to workers as it enables them to work more efficiently, repeating a similar task several times, becoming faster as they train, while only having to read the instructions once.</p><p>On the team-specific batch, the average pick-up rate after the first iteration was 2 HITs per hour. Just before starting the shared batch, the HIT management tool was altered to fake the number of available HITs on AMT. The tool put three times as many HITs on AMT as were actually available, automatically taking off the excessive HITs as soon as a HIT was accepted by a worker. This increased the average pick-up rate to 6 HITs per hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison of judgments vs consensus</head><p>In Figure <ref type="figure" coords="5,108.19,551.80,3.80,9.05" target="#fig_2">4</ref>, a grouped bar chart shows the frequency of labels chosen by accepted workers as the primary groups along the x-axis. Within each primary group, the chosen labels are distributed over the consensus on corresponding query-document pairs. The solid bars within each group are the judgments of accepted workers that agree with consensus. The majority of judgments agree with consensus or are on adjoining labels to consensus. While difference of opinion does exist, judgments are less likely to be made on the opposite end of the scale. Figure <ref type="figure" coords="5,362.32,227.51,4.98,9.05" target="#fig_3">5</ref> shows the same graph for rejected workers compared to the consensus taken from accepted workers. Both accepted and rejected workers rarely used the label corrupt/empty. The characteristic difference of spammers being more likely to vote further away from other workers on an ordinal voting scale is visible as the votes in Figure <ref type="figure" coords="5,552.70,284.99,4.98,9.05" target="#fig_3">5</ref> are distributed more randomly across the remaining four labels, resulting in 35% of their judgments on nonadjoining labels as opposed to 20% by accepted workers. Of the 40 workers that worked both batches, 13 were rejected for one batch while being accepted for the other. Without looking at the submitted judgments, we manually inspected the 61 query-document pairs where these rejected workers submitted a judgment at least two labels away from consensus. We agreed with the worker on 36% and with consensus on 64%. For 4 out of 13 workers, we found that they would not have been labeled as spammer if consensus was replaced by our judgments on the query-document pairs we judged. It could be that spam detection settings are a bit too strict, generating false detections. Further inspection showed that these 4 workers primarily voted away from consensus on the topics "Lake Murray Fishing", "Sudoku" and "DIY audio", and that these 3 topics may have conflicting instructions asking for references and software, while the general instructions state that information must be on the page that is shown, and promising hyperlinks are to be regarded as not relevant. Creating clear instructions may increase agreement amongst workers and help avoid false rejections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Estimation of correct judgments</head><p>The inconsistencies reported by Scholer et al. <ref type="bibr" coords="6,254.21,69.80,11.69,9.05">[7]</ref> can be used to estimate the proportion of incorrectly judged documents in previous TREC qrels (Table <ref type="table" coords="6,232.82,92.72,3.60,9.05">1</ref>). The ternary labels were converted to binary labels (partly relevant becoming relevant). In total, the authors reported 24,327 consistently judged pairs and 5,514 inconsistently judged pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: estimation of correct judgments</head><p>From these numbers we can estimate the probability that a qrel is correct (Formula 2). A pair of binary judgments is consistent when either both judgments are correct or both are incorrect. Using the quadratic formula we find the probability of a judgment being correct, which is 89.7% for previous TREC qrels. <ref type="bibr" coords="6,145.17,357.44,5.42,10.71" target="#b5">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> P</head><p>Formula 2: estimate probability judgment is correct For an analysis of inconsistencies within the crowdsourced results for Task 1, judgments performed by different workers on the same query-document pair are used as duplicates. Applying Formula 2 results in the estimation that 79.1% of the crowdsourced judgments are correct.</p><p>For the crowdsourcing task, we obtained 5 judgments per query-document pair. Sheng et al. estimate the integral quality by using majority voting, assuming uniform worker quality and uniform task difficulty <ref type="bibr" coords="6,195.17,539.92,10.69,9.05" target="#b9">[9]</ref>. Using Formula 3 we estimate that 93.5% of the crowdsourced qrels for Task 1 are correct. </p><formula xml:id="formula_1" coords="6,134.72,580.11,139.49,24.74">                            </formula><p>Formula 3: estimating quality by majority voting using uniform worker quality A possible weakness in this estimation is the implicit assumption that the probability of inconsistently judged documents is transferable to the probability of incorrectly judged consistent pairs. Consistently judged pairs could be incorrect more often if spammers get the upper hand or if false consensus is being obtained by simply removing workers that agree least.</p><p>Spammers can affect consistent pairs more than inconsistent pairs by giving the same judgments on the same query-document pairs. The detection mechanisms that are used guard specifically against the repeated voting patterns of uniform spammers. To guard against the possibility of organized spammers entering the same sequence of votes on the same query-document pairs, the pairs are shuffled within each set of 5 pairs and the sets are shuffled across HITs.</p><p>Looking into the possibility that false consensus is being obtained by simply removing workers that agree least, we note that disagreement with consensus alone does not lead to rejection by the algorithms used. Primarily disagreeing judgments that are not on adjoining labels to consensus or disagreements within repeating voting patterns will contribute to rejection, being tolerant on workers that work honestly and have a different opinion. The inter worker agreement between accepted workers is 67%, which is what can be expected under normal circumstances, close to what was found in previous studies <ref type="bibr" coords="6,447.66,278.51,11.69,9.05" target="#b1">[1]</ref> and comparable to the average agreement our workers have to the TREC consensus. This verifies that the inter worker agreement was not overestimated by rejecting too many workers that disagreed. The strength of crowdsourcing does not lie in superhuman quality of workers, but the extremely low costs making it feasible to aggregate the results of several ethical workers for every query-document pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Task 2</head><p>The dataset that was provided for Task 2 contains 19,033 binary relevance judgments submitted by 762 workers. The relevance judgments are not equally balanced over the query-document pairs; some query-document pairs have been judged by only 1 or 2 workers while others have been judged by over 10 workers.</p><p>When first running consensus without removing any workers, the large majority of documents was labeled relevant for their query. Inspection showed there to be twice as many votes on relevant as on irrelevant. In the result set, we found a large number of uniform spammers who judged the vast majority of documents as relevant. Given the high frequency of this happening, it could affect the resulting consensus.</p><p>Using a simple rule to replace the uniform separator, 256 workers that judged more than 80% with the same label were rejected, for giving unreliable results. When all workers that have less than 70% inter-worker agreement were removed, 19.5% of the query-document pairs had zero accepted workers left and 34.8% of the pairs had only one vote by an accepted worker. The threshold was lowered to 62% inter worker agreement, which decreased the number of query-document pairs with zero or one accepted workers to 31%. 148 workers were rejected for having less than 62% inter worker agreement with 'better' workers. calculated by multiplying the Pw of accepted workers voting relevant and (1-Pw) of accepted workers voting irrelevant. The same was done for the label irrelevant, balancing the sum of the label probabilities to 1. The most likely label was selected, and the probability that the correct label was chosen was used for ranking. For 8% of the query-document pairs, all workers were rejected. In those cases the judgment from the rejected worker with the highest inter worker agreement was taken.</p><p>The average agreement between accepted workers with consensus is 85.5%. However, because 31% of the votes were determined by the judgment of only one worker, the inter worker agreement is overestimated. The agreement between rejected workers and consensus is 58%. However, 8% of the query-document pairs have no accepted workers, so the 'best' rejected worker's vote was used, again overestimating agreement with consensus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>For the TREC 2011 Crowdsourcing track, teams were given the task to obtain high quality relevance judgments from individual crowdsourcing workers. Our strategy was to detect and eliminate workers that submit random votes. 45% of the submitted judgments were rejected.</p><p>The average agreement amongst accepted workers, between accepted workers and the TREC participants' consensus and between accepted workers and the gold standard are all close to 67%. To compare the quality of obtained crowdsourced judgments to those of expert annotators, the proportion of correctly judged documents was estimated from an analysis of the consistency of duplicate judgments. We estimate that 90% of the qrels of previous TREC datasets were correctly judged and 95% of the qrels that result from the crowdsourced annotations of Task 1 are correct. The strength of crowdsourcing does not come from the highest performance of individuals, but from aggregating several annotations obtained from ethical workers. The conclusion is that crowdsourcing can be a feasible alternative to relevance judgments submitted by expert annotators.</p><p>An analysis of time-on-task revealed a learning curve for workers, needing less time for judgments as they worked more HITs. Rejected workers appear to have the same time-on-task pattern for the first two HITs, indicating that rejected workers in general read the question on the first two hits. The analysis of time-on-task also indicates that rejected workers, after working three HITs, do not use more time when new topics are shown.</p><p>The second task for the TREC 2011 Crowdsourcing track was to estimate the most likely true label given a set of obtained relevance judgments obtained via crowdsourcing. The same strategy as for Task 1 was used, to remove workers that most likely submitted random votes before determining consensus. 53% of the workers were rejected. The relevance of each pair was computed by multiplying the probabilities that workers were (in)correct. The results scored well above average compared to other TREC participants, demonstrating that removal of random judgments should have priority over determining consensus.</p><p>Both on Task 1 and Task 2, spammers were found to vote more on the relevant labels, perhaps because they expect more documents to be relevant than irrelevant. A skewed label distribution amongst spammers increases their chance to coincide and affect consensus, increasing the necessity to properly filter out spammers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,322.51,369.97,232.56,9.05"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average time-per-question for accepted workers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,322.75,729.90,229.24,9.05"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average time-per-question for rejected workers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,333.55,200.48,208.11,9.05;5,391.27,212.12,92.59,9.05"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: judgment frequencies of accepted workers compared to consensus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,335.23,469.81,204.69,9.05;5,391.27,481.33,92.59,9.05"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: judgment frequencies of rejected workers compared to consensus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,56.95,55.00,496.05,205.20"><head></head><label></label><figDesc></figDesc><graphic coords="2,56.95,55.00,496.05,205.20" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We acknowledge the support received by <rs type="funder">Amazon</rs> for the accomplishment of these experiments.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,321.73,316.65,92.27,10.72" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,329.16,218.16,7.31;7,335.23,338.27,198.10,7.32" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,460.15,329.16,93.25,7.31;7,335.23,338.28,32.14,7.31">Crowdsourcing for relevance evaluation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,381.19,338.27,41.97,7.32">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,351.48,222.41,7.31;7,335.23,360.72,200.32,7.31;7,335.23,369.98,159.20,7.31" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,380.30,360.72,69.44,7.31">Learning from crowds</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,458.83,360.72,76.72,7.31;7,335.23,369.98,58.72,7.31">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,383.18,209.49,7.31;7,335.23,392.29,172.29,7.32" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,439.62,383.18,105.10,7.31;7,335.23,392.30,53.41,7.31">Crowdsourcing user studies with Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,402.79,392.29,30.24,7.32">CHI 2008</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,405.50,195.60,7.31;7,335.23,414.73,216.95,7.32;7,335.23,423.97,113.63,7.32" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,418.30,405.50,112.54,7.31;7,335.23,414.74,114.39,7.31">An analysis of assessor behavior in crowdsourced preference judgments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,464.02,414.73,88.16,7.32;7,335.23,423.97,46.90,7.32">Proceedings of SIGIR 2010 CSE Workshop</title>
		<meeting>SIGIR 2010 CSE Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,437.18,210.57,7.31;7,335.23,446.29,217.75,7.32;7,335.23,455.53,133.06,7.32" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,484.48,437.18,61.32,7.31;7,335.23,446.30,134.01,7.31">Ensuring quality in crowdsourced search relevance evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,482.86,446.29,70.12,7.32;7,335.23,455.53,64.90,7.32">Proceedings of SIGIR 2010 CSE Workshop</title>
		<meeting>SIGIR 2010 CSE Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,468.74,212.63,7.31;7,335.23,477.86,214.21,7.31;7,335.23,487.09,202.91,7.32" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,491.21,468.74,56.65,7.31;7,335.23,477.86,214.21,7.31;7,335.23,487.10,28.95,7.31">How Much Spam Can You Take? An Analysis of Crowdsourcing Results to Increase Accuracy</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B P</forename><surname>Vuurens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,378.55,487.09,135.83,7.32">Proceedings of SIGIR 2011 CIR Workshop</title>
		<meeting>SIGIR 2011 CIR Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,500.30,222.26,7.31;7,335.23,509.53,221.42,7.32;7,335.23,518.68,38.13,7.32" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,467.86,500.30,89.63,7.31;7,335.23,509.54,187.17,7.31">Quantifying Test Collection Quality Based on the Consistency of Relevance Judgments</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,536.64,509.53,20.01,7.32;7,335.23,518.68,14.47,7.32">SIGIR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,531.89,193.30,7.31;7,335.23,541.12,201.35,7.32" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,389.95,531.89,138.58,7.31;7,335.23,541.13,127.45,7.31">Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,476.50,541.12,36.55,7.32">SIGIR 1998</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,554.33,202.96,7.31;7,335.23,563.57,206.89,7.31;7,335.23,572.68,196.28,7.32" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,474.93,554.33,63.27,7.31;7,335.23,563.57,206.89,7.31;7,335.23,572.69,26.19,7.31">Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,375.43,572.68,83.60,7.32">Proceedings of KDD 2008</title>
		<meeting>KDD 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.23,585.89,216.96,7.31;7,335.23,595.12,222.42,7.32;7,335.23,604.37,50.10,7.31" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<title level="m" coord="7,376.21,585.89,175.98,7.31;7,335.23,595.12,199.19,7.32">Search of Quality in Crowdsourcing for Search Engine Evaluation</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
	<note>ECIR 2011 -Advances in Information Retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
