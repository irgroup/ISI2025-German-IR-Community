<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.88,165.85,267.55,15.12;1,161.83,187.76,269.65,15.12">Query Expansion and Message-passing Algorithms for TREC Microblog Track</title>
				<funder>
					<orgName type="full">Google and Yahoo!</orgName>
				</funder>
				<funder ref="#_t9W2zdm">
					<orgName type="full">NSF Science and Technology Center</orgName>
				</funder>
				<funder ref="#_p9ss5tr #_4ZC7aXn #_kmqQK2D">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-01-25">January 25, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.13,220.25,61.09,10.48"><forename type="first">Dzung</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.18,220.25,64.05,10.48"><forename type="first">Qifang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.64,220.25,54.46,10.48"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.85,220.25,33.32,10.48"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.88,165.85,267.55,15.12;1,161.83,187.76,269.65,15.12">Query Expansion and Message-passing Algorithms for TREC Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-01-25">January 25, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">A8BC5AC49DC36B58E49F889006BDF5CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the methods that our Information Retrieval Group at Purdue University used for the TREC Microblog 2011 track. The first method is the pseudo-relevance feedback, a traditional algorithm to reformulate the query by adding expanded terms to the query. The second method is the affinity propagation, a non parametric clustering algorithm that can group the top tweets according to their similarities. The final score of a tweet is based on its relevance score and the relevance score of its representative in the group. We found that query expansion is a very useful technique for microblog retrieval, while affinity propagation could achieve a comparable performance when combining with other techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Information Retrieval Group at Purdue University participated in the TREC Microblog track. Our approach was to get the initial ranked list of tweets for each query using the Indri query language <ref type="bibr" coords="1,366.10,531.10,98.17,8.74" target="#b2">[Strohman et al., 2004]</ref>. We then applied a non parametric clustering algorithm called affinity propagation <ref type="bibr" coords="1,155.64,555.01,100.49,8.74" target="#b0">[Frey and Dueck, 2007]</ref> to group the top relevant tweets based on their similarities. The goal of affinity propagation was to identify the exemplar (i.e. the representative) for each tweet in the list. The score of the exemplar was then used to boost the scores of the tweets for which the exemplar represents. It is expected that doing so will increase the scores of those tweets similar to the query-relevant tweets, and lower the scores of those similar to the irrelevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>In order to comply with the requirement of not using any future evidence, we chose to build different indexes for different queries. Specifically, for each query, we extract all the tweets posted before the issued time of that query, and build a separate index for those tweets. While this approach may require a large disk space, it is the method closest to what may occur in reality. In real systems, most indexes are built incrementally, and at one point of time, we can see only one copy of the index, which contains all the tweets appeared so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query reformulation</head><p>We explored using the two following techniques to reformulate the query 1. Exploring tags: Many tweets contain tags, which are marked by the '#' symbol. We extracted those tags and used them as an additional type of information for the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">N-gram:</head><p>We grouped any two consecutive words in the query, and added them to the query.</p><p>For example, with the original query "BBC World Service staff cuts", our reformulated query in the Indri language would become #weight(1 #combine(BBC.content World.content Service.content staff.content cuts.content ) λ 1 #combine(BBC.tag World.tag Service.tag staff.tag cuts.tag )</p><formula xml:id="formula_0" coords="2,124.80,366.52,343.72,20.69">λ 2 #combine( #2(BBC World) #2(World Service) #2(Service staff ) #2(staff cuts) ) )</formula><p>where ".content" indicates that the words must appear in the text of the tweet, and ".tag" indicates that the words must appear as a tag. In our experiments, we set λ 1 = 0.3 and λ 2 = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query expansion</head><p>We applied the relevance feedback model <ref type="bibr" coords="2,308.03,460.62,117.26,8.74" target="#b1">[Lavrenko and Croft, 2001]</ref> for query expansion. According to this model, we select the expanded terms based of their goodness, which are estimated as follows.</p><formula xml:id="formula_1" coords="2,232.42,507.88,128.49,20.14">P (t|Q) = d∈F P (t|d) * P (d|Q)</formula><p>where F is the set of feedback documents, usually chosen as the set of top documents for the query; P (t|d) is the probability that the term t appearing in the document d, and P (d|Q) is the probability that the document d is generated from the same language model as the query Q. We used the Laplace smoothing technique to estimate P (t|d), i.e.</p><formula xml:id="formula_2" coords="2,252.37,608.01,86.66,23.23">P (t|d) = tf d (t) + 1 tf d (t) + K</formula><p>where tf d (t) is the term frequency of term t with respect to document d, and K is the total number of different terms in the feedback document set. The other term P (d|Q) is estimated by the score of the document d with respect to the query Q. In this case, we used the Indri relevance score for the value of P (d|Q). The expanded query is created by looking at the top 10 documents for each original query, and selecting the top 10 terms for expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Affinity Propagation</head><p>We applied the affinity propagation algorithm <ref type="bibr" coords="3,323.87,147.35,100.49,8.74" target="#b0">[Frey and Dueck, 2007]</ref> to further cluster tweets. The idea is that tweets that are highly similar to relevant tweets should have higher chance of being relevant. We chose affinity propagation instead of the other clustering methods for the following reasons.</p><p>• Affinity propagation does not require to specify the number of cluster beforehand. This gives us more flexibility since it is hard to fix a single number of clusters for all different queries.</p><p>• While many clustering methods only consider the similarities between different data points (i.e. tweets), affinity propagation offers us a natural way to incorporate the relevance score of each tweet into the clustering algorithm. The relevance score of each tweet can effect its chance of being the representative of a cluster. Intuitively, highly relevant tweets should have a better chance of representing the other tweets.</p><p>• Affinity propagation is easy to implement and proved to be very fast in practice.</p><p>The affinity propagation algorithm can be described as follows. The final goal is to group all the tweets into different clusters, each cluster is represented by one tweet called the exemplar. The input of the algorithm is a matrix of similarities between all the tweets, where the similarity s(i, j) indicates how well the tweet j is to become the exemplar of the tweet i. In particular, the value s(i, i) indicates how well the tweet i is to be an exemplar. In our context, it is natural to set s(i, i) to be proportional to the initial relevance score of the tweet i with respect to the working query.</p><p>This algorithm also defines the two terms for message passing: responsibility and availability. The responsibility r(i, j), sent from i to j, denotes the accumulated evidence for how well the item j could be the exemplar of item i. The availability a(i, j), sent from j to i, denotes the accumulated evidence for how appropriate for i to choose j as its exemplar. For each iteration, the algorithm updates all the responsibilities and availabilities of all items, until no change is made for a prespecified number of iterations. The update rules are as follows.</p><p>1. All availabilities are initialized to 0.</p><p>2. The responsibilities are updated according to the rule</p><formula xml:id="formula_3" coords="3,217.81,586.84,182.60,14.66">r(i, j) = s(i, j) -max k s.t. k =j {a(i, k) + s(i, k)}</formula><p>3. The availabilities are updated according to the rule a(i, j) = min{0, r(j, j) + i s.t. i / ∈{i,j} max{0, r(i , j)}} 4. The self-availability is updated differently</p><formula xml:id="formula_4" coords="3,243.00,700.42,132.22,20.14">a(i, i) = j s.t. j =i max{0, r(j, i)}</formula><p>5. Repeat steps ( <ref type="formula" coords="4,213.48,128.96,4.11,8.74">2</ref>)-( <ref type="formula" coords="4,229.92,128.96,4.11,8.74">4</ref>) until local decisions (to be specified later) stay constant for a certain number of iterations.</p><p>At any point of the process, the value of j that maximizes a(i, j) + r(i, j) identifies the exemplar of the tweet i, which could be itself. It is also considered as the local decision of i, which serves for the stopping criteria mentioned above. In order to avoid numerical oscillations, we set a damping factor λ to be 0.5. That says that the values of the above terms should be updated as a combination of 50% of their previous values and 50% of their prescribed updated values.</p><p>After the affinity propagation process stops, we update the score of each tweet by adding to its initial score the relevance score of its exemplar, weighted by some constant α. In our experiments, we set α = 0.8.</p><p>That remains us to calculate the similarity score of two tweets. Formally, that similarity score consists of two parts: the similarity between the contents of the texts, and the similarity between the two users posting the tweets. For the first part, we built a language model for the content of each tweet, and then calculate the cosine similarity between those two models. The similarity between two users is quite the same, with the only difference is that the language model is now built based on all the tweets posted so far by the user. Finally we combine the two similarities as follows.</p><p>sim(i, j) = sim content (i, j) + βsim user (i, j)</p><p>In our experiments, β is set to be 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Other heuristics</head><p>We applied a couple of other heuristics to refine the results.</p><p>• In order to remove non-English tweets, we count the portion of English non-stopwords appearing in the tweets. Based on the fact that it is not natural to build an English sentence without using any stop-words, we remove all the tweets that have high ratio of non-stopwords in the sentence.</p><p>• Since the final judgements are based on the chronological order, not on the relevance scores, we apply a threshold on the final scores before arranging them in the chronological order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Table <ref type="table" coords="4,151.49,583.25,4.98,8.74" target="#tab_0">1</ref> shows the average precision results of our methods. All methods applied the query reformulation technique described above for the initial ranked list. The first column shows the results of applying only query reformulation with the heuristics. The remaining columns show the results of applying other algorithms on top of query reformulation, to further refine the results. The last two columns are our runs submitted to the competition. The first two columns are reported for comparison.</p><p>It is observed that query expansion produced the best precision at top 30. It can explained by the fact that tweets are generally short and share many common words with respect to a particular topic. Affinity propagation does not produce the best result by itself, but it is comparable with query expansion when two techniques are combined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This report presents our work for the Microblog retrieval task. Our approach involves applying the relevance feedback model and the affinity propagation clustering algorithm to rerank the initial list of tweets. The results show that query expansion is a very effective technique for microblog retrieval. We also applied the affinity propagation algorithm to cluster the initial ranked list of tweets, for the purpose of promoting similar tweets. However, the results of this approach are less successful. As for future work, we plan to apply some learning-to-rank algorithms for this task, as well as approaching the problem with other formal clustering algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,124.80,135.87,385.22,159.97"><head>Table 1 :</head><label>1</label><figDesc>Experimental Results of Four Methods: Query Reformulation Only, Query Expansion, Affinity Propagation and the Combination of All Methods</figDesc><table coords="5,130.78,179.11,379.24,116.73"><row><cell></cell><cell cols="3">Query Reformulation Only Query Expansion Affinity Propagation</cell><cell>All</cell></row><row><cell>P5</cell><cell>0.486</cell><cell>0.461</cell><cell>0.433</cell><cell>0.461</cell></row><row><cell>P10</cell><cell>0.455</cell><cell>0.457</cell><cell>0.427</cell><cell>0.455</cell></row><row><cell>P15</cell><cell>0.440</cell><cell>0.446</cell><cell>0.391</cell><cell>0.449</cell></row><row><cell>P20</cell><cell>0.413</cell><cell>0.425</cell><cell>0.361</cell><cell>0.421</cell></row><row><cell>P30</cell><cell>0.387</cell><cell>0.403</cell><cell>0.320</cell><cell>0.399</cell></row><row><cell>P100</cell><cell>0.116</cell><cell>0.121</cell><cell>0.096</cell><cell>0.120</cell></row><row><cell>P200</cell><cell>0.058</cell><cell>0.060</cell><cell>0.048</cell><cell>0.060</cell></row><row><cell>P500</cell><cell>0.023</cell><cell>0.024</cell><cell>0.019</cell><cell>0.024</cell></row><row><cell>P1000</cell><cell>0.012</cell><cell>0.012</cell><cell>0.010</cell><cell>0.012</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgement</head><p>This work is partially supported <rs type="funder">NSF</rs> grants <rs type="grantNumber">IIS-0746830</rs>, <rs type="grantNumber">CNS-1012208</rs> and <rs type="grantNumber">IIS-1017837</rs>. This work is also partially supported by the <rs type="institution">Center for Science of Information (CSoI)</rs>, an <rs type="funder">NSF Science and Technology Center</rs>, under grant agreement <rs type="grantNumber">CCF-0939370</rs>. We also thank the kind supports from <rs type="funder">Google and Yahoo!</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_p9ss5tr">
					<idno type="grant-number">IIS-0746830</idno>
				</org>
				<org type="funding" xml:id="_4ZC7aXn">
					<idno type="grant-number">CNS-1012208</idno>
				</org>
				<org type="funding" xml:id="_kmqQK2D">
					<idno type="grant-number">IIS-1017837</idno>
				</org>
				<org type="funding" xml:id="_t9W2zdm">
					<idno type="grant-number">CCF-0939370</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,124.80,579.22,75.52,12.62;5,124.80,604.08,343.71,8.74;5,135.32,616.03,333.20,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,375.94,604.08,92.57,8.74;5,135.32,616.03,125.91,8.74">Clustering by passing messages between data points</title>
		<author>
			<persName coords=""><forename type="first">Dueck ;</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,268.39,616.03,32.08,8.74">Science</title>
		<imprint>
			<biblScope unit="issue">5814</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007. 2007</date>
			<pubPlace>New York, N.Y.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,124.80,635.51,343.71,8.74;5,135.32,647.46,333.19,8.74;5,135.32,659.42,333.19,8.74;5,135.32,671.37,115.96,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,425.48,635.51,43.03,8.74;5,135.32,647.46,98.88,8.74">Relevance based language models</title>
		<author>
			<persName coords=""><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Croft ; Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,245.36,647.46,223.15,8.74;5,135.32,659.42,333.19,8.74;5,135.32,671.37,43.83,8.74">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR &apos;01</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,124.80,690.85,343.71,8.74;5,135.32,702.80,333.19,8.74;5,135.32,714.76,299.96,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,169.88,702.80,281.24,8.74">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><surname>Strohman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,135.32,714.76,295.61,8.74">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
