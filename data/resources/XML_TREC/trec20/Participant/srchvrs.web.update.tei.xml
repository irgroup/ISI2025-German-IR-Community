<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.00,107.34,347.81,14.32;1,126.00,127.27,131.12,14.32">Evaluating Learning-to-Rank Methods in the Web Track Adhoc Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-11">November 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,126.00,155.40,63.72,8.39"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
						</author>
						<author>
							<persName coords="1,211.47,155.40,52.86,8.39"><forename type="first">Anna</forename><surname>Belova</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Gaithersburg</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Internet Mathematics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.00,107.34,347.81,14.32;1,126.00,127.27,131.12,14.32">Evaluating Learning-to-Rank Methods in the Web Track Adhoc Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-11">November 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">781A6B1C03CEAC8A55581E685C795D61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning-to-rank methods are becoming ubiquitous in information retrieval. Their advantage lies in the ability to combine a large number of low-impact relevance signals. This requires large training and test data sets. A large test data set is also needed to verify the usefulness of specific relevance signals (using statistical methods). There are several publicly available data collections geared towards evaluation of learning-to-rank methods. These collections are large, but they typically provide a fixed set of precomputed (and often anonymized) relevance signals.</p><p>In turn, computing new signals may be impossible. This limitation motivated us to experiment with learning-to-rank methods using the TREC Web adhoc collection. Specifically, we compared performance of learning-to-rank methods with performance of a hand-tuned formula (based on the same set of relevance signals). Even though the TREC data set did not have enough queries to draw definitive conclusions, the hand-tuned formula seemed to be at par with learning-to-rank methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION 1.1 Motivation and Research Questions</head><p>Strong relevance signals are similar to antibiotics: development, testing, and improvement of new formulas that provide strong relevance signals take many years. On the other hand, the adversarial environment (i.e., the SEO community) constantly adapts to new approaches and learns how to make them less efficient. Because it is not possible to wait until new ranking formulas are developed and assessed, a typical strategy to improve retrieval effectiveness is to combine a large number of relevance signals using a learning-to-rank method.</p><p>A common belief is that this approach is extremely efficient. We wondered whether we could actually improve our standing among TREC participants by applying a general-purpose discriminative learning method. Was there sufficient training data for this objective? We were also curious about how large the gain in performance might be.</p><p>It is a standard practice to compute hundreds of potential relevance signals and make a training algorithm decide which signals are good and which are not. On the other hand, it would be useful to decrease dimensionality of the data by detecting and discarding signals that do not influence rankings in a meaningful way. A lot of signals are useful, but their impact on retrieval performance is small. Can we detect such low-impact signals in a typical TREC setting?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Paper Organization</head><p>In Section 1.3, it is explained why we chose the TREC Web adhoc collection over specialized learning-to-rank datasets. In Section 2, we describe our experimental settings. In particular, Section 2.3 has a description of relevance signals. The base-• line ranking formula and learning-to-rank methods used in our work are presented in Sections 2.4 and 2.5, respectively. Experimental results are discussed in Section 3, specifically, experiments with low-impact relevance signals are described in Section 3.2. Failure analysis is given in Section 3.3. Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Why TREC?</head><p>There are several large-scale learning-to-rank data sets designed to evaluate learningto-rank methods:</p><p>-The Microsoft learning-to-rank data set contains 30K queries from a retired training set previously used by the search engine Bing. 1 The semantics of all features is disclosed. Thus, it is known which relevance signals are represented by these features. However, there is no proximity information and it cannot be reconstructed. -The internal Yahoo! data set (used for the learning-to-rank challenge) does contain proximity information, but all features are anonymized and normalized. 2 -The internal Yandex data set (used in the Yandex learning-to-rank competition) is similar to the Yahoo! data set in that its features are not disclosed. 3 -The Microsoft LETOR 4.0 data set employs the publicly available Gov2 collection and two sets of queries used in the TREC conferences <ref type="bibr" coords="2,366.34,352.05,42.38,9.13">(2007 and</ref><ref type="bibr" coords="2,411.25,352.05,73.41,9.13;2,135.96,364.01,57.19,9.13">2008 One Million Query Track)</ref>. 4,5 Unfortunately, proximity information cannot be reconstructed from provided metadata: to be evaluated it requires full access to the Gov2 collection.</p><p>Because it is very hard to manually construct an efficient ranking formula that encompasses undisclosed and normalized features, Yahoo! and Yandex data sets were not usable for our purposes. On the other hand, the Microsoft data sets lack proximity features, which are known to be powerful relevance signals improving the mean average precision and ERR@20 by approximately 10% <ref type="bibr" coords="2,421.06,454.35,63.63,9.13;2,126.00,466.32,22.68,9.13" target="#b23">[Tao and Zhai 2007;</ref><ref type="bibr" coords="2,152.82,466.32,137.45,9.13" target="#b9">Cummins and O'Riordan 2009;</ref><ref type="bibr" coords="2,294.43,466.32,108.03,9.13" target="#b1">Boytsov and Belova 2010</ref>]. Because we did not have access to Gov2 and, consequently, could not enrich LETOR 4.0 with proximity scores, we decided to conduct similar experiments with the ClueWeb09 adhoc data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EXPERIMENTAL SETUP AND EMPLOYED METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">General Notes</head><p>We indexed the smaller subset of ClueWeb09 Web adhoc collection (Category B) that contained 50M pages. Both indexing and retrieval utilities worked on a cluster of 3 three laptops (running Linux) connected to a 2TB disk storage. We used our own retrieval and indexing software written mostly in C++. To generate different morphological word forms, we relied on the library from http://lemmatizer.org. We</p><p>• 3 also employed third-party implementations of learning-to-rank methods (described in Section 2.5).</p><p>The main Web adhoc performance metric in 2010 and 2011 was ERR@20 <ref type="bibr" coords="3,447.04,145.17,40.67,9.13;3,126.00,157.12,42.26,9.13" target="#b6">[Chapelle et al. 2009</ref>]. To assess statistical significance of results, we used the randomization test utility developed by Mark Smucker <ref type="bibr" coords="3,307.44,169.07,87.85,9.13" target="#b22">[Smucker et al. 2007</ref>]. The results were considered significant if the p-value did not exceed 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Changes in Indexing and Retrieval Methods from 2010</head><p>Our 2011 system was based on the 2010 prototype that was capable of indexing close word pairs efficiently <ref type="bibr" coords="3,246.53,229.54,115.50,9.13" target="#b1">[Boytsov and Belova 2010]</ref>. In addition to close word pairs, this year we created positional postings of adjacent pairs (i.e., bigrams), where one of the words was a stop word. These bigrams are called stop pairs.</p><p>The other important changes were:</p><p>-We considerably expanded the list of relevance signals that participated in the ranking formula. In particular, all the data was divided into fields, which were indexed separately. We also modified the method of computing proximity-related signals (see Section 2.3 for details). -Instead of index-time word normalization (i.e., conversion to a base morphological form), we performed a retrieval-time weighted morphological query expansion. In that, each exact occurrence of a query word contributed to a field-specific BM25 score with a weight equal to one, while other morphological forms of query words contributed with a weight smaller than one. -Stop pairs were disregarded during retrieval if a query had sufficiently many non-stop words (more than 60% of all query words). -Instead of enforcing strictly conjunctive query semantics, we used a quorum rule: only a fraction (75%) of query words had to be present in a document. Incomplete matches that missed some query words were penalized. In addition, stop pairs were treated as optional query terms and did not participate in the computation of the quorum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Signals</head><p>During indexing, the text of each HTML document was divided into several sections (called fields): title, body, text inside links (in the same document), URL, meta tag contents (keywords and descriptions), and anchor text (text inside links that point to this document).</p><p>Our retrieval engine operated in two steps. In the first step, for each document we computed a vector of features, where each element combined one or more relevance signals. In the second step, feature vectors were either plugged into our baseline ranking formula or were processed by a learning-to-rank algorithm.</p><p>Overall, we had 25 features that could be divided into field-specific and documentspecific relevance features. Given a modest size of the training set, having a small feature set is probably an advantage, because it makes overfitting less likely.</p><p>The main field-specific features were:</p><p>-The sum of BM25 scores <ref type="bibr" coords="3,247.63,658.61,69.09,9.13" target="#b20">[Robertson 2004</ref>] of non-stop query words, which were computed using field-specific inverted document frequencies (IDF). These scores</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>were incremented by BM25 scores of stop pairs;<ref type="foot" coords="4,344.26,120.06,3.97,4.32" target="#foot_0">6</ref> -The classic pair-wise proximity scores, where a contribution of a word pair was a power function of the inverse distance between words in a document;</p><p>-Proximity scores based on the number of close word pairs in a document. A pair of query words was considered close if the distance between the words in a document was smaller than a given threshold MaxCloseP airDist (in our experiments 10).</p><p>These pair-wise proximity scores are variants of the unordered BM25 bigram model ( <ref type="bibr" coords="4,160.40,220.09,108.08,9.13" target="#b17">[Metzler and Croft 2005;</ref><ref type="bibr" coords="4,272.41,220.09,85.76,9.13" target="#b11">Elsayed et al. 2010;</ref><ref type="bibr" coords="4,362.09,220.09,62.80,9.13" target="#b13">He et al. 2011]</ref>). They were computed as follows. Given a pair of query words q i and q j (not necessarily adjacent), we retrieved a list of all documents, where words q i and q j occured within a certain small distance. A pseudo-frequency f of the pair in a document was computed as a sum of values of a kernel function g(.): f = g(pos(q i ), pos(q j )),</p><p>where the summation was carried out over occurrences of q i and q j in the document (in positions pos(q i ) and pos(q j ), respectively).</p><p>To evaluate the total contribution of q i and q j to the document weight, the pseudo-frequency f was plugged into a standard BM25 formula, where the IDF of the word pair (q i , q j ) was computed as IDF(q i ) + IDF(q j ).</p><p>The classic proximity score relied on the following kernel function:</p><formula xml:id="formula_0" coords="4,262.92,390.23,84.82,9.13">g(x, y) = |x -y| -α ,</formula><p>where α = 2.5 was a configurable parameter.</p><p>The close-pair proximity score employed the rectangular kernel function:</p><formula xml:id="formula_1" coords="4,201.69,440.91,203.60,21.08">g(x, y) = 1, if |x -y| ≤ MaxCloseP airDist 0, if |x -y| &gt; MaxCloseP airDist</formula><p>In this case, the pseudo-frequency f is equal to the number of close pairs that occur within the distance MaxCloseP airDist in a document. The document-specific features included:</p><p>-Linearly transformed spam rankings provided by the Waterloo university <ref type="bibr" coords="4,462.56,516.63,17.67,9.13;4,135.96,528.59,72.67,9.13" target="#b7">[Cormack et al. 2010]</ref>;</p><p>-Logarithmically transformed PageRank scores <ref type="bibr" coords="4,339.10,544.25,86.57,9.13" target="#b2">[Brin and Page 1998</ref>] provided by the Carnegie Mellon University.<ref type="foot" coords="4,273.00,555.04,3.97,4.32" target="#foot_1">7</ref> During retrieval, these scores were additionally normalized by the number of query words;</p><p>-A boolean flag that is true if and only if a document belongs to the Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baseline Ranking Formula</head><p>The baseline ranking formula was a simple semi-linear function, where field-specific proximity scores and field-specific BM25 scores contributed as additive terms. The</p><p>• 5</p><p>overall document score was equal to</p><formula xml:id="formula_2" coords="5,163.55,148.91,321.10,19.81">DocumentSpecif icScore × i F ieldW eight i × F ieldScore i ,<label>(1)</label></formula><p>where F ieldW eight i was a configurable parameter and the summation was carried out over all fields. The DocumentSpecif icScore was a multiplicative factor equal to the product of the spam rank score, the PageRank score, and the Wikipedia score W ikiScore (W ikiScore &gt; 1 for Wikipedia pages and W ikiScore = 1 for all other documents).</p><p>Each F ieldScore i was a weighted sum of field-specific relevance features described in Section 2.3. These feature-specific weights were configurable parameters (identical for all fields). Overall, there were about 20 parameters that were manually tuned using 2010 data to maximize ERR@20. Note that even if we had optimized parameters automatically, this approach could not be classified as a discriminative learning-to-rank method, because it did not rely on minimization of the loss function (see a definition given by <ref type="bibr" coords="5,277.47,308.48,41.91,9.13" target="#b16">Liu [2009]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Learning-to-Rank Methods</head><p>We tested the following learning-to-rank methods:</p><p>-Non-linear SVM with polynomial and RBF kernels (package SVMLight <ref type="bibr" coords="5,442.18,365.84,42.45,9.13;5,135.96,377.80,16.75,9.13" target="#b14">[Joachims 2002</ref>]);</p><p>-Linear SVM (package SVMRank <ref type="bibr" coords="5,282.13,393.09,62.62,9.13" target="#b15">[Joachims 2006</ref>]);</p><p>-Linear SVM (package LIBLINEAR <ref type="bibr" coords="5,292.63,408.38,68.69,9.13" target="#b12">[Fan et al. 2008]</ref>);</p><p>-Random forests (package RT-Rank <ref type="bibr" coords="5,291.77,423.68,82.67,9.13" target="#b18">[Mohan et al. 2011]</ref>);</p><p>-Gradient boosting (package RT-Rank <ref type="bibr" coords="5,302.73,438.97,82.67,9.13" target="#b18">[Mohan et al. 2011]</ref>).</p><p>The final results include linear SVM (LIBLINEAR), polynomial SVM (SVMLight), and random forests (RT-Rank). The selected methods performed best in their respective categories.</p><p>In our approach, we relied on the baseline method (outlined in Section 2.4) to obtain the top 30K results, which were subsequently re-ranked using a learning-torank algorithm. Note that each learning method employed the same set of features described in Section 2.3.</p><p>Initially, we trained all methods using 2010 data and cross-validated using 2009 data. Unfortunately, cross-validation results of all learning-to-rank methods were poor. Therefore, we did not submit any runs produced by a learning-to-rank algorithm. After the official submission, we re-trained the random forests using the assessments from the Million Query Track (MQ Track). This time, the results were much better (see a discussion in Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparisons with Official Runs</head><p>We submitted two official runs: srchvrs11b and srchvrs11o. The first run was produced by the baseline method described in Section 2.4. The second run was generated by the 2010 retrieval system. <ref type="foot" coords="6,297.21,335.64,3.97,4.32" target="#foot_2">8</ref>Using the assessor judgments, we carried out unofficial evaluations of the following methods:</p><p>• A bug-fixed version of the baseline method;<ref type="foot" coords="6,325.93,376.88,3.97,4.32" target="#foot_3">9</ref> • BM25: an unweighted sum of field-specific BM25 scores; • Linear SVM;</p><p>• Polynomial SVM (we list the best and the worst run); • Random forests (trained using 2010 and MQ-Track data).</p><p>The results are shown in Table <ref type="table" coords="6,274.69,450.56,3.18,9.13" target="#tab_0">I</ref>. To provide a reference point, we also indicate performance metrics for 2010 queries.</p><p>For 2011 queries, our baseline method produced a very good run (srchvrs11b) that was better than the TREC median by more than 20% with respect to ERR@20 and NDCG@20 (these differences were statistically significant). The 2011 baseline method outperformed the 2010 method (represented by run srchvrs11o) with respect to ERR@20, while the opposite was true with respect to MAP. These differences, though, were not statistically significant.</p><p>None of the learning-to-rank methods was better than our baseline method. For most learning-to-rank runs, the respective value of ERR@20 was even worse than that of BM25. It was especially surprising in the case of the random forests, whose effectiveness was demonstrated elsewhere <ref type="bibr" coords="6,308.24,582.06,117.52,9.13">[Chapelle and Chang 2011;</ref><ref type="bibr" coords="6,429.08,582.06,55.56,9.13;6,126.00,594.02,21.20,9.13" target="#b18">Mohan et al. 2011]</ref>.</p><p>One possible explanation of this failure is either scarcity and/or low quality of the training data. To verify our hypothesis, we retrained the random forests using the MQ-Track data. The MQ Track data set represents 40K queries (as compared to 50 queries in 2010 Web adhoc Track). However, instead of the standard pooling method, the documents presented to assessors were collected using the Minimal Test Collections method <ref type="bibr" coords="7,238.79,321.23,97.06,9.13" target="#b3">[Carterette et al. 2009</ref>]. As a result, there were many fewer judgments per query (on average). To study the effect of the training set size, we divided the training data into 10 subsets of progressively increasing size (all increments represented approximately the same volume of training data). We then used this data to produce and evaluate the runs for 2010 and 2011 queries.</p><p>From the results of this experiment presented in Table <ref type="table" coords="7,380.14,381.01,7.47,9.13" target="#tab_1">II</ref> we concluded the following:</p><p>-The MQ-Track training data is apparently much better than 2010 data. Note especially the results for 2011 queries. The 10% training subset has about three times as few judgments as the full 2010 data set. However, random forests trained on the 10% subset generated a run that was 50% better than random forests trained on 2010 data (with respect to ERR@20). This difference was statistically significant. -The number of judgments in a training set does play an important role. The value of all performance metrics mostly increases with the size of the training set. This effect is more pronounced for 2010 queries than for 2011 queries. The best value of ERR@20 is achieved by using approximately 80% of the training data for 2011 queries and by using the entire MQ-track training data for 2010 queries. Also note that the difference between the worst and the best run was statistically significant only for 2010 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Measuring Individual Contributions of Relevance Signals</head><p>To investigate how different relevance signals affected the overall performance of our retrieval system, we evaluated several extensions of BM25. Each extension combined one or more of the following:</p><p>-Addition of one or more relevance features; -Exclusion of field-specific BM25 scores for one or more of the fields: URL, keywords, description, title, and anchor text; -Inclusion of BM25 scores of stop pairs; Note: Significant differences from BM25 are given in bold typeface.</p><p>-Application of the weighted morphological query expansion.</p><p>This experiment was conducted separately for two sets of queries. One set contained only 2011 queries and the other set contained a mix of 2011 queries with the first 450 MQ-Track queries. Table <ref type="table" coords="8,281.25,562.89,11.35,9.13" target="#tab_2">III</ref> contains the results. It shows the absolute values of the performance metrics as well as their percent differences from the BM25 score (significant differences are given in bold typeface). We also post performance results of the bug-fixed baseline method (runs denoted as srchvrs11b).</p><p>Let us consider 2011 queries first. It can be seen that improvement in search performance (as measured by ERR@20 and NDCG@20) was demonstrated by all signals except: PageRank; BM25 scores for URL, keywords, and description. Yet, very few improvements were statistically significant. Furthermore, if the Bonferroni adjustment for multiple pairwise comparisons were made <ref type="bibr" coords="8,382.87,658.61,42.80,9.13" target="#b19">[Rice 1989</ref>], even fewer results would remain significant.</p><p>Except for the two proximity scores, none of the other relevance signals significantly improved over BM25 according to the following performance metrics: ERR@20, NDCG@20, and MAP. Do the low-impact signals in Table <ref type="table" coords="9,434.85,145.13,11.35,9.13" target="#tab_2">III</ref> actually improve the performance of our retrieval system? To answer this question, let us examine the results for the second set of queries, which comprises 2011 queries and MQ-Track queries. As we noted in Section 3.1, MQ-Track relevance judgements are sparse. However, we compare very similar systems with largely overlapping result sets and similar ranking functions. 10 Therefore, we believe that the pruning of judgments in MQ-Track queries did not significantly affect our conclusions about relative contributions of relevance signals. 11 That said, we consider the results of comparisons that involve MQ-Track data only as a clue about performance of relevance signals.</p><p>It can be seen that for the mix of 2011 and MQ-Track queries, many more results were significant. Most of these results would remain valid even after the Bonferroni adjustment. Another observation is that results for both sets of queries were very similar. In particular, we can see that both proximity scores as well as the anchor text provided strong relevance signals. Furthermore, taking into account text of keywords and description considerably decreased performance in both cases. This aligns well with the common knowledge that these fields are abused by spammers and, consequently, many search engines ignore them.</p><p>Note the results of the three minor relevance signals -BM25 scores of stop pairs, Wikipedia scores, and morphology -for the mix of 2011 and MQ-Track queries. All of them increased ERR@20, but only the application of the Wikipedia scores resulted in a statistically significant improvement. At the same time, the run obtained by combining these signals resulted in a significant improvement of performance with respect to all performance metrics. Most likely, all three signals can be used to improve retrieval quality, but we cannot verify this fact using only 500 queries.</p><p>Note especially the spam rankings. In 2010 experiments, enhancing BM25 with spam rankings increased the value of ERR@20 by 43%. The significance test, however, showed that the respective p-value was 0.031, which was borderline high and would not remain significant after the Bonferroni correction. In 2011 experiments, there was only a 10% improvement in ERR@20 and this improvement was not statistically significant. In experiments with the mix of 2011 queries and MQ-track queries, the use spam rankings slightly decreased ERR@20 (although not significantly).</p><p>Apparently, the effect of spam rankings is not stable and additional experiments are required to confirm the usefulness of this signal. This example also demonstrates the importance of large-scale test collections as well the necessity for multiplecomparison adjustments.</p><p>10 Specifically, there is at least a 70% overlap among the sets of top-k documents returned by BM25 and any BM25 modification tested in this experiment (for k = 10 and k = 100). 11 Our conjecture was confirmed by a series of simulations in which we randomly eliminated entries from the 2011 qrel file (to mimic the average number of judgments per query in the mix of 2011 and MQ-Track queries). In a majority of simulations, the relative performance of methods with respect to BM25 matched that in Table <ref type="table" coords="9,272.58,661.92,8.99,7.30" target="#tab_2">III</ref>, which also conforms with previous research <ref type="bibr" coords="9,447.11,661.92,37.49,7.30;9,126.00,671.89,36.05,7.30" target="#b0">[Bompada et al. 2007</ref>]. To conclude this section, we note that PageRank (at least in our implementation) was not a strong signal either. Perhaps, this was not a coincidence. <ref type="bibr" coords="10,429.44,411.79,55.24,9.13;10,126.00,423.75,25.44,9.13" target="#b24">Upstill et al. [2003]</ref> argued that PageRank was not significantly better than a number of incoming links and could be gamed by spammers. The Microsoft team that used PageRank in 2009 abandoned it in 2010 <ref type="bibr" coords="10,244.96,447.66,89.93,9.13" target="#b8">[Craswell et al. 2010]</ref>. There is also anecdotal evidence that even in Google PageRank was ". . . largely supplanted over time" <ref type="bibr" coords="10,444.35,459.61,40.35,9.13;10,126.00,471.57,55.13,9.13">[Edwards 2011, p. 315]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Failure Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">2011: Learning-to-Rank failure</head><p>The attempt to apply learning-to-rank methods was not successful: the ERR@20 of the best learning-to-rank run was the same as that of the baseline method. The best result was achieved by using the random forests and MQ-Track relevance judgements for training. The very same method trained using 2010 relevance judgments produced one of the worst runs. This failure does not appear to be solely due to the small size of the training set. The random forests trained using a small subset of MQ-Track data (see Table <ref type="table" coords="10,254.28,610.14,7.06,9.13" target="#tab_1">II</ref>), which was three times smaller than 2010 training set, performed much better (with ERR@20 of the latter being 50% larger).</p><p>As noted by <ref type="bibr" coords="10,196.57,634.70,96.51,9.13">Chapelle et al. [2011]</ref> most discriminative learning methods use a surrogate loss function (mostly the squared loss) that is unrelated to the target ranking metric such as ERR@20. Perhaps, this means that performance of learningto-rank methods might be unpredictable unless the training set is very large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">2011: Possible Underfitting of the Baseline Formula</head><p>Because we manually tuned our baseline ranking formula (described in Section 2.4), we suspect that it might have been underfitted. Was it possible to obtain better results through a more exhaustive search in a parameter space? To answer this question, we conducted a grid search using parameters that control contribution of the three most effective relevance features -the classic proximity score, the close-pair score, and spam rankings -in our baseline method.</p><p>The graphs in the first row of Figure <ref type="figure" coords="11,302.24,209.17,4.98,9.13" target="#fig_0">1</ref> contain results for 2010 queries and the graphs in the second row contain results for 2011 queries. Each figure depicts a projection of a 4-dimensional graph. A color of points gradually changes from red to black: red points are the closest and black points are the farthest. Blue planes represent reference values of ERR@20 (achieved by the bug-fixed baseline method).</p><p>Note the first graph in the first row. It represents the dependency of ERR@20 on parameters for the classic proximity score and the close-pair score for 2010 queries. All points that have same values of these two parameters are connected by a vertical line segment. Points with high values of ERR@20 are mostly red and correspond to the values of the close-pair parameter from 0.1 to 1 (logarithms of the parameter fall in the range from -1 to 0). This can also be seen from the second graph in the first row. From the second graph in the second row, it follows that for 2011 queries high values of ERR@20 are achieved when close-pair coefficients fall in the range from 1 to 10.</p><p>In general, maximum values of ERR@20 for 2011 queries are achieved for somewhat larger values of all three parameters than those that result in high values of ERR@20 for 2010 queries. Apparently, these datasets are different and there is no common set of optimal parameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">2010: The Case of "To Be Or Not To Be"</head><p>The 2010 query number 70 ("to be or not to be that is the question") was particularly challenging. The ultimate goal was to find pages with one of the following:</p><p>-information on the famous Hamlet soliloquy;</p><p>-the text of the soliloquy; -the full text of play "Hamlet"; -the play's critical analysis; -quotes from Shakespeare's plays.</p><p>This task was very difficult, because the query had essentially only stop words, while most participants did not index stop words themselves, let alone their positions. Because our 2011 baseline method does index stop words and their positions, we expected that it would perform well for this query. Yet, this method found only a single relevant document (present in 2010 qrel-file).</p><p>Our investigation showed that the category B 2010 qrel-file "knew" only four relevant documents. Furthermore, one of them (clueweb09-enwp01-96-18148) was a hodge-podge of user comments on various Wikipedia pages requiring peer review. It is very surprising that this page was marked as "somewhat relevant". On the other hand, our 2011 method managed to find ten documents that, in our opinion, satisfied the above relevance criteria (see Figure <ref type="figure" coords="12,316.23,157.09,3.87,9.13">2</ref>). Documents 1-6 do not duplicate each other, but only one of them (clueweb09-en0009-68-23332) is present in the official 2010 qrel-file. <ref type="foot" coords="12,318.18,191.79,7.93,4.32" target="#foot_4">12</ref>The example of query 70 indicates that indexing of stop pairs can help to improve the citation-like queries that contain a lot of stopwords. Our analysis in Section 3.2 confirmed that stop pairs could improve retrieval performance of other types of queries as well. Yet, the observed improvement was small and not statistically significant in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>Our team experimented with several learning-to-rank methods to improve our TREC standing. We observed that learning-to-rank methods were very sensitive to the quality and volume of training data. They did not result in substantial improvements over the baseline hand-tuned formula. In addition, there was little information available to verify whether a given low-impact relevance signal significantly improved performance.</p><p>It was previously demonstrated that 50 queries (which is typical in a TREC setting) were sufficient to reliably distinguish among several systems <ref type="bibr" coords="12,431.38,411.78,53.26,9.13" target="#b25">[Zobel 1998;</ref><ref type="bibr" coords="12,126.00,423.74,112.61,9.13" target="#b21">Sanderson and Zobel 2005</ref>]. However, our experiments showed that, if differences among the systems were small, it might take assessments for hundreds of queries to achieve this goal.</p><p>We conclude our analysis with the following observations:</p><p>-The Waterloo spam rankings that worked quite well in 2010, had a much smaller positive effect in 2011. This effect was not statistically significant with respect to ERR@20 in 2011 and was borderline significant in 2010. We suppose that additional experiments are required to confirm the usefulness of Waterloo spam rankings. -The close-pair proximity score that employs a rectangular kernel function was as good as the proximity score that employed the classic proximity function. This finding agrees well with our last year observations <ref type="bibr" coords="12,357.42,565.16,114.99,9.13" target="#b1">[Boytsov and Belova 2010]</ref> as well as with the results of <ref type="bibr" coords="12,254.57,577.11,66.50,9.13" target="#b13">He et al. [2011]</ref>, who conclude that ". . . given a reasonable threshold of the window size, simply counting the number of windows containing the n-gram terms is reliable enough for ad hoc retrieval." It is noteworthy, however, that <ref type="bibr" coords="12,232.66,612.98,64.56,9.13" target="#b13">He et al. [2011]</ref> achieved a much smaller improvement over BM25 for the ClueWeb09 collection than we did (about 1% or less compared to 6-12% in our setting).</p><p>-Based on our experiments in 2010 and 2011, we also concluded that indexing of frequent close word pairs (including stop pairs) can speed up evaluation of proximity scores. However, the resulting index can be rather large: 200-300% of the text size (unless this index is pruned). It maybe infeasible to store a huge bigram index in the main memory (during retrieval time). A better solution is to rely on solid state drives, which are already capable of bulk read speeds in the order of gigabytes per second. 13  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,126.00,129.52,358.59,7.30;10,126.00,139.48,52.52,7.30;10,294.37,150.80,19.53,5.07"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Grid search over the coefficients for the following features: classic proximity, close pairs, spam ranking. 2010</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,126.00,118.61,147.07,9.13;12,126.00,130.56,48.61,9.13;12,126.00,148.60,136.36,9.13;12,126.00,162.54,136.36,9.13;12,126.00,176.49,136.36,9.13;12,126.00,190.44,136.36,9.13;12,126.00,204.38,136.36,9.13;12,126.00,218.33,138.86,9.13;12,126.00,232.28,138.86,9.13;12,126.00,246.23,138.86,9.13;12,126.00,260.17,138.86,9.13;12,126.00,274.12,143.83,9.13"><head>•Fig</head><label></label><figDesc>Fig. 2: Query 70: Relevant Documents(1) clueweb09-en0000-07-07458 (2) clueweb09-en0000-24-22960 (3) clueweb09-en0007-84-36482 (4) clueweb09-en0009-68-23332(5) clueweb09-en0011-78-11392 (6) clueweb09-enwp00-93-09674 (7) clueweb09-enwp01-86-02832 (8) clueweb09-enwp02-22-13980 (9) clueweb09-enwp02-25-00811 (10) clueweb09-enwp03-20-03393</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,117.58,129.52,381.07,191.61"><head>Table I :</head><label>I</label><figDesc>Summary of resultsBest results for a given year are in bold typeface.</figDesc><table coords="6,117.58,141.28,381.07,177.97"><row><cell>Run name</cell><cell>ERR@20</cell><cell>NDCG@20</cell><cell>MAP</cell><cell>P@5</cell><cell>P@15</cell></row><row><cell>TREC 2011</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>srchvrs11b (bugfixed, non-official)</cell><cell>0.13888</cell><cell>0.22879</cell><cell>0.1131</cell><cell>0.4160</cell><cell>0.3093</cell></row><row><cell>srchvrs11b (baseline method, official run)</cell><cell>0.13145</cell><cell>0.23310</cell><cell>0.1098</cell><cell>0.3920</cell><cell>0.3227</cell></row><row><cell>srchvrs11o (2010 method, official run)</cell><cell>0.11224</cell><cell>0.21366</cell><cell>0.1251</cell><cell>0.3760</cell><cell>0.3280</cell></row><row><cell>BM25</cell><cell>0.11559</cell><cell>0.17637</cell><cell>0.0834</cell><cell>0.3040</cell><cell>0.2333</cell></row><row><cell>SVM linear</cell><cell>0.08431</cell><cell>0.13178</cell><cell>0.0642</cell><cell>0.2560</cell><cell>0.2040</cell></row><row><cell>SVM polynomial (s = 1 d = 3)</cell><cell>0.08554</cell><cell>0.15267</cell><cell>0.0674</cell><cell>0.2560</cell><cell>0.2547</cell></row><row><cell>SVM polynomial (s = 2 d = 5)</cell><cell>0.12058</cell><cell>0.19418</cell><cell>0.0828</cell><cell>0.3480</cell><cell>0.2747</cell></row><row><cell>Random forests (2010-train k = 1 3000 iter.)</cell><cell>0.07692</cell><cell>0.11506</cell><cell>0.0578</cell><cell>0.2240</cell><cell>0.1773</cell></row><row><cell>Random forests (MQ-train-best k = 1 1000 iter.)</cell><cell>0.13093</cell><cell>0.21069</cell><cell>0.1204</cell><cell>0.3880</cell><cell>0.3080</cell></row><row><cell>TREC 2010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>blv79y00shnk (official run)</cell><cell>0.10935</cell><cell>0.18772</cell><cell>0.1334</cell><cell>0.3500</cell><cell>0.3444</cell></row><row><cell>BM25</cell><cell>0.07826</cell><cell>0.11477</cell><cell>0.0682</cell><cell>0.2917</cell><cell>0.2514</cell></row><row><cell>BM25 +spamrank</cell><cell>0.11229</cell><cell>0.18894</cell><cell>0.1365</cell><cell>0.3625</cell><cell>0.3486</cell></row><row><cell>BM25 (+close pairs +spamrank)</cell><cell>0.12265</cell><cell>0.21086</cell><cell>0.1340</cell><cell>0.4083</cell><cell>0.3639</cell></row><row><cell>Random forests (MQ-train-best k = 1 1000 iter.)</cell><cell>0.12388</cell><cell>0.19448</cell><cell>0.0901</cell><cell>0.3875</cell><cell>0.3014</cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,123.34,129.52,359.54,139.85"><head>Table II :</head><label>II</label><figDesc>Performance of random forests depending on the size of the training set The full training set has 22K relevance judgements, best results are given in bold typeface.</figDesc><table coords="7,123.34,143.27,359.54,126.10"><row><cell>Size</cell><cell>ERR@20</cell><cell>NDCG@20</cell><cell>MAP</cell><cell>P@5</cell><cell>P@15</cell><cell>ERR@20</cell><cell>NDCG@20</cell><cell>MAP</cell><cell>P@5</cell><cell>P@15</cell></row><row><cell></cell><cell></cell><cell cols="2">TREC 2010</cell><cell></cell><cell></cell><cell></cell><cell cols="2">TREC 2011</cell><cell></cell><cell></cell></row><row><cell>10%</cell><cell>0.06532</cell><cell>0.10827</cell><cell>0.0617</cell><cell>0.2458</cell><cell>0.2028</cell><cell>0.11963</cell><cell>0.18931</cell><cell>0.1030</cell><cell>0.3440</cell><cell>0.2800</cell></row><row><cell>20%</cell><cell>0.06484</cell><cell>0.09866</cell><cell>0.0637</cell><cell>0.1792</cell><cell>0.1750</cell><cell>0.11336</cell><cell>0.19504</cell><cell>0.1088</cell><cell>0.3440</cell><cell>0.2747</cell></row><row><cell>30%</cell><cell>0.11343</cell><cell>0.16425</cell><cell>0.0761</cell><cell>0.2875</cell><cell>0.2472</cell><cell>0.11828</cell><cell>0.21001</cell><cell>0.1207</cell><cell>0.4000</cell><cell>0.3000</cell></row><row><cell>40%</cell><cell>0.10329</cell><cell>0.15110</cell><cell>0.0740</cell><cell>0.3083</cell><cell>0.2458</cell><cell>0.11733</cell><cell>0.20189</cell><cell>0.1096</cell><cell>0.4080</cell><cell>0.2840</cell></row><row><cell>50%</cell><cell>0.11343</cell><cell>0.16425</cell><cell>0.0761</cell><cell>0.2875</cell><cell>0.2472</cell><cell>0.11828</cell><cell>0.21001</cell><cell>0.1207</cell><cell>0.4000</cell><cell>0.3000</cell></row><row><cell>60%</cell><cell>0.11030</cell><cell>0.16747</cell><cell>0.0818</cell><cell>0.3083</cell><cell>0.2611</cell><cell>0.13063</cell><cell>0.21592</cell><cell>0.1267</cell><cell>0.3960</cell><cell>0.3147</cell></row><row><cell>70%</cell><cell>0.11644</cell><cell>0.18014</cell><cell>0.0823</cell><cell>0.3292</cell><cell>0.2889</cell><cell>0.12767</cell><cell>0.21638</cell><cell>0.1249</cell><cell>0.3880</cell><cell>0.3093</cell></row><row><cell>80%</cell><cell>0.11771</cell><cell>0.18017</cell><cell>0.0841</cell><cell>0.3375</cell><cell>0.2875</cell><cell>0.13093</cell><cell>0.21069</cell><cell>0.1204</cell><cell>0.3880</cell><cell>0.3080</cell></row><row><cell>90%</cell><cell>0.11472</cell><cell>0.18487</cell><cell>0.0846</cell><cell>0.3458</cell><cell>0.2986</cell><cell>0.12749</cell><cell>0.20564</cell><cell>0.1200</cell><cell>0.3840</cell><cell>0.3093</cell></row><row><cell>100%</cell><cell>0.12388</cell><cell>0.19448</cell><cell>0.0901</cell><cell>0.3875</cell><cell>0.3014</cell><cell>0.12810</cell><cell>0.21309</cell><cell>0.1260</cell><cell>0.3800</cell><cell>0.3133</cell></row><row><cell>Notes:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,103.18,92.40,417.67,398.70"><head>Table III :</head><label>III</label><figDesc>Contributions of relevance signals to search performance</figDesc><table coords="8,103.18,141.28,417.67,349.81"><row><cell>Run name</cell><cell>ERR@20</cell><cell>NDCG@20</cell><cell>MAP</cell><cell>P@5</cell><cell>P@15</cell></row><row><cell></cell><cell cols="3">TREC 2011 (unofficial runs)</cell><cell></cell><cell></cell></row><row><cell>srchvrs11b (bugfixed, non-oficial)</cell><cell>0.13888 +20.2%</cell><cell>0.22879 +29.7%</cell><cell>0.1131 +35.6%</cell><cell>0.4160 +36.8%</cell><cell>0.3093 +32.6%</cell></row><row><cell>BM25 (+close pairs)</cell><cell>0.13427 +16.2%</cell><cell>0.22306 +26.5%</cell><cell>0.1144 +37.2%</cell><cell>0.3480 +14.5%</cell><cell>0.3080 +32%</cell></row><row><cell>BM25 (+prox +close pairs)</cell><cell>0.12984 +12.3%</cell><cell>0.22136 +25.5%</cell><cell>0.1146 +37.4%</cell><cell>0.3480 +14.5%</cell><cell>0.3053 +30.9%</cell></row><row><cell>BM25 (+spamrank)</cell><cell>0.12725 +10.1%</cell><cell>0.20234 +14.7%</cell><cell>0.0933 +12%</cell><cell>0.3280 +7.9%</cell><cell>0.2600 +11.4%</cell></row><row><cell>BM25 (-anchor ext)</cell><cell>0.10423 -9.7%</cell><cell>0.17252 -2.1%</cell><cell>0.0802 -3.7%</cell><cell>0.3080 +1.3%</cell><cell>0.2360 +1.1%</cell></row><row><cell>BM25 (+morph. +stop pairs)</cell><cell>0.12380 +7.1%</cell><cell>0.18242 +3.4%</cell><cell>0.0874 +4.8%</cell><cell>0.3160 +3.9%</cell><cell>0.2413 +3.4%</cell></row><row><cell>BM25 (+prox.)</cell><cell>0.12293 +6.4%</cell><cell>0.19916 +12.9%</cell><cell>0.1018 +22.1%</cell><cell>0.3400 +11.8%</cell><cell>0.2653 +13.7%</cell></row><row><cell>BM25 (-keywords -description)</cell><cell>0.12264 +6.1%</cell><cell>0.19419 +10.1%</cell><cell>0.0936 +12.2%</cell><cell>0.3400 +11.8%</cell><cell>0.2547 +9.1%</cell></row><row><cell>BM25 (+morph.)</cell><cell>0.12198 +5.5%</cell><cell>0.17923 +1.6%</cell><cell>0.0839 +0.7%</cell><cell>0.3160 +3.9%</cell><cell>0.2413 +3.4%</cell></row><row><cell>BM25 (+morph. +stop pairs +wiki)</cell><cell>0.11926 +3.2%</cell><cell>0.18522 +5%</cell><cell>0.0869 +4.3%</cell><cell>0.3160 +3.9%</cell><cell>0.2453 +5.1%</cell></row><row><cell>BM25 (+wiki)</cell><cell>0.11869 +2.7%</cell><cell>0.18378 +4.2%</cell><cell>0.0873 +4.7%</cell><cell>0.3080 +1.3%</cell><cell>0.2440 +4.6%</cell></row><row><cell>BM25 (-title)</cell><cell>0.11249 -2.6%</cell><cell>0.17020 -3.4%</cell><cell>0.0822 -1.3%</cell><cell>0.3040 +0%</cell><cell>0.2213 -5%</cell></row><row><cell>BM25 (+stop pairs)</cell><cell>0.11737 +1.5%</cell><cell>0.17973 +1.9%</cell><cell>0.0868 +4.1%</cell><cell>0.3040 +0%</cell><cell>0.2333 +0%</cell></row><row><cell>BM25 (-url)</cell><cell>0.11671 +1%</cell><cell>0.17588 -0.2%</cell><cell>0.0813 -2.4%</cell><cell>0.3160 +3.9%</cell><cell>0.2373 +1.7%</cell></row><row><cell>BM25 (+pagerank)</cell><cell>0.11568 +0.1%</cell><cell>0.17657 +0.1%</cell><cell>0.0837 +0.4%</cell><cell>0.3040 +0%</cell><cell>0.2347 +0.6</cell></row><row><cell>BM25</cell><cell>0.11559</cell><cell>0.17637</cell><cell>0.0834</cell><cell>0.3040</cell><cell>0.2333</cell></row><row><cell cols="5">TREC 2011 + 450 queries from MQ track (unofficial runs)</cell><cell></cell></row><row><cell>srchvrs11b (bugfixed, non-oficial)</cell><cell>0.09358 +29.2%</cell><cell>0.24282 +29.4%</cell><cell>0.1441 +23.1%</cell><cell>0.2424 +33.4%</cell><cell>0.1750 +24.5%</cell></row><row><cell>BM25 (+close pairs)</cell><cell>0.08148 +12.5%</cell><cell>0.22285 +18.8%</cell><cell>0.1400 +19.6%</cell><cell>0.2189 +20.5%</cell><cell>0.1633 +16.2%</cell></row><row><cell>BM25 (+prox +close pairs)</cell><cell>0.08118 +12.1%</cell><cell>0.22338 +19.1%</cell><cell>0.1402 +19.8%</cell><cell>0.2212 +21.8%</cell><cell>0.1626 +15.6%</cell></row><row><cell>BM25 (-keywords -description)</cell><cell>0.08151 +12.6%</cell><cell>0.20791 +10.8%</cell><cell>0.1242 +6.1%</cell><cell>0.2126 +17%</cell><cell>0.1526 +8.6%</cell></row><row><cell>BM25 (+prox.)</cell><cell>0.07673 +6%</cell><cell>0.20538 +9.5%</cell><cell>0.1302 +11.2%</cell><cell>0.2000 +10.1%</cell><cell>0.1511 +7.5%</cell></row><row><cell>BM25 (-anchor ext)</cell><cell>0.06818 -5.7%</cell><cell>0.17652 -5.8%</cell><cell>0.1061 -9.2%</cell><cell>0.1691 -6.8%</cell><cell>0.1354 -3.6%</cell></row><row><cell>BM25 (+morph. +stop pairs +wiki)</cell><cell>0.07661 +5.8%</cell><cell>0.19964 +6.4%</cell><cell>0.1257 +7.4%</cell><cell>0.1943 +6.9%</cell><cell>0.1479 +5.2%</cell></row><row><cell>BM25 (+morph. +stop pairs)</cell><cell>0.07663 +5.8%</cell><cell>0.19458 +3.7%</cell><cell>0.1234 +5.4%</cell><cell>0.1926 +6%</cell><cell>0.1440 +2.4%</cell></row><row><cell>BM25 (+wiki)</cell><cell>0.07480 +3.3%</cell><cell>0.19322 +3%</cell><cell>0.1203 +2.8%</cell><cell>0.1874 +3.2%</cell><cell>0.1457 +3.7%</cell></row><row><cell>BM25 (+morph.)</cell><cell>0.07498 +3.5%</cell><cell>0.19069 +1.7%</cell><cell>0.1199 +2.5%</cell><cell>0.1908 +5%</cell><cell>0.1435 +2%</cell></row><row><cell>BM25 (-title)</cell><cell>0.07031 -2.8%</cell><cell>0.17957 -4.2%</cell><cell>0.1092 -6.6%</cell><cell>0.1748 -3.7%</cell><cell>0.1337 -4.8%</cell></row><row><cell>BM25 (+stop pairs)</cell><cell>0.07395 +2.1%</cell><cell>0.19134 +2%</cell><cell>0.1204 +2.9%</cell><cell>0.1845 +1.6%</cell><cell>0.1410 +0.3%</cell></row><row><cell>BM25 (+spamrank)</cell><cell>0.07131 -1.4%</cell><cell>0.19165 +2.2%</cell><cell>0.1162 -0.6%</cell><cell>0.1857 +2.2%</cell><cell>0.1419 +1%</cell></row><row><cell>BM25 (+pagerank)</cell><cell>0.07315 +1%</cell><cell>0.18876 +0.6%</cell><cell>0.1172 +0.1%</cell><cell>0.1851 +1.9%</cell><cell>0.1415 +0.7%</cell></row><row><cell>BM25 (-url)</cell><cell>0.07195 -0.5%</cell><cell>0.18510 -1.2%</cell><cell>0.1130 -3.4%</cell><cell>0.1840 +1.3%</cell><cell>0.1372 -2.3%</cell></row><row><cell>BM25</cell><cell>0.07241</cell><cell>0.18759</cell><cell>0.1170</cell><cell>0.1817</cell><cell>0.1406</cell></row></table><note coords="8,147.55,92.40,3.99,13.14"><p>•</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="4,130.15,661.92,227.89,7.30"><p>Essentially we treated these bigrams as optional query terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="4,130.15,671.89,286.25,7.30"><p>http://boston.lti.cs.cmu.edu/clueweb09/wiki/tiki-index.php?page=PageRank</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="6,130.15,642.00,354.47,7.30;6,126.00,651.96,243.54,7.30"><p>This was the algorithm that produced the 2010 run blv79y00shnk, but it used a better value of the parameter controlling the contribution of the proximity score.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="6,130.15,661.92,354.47,7.30;6,126.00,671.89,139.34,7.30"><p>The version that produced the run srchvrs11b erroneously missed a close-pair contribution factor equal to the sum of query term IDFs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_4" coords="12,133.80,651.96,350.82,7.30;12,126.00,661.92,358.59,7.30;12,126.00,671.89,116.92,7.30"><p>It is not clear, however, if these pages were not assessed, because most stop words were not indexed, or because they were never ranked sufficiently high and, therefore, did not contribute to the pool of relevant documents.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Evangelos Kanoulas</rs> for references and the discussion on robustness of performance measures in the case of incomplete relevance judgements.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2009 contest http://imat2009.yandex.ru/en/datasets 4 http://research.microsoft.com/en-us/um/beijing/projects/letor/ 5 http://ir.dcs.gla.ac.uk/test collections/gov2-summary.htm</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="13,127.49,298.43,357.14,7.30;13,135.96,308.40,348.71,7.30;13,135.96,318.36,348.69,7.30;13,135.96,328.32,139.40,7.30" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,408.44,298.43,76.19,7.30;13,135.96,308.40,168.97,7.30">On the robustness of relevance measures with incomplete judgments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bompada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,321.91,308.40,162.76,7.30;13,135.96,318.36,345.40,7.30">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;07</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,339.90,357.15,7.30;13,135.96,349.86,214.43,7.30" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,260.65,339.90,169.08,7.30">Lessons learned from indexing close word pairs</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,445.95,339.90,38.70,7.30;13,135.96,349.86,210.47,7.30">TREC-19: Proceedings of the Nineteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,361.43,357.16,7.30;13,135.96,371.40,348.64,7.30;13,135.96,381.36,110.06,7.30" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,234.62,361.43,250.04,7.30;13,135.96,371.40,128.71,7.30">The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,335.07,371.40,149.53,7.30;13,135.96,381.36,106.15,7.30">Proceedings of the Seventh International World Wide Web Conference</title>
		<meeting>the Seventh International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,392.94,357.12,7.30;13,135.96,402.90,303.89,7.30" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,391.36,392.94,93.25,7.30;13,135.96,402.90,30.53,7.30">Million query track 2009 overview</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavluy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fangz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,184.64,402.90,251.25,7.30">TREC-18: Proceedings of the Eighteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,414.48,357.16,7.30;13,135.96,424.44,199.12,7.30" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,274.09,414.48,162.36,7.30">Yahoo! learning to rank challenge overview</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,458.29,414.48,26.37,7.30;13,135.96,424.44,142.24,7.30">JMLR: Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,436.02,357.16,7.30;13,135.96,445.98,207.57,7.30" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,309.59,436.02,131.70,7.30">Future directions in learning to rank</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,458.29,436.02,26.37,7.30;13,135.96,445.98,142.24,7.30">JMLR: Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,457.55,357.12,7.30;13,135.96,467.52,348.69,7.30;13,135.96,477.48,233.88,7.30" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,378.68,457.55,105.93,7.30;13,135.96,467.52,60.64,7.30">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,216.56,467.52,268.10,7.30;13,135.96,477.48,88.36,7.30">Proceeding of the 18th ACM conference on Information and knowledge management. CIKM &apos;09</title>
		<meeting>eeding of the 18th ACM conference on Information and knowledge management. CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,489.05,357.11,7.30;13,135.96,499.02,260.14,7.30" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,381.40,489.05,103.20,7.30;13,135.96,499.02,170.47,7.30">Efficient and effective spam filtering and re-ranking for large Web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>CoRR abs/1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,510.60,357.12,7.30;13,135.96,520.56,313.10,7.30" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,351.99,510.60,132.62,7.30;13,135.96,520.56,38.69,7.30">Microsoft research at TREC 2010 Web Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,192.87,520.56,252.24,7.30">TREC-19: Proceedings of the Nineteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,532.14,357.10,7.30;13,135.96,542.10,348.69,7.30;13,135.96,552.06,348.67,7.30;13,135.96,562.02,31.95,7.30" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,280.43,532.14,204.16,7.30;13,135.96,542.10,88.16,7.30">Learning in a pairwise term-term proximity framework for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>O'riordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,241.64,542.10,243.01,7.30;13,135.96,552.06,234.86,7.30">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;09</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,573.60,357.15,7.30;13,135.96,583.56,116.19,7.30" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,180.30,573.60,278.91,7.30">2011. I&apos;m Feeling Lucky: The Confessions of Google Employee Number 59</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<pubPlace>None ed. Houghton Mifflin Harcourt</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,595.14,357.16,7.30;13,135.96,605.10,348.73,7.30;13,135.96,615.06,82.14,7.30" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,156.69,605.10,130.58,7.30">Web Track experiments with Ivory</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>UMD and USC/ISI: TREC 2010</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,309.13,605.10,175.56,7.30;13,135.96,615.06,78.18,7.30">TREC-19: Proceedings of the Nineteenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.49,626.64,357.18,7.30;13,135.96,636.60,348.70,7.30;13,135.96,646.56,156.40,7.30" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,425.40,626.64,59.27,7.30;13,135.96,636.60,303.22,7.30">LIBLINEAR: A library for large linear classification (machine learning open source software paper)</title>
		<author>
			<persName coords=""><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,446.39,636.60,38.27,7.30;13,135.96,646.56,103.02,7.30">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,133.80,671.89,211.04,7.30;14,152.15,92.40,3.99,13.14;14,127.49,122.55,357.15,7.30;14,135.96,132.51,225.54,7.30" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,286.89,122.55,197.75,7.30;14,135.96,132.51,57.17,7.30">Modeling term proximity for probabilistic information retrieval models</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://www.storagesearch.com/ssd-fastest.html" />
	</analytic>
	<monogr>
		<title level="j" coord="14,200.93,132.51,78.85,7.30">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="3017" to="3031" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,143.47,357.19,7.30;14,135.96,153.43,348.68,7.30;14,135.96,163.40,155.39,7.30" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,206.88,143.47,188.66,7.30">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,416.47,143.47,68.22,7.30;14,135.96,153.43,348.68,7.30;14,135.96,163.40,9.87,7.30">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. KDD &apos;02</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. KDD &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,174.36,357.18,7.30;14,135.96,184.32,348.68,7.30;14,135.96,194.28,114.23,7.30" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,209.95,174.36,137.57,7.30">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,370.89,174.36,113.79,7.30;14,135.96,184.32,320.31,7.30">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,205.24,357.10,7.30" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,191.28,205.24,151.84,7.30">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,349.63,205.24,91.36,7.30">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,216.20,357.09,7.30;14,135.96,226.16,348.70,7.30;14,135.96,236.13,305.64,7.30" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,275.96,216.20,193.31,7.30">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,135.96,226.16,348.70,7.30;14,135.96,236.13,160.15,7.30">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,247.08,357.09,7.30;14,135.96,257.04,348.68,7.30;14,135.96,267.00,83.50,7.30" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,330.12,247.08,154.46,7.30;14,135.96,257.04,99.37,7.30">Web-search ranking with initialized gradient boosted regression trees</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,388.10,257.04,96.54,7.30;14,135.96,267.00,42.63,7.30">Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,277.97,303.16,7.30" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,200.14,277.97,128.93,7.30">Analyzing tables of statistical tests</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,336.10,277.97,35.53,7.30">Evolution</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="223" to="225" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,288.93,357.13,7.30;14,135.96,298.89,168.52,7.30" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,210.49,288.93,274.14,7.30;14,135.96,298.89,13.04,7.30">Understanding inverse document frequency: On theoretical arguments for IDF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,157.10,298.89,98.08,7.30">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,309.84,357.16,7.30;14,135.96,319.81,348.69,7.30;14,135.96,329.77,348.67,7.30;14,135.96,339.73,31.95,7.30" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,269.31,309.84,215.34,7.30;14,135.96,319.81,51.15,7.30">Information retrieval system evaluation: effort, sensitivity, and reliability</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,207.95,319.81,276.70,7.30;14,135.96,329.77,234.86,7.30">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,350.70,357.09,7.30;14,135.96,360.66,348.75,7.30;14,135.96,370.62,348.67,7.30;14,135.96,380.58,54.50,7.30" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,341.28,350.70,143.30,7.30;14,135.96,360.66,149.26,7.30">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,304.19,360.66,180.52,7.30;14,135.96,370.62,256.12,7.30">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management. CIKM &apos;07</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management. CIKM &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,391.54,357.16,7.30;14,135.96,401.50,348.71,7.30;14,135.96,411.46,298.13,7.30" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,238.21,391.54,230.59,7.30">An exploration of proximity measures in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,135.96,401.50,348.71,7.30;14,135.96,411.46,152.64,7.30">SIGIR &apos;07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,422.43,357.12,7.30;14,135.96,432.39,348.69,7.30;14,135.96,442.35,102.33,7.30" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,327.66,422.43,156.95,7.30;14,135.96,432.39,34.57,7.30">Predicting fame and fortune: PageRank or indegree?</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Upstill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,186.69,432.39,246.45,7.30">Proceedings of the Australasian Document Computing Symposium</title>
		<meeting>the Australasian Document Computing Symposium<address><addrLine>Canberra, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,127.49,453.31,357.12,7.30;14,135.96,463.27,348.70,7.30;14,135.96,473.23,305.64,7.30" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,189.78,453.31,283.17,7.30">How reliable are the results of large-scale information retrieval experiments?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,135.96,463.27,348.70,7.30;14,135.96,473.23,160.15,7.30">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;98</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
