<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,194.75,76.32,222.39,17.10;1,114.38,101.04,383.24,17.10">0 Weeks to TREC: STIRS Sienaʼs Twitter Information Retrieval System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.36,124.02,97.20,10.23"><forename type="first">Sharon</forename><surname>Gower Small</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.44,124.02,51.16,10.23"><forename type="first">Darren</forename><surname>Lim</surname></persName>
							<email>dlim@siena.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,104.99,137.22,47.76,10.23"><forename type="first">Karl</forename><surname>Appel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.77,137.22,52.57,10.23"><forename type="first">Denis</forename><surname>Kalic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.66,137.22,81.53,10.23"><forename type="first">Matthew</forename><surname>Kemmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.39,137.22,61.79,10.23"><forename type="first">David</forename><surname>Purcell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.56,137.22,67.01,10.23"><forename type="first">Carl</forename><surname>Tompkins</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,456.03,137.22,50.97,10.23"><forename type="first">Chan</forename><surname>Tran</surname></persName>
							<email>cs05tran@siena.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The Siena College Institute for Artificial Intelligence Siena College Loudonville</orgName>
								<address>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,194.75,76.32,222.39,17.10;1,114.38,101.04,383.24,17.10">0 Weeks to TREC: STIRS Sienaʼs Twitter Information Retrieval System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">327C86C3678279DC02B16002CF160C8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Algorithms</term>
					<term>Experimentation Twitter</term>
					<term>TREC 2011</term>
					<term>Microblog</term>
					<term>Lucene</term>
					<term>Weka</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been an increasing interest, both of the research community and federal funding agencies in microblogs as a source of viable information for a variety of tasks. NIST (National Institute of Standards and Technology) has added a microblog retrieval track to TREC (Text REtrieval Conference) for the first time in 2011. NIST has selected Twitter as the source of microblog data. Twitter is a dynamic social website that allows users to post tweets which are short posts to share news with friends and followers across the world. While some tweets provide useful information, this information is very limited by the restriction on length to 140 characters or less. Participating teams were provided with the code necessary to download the Twitter Corpus, consisting of 16,141,812 tweets from a 2-week time period, January 24, 2011 to February 8, 2011, inclusive. Teams were also provided with a training set of 12 example topics, and later the test set of 50 topics. In this paper, we describe three modules designed for this track, built within a system called STIRS, Siena's Twitter Information Retrieval System. After submitting three user-defined runs and a Lucene baseline run, the NIST judging showed our best run to be at 30.83% precision. The reported median from all runs of all 58 participating teams was 25.9%. We also describe our process of developing a new and complete end-to-end system in just 10 weeks time with six undergraduate researchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In May 2011, members of the Siena College community formed a group dedicated to working on the TREC Microblog competition. The group was lead by Sharon Small, a long-term researcher in the field of Computational Linguistics as well as a pervious participant in the TREC QA track. The team was comprised of a fellow colleague of hers, Darren Lim, and six undergraduate researchers: Karl Appel, Denis Kalic, Matthew Kemmer, David Purcell, Carl Tompkins and Chan Tran. Starting in the middle of May, the group began work on STIRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Information on TREC Microblog</head><p>The TREC microblog track is a new edition to the TREC tracks for 2011. The corpus for this track is a 16,141,812 tweet collection obtained from a two week period, January 24, 2011 to February 8, 2011, inclusive. Queries on this corpus took on a particular form, where "users" wanted to find up to date and relevant information about a news topic. In addition to a short stated topic of interest, e.g. "State of the Union and social media,"<ref type="foot" coords="1,512.64,508.99,3.48,6.11" target="#foot_0">1</ref> each query also contained a query time. Information returned by the systems could not be older than the moment of the query time or it would be judged as irrelevant. For each of the 50 test topics a system needed to return a time ordered set of tweets; the NIST assessors would judge the first 30 tweets.</p><p>Each participating group was allowed to submit up to four different runs, where each run consisted of a set of ordered tweets for all 50 topics. One of the runs was required to not utilize any outside information, i.e. future tweets, web mined information, etc.</p><p>The corpus and example topics were released May 20, 2011, with all runs from participating teams due to NIST by August 11, 2011. Official results were released in late September 2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The System</head><p>Our team utilized an 8-processor, 64-bit Dell Precision 490 for downloading the corpus, developing STIRS and executing various experiments. Each processor is an Intel Xeon 3.00 GHz CPU, each having a two CPU core. This server has 16 GB of memory and 2.25 TB of hard drive space. It is running Redhat Linux Enterprise Version 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our 10-week Trek to TREC begins</head><p>The timeline of our ten weeks consisted of a kickoff meeting between Dr. Small and the rest of the team two weeks prior to the official start of the student researchers' work period. Soon after Siena's classes ended, the team members commenced work on STIRS. The team met on a weekly basis (sometimes as many as three times in a week) to discuss progress on the system.</p><p>Preliminary Week 1: May 15-21: Conceptual system design of STIRS was completed during our first preliminary week. We envisioned an agile system that incorporated processing modules, which could interact with each other or by themselves, to support a variety of different experiments on the microblog data.</p><p>In order for the group to proceed with the development of STIRS, several organizational issues needed to be addressed. Firstly, we selected Lucene<ref type="foot" coords="2,143.76,526.19,3.00,5.27" target="#foot_1">2</ref> to index our tweets corpus, and we successfully installed and configured Lucene on our server. The undergraduate researchers were then split into teams of two, formed for the purposes of each generating an individual Twitter Module (TM) that would potentially increase our precision. Each module was required to work completely standalone within the STIRS system, or as input and/or output modules to other TMs as applicable.</p><p>On May 16, 2011, the downloadable materials (tweet ids and scripts) of the microblog corpus were made available by the organizers. After readying the scripts on our machine, the download process<ref type="foot" coords="2,398.16,99.23,3.00,5.27" target="#foot_2">3</ref> was started on May 20 th . Our initial download speed of this single process was extremely slow (1 .dat file<ref type="foot" coords="2,439.44,124.67,3.00,5.27" target="#foot_3">4</ref> pair processed every twenty minutes), which would have taken us far past our initial deadlines. We quickly ran some experiments to determine how many parallel processes we could spawn on our server before degradation in performance occurred. These experiments showed that four processes could successfully be run in parallel to speed up our download process without any degradation in performance. We began to modify our code to spawn multiple processes to download the entire corpus without repetition.</p><p>Preliminary Week 2: May 22-28: We established our communication resources, including shared work directories on our server, a Google sites/calendar location and a temporary "course" on our school's Blackboard server. This facilitated our resource organization.</p><p>In the meantime, other parts of STIRS were being developed. Code that would process TREC queries into input queries for Lucene was developed at this time, as well as code that would take Lucene output and generate TREC-format results for submission purposes. Also during this week git<ref type="foot" coords="2,356.64,436.19,3.00,5.27" target="#foot_4">5</ref> , a version control system was installed on our server.</p><p>Week 1: May 29-Jun 4: The 4-process download module was ready and started on May 29 th . The downloading of the corpus was completed on June 2 nd , and Lucene indexing immediately commenced. During this time, the module teams continued to propose and refine ideas for their modules. Additionally, the Query processor and TREC formatter modules were completed. Other parts of our system were also being developed, such as a textese component, which would transform Twitter slang into its full English counterpart. Given the 140 character length restriction per individual tweets, a great deal of textese, i.e.: g2g = got to go, is used by tweeters. The same textese was not used in the example topics and therefore we needed a conversion module.</p><p>Week 2: Jun 5-Jun 11: According to the microblog guidelines only English tweets would be judged as relevant by NIST assessors. Therefore, we developed a simple foreign language filter. This filter reduced our ~16 million tweets corpus to 5,448,156. On week 2, we met our deadline of having a complete indexed Twitter corpus by the first half of June. We then developed a simple script to query the Lucene index directly in a terminal window; this allowed for quicker prototyping of ideas and strategies for modules.</p><p>On this second week, teams also finalized their ideas for their Twitter modules. Section 2, 3, &amp; 4 below will give a detailed description of each of the three TMs. Section 5 of this paper will give a high level overview of the entire STIRS system. Finally, we will report on our team's NIST results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TM 1: Scraping URLs 2.1 TM1 Motivation</head><p>According to the micro-blog guidelines, NIST assessors would be allowed to follow urls within a tweet and utilize the subsequent web page content to judge whether a tweet was relevant or not. Given this, student researcher Team One proposed to use information from the URL links present in a tweet to determine whether a particular tweet was relevant. Throughout the course of this research, we utilized the 5.5 million English tweet corpus. Of these 5.5 million tweets, we automatically detected that approximately 1.3 million contained hyperlinks. Of those hyperlinks, 1.1 million hyperlink pages were able to be downloaded using web scrapers called Jericho(1) and Jsoup(2). After completing the download, we utilized Lucene to index and then search the content from the retrieved hyperlink pages. Each tweet could then be ranked based on how well their hyperlink page scored. This first approach only ranked tweets that actually contained a URL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TM1 Expansion</head><p>After error analysis on the initial results, we attempted two modifications to further improve the precision of our retrieved tweets. The first was to give a penalty to content that did not contain all the query terms or associated synonyms. The second was to penalize web pages with fewer words and to give a bonus to those with a higher word count. These modifications were inspired by results retrieved from the initial run of the query examples on our corpus. For example, the first result for "Australia natural disasters" was for the ten word home page of a natural body building site in Australia, or when searching for the terms "Chavez expropriate property" we got an adequate number of results dealing with people who had the last name of Chavez but nothing associated with Hugo Chavez. From our error analysis of this initial run we determined that in general, content with a higher word count were typically more reliable pages. This first approach only ranked tweets that contained a URL.</p><p>A separate approach to improve results was the consideration of both tweets with URLs and tweets without URLs. To begin the process, two additional Lucene searches were performed on two different indices. The first index was built using the text of the tweets themselves. The second index was built using the text from the web pages linked in the tweets. Each of these searches created a ranked list of tweet results. We attempted to improve results by merging these two lists into a single list by using a process we refer to as a "ranked join." The first step in the merge was to normalize the scores of both lists. All unique tweets were then taken from both lists and put into a single list in no particular order. Each tweet in the new list was then processed and assigned a new score. We experimented with several scoring modifiers:</p><p>• If a tweet contained multiple URLs, its score was increased by the sum of its URL scores.</p><p>• If a tweet occurred in both of the original lists, its score was increased by an intersection bonus value.</p><p>• If a tweet contained no URL its score was decreased by a penalty value.</p><p>Most of the development of this approach was focused on adjusting the scoring modifiers through iterative testing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TM1 Future Work</head><p>We were limited in our time, just 10 short weeks, as well as the lack of a judged corpus. We plan to train our module for improved performance utilizing the newly released TREC judged microblog corpus. This corpus will allow us to train for a variety of weights and thresholds: i.e. ideal penalty scores, determine the best formula for combining the Lucene tweet results and the Lucene web page results, etc. Additionally, as this module utilizes linked URLs, we believe we may be able to further increase precision by utilizing some reliability metrics for the web pages linked to, e.g. news article on cnn.com may be more reliable than those on a blogging site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TM 2: Feature Modeling with WEKA 3.1 TM2 Motivation</head><p>The motivation for this module came about while examining the corpus. Without any example topics, Team Two searched for names of politicians, such as Ron Paul and Hillary Clinton.</p><p>Looking at the results, we noticed a trend that the good Tweets tended to be longer and contain URLs linking to a news article. This lead us to question what other "traits" of tweets could make them relevant, which eventually lead to the idea of machine learning utilizing tweet attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TM2 Method</head><p>The majority of time spent implementing TM2 dealt with the Twitter API to download data about the users in our corpus. Earlier in week 3, we registered STIRS on Twitter in order to be allowed to use the Twitter API. Ultimately, the package used for implementation was Twitter4J, a Java library for the Twitter API.</p><p>Through trial and error and a review of relevant research, several attributes were chosen to be the ones used for feature modeling. They were as follows:</p><p>• Tweet length -the number of characters in the tweet • URL existence -whether a tweet contains a URL or not • Follower Count -the number of followers a user has • Friend Count -the number of other users a particular user follows</p><p>• List Count -the number of Lists<ref type="foot" coords="4,519.36,74.03,3.00,5.27" target="#foot_5">6</ref> on Twitter a user appears on</p><p>The first two attributes were computed for each Tweet in the corpus, while the next three were downloaded from the Twitter API for each user in the corpus. The download consisted of 2,237,031 users who both posted in English and had active accounts at the time of download. The process ran between July 2nd and July 5th, having to abide by Twitter's cap of 350 API requests per hour. The means for accessing the API is a simple registration process to receive authorization codes, which then must be presented when making API requests. To receive this particular information, all that's needed is the desired username, and all of their information which is publicly accessible on Twitter.com may be downloaded.</p><p>It wasn't until the final weeks that we were ready to utilize Weka, an open source machine learning package developed at the University of Waikato, New Zealand. Weka provides many options for aggregating and viewing data, as well as predicting results for new data given a learning set and selection of a learning model. The final stages of implementation revolved around trying different learning models, i.e. Naïve Bayes, Linear Regression, Decision Trees, etc. and automating the entire process.</p><p>The verification process alternated between judging our results as to whether we felt that they were relevant or not, and generating new machine learning models from our test data. 10-fold crossvalidation was used for training and testing purposes. Through our testing process, some attributes, such as expert ratings (calculated by using data from the site Listorius.com), did not prove to be useful. Judgments were made by all team members and were done on a relevant/nonrelevant basis for each graded tweet. Our results proved to be moderately successful, though not as good as we had hoped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TM2 Future Work</head><p>While the initial results showed an improvement over the baseline results, the combinations with other modules consistently produced better results than this module on its own. We feel we were severely limited by the size of our training corpus.</p><p>Given our time restrictions we were only able to generate judgments for our 33 example topics, 100 tweets each. We plan to utilize the much larger TREC judged corpus to train our system and hopefully achieve an improved precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TM 3: Query Expansion 4.1 TM3 Introduction</head><p>Initially, Team Three approached their module design with query expansion in mind. We examined query expansion by traditional successful techniques, i.e. looking for the synonyms of the query words. They used Princeton's WordNet, which allows the user to find synonyms, antonyms, and hyponyms of words. For each query, they used WordNet to expand the query by adding its synonyms. A second approach utilized Wikipedia search capabilities to find relevant articles for each word/phrase within the query. Once our program found a Wikipedia article, it used the first page it found to find the most commonly occurring words within the page. By using this methodology, the most commonly occurring words and phrases (after eliminating stop words) were utilized for query expansion terms.</p><p>When testing TM3, we experimented with four configurations: WordNet only, Wikipedia only, WordNet &amp; Wikipedia, and Wikipedia &amp; WordNet. We combined Wikipedia and WordNet in a linear fashion: we first expanded our query using either Wikipedia or WordNet, we then took that expanded query and sent it to the opposite query expander, expanding further on the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TM3 a New Approach</head><p>After our results for Wikipedia and WordNet, we decided to quickly change our approach to the query expansion problem. At first, we attempted to use Google News<ref type="foot" coords="5,168.72,602.59,3.48,6.11" target="#foot_6">7</ref> for query expansion. We decided to experiment with Google News because the test topics supplied by NIST were primarily related to current news stories of interest. However, we quickly found that due to the limit on the Google news archive, we could not find many stories that were relevant to the query.</p><p>However, for the results that Google News did find, the articles were very relevant to the topic, including exact phrase matching in some cases. This discovery illustrated the power of using Google search results for query expansion. We quickly switched to Google for query expansion and found that, on average, the top four results produced the most pertinent pages. We decided to fetch these pages, unless the URL contained the word "video" or "youtube.com" as we found those pages typically did not offer relevant terms.</p><p>Our module then detected the most common words/phrases in the text. By comparing this list of words between all of the pages, we retained the four most reoccurring words/phrases and added these to the query. We chose to use four words since the fifth word tended to pick up more tweets that were irrelevant when experimenting with the NIST supplied test topics.</p><p>We found our Google module results to be much better than WordNet and Wikipedia results. Five of our queries came out unusable in which the html code of the website was being returned was mistakenly added to the query. This expanded our query with irrelevant information that did not help our results. Even so, the results were not affected heavily by this bad query. Thirty-four of our results were changed and expanded while eleven remained unchanged. The expanded queries showed moderate improvements to the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TM3 Future Work</head><p>From this research, we found that query expansion through Google saw a greater increase in precision than our WordNet and Wikipedia classes. Though there were flaws such as HTML code being returned in some queries, our Google query expansion module worked far better than utilizing the synonyms of WordNet or common words of Wikipedia. We have illustrated that query expansion utilizing Google can improve the precision of our Twitter results. We plan to run additional experiments with the NIST judged microblog corpus to verify and/or improve our module: e.g. is 4 pages the best depth? what is the best number of words/phrases to add to our expanded query, could these thresholds be topical or related to the types of pages being returned, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">STIRS</head><p>We incorporated all three of our twitter modules with other necessary modules, i.e. Query</p><p>Processor, Lucene Processor, TREC formatter etc., into a fully automated end-to-end STIRS system, Figure <ref type="figure" coords="6,141.29,540.53,4.09,9.48" target="#fig_0">1</ref>. Our Query Processor module converted the TREC XML formatted queries into Lucene format. Our Lucene processor module returned a Ranked List of Tweets (RLT) for a given input query. The TREC formatter converted our RLTs into the standard TREC format. STIRS was developed such that any given module could be easily turned on or off to allow for multiple combinations of experiments, i.e. TM3 -&gt; TM1: run the query expansion module followed by the URL ranking module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">End-to-End STIRS Experiments</head><p>Once each team felt they had the best version of their module given the allowed time, full end-toend system experiments began.</p><p>We experimented with all possible combination of our TM modules on the example topics, in order to select the 3 best combinations to send to NIST for evaluation (one run sent would be our baseline run to fulfill the requirement of no outside resources utilized). Judgments were made by all team members and were done on a relevant/non-relevant basis for each tweet. We scored the top 30 tweets for each of the 33 example topics<ref type="foot" coords="6,390.72,640.67,3.00,5.27" target="#foot_7">8</ref> where each tweet was scored by at least two judges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,211.99,438.69,187.87,8.64;6,74.00,86.35,468.00,345.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: STIRS System Architecture Diagram</figDesc><graphic coords="6,74.00,86.35,468.00,345.74" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,323.31,705.00,216.49,7.80;1,324.96,715.08,55.70,7.80"><p>The xml format of the topic file used by NIST may be seen in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,77.33,705.00,216.73,7.80;2,79.20,715.08,98.94,7.80"><p>Apache Lucene™ is an open source high-performance, fullfeatured text search engine.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,324.35,645.24,215.35,7.80;2,324.96,655.56,214.81,7.80;2,324.96,665.64,174.46,7.80"><p>Teams were only supplied with Tweet ids, not the actual text of the tweet. Teams were required to mine Twitter to obtain the actual text of the tweets in the corpus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,323.97,680.28,215.79,7.80;2,324.96,690.36,101.92,7.80"><p>NIST provided 1673 .dat files, each containing a block of tweets ids to be downloaded</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,324.65,705.00,215.19,7.80;2,324.96,715.08,27.21,7.80"><p>git is a free and open source, distributed version control system.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,324.00,705.00,215.82,7.80;4,324.96,715.08,213.44,7.80"><p>Specifically, a List on Twitter is a sub-group of followers that may be further categorized to allow for easier viewing.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,77.25,715.08,86.91,7.80"><p>http://news.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,330.91,694.68,209.05,7.80;6,331.20,705.00,208.87,7.80;6,331.20,715.08,25.42,7.80"><p>We expanded on the 12 example topic set supplied by NIST for a total of 33 topics for testing purposes. See section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8" coords="6,358.93,715.08,11.19,7.80"><p>9.2   </p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our highest performing modules were: 1) TM3 -&gt; TM1; Query expansion followed by our module that utilized urls within the tweets 2) TM1 alone 3) TM3 -&gt; TM1-&gt; TM2; Query expansion followed by the url modules, followed by the Weka module</p><p>We selected these three versions of the system to run on the 50 test topics and return to NIST for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Official NIST Results</head><p>The judging showed our best run to be at 30.83% precision. The reported median from all runs of all 58 participating teams was 25.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">STIRS Conclusions and Future Work</head><p>We were able to build a fully functional STIRS system in just 10 short weeks that performed well above the median reported for the microblog track. We look forward to making significant progress on our system, as described in our module sections above, now that we have the valuable NIST judged microblog corpus. We plan to report improved results in our full TREC proceedings paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Sample Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Experiment One Test Topics</head><p>The first twelve were supplied by the TREC organizers. The rest were developed by Dr. Lim using back issues of the New York Times to find "appropriate" news stories which were similar in stature to previous queries. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,342.00,540.12,186.67,7.80;7,342.00,550.44,186.66,7.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.00,560.76,172.44,7.80;7,342.00,570.84,158.97,7.80" xml:id="b1">
	<monogr>
		<title level="m" coord="7,342.00,560.76,172.44,7.80;7,342.00,570.84,80.75,7.80">The WEKA Data Mining Software: An Update; SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,342.00,585.48,167.24,7.80;7,342.00,595.80,187.19,7.80;7,342.00,605.88,83.97,7.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,435.28,585.48,73.96,7.80;7,342.00,595.80,74.02,7.80">WordNet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,422.02,595.80,107.17,7.80">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
