<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.16,78.45,355.39,12.36;1,159.75,98.37,290.22,12.36">Implicit Feedback and Document Filtering for Retrieval Over Query Sessions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,216.00,136.36,76.20,13.53"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<email>carteret@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.69,136.36,93.04,13.53"><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.16,78.45,355.39,12.36;1,159.75,98.37,290.22,12.36">Implicit Feedback and Document Filtering for Retrieval Over Query Sessions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EA0CF7AA7E9DE4C73F3AC0A9C7A711E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The IR Lab at the University of Delaware participated in the 2011 Sessions track. The Sessions track features sequences of queries q1, ..., qm, with only qm being the subject for automatic retrieval. There are four separate experimental conditions for qm, each with a greater amount of data about user/system interaction for prior queries:</p><p>1. RL1: no interaction information; qm only.</p><p>2. RL2: previous queries q1, ..., qm-1 known to the system.</p><p>3. RL3: previous queries and retrieved results known to the system.</p><p>4. RL4: previous queries, retrieved results, and clicks on retrieved results known to the system.</p><p>We used the different experimental conditions in the track to explore three research questions:</p><p>1. the effect of simple implicit feedback on retrieval results;</p><p>2. the effect of corpus filters on retrieval results;</p><p>3. the effect of duplicate detection and removal on retrieval results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head><p>We used the same Indri index of ClueWeb09 that we built and used for last year's TREC submissions <ref type="bibr" coords="1,230.11,527.64,9.20,5.51" target="#b1">[1]</ref>.</p><p>All of our queries use the Indri query language. When we did not use feedback, we used a simple keyword query, resulting in scoring by a Dirichlet-smoothed language model. When we did use feedback, we used a weighted combination of the original query and weighted expansion terms derived from the feedback documents. An example is: #weight(0.8 #combine(peace corp application) 0.2 #weight(0.055 corps 0.054 peace 0.051 volunteer 0.037 peacecorp 0.035 kennedy 0.031 benefit 0.030 application 0.029 president 0.028 info ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Implicit feedback</head><p>The RL4 condition provided click data for results retrieved prior to the current query. We used this data as implicit feedback about relevance with which to expand the original query:</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">expansion based on clicked documents only;</head><p>2. expansion based on unclicked documents only;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">expansion based on all retrieved documents.</head><p>We expanded in a relatively simple way, calculating tfidf weights for terms in the documents chosen for feedback, then using those tf-idf weights as term weights in an Indri query. The expanded query gave 4 times as much weight to the original query as to the expansion terms; this is based on decent results from previous experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Corpus filtering</head><p>We used three different filters for the ClueWeb09 collection: a spam filter for the full collection based on the University of Waterloo spam scores <ref type="bibr" coords="1,443.88,364.74,9.21,5.51" target="#b2">[2]</ref>, a "category B" filter that limits retrieved results to the first English-language subdirectory, and a Wikipedia filter that limits retrieved results to Wikipedia pages.</p><p>The latter two of these only involved querying a subset of the index, and therefore do not need to be described further.</p><p>We filtered spam as a post-retrieval step: all pages were indexed, but after retrieving documents we removed any that had a Waterloo spam score percentile of 0.75 or lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Duplicate document filtering</head><p>Session track sessions include a sequence of queries q1, ..., qm-1 leading up to the current query qm that is the subject of experimentation. It is possible (and likely) that results retrieved for a query qi will be retrieved again for a later query. Whether a user would want to see such results is an open question (and probably depends a great deal on context). We wanted to see how effectiveness would be affected if duplicates were removed.</p><p>We used two different methods for this depending on the available data:</p><p>• For the RL3 condition, we are given the previous queries as well as retrieved results (by a custom search engine built for the Session track). We filtered from the results for qm any document that appeared in any ranking for the previous queries. • For the RL2 condition, we are only given the previous queries with no retrieved results. We submitted these queries to our own Indri system, and then filtered from the results for qm any document that appeared in the top 10 ranked results for the previous queries.</p><p>We expect the latter to result in more documents being filtered than the former. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We submitted three runs consisting of four input files each (for each of the four experimental conditions RL1, RL2, RL3, and RL4). Details are given in Table <ref type="table" coords="2,228.38,319.92,3.58,5.51" target="#tab_1">1</ref>.</p><p>We use these runs to test the experiments listed above as follows:</p><p>• experiment 1: compare implicit feedback with clicked documents and implicit feedback with unclicked documents to baselines of no feedback and feedback with all documents.</p><p>The runs for this experiment are udelASFe1new.RL1, udelASFe1new.RL4, udelBe2.RL4, and udelWPmnz.RL4.</p><p>• experiment 2: compare three "corpus filters" on the full ClueWeb09: a spam filter on the full Englishlanguage collection, a "category B" filter limiting retrieved results to only the first subdirectory of Englishlanguage pages, and a Wikipedia filter further limiting retrieved results to only the Wikipedia crawl included in the collection.</p><p>The runs for this experiment are udelASFe1new.RL1, udelBe2.RL1, and udelWPmnz.RL1.</p><p>• experiment 3: compare two different duplicate filtering methods: one that filters duplicate results that would have been retrieved by our own system for the previous queries in the session, and one that filters duplicate results retrieved by the Session track system.</p><p>The runs for this experiment are udelASFe1new.RL1, udelASFe1new.RL2, udelASFe1new.RL3; udelBe2.RL1, udelBe2.RL2, udelBe2.RL3; and udelWPmnz.RL1, udel-WPmnz.RL2, and udelWPmnz.RL3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Implicit feedback</head><p>Results for implicit feedback experiments are shown in Table 2. Interestingly, every feedback approach gave a big gain over the RL1 baseline for every measure we looked at. The obvious question is whether this gain is because we were expanding the query using results from a "good" search engine (the one that retrieved the documents in the RL3 condition); to test this, we should compare to feedback with documents retrieved by our Indri system. We have yet to do this.</p><p>There is no clear difference between the three feedback methods, though. Only small differences can be observed. It is interesting that the rank ordering of the results varies depending on evaluation measure: for nDCG@10, using clicked documents hurts performance, while for graded AP it provides a slight gain.</p><p>Our conclusion here is that feedback based on previous ranked results works well (though there may be a confounding effect), but which documents are used for feedback don't matter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Corpus and duplicate filtering</head><p>It is immediately clear from looking at the first column of Table <ref type="table" coords="2,342.22,248.63,4.61,5.51">3</ref> that retrieving only Wikipedia documents provides the best overall effectiveness. This is likely because Session track topics, which were informational in nature, are wellmatched to Wikipedia.</p><p>The effectiveness of retrieving from category A with spam filtering is still fairly good, and could likely be improved by giving more weight to Wikipedia pages. Retrieval in category B only is clearly the worst, though still fairly good considering it is a small subset of the entire collection.</p><p>Results from applying duplicate filtering depend heavily on both corpus filter and duplicate filtering method. First, regardless of corpus filter, using the filter based on RL3 data gives very little change in effectiveness. The simplest explanation for this is that few documents are actually being filtered; this would be the case if the RL3 results and our Indri system tend to retrieve very different results. Indeed, the mean overlap between the two systems (defined as the size of the intersection of their retrieved results divided by the size of the union) is only 0.03 even in the most direct comparison (spam-filtered category A); they retrieved almost no documents in common.</p><p>The Indri-based filter for the RL2 condition, on the other hand, sometimes makes a big difference and sometimes very little difference at all. In the Wikipedia case, filtering against Indri results in a very large decrease in effectiveness. This is likely because there is a fairly small subset of Wikipedia pages that are relevant and being retrieved for query after query; filtering them has a strong negative effect on effectiveness. This does raise the question: would a user really want to see the same Wikipedia page more than once in a session?</p><p>In the category B case, the filter has a small but positive effect. This may be because effectiveness in category B is relatively low, so nonrelevant documents tend to get filtered more than relevant documents.</p><p>In category A, the filter has a small negative effect. Contra the category B case, this may be because effectiveness in category B is higher, so more relevant documents are being filtered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We used the Session track to explore three questions regarding implicit feedback and filtering. ence in effectiveness between feedback with our Indri system and feedback with the custom system used for the Session track. Since these two systems have very little overlap in retrieved results, it is possible that all gain in Table <ref type="table" coords="3,262.36,273.58,4.61,5.51">2</ref> is due to the former system being of high quality. This would call into question the validity of the test collection formed for the Session track. The Session track also provided subtopic judgments that we have not had a chance to investigate yet. It is likely that we could use these for diversity across a session, however, and this is definitely a direction we intend to pursue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.81,693.13,239.11,26.43"><head>Table 1 :</head><label>1</label><figDesc>Results at this point are somewhat inconclusive. But it seems very clear that we must investigate feedback further, in particular the differ-Description of submitted runs. Note that udelBe2.RL4 and udelWPmnz.RL4 are part of an experiment involving the spam-filtered category A index, not the category B or Wikipedia indexes that their names suggest.</figDesc><table coords="3,73.34,57.44,463.05,119.06"><row><cell cols="3">run name condition description</cell></row><row><cell>udelASFe1new</cell><cell>RL1</cell><cell>ad hoc retrieval on spam-filtered category A (ASF) index</cell></row><row><cell></cell><cell>RL2</cell><cell>RL1 + removal of duplicates as identified by indri system</cell></row><row><cell></cell><cell>RL3</cell><cell>RL1 + removal of duplicates as identified by track data</cell></row><row><cell></cell><cell>RL4</cell><cell>implicit feedback by expanding query with clicked documents from previous interaction</cell></row><row><cell>udelBe2</cell><cell>RL1</cell><cell>ad hoc retrieval on category B index</cell></row><row><cell></cell><cell>RL2</cell><cell>RL1 + removal of duplicates as identified by indri system</cell></row><row><cell></cell><cell>RL3</cell><cell>RL1 + removal of duplicates as identified by track data</cell></row><row><cell></cell><cell>RL4</cell><cell>implicit feedback for ASF by expanding query with unclicked documents from previous interaction</cell></row><row><cell>udelWPmnz</cell><cell>RL1</cell><cell>ad hoc retrieval on Wikipedia (enwp*) index</cell></row><row><cell></cell><cell>RL2</cell><cell>RL1 + removal of duplicates as identified by indri system</cell></row><row><cell></cell><cell>RL3</cell><cell>RL1 + removal of duplicates as identified by track data</cell></row><row><cell></cell><cell>RL4</cell><cell>implicit feedback for ASF by expanding query with all documents from previous interaction</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,58.28,365.68,96.81,12.37" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,67.99,383.86,191.77,5.51;3,67.99,391.61,224.91,8.21;3,67.99,404.78,20.95,5.51" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,224.84,383.86,34.92,5.51;3,67.99,394.32,118.60,5.51">Sessions, diversity, and ad hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,204.49,391.61,82.67,8.21">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,67.99,416.24,212.08,5.51;3,67.99,426.70,218.47,5.51;3,67.99,434.45,165.26,8.21;3,67.99,447.62,83.88,5.51" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="3,123.64,426.70,162.83,5.51;3,67.99,437.16,131.01,5.51">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>CoRR, abs/1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
