<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,190.32,112.91,231.36,15.12;1,193.39,134.83,225.22,15.12">A Hierarchical Bayesian Model of Crowdsourced Relevance Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-10-24">October 24, 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.04,167.31,75.92,10.48"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
							<email>carp@lingpipe.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics LingPipe</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,190.32,112.91,231.36,15.12;1,193.39,134.83,225.22,15.12">A Hierarchical Bayesian Model of Crowdsourced Relevance Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-10-24">October 24, 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">F76A3DB34167638B439A88037D5F701C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We apply a generative probabilistic model of noisy crowdsourced coding to overlapping relevance judgments for documents in several topics (queries). We demonstrate the model's utility for Task 2 of the 2011 TREC Crowdsourcing Track (Karzai and Lease 2011).</p><p>Our model extends Dawid and Skene's (1979) approach to inferring gold standards from noisy coding in several ways: we add a hierarchical model of prevalence of relevant documents in multiple topics (queries), semi-supervision using known gold labels, and hierarchically modeled priors for coder sensitivity and specificity. We also replace Dawid and Skene's maximum likelihood point estimates with full Bayesian inference using Gibbs sampling and generalize their full-panel design in which every coder labels every document to a fully ad hoc design in which a coder may label each document zero, one or more times.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two subtasks making up Task 2 of the TREC 2011 Crowdsourcing Track.</p><p>The first subtask is to provide categorical relevance judgments for each topic/document pair (0 for irrelevant and 1 for relevant). These judgments may be fractions between 0 and 1.</p><p>Evaluation for the first task is with the traditional IR measures of precision (TP/[TP+FP]) and recall/sensitivity (TP/ <ref type="bibr" coords="2,186.38,140.37,35.22,7.86">[TP+FN]</ref>). Because precision and recall ignore true negatives, we've been lobbying to also have specificity (TN/[TN+FP]) evaluated.</p><p>Given these measures, we are skeptical about the utility of fractional judgments (see section 9.3).</p><p>The second subtask is to rank the documents by relevance for each topic. These will be scored by standard TREC ranking evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of Our Entries</head><p>We have entered three systems based on a semi-supervised model and an unsupervised model. For two of these entries, we quantize results to 0 or 1. For one entry, we use a Bayesian estimator of relevance probability. Specifically, we're minimizing expected squared estimation error, which amounts to using posterior averages for estimates. This is not a method that's tuned to the evaluation.</p><p>For all entries, we rank based on our Bayesian estimates of relevance probability.</p><p>4 The Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Constants</head><p>Data sizes are given by the following unmodeled constants.</p><p>• J &gt; 0: number of coders • T &gt; 0: number of topics (i.e., queries)</p><p>• I &gt; 0: number of document/topic pairs • K &gt; 0: number of judgments (i.e., labels)</p><p>In a complete panel design, every coder would judge each document/topic pair exactly once. Because of the mixed design of the TREC data, it is convenient to use the following constant indexing arrays.</p><p>• tt[i] ∈ 1:T : topic for document/topic pair i ∈ 1:I</p><p>• jj[k] ∈ 1:J: worker for judgment k ∈ 1:K</p><p>• ii[k] ∈ 1:I: document/topic pair for judgment k ∈ 1:K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Variables</head><p>The lowest-level random variables in the model are discrete.</p><p>• z[i] ∈ {0, 1}: relevance of document/topic pair i ∈ 1:I</p><formula xml:id="formula_0" coords="2,109.24,538.64,395.92,7.86">• y[k] ∈ {0, 1}: label provided by worker jj[k] for document/topic pair ii[k] for judgment k ∈ 1:K</formula><p>The labels y are fully observed. In the unsupervised case the true relevances zi are unknown. In the semi-supervised case, values of zi for are known for for some document/topic pairs i.</p><p>The labels and relevances are characterized by the following continuous parameters.</p><formula xml:id="formula_1" coords="2,109.24,588.77,257.20,34.40">• π[t] ∈ [0, 1]: prevalence of relevant documents in topic t ∈ 1:T • θ0[j] ∈ [0, 1]: specificity of worker j ∈ 1:J • θ1[j] ∈ [0, 1]: sensitivity of worker j ∈ 1:J</formula><p>The continuous parameters have priors, which characterize their distributions.</p><p>• φπ, φ0, φ1 ∈ (0, 1): prior mean for prevalence, specificity, and sensitivity • κπ, κ0, κ1 ∈ (0, ∞): prior count size for prevalence, specificity, and sensitivity</p><p>In our Bayesian hierarchical model, we also treat these as variable and provide one additional level of hard coded priors for them. The sensitivity and specificity parameters characterize the population of coders in terms of average accuracy, average bias, and the variation among coder accuracies and biases.</p><p>For the prevalence parameters, these priors characterize the mean and variation in the percentage of relevant documents across topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Probability Model</head><p>The full probability model defines the joint probability density over all of the variables. We define the joint probability using sampling notation to represent a directed graphical model. Working top down, the top-level prior means are sampled from a uniform Beta(1, 1) density.</p><p>• φπ, φ0, φ1 ∼ Beta(1, 1)</p><p>The precision parameters are sampled from a weakly informative Pareto (inverse polynomial) distribution slightly favoring lower counts.</p><p>• κπ, κ0, κ1 ∼ Pareto(3/2)</p><p>The mid-level population parameters for prevalence are sampled based on their prior parameters κπ and φπ, which characterize the prior mean and (inverse) variance respectively (we convert back to the standard Beta-distribution parameterization in terms of prior success and failure counts).</p><formula xml:id="formula_2" coords="3,109.24,222.87,157.58,7.96">• π[t] ∼ Beta(κπ × φπ, κπ × (1 -φpi))</formula><p>Sensitivity and specificty for annotators are sampled the same way from their own priors.</p><formula xml:id="formula_3" coords="3,109.24,252.75,155.84,22.91">• θ0[j] ∼ Beta(κ0 × φ0, κ0 × (1 -φ0)) • θ1[j] ∼ Beta(κ1 × φ1, κ1 × (1 -φ1))</formula><p>The lowest-level discrete parameters for relevance are generated according to prevalence for their topic.</p><formula xml:id="formula_4" coords="3,109.24,308.54,85.79,7.96">• z[i] ∼ Bern(π[tt[i]])</formula><p>The most complex sampling formula is for the labels provided by the coders.</p><formula xml:id="formula_5" coords="3,109.24,338.43,265.46,7.96">• y[k] ∼ Bern(z[ii[k]] × θ1[jj[k]] + (1 -z[ii[k]]) × (1 -θ0[jj[k]]))</formula><p>In this formula, ii[k] is the document/topic pair being coded and jj[k] is the coder. Thus θ1[jj <ref type="bibr" coords="3,490.83,353.47,10.78,7.86">[k]</ref>] is the sensitivity of the coder (i.e., the coder's accuracy on relevant document/topic pairs), and θ0[jj <ref type="bibr" coords="3,500.73,364.43,10.78,7.86">[k]</ref>] the specificity (i.e., accuracy on irrelevant pairs). The value of z[ii <ref type="bibr" coords="3,376.17,375.39,9.86,7.86">[k]</ref>] is the binary relevance of the document/topic pair being coded. If the relevance z[ii <ref type="bibr" coords="3,322.74,386.35,9.86,7.86">[k]</ref>] is 1 (relevant), the label is generated from the coder's sensitivity θ1 <ref type="bibr" coords="3,194.44,397.31,20.93,7.86">[jj[k]</ref>]; if it is 0 (irrelevant), the label is generated from the 1 minus the coder's specificity θ0 <ref type="bibr" coords="3,146.89,408.27,21.55,7.86">[jj[k]</ref>] (the inversion is because 0 is the correct answer for an irrelevant pair).</p><p>It's now straightforward to read the entire joint probability density from the sampling notation by converting indices to products.</p><p>p <ref type="bibr" coords="3,166.69,449.61,139.65,7.86">(φπ, φ0, φ1, κπ, κ0, κ1, π, θ0, θ1, y, z)</ref> = Beta(φπ|1, 1) × Beta(φ0|1, 1) × Beta(φ1|1, 1) × Pareto(κπ|1.5) × Pareto(κ0|1.5) × Pareto(κ1|1.5)</p><formula xml:id="formula_6" coords="3,196.96,503.86,252.62,90.31">× T t=1 Beta(π[t]|φπ, κπ) × I i=1 Bern(z[i]|π[tt[i]]) × K k=1 Bern(z[ii[k]] × θ1[jj[k]] + (1 -z[ii[k]]) × (1 -θ0[jj[k]]))</formula><p>The inference problem presented by the TREC 2011 challenge is to estimate the conditional probability of true labels given the observed labels from the coders, namely p(z|y). In the semi-supervised case, we take z = z , z , with z being unknown and z being known. So the semi-supervised case, we infer p(z |y, z ) and for the fully unsupervised case, we infer p(z , z |y).</p><p>Given that we are also interested in the other parameters, we will draw N samples from the full posterior, here shown for the semi-supervised case.</p><p>p <ref type="bibr" coords="3,230.91,677.80,154.44,7.86">(φπ, φ0, φ1, κπ, κ0, κ1, π, θ0, θ1, z |y, z )</ref> In this formulation, it is clear that the only data observed are the labels y and in the semi-supervised case, the subset z of true labels.</p><p>For inference, we draw from the posterior a sequence of samples,</p><formula xml:id="formula_7" coords="4,197.71,93.59,216.08,12.10">φ (m) π , φ (m) 0 , φ (m) 1 , κ (m) π , κ (m) 0 , κ (m) 1 , π (m) , θ (m) 0 , θ (m) 1 , z (m)</formula><p>for m ∈ 1:M . This supports full Bayesian inference, as we describe in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Bayesian Inference</head><p>Let γ be the full set of parameters and y be the data, so that p(γ|y) is the marginal posterior of parameters γ given observed data y. We are usually interested in expectations of functions f (γ) of the parameters. Given samples γ (1) , γ (2) , . . . drawn from the posterior p(γ|y), we are able to approximate posterior expectations using simple posterior averages,</p><formula xml:id="formula_8" coords="4,216.52,218.04,156.80,26.81">E[f (γ)|y] = f (γ) p(γ|y) dγ ≈ 1 M M m=1</formula><p>γ (m) .</p><p>TREC requires point estimates ẑi of relevance for every document/topic pair i. The posterior mean E[zi|y] (with y fixed and henceforth ellided) is a convenient estimator with two pleasant properties. First, it is unbiased, so the expected error is zero,</p><formula xml:id="formula_9" coords="4,96.91,276.67,418.18,19.32">E[zi -ẑi] = 0. Second, it minimizes expected squared error, E[(zi -ẑi) 2 ].</formula><p>Conveniently, the posterior mean can be estimated by averaging over the posterior samples</p><formula xml:id="formula_10" coords="4,96.91,297.39,259.95,39.85">z (m) i , ẑi = E[zi] ≈ 1 M M m=1 z (m) i .</formula><p>In some submissions, we reduced these to binary estimates by setting the estimate to 1 if ẑi &gt; 0.5 and to 0 otherwise. If TREC were using log loss (see section 9.1), it would make sense to bound values away from 0 or 1, which arise due to the limited precision of sampling (number of significant digits grows proportionally to the square root of the number of samples).</p><p>We use these estimates ẑi for ranking documents within topics purely out of convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation</head><p>We use Gibbs sampling with three parallel chains with diffuse starting values for continuous values and categorical assignments determined by voting (with coin flips for tied votes). We took 2000 samples in each of three chains and discarded the first 1000. The second 1000 samples per chain mixed well, with potential scale reduction statistics statistics R all being near 1.</p><p>Given that we have a directed graphical model with standard parametric sampling distributions, we are able to use JAGS 2.2.0 (Plummer 2010) for sampling. 64-bit JAGS is not very efficient, requiring 2.5 GB of memory to process the data, and requiring a few hours to gather the samples in both the unsupervised and semi-supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Posterior Fit</head><p>In this section, we provide several plots derived from posterior samples. Figure <ref type="figure" coords="4,422.35,565.28,4.61,7.86" target="#fig_0">1</ref> shows the monotonic relationship between voted relevance and estimated relevance in the semi-supervised model (the ends are not extrapolated properly due to the nearest-neighbor trend curve fitting).</p><p>Figure <ref type="figure" coords="4,139.48,598.15,4.61,7.86">2</ref> shows the broad range of sensitivities and specificities in evidence. Specificity is particularly variable. Of particular interest is the number of coders with performance no better than chance or slightly worse than chance (i.e., adversarial). On pleasant feature of our model is that it automatically discounts coders with chance performance, as their responses are independent of the true category and thus provide no information (see section 8).</p><p>Figure <ref type="figure" coords="4,139.87,652.95,4.61,7.86">3</ref> breaks out 16 coders who provided different number of labels. We see that the coders with very many labels have chance performance, with the two highest providing all zero (irrelevant) and all one (relevant) responses respectively. This negative correlation between label quantity and quality is common with Amazon's Mechanical Turk; for instance, in the data collected by Snow, O'Connor, <ref type="bibr" coords="4,449.47,685.82,65.63,7.86;4,96.91,696.78,24.14,7.86">Jurafsky and Ng (2008)</ref>, nearly half the annotations were by spam annotators who consistently annotated large numbers of examples. Sensitivity vs. Specificity (Estimated) specificity sensitivity 0.2 0.4 0.6 0.8 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.2 0.4 0.6 0.8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discounting Spam Coders</head><p>A nice feature of the Dawid and Skene model is that a coder that returns random responses will have no effect on posterior inferences for relevance. First not that a coder returns random responses if their response does not depend on the category. This happens when sensitivity is one minus specificity, or in symbols, θ1,j = 1 -θ0,j.</p><p>With such a coder, there is no effect on posteriors. First note that the contribution to the posterior of coder j for document/topic pair i given label y k is</p><formula xml:id="formula_11" coords="6,250.22,525.09,112.76,20.24">p(y k |zi = 1) p(y k |zi = 0) = θj,1 1 -θj,0 = 1.</formula><p>9 Probabilistic Scoring</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Log Loss Scoring</head><p>The negation of log probability is known as "log loss" or (sample) cross-entropy. It provides a natural approach to probabilistic scoring because it is the objective function maximized on the training data by maximum likelihood estimates and combines with the prior for maximum a posterior (MAP) estimation. Suppose the true answers are given in the vector y, with yn ∈ {0, 1} and the system repsonses are continuous values ŷn ∈ [0, 1]. Then log probability of the truth y estimated by system responses ŷ is</p><formula xml:id="formula_12" coords="6,234.16,665.94,143.68,26.81">L(y, ŷ) = N n=1 log(yn ? ŷn : (1 -ŷn)),</formula><p>where (y ? x : z) is the ternary operator that evaluates to x if y = 1 and z if y = 0. This is just the log probability assigned to the true labels y by the model estimates ŷ. coder specificity and sensitivity and prevalence of relevant documents. The horizontal axis is the sample for φ and the vertical for κ. The precision of the specificity prior is much lower than that of the sensitivity prior and prevalence prior. The prevalence is so high because the corpus consists entirely of documents judged relevant by information retrieval systems.</p><p>Of course, log probability scoring only makes sense for systems whose responses are interpretable as probabilty estimates of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Square Error Scoring</head><p>Another popular method for scoring is based on squared error, which is just the second moment of the residual. If expected error is zero, this is Squared error is just the sum of the squared residuals, that is, the sum of the squared differences between the true answer and system response,</p><formula xml:id="formula_13" coords="7,265.03,564.60,81.94,26.81">SE = N n=1 (yn -ŷn) 2 .</formula><p>It is often presented in terms of a sample mean, where mean square error is defined by</p><formula xml:id="formula_14" coords="7,253.91,613.92,104.18,26.81">MSE = 1 N N n=1 (yn -ŷn) 2 .</formula><p>To put the error back on the same scale as the data, the square root is often used, leading to root mean square error,</p><formula xml:id="formula_15" coords="7,245.65,671.90,120.69,26.81">RMSE = 1 N N n=1 (yn -ŷn) 2 .</formula><p>Given our use of Bayesian estimates minimizing expected squared loss, our approach should do well under this evaluation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Confusion Matrix Scoring</head><p>The obvious way to extend the standard confusion matrix measures of precision, recall/sensitivity, and specificity is to just count fractional responses as partly one response and partly another response. For instance, if the system sets ŷn = 0.72, we treat that as if it was 0.72 responses of 1 and 0.28 responses of 0.</p><p>The problem with this approach is that it double-counts mistakes. Consider an example topic/document pair with a 0.72 probability of being relevant. Now consider the scoring outcomes for a system that returns 0.72. There's a 72% chance the document/topic pair is relevant, yielding 0.72 TPs and 0.28 FNs, and a 28% chance its irrelevant, yielding 0.28 TNs and 0.72 FPs. So the total expectation is for 0.5184 TPs, 0.2016 FPs, 0.2016 FNs, and 0.0784 TNs.</p><p>Contrast this with the scoring outcomes for a system that returns 1, which yields a total expectation of 0.72 TPs and 0.28 FPs.</p><p>We ran a simulation in R that took uniform samples in [0,1] for probability estimate, then compared returning the sampled value or rounding it to the closer of 0 or 1. We then randomly generated the true label according to the estimate. Expected precision and recall were 0.75 in the quantized case and 0.66 in the probabilistic return case.</p><p>Even so, we're going to submit unquantized results and hope the scoring is reasonable or that the organizers also provide scores for quantized labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Previous Work</head><p>Models very much like ours were applied by <ref type="bibr" coords="8,285.13,320.07,101.57,7.86">Dawid and Skene (1979)</ref> to the problem of pooling the clinical diagnoses of doctors and medical tests. Our model generalizes Dawid and Skene's model as well as their inference procedure. Other than the hierarchical model of topic relevance, these generalizations were all introduced in (Carpenter 2008).</p><p>The first extension is to allow prevalence π[t] to vary by topic t. Dawid and Skene only considered the case of a single topic (though they were looking at medical diagnoses, not relevance judgments).</p><p>The second extension is to allow an incomplete survey design. Specifically, not all topic/document pairs need to be labeled by each worker. Also, a worker may label a single topic/document pair more than once.</p><p>The third extension adds general priors for the binomial parameters for topic prevalence of relevant documents π[t] and worker specificity θ0[j] and sensitivity θ1[j]. Dawid and Skene did not provide a full Bayesian model, instead taking maximum likelihood estimates (MLE), which is equivalent to maximum a posteriori (MAP) estimation in a full model with uniform priors.</p><p>On the inference side, we provide Bayesian estimates based on posterior averages rather than the maximum likelihood estimates of Dawid and Skene. In simple hierarchical models, MLE and MAP estimates are prone to degenerate, with likelihoods tending to infinity as prior variance parameters tend to zero. <ref type="bibr" coords="8,110.73,506.38,76.16,7.86">Snow et al. (2008)</ref> evaluated a fully supervised MAP estimate of Dawid and Skene's model. Wei and Lease (2010) evaluated MLE estimates of the Dawid and Skene model with some supervision. In general, any directed graphical model may be fit using EM or Gibbs sampling with no supervision, some supervision or full supervision; it's just a matter of which parameters are known. This fact has been widely exploited in the epidemiology models where all manner of partial supervision has been explored (particularly, positive-only follow-up testing).</p><p>There have been several publications in the past two years that have reinvented similar models, some with small representational twists. The only innovation of which we are aware beyond that reported in the epidemiology literature is that of <ref type="bibr" coords="8,252.38,594.05,81.23,7.86">Raykar et al. (2010)</ref>, who combined an estimated classifier with human coders. Also worth noting is <ref type="bibr" coords="8,194.06,615.96,87.06,7.86">Whitehill et al. (2009)</ref>, estimated annotation difficulty per item in a binary task. This approach is common in the epidemiology literature (e.g., Uebersax and Grove 1993). Carpenter (2008) found the posteriors for item difficulty very uncertain given up to 10 labels per item, the problem being too many degrees of freedom in the model.</p><p>A trend in the epidemiology literature (e.g, Qu, Tan and Kutner 1996; Tu, Kowalski and Jia 1999) that has so far not surfaced in the machine learning, search and natural language processing literature as far as we are aware is to use predictors based on the items being classified (e.g., their source, length, language, etc.) or the annotators (e.g., their training, native language, etc.).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,126.00,306.21,360.00,8.74;5,126.00,317.85,360.00,7.86;5,126.00,328.81,360.00,7.86;5,126.00,339.77,360.00,7.86;5,126.00,350.73,360.00,7.86;5,126.00,361.69,360.00,7.86;5,126.00,372.64,360.00,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Voted versus Estimated Relevance: Each point represents a document/topic pair. Position on the x axis represents the estimate of relevance through equally-weighted voting. Position on the y axis represents the estimated relevance, which adjusts votes based on estimated coder sensitivity and specificity and for the proportion of relevant documents in the topic. While the trend is monotonic (other than for edge effects of the estimator), it is highly non-linear, with estimated values being more extreme, representing higher model-based confidence in the estimates after adjusting for coder accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,126.00,634.20,360.00,8.74;5,126.00,645.83,360.00,7.86;5,126.00,656.79,360.00,7.86;5,126.00,667.75,360.00,7.86;5,126.00,678.71,360.00,7.86;5,126.00,689.67,360.00,7.86;5,126.00,700.63,75.03,7.86"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Coder Sensivity vs. Specificity: Each point represents a single coder.The position on the x axis is specificity (i.e., accuracy on irrelevant documents, TN/[TN+FP]). The position on the y axis is sensitivity (i.e., accuracy on relevant documents, TP/[TP+FN]). The diagonal red line is chance performance, for which sensitivity = 1 -specificity, θ1[j] = 1 -θ0[j]. Below the diagonal represents adversarial performance, though the estimates shown here below the line are likely due to sampling error rather than adversarial coding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,126.00,371.82,360.00,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Posteriors for Hyperpriors: 200 Posterior samples of the hyperpriors for</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We've provided Java code for munging the basic data and R and JAGS code for marshaling data, sampling, estimation and reporting. The complete set of code may be checked out of the LingPipe sandbox using the following anonymous subversion checkout command:</p><p>If that link goes away, search for the LingPipe sandbox and use the project name trec2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>• • Wei, Tang and Matthew Lease. 2011. Semi-supervised consensus labeling for crowdsourcing. In ACM SIGIR Workshop on Crowdsourcing for Information Retrieval.</p><p>• Whitehill, Jacob, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Optimal integration of labels from labelers of unknown expertise. NIPS Poster.</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
