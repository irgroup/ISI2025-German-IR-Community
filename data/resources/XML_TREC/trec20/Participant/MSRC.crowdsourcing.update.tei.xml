<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.11,83.99,305.49,16.49">MSRC at TREC 2011 Crowdsourcing Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.95,114.34,65.97,14.27"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.74,114.34,58.10,14.27"><forename type="first">Ece</forename><surname>Kamar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.67,114.34,81.37,14.27"><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.11,83.99,305.49,16.49">MSRC at TREC 2011 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D149813568454FC140EF7ED8DC1C17AA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crowdsourcing useful data, such as reliable relevance labels for pairs of topics and documents, requires a multidisciplinary approach that spans aspects such as user interface and interaction design, incentives, crowd engagement and management, and spam detection and filtering. Research has shown that the design of a crowdsourcing task can significantly impact the quality of the obtained data, where the geographic location of crowd workers was found to be a main indicator of quality. Following this, for the Assessment task of the TREC crowdsourcing track, we designed HITs to minimize attracting spam workers, and restricted participation to workers in the US. As an incentive, we included the possibility of a bonus pay of $5 for the best performing workers. When crowdsourcing relevance judgments, multiple judgments are typically obtained to provide greater certainty as to the true label. However, combining these judgments by a simple majority vote not only has the flawed underlying assumption that each assessor has comparable accuracy but also ignores the impact of topic specific effects (e.g. the amount of topic-expertise needed to accurately judge). We provide a simple probabilistic framework for predicting true relevance from crowdsourced judgments and explore variations that condition on worker and topic. In particular, we focus on the topic conditional model that was our primary submission for the Consensus task of the track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Crowdsourcing <ref type="bibr" coords="1,120.24,566.59,11.62,9.54" target="#b6">[7]</ref> as an online practice is increasingly adopted across a broad range of application areas, from advertising and data gathering to crisis response, design innovation, and problem solving. The term crowdsourcing, as coined by Jeff Howe, describes the act of outsourcing work to a large group of people or a community-a crowd <ref type="bibr" coords="1,267.62,626.36,10.58,9.54" target="#b6">[7]</ref>. It is an open call for contributions from members of the crowd in order to solve a problem or complete a task, often in exchange for micro-payments, social recognition, or entertainment.</p><p>A specific area where crowdsourcing can provide the required scale and efficiency is the comparative evaluation of search engines <ref type="bibr" coords="1,116.54,710.57,46.93,9.54">[1-3, 6, 10]</ref>. Indeed, the crowdsourcing of relevance judgments is receiving growing attention as a possible solution to enable search engine evaluation at a very large scale. However, crowdsourcing, and more specifically crowdsourcing when monetary incentives are involved, is a solution with its own set of challenges <ref type="bibr" coords="1,485.71,244.51,15.77,9.54" target="#b11">[12,</ref><ref type="bibr" coords="1,504.84,244.51,11.83,9.54" target="#b14">15]</ref>. Indeed, crowdsourcing has been widely criticized for its mixed quality output. Marsden, for example, argues that 90% of crowdsourcing contributions are rubbish <ref type="bibr" coords="1,457.15,280.38,15.27,9.54" target="#b13">[14]</ref>. On the other hand, several studies in relevance data collection concluded that crowdsourcing leads to reliable labels <ref type="bibr" coords="1,474.84,304.29,10.79,9.54" target="#b1">[2,</ref><ref type="bibr" coords="1,489.18,304.29,7.19,9.54" target="#b5">6]</ref>. At the same time, works such as <ref type="bibr" coords="1,401.98,316.24,15.77,9.54" target="#b11">[12,</ref><ref type="bibr" coords="1,421.23,316.24,13.28,9.54" target="#b22">23]</ref> provide evidence of cheating and random behavior among members of the crowd. Clearly, the gathering of useful data requires not only technical capabilities, but also sound experimental design. This is especially important in crowdsourcing where the interplay of the various motivations and incentives affects the quality of the collected data <ref type="bibr" coords="1,374.36,387.98,15.77,9.54" target="#b14">[15,</ref><ref type="bibr" coords="1,392.62,387.98,11.83,9.54" target="#b15">16]</ref>.</p><p>Prompted by this growing interest in crowdsourcing for search evaluation, the new crowdsourcing track was launched at TREC with the aim to study crowdsourcing approaches for search engine evaluation. Two tasks were defined to be investigated in the first year of the track:</p><p>The Assessment task investigates the effectiveness of crowdsourcing methods to gather relevance labels. In this task, participating teams were asked to collect topical relevance judgments from crowd workers for a small set of topics over a subset of the ClueWeb09 collection using any crowdsourcing approach, design, incentives, and platforms. In the Consensus task, teams were asked to compute consensus over a fixed data set containing 89k previously crowdsourced labels for a set of 19k topicdocument pairs. We participated in both the tasks. Our goal for the first task was to minimize attracting spam workers through restricting workers by geographic location and by employing more interactive user interface controls, e.g., replacing standard radio buttons, which may attract more random clicking behavior <ref type="bibr" coords="1,353.90,674.70,10.58,9.54" target="#b4">[5]</ref>, with drag and drop interaction models. For the second task, we provide a probabilistic framework for predicting true relevance labels from crowdsourced judgments and explore variations that condition on worker and topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Over the last few years, crowdsourcing has attracted a lot of attention as a valuable approach to harness human abilities from a large population of workers. A significant portion of crowdsourcing efforts has focused on consensus tasks for which a crowdsourcing system collects multiple noisy reports from workers to identify a truth about the world. Examples of consensus tasks can be found in games with a purpose (e.g., image labelling in the ESP game) <ref type="bibr" coords="2,234.27,163.19,15.27,9.54" target="#b19">[20]</ref>, in citizen science projects (e.g., galaxy labelling in Galaxy Zoo) <ref type="bibr" coords="2,273.81,175.14,15.27,9.54" target="#b12">[13]</ref>, and in paid crowdsourcing systems (e.g., relevance judgment for topic-document pairs) <ref type="bibr" coords="2,156.87,199.05,10.58,9.54" target="#b0">[1]</ref>. One of the primary challenges in solving consensus tasks with crowdsourcing is the recovery of the true relevance signal from the noise, i.e., both the unintentional errors and the malicious behavior in the way workers report for a consensus task <ref type="bibr" coords="2,196.91,246.88,15.27,9.54" target="#b10">[11]</ref>.</p><p>There has been previous work on empirically evaluating the accuracy of non-expert workers when they report to consensus tasks and the factors that may affect their accuracy <ref type="bibr" coords="2,83.63,294.81,10.79,9.54" target="#b7">[8,</ref><ref type="bibr" coords="2,97.84,294.81,12.45,9.54" target="#b14">15,</ref><ref type="bibr" coords="2,113.72,294.81,11.83,9.54" target="#b16">17]</ref>. In particular, Alonso and Baeza-Yates, and Kazai present an empirical evaluation of the accuracy of non-expert workers in providing relevance judgments for document-topic pairs <ref type="bibr" coords="2,142.82,330.68,10.79,9.54" target="#b0">[1,</ref><ref type="bibr" coords="2,157.18,330.68,7.19,9.54" target="#b8">9]</ref>. In a related line of work, researchers explored approaches for learning worker models <ref type="bibr" coords="2,53.80,354.59,10.79,9.54" target="#b3">[4,</ref><ref type="bibr" coords="2,68.15,354.59,11.83,9.54" target="#b21">22]</ref>. Finally, there has been previous work on predicting consensus based on multiple noisy reports of workers. Previous approaches to this problem include majority voting, naive Bayes classifiers, and unsupervised and semisupervised learning techniques <ref type="bibr" coords="2,179.14,402.41,15.77,9.54" target="#b17">[18,</ref><ref type="bibr" coords="2,197.40,402.41,12.45,9.54" target="#b18">19,</ref><ref type="bibr" coords="2,212.34,402.41,11.83,9.54" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Assessment Task</head><p>This section deals with the problem investigated in the Assessment task of the track, that is the effective gathering of relevance labels for a fixed set of topic-document pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>The task was to collect topical relevance labels for 2175 documents over 25 topics as effectively as possible. Effectiveness was defined in terms of the quality of the workers attracted to the task, measured primarily based on the quality of the labels contributed by the individual workers. Since all crowdsourced labels had to be submitted, regardless whether a given label was later identified as being of poor quality, it was important to try to minimize attracting so-called spammers to the task.</p><p>Our team was assigned a total of 2175 topic-document pairs to judge, 520 pairs in the "assigned" set, which was shared with another 4 teams (different sets of teams per topic) and the same 1655 topic-document pairs that was shared across all participating teams. All the data was provided in sets of 5 documents per given topic, where the track guidelines required that a given worker judge all 5 pairs in a set. The data also contained gold set labels for 395 pairs, which were all grouped into sets of 5 as well, giving a total of 58 sets. Since the gold set was not distributed among the other topic-document pairs, we decided to randomly redistribute them among the total of 435 sets, thus ending up with 6 topic-document pairs per set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approach</head><p>Our goal was to evaluate and compare an interactive HIT design with a traditional design based on multiple radio buttons to obtain relevance labels from crowd workers. This was motivated by previous research that found significant differences in the quality of crowdsourced labels between two HIT designs, differing in the richness of the employed quality controls <ref type="bibr" coords="2,383.95,240.47,15.27,9.54" target="#b9">[10]</ref>. Other research has found that more simple designs and more mundane tasks are more susceptible to spam <ref type="bibr" coords="2,365.24,264.38,10.58,9.54" target="#b4">[5]</ref>.</p><p>Our hypothesis was that more interactive HIT designs would lead to reduced spam behavior, which was shown to be more prevalent in designs that rely on multiple choice radio buttons <ref type="bibr" coords="2,364.21,312.84,10.58,9.54" target="#b4">[5]</ref>. Our interactive design is shown in Figure <ref type="figure" coords="2,550.93,312.84,4.98,9.54" target="#fig_0">1</ref> and our baseline design is shown in Figure <ref type="figure" coords="2,489.38,324.79,3.74,9.54" target="#fig_1">2</ref>. In the interactive design, workers were asked to drag and drop the thumbnails of the web pages to be judged onto a two dimensional grid, where the horizontal placement indicates the usefulness of the page to the topic, in the worker's opinion, while the vertical placement reflects how certain the worker is in their rating of the web page. In the baseline design, we rely on a series of multiple choice inputs. These reflect the two dimensions of usefulness and confidence as in the interactive design. In addition, in the baseline, we also asked workers to pick the best document out of the 6 shown in a HIT. In both designs, workers had to click on the thumbnails to see the web pages as rendered images, as provided by the track organizers.</p><p>We paid $0.15 per HIT and offered a $5 bonus to the best performing workers in the event that we win the challenge. Given the 435 sets of data that needed to be judged, where one set was allocated into one HIT, and that we asked 3 workers to judge each HIT, our total cost per experiment (design) was $195.75 (without the bonus payments).</p><p>Following on from the findings in <ref type="bibr" coords="2,464.07,565.18,15.27,9.54" target="#b9">[10]</ref>, we restricted participation to workers located in the US, and with a HIT approval rate of over 85%, and with a minimum of 50 completed HITs.</p><p>For each topic, we showed the title, description and narrative fields.</p><p>In addition to the relevance judgments, we also collected self-reported information on the worker's knowledge of the topic being judged, on their Big Five personality traits, whether they enjoyed the task, and if they wanted to be considered for the bonus of $5 in case we win the challenge.</p><p>Unfortunately, due to issues with cross site scripting, where the communication between our iframe and Amazon's  Mechanical Turk broke down, we failed to run the experiments with the interactive design. Thus, we only report results for our baseline run. We hope to complete the interactive runs in the near future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>A total of 1047 HITs were completed by 99 unique workers by the TREC submission deadline (the total of 1305 HITs were completed by 111 workers). As common for most crowdsourcing engagements, the majority of the HITs were completed by a few workers (62% of HITs by 4 workers). The average time workers spent on the task was 331 seconds and the average accuracy on the test questions (data with gold standard) was 0.57. Out of the four 'keen' workers, three were above average quality (accuracy levels around 0.65, with average time of 220 seconds spent per HIT), while the fourth may have been a spam worker (accuracy of 0.25, average time spent of 168 seconds). Workers reported an average familiarity level of 1.86 (the scale was 0 to 5, 0 being "never heard about the topic before" and 5 meaning "I know a lot about the topic"). Workers found the task more fun than not: average reported fun level was 3.7 (scale was 0 to 5, 0 meaning not fun at all and 5 meaning "very much enjoyed the task').'</p><p>Figure <ref type="figure" coords="3,354.34,493.72,4.98,9.54" target="#fig_2">3</ref> shows the distribution of workers per bins of observed accuracy (on gold set). This suggests that most workers were reliable and that most of the HITs resulted in high quality data. The 23 workers with 0 accuracy only contributed a total of 26 HITs, each completing on average a single test (thus we have low certainty in the obtained accuracy scores). The only clearly 'bad apple' in the experiment was the single keen worker who completed 106 HITs and whose accuracy was 0.25.</p><p>Table <ref type="table" coords="3,352.81,602.14,4.98,9.54" target="#tab_0">1</ref> shows the official results against the consensus ground-truth (binary labels) and the gold set, showing the number of topic-document pairs, the number of unique workers, the Accuracy, Recall, Precision, Specificity, (normalized) Log-loss, (normalized) KL-divergence, and the root mean square error. For reference we also evaluate the consensus labels against the gold set.</p><p>We obtain a label accuracy of 77% against the consensus data, and 65% against the editorial judgments from TREC. In comparison, TREC assessors have pairwise agreement levels of 70-80% on average, with high variability across topics <ref type="bibr" coords="4,80.24,165.59,33.96,9.54">[21, p.44</ref>]. Thus, we obtain a similar agreement level to that reported in TREC for the evaluation against the consensus labels, but agreement with the gold set labels is lower.</p><p>The difference in the two different evaluations is relatively large, but we can also see that the consensus labels obtain only a label accuracy of 80% when evaluated against the gold set.This can raise questions regarding the use of consensus or gold standard as ground-truth.</p><p>The precision, 86% against consensus and 79% against the gold set, is higher than the recall, 71% against consensus and 64% against the gold set, indicating that the design led to judgments based on a relatively strict relevance criterion. Plausible design factors impacting this are the following. First, we showed the full narrative of the topic, restricting the topic to the interpretation of the original topic creator. Second, we used an operational definition of "topical relevance", defining it in terms of "usefulness", which may be regarded as a more restrictive criterion-not all topically relevant documents are useful. Third, we also asked workers to indicate their confidence in their ratings, which may have averted more speculative guessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Consensus Task</head><p>This section deals with the problem that is investigated in the second part of the TREC challenge of predicting consensus of a set of noisy worker judgments collected for a relevance judgment task. We start by introducing the consensus task and the dataset that is used for studying the consensus task. We then present a short survey of related work on solving consensus tasks in the domain of query judgments and in other domains. Next, we propose multiple approaches for solving consensus tasks that fall out of the same probabilistic formulation by making different independence assumptions. Finally, we conclude the section by evaluating these approaches on the relevance judgment dataset provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Description</head><p>The second part of the TREC Crowdsourcing Track, referred to as the "consensus task", focuses on computing consensus about the relevance of a given document to a topic over a set of individual reports collected from multiple workers. The consensus task aims at recovering the actual relevance of a topic-document pair based on multiple worker reports, thus eliminating the noise in the way workers judge relevance. Supervised, semi-supervised and unsupervised learning approaches can be used for predicting consensus from a set of noisy worker reports. In this paper, we propose using supervised learning approaches based on the naive Bayes method to predict relevance.</p><p>We use a dataset provided by the organizers of the challenge to train predictive models and to evaluate the prediction accuracy of these models. The dataset is composed of individual worker judgments of topic-document pairs, which were collected as part of the TREC 2010 Relevance Feedback track <ref type="bibr" coords="4,362.13,285.96,15.27,9.54" target="#b18">[19]</ref>. A small subset of instances in the dataset has ground truth NIST judgments. Each topic-document pair is judged by a worker as a part of a human intelligence task (HIT) advertised on the Mechanical Turk platform. Each instance of the dataset is composed of identifiers for a given topic and document pair, an identifier for the HIT in which the relevance is collected, an identifier for the worker who judged the pair, the judgment of the worker (relevant/not relevant) and the ground truth judgment of the pair. The dataset includes 89,624 relevance judgments collected from 762 workers for 19,033 topic-document pairs. A subset of this dataset was released as the development (training) set.</p><p>The training set includes a total of 10,770 judgments collected from 181 unique workers. Approximately 15% of the training dataset has ground truth NIST judgments. The majority of the document-topic pairs have 3 judgments, and the remaining small subset of the training set has 6 judgments.</p><p>The track evaluation uses both the ground truth NIST judgments (gold set) for evaluation as well as the consensus label computed from other participating teams' predictions for topic-document pairs in the test set. In this report, we focus on results over the gold set in the development set and over the gold set within the test set. Section 4.3 presents a set of results from these empirical evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approach</head><p>The dataset provided for the TREC challenge includes labeled and unlabeled relevance judgment instances; approximately 15% of the instances include gold standard relevance judgments. In a crowdsourcing environment, often some of the examples have been labeled by experts to either serve as teaching items for workers or to act as honeypots, challenges, etc. We define the problem of predicting whether a document is relevant to a topic as a supervised learning problem. We train predictive models with the subset of the data instances that have expert or "gold" labels. We first define the learning problem of predicting relevance. Then, we present different approaches for predicting the relevance of a document to a topic.</p><p>We build models based on various assumptions. These models correspond to three broad conditions: (1) an assumption that workers have comparable accuracy across all tasks;</p><p>(2) an assumption that workers have comparable accuracy withing a task, but varying across tasks (e.g. the amount of topic-expertise needed to accurately judge is the primary determiner of worker accuracy); ( <ref type="formula" coords="5,190.55,176.89,3.87,9.54" target="#formula_6">3</ref>) each worker has a particular skill/accuracy in making relevance judgments across all tasks.</p><p>Ultimately, we desire to infer the probability of the actual relevance label of a document, taking into account topicspecific effects, document-specific effects, and workerspecific effects. That is, we wish to compute:</p><formula xml:id="formula_0" coords="5,60.85,271.40,232.06,16.42">Pr(R i,j | t i , d j , {w 1 , . . . , w n | w k is elicited for i, j}) (1)</formula><p>Here t i ∈ T is a particular topic, d j ∈ D is a particular document, R i,j ∈ {0, 1} is the event that document d j is relevant to topic t i , and w k ∈ {0, 1} is the relevance label provided by worker k (out of n total workers across all topics). We will abbreviate the worker labels elicited for a pair as w i:j = w 1,i:j , . . . , w n,i:j to simplify. Depending on what independence assumptions we make when computing this term, the majority of topic and worker specific effects can be captured. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Naive Bayes Approach</head><p>The first approach we take to predict relevance is the naive Bayes approach. This approach applies Bayes' theorem and makes strong independence assumptions between the features that are used to predict relevance. In particular, it assumes that a given document, a given topic and judgements obtained from workers for the document-topic pair are independent given the relevance of the document to the topic. The way Pr(r i,j | t i , d j , w i:j ) is computed with the naive Bayes approach is given below:</p><formula xml:id="formula_1" coords="5,86.93,555.42,172.83,46.31">Pr(r i,j | t i , d j , w i:j ) By Bayes rule ∝ Pr( w i:j | r i,j , t i , d j ) Pr(r i,j | t i , d j )</formula><p>Assuming the relevance prior is independent of topic and document = Pr( w i:j | r i,j , t i , d j ) Pr(r i,j )</p><p>Assuming conditional independence given relevance = Pr(r i,j )</p><formula xml:id="formula_2" coords="5,148.57,660.23,84.32,32.11">| w i:j | k=1 Pr(w k,i:j | r i,j )</formula><p>1 If elicitation is not random, then a vector of elicitation variables should be explicitly added.</p><p>Assuming exchangeability among judges = Pr(r i,j )</p><formula xml:id="formula_3" coords="5,411.59,74.29,144.33,25.86">w k ∈| w i:j Pr(w k | r i,j )<label>(2)</label></formula><p>Here Pr(r i,j ) is simply the probability of seeing a relevant document in the training set. Pr(w k | r i,j ) is the probability of a worker will say relevant/irrelevant conditional on the actual document's relevance. These probabilities are computed from the subset of the training set with gold standard relevance judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Topic-Conditional Naive Bayes</head><p>Next, we present an approach for relaxing the independence assumptions of the naive Bayes model. The topicconditional naive Bayes model assumes that a given document and judgements obtained from workers are independent given a topic and the relevance of the document to the topic. This model takes into account that the prior probability of a document being relevant to a topic depends on the topic, and also takes into account that the likelihood of a worker reporting relevant or not may depend on the topic. The way Pr(r i,j | t i , d j , w i:j ) is computed with the topicconditional naive Bayes approach is given below:</p><formula xml:id="formula_4" coords="5,349.11,364.72,172.83,46.31">Pr(r i,j | t i , d j , w i:j ) By Bayes rule ∝ Pr( w i:j | r i,j , t i , d j ) Pr(r i,j | t i , d j )</formula><p>Assuming the relevance prior primarily depends on topic = Pr( w i:j | r i,j , t i , d j ) Pr(r i,j | t i )</p><p>Assuming conditional independence given topic and relevance = Pr(r i,j | t i )</p><formula xml:id="formula_5" coords="5,425.97,482.55,95.66,32.10">| w i:j | k=1 Pr(w k,i:j | r i,j , t i )</formula><p>Assuming exchangeability among judges = Pr(r i,j | t i )</p><formula xml:id="formula_6" coords="5,425.97,536.33,129.95,25.86">w k ∈| w i:j Pr(w k | r i,j , t i )<label>(3)</label></formula><p>Here Pr(r i,j | t i ) is simply the probability of seeing a relevant document for this topic. While Pr(w k | r i,j , t i ) is the probability within this topic that a worker will say relevant/irrelevant conditional on the actual document's relevance. Both of these can be computed from the known gold set for a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Worker-Conditional Naive Bayes</head><p>A second approach to relaxing the independence assumptions of the naive Bayes model is reasoning about the fact that the way workers report may differ from worker to worker. The worker-conditional naive Bayes model assumes that a given document, a given topic and the relevance reports obtained from workers are independent given the relevance of the document to the topic and the history of workers in the training set. While calculating the likelihood of a worker reporting relevant for a relevant (or irrelevant) document-topic pair, this model computes the ratio of instances in which the same worker reported relevant to a relevant (or irrelevant) document-topic pair in the training set. To compute relevance probabilities with this model, we introduce a new feature h k . h k represents the reporting history in the training set of the worker reporting w k,i:j for the current task. h includes the histories of all workers reporting for the current task. The way Pr(r i,j | t i , d j , w i:j ) is computed with the worker-conditional naive Bayes approach is given below:</p><formula xml:id="formula_7" coords="6,76.76,260.74,92.33,16.42">Pr(r i,j | t i , d j , w i:j , h)</formula><p>By Bayes rule ∝ Pr(</p><formula xml:id="formula_8" coords="6,118.99,290.63,150.94,16.42">w i:j | r i,j , t i , d j , h) Pr(r i,j | t i , d j , h)</formula><p>Assuming the relevance prior is independent of topic, document and history = Pr( w i:j | r i,j , t i , d j , h) Pr(r i,j )</p><p>Assuming conditional independence given worker history and relevance = Pr(r i,j )</p><formula xml:id="formula_9" coords="6,138.41,380.50,99.39,32.10">| w i:j | k=1 Pr(w k,i:j | r i,j , h k )</formula><p>Assuming exchangeability among judges with the same history = Pr(r i,j )</p><formula xml:id="formula_10" coords="6,138.41,449.23,154.50,25.86">w k ∈| w i:j Pr(w k | r i,j , h k )<label>(4)</label></formula><p>Here, P r(w k | r i,j , h k ) is the probability that a worker will say relevant/irrelevant conditional on the actual document's relevance and the history of this worker in the training set. This probability is estimated from the training set by counting the number of times this worker reported relevant/irrelevant in the training set for document-topic pairs with the given relevance value. For workers that do not have judgements in the training set, we used a general worker history which includes judgements from all workers in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>During the development phase, we randomly split the development data into a train (80%) and a validation portion (20%) by topic-document ID. That is, all of the ratings for a topic-document ID were either completely in the training set or completely in the testing set. Table <ref type="table" coords="6,223.55,686.66,4.98,9.54" target="#tab_1">2</ref> presents results computed by estimating model parameters over the training split of the development data and estimating performance on the validation portion of the development data. The column DefaultAcc presents the accuracy that can be obtained by guessing the most common class (relevant).</p><p>Table <ref type="table" coords="6,351.71,92.93,4.98,9.54" target="#tab_2">3</ref> presents the preliminary evaluation results provided by the track organizers over the gold portion of the test set. In the test set, 1000 documents had gold labels with 500 relevant and 500 irrelevant. The results have been sorted by the log-loss measure from the best (top of table) to the worst (bottom of table).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>From the result in Table <ref type="table" coords="6,426.48,200.24,3.74,9.54" target="#tab_1">2</ref>, we see that only the topicconditional naive Bayes outperformed the default model of predicting the most common class. However, all of the methods do outperform the majority vote method. The failure to outperform the default model may therefore be more of a result of the class skew in the data than an indication of the inferiority of the model. We decided to focus on accuracy as our decision criterion. This lead us to choose the topicconditional model as the run to submit.</p><p>In Table <ref type="table" coords="6,367.76,307.88,3.74,9.54" target="#tab_2">3</ref>, we see that the topic-conditional model (MSRC) performs in the middle of the pack in most classification measures (accuracy, precision, recall, specificity). However, in the two "soft measures" that measure the quality of probability estimates, log loss and root mean squared error (RMSE), the method is the best performer. This may indicate that the optimal threshold for the probability to make a hard classification decision may be other than the default (of 0.5). This requires further investigation to determine whether the models can be further optimized for classification procedures. Given the simplicity of the model, it is surprising that it can outperform the other submissions on the probability measures by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Limitations</head><p>As seen in Table <ref type="table" coords="6,384.05,498.88,4.98,9.54" target="#tab_1">2</ref> evaluation over the development data was problematic for two reasons. First, there was a large skew in class prevalance. This made determining overall model. Second, the dataset was small -how these models perform as a function of the amount of data is an important question we intend to investigate over the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>For the assessment task, our results were ranked 3rd based on the accuracy metric on both the gold set and the consensus ground-truth set. This is promising and suggests that limiting workers to those with high HIT approval rate in the US is a good start. We plan to run the interactive experiments in the very near future to investigate our original research question.</p><p>Considering the simplicity of the topic-conditional naive Bayes model and its relative high performance with respect </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,53.80,195.84,239.10,9.54;3,53.80,207.80,126.72,9.54;3,53.80,53.80,239.10,126.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Drag and drop based HIT template design (showing only the drag and drop part)</figDesc><graphic coords="3,53.80,53.80,239.10,126.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,94.33,633.64,158.05,9.54;3,53.80,238.66,239.10,379.81"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline HIT template design</figDesc><graphic coords="3,53.80,238.66,239.10,379.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,316.81,203.82,239.10,9.54;3,316.81,215.78,65.68,9.54;3,316.81,53.80,239.10,134.85"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of workers and percentage of HIT volume per accuracy bin</figDesc><graphic coords="3,316.81,53.80,239.10,134.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,81.87,63.81,445.99,67.51"><head>Table 1 :</head><label>1</label><figDesc>Assessment task results for primary runs, against gold and consensus ground-truth sets</figDesc><table coords="4,81.87,87.48,445.99,43.84"><row><cell>Evaluation</cell><cell cols="2">Pairs Wrks Acc</cell><cell>R</cell><cell>P</cell><cell>S</cell><cell>LL</cell><cell>nLL</cell><cell>KL</cell><cell>nKL RMSE</cell></row><row><cell>Consensus ground-truth</cell><cell>2005</cell><cell cols="5">95 0.77 0.71 0.86 0.83 1009.29</cell><cell cols="3">7.57 2551.40 17.26</cell><cell>0.70</cell></row><row><cell>Gold set ground-truth</cell><cell>2005</cell><cell cols="4">95 0.65 0.64 0.79 0.62</cell><cell cols="2">198.20 11.11</cell><cell>198.23</cell><cell>7.66</cell><cell>0.54</cell></row><row><cell>Consensus vs. gold set</cell><cell>1875</cell><cell cols="4">1 0.80 0.87 0.85 0.66</cell><cell>828.93</cell><cell>6.85</cell><cell>829.16</cell><cell>3.14</cell><cell>0.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,76.02,63.81,457.68,78.07"><head>Table 2 :</head><label>2</label><figDesc>Results of Models Over Development Set</figDesc><table coords="7,76.02,87.48,457.68,54.40"><row><cell>Model</cell><cell cols="6">TruePos TrueNeg FalsePos FalseNeg Accuracy DefaultAcc</cell><cell cols="2">Prec Recall Specificity</cell></row><row><cell>Majority Vote</cell><cell>101</cell><cell>8</cell><cell>17</cell><cell>19</cell><cell>0.7517</cell><cell cols="2">0.8276 0.8559 0.8417</cell><cell>0.3200</cell></row><row><cell>naive Bayes</cell><cell>120</cell><cell>0</cell><cell>25</cell><cell>0</cell><cell>0.8276</cell><cell cols="2">0.8276 0.8276 1.0000</cell><cell>0.0000</cell></row><row><cell>nB Topic</cell><cell>115</cell><cell>7</cell><cell>18</cell><cell>5</cell><cell>0.8414</cell><cell cols="2">0.8276 0.8647 0.9583</cell><cell>0.2800</cell></row><row><cell>nB Worker</cell><cell>117</cell><cell>1</cell><cell>24</cell><cell>3</cell><cell>0.8138</cell><cell cols="2">0.8276 0.8298 0.9750</cell><cell>0.0400</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,53.80,163.62,502.11,210.76"><head>Table 3 :</head><label>3</label><figDesc>Preliminary assessment task results over gold set ground-truth -sorted by (negative) LogLoss from best (top) to worst (bottom)</figDesc><table coords="7,139.78,198.86,330.16,120.16"><row><cell>Team</cell><cell cols="6">Accuracy Recall Precision Specificity LogLoss RMSE</cell></row><row><cell>MSRC</cell><cell>0.64</cell><cell>0.70</cell><cell>0.62</cell><cell>0.58</cell><cell>610.28</cell><cell>0.45</cell></row><row><cell>uogTr</cell><cell>0.44</cell><cell>0.34</cell><cell>0.43</cell><cell>0.54</cell><cell>931.74</cell><cell>0.59</cell></row><row><cell>LingPipe</cell><cell>0.66</cell><cell>0.73</cell><cell>0.64</cell><cell>0.59</cell><cell>975.88</cell><cell>0.50</cell></row><row><cell>GeAnn</cell><cell>0.58</cell><cell>0.74</cell><cell>0.56</cell><cell>0.42</cell><cell>1150.44</cell><cell>0.51</cell></row><row><cell>UWaterlooMDS</cell><cell>0.67</cell><cell>0.78</cell><cell>0.64</cell><cell>0.57</cell><cell>1435.77</cell><cell>0.50</cell></row><row><cell>uc3m</cell><cell>0.70</cell><cell>0.75</cell><cell>0.68</cell><cell>0.64</cell><cell>2772.31</cell><cell>0.55</cell></row><row><cell>BUPT-WILDCAT</cell><cell>0.69</cell><cell>0.79</cell><cell>0.65</cell><cell>0.58</cell><cell>2901.26</cell><cell>0.56</cell></row><row><cell>TUD DMIR</cell><cell>0.66</cell><cell>0.76</cell><cell>0.63</cell><cell>0.56</cell><cell>3113.10</cell><cell>0.58</cell></row><row><cell>UTaustin</cell><cell>0.60</cell><cell>0.91</cell><cell>0.56</cell><cell>0.30</cell><cell>3647.29</cell><cell>0.63</cell></row><row><cell>qirdcsuog</cell><cell>0.53</cell><cell>0.82</cell><cell>0.52</cell><cell>0.23</cell><cell>4338.07</cell><cell>0.69</cell></row></table><note coords="7,53.80,340.93,239.10,9.54;7,53.80,352.89,239.10,9.54;7,53.80,364.84,195.08,9.54"><p>to probability prediction, it offers a promising path for future development. Of particular interest is a model that accounts for both worker and topic effects simultaneously.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>We would like to thank <rs type="person">Emine Yilmaz</rs> and <rs type="person">Ryen White</rs> for their valuable discussions and feedback.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,75.38,492.21,217.52,9.54;7,75.38,504.17,217.52,9.54;7,75.38,516.12,217.53,9.54;7,75.38,528.08,217.53,9.54;7,75.38,540.03,188.33,9.54" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,218.44,492.21,74.46,9.54;7,75.38,504.17,217.52,9.54;7,75.38,516.12,11.42,9.54">Design and implementation of relevance assessments using crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,104.46,519.61,188.44,5.95;7,75.38,531.56,176.96,5.95">Advances in Information Retrieval -33rd European Conference on IR Research (ECIR 2011</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,561.01,217.52,9.54;7,75.38,572.97,217.52,9.54;7,75.38,584.92,217.52,9.54;7,75.38,596.88,217.52,9.54;7,75.38,612.32,217.52,5.95;7,75.38,620.79,97.96,9.54" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,191.56,561.01,101.34,9.54;7,75.38,572.97,217.52,9.54;7,75.38,584.92,34.19,9.54">Can we get rid of TREC assessors? using Mechanical Turk for relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,232.67,600.36,60.22,5.95;7,75.38,612.32,217.52,5.95;7,75.38,624.27,14.39,5.95">Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<meeting>the SIGIR 2009 Workshop on the Future of IR Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,641.77,217.52,9.54;7,75.38,653.72,217.52,9.54;7,75.38,665.68,66.66,9.54" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,241.14,641.77,51.77,9.54;7,75.38,653.72,111.64,9.54">Crowdsourcing for relevance evaluation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,197.35,657.21,52.06,5.95">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2008-11">November 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,75.38,686.66,217.52,9.54;7,75.38,698.61,217.52,9.54;7,75.38,710.57,112.25,9.54" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,130.03,686.66,162.87,9.54;7,75.38,698.61,62.38,9.54">Artificial intelligence for artificial artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,157.15,702.10,135.75,5.95;7,75.38,714.05,83.54,5.95">Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,340.93,217.52,9.54;7,338.39,352.89,217.52,9.54;7,338.39,368.33,217.52,5.95;7,338.39,376.80,104.76,9.54" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,462.89,340.93,93.02,9.54;7,338.39,352.89,41.24,9.54">How crowdsourcable is your task?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,394.68,356.37,161.23,5.95;7,338.39,368.33,213.24,5.95">Proceedings of the Workshop on Crowdsourcing for Search and Data Mining (CSDM 2011)</title>
		<meeting>the Workshop on Crowdsourcing for Search and Data Mining (CSDM 2011)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,398.12,217.52,9.54;7,338.39,410.08,217.52,9.54;7,338.39,425.52,217.52,5.95;7,338.39,437.47,217.52,5.95;7,338.39,445.94,217.52,9.54;7,338.39,457.90,217.52,9.54;7,338.39,469.85,46.77,9.54" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,436.83,398.12,119.08,9.54;7,338.39,410.08,161.29,9.54">Crowdsourcing document relevance assessment with mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,519.85,413.56,36.06,5.95;7,338.39,425.52,217.52,5.95;7,338.39,437.47,217.52,5.95;7,338.39,445.94,107.07,9.54">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk, CSLDAMT &apos;10</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk, CSLDAMT &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,491.18,217.53,9.54;7,338.39,503.13,217.52,9.54;7,338.39,515.09,181.38,9.54" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,375.88,494.66,180.03,5.95;7,338.39,506.62,134.48,5.95">Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Crown Publishing Group</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct coords="7,338.39,536.41,217.52,9.54;7,338.39,548.37,192.46,9.54" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,399.61,536.41,156.30,9.54;7,338.39,548.37,46.82,9.54">Analyzing the amazon mechanical turk marketplace</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,393.06,551.85,21.47,5.95">XRDS</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,569.69,217.52,9.54;7,338.39,581.65,217.53,9.54;7,338.39,597.09,217.52,5.95;7,338.39,605.56,217.53,9.54" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,385.24,569.69,170.67,9.54;7,338.39,581.65,97.54,9.54">In search of quality in crowdsourcing for search engine evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,456.39,585.13,99.53,5.95;7,338.39,597.09,217.52,5.95;7,338.39,609.04,45.74,5.95">Advances in Information Retrieval -33rd European Conference on IR Research (ECIR 2011</title>
		<imprint>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,338.39,638.83,217.52,9.54;7,338.39,650.79,217.52,9.54;7,338.39,662.75,217.52,9.54;7,338.39,678.19,217.52,5.95;7,338.39,690.14,217.52,5.95;7,338.39,698.61,217.52,9.54;7,338.39,710.57,74.60,9.54" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,338.39,650.79,217.52,9.54;7,338.39,662.75,173.49,9.54">Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Milic-Frayling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,538.10,666.23,17.81,5.95;7,338.39,678.19,217.52,5.95;7,338.39,690.14,217.52,5.95;7,338.39,698.61,82.63,9.54">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,57.02,217.52,9.54;8,75.38,68.97,217.52,9.54;8,75.38,84.41,217.52,5.95;8,75.38,92.88,217.52,9.54;8,75.38,104.84,22.42,9.54" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,211.12,57.02,81.78,9.54;8,75.38,68.97,120.38,9.54">Crowdsourcing user studies with Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,219.90,72.46,73.00,5.95;8,75.38,84.41,217.52,5.95;8,75.38,96.37,117.24,5.95">Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems</title>
		<meeting>eeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,124.04,217.52,9.54;8,75.38,135.99,217.52,9.54;8,75.38,147.95,217.52,9.54;8,75.38,163.39,217.52,5.95;8,75.38,171.86,126.19,9.54" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,266.54,124.04,26.36,9.54;8,75.38,135.99,217.52,9.54;8,75.38,147.95,199.19,9.54">Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,75.38,163.39,217.52,5.95;8,75.38,175.35,42.02,5.95">SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,191.06,217.52,9.54;8,75.38,203.01,217.52,9.54;8,75.38,214.97,217.52,9.54;8,75.38,226.92,217.52,9.54;8,75.38,238.88,217.52,9.54;8,75.38,250.84,169.90,9.54" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,166.30,214.97,126.61,9.54;8,75.38,226.92,217.52,9.54;8,75.38,238.88,67.90,9.54">Galaxy zoo: morphologies derived from visual inspection of galaxies from the sloan digital sky survey</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lintott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Schawinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Slosar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raddick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Szalay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Andreescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,150.53,242.37,142.37,5.95;8,75.38,254.32,61.94,5.95">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1179" to="1189" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,270.04,217.52,9.54;8,75.38,281.99,65.04,9.54" xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Marsden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,130.68,270.04,157.59,9.54">Crowdsourcing. Contagious Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="24" to="28" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,301.19,217.52,9.54;8,75.38,313.15,217.53,9.54;8,75.38,325.10,134.07,9.54" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,180.09,301.19,112.81,9.54;8,75.38,313.15,91.80,9.54">Financial incentives and the performance of crowds</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,177.75,316.63,115.16,5.95;8,75.38,328.59,40.87,5.95">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,344.30,217.52,9.54;8,75.38,356.26,217.52,9.54;8,75.38,371.70,217.52,5.95;8,75.38,380.17,217.52,9.54;8,75.38,392.12,163.00,9.54" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,221.91,344.30,70.99,9.54;8,75.38,356.26,198.07,9.54">Human computation: a survey and taxonomy of a growing field</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,75.38,371.70,217.52,5.95;8,75.38,380.17,158.24,9.54">Proceedings of the 2011 annual conference on Human factors in computing systems, CHI &apos;11</title>
		<meeting>the 2011 annual conference on Human factors in computing systems, CHI &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,411.32,217.52,9.54;8,75.38,423.28,217.53,9.54;8,75.38,438.72,217.52,5.95;8,75.38,447.19,97.04,9.54" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,223.39,411.32,69.51,9.54;8,75.38,423.28,122.54,9.54">Designing incentives for inexpert human raters</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,217.73,426.77,75.18,5.95;8,75.38,438.72,217.52,5.95;8,75.38,447.19,68.20,9.54">Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW &apos;11</title>
		<meeting>the ACM Conference on Computer Supported Cooperative Work, CSCW &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,466.39,217.52,9.54;8,75.38,478.35,217.52,9.54;8,75.38,490.30,217.52,9.54;8,75.38,505.74,217.52,5.95;8,75.38,514.21,217.53,9.54;8,75.38,526.17,133.40,9.54" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,267.45,466.39,25.45,9.54;8,75.38,478.35,217.52,9.54;8,75.38,490.30,145.12,9.54">Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,243.55,493.79,49.35,5.95;8,75.38,505.74,217.52,5.95;8,75.38,517.70,84.32,5.95">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,545.37,217.52,9.54;8,75.38,557.32,217.52,9.54;8,75.38,569.28,216.82,9.54" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,170.71,545.37,122.19,9.54;8,75.38,557.32,98.19,9.54">Semi-supervised consensus labeling for crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,191.73,560.81,101.17,5.95;8,75.38,572.76,187.72,5.95">SIGIR 2011 Workshop on Crowdsourcing for Information Retrieval (CIR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,588.48,217.52,9.54;8,75.38,600.43,180.67,9.54" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,195.53,588.48,97.38,9.54;8,75.38,600.43,29.78,9.54">Designing games with a purpose</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,113.00,603.92,112.34,5.95">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,619.63,217.52,9.54;8,75.38,635.08,217.52,5.95;8,75.38,643.54,101.12,9.54" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="8,265.23,623.12,27.68,5.95;8,75.38,635.08,217.52,5.95;8,75.38,647.03,24.81,5.95">TREC: Experimentation and Evaluation in Information Retrieval</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,75.38,662.75,217.52,9.54;8,75.38,674.70,217.52,9.54;8,75.38,686.66,217.52,9.54;8,75.38,698.61,217.52,9.54;8,75.38,710.57,108.20,9.54" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,129.20,674.70,163.71,9.54;8,75.38,686.66,217.52,9.54;8,75.38,698.61,13.06,9.54">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,96.73,702.10,196.17,5.95;8,75.38,714.05,16.60,5.95">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,338.39,57.02,217.52,9.54;8,338.39,68.97,217.52,9.54;8,338.39,84.41,217.52,5.95;8,338.39,92.88,126.18,9.54" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,457.26,57.02,98.65,9.54;8,338.39,68.97,196.09,9.54">An analysis of assessor behavior in crowdsourced preference judgments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,338.39,84.41,217.52,5.95;8,338.39,96.37,42.01,5.95">SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
