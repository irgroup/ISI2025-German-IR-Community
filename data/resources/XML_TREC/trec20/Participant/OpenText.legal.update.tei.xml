<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,189.42,112.00,233.15,15.15;1,175.34,133.91,261.32,15.15">Learning Task Experiments in the TREC 2011 Legal Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-01-25">January 25, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,264.32,167.81,83.35,8.74"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<email>stomlins@opentext.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,189.42,112.00,233.15,15.15;1,175.34,133.91,261.32,15.15">Learning Task Experiments in the TREC 2011 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-01-25">January 25, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">2BB98543656E5EFEE3C9433D7279C119</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Learning Task of the TREC 2011 Legal Track investigated the effectiveness of e-Discovery search techniques at selecting training examples and learning from them to estimate the probability of relevance of every document in a collection. The task specified 3 test topics, each of which included a one-sentence request for documents to produce from a target collection of 685,592 e-mail messages and attachments. In this paper, we describe the experimental approaches used and report the scores that each achieved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>OpenText Search Server R , eDOCS Edition (formerly known as Open Text eDOCS SearchServer TM ) is a toolkit for developing enterprise search and retrieval applications. The eDOCS SearchServer kernel is also embedded in various components of the OpenText eDOCS Suite <ref type="foot" coords="1,352.89,424.30,3.97,6.12" target="#foot_0">1</ref> .</p><p>The eDOCS SearchServer kernel works in Unicode internally <ref type="bibr" coords="1,354.11,437.83,10.52,8.74" target="#b6">[7]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval experimentation (TREC <ref type="bibr" coords="1,479.08,449.79,14.61,8.74" target="#b11">[12]</ref>, CLEF <ref type="bibr" coords="1,529.48,449.79,10.52,8.74" target="#b3">[4]</ref> and NTCIR <ref type="bibr" coords="1,128.07,461.74,10.79,8.74" target="#b7">[8]</ref>) have provided judged test collections for objective experimentation with the SearchServer kernel in more than a dozen languages.</p><p>This paper describes experimental work with the eDOCS SearchServer kernel (experimental post-6.0 builds) conducted in part by participating in the Learning Task of the TREC 2011 Legal Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Task</head><p>The Learning Task of the TREC 2011 Legal Track investigated the effectiveness of e-Discovery search techniques at selecting training examples and learning from them to estimate the probability of relevance of every document in a collection. This is the sixth year of the TREC Legal Track and second year of the Learning Task. We have participated in the 6 years of the Legal Track to date <ref type="bibr" coords="1,285.52,594.22,22.37,8.74">(2006)</ref><ref type="bibr" coords="1,307.89,594.22,4.47,8.74">(2007)</ref><ref type="bibr" coords="1,307.89,594.22,4.47,8.74">(2008)</ref><ref type="bibr" coords="1,307.89,594.22,4.47,8.74">(2009)</ref><ref type="bibr" coords="1,307.89,594.22,4.47,8.74">(2010)</ref><ref type="bibr" coords="1,312.37,594.22,22.37,8.74">(2011)</ref>. (We also helped with coordinating the Legal Track in 3 of these years (2007-2009) as described in <ref type="bibr" coords="1,312.10,606.17,14.61,8.74" target="#b20">[21]</ref>, <ref type="bibr" coords="1,334.63,606.17,15.50,8.74" target="#b9">[10]</ref> and <ref type="bibr" coords="1,374.35,606.17,9.96,8.74" target="#b5">[6]</ref>; however, we were not part of the coordination of this year's track.)</p><p>The Learning Task used the same document collection as last year (described below). Like last year, this task requires the system to estimate of the probability of relevance of each e-mail or attachment for each test topic. New this year was the opportunity for the system to specify in advance a few hundred documents (in batches of 100) and be given the relevance assessments for those documents (last year the set of example documents was the same for all groups).</p><p>The document collection was called the "EDRM Enron Email Data Set v2" collection which consisted of 685,592 e-mail messages and attachments (approximately 4GB of text) from 159 mailbox directories. (By our count, there were 146 different employee mailboxes, with a few large mailboxes split into multiple directories.) We just used the "Deduplicated text-only" version of this collection available in a compressed file called edrmv2txt-v2.tar.bz2. (For binary attachments, this version contained the text extracted by a 3rd-party tool, which was of variable quality.) Uncompressed, the collection contained 685,592 .txt files, totaling 3,991,162,863 bytes. The document id was the part of the filename before the .txt suffix. Each attachment to a message was in a separate .txt file, numbered .1, .2, and so on. For example, container message "3.129461.NC5X5LNTR5XI1CBA3P4QVXG4YOWV5J0NB.txt" had 2 attachments called "3.129461.NC5X5LNTR5XI1CBA3P4QVXG4YOWV5J0NB.1.txt" and "3.129461.NC5X5LNTR5XI1CBA3P4QVXG4YOWV5J0NB.2.txt"; these were 3 of the 685,592 "documents" in the collection.</p><p>To test the systems, there were 3 production requests, herein called "topics", numbered 401 to 403. Each topic included a one-sentence request for documents to produce for each topic. A topic authority (assessor) also produced a "coding manual" for each topic with more details of what was considered relevant or not.</p><p>Please see the task guidelines <ref type="bibr" coords="2,223.34,290.35,15.50,8.74" target="#b21">[22]</ref> for more details on the task and track. <ref type="bibr" coords="2,426.37,290.35,9.96,8.74" target="#b8">[9]</ref>, <ref type="bibr" coords="2,444.25,290.35,10.52,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="2,479.52,290.35,10.52,8.74" target="#b1">[2]</ref> have more background on e-Discovery in general. Also, background on our past participations in the track are in <ref type="bibr" coords="2,521.73,302.31,14.61,8.74" target="#b14">[15]</ref>, <ref type="bibr" coords="2,72.00,314.26,14.61,8.74" target="#b15">[16]</ref>, <ref type="bibr" coords="2,93.58,314.26,14.61,8.74" target="#b16">[17]</ref>, <ref type="bibr" coords="2,115.18,314.26,14.61,8.74" target="#b17">[18]</ref>, <ref type="bibr" coords="2,136.76,314.26,14.61,8.74" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>To index the collection, we processed the 685,592 .txt files in the same way as last year, as follows:</p><p>For each container message (i.e. non-attachment messages, which were identified as those having just 2 dots in the document id instead of 3 dots), we discarded lines which appeared to be "noise" lines, which were those starting with "X-SDOC: ", "X-ZLID: zl-edrm-enron-v2-" or "EDRM Enron Email Data Set has been produced in EML, PST and NSF format by ZL Technologies, Inc". (Note that, for these experiments, we did not bother to take advantage of any of the structure of the e-mail messages. In particular, the "Date:", "From:", "To:" and "Subject:" lines were just treated as plain text like any line of the body of the email.)</p><p>Then for each message (including attachments), we added a "&lt;record&gt;" tag before each message, followed by the document id inside "&lt;tid&gt;..&lt;/tid&gt;" tags, followed by the message text converted to an XML-safe form (e.g. special characters such as "&amp;" were converted to XML entities such as "&amp;amp;"), followed by a closing "&lt;/record&gt;" tag. The output of the re-formatting of the .txt files of each subdirectory was sent to one file, resulting in 228 .xml files (as some of the 159 mailbox directories had more than one subdirectory), but still comprising 685,592 records.</p><p>The reason for converting the collection to this XML format was that we could then index it with the same scripts we had used for the IIT CDIP collection in 2006-2009. As in those years, for each record, we indexed from the "&lt;/tid&gt;" tag to the "&lt;/record&gt;" tag. Any tags themselves were indexed (we just didn't bother to discard them; a minor side effect is that this meant the term "record" matched every document). Entities (e.g. "&amp;amp;") were converted back to the character they represented (e.g. "&amp;").</p><p>We did not use a stopword list. The index supported both searching on just the surface forms of the words and also searching on inflections from English lexical stemming. The documents were assumed to be in the Windows-1252 character set when converted to Unicode. Words were normalized to upper-case and any accents were dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Baseline Runs (No Training Examples)</head><p>Participants could submit up to 3 baseline runs, also known as the "first interim submissions", which did not make use of any example judgments from the topic authority. Our 3 baseline runs are described in the next 3 sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Boolean Run -otL11BT1</head><p>The submitted experimental otL11BT1 run was a Boolean-based run.</p><p>We created the following short Boolean queries (1 or 2 words or phrases) based on reading the topic statements and coding manuals: 401: FT_TEXT CONTAINS 'enrononline%' 402: FT_TEXT CONTAINS 'over-the-counter'|'OTC' 403: FT_TEXT CONTAINS 'environmental'|'environment%' Unlike last year, no linguistic expansion from English inflectional stemming was applied for our Boolean queries. The "%" character however indicated wildcard expansion, e.g. 'environment%' would also match 'environmentalist'. In the topic 403 query, the function of separately listing 'environmental' (which would already be matched by 'environment%') was to increase the weight on that particular term for relevance calculations. The hyphenated 'over-the-counter' phrase would also match non-hyphenated variations such as 'over the counter'.</p><p>It was hoped that these simple queries would have good recall of the relevant documents. With the final judgments we can now see that the first one had 0.59 precision but just 0.21 recall, the second one had just 0.18 precision and 0.12 recall, and the third one had 0.04 precision and 0.60 recall, as listed in the P@B and R@B columns of Tables <ref type="table" coords="3,181.53,292.80,4.01,8.74" target="#tab_6">4</ref><ref type="table" coords="3,185.54,292.80,4.01,8.74" target="#tab_7">5</ref><ref type="table" coords="3,189.56,292.80,4.01,8.74">6</ref>. (The B values, i.e. the number matches for the Boolean queries, were 8241, 5085 and 19435 for the 3 topics respectively.)</p><p>The matches were still relevance-ranked. (The relevance ranking approach was the same for all runs, and also the same as in past years. The relevance function dampened the term frequency and adjusted for document length in a manner similar to Okapi <ref type="bibr" coords="3,284.50,340.62,15.50,8.74" target="#b10">[11]</ref> and dampened the inverse document frequency using an approximation of the logarithm. For wildcard terms (e.g. "televis%"), all variants (e.g. "television", "televised", "televisions", etc.) were treated as occurrences of the same term for term frequency purposes, and inverse document frequency was based on the most common variant. For runs which used inflectional matching (which was the case for the Request and Feedback runs described later), these calculations were based on the stems of the terms.) For terms in phrases of Boolean queries, only occurrences of the term satisfying the phrase counted towards term frequency.)</p><p>To assign a probability of relevance to each matching document, we used the same experimental "generalpurpose" probability formula as last year, which was to take the raw relevance() score (which was usually between 0 and 500), multiply it by 0.002, square it, divide by 0.75, and enforce a max of 0.75 and min of 0.0001. Any documents unmatched by the Boolean query were appended to the end with the probability set to 0.0001. (It was required to submit all of the documents for each topic.)</p><p>Note that the sum of the probabilities was typically different than the number of matches for the Boolean query. e.g. for topic 401, the Boolean query had 8241 matches, but the sum of the probabilities from the (experimental) general-purpose formula was just 7319.</p><p>This experimental otL11BT1 run was submitted August 1, 2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Request Run -otL11FT1</head><p>The submitted experimental otL11FT1 run was just based on the terms in the one-sentence request (topic statement) after manually removing what seemed to be common instruction words (e.g. "please", "produce", "documents"). The resulting queries were as follows:</p><formula xml:id="formula_0" coords="3,72.00,610.78,444.58,80.03">401: FT_TEXT CONTAINS 'design'|'development'|'operation'|'marketing'|'enrononline'| 'online'|'service'|'offered'|'provided'|'used'|'subsidiaries'|'predecessors'| 'successors'|'interest'|'purchase'|'sale'|'trading'|'exchange'|'financial'| 'instruments'|'products'|'limited'|'derivative'|'instruments'|'commodities'| 'futures'|'swaps' 402: FT_TEXT CONTAINS 'purchase'|'sale'|'trading'|'exchange'|'counter'|'derivatives'| 'actual'|'contemplated'|'financial'|'instruments'|'products'|'legal'|'illegal'| 'permitted'|'prohibited'|'proposed'|'rule'|'regulation'|'law'|'standard'| 'proscription'|'domestic'|'foreign' 403: FT_TEXT CONTAINS 'environmental'|'impact'|'activity'|'activities'|'undertaken'| 'limited'|'measures'|'taken'|'conform'|'comply'|'avoid'|'circumvent'|'influence'| 'proposed'|'rule'|'regulation'|'law'|'standard'|'proscription'|'governing'| 'environmental'|'emmissions'|'spills'|'pollution'|'noise'|'animal'|'habitats'</formula><p>Note that linguistic expansion from English inflectional stemming was also applied for these queries, e.g. the search for 'design' also matched 'designs'.</p><p>The probability formula was the same as for the short Boolean query run, i.e. take the raw relevance() score (usually between 0 and 500), multiply by 0.002, square it, divide by 0.75, and enforce a max of 0.75 and min of 0.0001. Non-matches were all assigned 0.0001.</p><p>With the final judgments, we can get an idea of how these queries fared compared to the short Boolean queries by cutting off the ranked list at the same number of matches as the corresponding Boolean query ("depth B"). As listed in the P@B and R@B columns of Tables <ref type="table" coords="4,360.25,240.45,4.01,8.74" target="#tab_6">4</ref><ref type="table" coords="4,364.26,240.45,4.01,8.74" target="#tab_7">5</ref><ref type="table" coords="4,368.27,240.45,4.01,8.74">6</ref>, we see that the request-based query had a lower precision and recall at depth B for the first and third queries, but a little higher for the second query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Fusion Run -otL11HT1</head><p>The submitted experimental otL11HT1 run was a fusion run which just assigned the probability to each document by summing half of the probability assigned in the otL11BT1 run and half of the probability assigned in the otL11FT1 run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Selection of 100 Training Examples</head><p>Unlike last year, this year we could choose our own training example documents for each topic (up to 1000 per topic, but only 100 at a time).</p><p>Our approach was to start by drawing 100 samples from the Boolean run otL11BT1 by selecting the documents of the following ranks for each topic: 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000, 95000, 100000, 150000, 200000, 250000, 300000, 350000, 400000, 450000, 500000, 550000, 600000, 650000</p><p>We favored documents of earlier ranks in hopes of getting a lot of example relevant documents, but we also sampled from the rest of the run (which included the entire target document set) so that we could compute an estimate of the number of relevant documents in the document set for each topic (albeit a coarse estimate) which would be useful for estimating the probabilities of relevance.</p><p>We used the Boolean run as our base instead of the Request run because the short Boolean queries tended to retrieve short documents at the top of the results, whereas the long queries of the Request run tended to retrieve a lot of longer documents at the top of the list. We suspect that our learning method works better with shorter training documents.</p><p>On August 2, 2011, we submitted our request for the judgments for these 100 documents for each topic. We received the 100 relevance assessments for topic 402 on August 4, for topic 403 on August 5, and for topic 401 on August 8.</p><p>Of the 100 documents for each topic, it turned out that 58 were judged relevant for topic 401, 23 were judged relevant for topic 402, and 18 were judged relevant for topic 403. Hence we now had some example relevant documents to work with for each topic.</p><p>We converted the 100 examples for each topic into a "qrels" file compatible with the l07 eval utility. For our Boolean run (otL11BT1), it estimated the following scores: All of all example relevant documents came from the documents that matched the Boolean query (leading to the overly high estimates of 100% recall) presumably because the set was in arbitrary order after depth B. More thought should be put into the sampling approach in the future.</p><formula xml:id="formula_1" coords="5,72.00,141.95,41.84,8.30">otL11BT1</formula><p>Our estimates of the number of relevant documents for each topic were also off compared to the full official judgments, especially for the first topic: </p><formula xml:id="formula_2" coords="5,72.00,393.66,15.69,8.30">our</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Runs Using 100 Training Examples</head><p>The "second interim submissions" incorporated learning from the 100 example judgments for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Feedback Run using 100 Examples -otL11FT2</head><p>The submitted experimental otL11FT2 run was a pure feedback run that did not make any use of the topic statements. The input was just the example relevant documents, which were the 100 example judged documents after discarding non-relevant documents and discarding documents of 10,000 bytes or more (in the XML formatting described earlier) in hopes of reducing the percentage of input text that was not relevant. These example relevant documents for each topic were input to the SearchServer IS ABOUT predicate which created a vector query from the highest weighted terms (based on a tf.idf calculation after appending the input documents together). English inflections were enabled, and stems in more than 5% of the collection's documents were omitted.</p><p>Our probability formula was updated to take advantage of our estimates of the number of relevant documents for each topic. The updated formula was to take the relevance() score (usually between 0 and 500), multiply by 0.002, apply the exponent x, divide by 0.98, and enforce a max of 0.98 and min of 0.0001. Documents not matched by the feedback query were also assigned 0.0001. The known relevant documents were moved to the front and assigned 0.99, while the known non-relevant documents were assigned 0.01. Exponent x was 2.237, 3.339, 3.301 for the 3 topics respectively, chosen to make the probabilities sum to the estimated number of relevant documents based on our sample of 100 documents per topic.</p><p>The reason for assigning 0.99 to the known relevant documents instead of 1.00 (and 0.01 for known non-relevant documents instead of 0.00) was that we were advised that it was possible the assessors might change their assessment in the final judgments (e.g. to fix an error).</p><p>This experimental otL11FT2 run was submitted August 27, 2011. At this point, we were eligible again to request another 100 judgments for each topic, but the deadline for runs using up to 1000 example judgments was August 28, and even if the judgments came back instantaneously we wouldn't have had time to incorporate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Boolean Run using 100 Examples -otL11BT2</head><p>The submitted experimental otL11BT2 run was the same as the baseline otL11BT1 run except that the known relevant documents were moved to the front and the known non-relevant documents were moved down (from our example 100 judgments per topic). The probabilities were assigned using the same approach as for otL11FT2 except that the exponents worked out to 3.14, 1.61 and 8.54 for the 3 topics respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Fusion Run using 100 Examples -otL11HT2</head><p>The submitted experimental otL11HT2 run was a fusion run which just assigned the probability to each document by summing half of the probability assigned in the otL11BT2 run and half of the probability assigned in the otL11FT2 run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Mop-up Runs</head><p>On August 30, 2011, the track organizers released all of the example judgments for each topic that had been requested by any participating group. "Mop-up" runs using all of these examples were due September 6.</p><p>Here are the counts of the number of example judgments, judged relevant and judged non-relevant for each topic in the mop-up examples: Topic 401: count=2500, rel=1040, non=1460 Topic 402: count=2102, rel= 238, non=1864 Topic 403: count=2199, rel= 245, non=1954</p><p>The extra examples weren't of much help for improving our estimates of the number of relevant documents for each topic because we did not have information on how these examples were sampled, so we continued to target the same numbers of relevant documents as we did with just 100 training examples per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Feedback Run using 2000+ Mop-up Examples -otL11FTM</head><p>The submitted experimental otL11FTM run was produced in the same way as the otL11FT2 run except that we had a lot more example relevant documents to start with. The exponents in the probability formulas worked out to 2.72, 3.92, 3.48 for the 3 topics respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Boolean Run using 2000+ Mop-up Examples -otL11BTM</head><p>The submitted experimental otL11BTM run was produced in the same way as the otL11BT2 run except that we had a lot more example relevant documents to move to the front. The exponents in the probability formulas worked out to 3.86, 1.83 and 9.07 for the 3 topics respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Fusion Run using 2000+ Mop-up Examples -otL11HTM</head><p>The submitted experimental otL11HTM run was a fusion run which just assigned the probability to each document by summing half of the probability assigned in the otL11BTM run and half of the probability assigned in the otL11FTM run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Preliminary and Final Judgments</head><p>On October 16, 2011, preliminary relevance assessments were released. Here are the counts of the number of judged documents, relevant documents and non-relevant documents for each topic in these preliminary assessments:</p><p>Topic 401: count=5871, rel=2585, non=3286 Topic 402: count=5583, rel= 843, non=4740 Topic 403: count=5545, rel= 534, non=5011</p><p>Here are the estimated number of relevant documents for each topic based on the preliminary judgments: As you can see, some of the judgments changed for the first two topics. The third topic did not have any changes between the preliminary and final judgments.</p><p>Here are the estimated number of relevant documents for each topic based on the final judgments: After all the participants submitted their experimental runs (due September 6, 2011), the task organizers then had a sample of the test collection judged for relevance as the basis for estimating the various scores, such as recall, precision and F 1 . The details presumably will be in the track overview paper <ref type="bibr" coords="10,489.13,120.89,9.96,8.74" target="#b4">[5]</ref>, but our understanding is that it proceeded as follows.</p><p>The test collection was divided into 2 strata, called stratum 100 and stratum 1000. Our understanding is that any document that was ranked in the top-100 by any participant submission was in stratum 100, and this stratum was almost completely judged. The remaining documents were in the 'stratum 1000' which was uniformly sampled (e.g. approx 1 in every 353 documents was sampled for topic 403, as per below).</p><p>The task organizers produced a preliminary set of judgments (qrels.t11legallearn.prelim) on October 16, 2011, in time for the October 24 notebook paper deadline. The final set of judgments (qrels.t11legallearn) were released during the conference (November <ref type="bibr" coords="10,279.65,216.53,8.54,8.74" target="#b14">[15]</ref><ref type="bibr" coords="10,288.19,216.53,4.27,8.74" target="#b15">[16]</ref><ref type="bibr" coords="10,288.19,216.53,4.27,8.74" target="#b16">[17]</ref><ref type="bibr" coords="10,292.46,216.53,12.81,8.74" target="#b17">[18]</ref>. In this paper, we just use the final set of judgments.</p><p>Based on the final judgments in qrels.t11legallearn, we produced our own counts of the number of documents in each stratum, the number judged in each stratum, and the ratio (which is the probability of each document in that stratum being chosen for judging), which are listed here: Topic 401: stratum 100: count=4309, judged=4308, prob=0.999767927593 stratum 1000: count=681283, judged=1563, prob=0.002294200795 Topic 402: stratum 100: count=3689, judged=3689, prob=1.000000000000 stratum 1000: count=681903, judged=1894, prob=0.002777521143 Topic 403: stratum 100: count=3615, judged=3615, prob=1.000000000000 stratum 1000: count=681977, judged=1930, prob=0.002830007464</p><p>The counts for each topic should sum to 685,592 (the number of documents in the collection). The number of judged documents was between 5500 and 6000 for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Score Coarseness Issue</head><p>The aforementioned sampling into 2 strata led to a coarseness issue with the scores. For example, for the otL11FTM run, if you cutoff the ranked list for topic 403 at 1341 items, the estimated recall is 30%, the estimated precision is 41%, and the estimated F 1 is 35%. But if you cutoff the ranked list just one document later, i.e. at 1342 items, the estimated recall jumps to 59%, the estimated precision jumps to 58%, and the estimated F 1 jumps to 58%.</p><p>Why does retrieving just one more document make such a difference to the estimated scores? For topic 403, as shown in the previous section, the sampling rate of the 2nd stratum was approximately 1 in 353.4 (the number 353.4 comes from dividing the listed number of documents in stratum, 681977, by the listed number of judged documents in the stratum, 1930). Of the 534 documents judged relevant for topic 403, just 2 of them were in the 2nd stratum (the other 532 were in the 1st stratum, which was fully judged). The total estimated number of relevant documents for topic 403 is 1239 (which comes from 532 plus 2 times 353.4). In the case of run otL11FTM, the document ranked at position 1342 for topic 403 was one of the relevant documents from that 2nd stratum, hence including it added approximately 353.4 to the estimated number of relevant documents retrieved, which compared to the total estimate of 1239 relevant documents leads to the aforementioned jump in recall. (At rank 1341, the estimated recall comes from 376/1239 and the estimated precision comes from 376/(376+532). At rank 1342, the estimated recall comes from 729/1239 and the estimated precision comes from 729/(729+532).)</p><p>Even without increasing the number of judgments, the coarseness issue could be reduced a lot with a different sampling strategy, like the one used in the 2009 Batch Task <ref type="bibr" coords="10,380.54,667.81,9.96,8.74" target="#b5">[6]</ref>, for which (simplifying a bit) the sampling rate was p(d)=C/hiRank(d), where hiRank(d) was the highest rank at which any submission ranked document d (where 1 is the highest possible rank), and C was chosen so that the probabilities summed to the number of judgments that could be done. With this approach, the impact of 1 document at rank 1342 on recall would probably have been less than 0.10, instead of 0.29.</p><p>With a small number of strata, it's hard to not have dry spots where the coarseness is high (e.g. for the topic 403 sampling, the driest spot would be documents of hiRank just past 100 where the weight of 1 document might be 353 which swamps all the previous judgments; by rank 10000 the 1/353 sampling rate is probably fine.) Using the non-uniform sampling formula p(d)=C/hiRank(d) is an easy way to treat all potential cutoffs equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">L07 vs. L10 measures</head><p>In 2010, the task organizers developed a new approach to estimating scores from the samples, based on individually estimating the precision on each stratum and then extrapolating over the entire stratum. We call this approach the "L10" approach, in contrast to the "L07" approach that was used the previous 3 years (for which we led the design when helping to coordinate the task). The L07 approach essentially assigned a fixed weight to each judged document based on the reciprocal of its probability of being judged. We reported on the track mailing list (Oct 18, 2010) that the new L10 approach had some anomalies, such as that if set D1 was a strict superset of set D2, it could still estimate recall(D1) to be less than recall(D2). Furthermore, the recall of a set could be estimated to be greater than 100%. (These particular anomalies could not happen with the L07 approach.) In this paper, we generally just report the L07-based scores (re-using scripts that we had set up in past years).</p><p>To compute the L07-based scores, we created a qrelsL11.probs file (for use with the l07 eval scoring utility) by taking the judged documents from qrels.t11legallearn and assigning them the probability as listed in the previous section. Last year we reported some example comparisons of F 1 @K scores from the two approaches and found that they were almost the same, suggesting that both estimation approaches are likely to lead to similar conclusions <ref type="bibr" coords="11,202.27,374.42,14.61,8.74" target="#b18">[19]</ref>.</p><p>Note: The detailed L07 formulas for estimating the number of relevant and non-relevant documents for each topic, and also for estimating precision and recall, were reported in the 2007 Ad Hoc task section of <ref type="bibr" coords="11,72.00,410.29,14.61,8.74" target="#b20">[21]</ref>, and the detailed formulas for estimating F 1 were reported in the 2008 Ad Hoc task section of <ref type="bibr" coords="11,500.23,410.29,14.61,8.74" target="#b9">[10]</ref>. The l07 eval software used to compute the L07 evaluation measures is online at http://trec.nist.gov/data/ legal09.html .</p><p>In this paper, we actually used the following simpler versions of the L07 estimators, which are analyzed in more detail in <ref type="bibr" coords="11,147.85,458.11,14.61,8.74" target="#b19">[20]</ref>: estRel@k = sum(d is Rel)(1/p(d)) estNon@k = sum(d is Non)(1/p(d)) recall@k = estRel@k / estRel@D prec@k = estRel@k / (estRel@k + estNon@k)</p><p>To summarize, the L07 estimators have the following advantages over the L10 estimators:</p><p>• recall is non-decreasing as more results are retrieved (monotonicity property)</p><p>• recall is always in the 0..1 range</p><p>• the qrels only need to list the few thousand judged documents, not all 680,000+ documents</p><p>• they work naturally with non-uniform probability formulas such as the p(d) formula recommended earlier (no need to define strata).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">L07 measures</head><p>For each topic, we report a table of set-based scores (Tables <ref type="table" coords="11,327.62,664.08,4.29,8.74" target="#tab_3">1</ref><ref type="table" coords="11,331.91,664.08,4.29,8.74" target="#tab_4">2</ref><ref type="table" coords="11,336.20,664.08,4.29,8.74" target="#tab_5">3</ref>) and a table of rank-based scores (Tables <ref type="table" coords="11,520.08,664.08,3.99,8.74" target="#tab_6">4</ref><ref type="table" coords="11,524.06,664.08,3.99,8.74" target="#tab_7">5</ref><ref type="table" coords="11,528.05,664.08,3.99,8.74">6</ref>).</p><p>Of the various measures, probably the most informative set-based measure is F 1 @K and most informative rank-based measure is F 1 @R.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,72.00,280.54,88.92,8.30;7,197.53,280.54,73.23,8.30;7,72.00,292.49,88.92,8.30;7,197.53,292.49,68.00,8.30;7,72.00,304.45,88.92,8.30;7,197.53,304.45,68.00,8.30;7,86.94,325.65,453.05,8.74;7,72.00,337.60,468.00,8.74;7,72.00,349.56,167.18,8.74;7,72.00,372.19,214.44,8.30;7,72.00,384.15,214.44,8.30;7,72.00,396.10,214.44,8.30"><head></head><label></label><figDesc>prelim: :est_rel: 401 30852.8914 prelim: :est_rel: 402 1920.0998 prelim: :est_rel: 403 1238.7119 At about the time of the conference (November 15-18, 2011), the final relevance assessments were released. Here are the counts of the number of judged documents, relevant documents and non-relevant documents for each topic in the final assessments: Topic 401: count=5871, rel=2621, non=3250 Topic 402: count=5583, rel= 858, non=4725 Topic 403: count=5545, rel= 534, non=5011</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.00,461.86,193.52,32.21"><head>Table 1 :</head><label>1</label><figDesc>Set-based Scores for Topic 401 (20016.9 Est. Relevant Documents)</figDesc><table coords="7,72.00,461.86,193.52,32.21"><row><cell>final: :est_rel:</cell><cell>401 20016.8646</cell></row><row><cell>final: :est_rel:</cell><cell>402 3012.1996</cell></row><row><cell>final: :est_rel:</cell><cell>403 1238.7119</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,139.69,462.97,332.62,196.58"><head>Table 2 :</head><label>2</label><figDesc>Set-based Scores for Topic 402 (3012.2 Est. Relevant Documents)</figDesc><table coords="8,139.69,499.78,332.62,159.77"><row><cell>Run</cell><cell cols="2">K Recall Precision</cell><cell>F 1</cell><cell>Num. Judged</cell></row><row><cell>otL11HTM</cell><cell>923 0.270</cell><cell>0.689</cell><cell>0.389</cell><cell>486 (335r, 151n, 0g)</cell></row><row><cell>otL11FTM</cell><cell>304 0.215</cell><cell>0.905</cell><cell>0.347</cell><cell>294 (266r, 28n, 0g)</cell></row><row><cell>mopup-rels</cell><cell>245 0.195</cell><cell>0.984</cell><cell>0.325</cell><cell>245 (241r, 4n, 0g)</cell></row><row><cell>otL11BTM</cell><cell>1808 0.281</cell><cell>0.381</cell><cell>0.323</cell><cell>561 (348r, 213n, 0g)</cell></row><row><cell>otL11FT2</cell><cell>3236 0.519</cell><cell>0.149</cell><cell>0.231</cell><cell>807 (291r, 516n, 0g)</cell></row><row><cell>otL11HT2</cell><cell>2860 0.220</cell><cell>0.123</cell><cell>0.158</cell><cell>815 (273r, 542n, 0g)</cell></row><row><cell>otL11BT2</cell><cell>3017 0.208</cell><cell>0.101</cell><cell>0.136</cell><cell>795 (258r, 537n, 0g)</cell></row><row><cell>otL11BT1</cell><cell>12739 0.585</cell><cell>0.057</cell><cell cols="2">0.104 1388 (372r, 1016n, 0g)</cell></row><row><cell>otL11HT1</cell><cell>11370 0.586</cell><cell>0.054</cell><cell>0.100</cell><cell>1368 (373r, 995n, 0g)</cell></row><row><cell>otL11FT1</cell><cell>14442 0.533</cell><cell>0.040</cell><cell>0.074</cell><cell>1078 (308r, 770n, 0g)</cell></row><row><cell>mopup-nons</cell><cell>1954 0.019</cell><cell>0.012</cell><cell>0.015</cell><cell>1954 (24r, 1930n, 0g)</cell></row><row><cell>fullsetL11</cell><cell>685592 1.000</cell><cell>0.002</cell><cell cols="2">0.004 5545 (534r, 5011n, 0g)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,143.48,673.23,325.03,8.74"><head>Table 3 :</head><label>3</label><figDesc>Set-based Scores for Topic 403 (1238.7 Est. Relevant Documents)</figDesc><table coords="9,105.26,97.40,401.48,123.78"><row><cell>Run</cell><cell>Retrieved P@B</cell><cell cols="5">R@B F 1 @R R@ret indAP GS10J First 10 Ret</cell></row><row><cell>otL11HTM</cell><cell>685592 0.651</cell><cell cols="3">0.212 0.383 1.000</cell><cell>0.940</cell><cell>1.000 RRRRRNRRRR</cell></row><row><cell>otL11FTM</cell><cell cols="3">685592 0.691 0.255 0.371</cell><cell>1.000</cell><cell>0.946</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HT2</cell><cell>685592 0.591</cell><cell>0.205</cell><cell>0.306</cell><cell>1.000</cell><cell>0.916</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11FT2</cell><cell>685592 0.675</cell><cell>0.242</cell><cell>0.306</cell><cell>1.000</cell><cell>0.916</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HT1</cell><cell>685592 0.550</cell><cell>0.204</cell><cell>0.273</cell><cell>1.000</cell><cell>0.880</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11BTM</cell><cell>685592 0.610</cell><cell>0.213</cell><cell>0.246</cell><cell>1.000</cell><cell>0.908</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11BT1</cell><cell>685592 0.593</cell><cell>0.206</cell><cell>0.239</cell><cell>1.000</cell><cell>0.869</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11BT2</cell><cell>685592 0.593</cell><cell>0.206</cell><cell>0.239</cell><cell>1.000</cell><cell>0.870</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11FT1</cell><cell>685592 0.392</cell><cell>0.120</cell><cell>0.196</cell><cell>1.000</cell><cell>0.713</cell><cell>0.926 NRRRRNNRRR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,105.26,234.58,401.48,196.86"><head>Table 4 :</head><label>4</label><figDesc>Rank-based Scores for Topic 401 (B=8241, R=20017)</figDesc><table coords="9,105.26,307.65,401.48,123.79"><row><cell>Run</cell><cell>Retrieved P@B</cell><cell cols="5">R@B F 1 @R R@ret indAP GS10J First 10 Ret</cell></row><row><cell>otL11FTM</cell><cell cols="4">685592 0.231 0.318 0.210 1.000</cell><cell>0.780</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11FT2</cell><cell>685592 0.217</cell><cell>0.297</cell><cell>0.194</cell><cell>1.000</cell><cell>0.731</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HT1</cell><cell>685592 0.127</cell><cell>0.156</cell><cell>0.156</cell><cell>1.000</cell><cell>0.604</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HTM</cell><cell>685592 0.121</cell><cell>0.175</cell><cell>0.137</cell><cell>1.000</cell><cell>0.702</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11BTM</cell><cell>685592 0.094</cell><cell>0.144</cell><cell>0.134</cell><cell>1.000</cell><cell>0.609</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11FT1</cell><cell>685592 0.097</cell><cell>0.123</cell><cell>0.113</cell><cell>1.000</cell><cell>0.597</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HT2</cell><cell>685592 0.106</cell><cell>0.153</cell><cell>0.101</cell><cell>1.000</cell><cell>0.615</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11BT1</cell><cell>685592 0.075</cell><cell>0.115</cell><cell>0.098</cell><cell>1.000</cell><cell>0.419</cell><cell>1.000 RRNRRRRRNR</cell></row><row><cell>otL11BT2</cell><cell>685592 0.074</cell><cell>0.113</cell><cell>0.097</cell><cell>1.000</cell><cell>0.435</cell><cell>1.000 RRRRRRRRRR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,105.26,444.84,401.48,218.99"><head>Table 5 :</head><label>5</label><figDesc>Rank-based Scores for Topic 402 (B=5085, R=3013)</figDesc><table coords="9,105.26,517.91,401.48,145.92"><row><cell>Run</cell><cell>Retrieved P@B</cell><cell cols="5">R@B F 1 @R R@ret indAP GS10J First 10 Ret</cell></row><row><cell>otL11BTM</cell><cell>685592 0.041</cell><cell cols="3">0.638 0.384 1.000</cell><cell>0.729</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11FTM</cell><cell cols="2">685592 0.044 0.674</cell><cell>0.347</cell><cell>1.000</cell><cell>0.796</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HTM</cell><cell cols="3">685592 0.042 0.684 0.329</cell><cell>1.000</cell><cell>0.760</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11BT2</cell><cell>685592 0.038</cell><cell>0.595</cell><cell>0.222</cell><cell>1.000</cell><cell>0.345</cell><cell>1.000 RRRRNRRRRR</cell></row><row><cell>otL11BT1</cell><cell>685592 0.038</cell><cell>0.595</cell><cell>0.214</cell><cell>1.000</cell><cell>0.286</cell><cell>0.681 NNNNNRRNNR</cell></row><row><cell>otL11FT2</cell><cell>685592 0.038</cell><cell>0.625</cell><cell>0.205</cell><cell>1.000</cell><cell>0.419</cell><cell>1.000 RRRRRRRRRR</cell></row><row><cell>otL11HT2</cell><cell>685592 0.041</cell><cell>0.628</cell><cell>0.190</cell><cell>1.000</cell><cell>0.389</cell><cell>0.926 NRRRRRRRRR</cell></row><row><cell>otL11HT1</cell><cell>685592 0.037</cell><cell>0.600</cell><cell>0.149</cell><cell>1.000</cell><cell>0.295</cell><cell>0.857 NNRRNNNNRN</cell></row><row><cell>otL11FT1</cell><cell>685592 0.034</cell><cell>0.558</cell><cell>0.100</cell><cell>1.000</cell><cell>0.269</cell><cell>0.857 NNRRNNNNRN</cell></row><row><cell></cell><cell cols="6">Table 6: Rank-based Scores for Topic 403 (B=19435, R=1239)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,660.96,452.76,6.99;1,72.00,670.43,468.00,6.99;1,72.00,679.89,468.00,6.99;1,72.00,689.36,340.59,6.99"><p>OpenText, Open Text eDOCS SearchServer and Open Text eDOCS Suite are trademarks or registered trademarks of Open Text Corporation in the United States of America, Canada, the European Union and/or other countries. This list of trademarks is not exhaustive. Other trademarks, registered trademarks, product names, company names, brands and service names mentioned herein are property of Open Text Corporation or other respective owners.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>F 1 @R is easy to interpret because depth R (where R is the estimated number of relevant documents) is the special depth at which recall, precision and F 1 all have the same value. For F 1 @R, just the relative probabilities of relevance matter (i.e., the ranking) not the absolute probabilities.</p><p>F 1 @K is a more challenging measure because the system has to choose the depth K at which to be evaluated. This K value is implied by the absolute probabilities of relevance calculated by the system. Normally the best K value is approximately the same as R (which, of course, is not known to the system in advance). If F 1 @K is substantially less than F 1 @R, then the system likely substantially overestimated or underestimated the probabilities of relevance. The F 1 measure requires both high precision and high recall to achieve a high score. Overestimating the probabilities leads to too high a K value and typically lowers precision substantially, which in turn lowers F 1 . Underestimating the probabilities leads to too low a K value and typically lowers recall substantially, which in turn lowers F 1 .</p><p>In the tables of set-based scores, we include not just the 9 experimental submissions, but also 3 reference runs as follows:</p><p>• "mopup-rels" is the set of example relevant documents (released August 30, 2011) for use in the "Mopup" runs. Some of the example relevant documents ended up being judged non-relevant in the final judgments, as per Tables <ref type="table" coords="12,208.90,260.31,4.01,8.74">1</ref><ref type="table" coords="12,212.92,260.31,4.01,8.74">2</ref><ref type="table" coords="12,216.93,260.31,4.01,8.74">3</ref>.</p><p>• "mopup-nons" is the set of example non-relevant documents (released August 30, 2011) for use in the "Mop-up" runs. Some of the example non-relevant documents ended up being judged relevant in the final judgments, as per Tables <ref type="table" coords="12,231.04,303.07,4.01,8.74">1</ref><ref type="table" coords="12,235.05,303.07,4.01,8.74">2</ref><ref type="table" coords="12,239.07,303.07,4.01,8.74">3</ref>.</p><p>• "fullsetL11" is the set consisting of the entire document collection.</p><p>The tables of set-based measures have the following columns:</p><p>• "K": For the 9 submitted runs, K came from the organizer-provided "Cutoff Estimate", i.e. the cutoff at which the (L10) F 1 would be expected to be maximized if the run's probabilities of relevance were accurate. (Note that the K value is a property of the run and is computable before the relevance judgments are known.) For the 3 reference runs, K is the size of the set.</p><p>• "R@K": The estimated recall at depth K. Recall is the estimated number of relevant documents retrieved (at depth K) divided by the estimated total number of relevant documents (in the entire collection). (From ":est K-Recall:" in l07 eval output.)</p><p>• "P@K": The estimated precision at depth K. Precision is the estimated number of relevant documents retrieved (at depth K) divided by the sum of the estimated number of relevant and non-relevant documents (at depth K). (From ":est K-Prec:" in l07 eval output.)</p><p>• "F 1 @K": The estimated F 1 score at depth K. F 1 is 2*Precision*Recall/(Precision+Recall) or 0 if both Precision and Recall are 0. (Note that this F 1 formula is only applicable for individual topics; the mean F 1 across topics may differ from plugging the mean precision and recall into the formula.) (From ":est K-F1:" in l07 eval output.)</p><p>• "Num. Judged@K" is the actual number of judged documents in the top-K, followed in parentheses by the actual number of judged relevant (r), non-relevant (n) and gray (g) documents. Note that because not all documents were drawn for judging with the same probability, the estimated numbers of relevant and non-relevant documents in a result set are not in general exactly proportional to the drawn numbers. (From ":K-jg ret:", ":K-rel ret:", ":K-nonrel ret:" and ":K-gray ret:" in l07 eval output respectively.)</p><p>The table caption reports the estimated number of relevant documents for the topic. The tables of rank-based measures have the following columns:</p><p>• "P@B" and "R@B": Estimated Precision and Recall at Depth B (where B is the number of documents matching our experimental Boolean query (used for otL11BT1), which is listed in the table caption).</p><p>(From ":est PB:" and ":est RB:" in l07 eval output respectively.)</p><p>• "F 1 @R": Estimated F 1 at Depth R (where R is the estimated number of relevant documents, which is listed in the table caption). (From ":est R-F1:" in l07 eval output.)</p><p>• "R@ret": Estimated Recall of the full retrieval set.</p><p>• "indAP": Induced Average Precision (the popular "average precision" after discarding unjudged documents; the sampling probabilities are not used for this measure, i.e. indAP is not infAP or statAP).</p><p>(From ":mapJudged:" in l07 eval output.)</p><p>• "GS10J": Generalized Success@10 on Judged Documents (1.08 1-r where r is the rank of the first relevant document, only counting judged documents, or zero if no relevant document is retrieved). GS10J is a robustness measure which exposes the downside of blind feedback techniques <ref type="bibr" coords="13,459.82,194.71,14.61,8.74" target="#b12">[13]</ref>. "Generalized Success@10" was originally introduced as "First Relevant Score" (FRS) in <ref type="bibr" coords="13,427.08,206.67,14.61,8.74" target="#b13">[14]</ref>. Intuitively, GS10J is a predictor of the percentage of topics for which a relevant document is returned in the first 10 rows.</p><p>(From ":GS10J:" in l07 eval output.)</p><p>• "First 10 Ret": The judgments of the top-10 ranked documents of the run. 'R' indicates judged relevant. 'N' indicates judged non-relevant. (From ":relstring:" in l07 eval output.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The Learning Task of the TREC 2011 Legal Track investigated the effectiveness of e-Discovery search techniques at selecting training examples and learning from them to estimate the probability of relevance of every document in a collection. The task specified 3 test topics, each of which included a one-sentence request for documents to produce from a target collection of 685,592 e-mail messages and attachments. For our participation, we produced nine retrieval sets to compare experimental feedback-based, topic-based and Boolean-based techniques. In this paper, we described the experimental approaches used and reported the scores that each achieved on each topic on various set-based and rank-based measures. Generally speaking, approaches based on relevance feedback were found to outperform the other approaches. We also identified a coarseness issue with the estimated scores and recommended a sampling approach and associated estimators for addressing this issue.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,86.71,467.31,453.29,7.86;13,82.52,478.27,457.48,7.86;13,82.52,489.23,20.99,7.86" xml:id="b0">
	<monogr>
		<title level="m" coord="13,228.71,467.31,311.30,7.86;13,82.52,478.27,342.19,7.86">The Sedona Conference R Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery. The Sedona Conference Journal</title>
		<editor>
			<persName><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">VIII</biblScope>
			<biblScope unit="page" from="189" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,86.71,508.16,453.28,7.86;13,82.52,519.12,320.24,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<title level="m" coord="13,154.79,508.16,385.21,7.86;13,82.52,519.12,190.69,7.86">Toward A Federal Benchmarking Standard for Evaluating Information Retrieval Products Used in E-Discovery. The Sedona Conference Journal</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">VI</biblScope>
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,86.71,538.05,453.29,7.86;13,82.52,549.01,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,310.10,538.05,135.81,7.86">TREC-2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,452.97,538.05,87.03,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,86.71,567.94,317.80,8.12" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><surname>Cross-Language</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="13,153.52,567.94,107.63,7.86">Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,86.71,586.87,453.29,7.86;13,82.52,597.83,202.48,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,403.10,586.87,136.90,7.86;13,82.52,597.83,21.14,7.86">Overview of the TREC 2011 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,172.92,597.83,107.87,7.86">Proceedings of TREC 2011</title>
		<meeting>TREC 2011</meeting>
		<imprint/>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct coords="13,86.71,616.75,453.29,7.86;13,82.52,627.71,140.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,395.85,616.75,144.15,7.86;13,82.52,627.71,21.14,7.86">Overview of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,110.95,627.71,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,86.71,646.64,453.29,7.86;13,82.52,657.60,20.99,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,160.04,646.64,196.98,7.86">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,363.63,646.64,172.11,7.86">Sixteenth International Unicode Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,86.71,676.53,421.60,9.85" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,124.30,676.53,154.83,7.86">NII-Test Collection for IR) Home Page</title>
		<author>
			<persName coords=""><surname>Ntcir</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/~ntcadm/index-en.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,86.71,75.84,453.29,7.86;14,82.52,86.80,432.53,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,459.67,75.84,80.33,7.86;14,82.52,86.80,130.48,7.86">Evaluation of Information Retrieval for E-Discovery</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,220.23,86.80,119.74,7.86">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="347" to="386" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,105.73,448.68,7.86;14,82.52,116.68,140.50,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,397.24,105.73,142.76,7.86;14,82.52,116.68,21.14,7.86">Overview of the TREC 2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,110.95,116.68,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,135.61,448.67,7.86;14,82.52,146.57,60.66,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="14,402.18,135.61,137.82,7.86;14,82.52,146.57,31.38,7.86">Okapi at TREC-3. Proceedings of TREC-3</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,165.50,297.17,8.11" xml:id="b11">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="14,91.32,165.50,190.87,7.86">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,184.43,448.68,7.86;14,82.52,195.39,49.14,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="14,173.53,184.43,308.73,7.86">Early Precision Measures: Implications from the Downside of Blind Feedback</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="705" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,214.32,375.86,7.86;14,467.18,212.55,11.72,5.24;14,481.64,214.32,58.36,7.86;14,82.52,225.28,89.95,7.86;14,203.31,225.28,64.32,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<title level="m" coord="14,172.39,214.32,294.79,7.86;14,467.18,212.55,11.72,5.24;14,481.64,214.32,58.36,7.86;14,82.52,225.28,89.95,7.86;14,203.31,225.28,59.57,7.86">European Ad Hoc Retrieval Experiments with Hummingbird SearchServer TM at CLEF 2005. Working Notes for the 2005 Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,244.21,448.68,7.86;14,82.52,255.17,112.07,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,172.69,244.21,363.09,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2006 Legal Discovery Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,82.52,255.17,88.01,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,274.09,448.68,7.86;14,82.52,285.05,112.07,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,172.69,274.09,363.09,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2007 Legal Discovery Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,82.52,285.05,107.87,7.86">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,303.98,448.68,7.86;14,82.52,314.94,79.92,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,173.21,303.98,366.79,7.86;14,82.52,314.94,55.85,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2008 Legal Track. Proceedings of TREC</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,333.87,448.68,7.86;14,82.52,344.83,79.92,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="14,173.21,333.87,324.70,7.86">Experiments with the Negotiated Boolean Queries of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Proceedings of TREC</note>
</biblStruct>

<biblStruct coords="14,91.32,363.76,439.83,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,174.06,363.76,237.72,7.86">Learning Task Experiments in the TREC 2010 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,419.09,363.76,107.87,7.86">Proceedings of TREC 2010</title>
		<meeting>TREC 2010</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,382.69,448.68,7.86;14,82.52,393.65,404.62,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,243.32,382.69,198.55,7.86">Measuring Effectiveness in the TREC Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,212.78,393.65,206.95,7.86">Current Challenges in Patent Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lupu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mayer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trippe</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,412.58,448.68,7.86;14,82.52,423.53,140.50,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,401.42,412.58,138.58,7.86;14,82.52,423.53,21.14,7.86">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,110.95,423.53,107.87,7.86">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.32,442.46,448.68,9.85;14,82.52,454.07,75.32,7.47" xml:id="b21">
	<monogr>
		<ptr target="http://http://plg.uwaterloo.ca/~gvcormac/legal11/treclegal11.html" />
		<title level="m" coord="14,91.32,442.46,210.83,7.86">TREC 2011 Legal Track -Learning Task Guidelines</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
