<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,70.80,72.44,468.16,16.59;1,78.72,92.36,452.17,16.59">University of Glasgow at TREC 2011: Experiments with Terrier in Crowdsourcing, Microblog, and Web Tracks</title>
				<funder>
					<orgName type="full">Amazon&apos;s Mechanical Turk</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,103.92,137.94,99.24,11.06"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
							<email>richardm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.64,137.94,87.17,11.06"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.48,137.94,111.82,11.06"><forename type="first">Rodrygo</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
							<email>rodrygo@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,447.96,137.94,57.82,11.06"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,70.80,72.44,468.16,16.59;1,78.72,92.36,452.17,16.59">University of Glasgow at TREC 2011: Experiments with Terrier in Crowdsourcing, Microblog, and Web Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A1C18E889649B2818C5C76843F7DCD7E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2011, we focus on tackling the new challenges proposed by the pilot Crowdsourcing and Microblog tracks, using our Terrier Information Retrieval Platform. Meanwhile, we continue to build upon our novel xQuAD framework and data-driven ranking approaches within Terrier to achieve effective and efficient ranking for the TREC Web track. In particular, the aim of our Microblog track participation is the development of a learning to rank approach for filtering within a tweet ranking environment, where tweets are ranked in reverse chronological order. In the Crowdsourcing track, we work to achieve a closer integration between the crowdsourcing marketplaces that are used for relevance assessment, and Terrier, which produces the document rankings to be assessed. Moreover, we focus on generating relevance assessments quickly and at a minimal cost. For the Web track, we enhance the data-driven learning support within Terrier by proposing a novel framework for the fast computation of document features for learning to rank.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In TREC 2011, we participate in the Web adhoc and diversity tasks, the Microblog real-time search task, and the Crowdsourcing assessment and consensus tasks. Our focus is the improvement of the support for data-driven ranking models within the Terrier Information Retrieval (IR) platform <ref type="bibr" coords="1,75.96,502.62,13.37,8.66" target="#b19">[19]</ref>, the effective application of these models for new tasks, e.g. microblog search, and the development of new features for these tasks.</p><p>In the assessment task of the Crowdsourcing track <ref type="bibr" coords="1,263.02,533.94,13.37,8.66" target="#b13">[13]</ref>, we propose a close integration of a crowdsourcing infrastructure into our Terrier IR platform, for achieving fast generation of relevance assessments based upon Amazon's Mechanical Turk (MTurk) <ref type="foot" coords="1,112.56,574.42,3.65,4.10" target="#foot_0">1</ref> . For the consensus task, we propose and evaluate a machine learning approach, which leverages prior worker accuracy, topic difficulty and worker impact for calculating consensus between multiple assessors.</p><p>The major goal of our participation within the Microblog track <ref type="bibr" coords="1,76.80,628.14,14.19,8.66" target="#b20">[20]</ref> is to determine whether filtering is an effective approach to improve the relevance and quality of a tweet ranking. In particular, we propose a learning to rank approach that uses multiple features extracted from each tweet to determine if that tweet should be filtered from the ranking.</p><p>In our participation in the Web track <ref type="bibr" coords="1,211.68,680.34,9.11,8.66" target="#b8">[8]</ref>, our primary goal is to improve the underlying learning infrastructure within Terrier, such that it becomes easier to generate and evaluate many data-driven models in a short timeframe. In particular, in the adhoc task, we propose a framework for the fast computation of multiple query-dependent features for learning to rank. In the diversity task, we leverage learned models within our state-of-the-art xQuAD framework <ref type="bibr" coords="1,501.12,269.10,14.19,8.66" target="#b24">[24]</ref> for search result diversification, so as to learn both the relevance of a document to a query and its coverage of the multiple aspects underlying this query.</p><p>The remainder of this paper is structured as follows. In Sections 2 and 3, we describe our participation in the Crowdsourcing track assessment and consensus tasks, respectively. Section 4 details our participation in the new Microblog track real-time search task. In Sections 5 and 6, we describe our Web track adhoc task and Web track diversity task participations, respectively. Conclusions are provided in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CROWDSOURCING TRACK: ASSESSMENT TASK</head><p>The aim of the assessment task of the Crowdsourcing track is to develop effective approaches to generate accurate relevance assessments for Web documents in a fast and cheap manner <ref type="bibr" coords="1,350.52,462.90,13.37,8.66" target="#b13">[13]</ref>. For our participation in the assessment task, we integrate a crowdsourcing infrastructure into our Terrier IR platform, to achieve the fast generation of relevance assessments based upon Amazon's Mechanical Turk (MTurk). The aim of integrating this crowdsourcing infrastructure is to decrease the time, effort and expertise required to crowdsource relevance assessments for novel tasks and new collections. In particular, through a closer integration to the Terrier platform that performs the searches being evaluated, aspects of the crowdsourcing, e.g. breaking down searches into crowdsourcable jobs, can be automated.</p><p>Our proposed integrated infrastructure extends Terrier with two types of functionalities facilitating fast and easy crowdsourcing of relevance assessments. Firstly, the infrastructure facilitates automatic generation of one or more relevance assessments for a Web document. In particular, it takes lists of ranked results returned by Terrier and automatically constructs a series of assessment tasks for the Web documents within the ranked results. Each assessment task is expressed as MTurk Human Intelligence Task (HIT). Each HIT is operationalised as a MTurk ExternalQuestion <ref type="bibr" coords="1,527.12,672.06,9.63,8.66" target="#b3">[3]</ref> that redirects workers to a modified instance of the Terrier Web interface<ref type="foot" coords="1,351.00,691.66,3.65,4.10" target="#foot_1">2</ref> . This Web interface is hosted on our local ma-  chines and is used by the workers to assess each document.</p><p>With regard to the Web interface design, we focus on achieving fast turn-around times, with only 1-click needed to assess each document. We also use pre-rendered Web pages in conjunction with a 'floating' assessment panel, which is always visible in the assessment interface, hence minimising the worker's scrolling effort. The interface used is shown in Figure <ref type="figure" coords="2,82.80,407.58,3.56,8.66" target="#fig_0">1</ref>.</p><p>Secondly, the proposed infrastructure provides automatic and supervised validation for quality assurance on the crowdsourced assessments produced. In particular, our validation is a two-stage process. In the first, the time spent on the task was used to automatically identify poorly performing workers <ref type="bibr" coords="2,68.76,470.34,13.37,8.66" target="#b15">[15]</ref>. During the second stage, we perform a variant of gold judgement validation <ref type="bibr" coords="2,162.10,480.78,13.37,8.66" target="#b12">[12]</ref>. From the crowdsourced assessments, a 10% subset is selected that maximises the coverage of the workers participating in the assessment task, while also accounting for the number of assessments that each worker has made, i.e. more assessments from a prolific worker will be validated than from a worker that judged only a single document. The selected assessments are manually assessed by the crowdsourcer. The resultant validation assessments are used to identify workers that are not completing the assessment task in good faith, e.g. workers that are just randomly clicking.</p><p>We submitted one run, namely uogTrP1rg, which uses our integrated crowdsourcing infrastructure to generate relevance assessments using Amazon's Mechanical Turk. Assessments were generated for the test topics over the course of a three-day period. Notably, in contrast to standard crowdsouring practices <ref type="bibr" coords="2,150.47,648.18,13.37,8.66" target="#b15">[15]</ref>, only one assessment per document was collected. This assured that the crowdsourcing costs incurred were kept very low. Indeed, we payed about US$39 for our participation. As per the TREC submission guidelines <ref type="bibr" coords="2,97.18,689.94,13.37,8.66" target="#b13">[13]</ref>, assessments marked for rejection from our validation process were also included in the run (this will degrade run performance). Runs were evaluated in compar-ison to two ground truths, namely: TREC assessors; and the majority vote of all submitted runs to the Crowdsourcing track (Consensus). Table <ref type="table" coords="2,438.94,203.34,4.61,8.66" target="#tab_0">1</ref> reports the performance of uogTrP1rg compared to the TREC median. Performance is reported in terms of classification measures (higher is better) and error measures (lower is better).</p><p>From Table <ref type="table" coords="2,374.99,245.10,3.56,8.66" target="#tab_0">1</ref>, we observe that uogTrP1rg marginally underperformed the TREC median, except under Recall, in comparison to the TREC assessor ground truth. However, it markedly outperformed the TREC median under all measures but Specificity on the consensus ground truth. This shows that our approach generated assessments of similar quality to that of other participating systems. Overall, bearing in mind the limited resources allocated, i.e. only US$39, we believe that achieving such a reasonable performance validates the effectiveness of our proposed infrastructure for automatic crowdsourcing of relevance assessments. Moreover, if the assessments that were marked for rejection during validation but included in the run were removed, it is expected that performance will be markedly increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CROWDSOURCING TRACK: CONSENSUS TASK</head><p>The aim of the consensus task of the Crowdsourcing track is to investigate techniques for aggregating multiple assessments of varying quality for a single document into a single high-quality assessment for that document, i.e. calculate consensus among multiple assessors <ref type="bibr" coords="2,460.40,472.26,13.37,8.66" target="#b13">[13]</ref>. Participants calculate consensus across five crowdsourced assessments for Web documents. For our participation in the consensus task, we propose a data-driven approach that learns a model for consensus calculation. Our approach trains a model comprised of three features, namely: prior worker accuracy -how well the worker performed on previous assessments; topic difficulty -how much disagreement there is between workers for the topic; and worker impact -what proportion of the assessment task has the worker attempted. We linearly combine these features extracted for an assessment to score that assessment as shown below:</p><formula xml:id="formula_0" coords="2,318.00,610.18,456.06,24.54">Score(a, w, d) = ˛(a -0.5) + N 2 α • Accuracy(w) + β • HIT sDone(w) T otalHIT s + γ • Agreement(d) !w</formula><p>here a is the assessment, w is the worker, d is the document being assessed, Accuracy(w) is the prior accuracy for worker w, HIT sDone(w) is the number of HITs completed by worker w, T otalHIT s is the number of HITs that were available to the worker and Agreement(t) is the level of Kappa Fleiss <ref type="bibr" coords="2,382.06,700.38,14.19,8.66" target="#b11">[11]</ref> agreement between all workers that attempted document d. N is a normalising factor to ensure that the numerator remains within the range [0-1]. α, β and γ are weights for each feature. We learn these weights using a line-search optimisation <ref type="bibr" coords="3,184.65,78.30,9.11,8.66" target="#b6">[6]</ref>. The objective function minimises Root Mean Squared Error (RMSE) on the development topic set provided for the task <ref type="bibr" coords="3,229.42,99.18,13.37,8.66" target="#b13">[13]</ref>. Each new assessment is weighted via the linear combination of feature scores. Weighted assessments for a single document are summed together to provide a final assessment value for that document as shown below:</p><formula xml:id="formula_1" coords="3,70.68,155.37,222.23,30.94">Assessment(A, W, d) = 1 |A| |A| X i=1 Score(Ai, Wi, d)<label>(1)</label></formula><p>where d is the document being assessed, A is the set of assessments for document d, W is the set of workers that made the assessments in A and |A| is the size of A, i.e. the number of assessors.</p><p>For the consensus task, we submitted one run, uogTrP2O4wtr that uses our learned model to calculate consensus judgements. Table <ref type="table" coords="3,108.22,252.42,4.61,8.66" target="#tab_2">2</ref> reports the performance of our uogTrP2O4wtr run at consensus calculation against a ground truth by TREC assessors. Performance is measured in terms of classification based measures (higher is better) and error based measures (lower is better). From Table <ref type="table" coords="3,201.21,294.30,3.56,8.66" target="#tab_2">2</ref>, we observe that under classification metrics, uogTrP2O4wtr achieved less than the TREC median performance. However, under two of the three error based metrics, i.e. Log-Loss and KL-Divergence, uogTrP2O4wtr improved over the TREC median, i.e. a lower overall error was observed. This indicates that our learned model is good at expressing uncertainty, such as when we have no prior evidence about the workers making the assessments. However, as a consequence, assessments from good workers that we have never seen before receive little weight. Overall, our approach generates useful estimates of the performance of a worker when prior evidence exists. However, are currently further developing this approach to better account for cases where little is known about a worker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MICROBLOG TRACK: REAL-TIME SEARCH TASK</head><p>The aim of the TREC Microblog track real-time search task is to investigate approaches to retrieve relevant tweets from before a point in time, when tweets are ranked in reverse-chronological order <ref type="bibr" coords="3,166.91,512.10,13.37,8.66" target="#b20">[20]</ref>. Participants rank tweets from the Tweets11 Twitter corpus for a set of query/timestamp pairs, referred to as topics. For our participation, we investigate a learning to rank approach, with the aim of learning a model that identifies tweets that can be discarded from a reverse-chronologically ordered ranking, hence improving relevance in the top ranks. In particular, this approach learns the features of a tweet that indicate when the tweet should be discarded, e.g. when it has poor relevance to the query or is written in a non-English language.</p><p>The Tweets11 corpus is unusual, in that it is not preprovided by TREC. Instead, a set of approximately 16 million tweet identifiers are provided along with a tool for downloading the tweets for those tweet identifiers <ref type="bibr" coords="3,228.22,648.18,13.37,8.66" target="#b20">[20]</ref>. The corpus is dynamic, i.e. the number of available tweets changes over time, as users delete their own tweets, or spam accounts are removed by Twitter itself. There are two alternate methods via which the tweet downloading tool can collect tweets, namely: JSON -where full details of the tweet are downloaded; or HTML -where instead the HTML page for each tweet is scraped for content. We developed a customised version of the tool that uses the HTML method to download the corpus. Statistics of our version of Tweets11 are provided in Table <ref type="table" coords="3,342.59,209.22,3.56,8.66" target="#tab_1">3</ref>. Using the Terrier IR platform <ref type="bibr" coords="3,480.21,209.22,14.19,8.66" target="#b19">[19]</ref> in conjunction with a new Twitter collection class to enable the parsing of the corpus 3 , we indexed the Tweets11, removing stopwords and stemming each tweet using the English Porter Stemmer. Before applying our proposed learning to rank approach, we first create a time-ordered ranking of tweets from Tweets11 for each topic. In particular, for a topic -comprised of a query/timestamp pair -we retrieve a ranking of tweets containing one or more query terms in reverse-chronological order, which were posted before the timestamp. This ranking is referred to as the sample. The sample has high recall but low precision, i.e. most of the relevant tweets are within the ranking, but due to the reverse chronological order these are unlikely to occur in the top ranks. We then reduce the size of the sample, by removing any tweets not containing one or more hashtags (as we will show later, this proved to be a mistake).</p><p>Next, we filter the sample using our new learning to rank approach. In particular, we define six feature sets, each describing one aspect of the tweets in the sample. Table <ref type="table" coords="3,551.24,407.94,4.61,8.66" target="#tab_3">4</ref> describes these feature sets and lists the number of individual features within each. Notably, the last two feature sets -Referred Page and Twitterer -were extracted from external corpora in a timely fashion, i.e. a crawled corpus of documents linked from Tweets11 and from the Twitter Gardenhose stream, respectively.</p><p>Using the Automatic Feature Selection <ref type="bibr" coords="3,492.36,481.26,14.19,8.66" target="#b17">[17]</ref> learning to rank technique on a separate topic set comprised of 55 topics crowdsourced using Amazon's Mechanical Turk, we trained models for estimating whether tweets should be filtered out. In particular, we train one model using the 66 features from the first four feature sets in Table <ref type="table" coords="3,457.43,533.46,3.56,8.66" target="#tab_3">4</ref>, and two models using the 76 features from all feature sets. The objective function used was Normalised Discounted Accumulative Gain at rank 30 (NDCG@30).</p><p>Furthermore, during the training process, a time-decay function can be applied to the score of each tweet to create a model that promotes more recent tweets. In particular, the time-decay function discounts the score for each tweet as follows:</p><formula xml:id="formula_2" coords="3,327.00,631.50,228.95,28.09">decay(score, age) = score • 1 √ 18 • π • e -age-15 9 + 0.3 (2)</formula><p>where score is the score for a tweet as defined by one of our learned models and age is the age of that tweet in comparison to the query time (in hours). This particular function was chosen to primarily promote tweets made during the 6 hour period prior to the query time.</p><p>We apply a learned model, either with our time-decay function or not, over 66 or 76 features on the tweet sample to create each run. We submitted the following four runs to the TREC Microblog track:</p><p>• uogTrUB2: The sample (with hashtag filtering) before applying the learned model, as a baseline.</p><p>• uogTrLea: uogTrUB2 filtered using a model learned from 66 internal (extracted from the Tweets11 corpus) features: Tweet Relevance, Tweet Quality, Tweet Language and Spam Detection. The time-decay function was not used during training.</p><p>• uogTrLqeabd: uogTrUB2 filtered using the same 66 internal features as uogTrLea, but adds the 10 external features from the Referred Page and Twitterer feature sets. The time-decay function was not used during training.</p><p>• uogTrLqeabdd: uogTrUB2 filtered using the same 76 features as uogTrLqeabd, but where the time-decay function was applied to create a model that promotes more recent tweets.</p><p>Table <ref type="table" coords="4,87.23,658.62,4.61,8.66" target="#tab_4">5</ref> reports our run performances in terms of the mean average precision (MAP) and precision at 30 (P@30) measures for the two official topic sets, namely: High Rel (only highly relevant tweets) and All Rel (all relevant tweets) <ref type="bibr" coords="4,275.96,689.94,13.41,8.66" target="#b20">[20]</ref>. We observe that our runs are sub-median in effectiveness and that our baseline run (uogTrUB2) -that only uses the sample -outperformed runs that added the learned filtering approach. Upon analysis we observed that the hashtag filtering applied to the tweet sample proved to be overly restrictive on the test topics, in that it filtered out many (actually the majority) of relevant tweets. Further filtering of this already over-filtered ranking by the learned model could only harm performance. This shows that many relevant tweets do not use hashtags at all, hence it is a mistake to use it as a filter.</p><p>In light of this, to test our learned filtering approach, we tested two additional unsubmitted runs, uogTrUB2 NoFilter and uogTrLqeabd NoFilter that are also shown in Table <ref type="table" coords="4,548.74,366.06,3.56,8.66" target="#tab_4">5</ref>. uogTrUB NoFilter uses a sample without the hashtag-based reduction or any learned model filtering. Meanwhile, uogTr-Lqeabd NoFilter uses the same learned model as our third official run (uogTrLqeabd), but does not filter the sample before applying the learned model. We see that the completely unfiltered run, uogTrUB2 NoFilter, is markedly worse than its filtered counterpart, i.e. uogTrUB2. But the additional learned run, uogTrLqeabd NoFilter, improves over our initial baseline run and provides similar performance to the TREC median. Indeed this run improves over the TREC median under P@30 on the All Rel topic set. This result is promising, as it indicates that our learning to rank approach that filters a reverse-chronologically ordered ranking of tweets may be able to improve the quality and relevance of a tweet ranking. Indeed, with training data more representative of the topics, we expect that effectiveness will improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">WEB TRACK: ADHOC TASK</head><p>In the adhoc task <ref type="bibr" coords="4,407.04,595.86,9.11,8.66" target="#b8">[8]</ref>, our primary aim is to enhance our data-driven learning infrastructure that has proven effective during previous participations <ref type="bibr" coords="4,471.93,616.74,14.19,8.66" target="#b16">[16,</ref><ref type="bibr" coords="4,489.93,616.74,10.64,8.66" target="#b28">28]</ref>, such that it becomes easier to generate and evaluate many learning to rank models in a short timeframe. To this end, we extend Terrier with a framework for the fast computation of document features for learning to rank for web search. This framework ensures that the postings of the documents most likely to make it into the final ranking are readily available for an on-the-fly computation of multiple query-dependent features, without requiring multiple passes over the posting lists in the inverted file. For instance, this permits learned Features Total Weighting models (DPH <ref type="bibr" coords="5,210.94,64.39,8.51,7.68" target="#b2">[2]</ref>, PL2 <ref type="bibr" coords="5,243.10,64.39,8.51,7.68" target="#b2">[2]</ref>, BM25 <ref type="bibr" coords="5,282.33,64.39,12.44,7.68" target="#b23">[23]</ref>, LM, MQT <ref type="bibr" coords="5,342.08,64.39,13.19,7.68" target="#b27">[27]</ref>) 21 Fields-based models (PL2F <ref type="bibr" coords="5,221.37,73.27,13.19,7.68" target="#b14">[14]</ref>) 1 URL and link analysis features (e.g. PageRank, Absorbing Model* <ref type="bibr" coords="5,368.15,82.27,12.44,7.68" target="#b22">[22]</ref>, EdgeReciprocity*) 13 Quality features (e.g., fraction of stopwords, table text <ref type="bibr" coords="5,322.74,91.27,9.26,7.68" target="#b5">[5]</ref>) 8 Click feature (click count) 1 Spam feature (Cormack's fusion score <ref type="bibr" coords="5,260.24,109.15,9.22,7.68">[9]</ref>) 1 Term-dependence models (MRF <ref type="bibr" coords="5,239.36,118.15,12.44,7.68" target="#b18">[18]</ref>, pBiL <ref type="bibr" coords="5,278.83,118.15,13.19,7.68" target="#b21">[21]</ref>) 2 TOTAL 45* / 47 models to efficiently combine multiple standard weighting models (e.g. PL2 <ref type="bibr" coords="5,126.10,327.90,9.45,8.66" target="#b1">[1]</ref>), proximity models (e.g. Markov Random Fields <ref type="bibr" coords="5,101.75,338.34,13.54,8.66" target="#b18">[18]</ref>), and fields-based models (e.g. PL2F <ref type="bibr" coords="5,272.35,338.34,13.54,8.66" target="#b14">[14]</ref>). Moreover, the framework is compatible with dynamic pruning strategies such as MaxScore <ref type="bibr" coords="5,185.27,359.22,14.19,8.66" target="#b31">[31]</ref> and WAND <ref type="bibr" coords="5,253.19,359.22,9.63,8.66" target="#b7">[7]</ref> (which permit increased efficiency without loss of effectiveness), ensuring that learned models deploying many features can be efficiently and effectively applied. We index the category A (∼500M English documents) and category B (∼50M English documents) subsets of the ClueWeb09 corpus without stemming or stopwords. At retrieval time, a weak Porter stemmer and the DPH <ref type="bibr" coords="5,251.84,432.54,9.63,8.66" target="#b2">[2]</ref> weighting model are used to identify 5000 documents to re-rank using the learned models. In particular, our category A and B runs use a total of 45 and 47 features, respectively, as described in Table <ref type="table" coords="5,143.86,474.30,3.56,8.66" target="#tab_5">6</ref>. For learning, we employ the AFS algorithm <ref type="bibr" coords="5,96.83,484.74,13.37,8.66" target="#b17">[17]</ref>, with the 98 TREC 2009 and 2010 queries used as training data. In particular, we consider two basic learning scenarios: with and without validation data. In the former scenario, the 98 queries are randomly split into training (60%) and validation (40%), so as to prevent overfitting; in the latter scenario, all queries are used for training and no validation is performed.</p><p>We submitted three runs to the adhoc task:</p><p>• uogTrA45Nm (category A) deploys ranking models learned using 45 features, without a validation step;</p><p>• uogTrA45Vm (category A) deploys ranking models learned using 45 features, with a validation step;</p><p>• uogTrB47Vm (category B) deploys ranking models learned using 47 features, with a validation step.</p><p>Table <ref type="table" coords="5,87.11,658.62,4.61,8.66" target="#tab_6">7</ref> reports the performance of the three submitted adhoc runs under the normalised discounted cumulative gain at rank 20 (NDCG@20) and expected reciprocal rank at rank 20 (ERR@20) measures. We observe that we are substantially above the TREC median for the adhoc ranking task. Indeed, run uogTrA45Vm outperforms the median by 66% for NDCG@20 and 39% for ERR@20. Moreover, it was the best submitted run by NDCG@20 among all participating groups in the adhoc task of the Web track, and second ranked by ERR@20 <ref type="bibr" coords="5,401.14,221.22,9.11,8.66" target="#b8">[8]</ref>. Furthermore, our category A run that used validation marginally improved over the category A run that did not use validation, indicating that the use of validation data might be useful in an adhoc setting. Finally, we note the higher performance of the category A runs compared to the category B run, contrasting with previous experiences in the 2009 and 2010 TREC Web track <ref type="bibr" coords="5,539.02,283.98,13.37,8.66" target="#b26">[26]</ref>. This suggests that our learned models for category A runs are better able to identify high quality documents within the larger category A corpus than in previous years. Overall, we find that our learned approach for Web search has been very effective for the 2011 Web track adhoc task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">WEB TRACK:</head><p>DIVERSITY TASK  In the diversity task <ref type="bibr" coords="5,408.10,523.98,9.11,8.66" target="#b8">[8]</ref>, we continue to improve our stateof-the-art xQuAD framework for search result diversification <ref type="bibr" coords="5,336.12,544.86,14.19,8.66" target="#b24">[24,</ref><ref type="bibr" coords="5,353.99,544.86,11.68,8.66" target="#b25">25,</ref><ref type="bibr" coords="5,369.23,544.86,11.68,8.66" target="#b27">27,</ref><ref type="bibr" coords="5,384.59,544.86,10.64,8.66" target="#b30">30]</ref>. In particular, xQuAD models an ambiguous query as an ensemble of the multiple possible information needs underlying this query, represented as different query aspects <ref type="bibr" coords="5,373.91,576.18,13.37,8.66" target="#b29">[29]</ref>. Given an initial ranking R for the query q, and a set of aspects A identified for this query, xQuAD iteratively builds a re-ranking S by selecting, at each iteration, a document d * ∈ R \ S such that:</p><formula xml:id="formula_3" coords="5,351.12,621.90,204.83,17.60">d * = arg max d∈R\S (1 -λ) P(d|q) + λ P(d, S|q),<label>(3)</label></formula><p>where P(d|q) is the probability that a document d satisfies q and P(d, S|q) is the probability that d, but none of the documents in S, selected in previous iterations, satisfies the multiple aspects of q. In practice, these two probabilities can be thought of as representing the relevance and the diversity of d, respectively, with the parameter λ controlling the trade-off between the two probabilities. Additionally, the probability P(d, S|q) can be further expanded as follows: P(d, S|q) = X a∈A P(a|q) P(d|q, a) Y</p><formula xml:id="formula_4" coords="6,207.00,74.46,85.91,21.78">d j ∈S P( dj|q, a),<label>(4)</label></formula><p>where the aspect a ∈ A is one of the possible aspects underlying the query q, P(a|q) conveys the importance of this aspect in light of q, P(d|q, a) estimates the coverage of d with respect to a, and Q P( dj|q, a) estimates the novelty of any document satisfying a, given how badly this aspect is satisfied by the previously selected documents dj ∈ S.</p><p>In TREC 2011, we deploy learning-to-rank to produce refined estimations for two key components of xQuAD: the relevance of a search result to the initial query (i.e., P(d|q)), and the coverage of this result with respect to each of the aspects identified for this query (i.e., P(d|q, a)). In our participation, we exploit query reformulations from a commercial search engine in order to identify multiple query aspects <ref type="bibr" coords="6,77.88,240.54,13.37,8.66" target="#b24">[24]</ref>. For learning the relevance and coverage components of xQuAD, we leverage the same document features used for our adhoc runs described in Section 5. In particular, we estimate P(d|q, a) as:</p><formula xml:id="formula_5" coords="6,117.96,289.29,174.95,20.21">P(d|q, a) = X i wifi(d, q, a),<label>(5)</label></formula><p>where fi is one of the features from Table <ref type="table" coords="6,230.50,316.86,3.56,8.66" target="#tab_5">6</ref>, and wi is its learned weight, obtained using the AFS algorithm <ref type="bibr" coords="6,263.86,327.30,13.37,8.66" target="#b17">[17]</ref>, as described in Section 5. As in the adhoc task, we also consider two basic learning scenarios: with and without validation data, based upon the same set of 98 queries. We submitted three runs to the diversity task, all of which leverage machine learned models within xQuAD:</p><p>• uogTrA45Nmx2 (category A) deploys xQuAD to diversify the top 1000 results retrieved by our adhoc uog-TrA45Nm run, with document coverage learned without validation;</p><p>• uogTrA45Vmx (category A) deploys xQuAD to diversify the top 1000 results retrieved by our adhoc uog-TrA45Vm run, with document coverage learned with validation;</p><p>• uogTrB47Vmx (category B) deploys xQuAD to diversify the top 1000 results retrieved by our adhoc uog-TrB47Vm run, with document coverage learned with validation.</p><p>Table <ref type="table" coords="6,88.07,553.98,4.61,8.66" target="#tab_8">8</ref> shows the diversification performance of our submitted runs to the diversity task, as well as their corresponding baseline adhoc runs from Section 5. From the table, we first observe that all our category A runs are substantially above the TREC median (up to 29.5% and 14% above the median ERR-IA@20 and α-NDCG@20, respectively). Contrarily to previous editions <ref type="bibr" coords="6,162.46,616.74,14.19,8.66" target="#b16">[16,</ref><ref type="bibr" coords="6,179.37,616.74,10.64,8.66" target="#b28">28]</ref>, but similar to our adhoc task observations this year, we note the higher performance of our diversity task category A runs compared to our category B run <ref type="bibr" coords="6,108.00,648.18,13.37,8.66" target="#b26">[26]</ref>. More importantly, we observe that our diversity runs consistently improve upon our strongly performing adhoc runs, once again attesting the effectiveness of our xQuAD framework for search result diversification. As for the investigated learning scenarios, we observe no marked benefit in deploying separate validation data during learning. Finally, we note that run uogTrA45Nmx2 achieved the highest ERR-IA@20 and NRBP out of all submitted runs to the Web track 2011 diversity task, while uog-TrA45Vmx achieved the overall best performance in terms of α-NDCG@20 <ref type="bibr" coords="6,383.52,88.74,9.11,8.66" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In TREC 2011, we participated in the Web adhoc and diversity tasks, the Microblog real-time search task and Crowdsourcing assessment and consensus tasks, using our Terrier IR platform. In particular, we focused on improving our infrastructure and support for efficient data-driven ranking models within Terrier for the Web and Microblog tracks, and then developed an effective new data-driven filtering approach for tweet search. For the Crowdsourcing track, we integrated a crowdsourcing infrastructure into Terrier for achieving fast generation of relevance assessments based upon Amazon's Mechanical Turk and proposed a new machine learning approach for calculating consensus over multiple assessors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.76,284.12,239.07,8.52;2,53.76,294.56,224.48,8.52;2,55.63,179.00,236.71,93.12"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of the Web interface used during the assessment task of the Crowdsourcing track.</figDesc><graphic coords="2,55.63,179.00,236.71,93.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.76,55.14,502.20,104.90"><head>Table 1 :</head><label>1</label><figDesc>Performance of uogTrP1rg compared to the two TREC median ground truth assessment sets within the assessment task of the Crowdsourcing track. The best run is highlighted for each ground truth and measure.</figDesc><table coords="2,83.28,55.14,443.16,62.06"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Classification Metrics</cell><cell></cell><cell></cell><cell>Error Metrics</cell></row><row><cell>Ground Truth</cell><cell>Run</cell><cell cols="7">Accuracy Recall Precision Specificity Log-Loss KL-Divergence RMSE</cell></row><row><cell>Consensus</cell><cell cols="2">TREC Median 74.0%</cell><cell>75.4%</cell><cell>79.1%</cell><cell>70.4%</cell><cell>954.6</cell><cell>2058.2</cell><cell>63.8%</cell></row><row><cell></cell><cell>uogTrP1rg</cell><cell cols="3">78.2% 86.6% 80.4%</cell><cell>64.0%</cell><cell>822.5</cell><cell>612.8</cell><cell>54.4%</cell></row><row><cell cols="5">TREC Assessors TREC Median 67.5% 73.1% 78.1%</cell><cell>52.5%</cell><cell>103.0</cell><cell>103.1</cell><cell>50.6%</cell></row><row><cell></cell><cell>uogTrP1rg</cell><cell cols="3">61.7% 73.8% 67.7%</cell><cell>33.0%</cell><cell>125.9</cell><cell>126.0</cell><cell>50.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,316.80,55.06,239.31,107.26"><head>Table 3 :</head><label>3</label><figDesc>Tweets11 corpus statistics. Values marked with * are relative due to the dynamic nature of the Tweets11 corpus.</figDesc><table coords="3,322.41,55.06,232.28,64.60"><row><cell>Data</cell><cell>Quality</cell><cell>Value</cell></row><row><cell cols="2">Tweets11 Corpus Time Range Days</cell><cell>23/01/11 → 08/02/11 16</cell></row><row><cell></cell><cell># Tweets</cell><cell>15,663,909*</cell></row><row><cell></cell><cell>Avg. Tweets Per Day</cell><cell>978,994*</cell></row><row><cell></cell><cell>Unique URLs</cell><cell>2,274,350*</cell></row><row><cell></cell><cell>Unique Twitterers</cell><cell>5,218,687*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,317.28,709.54,214.83,9.86"><head>Table 2 :</head><label>2</label><figDesc>Performance of uogTrP2O4wtr at consensus calculation against TREC assessors within the consensus task of the Crowdsourcing track. The best run is highlighted for each measure.</figDesc><table coords="4,58.93,55.14,494.37,151.82"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Classification Metrics</cell><cell></cell><cell>Error Metrics</cell></row><row><cell cols="2">Ground Truth</cell><cell>Run</cell><cell cols="5">Accuracy Recall Precision Specificity Log-Loss KL-Divergence RMSE</cell></row><row><cell cols="6">TREC Assessors TREC Median 61.7% 73.3% 59.5%</cell><cell>50.2%</cell><cell>2047.6</cell><cell>2047.8</cell><cell>54.8%</cell></row><row><cell></cell><cell></cell><cell cols="2">uogTrP2O4wtr 44.1%</cell><cell>34.5%</cell><cell>42.7%</cell><cell>53.7%</cell><cell>931.7</cell><cell>931.7</cell><cell>58.8%</cell></row><row><cell>Feature Set</cell><cell cols="2">Internal Summary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Number</cell></row><row><cell>Tweet Relevance</cell><cell></cell><cell cols="6">Features encapsulating the relevance of the tweet to the query, e.g. document retrieval model scores like BM25 [23].</cell><cell>38</cell></row><row><cell>Tweet Quality</cell><cell></cell><cell cols="6">Features from the tweet that may be indicators of quality, e.g. URLs and tweet length.</cell><cell>7</cell></row><row><cell>Tweet Language</cell><cell></cell><cell cols="6">Features generated describing the language of the tweet, e.g. the classification probability of being written in English.</cell><cell>17</cell></row><row><cell>Spam Detection</cell><cell></cell><cell cols="6">Features from the tweet that may be indicate that it is spam, e.g. a high hashtag count [10].</cell><cell>4</cell></row><row><cell>Referred Page</cell><cell>"</cell><cell cols="6">Features extracted describing Web pages referenced from each tweet, e.g. stopwords contained [4]</cell><cell>5</cell></row><row><cell>Twitterer</cell><cell>"</cell><cell cols="6">Features about the user that made the tweet, e.g. number of followers and statuses</cell><cell>5</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell></row></table><note coords="3,317.28,709.54,3.65,4.10;3,321.36,711.76,210.75,7.65"><p>3 http://ir.dcs.gla.ac.uk/wiki/Terrier/Tweets11</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,59.42,220.04,479.09,105.00"><head>Table 4 :</head><label>4</label><figDesc>A summary of all tweet filtering features used in the Microblog track real-time search task.</figDesc><table coords="4,59.42,248.55,230.35,76.49"><row><cell></cell><cell></cell><cell cols="2">All Rel</cell><cell cols="2">High Rel</cell></row><row><cell>Run</cell><cell cols="2">Submitted MAP</cell><cell>P@30</cell><cell>MAP</cell><cell>P@30</cell></row><row><cell>TREC median</cell><cell>-</cell><cell cols="4">0.1421 0.2229 0.1510 0.2343</cell></row><row><cell>uogTrUB2</cell><cell></cell><cell cols="4">0.1014 0.1939 0.0714 0.0818</cell></row><row><cell>uogTrLqea</cell><cell></cell><cell cols="4">0.0625 0.1068 0.0402 0.0485</cell></row><row><cell>uogTrLqeabd</cell><cell></cell><cell cols="4">0.0625 0.1068 0.0402 0.0485</cell></row><row><cell>uogTrLqeabdd</cell><cell></cell><cell cols="4">0.0625 0.1068 0.0402 0.0485</cell></row><row><cell>uogTrUB2 NoFilter</cell><cell>"</cell><cell cols="4">0.0427 0.0532 0.0473 0.0559</cell></row><row><cell>uogTrLqeabd NoFilter</cell><cell>"</cell><cell cols="4">0.1136 0.2381 0.0973 0.2262</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,53.76,338.12,239.32,39.96"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note coords="4,99.60,338.12,193.35,8.52;4,53.76,348.56,239.32,8.52;4,53.76,359.12,239.11,8.52;4,53.76,369.56,192.67,8.52"><p>Results of our submitted and unsubmitted runs for the Microblog track under mean average precision (MAP) and precision at 30 (P@30) for both the High Rel and All Rel topic sets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,53.76,148.52,502.23,83.39"><head>Table 6 :</head><label>6</label><figDesc>Document features used in the Web track. Category A runs use all features, except for the two marked with a star (*). Category B runs use all features.</figDesc><table coords="5,83.16,187.39,180.52,44.52"><row><cell>Run</cell><cell cols="3">Category NDCG@20 ERR@20</cell></row><row><cell>TREC median</cell><cell></cell><cell>0.1876</cell><cell>0.1061</cell></row><row><cell>uogTrA45Nm</cell><cell>A</cell><cell>0.3043</cell><cell>0.1481</cell></row><row><cell>uogTrA45Vm</cell><cell>A</cell><cell>0.3052</cell><cell>0.1485</cell></row><row><cell>uogTrB47Vm</cell><cell>B</cell><cell>0.2278</cell><cell>0.1231</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,53.76,245.12,239.19,50.40"><head>Table 7 :</head><label>7</label><figDesc>Results of the three submitted runs for the Web track adhoc task under the normalised discounted cumulative gain at rank 20 (NDCG@20) and expected reciprocal rank at rank 20 (ERR@20) measures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="5,316.80,488.96,239.21,18.96"><head>Table 8 :</head><label>8</label><figDesc>Results of the submitted runs to the diversity task of the Web track.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,58.44,711.76,98.31,7.65"><p>https://www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,321.36,711.76,215.43,7.65"><p>http://terrier.org/docs/v3.5/terrier_http.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGEMENTS</head><p>We would like to thank <rs type="funder">Amazon's Mechanical Turk</rs> for funding support provided for experiments detailed in this paper.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.30,339.11,96.83,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,356.70,217.66,8.66;6,335.64,367.14,211.22,8.66;6,335.64,377.58,114.73,8.66" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,379.44,356.79,173.85,8.16;6,335.64,367.23,156.63,8.16">Probability models for information retrieval based on Divergence From Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="6,335.63,389.10,211.68,8.66;6,335.64,399.54,211.41,8.66;6,335.64,409.98,219.76,8.66;6,335.64,420.41,20.80,8.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,389.49,399.54,157.55,8.66;6,335.64,409.98,137.67,8.66">FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,491.76,410.07,57.89,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,431.94,194.42,8.66;6,335.64,442.38,134.41,8.66;6,335.64,453.63,215.43,7.65;6,335.64,464.07,178.83,7.65;6,335.64,474.63,194.44,7.65" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Web</forename><surname>Amazon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Services</surname></persName>
		</author>
		<ptr target="http://docs.amazonwebservices.com/AWSMechTurk/2008-02-14/AWSMechanicalTurkRequester/ApiReference_ExternalQuestionArticle.html" />
		<title level="m" coord="6,335.64,442.38,130.28,8.66">ExternalQuestion API Reference</title>
		<meeting><address><addrLine>Amazon Mechanic Turk</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,485.21,166.56,8.66;6,335.64,495.65,215.83,8.66;6,335.64,506.09,55.95,8.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,335.64,495.65,165.12,8.66">Quality-biased ranking of web documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,519.84,495.75,31.63,8.16;6,335.64,506.19,25.76,8.16">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,517.61,166.56,8.66;6,335.64,528.05,215.83,8.66;6,335.64,538.49,114.27,8.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,335.64,528.05,165.12,8.66">Quality-biased ranking of web documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,519.84,528.15,31.63,8.16;6,335.64,538.59,25.76,8.16">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,550.01,187.73,8.66;6,335.64,560.45,184.69,8.66" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,480.72,550.11,42.64,8.16;6,335.64,560.55,92.35,8.16">Non-linear optimization techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Swann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Oliver &amp; Boyd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,571.85,220.23,8.66;6,335.64,582.41,206.38,8.66;6,335.64,592.85,204.64,8.66;6,335.64,603.29,20.80,8.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,368.99,582.41,173.04,8.66;6,335.64,592.85,63.64,8.66">Efficient query evaluation using a two-level retrieval process</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,417.84,592.95,54.17,8.16">Proc.of CIKM</title>
		<meeting>.of CIKM</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,614.69,208.65,8.66;6,335.64,625.25,219.04,8.66;6,335.64,635.69,87.52,8.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,377.40,625.25,161.38,8.66">Overview of the TREC 2011 Web Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.64,635.79,58.01,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.63,647.09,215.49,8.66;6,335.64,657.53,220.18,8.66;6,335.64,668.09,208.24,8.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,335.64,657.53,220.18,8.66;6,335.64,668.09,75.66,8.66">Efficient and effective spam filtering and re-ranking for large Web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,418.32,668.19,36.57,8.16">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.64,679.49,207.35,8.66;6,335.64,689.93,145.71,8.66;6,335.64,701.19,235.48,7.65;6,335.64,710.93,97.36,8.66" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="6,376.54,679.49,162.50,8.66">Google reveals factors for ranking tweets</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Crum</surname></persName>
		</author>
		<ptr target="http://www.webpronews.com/google-reveals-factors-for-ranking-tweets-2010-01" />
		<imprint>
			<date type="published" when="2010-09-29">2010. 29/09/2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,57.30,193.00,8.66;7,72.60,67.86,173.80,8.66;7,72.60,78.30,82.13,8.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,122.36,57.30,143.24,8.66;7,72.60,67.86,76.65,8.66">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,156.00,67.96,86.56,8.16">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,89.70,218.24,8.66;7,72.60,100.14,211.71,8.66" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,211.64,89.70,79.19,8.66;7,72.60,100.14,114.85,8.66">Crowdsourcing user studies with mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.92,100.24,49.74,8.16">Proc. of CHI</title>
		<meeting>of CHI</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,111.66,213.35,8.66;7,72.60,122.10,190.95,8.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,171.21,111.66,114.74,8.66;7,72.60,122.10,84.48,8.66">Overview of the TREC 2011 Crowdsourcing Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,176.04,122.20,58.01,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,133.50,210.75,8.66;7,72.60,144.06,208.07,8.66;7,72.60,154.50,206.74,8.66;7,72.60,164.94,174.03,8.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,110.74,144.06,169.93,8.66;7,72.60,154.50,206.74,8.66;7,72.60,164.94,68.85,8.66">University of Glasgow at WebCLEF 2005: Experiments in per-field normlisation and language specific stemming</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,160.68,165.04,56.65,8.16">Proc. of CLEF</title>
		<meeting>of CLEF</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,176.34,176.39,8.66;7,72.60,186.90,207.56,8.66;7,72.60,197.34,131.79,8.66" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,72.60,186.90,207.56,8.66;7,72.60,197.34,23.06,8.66">Crowdsourcing Blog Track Top News Judgments at TREC</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,116.04,197.44,58.69,8.16">Proc. of CSDM</title>
		<meeting>of CSDM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,208.74,212.06,8.66;7,72.60,219.18,220.06,8.66;7,72.60,229.74,194.30,8.66;7,72.60,240.18,217.50,8.66;7,72.60,250.62,63.28,8.66" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,140.26,219.18,152.40,8.66;7,72.60,229.74,194.30,8.66;7,72.60,240.18,178.35,8.66">University of Glasgow at TREC 2009: Experiments with Terrier-Blog, Entity, Million Query, Relevance Feedback, and Web tracks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,269.28,240.28,20.82,8.16;7,72.60,250.72,33.77,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,262.02,219.76,8.66;7,72.60,272.58,215.94,8.66;7,72.60,283.02,125.56,8.66" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,121.30,262.02,171.05,8.66;7,72.60,272.58,176.93,8.66">Automatic feature selection in the Markov random field model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,267.72,272.68,20.82,8.16;7,72.60,283.12,33.29,8.16">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,294.42,212.33,8.66;7,72.60,304.86,196.83,8.66;7,72.60,315.42,83.68,8.66" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,190.78,294.42,94.15,8.66;7,72.60,304.86,114.33,8.66">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.68,304.96,58.97,8.16">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,326.82,169.67,8.66;7,72.60,337.26,184.48,8.66;7,72.60,347.70,187.81,8.66;7,72.60,358.26,175.95,8.66" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="7,195.69,337.26,61.39,8.66;7,72.60,347.70,187.81,8.66;7,72.60,358.26,32.88,8.66">Terrier: A high performance and scalable information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,124.20,358.36,95.81,8.16">Proc. of OSIR at SIGIR</title>
		<meeting>of OSIR at SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,369.66,192.23,8.66;7,72.60,380.10,198.77,8.66;7,72.60,390.54,87.51,8.66" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="7,72.60,380.10,182.86,8.66">Overview of the TREC-2011 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,72.60,390.64,58.01,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,402.06,203.56,8.66;7,72.60,412.50,214.47,8.66;7,335.64,57.30,209.80,8.66" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="7,110.74,412.50,176.33,8.66;7,335.64,57.30,40.20,8.66">Incorporating term dependency in the DFR framework</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,394.92,57.40,58.97,8.16">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="843" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,68.82,202.65,8.66;7,335.64,79.26,176.44,8.66;7,335.64,89.70,77.45,8.66" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="7,497.47,68.82,40.81,8.66;7,335.64,79.26,115.18,8.66">The static absorbing model for the Web</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,459.24,79.36,48.63,8.16">J. Web Eng</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="186" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,101.22,149.39,8.66;7,335.64,111.66,195.68,8.66;7,335.64,122.10,138.64,8.66" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="7,496.30,111.66,35.02,8.66;7,335.64,122.10,31.29,8.66">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,386.76,122.20,58.01,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,133.50,183.11,8.66;7,335.64,144.06,217.62,8.66;7,335.64,154.50,200.68,8.66;7,335.64,164.94,20.80,8.66" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="7,335.64,144.06,217.62,8.66;7,335.64,154.50,54.69,8.66">Exploiting query reformulations for Web search result diversification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,408.48,154.60,57.75,8.16">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,176.34,183.11,8.66;7,335.64,186.90,219.19,8.66;7,335.64,197.34,124.00,8.66" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="7,335.64,186.90,169.19,8.66">Selectively diversifying Web search results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,523.08,187.00,31.75,8.16;7,335.64,197.44,22.49,8.16">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,208.74,183.11,8.66;7,335.64,219.18,207.91,8.66;7,335.64,229.74,124.00,8.66" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="7,335.64,219.18,158.38,8.66">Effectiveness beyond the first crawl tier</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,511.92,219.28,31.63,8.16;7,335.64,229.84,22.49,8.16">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1937" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,241.14,183.11,8.66;7,335.64,251.58,211.63,8.66;7,335.64,262.02,115.36,8.66" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="7,335.64,251.58,161.73,8.66">Intent-aware search result diversification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,515.64,251.68,31.63,8.16;7,335.64,262.12,23.93,8.16">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,273.54,206.19,8.66;7,335.64,283.98,191.03,8.66;7,335.64,294.42,215.67,8.66;7,335.64,304.86,87.51,8.66" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="7,373.66,283.98,153.01,8.66;7,335.64,294.42,200.42,8.66">University of Glasgow at TREC 2010: Experiments with Terrier in Blog and Web tracks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,335.64,304.96,58.01,8.16">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,316.38,219.66,8.66;7,335.64,326.82,209.67,8.66;7,335.64,337.26,49.36,8.66" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="7,457.03,316.38,98.26,8.66;7,335.64,326.82,70.32,8.66">Diversifying for multiple information needs</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,424.68,326.92,90.53,8.16">Proc. of DDR at ECIR</title>
		<meeting>of DDR at ECIR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,346.98,218.85,8.66;7,335.64,357.42,178.85,8.66;7,335.64,367.98,200.19,8.66" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="7,335.64,357.42,178.85,8.66;7,335.64,367.98,43.58,8.66">Explicit search result diversification through sub-queries</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,397.56,368.08,55.61,8.16">Proc. of ECIR</title>
		<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="87" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,379.38,211.07,8.66;7,335.64,389.82,188.57,8.66;7,335.64,400.26,139.00,8.66" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="7,433.30,379.38,113.41,8.66;7,335.64,389.82,70.50,8.66">Query evaluation: strategies and optimizations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,413.40,389.92,110.81,8.16;7,335.64,400.36,48.80,8.16">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="831" to="850" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
