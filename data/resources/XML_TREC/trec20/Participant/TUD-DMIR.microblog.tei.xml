<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.48,72.35,258.75,16.84">DMIR on Microblog Track 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.18,118.05,36.85,11.06"><forename type="first">Wen</forename><surname>Li</surname></persName>
							<email>wen.li@tudelft.nl</email>
						</author>
						<author>
							<persName coords="1,261.13,118.05,87.46,11.06"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<email>c.eickhoff@tudelft.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft, Neterlands</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Arjen P. de Vries CWI Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.48,72.35,258.75,16.84">DMIR on Microblog Track 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">50DC4956B2C1A281AED5F969DF539BFB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Microblog Retrieval Twitter</term>
					<term>self-information</term>
					<term>portfolio</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present our work on the Microblog Track of TREC 2011. We tried two methods to tackle the problem of tweet retrieval, namely EMAX and RTB. The first method EMAX is mainly based on the intuition that not only should retrieved tweets related to the keywords in given queries but also provide more information. This results in a ranking method based on self-information. Our second method RTB tries to incorporate the importance of recency along with relevance in microblog retrieval tasks. Therefore, we adapt portfolio theory to balance the relevance dimension and recency dimension. However, the evaluation results suggest no significant improvement from both two methods because of the short lengths of documents, the noisy and spam tweets and the re-ordering in recency. Meanwhile, we also present some ideas during the course of participation. By closely examining the judgments, we find that most of relevant documents are those containing a link to external resource and have a length of around 17 words, which is different from the statistics observed in the collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The 2011 edition of TREC is the first to feature a dedicated track on microblog retrieval. As one of the participants, we submitted 2 runs on the JSON version of the provided data. In this paper, we will present our results as well as some further thoughts on the task and setting. The task of the track was to retrieve relevant tweets from a 14-day sample of Twitter. As a holder of a white-listed IP, Delft Multimedia Information Retrieval Lab uses the JSON version of the data to carry out all the experiments. In the following sections, we present our approaches EMAX and RTB alongside some general insights into the subject of microblog retrieval. To show the performance of our proposed system, we compared our runs with generic BM25 and TF-IDF implementations and a baseline provided by the organizers. Furthermore, we discuss some noteworthy observations, before concluding with an outlook on future directions in the domain of microblog retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MICROBLOG TRACK 2.1 Task</head><p>This is the first time for the TREC conference series to address the task of microblog retrieval with a dedicated track. The organizer released a set of 50 topics and all participants were asked to submit their retrieved documents for relevance judgment. For each query a proposed system should return up to 1000 documents for evaluation. A difference between this track from others is that queries are simulated as they are issued during sampling the collection. Therefore, no documents newer than the time of query should be retrieved. The top 30 retrieved documents of each query from each run were collected as a pool and judged by assessors arranged by the organizer. The judgment is based on a 3-level scale, namely high-relevant, relevant and irrelevant. During the judgment process, the assessors were allowed to use any resource to make a decision, e.g., by following links in tweets. However, all non-English and offensive documents were judged as irrelevant. The evaluation is done in a similar way. For each query in each run, all retrieved document are ordered by recency and then evaluated by traditional metrics, such as Precision @ 30 and R-precision.</p><p>In addition, participants were asked to submit at least one run without using any form of external (from outside of the collection provided) or future (from tweets with a timestamp newer than that of a query) information. Since microblog search is a realtime task, queries issued at different times request different parts of the document collection. This has to be reflected in the implementation of experiment systems, since the term statistics may vary across queries. One of the major contributions of our submission lies in providing the community with code to account for this special requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>The document collection provided for all participants is a collection of tweets sampled during a two-week period from Jan 24 to Feb 8 of 2011, covering major events such as the Egyptian revolution and the US Superbowl. The document collection is crawled through Twitter's API following the track's ID list with our own multi-threaded crawler<ref type="foot" coords="1,525.67,640.54,3.65,5.24" target="#foot_0">1</ref> . As a result, we obtained a collection of about 15 million tweets. It should be notice that all participants had to crawl the data themselves, as a result, it is likely that every participant uses a slightly different version of data. By matching the collection we crawled with the collection of relevant tweets judged by official assessors, we found that we lost 21 relevant tweets, which is about 0.7% of the relevant. Since this task requires only English tweets, we apply an English language detection scheme to filter out all non-English documents. Concretely, we reject any tweet that contains more than 50% non-English terms. We used WordNet<ref type="foot" coords="2,209.84,201.63,3.65,5.24" target="#foot_1">2</ref> to identify English words. After this step, 5.7 million English tweets remain in the collection. There is no relevant tweet lost in this step. A generic statistics for our collection before and after filtering is shown in Table <ref type="table" coords="2,130.06,245.24,3.58,7.86" target="#tab_0">1</ref>. By comparing the statistics between English collection (indexed) and relevant collection, we have several observations discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR APPROACH</head><p>As a feature of microblogs, the limitation of text length forces users to write their messages concisely. Therefore, the tweets themselves usually are not able to carry as much information as normal documents in traditional retrieval tasks. As a matter of fact, the usual way to augment tweets with much richer information is to include hyperlinks pointing at external resources. With links, tweet writers can indirectly include any amount of information in any form (images, videos, etc.).</p><p>However, it also makes tweet retrieval more difficult since document relevance can hardly be decided by only looking at the tweet content. A solution to this problem can be to include the external evidence (resource) linked to in the tweet for ranking. Well-written web pages, images, video clips are common external evidence in Twitter. Among these types of evidence, web pages are more easy and promising to be integrated to a text retrieval system. However, a carefully composed web page could cost several hours to be put up on the Internet, which weakens the realtime feature of the tweets linking to it as they are delayed until the web page is available online. Furthermore, if we put too much weight on web pages linked to other than tweets themselves, retrieving tweets could degenerate to retrieving web pages which are manually collected and composed as tweets. This very problem of web retrieval is well studied in the past decades and has had a longterm dedicated track in TREC.</p><p>Meanwhile, other forms of external evidence is more interesting as they present information in multimedia dimension other than just text. For example, a user could tweet a picture showing the people marching and celebrating a festival or a short video shot at a football match when his favorite player gets a goal. This would be more interesting than just 140 characters. However, it is usually not an easy task to retrieve multimedia content. Therefore the only information we could easily use is the very descriptions of these images and video clips in tweets linking to them. At this stage of our research, we limit ourselves to the scenario of ranking tweets without using any external evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EMAX</head><p>We assume a relevant and informative tweet to have two properties. It is i) related to the query and ii) carries sufficient information. For the first assumption, there are a number of state-of-the-art models, such as MB25 <ref type="bibr" coords="2,496.61,101.97,9.20,7.86" target="#b2">[3]</ref>, TF-IDF <ref type="bibr" coords="2,546.20,101.97,9.72,7.86" target="#b3">[4,</ref><ref type="bibr" coords="2,316.81,112.43,6.48,7.86" target="#b1">2]</ref>. The second assumption is the key contribution of our approach to this ranking problem. Tweets themselves are usually short, particularly, they tend to be just one or two sentences. As a result, tweets usually carry a limit amount of information. Our approach tries to rank those carrying more information higher. Therefore, we use self-information to measure how much information a tweet (document) is loaded with. Self-information [1] is a concept from information theory which is used to measure how informative an event is in a probabilistic model. In our system, it expresses how much information a document d contains.</p><formula xml:id="formula_0" coords="2,392.52,233.03,87.69,17.86">E(d) = - t∈d log df (t)</formula><p>where E(d) is the information carried by document d, and df (ti) is the document frequency of term ti.</p><p>Given a query, we first rank all documents by one of traditional retrieval models (namely, BM25 and TF-IDF) and then select the top 1000 documents as candidates. These top tweets are re-ranked in the descending order of their self-information. However, due to the fact that tweets commonly contain more informal terms than carefully edited resources, we expect higher entropy from them. In order to counter this effect, we filter out those words appearing less than 3 times in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RTB</head><p>Our second approach RTB (Relevance Time Balancing) is based on an application of the economical portfolio theory <ref type="bibr" coords="2,333.41,414.86,9.72,7.86" target="#b4">[5]</ref> and tries to balance multiple relevance criteria into one coherent ranking. As mentioned in the previous section, time is an important aspect in realtime retrieval tasks. Users are supposed in favor of recent relevant messages other than old well-known facts. Therefore, we propose a method to incorporating users' preference for recency by considering it a property of tweets. We combine the aspects of recency and topical relevance in the ranking by maximizing the target function O(r) that takes into account the mean relevance E[R] and relevance variance Var(R) across multiple (in the current scenario 2) dimensions. The risk averseness parameter b can be used for further tuning towards particular user preferences. In the course of this work, it was statically set to 0.5.</p><formula xml:id="formula_1" coords="2,389.70,565.96,93.33,7.86">O(r) = E[R] -bVar(R)</formula><p>Similar to EMAX, we also apply RTB on top of several traditional retrieval models and also EMAX respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMPLEMENTATION</head><p>As we mentioned in Section 1, the temporal aspect is one of the important properties that makes microblog retrieval different from other ad hoc retrieval tasks. Traditionally, time-information is not directly involved in retrieval. As a result, all statistic figures (e.g., term frequencies, collection sizes, etc.) can be obtained statically at indexing time. However, for the realtime task required by this track, we know the concrete document collection to be included only at query time. During our work on this year's microblog track, we explored two ways to address this challenge. The first approach is straightforward. We index part of the collection according to the earliest overall query time and query the resulting index. For each subsequent query, we expand the index by the documents that were posted in the time between q1 and q2, and so on. Obviously, using this method, it takes very long to run 50 queries under different system settings. As a solution, we created a dynamic calculation of the term statistics used by the retrieval model. Our first version of implementation deals with document frequency which is typically used in many probability models.</p><p>A patch to the Terrier search engine that accounts for the dynamic calculation of document frequencies can be downloaded from http://homepage.tudelft.nl/9y54n/terrier-3.5realtime.patch.gz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>In this section, we focus on the results of our unofficial runs, as unfortunately, at the time of official submission we had an implementation bug in our experimental system which led to a reverse ranking. After the submission deadline, we were able to correct for this. As suggested during discussion on the mailing list, we use both precision at 30 retrieved documents and R-precision as our evaluation methods.</p><p>To evaluate our first proposed method, EMAX, we compare it with a BM25 baseline and the official baseline. Figure <ref type="figure" coords="3,53.80,470.59,4.61,7.86">1</ref> shows that there is no single method that can dominate the benchmark of P@30 and there is very little difference in the overall. Table <ref type="table" coords="3,126.17,491.51,4.61,7.86" target="#tab_1">2</ref> further emphasises this trend. The same can also be found for R-precision, as shown in Figure <ref type="figure" coords="3,264.57,501.97,3.58,7.86">2</ref>. Possible explanations for the insignificant differences are: i) all the methods rely on the same underlying components, term frequencies and inverse document frequencies, and ii) tweets contain too few words to properly characterize the topic they represent. A close look on the retrieval results suggests that EMAX tends to rank certain spam tweets higher since they have a lot of carefully selected words boosting their ranking. These tweets have three characteristics: i) they have many hashtags, ii) many repeated keywords and iii) they are usually very long. In the future, we would like to see whether we can apply spam filtering before ranking to resolve the problem.</p><p>Another interesting observation to be made is TF-IDF with a cut off at 30 scoring significantly higher than competing methods. Actually this can be considered a side-effect of the evaluation methodology, which we will address further in Section 6.</p><p>To evaluate our second method, RTB, we apply the balanced re-ranking algorithm on top of EMAX, BM25 and TF-IDF respectively. From the Figure <ref type="figure" coords="3,195.16,711.19,4.61,7.86">3</ref> and<ref type="figure" coords="3,220.63,711.19,3.58,7.86">4</ref>, we can see that   the results are worse than that before applying the balancing method. Because we did not consider a method to cut-off the retrieved list at a point higher than 1000 results, our approach does not lead to effective results under the evaluation. A possible reason is that not only recent relevant tweets get higher in the ranking but also recent irrelevant tweets are boosted into the top 1000 retrieved tweets. By the official evaluating method, those irrelevant but recent tweets may be then re-ordered at the top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>As we mentioned previously, tweets are usually a sentence composed of several words because of the limitation of 140 characters. As can be seen from Figure <ref type="figure" coords="3,475.67,512.43,3.58,7.86" target="#fig_1">5</ref>, the typical length of a tweet is around 8 words. To our curiosity, we compare it with the lengths of relevant tweets judged by the assessors, which is shown in Figure <ref type="figure" coords="3,421.38,543.81,3.58,7.86" target="#fig_2">6</ref>. Besides the spike at around 9, relevant tweets are more likely to have a length of 17 words, which is about twice longer than normal ones. This may suggest that longer tweets are generally more probable to be relevant. It also supports our proposed assumption, as long tweets usually carry more information than short ones.</p><p>Inspired by the previous observation, we tried to find the difference in user dimension, which is shown in Figure <ref type="figure" coords="3,533.85,617.04,4.61,7.86" target="#fig_4">7</ref> and Figure <ref type="figure" coords="3,347.32,627.50,3.58,7.86" target="#fig_5">8</ref>. However, there is no observable difference between the two collections in such user statistics. Meanwhile, we find an interesting user who contributes the highest number of relevant tweets. These tweets are judged relevant to query MB027 (reduce energy consumption), and the user is a broadcast account for a life hacking site on saving energy. This could be an interesting dimension to explore since it is reasonable that professional users usually provide more specific and informative tweets then others.   By manual inspection of the evaluation results, we find that most of the provided queries are about events that would be expected to show up in news reports. We also checked the judgments used for relevance evaluation, and find that most of relevant tweets (94% of highly relevant tweets and 81% of all relevant tweets) contain links to external resources. Meanwhile, in the collection of judged irrelevant tweets, the proportion is only 53%, as shown in Table <ref type="table" coords="4,79.48,469.79,3.58,7.86" target="#tab_2">3</ref>. It seems that tweets with links are more likely to be relevant in general.</p><p>This observation may be biased to the strategies employed by the majority of participants. It could be the case that some systems in favor of external evidence happen to be the ones who retrieve most relevant tweets and, consequently, influence the resource selection. It could also be influenced by assessors' perception of the relevance criteria and many queries are regarding news events. However, we could not provide hard evidence to support our suspicions. In spite of those, external evidence is assumed to greatly support the task, as they usually provide more relevant information to given queries.</p><p>The evaluation scheme use in the Microblog Track is a bit different from other tracks since recency becomes an im- portant dimension to evaluate. The official evaluation with ordering in recency before traditional metrics is a natural way to reflect the demands on latest information while being compatible with default TREC evaluation scheme. Considering the set-based nature of the evaluation, a good system would actually separate the relevant from the non-relevant tweets, and only return the assumed relevant set. Our strategy to return 1000 tweets for every topic does not satisfy that evaluation design, and to test this we constructed the TFIDF30 run. Here, we only take the top 30 tweets ranked by TF-IDF and feed them to the evaluation system and produce better results than the baseline as shown in Table <ref type="table" coords="4,548.76,172.70,3.58,7.86" target="#tab_1">2</ref>. However, identifying a good cutting point is usually hard in IR domain, and we would like to investigate this problem further in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we present our proposed methods for microblog retrieval and discuss some ideas that influenced our TREC participation. Generally, our proposed methods were not able to significantly outperform the baseline. The reasons probably are i) there are too few words in tweets which makes characterizing their topic hard for term-based models. ii) Spam tweets can get higher ranks in EMAX as their content is designed to cheat. iii) RTB tries to balance recency and topicality in retrieved documents which does not fit the design of the official evaluation methods. As a result, we only achieve performance similar to the baseline. The inspection of queries and the official judgments shows that tweets have a length of 17 words and with external links are more likely to end up in the relevant set. In the future we may experiment with length and URL presence priors.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,319.27,183.27,234.21,7.89"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Length distribution in English collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,316.81,331.84,239.11,7.89;3,316.81,342.30,18.26,7.89"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Length distribution in the relevant collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,53.80,183.27,239.11,7.89;4,53.80,193.73,81.34,7.89"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: User distribution over tweet frequency in English collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="4,53.80,344.89,239.10,7.89;4,53.80,355.35,102.59,7.89"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: User distribution over tweets frequency in the relevant collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,115.72,198.14,378.29,7.89"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Precision@30: Comparison between Baseline, EMAX, BM25, TFIDF30</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="5,82.67,533.88,444.37,7.89"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Precision@30: Comparison between Baseline, RTB EMAX, RTB BM25, RTB TFIDF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,86.43,63.80,173.86,62.28"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="2,86.43,63.80,173.86,62.28"><row><cell></cell><cell cols="2">: Corpus Statistics</cell></row><row><cell></cell><cell>Corpus</cell><cell>English</cell></row><row><cell>With Links</cell><cell>2,665,155</cell><cell>1,334,288</cell></row><row><cell>Users</cell><cell>5,228,689</cell><cell>2,554,641</cell></row><row><cell cols="2">Retweeted Tweets 1,672,912</cell><cell>762,907</cell></row><row><cell>Total</cell><cell cols="2">15,657,240 5,703,979</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,87.66,63.80,171.40,93.42"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="3,87.66,63.80,171.40,93.42"><row><cell></cell><cell>: Evaluation</cell><cell></cell></row><row><cell></cell><cell cols="2">Precision@30 R-precision</cell></row><row><cell>baseline</cell><cell>0.0986</cell><cell>0.1486</cell></row><row><cell>Emax</cell><cell>0.1007</cell><cell>0.1423</cell></row><row><cell>MB25</cell><cell>0.1041</cell><cell>0.1531</cell></row><row><cell>TFIDF32</cell><cell>0.2694</cell><cell>0.1828</cell></row><row><cell>RTB EMAX</cell><cell>0.049</cell><cell>0.0472</cell></row><row><cell>RTB TFIDF</cell><cell>0.049</cell><cell>0.0433</cell></row><row><cell>RTB BM25</cell><cell>0.049</cell><cell>0.0433</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,75.54,654.87,195.64,60.54"><head>Table 3 :</head><label>3</label><figDesc>Links in Relevant tweets</figDesc><table coords="4,75.54,663.71,195.64,51.70"><row><cell></cell><cell cols="3">With links Total Proportion</cell></row><row><cell>High relevant</cell><cell>527</cell><cell>558</cell><cell>94%</cell></row><row><cell>All relevant</cell><cell>2330</cell><cell>2864</cell><cell>81%</cell></row><row><cell>Not relevant</cell><cell>20200</cell><cell>37900</cell><cell>53%</cell></row><row><cell>Judged</cell><cell>4350</cell><cell>40764</cell><cell>11%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.42,711.19,195.74,7.86"><p>Available at https://github.com/spacelis/tcrawl.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,58.40,711.19,118.68,7.86"><p>http://wordnet.princeton.edu</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,321.30,409.19,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,435.92,189.81,7.86;4,331.01,446.38,219.97,7.86;4,331.01,456.84,148.68,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,388.72,435.92,132.09,7.86;4,331.01,446.38,172.28,7.86">Understanding inverse document frequency: on theoretical arguments for idf</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,509.61,446.38,41.36,7.86;4,331.01,456.84,58.81,7.86">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,468.30,209.49,7.86;4,331.01,478.76,219.65,7.86;4,331.01,489.22,193.95,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,505.46,468.30,35.03,7.86;4,331.01,478.76,215.92,7.86">Okapi at trec-7: automatic ad hoc, filtering, vlc and interactive</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,357.78,489.22,138.90,7.86">Seventh Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,500.68,210.79,7.86;4,331.01,511.14,211.75,7.86;4,331.01,521.60,127.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,401.78,500.68,140.01,7.86;4,331.01,511.14,163.76,7.86">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,501.39,511.14,41.36,7.86;4,331.01,521.60,58.81,7.86">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,533.05,210.11,7.86;4,331.01,543.51,209.94,7.86;4,331.01,553.98,210.86,7.86;4,331.01,564.44,150.56,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,416.91,533.05,124.20,7.86;4,331.01,543.51,32.04,7.86">Portfolio theory of information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,381.16,543.51,159.79,7.86;4,331.01,553.98,210.86,7.86;4,331.01,564.44,122.02,7.86">Proc. of the Annual International ACM SIGIR Conference on Research and Development on Information Retrieval (SIGIR)</title>
		<meeting>of the Annual International ACM SIGIR Conference on Research and Development on Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
