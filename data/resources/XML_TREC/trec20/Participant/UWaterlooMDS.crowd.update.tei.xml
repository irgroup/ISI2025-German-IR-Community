<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.63,102.01,416.74,15.46;1,153.41,123.93,305.15,15.46">Crowdsourcing with a Crowd of One and Other TREC 2011 Crowdsourcing and Web Track Experiments</title>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute/Calcul Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,260.88,155.75,90.24,11.45"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.63,102.01,416.74,15.46;1,153.41,123.93,305.15,15.46">Crowdsourcing with a Crowd of One and Other TREC 2011 Crowdsourcing and Web Track Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F29E4FB29BC5EDCFA7A7A208E3AD35FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our submissions to the Crowdsourcing and Web tracks emphasized simplicity in either method, construction or both. Based on preliminary results, we found that if the number of relevance assessments is low, researchers may be better off "self-sourcing" the assessments, i.e. performing the relevance assessments themselves, rather than crowdsourcing the work. For the Crowdsourcing consensus task, we found that a simple weighted majority vote with iteratively refined workers' quality as measured by d (d-prime) performed slightly above the median on the gold test set. Finally, we submitted easy to construct runs to the Web ad-hoc track which had P@10 scores above the median on a majority of the topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year we participated in both the Crowdsourcing and Web tracks. For the Crowdsourcing track, we participated in both the judging task (task 1) and the consensus task (task 2). For the Web track, we only participated in the ad-hoc retrieval task, although our runs were also evaluated for the diversity task. As our efforts in these tracks were unrelated, we discuss each track separately and begin first with the Crowdsourcing track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Crowdsourcing Track</head><p>The Crowdsourcing track had two sub-tasks. Task 1 was called the "assessment" task and involved the study of relevance assessing. Task 2 was called the "consensus" task and involved the study of methods for determining the relevance assessment of a document given the relevance judgments of one or more assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1: Relevance Assessing</head><p>In this task, the Crowdsourcing track provided each group with a set of topic-document pairs to be judged. In our case, we had 20 topics and between 70 and 100 documents per topic assigned to us for a total of 1875 topic-document pairs. The track also provided between 10 and 35 "gold" judgments for each topic that had come from NIST assessors.</p><p>We decided to "self-source" the crowdsourcing task to ourselves given its modest size. We knew from past experience with relevance assessing, that qualified crowd sourced workers can average one document judged every 15 seconds <ref type="bibr" coords="1,416.11,397.46,9.95,8.94" target="#b7">[8]</ref>, and the top 25% of workers judge at a rate of between 3.1 and 8.1 seconds per document. We have previously been involved in judging large numbers of documents <ref type="bibr" coords="1,450.89,433.33,10.51,8.94" target="#b0">[1,</ref><ref type="bibr" coords="1,464.22,433.33,7.00,8.94" target="#b6">7]</ref>, and knew that we could rapidly judge documents. As such, we estimated that we would take between 2 and 4 hours to judge all 1875 documents.</p><p>We built a graphical user interface in C# similar to the command line based interface used by the University of Waterloo in previous TREC tracks <ref type="bibr" coords="1,505.31,506.69,9.95,8.94" target="#b1">[2]</ref>. The interface allows for single keystroke judging of relevance. The interface has two windows. One window shows the plain text version of the web page and the other window shows the jpeg rendered version. We maximized each window in a separate monitor. To allow maximal viewing with minimal scrolling, each monitor was rotated to be in portrait mode. The plain text version highlighted preselected keywords for each topic.</p><p>We did our best to begin judging a topic and not take a break until we were between topics. Sometimes we ran into difficulty with the jpeg display and would need to restart the system. We timed and logged each judgment.</p><p>Contrary to the track instructions, we did see and judge documents multiple times before recording a final judgment. For each document that we saw more than once, we only recorded the final judgment and final amount of time. We needed to judge documents multiple times for a variety of reasons. First, our interface was not bug-free when we began the judging process. As such, we had to rejudge documents until we had the bugs removed from the system. Second, for topic 20812, "free email directory", we marked all but 1 document as non-relevant. Realizing that such a high rate of non-relevance was unlikely, we examined the gold documents and discovered that the NIST assessor had decided to judge documents that linked to email directories as relevant as opposed to following the NIST guidelines that require relevant pages to be the desired relevant page. As such, we rejudged the entire topic to include pages with links to apparently free email directories. Because we examined the gold documents for some topics, we did not submit judgments on the gold.</p><p>Excluding the time to build the interface, the time between topics, and the time for rejudged documents, we took an average of 4.7 seconds per document. We spent about one working day to build the interface, but we believe we would have spent at least this amount of time to build the infrastructure required to crowd-source the work.</p><p>Our self-sourced judging appears to have fared well compared to the crowd-sourced workers of the other teams. For the reported preliminary results, on consensus judged documents, we obtained a true positive rate (precision) of 90.5% and a false positive rate (1 -specificity) of 8.8%. The average true positive rate of the other teams was 77.5% and the false positive rate was 32.5%. While these rates may not hold out for the final reported results, the rates of the crowdsourced workers appear consistent with our experience <ref type="bibr" coords="2,94.14,494.90,9.95,8.94" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Observations</head><p>For some topics we found the plain text representation easier to judge than the jpeg rendered page. The jpeg page can have distracting images shown in it, while with the plain text these are hidden. Likewise, for some topics, it seemed as easy or easier to judge the rendered jpegs. We suspect there is an advantage to having both views available at once, even though we rarely looked at both views for a given document.</p><p>We found the topic descriptions to be very lacking. In many cases, without looking at the gold, it was hard to understand what was to be considered relevant.</p><p>We wish we had placed a "pause timing" button on our interface to allow us to take breaks.</p><p>Relevance assessments of web documents need more than a binary scale. There appears to be little high quality content compared to the large number of junky, but still relevant as per TREC standards, pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 2: Consensus</head><p>For the consensus task, we submitted two runs that differed in how we used the supplied gold judgments. UWatCS2Semi is a semi-supervised run while UWatCS2Unsup is nearly a fully-unsupervised run. UWatCS2Semi was our primary run and the only run for which we have preliminary results at this time.</p><p>The supplied input data had duplicate judgments in it, i.e. a given worker submitted more than one judgment for a given topic-document pair. We simply threw out duplicate judgments and retained the first judgment that we read into our program.</p><p>We computed a document's consensus judgment as the weighted combination of all worker judgments for that document:</p><formula xml:id="formula_0" coords="2,379.55,332.20,160.44,27.29">C(D) = i q 2 i j i i q 2 i (1)</formula><p>where q i is the quality of the ith worker with a judgment j i for the document D. Judgments had a value of 0 for non-relevant and value of 1 for relevant. If the sum i q 2 i was so small as to produce a floating point NaN value for C(D), we set C(D) to 0.5. We set a worker's quality to be the signal detection measure d which equals:</p><formula xml:id="formula_1" coords="2,363.07,462.66,176.92,9.96">d = z(T P R) -z(F P R) ( 2 )</formula><p>where the function z is the inverse of the normal distribution function and converts the TPR or FPR to a z score <ref type="bibr" coords="2,368.79,509.54,9.95,8.94" target="#b4">[5]</ref>, and TPR is the true positive rate and FPR is the false positive rate. We measured a worker's TPR and FPR for the documents for which the worker had a judgment. For the UWatCS2Semi run, we used the gold judgment as truth if it existed, otherwise we used the computed consensus judgment.</p><p>For the UWatCS2Unsup run, we used the computed consensus judgment in all cases. We initialized all workers' quality to be 1. After computing consensus judgments for all documents, we recomputed worker quality with the new judgments. We repeated this process and selected the iteration with the highest accuracy as measured on the gold documents. The process reached the maximum accuracy after 3-4 iterations and showed little decrease in accuracy with additional iterations.</p><p>The UWatCS2Semi run appears to have done okay, scoring above the median but short of the maximum on the held-out gold judgments assessment for key measures of accuracy, true positive rate, and false positive rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Web Track</head><p>For the web track, we submitted 6 runs oriented for evaluation in the ad-hoc task. All web track runs were evaluated for both the ad-hoc and diversity tasks. In general, our runs were designed to be examples of what can easily be done with existing resources.</p><p>All of our runs used Category A (English only) filtered to remove 70% of the spammiest material <ref type="bibr" coords="3,287.79,228.60,9.95,8.94" target="#b2">[3]</ref>. For each document, we converted it into a plaintext representation first using the Jericho HTML parser (http://jericho.htmlparser.net/). We included the HTML page's title and url in the content as well. We also included the Twente anchortext <ref type="bibr" coords="3,255.39,288.38,10.51,8.94" target="#b3">[4]</ref> as content.</p><p>We then indexed and did retrieval using Indri <ref type="bibr" coords="3,287.75,312.66,9.96,8.94" target="#b8">[9]</ref>. We stemmed words with the Krovetz stemmer. Each of the runs was as follows:</p><p>• UWatMDSdm: We used dependence models <ref type="bibr" coords="3,290.51,357.61,10.51,8.94" target="#b5">[6]</ref> on the full query.</p><p>• UWatMDSdmsr: We used dependence models on the query after stopword removal.</p><p>• UWatMDSsql: Query likelihood of full query.</p><p>• UWatMDSqlsr: Query likelihood of query after stopword removal.</p><p>• UWatMDSqlt: Query likelihood of full query. Documents' titles were given extra weight.</p><p>• UWatMDSqltsr: Query likelihood of query after stopword removal. Documents' titles were given extra weight.</p><p>We did not remove stopwords from the documents when we indexed them, and thus were interested in seeing the effect of querying with the raw queries or with stopwords removed from the queries. For the ad-hoc evaluation, UWatMDSqlt scored an ERR@20 of 0.144, which placed our group third among all groups. For the diversity evaluation, UWatMDSqltsr scored an ERR-IA@20 of 0.494, which again placed our group third. Across measures, it appears that in general, the dependence model runs did better than the query likelihood with title emphasized runs, which did better than the plain query likelihood runs. There was little difference between retaining or removing stopwords, but there might be some small advantage to retaining stopwords in the queries.</p><p>For the ad-hoc evaluation, the UWatMDSdm run did as well or better than the median performance for P@10 for 42 of the 50 topics and had the best P@10 on 3 topics and the worst P@10 on 10 topics (on 12 topics the median P@10 equaled the minimum P@10). UWatMDSqlt and UWatMDSql did as well or better than the median on 39 and 38 of the 50 topics for P@10, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This year we participated in the Crowdsourcing and Web tracks. For the Crowdsourcing track, we submitted a "self-sourced" set of relevance assessments that fared well compared to traditional crowd-sourced workers. Our crowdsourcing consensus method was simple in nature but performed a little above the median on primary measures. Finally, our easy to construct web track runs appear to have achieved reasonable performance.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p><rs type="person">David Hu</rs> and <rs type="person">Aiman L. Al-Harbi</rs> assisted with processing of the ClueWeb09 corpus. This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, and in part by the <rs type="funder">University of Waterloo</rs>. This work was made possible by the facilities of the <rs type="institution">Shared Hierarchical Academic Research Computing Network</rs> (SHARCNET:www.sharcnet.ca) and <rs type="funder">Compute/Calcul Canada</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,326.48,583.96,213.52,8.94;3,326.47,595.91,213.53,8.94;3,326.47,607.86,213.54,8.94;3,326.47,619.82,213.53,8.94;3,326.47,631.77,213.56,8.93;3,326.47,643.73,213.57,8.94;3,326.47,655.69,175.79,8.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,326.47,607.86,191.21,8.94">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,523.20,619.82,16.80,8.93;3,326.47,631.77,174.32,8.93">The Thirteenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="3,326.48,677.34,213.52,8.94;3,326.47,689.29,213.51,8.94;3,326.47,701.25,213.52,8.94;3,326.47,713.21,26.56,8.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,466.02,677.34,73.98,8.94;3,326.47,689.29,213.51,8.94;3,326.47,701.25,136.36,8.94">Machine learning for information retrieval: TREC 2009 web, relevance feedback and legal tracks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mojdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,484.80,701.25,50.57,8.93">TREC 2009</title>
		<imprint>
			<publisher>NIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.50,75.17,213.52,8.94;4,87.50,87.12,213.52,8.94;4,87.50,99.08,213.52,8.94;4,87.50,111.03,213.45,8.94;4,87.50,122.99,30.43,8.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,123.64,87.12,177.37,8.94;4,87.50,99.08,148.09,8.94">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-011-9162-z</idno>
	</analytic>
	<monogr>
		<title level="j" coord="4,248.93,99.08,52.08,8.93;4,87.50,111.03,37.31,8.93">Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.51,142.91,213.51,8.94;4,87.50,154.87,213.53,8.94;4,87.50,166.83,213.53,8.94;4,87.50,178.78,213.49,8.94;4,87.50,190.74,94.84,8.94" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,217.64,142.91,83.38,8.94;4,87.50,154.87,144.13,8.94">Mirex: Mapreduce information retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<idno>TR-CTIT-10-15</idno>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
			<pubPlace>Enschede</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Centre for Telematics and Information Technology University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="4,87.50,210.67,213.51,8.94;4,87.50,222.62,213.51,8.94;4,87.50,234.57,22.67,8.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,227.33,210.67,73.68,8.93;4,87.50,222.62,62.25,8.93">Detection theory: a user&apos;s guide</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Creelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.51,254.50,213.53,8.94;4,87.50,266.46,213.51,8.94;4,87.50,278.41,120.41,8.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,219.89,254.50,81.15,8.94;4,87.50,266.46,146.52,8.94">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,256.75,266.46,39.33,8.93">SIGIR&apos;05</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.50,298.33,213.50,8.94;4,87.50,310.29,213.50,8.94;4,87.50,322.25,213.51,8.94;4,87.50,334.20,208.08,8.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,186.18,310.29,114.82,8.94;4,87.50,322.25,128.21,8.94">University of Waterloo at TREC 2010: Legal interactive</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,234.71,322.25,66.29,8.93;4,87.50,334.20,177.56,8.93">The Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.50,354.13,213.51,8.94;4,87.50,366.08,213.50,8.94;4,87.50,378.04,78.35,8.94;4,221.91,378.04,79.10,8.94;4,87.50,389.99,213.51,8.93;4,87.50,401.94,200.62,8.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,237.27,354.13,63.74,8.94;4,87.50,366.08,213.50,8.94;4,87.50,378.04,78.35,8.94;4,221.91,378.04,35.67,8.94">The crowd vs. the lab: A comparison of crowd-sourced and university laboratory behavior</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jethani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,281.92,378.04,19.10,8.93;4,87.50,389.99,213.51,8.93;4,87.50,401.94,148.47,8.93">Proceedings of the SIGIR 2011 Workshop on Crowdsourcing for Information Retrieval</title>
		<meeting>the SIGIR 2011 Workshop on Crowdsourcing for Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,87.51,421.87,213.52,8.94;4,87.50,433.83,213.53,8.94;4,87.50,445.78,213.47,8.94;4,87.50,457.74,213.52,8.94;4,87.50,469.70,66.20,8.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="4,118.23,433.83,182.80,8.94;4,87.50,445.78,182.84,8.94">Indri: A language-model based search engine for complex queries (extended version)</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>IR-407</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CIIR, CS Dept., U. of Mass. Amherst</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
