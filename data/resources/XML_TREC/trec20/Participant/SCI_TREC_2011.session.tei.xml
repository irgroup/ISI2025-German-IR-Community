<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.80,76.44,290.06,13.86">Rutgers at the TREC 2011 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.34,108.28,49.08,10.72"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.34,108.28,29.26,10.72"><forename type="first">Si</forename><surname>Sun</surname></persName>
							<email>sisun@eden.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.02,108.28,64.98,10.72"><forename type="first">Michael</forename><surname>Cole</surname></persName>
							<email>m.cole@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.33,108.28,88.32,10.72"><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
							<email>belkin@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.80,76.44,290.06,13.86">Rutgers at the TREC 2011 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7AD4BEAA8D2AA976CF6128A8CD3A4CF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At Rutgers, we approached the Session Track task as an issue of personalization, based on both the behaviors exhibited by the searcher during the course of an information-seeking episode, and a classification of the task that led the person to engage in information-seeking behavior. Our general approach is described in detail at the Web site of our project, and in the papers available there (http://comminfo.rutgers.edu/imls/poodle); in this section, we give an overview of our approach and how we applied results from our previous studies to the TREC 2011 Session Track. Subsequent sections give details of how we actually did things, our results, and our conclusions about the results. The PoODLE project aimed to develop a "personalization assistant", a client-side application which would monitor the behaviors of a single person on all that person's computing devices, including, but not limited to informationseeking behaviors, and on the basis of these data, construct a model of the person which would be used either to modify the person's queries to search engines, and/or to modify the results of the queries returned by the search engines. The most fundamental aspect of the personalization is determination of the task or goal which led the person to engage in information seeking behavior. Other aspects that are important include determination of the person's degree of knowledge of both the task and of the search topic, and the person's cognitive abilities. The intention is to make these determinations implicitly, through the evidence of past and current behaviors. To this end, the PoODLE project conducted a series of user studies/experiments, in which we controlled the tasks in which the participants were engaged, conducted psychometric tests to judge two cognitive abilities, and elicited, in various ways, estimates of the participants' knowledge of the tasks and topics of the searches they were asked to perform. In these studies, we logged a large variety of searcher behaviors, ranging from eye-tracking on search engine result pages (SERPs) and content pages, to querying and temporal behaviors of various sorts. The analysis of the data thus collected was aimed at determining associations among the different behaviors (our dependent variables) and the task, knowledge and cognitive abilities information we controlled or elicited (our independent variables), and on the influence of the independent variables and the predictive power of the dependent variables on searcher evaluation of the usefulness of documents with respect to the search task. The end result of our PoODLE studies has been the generation of several models for prediction of document usefulness, some based solely on behavioral evidence during the searching process, and some modified according to task type, and/or knowledge. Prediction of document usefulness is then to be used as the basis for either query modification using relevance feedback, or search result re-ranking, based on similarity to predicted useful documents. Since our prediction models were generated on the basis of a relatively small number of searches (typically four searches, by each of between 32 and 40 participants, in two or three studies), on quite specific and controlled search task types, in both TREC genomic track tasks and in uncontrolled searching in the Web, and on behaviors on both SERPs and content pages, it is not clear how these models will work with the data available for the TREC Session Track. So, our general aim in this Track is to discover whether our models will work on these different task types, and with this different type of data, and if not, why not. We addressed the Session Track tasks as follows. First, we manually classified the 76 Sessions by task facets, using the scheme and method described in section 2, based on the Session topic descriptions and narratives. This information was then used for one of our experimental runs, in which the prediction model was specific to each search task type, combined with search behaviors. For the other runs, we used our so-called general prediction models, which are based on different search behaviors, without reference to task type. Since the Session Track data did not allow us to incorporate evidence from behaviors on content pages, we used only data associated with SERPs and various temporal characteristics, such as dwell time on content pages, and time between queries (section 3 describes the models and data in detail). The prediction of both useful and not useful documents was then used to modify the last query but one in each search session in a standard relevance feedback mode, one run with positive only feedback, one with positive and negative relevance feedback, using the Lemur system in remote mode (section 4 describes our methods in detail). The results of this modification are compared against the results of our baseline search using the last query in the search session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The task classification scheme and method 2.1 Overview</head><p>The classification of task types as an important factor influencing information seeking and search has gained increasing attention recent years <ref type="bibr" coords="2,203.61,119.58,78.15,8.79" target="#b1">(Li &amp; Belkin, 2008)</ref>. Among the various ways to conceptualize search tasks, <ref type="bibr" coords="2,511.59,119.58,19.04,8.79;2,72.00,131.10,55.78,8.79" target="#b1">Li &amp; Belkin (2008)</ref> proposed a holistic faceted approach which featured fifteen essential facets. <ref type="bibr" coords="2,434.93,131.10,64.94,8.79">Liu et al. (2010)</ref> focused Li's search task classification scheme on three facets, namely product, goal, complexity, and added another facetlevel -to Li's work. Findings indicate that these task facets were associated with search behaviors including task completion time and decision time. <ref type="bibr" coords="2,215.30,165.66,147.43,8.79" target="#b3">Liu, Belkin, Cole &amp; Gwizdka (2011)</ref> identified naming as an additional facet for usefulness prediction models based on <ref type="bibr" coords="2,228.36,176.94,63.29,8.79">Liu et al. (2010)</ref>. Built on the previous work introduced above, the Rutgers TREC Session Track examined the Product, Goal, Complexity, Level, and Naming of the search tasks (defined in Table <ref type="table" coords="2,351.09,203.10,4.14,8.79" target="#tab_0">1</ref>) while holding constant the other task facets identified in <ref type="bibr" coords="2,123.10,214.62,75.80,8.79" target="#b1">Li &amp; Belkin (2008)</ref>, including Source of task; Task doer; Time (length) Process; Goal (quantity); Interdependence; and Urgency. For TREC session data, we found only tasks with factual products and no task with combined goals. A task with a goal that cannot be measurable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Level</head><p>Judgment for task level is mainly based on questions asked in the field "Narratives". Tasks with only one question or with a second or further questions asking for more details on the first one were classified as being at Document level. For example, Session No. 28 asked "What are the origins of nicknames? Is there a religious, cultural, or ethnic factor? Not relevant are websites with searches for nicknames. Specific nicknames are not relevant". The second question is a specification of the first question. Rather than asking for more information, the second question narrows down or provide examples of answers for the first question. Session No.76 required searchers to "Give me any information on glenohumeral subluxation, including pictures". Because searchers participating in TREC were permitted to end their search when they found any piece of information on the topic, sessions whose descriptions include only one question were answerable with information from a single document. Sessions with multiple questions in their "Narratives" usually require searching for pieces of segment information from one or multiple documents/sources. Session No.4, for example, asked for information on infections in the groin: "How are they caused? What treatments are there? Can the infections be painful? Are any contagious? Are any groin infections strictly gender related?" Four aspects of the disease including cause, treatment, symptoms, and characteristics were asked in the five questions which require capturing of segments of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Goal</head><p>A task is classified as "specific" when it specifies at least one aspect of the object in the field "Description" and "amorphous" when "all information" or "any information" is required about a subject, and does not specify any aspects in the "Description" field. Session 76, for example, asked for "any information on glenohumeral subluxation" which makes the task goal unclear. Participants might look for definitions on the concept, discussion on the concept, or any other pieces of information related to the concept. Session No.23, on the other hand, has specific goals including searching for information on the time, reason, objective, requirements, target contestants, and prize structure of "dupont science essay contest".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Naming</head><p>Distinguishing between named and unnamed tasks is based on interpretation from the field "Description". For example, the task used in Session No.1 was described as "Find information about the peace corps". Searchers locate factual information about the named fact "peace corps". Session No.1 was thus classified as "Named". Session No. 22 (described as "Why do people get shoulder joint pain?"), on the other hand, asked for the causes of shoulder joint pain which is not named in the description -searchers have to infer what information is needed in order to answer this question; searching on definition of shoulder joint pain is not likely to result in retrieval of relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Product</head><p>Decision for the product of tasks was made based on questions listed in the field "Narratives". For example, in Session No.3, questions asked included "What are the treatments for renal cell cancer? Which treatments are experimental? What organizations are doing research for treatments?" should result in factual products: treatment, experimental treatment, organizations researching on treatments respectively. Unfortunately, none of the tasks led to mixed product, as all of them ask for information that requires no intellectual processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tasks types for TREC</head><p>The task types of 76 sessions were manually classified by two doctoral students independently according to the classification scheme introduced above. An initial classification was produced after the two coders compared notes and discussed to reach an agreement. A third coder (faculty) confirmed and made minor revisions to the discussion results which were agreed upon by all three coders. The final classification is presented in Table <ref type="table" coords="3,459.37,338.46,3.73,8.79" target="#tab_1">2</ref>. All tasks were expected to have Factual products. A majority of the tasks (66) were at a Segment level, with Specific goal(s), and Named. The other three task types add up to 10 tasks together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The prediction models</head><p>In this section we provide a description of how we arrived at the models that were used for prediction of document usefulness, and a specification of the models themselves.</p><p>Our group implemented an implicit relevance feedback method to personalize search results. In particular, we used several prediction models generated from our previous studies to predict the usefulness of documents returned by queries through analysis of users' interactions during their search sessions, moderated by considering task type as a contextual factor. RL1 is our baseline run, which used Pseudo Relevance Feedback on the last queries users issued in each session. We used the default parameters in the Indri Retrieval System, as follows:</p><p>Parameters for Pseudo Relevance Feedback: int fbDocs = _param.get( "fbDocs" , 10 ); int fbTerms = _param.get( "fbTerms" , 10 ); double fbOrigWt = _param.get( "fbOrigWeight", 0.5 ); double mu = _param.get( "fbMu", 0 ); RL2 only considered queries in the search session. In <ref type="bibr" coords="4,287.50,74.46,65.79,8.79" target="#b2">Liu, et al. (2010)</ref>, we analyzed and evaluated the effectiveness of query reformulations in different types of tasks. To categorize users' reformulation of queries in search sessions, we created an algorithm to detect the length and term changes in two successive queries and marked the type of query reformulation for all the queries excluding the last queries, since users did not reformulate queries after the last queries. The categories of reformulation are described in <ref type="bibr" coords="4,317.40,120.54,61.26,8.79" target="#b2">Liu, et al., 2010</ref>. One of our results is that after visiting and saving a useful web page, Generalization was less likely to be used while New Query was more likely to be used. In the Session Track log, there were 190 queries excluding the last queries, and we marked their reformulation types using our exiting algorithm. Among them, 51 were marked as Generalization using our algorithm, and they were regarded as "not good" queries; all other queries were regarded as "good" queries. To generate the personalized good queries, we combined users' last queries with all "good" queries in each session, and then selected only distinct query terms from them as the final queries for each session to generate the personalized results for RL2. Our RL3 run is based on the "good" and "not good" queries selected in RL2. In RL3, we regarded all documents on search result pages (SERPs) under "good" queries as "useful documents", and all documents on SERPs under "not good" queries as "non-useful documents". These results were used to do positive and negative Relevance Feedback to generate the expanded queries.</p><p>In RL4, we considered all user interactions available in the log, and used several of them that had been shown to be included in the prediction models in our PoODLE project. In one of our previous efforts, Liu, Belkin, Cole and Gwizdka (2011), we examined multiple user interactions on both content pages and search result pages, with respect to document usefulness and task type, and generated several prediction models of document usefulness. Our results demonstrated that combining multiple behaviors on content pages and search result pages could improve prediction of useful documents. In addition, the specific prediction models for each type of task demonstrated improved prediction results. User behavioral measures in our prediction models include: dwell time on content pages; number of times a page has been visited in one search episode (visit_id); time to first click after issuing a query (time_to_first_click); number of mouse clicks and number of keyboard activities on content pages; the total dwell time on SERPs during that query interval (serp duration); the proportion of time on content pages of the total dwell time during that interval (prop_content); the total number of content pages visited during that query interval (content_count); and, the difference between the dwell time on a content page, and the average dwell time on all content pages during its associated query interval (diff_content). Among these behavioral measures, users' interactions on content pages (i.e. number of mouse movements and keyboard activities) are not available in the interaction log of Session Track. Therefore, we only considered the other available variables in the prediction models for RL4 in our submissions.</p><p>The general model we use is described below. This prediction model was used to generate results for RL4 in submission 1: Rgposneg (general model with pos/neg RF).</p><p>We have existing prediction models for two of four types of tasks we identified in the 76 sessions in the Session track (Table <ref type="table" coords="4,123.08,617.34,3.60,8.79" target="#tab_1">2</ref>). One is task type "SSN", which has task facets of Segment level, Specific goal, Named information objects..This model is called CPE 1 . The other is task type "DAN", which has task facets of Document level, Amorphous goal and Named information objects. This model is called OBI. These two types of tasks covered 68 sessions of the 76 TREC sessions (89.5%). We used the general model for all other types of tasks. We selected the median value of all probabilities in the results as the cutoff point, which was 0.3.</p><p>1 These models are based on the different tasks we asked searchers to perform in our initial studies. CPE is a "copy editing", or fact-checking task, and OBI is the task of writing an advance obituary.</p><p>rule_general: if visit_id &gt; 1, then it is a useful page; if dwell time &gt; 28.55 seconds (this is the median of dwell time) then it is a useful page; else if time-to-first-click &gt; 6.33 seconds and time-to-first-click &lt;14.55 seconds [this is the median], then it is a useful page;</p><p>else non-useful pages.</p><p>The specific model we use is shown below. We then performed two types of Relevance Feedback on the prediction results: positive and negative Relevance Feedback to generate submission 2; Rsposneg (specific model with positive and negative RF). For submission 3: Rspos (specific model with positive RF only), we performed only positive Relevance Feedback on the prediction results.</p><p>For all of the RL4 runs, the expanded query terms from the prediction models were added to the last-1 queries. The reason for this is that there is no information about the user interaction on the last queries in the log, and our models could only take account users' interactions through last-1 queries. Thus, we compared the final user query results, with the results of our modification of the last query-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Queries and runs</head><p>This section describes the construction of queries for each method in each run, how they were submitted to Lemur, and what we did to the results. RL1 is our baseline run, which used Pseudo Relevance Feedback on the last queries users issued in each session. We used the default parameters in Indri Retrieval System, as follows:</p><p>For RL4 in our three official submissions, we did two types of Relevance Feedback (RF) on the prediction results: both positive and negative RF. Submission 1 is called Rgposneg (general model with pos/neg RF); submission 2 is called Rsposneg (task type-specific model with pos/neg RF); submission 3 is positive RF only and is called Rspos (task type-specific model with positive RF only).</p><p>From the prediction of the usefulness of documents (as described in section 3), we calculated the term frequency for each term in the corpus of useful documents, and in the corpus of non-useful documents. The observed (i.e. predicted) term frequency was then discounted by the prior of the expectation of appearance in a random document in the language using the Brown corpus.</p><p>With respect to the number of useful and non-useful terms for query expansion, we used the approach described in TREC-6 RU <ref type="bibr" coords="5,124.78,643.50,75.92,8.79" target="#b0">(Belkin et al, 1998)</ref>, in which a negative RF system was implemented. The number of suggested feedback terms was determined by the formula: 5n + 5, where n = number of judged documents to a maximum of 25 suggested terms. The query was parsed as a weighted sum, using the default weighting for RF term addition for positive terms, and adding the negative terms under the InQuery "NOT" operator, with 0.6 weight. Two relevance feedback methods were implemented: Parameters for Pseudo Relevance Feedback: int fbDocs = _param.get( "fbDocs" , 10 ); int fbTerms = _param.get( "fbTerms" , 10 ); double fbOrigWt = _param.get( "fbOrigWeight", 0.5 ); double mu = _param.get( "fbMu", 0 ); * diff_content: the difference between the dwell time on a content page and the average dwell time on all content pages during its associated query interval Positive relevance feedback only. In the runs with positive RF only, the predicted "useful" documents were used to calculate the term frequency and the top 25 terms were selected to be useful terms and then expanded with the last-1 queries in the session. Both positive and negative relevance feedback. In the runs with both positive and negative RF, the predicted "useful" documents were used to calculate the term frequency for "useful" terms and the top 15 terms were selected to be "useful" terms; the predicted "non-useful" documents were used to calculate the term frequency for "nonuseful" terms and the top 10 terms were selected to be "non-useful" terms. We then combined the last-1 queries with the 25 "useful" terms (with weight 1.0), and the 10 "non-useful" terms (with weight 0.6) using the Indri query language. If the session had only useful documents clicked, then only the useful documents were considered to select "useful" terms to accomplish positive RF. If the session had only non-useful documents clicked, then only the non-useful documents were considered for selecting "non-useful" terms to do negative RF. If the session contained no clicked documents, then the SERP documents were used to supply the documents for the corpus of "non-useful" documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mean of our results</head><p>We first calculated the average performance of each of our models on all measures. For most measures, the performance of RL2 was better than RL1, RL3 was better than RL2, and RL4 was better than RL3. Among the three RL4 models, Rgposneg and Rgpos (prediction based on the complete general model) was better than those generated from task-specific models (Rsposneg and Rspos). Comparing between Rgposneg and Rgpos, Rgpos (general model with only positive relevance feedback) performed a bit better than Rgposneg (general model with both positive and negative relevance feedback); and comparing between Rsposneg and Rspos, we found that Rspos (task-specific model with only positive relevance feedback) was better than Rsposneg (task-specific model with both positive and negative relevance feedback). Comparing between Tables <ref type="table" coords="6,306.06,347.10,4.92,8.79" target="#tab_2">3</ref> and<ref type="table" coords="6,330.50,347.10,3.73,8.79" target="#tab_3">4</ref>, we see that our models performed better when all subtopics were considered for evaluation than when only subtopics in last query were evaluated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improvement over baseline</head><p>We then further examined the improvement of our models over our baseline, and between each other. We also compared our results with minimum, median and maximum of all TREC results. We use ERR (Expected Reciprocal Rank), and ndcg@10 for evaluation of our results. ERR was selected because it was based on the "cascade" user model, which is similar to the personalization models we adopted in our method, and ndcg@10 because it is the "basic" evaluation measure for the track.</p><p>When calculating the improvement of each of our models over our baseline, we calculated both absolute and percent improvement. In the calculation of percent improvement, the sessions whose baseline measure was 0 (i.e., no relevant documents retrieved) were excluded because we are unsure how to calculate the percent improvement for those sessions.</p><p>The average improvements on ERR and ndcg@10 are shown in Table <ref type="table" coords="7,354.78,200.70,3.71,8.79" target="#tab_4">5</ref>. When all subtopics were considered for evaluation, all models achieve some improvement, and the models which were based on all interactions in the session (RL4) achieved much more improvement than RL2 and RL3. Among them, RL4_rgpos had highest mean improvement on both ERR and ndcg@10. When comparing the improvement using Wilcox tests, we did not find significant difference among them. When the subtopics of the last query were considered for evaluation, all models achieve some improvement. Among them, RL4_rspos and RL4_rsposneg had highest mean improvement. When comparing the improvement using Wilcox tests, we did not find significant difference among them either. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance between our different models</head><p>When comparing the performance between our different models, we also adopted the similar method of improvement, which is to calculate the improvement from one model to another.</p><p>Table <ref type="table" coords="7,97.25,670.62,4.92,8.79" target="#tab_6">7</ref> shows improvements when the all subtopics were considered for the evaluation on ERR. It demonstrats that all our RL4 models had some improvement over RL2 and RL3. The absolute improvement shows that the improvement from RL4 rgpos over RL2 was the greatest. The percent improvement showed that the improvement from RL4 rgpos over RL4 rgposneg was the greatest. Table <ref type="table" coords="8,97.26,193.02,4.92,8.79" target="#tab_7">8</ref> shows improvements when the all subtopics were considered for the evaluation using ndcg@10. It also demonstrats that all our RL4 models had some improvement over RL2 and RL3. The absolute improvement shows that the improvement from RL4 rgpos and RL4 rgponeg over RL2 was the greatest, and the percent improvement showed RL4 rgpos achieved better improvement over RL2 than RL4 rgposneg. </p><p>Table <ref type="table" coords="8,97.26,386.46,4.92,8.79" target="#tab_8">9</ref> shows improvements when the subtopics of last queries were considered for evaluation using ERR. It demonstrats again that all our RL4 models had some improvement over RL2 and RL3, and the most improvement was achieved from RL4rspos over RL2. Table <ref type="table" coords="8,97.26,553.26,9.92,8.79" target="#tab_9">10</ref> showed improvements when the subtopics of last queries were considered for the evaluation on ndcg@10. It demonstrated again that all our RL4 models had some improvement over RL2 and RL3, and the models based on the specific model (RL4 rsposneg, and RL4 rspos) achieved more improvements than the other two models based on the general model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>To recap, we used standard Indri techniques, including pseudo-relevance feedback based on the results of the last query but one in each session to modify the final query, as our baseline performance. For our experimental runs, we used the document usefulness prediction models that were developed from our PoODLE data for Indri relevance feedback to modify the last query but one. We found that, in general, and evaluated by ERR and ndcg@10, all our prediction models led to consistent improvement over our baseline results, and that performance improved monotonically as more data was used by the models (RL4 &gt; RL3 &gt; RL2). However, the absolute results of our techniques are not especially great when compared to median and maximum results for the Track as a whole. However, our baseline technique, to which we applied our prediction models, was itself rather low, compared to the overall Track baselines. Because the data that we used for our improvement algorithms should be applicable to any general retrieval engine, one might expect that our levels of improvement would be applicable to techniques with much higher baseline performance, resulting in higher absolute performance levels. It is also the case that our usefulness prediction models were used as input to quite standard, and rather simple relevance feedback techniques, and that more sophisticated use of the models could result in better overall performance improvement.</p><p>It is of some interest that our "general" prediction model led to better performance improvement than out taskspecific models. One reason for this result could be that our general prediction model does not depend upon "clientside" data, such as activity on SERPs and content pages, which was unavailable, whereas the prediction models depend upon such data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our results have shown that the document usefulness prediction models which were developed from radically different search sessions than those represented in the TREC Session Track, nevertheless led to consistently improved performance over a reasonable baseline that did not take account of session-level information. This positive "transfer" effect leads us to believe that the models we have developed could be used for personalization of retrieval in a variety of searching circumstances, and that we could expect even greater performance benefit when the richer, client-side data that our prediction models depend upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>Thanks to David Pane at CMU, who helped us greatly and generously in performing the Indri runs. The research that led to this work was funded by the IMLS, under grant number LG-06-07-0105-07. We thank all of the members of the PoODLE research team, without whose efforts this work could not have been accomplished.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,79.92,142.38,54.41,8.79;5,79.92,156.78,119.48,8.79;5,79.92,171.42,416.26,8.79;5,79.92,185.82,121.69,8.79;5,79.92,200.22,419.90,8.79;5,79.92,214.86,68.00,8.79;5,79.92,229.26,283.81,8.79;5,79.92,243.90,395.71,8.79"><head></head><label></label><figDesc>rule_specific: if type = SSN, model CPE_L; Model CPE_L: log(p/1-p) = -1.70 + 0.04 * dwelltime-0.01 * time_to_first_click + 0.23 * prop_content if type = DAN, model OBI_L; Model OBI_L: log(p/1-p) = -3.27 + 0.10 * dwelltime+ 0.01 * time_to_first_click -0.01 * diff_content else rule_general * cutoff point is 0.3, the median value of all probabilities in the results. * prop_content: the proportion of time on content pages of the total dwell time during that interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,71.76,252.24,466.90,208.53"><head>Table 1 . Facets of task which were varied for the TREC Session Track</head><label>1</label><figDesc></figDesc><table coords="2,71.76,281.58,466.90,179.19"><row><cell>Facets</cell><cell>Values</cell><cell>Operational Definitions/Rules</cell></row><row><cell>Naming</cell><cell>Named</cell><cell>A task locating factual information to confirm or disconfirm named fact</cell></row><row><cell></cell><cell>Unnamed</cell><cell>A task locating factual information to about unnamed fact</cell></row><row><cell>Product</cell><cell>Factual</cell><cell>A task locating facts, data, or other similar items in information systems</cell></row><row><cell></cell><cell>Mixed</cell><cell>A task which produces new ideas or findings on the basis of locating facts,</cell></row><row><cell></cell><cell></cell><cell>data, or other similar items in information systems</cell></row><row><cell></cell><cell>Intellectual</cell><cell>A task which produces new ideas or findings on the basis of locating facts</cell></row><row><cell>Level</cell><cell>Document</cell><cell>A task for which a document as a whole is judged</cell></row><row><cell></cell><cell>Segment</cell><cell>A task for which a part or parts of a document are judged</cell></row><row><cell>Goal</cell><cell>Specific goal</cell><cell>A task with a goal that is explicit and measurable</cell></row><row><cell>(Quality)</cell><cell cols="2">Combined goal A task with both concrete and amorphous goal</cell></row><row><cell></cell><cell>Amorphous</cell><cell></cell></row><row><cell></cell><cell>goal</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,71.76,353.28,466.59,106.53"><head>Table 2 . Variable facet values Task type</head><label>2</label><figDesc></figDesc><table coords="3,71.76,368.16,466.59,91.65"><row><cell></cell><cell>Task type</cell><cell>Level</cell><cell>Goal(quality)</cell><cell>Naming</cell><cell>Product</cell><cell>number of</cell></row><row><cell></cell><cell>(abbreviation)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tasks</cell></row><row><cell>A</cell><cell>SSN</cell><cell>Segment</cell><cell>Specific</cell><cell>Named</cell><cell>Factual</cell><cell>66</cell></row><row><cell>B</cell><cell>SSU</cell><cell>Segment</cell><cell>Specific</cell><cell>Unnamed</cell><cell>Factual</cell><cell>4</cell></row><row><cell>C</cell><cell>DSN</cell><cell>Document</cell><cell>Specific</cell><cell>Named</cell><cell>Factual</cell><cell>4</cell></row><row><cell>D</cell><cell>DAN</cell><cell>Document</cell><cell>Amorphous</cell><cell>Named</cell><cell>Factual</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76 (total)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,387.60,499.46,136.77"><head>Table 3 . TREC 2011 allsubtopics Evaluation</head><label>3</label><figDesc></figDesc><table coords="6,72.00,402.54,499.46,121.83"><row><cell>Run</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell><cell cols="2">Rgposneg RL4 Rgpos RL4</cell><cell>Rsposneg RL4</cell><cell>Rspos RL4</cell></row><row><cell>Measure</cell><cell>(Baseline)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>err</cell><cell>0.1915</cell><cell>0.2131</cell><cell>0.2417</cell><cell>0.2647</cell><cell>0.2712</cell><cell>0.2468</cell><cell>0.2534</cell></row><row><cell>err@10</cell><cell>0.1779</cell><cell>0.1999</cell><cell>0.2367</cell><cell>0.2592</cell><cell>0.2656</cell><cell>0.2400</cell><cell>0.2465</cell></row><row><cell>nerr</cell><cell>0.2952</cell><cell>0.3346</cell><cell>0.3896</cell><cell>0.4360</cell><cell>0.4471</cell><cell>0.3930</cell><cell>0.4043</cell></row><row><cell>nerr@10</cell><cell>0.2738</cell><cell>0.3151</cell><cell>0.3846</cell><cell>0.4307</cell><cell>0.4415</cell><cell>0.3840</cell><cell>0.3950</cell></row><row><cell>ndcg</cell><cell>0.2939</cell><cell>0.3213</cell><cell>0.2160</cell><cell>0.2418</cell><cell>0.2503</cell><cell>0.2401</cell><cell>0.2488</cell></row><row><cell>ndcg@10</cell><cell>0.1970</cell><cell>0.2297</cell><cell>0.3030</cell><cell>0.3395</cell><cell>0.3442</cell><cell>0.3053</cell><cell>0.3100</cell></row><row><cell>ap</cell><cell>0.0868</cell><cell>0.1002</cell><cell>0.0778</cell><cell>0.0865</cell><cell>0.0890</cell><cell>0.0833</cell><cell>0.0858</cell></row><row><cell>gap</cell><cell>0.0807</cell><cell>0.0943</cell><cell>0.0753</cell><cell>0.0885</cell><cell>0.0911</cell><cell>0.0851</cell><cell>0.0877</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.00,542.16,499.45,136.53"><head>Table 4 . TREC 2011 lastquerysubtopics Evaluation</head><label>4</label><figDesc></figDesc><table coords="6,72.00,557.10,499.45,121.59"><row><cell>Run</cell><cell>RL1</cell><cell>RL2</cell><cell>RL3</cell><cell>Rgposneg</cell><cell>Rgpos RL4</cell><cell>Rsposneg RL4</cell><cell>Rspos RL4</cell></row><row><cell>Measure</cell><cell>(Baseline)</cell><cell></cell><cell></cell><cell>RL4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>err</cell><cell>0.1135</cell><cell>0.1325</cell><cell>0.1407</cell><cell>0.1430</cell><cell>0.1459</cell><cell>0.1664</cell><cell>0.1732</cell></row><row><cell>err@10</cell><cell>0.0990</cell><cell>0.1200</cell><cell>0.1363</cell><cell>0.1371</cell><cell>0.1400</cell><cell>0.1597</cell><cell>0.1663</cell></row><row><cell>nerr</cell><cell>0.1730</cell><cell>0.2039</cell><cell>0.1972</cell><cell>0.2240</cell><cell>0.2295</cell><cell>0.2510</cell><cell>0.2626</cell></row><row><cell>nerr@10</cell><cell>0.1482</cell><cell>0.1833</cell><cell>0.1895</cell><cell>0.2144</cell><cell>0.2199</cell><cell>0.2408</cell><cell>0.2521</cell></row><row><cell>ndcg</cell><cell>0.2824</cell><cell>0.2953</cell><cell>0.1652</cell><cell>0.1864</cell><cell>0.1902</cell><cell>0.1955</cell><cell>0.2044</cell></row><row><cell>ndcg@10</cell><cell>0.1069</cell><cell>0.1257</cell><cell>0.1489</cell><cell>0.1531</cell><cell>0.1538</cell><cell>0.1760</cell><cell>0.1808</cell></row><row><cell>ap</cell><cell>0.0731</cell><cell>0.0735</cell><cell>0.0622</cell><cell>0.0701</cell><cell>0.0708</cell><cell>0.0674</cell><cell>0.0700</cell></row><row><cell>gap</cell><cell>0.0679</cell><cell>0.0700</cell><cell>0.0596</cell><cell>0.0675</cell><cell>0.0684</cell><cell>0.0663</cell><cell>0.0691</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,261.36,438.72,143.73"><head>Table 5 . Improvement over baseline model (allsubtopics Evaluation)</head><label>5</label><figDesc></figDesc><table coords="7,72.00,276.30,438.72,128.79"><row><cell>Model to be compared</cell><cell>ERR</cell><cell></cell><cell>ndcg@10</cell><cell></cell></row><row><cell>with the baseline</cell><cell>absolute</cell><cell>percent</cell><cell>absolute</cell><cell>percent</cell></row><row><cell></cell><cell>improvement</cell><cell>improvement</cell><cell>improvement</cell><cell>improvement</cell></row><row><cell>RL2</cell><cell>0.02</cell><cell>1.05</cell><cell>0.03</cell><cell>0.77</cell></row><row><cell>RL3</cell><cell>0.05</cell><cell>3.68</cell><cell>0.11</cell><cell>2.75</cell></row><row><cell>RL4 rgposneg</cell><cell>0.07</cell><cell>6.58</cell><cell>0.14</cell><cell>1.79</cell></row><row><cell>RL4 rgpos</cell><cell>0.08</cell><cell>8.04</cell><cell>0.15</cell><cell>2.21</cell></row><row><cell>RL4 rsposneg</cell><cell>0.06</cell><cell>6.68</cell><cell>0.11</cell><cell>2.21</cell></row><row><cell>RL4 rspos</cell><cell>0.06</cell><cell>6.70</cell><cell>0.11</cell><cell>2.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,72.00,463.44,438.72,143.49"><head>Table 6 . Improvement over baseline model (lastquerysubtopics Evaluation)</head><label>6</label><figDesc></figDesc><table coords="7,72.00,478.14,438.72,128.79"><row><cell>Model to be compared</cell><cell>ERR</cell><cell></cell><cell>ndcg@10</cell><cell></cell></row><row><cell>with the baseline</cell><cell>absolute</cell><cell>percent</cell><cell>absolute</cell><cell>percent</cell></row><row><cell></cell><cell>improvement</cell><cell>improvement</cell><cell>improvement</cell><cell>improvement</cell></row><row><cell>RL2</cell><cell>0.02</cell><cell>1.16</cell><cell>0.02</cell><cell>0.66</cell></row><row><cell>RL3</cell><cell>0.03</cell><cell>1.60</cell><cell>0.04</cell><cell>1.75</cell></row><row><cell>RL4 rgposneg</cell><cell>0.03</cell><cell>6.11</cell><cell>0.05</cell><cell>0.99</cell></row><row><cell>RL4 rgpos</cell><cell>0.04</cell><cell>8.9</cell><cell>0.05</cell><cell>1.91</cell></row><row><cell>RL4 rsposneg</cell><cell>0.05</cell><cell>7.30</cell><cell>0.07</cell><cell>1.07</cell></row><row><cell>RL4 rspos</cell><cell>0.06</cell><cell>7.32</cell><cell>0.07</cell><cell>1.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,72.00,74.64,467.69,100.53"><head>Table 7 . Comparison of performance on err between our different models (allsubtopics, absolute and percent)</head><label>7</label><figDesc></figDesc><table coords="8,72.00,89.34,451.57,85.83"><row><cell></cell><cell>over RL2</cell><cell>over RL3</cell><cell>over RL4 rgposneg</cell><cell>over RL4 rgpos</cell><cell>over RL4 rsposneg</cell></row><row><cell>RL3</cell><cell>0.03 (1.69)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgposneg</cell><cell>0.05 (5.04)</cell><cell>0.02 (26.73)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgpos</cell><cell>0.06 (5.86)</cell><cell>0.03 (3.11)</cell><cell>0.01 (55.86)</cell><cell></cell><cell></cell></row><row><cell>RL4 rsposneg</cell><cell>0.03(4.91)</cell><cell>0.01 (11.44)</cell><cell>0.02(0.95)</cell><cell>0.02 (11.07)</cell><cell></cell></row><row><cell>RL4 rspos</cell><cell>0.04 (5.75)</cell><cell>0.01 (11.44)</cell><cell>0.01(0.95)</cell><cell>0.01 (11.08)</cell><cell>-0.01 (-0.00)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,72.00,256.56,456.16,112.05"><head>Table 8 . Comparison of performance on ndcg@10 between our different models (allsubtopics, absolute and percent)</head><label>8</label><figDesc></figDesc><table coords="8,72.00,283.02,438.77,85.59"><row><cell></cell><cell>over RL2</cell><cell>over RL3</cell><cell>over RL4 rgposneg</cell><cell>over RL4 rgpos</cell><cell>over RL4 rsposneg</cell></row><row><cell>RL3</cell><cell>0.07 (1.72)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgposneg</cell><cell>0.11 (1.45)</cell><cell>0.04 (0.92)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgpos</cell><cell>0.11 (1.91)</cell><cell>0.04 (1.09)</cell><cell>0 (0.44)</cell><cell></cell><cell></cell></row><row><cell>RL4 rsposneg</cell><cell>0.08 (1.29)</cell><cell>0(0.75)</cell><cell>0.03 (0.62)</cell><cell>0.03 (0.97)</cell><cell></cell></row><row><cell>RL4 rspos</cell><cell>0.08 (1.29)</cell><cell>0.01 (0.75)</cell><cell>0.03 (0.62)</cell><cell>0.03 (1.03)</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,72.00,424.08,459.05,112.05"><head>Table 9 . Comparison of performance on err between our different models (lastquerysubtopics, absolute and percent)</head><label>9</label><figDesc></figDesc><table coords="8,72.00,450.30,438.77,85.83"><row><cell></cell><cell>over RL2</cell><cell>over RL3</cell><cell>over RL4 rgposneg</cell><cell>over RL4 rgpos</cell><cell>over RL4 rsposneg</cell></row><row><cell>RL3</cell><cell>0.01 (1.42)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgposneg</cell><cell>0.01 (4.25)</cell><cell>0.00 (28.04)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgpos</cell><cell>0.02 (11.99)</cell><cell>0.01 (61.51)</cell><cell>0.01(8.21)</cell><cell></cell><cell></cell></row><row><cell>RL4 rsposneg</cell><cell>0.03 (4.70)</cell><cell>0.03(10.39)</cell><cell>0.02 (1.52)</cell><cell>0.02 (19.31)</cell><cell></cell></row><row><cell>RL4 rspos</cell><cell>0.04 (6.23)</cell><cell>0.02(10.39)</cell><cell>0.03 (1.52)</cell><cell>0.02 (19.35)</cell><cell>0.01 (0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,72.00,602.40,438.77,112.05"><head>Table 10 . Comparison of performance on ndcg@10 between our different models (lastquerysubtopics, absolute and percent)</head><label>10</label><figDesc></figDesc><table coords="8,72.00,631.26,438.77,83.19"><row><cell></cell><cell>over RL2</cell><cell>over RL3</cell><cell>over RL4 rgposneg</cell><cell>over RL4 rgpos</cell><cell>over RL4 rsposneg</cell></row><row><cell>RL3</cell><cell>0.02 (1.68)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgposneg</cell><cell>0.03 (0.91)</cell><cell>0 (1.2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RL4 rgpos</cell><cell>0.03 (1.17)</cell><cell>0.01 (0.68)</cell><cell>0 (0.35)</cell><cell></cell><cell></cell></row><row><cell>RL4 rsposneg</cell><cell>0.05 (1.32)</cell><cell>0.03 (1.72)</cell><cell>0.02 (0.79)</cell><cell>0.02 (0.09)</cell><cell></cell></row><row><cell>RL4 rspos</cell><cell>0.05 (1.32)</cell><cell>0.03 (1.73)</cell><cell>0.03 (0.79)</cell><cell>0.03 (0.01)</cell><cell>0 (0)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,86.40,478.33,55.96,10.50;9,72.00,494.70,452.54,8.79;9,72.00,506.22,318.18,8.79" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,410.61,494.70,113.94,8.79;9,72.00,506.22,64.08,8.79">Rutgers&apos; TREC-6 interactive track experience</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>References Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Rieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,142.79,506.73,204.65,8.28">Proceedings of the Sixth Text REtrieval Conference</title>
		<meeting>the Sixth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,520.62,442.36,8.79;9,72.00,532.14,316.32,8.79;9,72.00,543.66,176.29,8.79" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,190.33,520.62,267.71,8.79">A faceted approach to conceptualizing tasks in information seeking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="DOI">=10.1016/j.ipm.2008.07.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ipm.2008.07.005" />
	</analytic>
	<monogr>
		<title level="j" coord="9,464.69,520.62,49.67,8.79;9,72.00,532.14,34.64,8.79">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1822" to="1837" />
			<date type="published" when="2008-11">2008. November 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,558.30,449.30,8.79;9,72.00,569.82,457.93,8.79;9,72.00,581.10,448.08,8.79;9,72.00,592.62,112.99,8.79" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,317.53,558.30,203.77,8.79;9,72.00,569.82,75.19,8.79">Analysis and evaluation of query reformulations in different task types</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwizdka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,164.48,570.33,365.45,8.28;9,72.00,581.61,42.08,8.28">Proceedings of the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem</title>
		<meeting>the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem<address><addrLine>Silver Springs, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Society for Information Science</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,607.26,456.82,8.79;9,72.00,618.78,457.94,8.79;9,72.00,630.30,317.37,8.79" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,279.76,607.26,249.05,8.79;9,72.00,618.78,21.49,8.79">Personalization of Information Retrieval in Different Types of Tasks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwizdka</surname></persName>
		</author>
		<ptr target="http://select.cs.cmu.edu/meetings/enir2011/papers/liu-belkin-cole-gwizdka.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,166.14,619.29,237.01,8.28">Workshop on Enriching Information Retrieval (ENIR 2011)</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-28">2011. July 28, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,644.70,460.16,8.79;9,72.00,656.22,450.44,8.79;9,72.00,667.74,158.82,8.79" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,416.41,644.70,115.75,8.79;9,72.00,656.22,38.25,8.79">Search behaviors in different task types</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bierig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwizdka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.55,656.22,394.90,8.79;9,72.00,667.74,80.74,8.79">Proceedings of ACM-IEEE Computer Society Joint Conference on Digital Libraries (JCDL) 2010. Goldcoast, Australia</title>
		<meeting>ACM-IEEE Computer Society Joint Conference on Digital Libraries (JCDL) 2010. Goldcoast, Australia</meeting>
		<imprint>
			<date type="published" when="2010-06-21">2010. June 21-25, 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
