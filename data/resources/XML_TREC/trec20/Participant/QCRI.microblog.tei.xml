<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,185.54,74.87,240.97,13.34">QCRI @ TREC 2011: M icroblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.29,103.07,55.19,9.86"><forename type="first">Ali</forename><surname>El-Kahki</surname></persName>
							<email>ali_kahki@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.91,103.07,74.81,9.86"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
							<email>kdarwish@qf.org.qa</email>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,185.54,74.87,240.97,13.34">QCRI @ TREC 2011: M icroblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">354B3B02D310FCEB48061EF48B0C49B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper briefly describes the Qatar Computing Research Institute (QCRI) participation in the TREC 2011 Microblog track. The focus of our TREC submissions was on using a generative graphic model to perform query expansion. We trained a model that attempted to predict appropriate hashtags to expand tweets as well as queries. In essence, we used hashtags to represent latent topics in tweets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">I ntroduction</head><p>Searching tweets, a popular type of microblogs, poses interesting research problems. Some of these problems include: 1) the short length of tweets limits the contexts that are available for search; and 2) the language of tweets typically contains non-standard abbreviations and colloquial expressions. We focused on solving the first problem that is related to the short length of tweets. In particular, we focused on bringing more contexts to tweets by expanding tweets and queries alike using appropriate hashtags. Essentially, we used hashtags to represent latent underlying topics in tweets. <ref type="bibr" coords="1,413.36,356.90,99.32,9.86" target="#b1">Massoudi et al. (2011)</ref> showed that Hashtags can be effective expansion terms in the context of search in microblogs. Though hashtags appear in less than 19% of all tweets 1 and popular hashtags are often used by spammers, there are sufficient numbers of tagged tweets to build effective hashtag models. We used a Latent Dirichlet Allocation (LDA) like graphical model to learn the relationship between words, latent topics, and hashtags. We assumed that the relationship between latent topics and hashtags to be m to n and that each tweet contains only one topic.</p><p>In remainder of the paper, we will describe: the preprocessing we performed on tweets (Sec. 2); the graphical model that we employed (Sec. 3); the experimental setup of the submitted runs (Sec. 4); and official TREC results for our runs (Sec. 5). We finally conclude the paper (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tweet Preprocessing</head><p>According to the track guidelines, only English tweets are considered relevant. Thus, we needed to extract the English from the approximately 16 million tweets in the collection. We used the languagedetection open source Java library 2 . In all, we extracted roughly 4.8 million English tweets. We signify hashtags and user mentions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graphic M odel</head><p>We used an LDA-like graphical model. Figure <ref type="figure" coords="1,282.60,656.83,5.52,9.86" target="#fig_0">1</ref> shows the plate representation of the model that we used. Formally, for each tag T, a set of documents D, t represents a distribution over different topics Z.</p><p>1 Based on the English tweets in the TREC Microblog dataset 2 http://code.google.com/p/language-detection/ For each tag also, there is a set of topics kt and distribution t of for background and foreground probability. Each document is represented by its topic Z and a set of words. Words w are generated from bg or from corresponding topic distribution Z,t based on whether the word is background or foreground which is determined by the binary variable y. The main differences between the standard LDA model and our model are 1. We assumed that each tweets is generated from only topic. This is a reasonable assumption for short documents like tweets and it has been used by <ref type="bibr" coords="2,338.47,458.92,77.12,9.86" target="#b2">Zhao et al. (2011)</ref>. 2.</p><p>t to model different topics covered by the same tag. We limited the number of topics per tag to 3. 3. We used a global background distribution over all tags denoted by bg . Background word distribution has been used before in LDA models <ref type="bibr" coords="2,327.55,517.12,79.62,9.86" target="#b2">(Zhao et al., 2011)</ref>. Our Model is different because we used separate background to foreground probability for each tag denoted by t</p><p>We used the Factorie toolkit to describe the model and to perform inference. Factorie is an open source package that allows for factored graph construction, parameter estimation, and inference <ref type="bibr" coords="2,463.89,570.76,61.24,9.86;2,72.02,585.31,40.00,9.86">(McCullam et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>In inspecting the hashtags that were used in the tweets, we found that there were 223,145 unique hashtags, of which 203,065 were used 5 times or fewer. Hashtags that appear very few times may represent hashtags that were not adopted by other users for reasons beyond the scope of this paper. Some of the hashtags that were used just once include: #ilovejakewolf, #federalism, #whencanistart, #grungy, #andywho, and #promisingnight. We chose tags that appeared at least 100 times in the tweets, limiting the number of hashtags to 1,208. There were two main reasons for this choice: 1) we wanted tags that cover broad or more popular interests, and 2) we were constrained by the computational capabilities<ref type="foot" coords="3,383.59,295.34,3.48,6.22" target="#foot_0">3</ref> .</p><p>Then given all tweets in the collection (regardless of whether or not they had hashtags), we used our model to generate the most likely 5 hashtags for each tweet and for each query. The hashtags were appended to the query or the tweet if the probability of the inferred hashtag was greater than 0.001. Hashtag prediction had mixed results with variable success. Table <ref type="table" coords="3,368.20,365.78,5.52,9.86" target="#tab_1">1</ref> shows some the inferred hashtags for some tweets and queries. Some of the successfully inferred hashtags are those for T2 and Q10. Partial success was achieved for T1 and Q2, and no success was achieved for T3 and Q50. Further analysis is required to approximate the accuracy of hashtag prediction. However, it is noteworthy that consistency of prediction could be more important than correctness of prediction. For example, for the two tweets: he bought sugarfree apple juice and inferred. Though it is incorrect for both, perhaps eating apples is related to drinking apple juice and the inferred hashtag can help connect them. Again, further error analysis is required to ascertain the effect of such errors. We used Indri to index all tweets twice: once in their original form, and a second time with inferred hashtags. Likewise, we submitted two runs: first without the inferred hashtags; and second with the inferred hashtags. When inferred hashtags were included in queries, the original query was assigned 75% of the weight of the query and inferred hashtags were given 25% of the weight. The inferred hashtags were treated as weighted synonyms using the Indri #wsyn operator. Since the top 30 tweets were to be judged by NIST, we limited the number of results to 30 per query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>Table <ref type="table" coords="4,99.86,197.63,5.52,9.86" target="#tab_2">2</ref> shows the results of the two runs that we submitted to TREC. As can be seen from the results, the inclusion of the inferred hashtags had a very slight positive effect on retrieval effectiveness, but all differences were not statistically significant. We used a paired two-tailed t-test with p &lt; 0.05 to ascertain statistical significance. The reasons why inferring hashtags had little effect need further investigation. We suspect that there is an error in the run with inferred hashtags. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we described the QCRI submissions to the TREC Microblog track. The essence of the work revolved around expanding tweets and queries using hashtags. We used a generative graphical model for inferring hashtags. Further investigation is required to estimate the accuracy of hashtag prediction. The effect of expansion using hashtags was very limited. We need to debug our experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,198.77,142.67,214.46,9.86"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Generative process of proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,218.46,196.72,238.45,186.41"><head></head><label></label><figDesc>Tags that are used tens of times (not hundreds of times) tend to belong some of the following categories: Too specific such as #veronicamarsmovie (used 55 times) Unpopular tags such as #greatmovie (used 14 times) and #cinema (used 30 times), where there are more popular tags such #movies and #movie (used 400 and 220 times respectively)</figDesc><table coords="2,218.46,196.72,238.45,186.41"><row><cell cols="3">Topics with waning interest such as #tudors, #thetudors, #tudors4, and #tudor (used 7, 3, 3, 1</cell></row><row><cell>times respectively)</cell><cell></cell><cell></cell></row><row><cell cols="2">Nondescript tags such as #days and #all (used 37 and 70 times respectively)</cell><cell></cell></row><row><cell cols="3">t Tags of narrow interest such as #luton and #medicaljobs (used 38 and 36 times respectively) Z</cell></row><row><cell cols="3">More frequent tags, that are used hundreds of times, typically have broad interest. Some examples with</cell></row><row><cell cols="3">high count include #xbox (used 240 times), #breakingnews (used 170 times), #blackhistorymonth (used</cell></row><row><cell cols="2">869 times), #packers (used 2,226 times), and #jan25 (used 17,810 times).</cell><cell></cell></row><row><cell>kt</cell><cell>w</cell><cell>bg</cell></row><row><cell>K t</cell><cell></cell><cell></cell></row><row><cell></cell><cell>y</cell><cell>t</cell></row><row><cell></cell><cell>N w</cell><cell></cell></row><row><cell>t</cell><cell></cell><cell></cell></row><row><cell>Tag</cell><cell>D</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.02,506.68,459.39,142.98"><head>Table 1 :</head><label>1</label><figDesc>Sample tweets and queries and the inferred hashtagsNo.</figDesc><table coords="3,72.02,521.80,459.39,127.86"><row><cell></cell><cell>Tweet/Query</cell><cell>Inferred hashtags</cell></row><row><cell>T1</cell><cell>saying no to carbs is saying no to fat loss. what follows is a dull mind, tired body &amp; frustrated soul.</cell><cell>#health, #fitness, #gemini, #sagittarius, #knockitoff</cell></row><row><cell>T2</cell><cell>with journalists hounded out of tahrir square the crackdown on protestors could be worse overnight, with few cameras to catch it</cell><cell>#tahrir, #cairo, #jan25, #egypt, #aljazeera</cell></row><row><cell>T3</cell><cell>"great things are not done by impulse, but by a series of small things brought together." vincent van gogh</cell><cell>#blackparentquotes, #cricket, #skins, #arsenal, #neversaynever</cell></row><row><cell>Q2</cell><cell>2022 FIFA soccer</cell><cell>#football, #sports</cell></row><row><cell cols="2">Q10 Egyptian protesters attack museum</cell><cell>#jan25, #egypt, #mubarak, #cairo, #tahrir</cell></row><row><cell cols="2">Q50 war prisoners hatch act</cell><cell>#ipod, #iphone, #ipad</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,208.01,280.46,209.60,103.22"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="4,208.01,280.46,209.60,103.22"><row><cell></cell><cell cols="2">. Official TREC Results</cell></row><row><cell></cell><cell>Original Queries</cell><cell>Original +</cell></row><row><cell></cell><cell></cell><cell>Inferred Hashtags</cell></row><row><cell>P5</cell><cell>0.388</cell><cell>0.396</cell></row><row><cell>P10</cell><cell>0.384</cell><cell>0.380</cell></row><row><cell>P15</cell><cell>0.352</cell><cell>0.367</cell></row><row><cell>P20</cell><cell>0.347</cell><cell>0.347</cell></row><row><cell>P30</cell><cell>0.318</cell><cell>0.318</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,77.78,699.31,441.40,8.89;3,72.02,710.83,18.17,8.89"><p>Even with the filtering of tags based on the number of times they appeared, the training of the graphical model used</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="40" xml:id="foot_1" coords="3,105.34,710.83,45.71,8.89"><p>G of RAM.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,76.69,505.63,71.10,11.40" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.02,531.52,459.87,9.86;4,72.02,546.04,465.63,9.86;4,72.02,560.56,46.92,9.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,441.81,531.52,90.09,9.86;4,72.02,546.04,337.18,9.86">Incorporating Query Expansion and Quality Indicators in Searching Microblog Posts. ECIR-2011</title>
		<author>
			<persName coords=""><forename type="first">Kamran</forename><surname>Massoudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,417.31,546.04,28.21,9.86">LNCS</title>
		<imprint>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="362" to="367" />
			<date type="published" when="2011">2011. 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.02,585.19,447.79,9.86;4,72.02,599.71,448.43,9.86;4,72.02,614.23,24.84,9.86;4,72.02,638.83,458.81,9.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,325.39,585.19,194.42,9.86;4,72.02,599.71,157.57,9.86">FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karl</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Wayne Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,249.07,599.71,266.54,9.86">Advances on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.02,653.35,463.83,9.86;4,72.02,667.87,19.32,9.86" xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="4,72.02,653.35,335.40,9.86">Comparing Twitter and Traditional Media Using Topic Models. ECIR-2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
