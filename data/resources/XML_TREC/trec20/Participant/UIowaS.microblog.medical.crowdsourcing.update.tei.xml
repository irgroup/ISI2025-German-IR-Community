<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,74.23,75.81,466.47,11.68">The University of Iowa at TREC 2011: Microblogs, Medical Records and Crowdsourcing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,82.28,104.05,104.77,9.73"><forename type="first">Sanmitra</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.02,104.05,95.83,9.73"><forename type="first">Christopher</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Informatics Program</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.80,104.05,67.74,9.73"><forename type="first">Yelena</forename><surname>Mejova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.49,104.05,47.73,9.73"><forename type="first">Chao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,434.18,104.05,86.63,9.73"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Informatics Program</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,74.23,75.81,466.47,11.68">The University of Iowa at TREC 2011: Microblogs, Medical Records and Crowdsourcing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF0A25BD70836BAF7F8AC94132EDF7D6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Text Retrieval and Text Mining group at the University of Iowa participated in three tracks, all new tracks introduced this year: Microblog, Medical Records (Med) and Crowdsourcing. Details of our strategies are provided in this paper. Overall our effort has been fruitful in that we have been able to understand more about the nature of medical records and Twitter messages, and also the merits and challenges of working with games as a framework for gathering relevance judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC Microblog Track</head><p>This track addresses the problem of retrieval from a collection of short messages from a well--known microblogging service Twitter. The real--time ad--hoc retrieval task involved finding the "most recent but relevant information to the query" with an emphasis on returning "interesting" but "newer" relevant tweets 1 . The queries presented "an information need at a specific point in time" by specifying the title of the query and also the time it is issued. This paper describes our four run submissions. Our baseline Run 1 was generated by merging results of several retrieval approaches, with the weakest approach improved by applying a heuristic filter. The final merged result was then cleaned using text--based duplicate detection followed by temporal reordering. Run 2 uses a dataset enriched using external resources, as described below. It is also a merge of several retrieval results -those used in the previous run as well as new ones utilizing new information. Run 3 extends Run 2 by adding query expansion. Finally, Run 4 is the same as Run 3, except for the last temporal sorting of the tweets is skipped, in order to see the effect of "newness" on performance.</p><p>Below we describe the acquisition and enrichment of the dataset, our various retrieval approaches, precision-driven filter heuristics, and query expansion experiments. We conclude with a discussion of the performance of our submitted runs. Dataset Acquisition. Unlike many other TREC datasets, the Microblog collection had to be downloaded from the Twitter server using tools that were provided. This proved to be problematic, since the data on Twitter's servers is dynamic, resulting in differences between dataset across research groups. The problem arose because we were to download tweets based on an ID partially identified by a username. Unfortunately, when a user chooses to change their username, the access path to their previous tweet changes, thus not allowing researchers to download it. This resulted in smaller datasets for research groups that downloaded their datasets later in time. Dataset Preprocessing. The total number of tweets in our corpus is 15,231,082. Firstly because the track evaluates only tweets in English, we identified the English tweets and tweets in other languages using Python NLTK library 2 . This resulted in 5,857,982 English tweets by 2,681,673 users. We manually conducted error analysis by randomly examining 200 tweets. The result of analysis showed 15 non--English tweets (7.5%) detected as English and 4 English tweets (2%) detected as using other languages. The 7.5% error rate of non--English tweets detected as English appears not crucial, because these non--English tweets are usually ranked lower than English tweets during the retrieval time due to the lack of similarity between them and mostly English queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to enrich the information about each tweet, we collected some external information on the URLs and hashtags mentioned. This information was used for Runs 2--4, but not for the baseline Run 1.</p><p>1. We expanded the short URL into long URL, and got the webpage title, keywords, and description for every URL. This information was attained using the LongURL API 3 . 2. We got the description of hashtags, retrieved from tagdef service <ref type="foot" coords="2,382.56,134.62,3.29,5.84" target="#foot_1">4</ref> . For each hashtag it returns a list of descriptions, in case several are available. Of these, we used the most popular description. Furthermore, we created two more text--based fields: text reduce (TR) which excludes all URLs, hashtags, and mentions in the original tweet; and text reduce with hashtag (TRH), which excludes only URLs and mentions but includes hashtags. In consequent retrieval experiments instead of using the full message text we use TRH. URLs and mentions are removed as non--content parts that would only introduce noise. We keep the hashtags in the field because they are often used as a part of a sentence, e.g. #Obama is the president of U.S, the intent generally being to provide information about the tweet topic. Initial Retrieval. Two versions of the English part of the collection -one without external information used for Run 1 and with external information for Runs 2--4 -were indexed using Indri <ref type="foot" coords="2,379.20,283.66,3.29,5.84" target="#foot_2">5</ref> . Using logical operations we retrieve several sets of results, ranging from most conservative to least shown below. In each run we use one of three logic operators: QUOTE, AND, and OR, and perform the search on either the whole tweet record (anywhere) or on a particular field: TRH (as described earlier), and URL title , URL desc , URL keys (information gathered on the URLs mentioned in the tweet). The most precise results come from QUOTE and AND queries, but often they do not return any results, or return very few. OR queries, on the other hand, often return a good amount, but of lesser quality. We attempt to improve these using a filter heuristic described below. Before processing these results any further, though, we remove duplicate documents. We do this using TRH field (which excludes mentions and URLs), since sometimes the same content is retweeted, but with a different short URL (which points to the same webpage). Capitalized Word Result Filters. Intuitively, we postulate that capitalized words in the query are very important. Manual analysis of initial results supported our hypothesis. Hence, we designed three different strategies to filter out the tweets:</p><p>Strategy 1: Filter out tweets which do not contain all capitalized words in query. For example, for query "Saleh Yemen overthrow", both Saley and Yemen must be in the tweet text. Strategy 2: The same with Strategy 1 but if the number of capitalized word is only one, take out tweets that do not contain the capitalized words and the right--most non--capitalized word in query; if there is no capitalized word, filter tweets using all possible combinations of words in query (this combination must include the last word). For example, for query "Texas school robot", because there is only one capitalized word Texas, we filter out tweets that do not contain Texas and the right--most non--capitalized word robot.</p><p>For query "kepler discovers new planets" we would require tweets to have either of the following: kepler AND planets, discovers AND planets, and new AND planets. Strategy 3: Because Strategy 2 is restrictive, if after applying it we get no results, we resort to using the more liberal Strategy 1. Note that this filter has to be applied to an appropriate field. If we're processing an OR query that was run on URL title field, then this is the field to which we will apply this constraint.</p><p>To test these strategies we developed a training set using twelve sample queries provided by TREC. For each, we labeled the top 20 results of the OR queries run on TRH, URL title , URL desc , and URL keys fields resulting in 802 qrel entries. Precision @ 20 and the percentage improvement over baseline can be seen in the Table <ref type="table" coords="3,466.17,229.46,24.88,8.87">below</ref> The improvement each strategy contributes is significant. Among the three strategy 3 appears the best. However, as discussed later, once the results are merged, strategy 1 performs the best. It is used for our submissions. Result Merging. There are two merging stages that take place in our runs: first we merge the four OR result sets, and then we merge this set with the more precision--driven QUOTE and AND results. Because Run 1 does not have several OR results, the first merge is unnecessary. First we describe the final merge, being the easier one. The final result set is produced by putting the results of the most conservative runs at the top (QUOTE [anywhere], AND [TRH] and AND [anywhere]), and the rest at the bottom (OR [*]). That is, the final set contains results as ordered in Initial Retrieval section. Next for merging the OR results for Runs 2--4 we examined five approaches:</p><p>All Or Nothing: take all TRH results, then all URL title results, then all URL desc , then all URL keys . Note the order of the results is determined using each run's performance as estimated in previous experiments, taking results of the best performing run first, then second best performing run, etc. Round Robin: take one top document from TRH, one from URL title , one from URL desc , one from URL keys , then do another round, etc. Balanced Round Robin: take three documents from TRH, two from URL title , one from URL desc , one from URL keys , etc. The number of documents taken from each result set is proportional to the difference in performance of each run. Straight Borda Count <ref type="bibr" coords="3,179.20,610.58,12.08,8.87" target="#b0">[1]</ref>: each tweet gets a score, which is a sum of its ranks in all returned result sets. The resulting set is ranked using this score. Weighted Borda Count: the score is now a weighted sum, using weights reflecting the "goodness" of a run.</p><p>We use Precision scores attained for each run as weights. The Table <ref type="table" coords="3,136.76,680.90,27.17,8.87">below</ref> shows performance of these merging strategies added to each of the filter strategies described in Capitalized Word Result Filter section. Note that we are now using Precision @ 30 (official track measure) instead of Precision @ 20 as earlier. The Table also shows MAP. Query Expansion. Because of the queries are rather short, we introduced a query expansion step in Run 3. As in the result filter described earlier, we decided to focus on capitalized words in retrieved tweets as potential candidates for query expansion. We use results from QUOTE and AND queries as pseudo--relevant documents (as in, assumed to be relevant to our query). We then rank all of the capitalized words in these documents by frequency of their occurrence. Lexically similar words such as America and American were detected using Edit Distance <ref type="bibr" coords="4,109.21,341.30,13.42,8.87" target="#b1">[2]</ref> and the longer alternatives excluded. We then explored four strategies for choosing expansion terms:</p><p>1. select top 1 word ranked higher than the capitalized words in the query 2. select top 2 words ranked higher than the capitalized words in the query 3. select top 1 word which is not one of the capitalized words in the query 4. select top 2 words which are not one of the capitalized words in the query The selected terms are then added to the original query. For example, training query "natural disasters Australia" would have the following list of potential expansion terms (with document frequencies in the brackets): Yasi [21]; Cyclone <ref type="bibr" coords="4,161.89,460.33,13.83,8.88">[14]</ref>; Australia <ref type="bibr" coords="4,231.04,460.33,13.83,8.88">[13]</ref>; World <ref type="bibr" coords="4,286.36,460.33,13.83,8.88">[11]</ref>. Using strategies 1 and 3, the new query would be "natural disasters Australia Yasi". Using strategies 2 and 4, the new query would be "natural disasters Australia Yasi Cyclone".</p><p>These expanded queries are then used to perform an OR search. Capitalized word result filter is then applied to improve these results. Precision and Recall @ 30, as well as MAP scores for these results are shown in Table <ref type="table" coords="4,538.51,516.50,3.51,8.87" target="#tab_3">3</ref> (with percentage difference in performance from no expansion results in parentheses). Precision numbers improve noticeably for three out of four strategies. Recall and MAP, however, are hurt. This is expected, since by adding keywords to the query (and the consequent filter), we are improving precision but may disregard documents which otherwise may have been considered relevant. We use Strategy 1 for our Run 3 submission. Because the main measure for this track is Precision @ 30, we drop results beyond the rank of 30. To examine the relationship between relevance and timeliness, we examined Run 1--3 strategies on our training set. We see that precision and recall @ 30 are not significantly affected, whereas MAP is hurt for all three runs by an average of 9.43% (in the interest of space, we do not include the full table of results).</p><p>To explore this relationship further, we reorder results using a linear combination of timeliness (as expressed in number of days from query date) and relevance (as a rank in the result set). Interestingly, we did not find a good proportion for this combination. TREC results below show that skipping the reordering step (Run 4) hurts the precision of our run. TREC Results. In this section we present performance of our runs as evaluated by TREC organizers. Figure <ref type="figure" coords="5,507.12,215.54,7.29,8.87" target="#fig_0">1</ref> shows Precision @ 30 for the four runs using two relevance sets: all relevant documents (allrel) and only highly relevant documents (highrel). Figure <ref type="figure" coords="5,187.25,243.62,7.29,8.87">2</ref> shows the performance of one of our best performing runs (Run 3) compared to the average of median scores reported by TREC across all runs submitted. Unfortunately, at the time of the submission we did not know the requirement that all retweets (the tweets have 'RT' in the text or the response codes of the tweets in the HTML crawl are 302) are considered as irrelevant. Therefore, many of tweets in our result are retweets. We tried to remove all the retweets (the tweets have 'RT' in the text) and generated a new result set. The Precision@30 of allrel improves to 0.305 (7.95% improvement). The Precision@30 of highrel improves to 0.090 (8.54% improvement). Besides, there are still 18% (216/1200) tweets which were not judged because they have response code 302 (they are retweets but there is no 'RT' in the text). So if we eliminate all the retweets, our result would be even higher. In rest of paper, we still use the result set we submitted to TREC.</p><p>Runs 2 and 3 performed equally well, performing slightly better than the average TREC submission. Such a close performance of Runs 2 and 3 suggests that query expansion is not contributing enough benefit. The reason is that the strictness of the expansion strategy allows for only are few queries to be expanded. In the training queries, only 3 queries out of 12 satisfied the query expansion strategy, which requires results of QUERY and AND queries as input (which often return no results). In test queries, only 8 queries out of 50 were expanded. When evaluated using allrel, only 2 queries got better results using query expansion, and the results are the same when using highrel. Hence, we need a strategy that may not achieve the best average precision, but can expand majority of queries with decent precision improvement.</p><p>The inclusion of external information (the difference between Runs 1 and 2) does prove to be beneficial. In particular, in allrel, the results of 19 queries have been improved with average improve ratio of 46.4%, but the results of 13 queries have been hurt with the average improve ratio of --3.7%. In highrel, the result of 7 queries has been improved with average improve ratio of 147.3%, but the result of 9 queries has been hurt with the average improve ratio of --58.8% (improve ratio = (P@30R2 -P@30R1)/(P@30R1 + 0.001) ×100%). Hence, although overall the external information improves precision measures, some queries are negatively affected by it. We leave the study of the contribution of external resources to the retrieval performance for future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC Medical Records Track (TREC--MED)</head><p>The TREC Medical Records Track (TREC--MED) in 2011 addresses the problem of content--based information retrieval from the free--text fields of electronic medical records (EMRs). The corpus for this track consists of de-identified EMRs made available through the University of Pittsburgh BLULab NLP Repository. A typical XML formatted record consists of several tagged elements, namely, checksum, subtype, type, chief_complaint, admit_diagnosis, discharge_diagnosis, year, download_time, update_time, deid and report_text. The checksum tag consists of a unique identifier for a particular report. The type and subtype elements contain information about the nature of the diagnosis. The admit and discharge diagnosis fields contain the International Classification of Diseases, Ninth Revision, Clinical Modification (ICD--9--CM) codes which are used to classify diseases, signs and symptoms and other findings related to health conditions. The report_text field comprises of the body of the EMR and was the primary textual element of each report. The other elements of the records are mostly uninformative and irrelevant for the purpose of this task.</p><p>One particularly interesting aspect of the task is that multiple reports identified by unique checksum identifiers can be associated with a particular visit identified by a visit_id. A mapping table is provided for linking each report to a particular visit_id. The visit_ids are considered as retrieval units for this task. Similar to traditional TREC tasks, participants are provided with ad hoc query topics specifying a particular disease, sign or symptom and/or diagnosis and treatment. Participants are required to return a ranked list of visit_ids that satisfied such queries. Participants are provided with four training queries and corresponding relevance judgments for tuning their system before the release of the test queries. The test topic set consists of 35 queries for which participants had to report a ranked list of visit_ids.</p><p>The Text Retrieval and Mining group at the University of Iowa participated in the TREC--MED track and submitted four runs (by the original deadline). In the following sections we first describe the general techniques we followed and then provide the more specific information for the particular runs. Finally we present the results for the different runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Dataset. The dataset consisted of 101,711 records identified by unique identifiers (checksum). Since the retrieval unit was visit_id we mapped the checksum to visit_ids using a report--to--visit mapping table. Several reports were not mapped to visits and this gave us a reduced dataset containing 95,702 reports. These reports were mapped to 17,198 unique visits. The number of reports mapped to a particular visit_id varied between 418 (visit_id: GAgv80e1XdlW) and 1.</p><p>Preprocessing. The EMRs distributed for this task contained ICD--9--CM codes in the admit and discharge diagnosis fields which, being numbers, do not convey any information for text--based retrieval. In this step all ICD--9--CM codes of the reports were translated to their literal form using the mappings from the Unified Medical Language System<ref type="foot" coords="6,536.64,582.94,3.29,5.84" target="#foot_3">6</ref> (UMLS) metathesaurus. An index of these translated files was created using the INDRI information retrieval system. As a variant of this, we also merged reports with the same visit id as a single file and indexed those files. Individual fields like admit and discharge diagnosis and report--text were also indexed.</p><p>Query Expansion. While training we found that the TREC ad hoc queries contained some words or concepts that do not contribute any useful information for retrieval. Words like patients, treat, take, etc. were present in most of the topics and hence do not convey important and discriminative information. Such words and their variations were filtered as stopwords. The training queries were then run through MetaMap <ref type="bibr" coords="7,434.37,75.14,13.43,8.87" target="#b2">[3]</ref> to identify important concepts and their identifiers (CUIs). Certain semantic types like "Functional Concept", "Activity", etc. were found to be redundant in the different queries and were removed. After the two--step filtering, only the important concepts of the topics were retained. These were expanded using UMLS following various strategies described in Submitted Runs section.</p><p>Retrieval Strategies. We explored different query expansion strategies going from most stringent query expansion strategy (using synonyms, narrowly--related terms, etc.) to the least stringent strategy(parent terms, broadly-related terms, etc.). In addition to the high precision Boolean AND queries of synonymous concepts, we also used less stringent INDRI belief operators and window--based matching for improved recall in some queries. Queries were also run on individual fields like admit_diagnosis, discharge_diagnosis and report_text. These variations in retrieval strategy were run--dependent and are explained in details in the Submitted Runs section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging of Results.</head><p>Since we executed different types of the query expansion and retrieval strategies, for each run we needed to merge the results of several sub--runs. We used the Balanced Round--Robin merging technique as it had given us good performance in other experiments.</p><p>Based on the above strategies we submitted four runs which are summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submitted Runs:</head><p>Run 1: For this run, we execute 3 Indri Boolean AND (#band) sub--runs using different UMLS--based query expansion techniques with decreasing stringency in expansion term selection for successive runs. For the first sub--run the filtered concepts were expanded using terms having narrower relationship ("RN"), source asserted synonymy ("SY") and child relationship ("CHD") to the query term identifier (i.e. CUI). For the second sub--run, in addition to the above expansion relationships, we make the range of related concepts broader by incorporating terms having parent relationship ("PAR") and broader relationship ("RB") to the query terms. For the third sub--run we expanded the query terms using terms satisfying all the previous relationship and also similar or "alike" terms ("RL") from UMLS. Other expansion terms having relationship types like "RO" (Other), "RQ" (Possible synonymy) or "SIB" (Sibling) was excluded as they did not perform well on the training topics. The expansion terms are treated as synonyms for the purpose of retrieval. The expansion terms for all the sub--runs were restricted within the same semantic classes as the original query term. Additionally, the source vocabularies were restricted to SNOMED--CT and ICD--9--CM. For the fourth sub--run, which is the least stringent method, we used INDRI belief operator (#combine). Unlike the Boolean AND queries which returns only binary values, the #combine operator weights each term equally and prioritizes documents containing more query terms. These sub--runs were then merged using the Balanced Round--Robin strategy in a ratio of 3:3:2:2 giving more priority to the two more stringent sub-runs.</p><p>Run 2: For this run, we follow the same steps as in Run 1 but here we execute the queries on the index of documents merged on the basis of identical visit ids.</p><p>Run 3: For three sub--runs of this run, we follow the same strategies as the previous two runs but here the queries are executed on individual fields like admit diagnosis, discharge diagnosis and report--text only using Indri's #combine operator. For the fourth sub--run, the query tries to find all the terms of the query within an unordered window of 4 words. The queries were executed on the same index as in Run 2. Results are finally combined using the Balanced Round--Robin technique in the ratio of 2:3:3:2 giving more priority to topics retrieved in discharge diagnosis and report--text fields compared to topics retrieved in admit diagnosis and windowed matching strategies.</p><p>Run 4: This run is similar to Run 1 but results are ranked by first selecting documents satisfying all the terms (or their synonyms) in the query, and then ranking the results according to the #combine of all the query terms (and their synonyms) using INDRI's filter--require operator (#filreq).</p><p>Results. The results of our four runs in terms of bpref, R--precision (R--prec) and precision at 10 documents retrieved (P@10 ) are presented in Table <ref type="table" coords="8,239.87,141.38,4.91,8.87" target="#tab_5">4</ref>. Bpref calculates the number of times judged non--relevant documents are returned before relevant documents <ref type="bibr" coords="8,238.72,155.30,11.98,8.87" target="#b3">[4]</ref>. R--prec calculates the precision after R number of retrieved relevant documents for a query <ref type="bibr" coords="8,166.64,169.46,11.95,8.87" target="#b3">[4]</ref>. The evaluations are over 34 topics (topic 130 was dropped). From Table <ref type="table" coords="8,120.61,306.02,4.89,8.87" target="#tab_5">4</ref>, we can see that Run 3 performs the best using both bpref and R--prec measures and achieves scores of 0.4635 and 0.2873, respectively. The Run 2 performs the best using the P@10 metric with a score of 0.4059. We are surprised to note that Run 3, where one of the sub--runs had the most basic retrieval operator (unordered window of four words), performed better that other more sophisticated strategies. Also, it is interesting to note that runs executed on the index of files merged using visit ids performed better. Using the same metrices, results for individual topics show that few topics (such as 108 and 109) have performed much better than some other topics (e.g. 110). We are investigating into the reasons for such anomalies. Overall, we seem to have achieved better scores compared to the median scores in terms of the bpref evaluation metric than the R--prec and P@10 measures for the individual topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC Crowdsourcing Track</head><p>Our group participated in the Crowdsourcing track in collaboration with the Delft University of Technology. In this joint participation effort, we employed a term association game called GeAnn to generate relevance judgments in an engaging way that encourages quality submissions from the crowd. A comprehensive explanation of our approach is described in a separate paper <ref type="bibr" coords="8,243.63,534.74,13.43,8.87" target="#b4">[5]</ref> written in conjunction with the Delft University of Technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper we describe our approach to real--time ad--hoc retrieval in Twitter. Using external resources, we extended the dataset by obtaining more information about URLs and hashtags mentioned in the text. The final results were generated by merging several result sets, coming from approaches ranging from most conservative to more liberal. The less conservative (thus less accurate) approaches were improved using heuristics and query expansion. All of these techniques were tested using a training set of queries with 802 relevance judgments. We show that the capitalized word filter does improve the retrieval precision, but we still have room to achieve better performance. In addition, our query expansion strategy may be too strict to improve performance, calling for a method that can be applied more broadly. Finally, our best submission to TREC outperforms the average precision of all submitted runs, and clearly shows that using external information benefits retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,129.78,490.95,160.86,8.00;5,329.30,490.95,172.93,8.00;5,112.43,380.89,176.73,98.55"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Precision @ 30 for submitted runs Figure 2. Precision @ 30 for Run 3 and all TREC</figDesc><graphic coords="5,112.43,380.89,176.73,98.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,145.68,229.46,350.35,110.63"><head>Table 1 .</head><label>1</label><figDesc>. Precision @ 20 for Capitalized Word Filters</figDesc><table coords="3,145.68,280.34,308.33,59.75"><row><cell>Field</cell><cell>Baseline</cell><cell>Strategy 1</cell><cell>Strategy 2</cell><cell>Strategy 3</cell></row><row><cell>URL title</cell><cell>0.20</cell><cell>0.54 (+170%)</cell><cell>0.50 (+150%)</cell><cell>0.61 (+205%)</cell></row><row><cell>URL desc</cell><cell>0.25</cell><cell>0.38 (+58%)</cell><cell>0.44 (+83%)</cell><cell>0.44 (+83%)</cell></row><row><cell>URL keys</cell><cell>0.23</cell><cell>0.55 (+139%)</cell><cell>0.50 (+117%)</cell><cell>0.59 (+157%)</cell></row><row><cell>TRH</cell><cell>0.42</cell><cell>0.66 (+57%)</cell><cell>0.57 (+36%)</cell><cell>0.70 (+67%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,89.06,470.22,162.71"><head>Table 2 .</head><label>2</label><figDesc>Result Merging Strategies Performance The best performance in each column is bolded. Notice that comparing the three filter strategies shows that the best is Strategy 1 (compared to superior numbers Strategy 3 showed on individual OR runs earlier). Thus we use Strategy 1 with Balanced Round Robin for submitted Runs 2--4.</figDesc><table coords="4,113.28,111.86,374.13,85.19"><row><cell>Strategy</cell><cell cols="2">Strategy 1 filter</cell><cell cols="2">Strategy 2 filter</cell><cell cols="2">Strategy 3 filter</cell></row><row><cell></cell><cell>P @ 30</cell><cell>MAP</cell><cell>P @ 30</cell><cell>MAP</cell><cell>P @ 30</cell><cell>MAP</cell></row><row><cell>AllOrNothin</cell><cell>0.583</cell><cell>0.359</cell><cell>0.552</cell><cell>0.241</cell><cell>0.640</cell><cell>0.312</cell></row><row><cell>RoundRobin</cell><cell>0.647</cell><cell>0.433</cell><cell>0.555</cell><cell>0.246</cell><cell>0.643</cell><cell>0.319</cell></row><row><cell>BalancedRoundRobin</cell><cell>0.647</cell><cell>0.439</cell><cell>0.555</cell><cell>0.253</cell><cell>0.643</cell><cell>0.332</cell></row><row><cell>StraightBorda</cell><cell>0.616</cell><cell>0.352</cell><cell>0.552</cell><cell>0.246</cell><cell>0.640</cell><cell>0.331</cell></row><row><cell>WeightedBorda</cell><cell>0.591</cell><cell>0.349</cell><cell>0.552</cell><cell>0.257</cell><cell>0.640</cell><cell>0.370</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,100.80,600.50,397.85,82.07"><head>Table 3 .</head><label>3</label><figDesc>Query Expansion Results</figDesc><table coords="4,100.80,623.30,397.85,59.27"><row><cell></cell><cell>No</cell><cell>Expansion</cell><cell>Expansion</cell><cell>Expansion</cell><cell>Expansion</cell></row><row><cell></cell><cell>expansion</cell><cell>strategy 1</cell><cell>strategy 2</cell><cell>strategy 3</cell><cell>strategy 4</cell></row><row><cell>Precision</cell><cell>0.717</cell><cell>0.917 (+28%)</cell><cell>0.911 (+27%)</cell><cell>0.917 (+28%)</cell><cell>0.697 (--3%)</cell></row><row><cell>Recall</cell><cell>0.443</cell><cell>0.401 (--9%)</cell><cell>0.364 (--18%)</cell><cell>0.401 (--10%)</cell><cell>0.168 (--62%)</cell></row><row><cell>MAP</cell><cell>0.405</cell><cell>0.397 (--1%)</cell><cell>0.353 (--13%)</cell><cell>0.397 (--2%)</cell><cell>0.161 (--60%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,72.00,75.14,470.13,22.79"><head>Time Sort. A final step in the creation of all four submitted runs in a sort of the top 30 documents by published time</head><label></label><figDesc></figDesc><table /><note coords="5,90.68,89.06,4.67,8.87"><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,191.67,193.46,226.68,84.47"><head>Table 4 :</head><label>4</label><figDesc>Performance evaluations of submitted runs</figDesc><table coords="8,191.67,217.94,226.68,59.99"><row><cell>Run</cell><cell>bpref</cell><cell>R--prec</cell><cell>P@10</cell></row><row><cell>Run 1</cell><cell>0.3619</cell><cell>0.2648</cell><cell>0.3588</cell></row><row><cell>Run 2</cell><cell>0.3841</cell><cell>0.2580</cell><cell>0.4059</cell></row><row><cell>Run 3</cell><cell>0.4635</cell><cell>0.2873</cell><cell>0.3559</cell></row><row><cell>Run 4</cell><cell>0.4131</cell><cell>0.2712</cell><cell>0.3706</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,77.08,689.64,77.25,7.80"><p>http://longurl.org/api</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,77.08,700.44,79.71,7.80"><p>http://api.tagdef.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,77.08,711.48,127.71,7.80"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="6,77.25,712.20,122.53,7.80"><p>https://uts.nlm.nih.gov/home.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>For the medical records track, we used some basic preprocessing steps to translate the ICD--9--CM codes in the EMRs to textual information. We also explored several query expansion strategies based on the UMLS metathesaurus. While the combination of these strategies gave us some encouraging results, additional work is required to improve the P@10 scores. In future research we would like to find ways to leverage the information content of the report text by using entity recognition methods for identifying standardized biomedical entities.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,76.98,194.90,441.36,8.87;9,72.00,208.82,457.28,8.87;9,72.00,222.98,285.80,8.87" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,183.87,194.90,136.97,8.87">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,411.13,194.90,107.22,8.87;9,72.00,208.82,49.03,8.87">The Second Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09-02">August 31--September 2, 1993. 1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
		<respStmt>
			<orgName>The Department of Commerce and the National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.98,242.89,352.49,8.88" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,172.63,242.90,70.14,8.87">Algorithm Design</title>
		<author>
			<persName coords=""><forename type="first">Tardos</forename><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<publisher>Addison Wesley/Pearson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.98,263.06,440.74,8.87;9,72.00,276.97,346.52,8.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,231.71,263.06,281.29,8.87">An overview of MetaMap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.--M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,72.00,276.97,228.19,8.88">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>BMJ Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.98,297.13,463.56,8.88;9,72.00,311.05,459.85,8.88;9,72.00,325.22,26.20,8.87" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,237.41,297.14,200.54,8.87">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,442.40,297.13,98.14,8.88;9,72.00,311.05,388.43,8.88">Proceedings of the 27th annual international conference on Research and development in information retrieval SIGIR 04</title>
		<meeting>the 27th annual international conference on Research and development in information retrieval SIGIR 04</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.98,345.14,453.07,8.87;9,72.00,359.29,444.35,8.88;9,72.00,373.22,435.36,8.87" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,338.82,345.14,186.78,8.87">GeAnn at the TREC 2011 Crowdsourcing Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,82.03,359.29,226.12,8.88">Proceedings of the Twentieth Text REtrieval Conference</title>
		<meeting>the Twentieth Text REtrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11-15">2011. November 15--18, 2011. 2011</date>
		</imprint>
		<respStmt>
			<orgName>The Department of Commerce and the National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">TREC--2011</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
