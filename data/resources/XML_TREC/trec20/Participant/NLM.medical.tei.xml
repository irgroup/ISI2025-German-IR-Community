<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,133.32,75.37,345.33,12.64">A knowledge-based approach to medical records retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,99.96,102.28,91.59,8.96"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.60,102.28,76.41,8.96"><forename type="first">Swapna</forename><surname>Abhyankar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.77,102.28,90.18,8.96"><forename type="first">Antonio</forename><surname>Jimeno-Yepes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.60,102.28,54.82,8.96"><forename type="first">Russell</forename><surname>Loane</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,452.05,102.28,55.30,8.96"><forename type="first">Bastien</forename><surname>Rance</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.84,113.80,55.27,8.96"><forename type="first">Fran√ßois</forename><surname>Lang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.76,113.80,49.53,8.96"><forename type="first">Nicholas</forename><surname>Ide</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.20,113.80,73.20,8.96"><forename type="first">Emilia</forename><surname>Apostolova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.01,113.80,67.98,8.96"><forename type="first">Alan</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,133.32,75.37,345.33,12.64">A knowledge-based approach to medical records retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">294FCE695FCE24ABE7B66C2DC3BA382B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The NLM LHC team approached the cohort selection task of the 2011 Medical Records Track as a question answering problem. We developed 60 training topics and then manually converted those topics to question frames. We started with the evidence-based medicine well-formed question frame and expanded it to explicitly capture temporal and causal relations. We then implemented a syntactic-semantic method for extracting the question frames from the free text topics.</p><p>Based on the clinical documentation standards and knowledge of the clinical documentation structure, we split each report type into sections corresponding to different categories of clinical content, with the result that each section contained a specific class of data. We then ranked each document section according to its likelihood of containing answers to specific question frame slots. For example, if a question concerns medications prior to admission, the answers should be found in the Medications on Admission and the Medical History sections. In addition, we split each section into Positive (containing asserted findings, problems, and interventions), Negative (in which findings are negated) and Possible (that includes all uncertain statements).</p><p>After structuring the questions and the documents, we developed algorithms for expressing question frames in the languages of the two search engines used for retrieval: Essie and Lucene. In addition to the UMLS synonymybased query expansion built into Essie and implemented externally for Lucene, we expanded the terms in the documents with their ancestors and children from the MeSH hierarchy. We also expanded query terms for recognized drug names using RxNorm and Google searches.</p><p>In addition to the automatically generated baseline and expanded queries that we ran against the original and the structured documents, we used the Essie user interface for manual query generation. During this process, we determined that a third of the automatically generated question frames, although technically correct, needed significant modifications due to different sub-languages used in the documents and in the queries. The manually created queries were used to search the collection with each search engine.</p><p>Our manual queries submitted to Essie significantly outperformed all of our other runs (achieving 0.73 P@10, 0.66 bpref, and 0.49 R-prec). Interestingly, the best automatic run for Lucene was the baseline run (P@10 = 0.44, bpref = 0.47, R-prec = 0.33) that used the topics "as is" to search the original documents. The results for this run are not significantly different from the manual Lucene (P@10 = 0.51, bpref = 0.51, R-prec = 0.35) and the automatic Essie (P@10 = 0.49, bpref = 0.48, R-prec = 0.33) runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The 2011 TREC Medical Records Track focused on finding patient cohorts based on short descriptions of the cohort inclusion criteria and clinical narrative documents generated during patients' hospital stays. For any given patient, all of the documents from one hospital stay were collated into a single visit.</p><p>Our previous efforts in clinical text processing showed that information in different document sections is more reliable for specific questions. For example, for a question regarding a patient's active medications, the medications listed in the allergy section should not be included in the results because they have the potential to cause adverse reactions and therefore are not given to the patient <ref type="bibr" coords="1,107.37,685.77,91.51,8.96" target="#b6">(Mork et al., 2010)</ref>. Our approach to document segmentation is described in Section 2.</p><p>Another important issue in clinical text processing is distinguishing information that is negated from that which is asserted. For example, if we need to find radiology reports for patients with pneumonia, we don't want to see the reports for patients that had a chest x-ray done to exclude pneumonia and who, in fact, did not have evidence of pneumonia. Our approach to identifying assertions and negation is described in Section 3.</p><p>We believe that the evidence-based medicine approach to building a well-formed clinical question provides a good framework for simple questions, but we also know that this framework is not capable of capturing some nuances (for example, temporal relations) that might be very important for cohort identification <ref type="bibr" coords="1,418.15,674.32,94.28,8.96" target="#b2">(Huang et al., 2006)</ref>. Our extensions to the basic clinical question framework are presented in Section 4.</p><p>To expand the question framework, test our query translation algorithms, and validate designating specific document sections as most likely to contain answers to specific question frame slots, SA (a clinical informatics fellow and a practicing pediatrician) generated 60 training topics that we shared with other track participants. While training the system, we realized that the levels of granularity in the questions and in the documents are often different: whereas the questions often contain drug and disease classes, the documents mostly contain specific disease codes and drug names. To compensate for the differences, we expanded the disease and drug terms in the documents and the drug and drug class names in the queries. This work is discussed in Section 5.</p><p>For retrieval, we used Essie <ref type="bibr" coords="2,221.41,258.22,66.68,8.96" target="#b3">(Ide et al., 2007)</ref> and Lucene. 1 We briefly describe these two search engines in Section 6, along with our query formulation and retrieval strategies. We conclude the report with a preliminary analysis of our experiments and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Document Segmentation</head><p>We developed rules to segment all documents in the collection into unique sections containing specific clinical content based on manual examination of a random sample of documents. We then automatically segmented all of the documents based on these rules and manually evaluated the segmentation. Specifically, we iteratively: 1) reviewed a selection of documents, 2) created section headings for each document type, 3) created rules for assigning the section headers based on specific text indicators in the documents, 4) automatically segmented the documents, and 5) went back to step one and manually reviewed a new selection of segmented documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Manual rule creation</head><p>In the first step of the manual review, we examined the content and structure of sample documents from each of the collection's nine different document types. Some, such as SP (Surgical pathology) and ECHO (Echocardiogram), had a structured format and contained limited, specific content in a predictable sequence within that document type. Others, such as RAD (Radiology), had document subtypes (e.g., Chest, Angio), each with their own uniform structure and content. Other document types were far less structured, such as HP (History and physical) and DS (Discharge summary); in some cases, these documents contained similar data, while in others, the content was disparate in 1 http://lucene.apache.org/java/docs/index.html terms of the depth and breadth of the information as well as its sequence and length.</p><p>Second, we created section headings to encompass the specific clinical content contained in each document type. We wanted to create enough sections to appropriately segment different types of clinical data within each document; however, we did not want to make the sections so granular as to become unmanageable. As mentioned above, ECHO reports were very structured and only contained echocardiogram results, so we only needed to assign three unique section headings to encompass all of the relevant data: reason for the study, procedure details, and final diagnosis. Document types such as DS and HP contained a wide range of clinical content, including the reason for admission, past medical history, home medications, physical examination, lab and radiology results, code status, and discharge information. Up to sixteen different section headings were used to cover the diverse types of data contained in these more complex documents.</p><p>Third, we created detailed rules for the automatic report annotator to assign appropriate section headings based on specific indicators in the document text. Most of these rules were based on variations in the indicators used to indicate different types of information in the original text. The documents that originally had the most structure also had the least variability in how the different clinical content was named. For example, the admitting diagnosis (i.e. reason for the study) in the ECHO reports was identified by referring diagnosis (caseinsensitive) either followed by a colon or occurring at the end of a line. However, in DS documents, the admitting diagnosis was identified by admission or admitting plus diagnosis or diagnoses plus either a colon or the end of a line.</p><p>In some instances, important types of clinical data, such as drug allergies or family history, did not have any indicators that signaled what information was following. For example, allergies: would sometimes indicate the start of the drug allergies section; however, many times the text would simply say allergies are to‚Ä¶ or penicillin allergy without any introduction. Similarly, a patient's family history often started on a new line of the text without any indication of what was following. For these, we created more complicated rules that not only included information about a colon or the end of the line, but also about specific words to precede or follow potential section indicators. For example, allergy could be preceded by penicillin or followed by to or are to; family history could be followed by a colon, dash, or end of the line or of, to, is, or shows followed by a colon, dash, or end of the line.</p><p>In the fourth step, the automated report annotator segmented all of the documents according to the section indicator rules as described below. Following each annotation cycle, we reviewed a new subset of documents from each document type and adjusted the section indicator rules as necessary. We went through multiple iterations of this process and ultimately created 548 section indicators to segment the 9 document types into 3 to 16 sections, which does not include the variation in case or spacing, or the different punctuation marks (colon, dash) and words that could follow an indicator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automated Annotation of Reports</head><p>After the 548 indicator rules and the assertion data (described in Section 3) were created, we automatically annotated all documents in the collection using those rules and data. Our report annotator examined each report one line at a time. If a line in the report matched one or more indicators, the annotator selected the longest (case insensitive) matching indicator, and annotated the text with an XML tag indicating the beginning of the corresponding section type. For example, in a document of type RAD, the string clinical history occurring either at the end of a line or immediately followed by a colon indicates the beginning of a history_of_present_illness section. Accordingly, the tag was inserted when the annotator encountered the line:</p><p>CLINICAL HISTORY: DIFFICULTY FEEDING, FEEDING TUBE. This indicator rule would not be applicable, however, for the line:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WALL DEHISCENCE. GIVEN THE CLINICAL HISTORY OF CHANGE IN MENTAL</head><p>The end of a section was not explicitly detected; a section was deemed to end simply when the next section began. Finally, any text appearing in a report before the first matching indicator was tagged as the preamble.</p><p>Assertion tags were inserted using the output of the assertion extractor. For an example of assertion data, consider the text: no erythema, no drainage, which was identified as a negated statement. The section containing that string was annotated with: &lt;negation_assertion&gt;no erythema, no drainage &lt;/negation_assertion&gt;</p><p>Our annotator ran multiple parallel processes on 24 3.3-GHz processors and completed the annotation of the 100,000+ reports in about ten minutes. After all the reports in the collection were annotated as described above, they were passed to both Essie and Lucene for indexing and retrieval, as described in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Assertion Status Detection</head><p>Clinical texts are abundant in statements expressing the absence of or uncertainty associated with medical conditions. Thus clinical information retrieval systems need to accurately differentiate between the assertion status of statements, i.e. whether a statement is affirmed, negated, or uncertain (interchangeably called speculative). Similarly, medical conditions pertaining to the patient's family history need to be differentiated from ones pertaining to the patient.</p><p>Table <ref type="table" coords="3,98.88,471.40,4.98,8.96">1</ref> Sample corpus sentences demonstrating the assertion status associated with the medical condition liver disease: affirmed, negated, uncertain, and family history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assertion Status Example</head><p>Affirmed Patient with end-stage liver disease/ascites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negated</head><p>No liver disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertain</head><p>Questionable liver disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Family history</head><p>Family history is significant for liver disease in the father.</p><p>Consider the sample corpus sentences shown in Table <ref type="table" coords="3,139.78,608.33,3.76,8.96">1</ref>. While all four sentences mention the medical condition liver disease, the disease is affirmed and pertaining to the patient in only one of the examples. We determined that our system needs to be aware of the assertion status associated with the condition in order to accurately answer a query requesting, for example, patients with liver disease.</p><p>In our approach, we first detected the linguistic scope of negated and uncertain statements as well as those describing the patient's family history. Such statements (typically sentence clauses) were extracted from the text and annotated as described in Section 2.2.</p><p>The linguistic scope of negated, uncertain, and family history statements was detected utilizing a previously developed, open-source system -ScopeFinder. 2 2 http://scopefinder.sourceforge.net/ The ScopeFinder system is a linguistically motivated rule-based system for the detection of negation and speculation scopes <ref type="bibr" coords="4,72.00,97.24,112.52,8.96" target="#b0">(Apostolova et al., 2011)</ref>. The system rule set consists of lexico-syntactic patterns. The lexicosyntactic patterns contain a combination of a lexical trigger (i.e. a cue word) and its associated syntactic scope expressed in Penn Treebank syntactic notation. For example, one of the negation scope rules matches the complement of the verb denies. When applied to the sentence She also had cough but denies fever, the rule matches the sentence snippet shown in bold. The problem fever is then marked as negated.</p><p>The lexico-syntactic rules were initially automatically extracted from the BioScope corpus <ref type="bibr" coords="4,71.97,235.29,83.22,8.96" target="#b9">(Vincze et al., 2008)</ref>, a biomedical corpus annotated with negation/speculation cues and their scopes. Additional lexico-syntactic rules were identified in the analysis of the 2011 Medical Records Track dataset and manually added to the rule set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Automated Conversion of Topics to Question Frame</head><p>As mentioned above, SA developed a set of 60 training questions: 30 based on her patient encounters during the time of question generation as well as on interesting topics contained in recent issues of the General Medicine Journal Watch<ref type="foot" coords="4,266.16,370.93,3.24,5.83" target="#foot_0">3</ref> and 30 based on the Institute of Medicine's priority topics. <ref type="foot" coords="4,98.88,393.97,3.24,5.83" target="#foot_1">4</ref> These questions were used to create the frame structure and the algorithms for automatic structuring of the free-text topic into a frame structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Frame development</head><p>The original evidence-based medicine wellformed clinical question frames consist of four slots: Patient/Problem, Intervention, Comparison, and Outcome <ref type="bibr" coords="4,112.91,499.75,103.16,8.96" target="#b8">(Richardson et al., 1995)</ref>. We refined the basic frame elements with syntactically related words; captured conjunction and prepositional attachment; and augmented the basic four-slot PICO frame (P (split into Patient, Problem and Anatomy), Intervention (merged with Comparison), Outcome) with relational slots that express question elements using predicate-argument structures (</p><formula xml:id="formula_0" coords="4,71.98,580.17,215.94,20.49">[concept]- (relation)-[concept]).</formula><p>As we manually encoded our 60 test questions according to the expanded PICO framework, we further refined the frames as necessary to capture the intricacies contained in the test questions, such as temporal relations. For example, we defined three medication slots (medications before admission, on discharge, and the fall-back -medication for problem). These distinctions were needed to encode (and answer) temporal questions, such as Find patients with HIV admitted for a secondary infection who were not on prophylaxis for opportunistic infection and Find patients with COPD who were discharged on inhaled steroids. In the first example, only prophylactic drugs the patient was on prior to admission should have been considered, while in the second, an inhaled steroid was only relevant if the patient was discharged on that medication. The XML surface representation of our frame slots was chosen for the convenience of then automatically translating the frames to the query syntax of the search engines used for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic frame extraction</head><p>Our system automatically extracted the frames in four steps. In the first step, the system submitted the question to MetaMap 2010 with the default settings to extract the UMLS ¬Æ concepts <ref type="bibr" coords="4,324.00,327.16,112.63,8.96" target="#b1">(Aronson and Lang, 2010)</ref>. For each concept, the system stored the lexical match with offset and length, negation and semantic types in a lookup table. Second, the system used regular expressions to extract Patient demographics and social history. The Population slot was limited to the occupations and ethnicities defined by the UMLS semantic types Professional or Occupational Group and Population Group, respectively. The patterns for social history were limited to identifying smoker status and alcohol and illicit drug use.</p><p>In the third step, the system processed the topic sentences using the Stanford dependency parser (de <ref type="bibr" coords="4,339.48,476.73,86.00,8.96" target="#b5">Marneffe et al., 2006)</ref>. To prevent the parser from breaking up multi-word terms, the system concatenated the words in the terms prior to parsing. We focused on extracting a limited set of typed dependency relations, conjunctions, and modifiers. The frame slot was extracted only if the semantic and syntactic constraints were satisfied. If a rule was applied, the terms used in the rule were marked as used in the look-up table.</p><p>After completing iterations over the dependency paths, in the fourth and final step, all of the basic PICO elements not used in the previous steps were added to the frame. That is, if the concept lookup table for a given question contained concepts in the semantic groups Disorders (Problems), Interventions or Anatomy that were not marked as used in generating the question frame, the concepts were assigned to the traditional PICO frame slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mapping frame elements to document sections</head><p>Once we had finalized our expanded PICO frame slots, we mapped each one to its corresponding document section(s) to enable the automatic retrieval runs by Essie and Lucene. For each slot, we created rules for: 1) which document sections should be searched, 2) how much weight should be given to the search results found in each particular section, and 3) which document sections should not be searched for that particular data element. For example, the drug allergies slot was mapped to the allergies section of the document with a weight of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Google API HTML filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MetaMap</head><p>Drug filtering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>HTML Text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UMLS entities Drug list</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Web-based query expansion</head><p>Another rule explicitly stated that the drug allergies slot should not be mapped to home medications, discharge medications, or hospital medications. In another case, the family history frame slot was mapped to the family history document section with a weight of 1.0, and to other sections including the history of present illness and past medical history with a weight of 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Term Expansion</head><p>As mentioned above, the language and granularity of the Problem and Intervention terms in the documents were significantly different compared to those in our training questions. Therefore, we developed expansion techniques for terms found in the documents as well as those in the queries, and a specific process for drug expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document term expansion</head><p>We determined that the standard UMLS - based 5 5 synonymy expansion in Essie, which was provided to Lucene to create equal conditions for both search engines, was not sufficient to find good documents for the training topics. In general, the terms contained in the patient documents were much more granular than those in the queries. For example, a document might have invasive ductal carcinoma (a type of breast cancer) as a patient's diagnosis, whereas a query might ask for patients with breast cancer. Therefore, we decided to expand the terms identified in the documents with their parent and http://www.nlm.nih.gov/research/umls/ child terms in MeSH<ref type="foot" coords="5,411.60,311.29,3.24,5.83" target="#foot_2">6</ref> . The expansion was based on finding a term that MeSH identified and then using the MeSH tree for expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Query term expansion</head><p>We also found that the standard reference resources (e.g., RxNorm) were lacking the breadth of information necessary for query expansion. Even the UMLS, which contains information from multiple vocabularies, does not include all possible ways of classifying each medication or problem and available treatments. In addition, there is a lag time between when a particular drug is approved (or removed from the market) and when the updated information is incorporated into various resources. The same applies for treatments and problems.</p><p>However, there are an increasing number of websites not traditionally considered to be reference sites (e.g., Wikipedia) that contain lists of drugs and treatments relevant to problems that may not yet be curated into the existing standard databases and terminologies. We developed a process for extracting expansion terms from websites which we then combine with the drug expansion process described in 5.3. The approach consists of two main steps: identification of reference sites and the extraction of drugs or treatments from these sites. Figure <ref type="figure" coords="5,506.54,612.48,4.98,8.96">1</ref> shows the flowchart for this approach.</p><p>In the first step, we queried Google's AJAX API based on the information need (e.g., a drug class, a drug class related to a problem or a problem) to retrieve relevant websites. The HTML code from the top 5 web pages for each query was retained for entity extraction.</p><p>The websites provided enumerations of elements in well-structured HTML &lt;li&gt; tags. The HTML pages were filtered to keep the text delimited by the tags.</p><p>From the extracted text, we kept only the terms belonging to specific entities of interest. To do so, we used MetaMap 2011 to identify terms of the UMLS Pharmacological substance or the Therapeutic or Preventive Procedure semantic types. We further filtered the drugs identified by MetaMap by estimating term frequency and removing the terms with only one mention in order to further increase the quality of the extracted list. Since not all the Pharmacological substances in the UMLS are drugs, a stop word list was used to filter out false positives. The stop word list was prepared by close examination of the terms linked to concepts within the Pharmacological substance semantic type in the UMLS and includes terms like water <ref type="bibr" coords="6,491.25,166.30,48.56,8.96;6,323.97,177.70,21.57,8.96" target="#b4">(Humphrey, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 Drug expansion process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Drug expansion</head><p>We extended the website processing to include the expansion of brand names based on RxNorm 7</p><p>In RxNorm, a brand name has at least one ingredient, and an ingredient may be related to several brand names. For example the brand name Ritalin is the trade name of the ingredient methylphenidate.</p><p>Other brand names of methyphenidate include Metadate and Methylin. In this example, RxNorm would provide four additional brand names and a generic name. We used the RxNorm API <ref type="bibr" coords="6,136.32,582.31,137.37,8.96" target="#b7">(Peters and Bodenreider, 2008)</ref> to normalize and query drug names. The drug expansions were used by both search engines.</p><p>. Figure <ref type="figure" coords="6,148.44,421.39,4.98,8.96">2</ref> shows the overall flowchart of the approach. When considering a candidate term for expansion, the term was first checked against RxNorm. If the term was in RxNorm, RxNorm was used for the expansion. If the term was not found in RxNorm, websites were used for the expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Indexing and Retrieval</head><p>The XML documents prepared as described in Section 2 were indexed using Essie and Lucene. Briefly, the original documents were annotated with additional tags and text to facilitate targeted searches.</p><p>7 http://www.nlm.nih.gov/research/umls/rxnorm/ Extra XML tags delimited the recognized sections (allergies, complications, discharge medications, etc.) and assertions within them.</p><p>The remaining step for the automatic retrieval experiments was to generate rules for translating question frames to queries. The rules and retrieval strategies are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Essie indexing and retrieval</head><p>We wrote a translation utility to convert the results of question frame extraction into Essie query syntax. Conceptually, translation is a simple process:</p><p>1. predicates map to search areas, defined as a weighted set of XML tags (for example, predicate &lt;PMH&gt; &lt;Prblm&gt; hepatitis &lt;/Prblm&gt; &lt;Cause&gt; blood transfusion &lt;/Cause&gt; &lt;/PMH&gt;, extracted from the topic patients with a history of hepatitis related to blood transfusion, now with liver cancer, was mapped to the past medical history section) 2. predicate arguments (in the above example, hepatitis and blood transfusion) are terms and are searched with concept expansion, which includes UMLS term variants and synonyms 3. multiple predicates produce multiple query clauses, which are combined with an AND operator Several rounds of manually inspecting test results and modifying the translation utility produced a tangled, complicated algorithm with the following highlights:</p><p>Multiple arguments to a single predicate were searched three ways and the results were combined with an OR operator:</p><p>1. arguments searched and combined with a NEAR operator at weight 0.7 (requires arguments to be together) 2. arguments searched and combined with an AND operator at weight 0.7 (allows arguments to be separated) 3. arguments searched and combined with an OR operator at weight 0.1 (allows arguments to be missing) Note that anything found by method 1 is also found by methods 2 and 3, resulting in a combined weight of &gt;0.9.</p><p>Likewise, arguments with modifiers (for example, &lt;Procedure&gt;surgery&lt;MOD&gt;roboticassisted &lt;/MOD&gt;&lt;/Procedure&gt;) were searched two ways and the results were combined with an OR operator:</p><p>1. argument and modifier searched and combined with a NEAR operator at weight 0.7 2. argument searched without the modifier at weight 0.7</p><p>Modifiers were dropped entirely if they were too common (found in the corpus more than 200 times). Some corpus-based ad hoc synonymy was added, including: status post ÔÉ† status post OR s/p male ÔÉ† male OR man OR mr. OR his OR he OR gentleman female ÔÉ† female OR woman OR mrs. OR ms. OR her OR she OR lady adult ÔÉ† in 20s OR in 30s OR in 40s OR in 50s OR in 60s</p><p>As a final fallback strategy, the original topic text was searched at weight 0.01 with lossy expansion, which finds documents with most of the query words. This Essie feature is known to perform poorly and is rarely used in practice (except as a last resort in TREC). Our final searches were restricted to positive text, defined as original text without negative and family assertions and with speculative assertions weighted at 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Lucene indexing and retrieval</head><p>Indexing of the visits was done based on a standard Lucene analyzer and stop word list removal. The section processing presented above provided a rich set of sections which were stored and retrieved based on Lucene fields. For each visit, we prepared one field that contained all the text in the report, and additional fields that segmented each section's text into positive, speculative and negative subsections.</p><p>The extracted question frames were reformulated based on Lucene's query language, which allows for field queries and in addition, weighting query expressions using the character ^ at the end of the search expression. In addition, in specific cases the query terms in a given expression were constrained to be found within a specific number of words using the character ~ followed by the maximum allowed length of the span of text. The original query, with a lower weight (0.02), was combined with the reformulated query to retrieve missing documents. Finally, expansion of the PICO predicate Age was based on the ad hoc synonymy described above. Age was used to filter out visits that were not in the range specified by the query.</p><p>An example of the expansion of topic 114 is presented below: ((Adult patients discharged home with palliative care / home hospice)^0.02) OR (((assessment_and_plan_positive_text: home OR hospice palliative care))^1.0 ((addendum_positive_text: home OR hospice palliative care ) (course_positive_text: home OR hospice palliative care))^0.7 ((assessment_and_plan_speculative_text: home OR hospice palliative care ))^0.5 ((addendum_speculative_text: home OR hospice palliative care ) (course_speculative_text: home OR hospice palliative care))^0.35) AND ((age_in_section_best: "in 50s") OR (age_in_section_best: "in 60s") OR (age_in_section_best: "in 40s") OR (age_in_section_best: "in 30s") OR (age_in_section_best: "in 20s"))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>Our experiments had three goals: 1) to establish if domain knowledge is absolutely necessary for clinical document retrieval, 2) to establish if a widely-used general purpose search engine would benefit from domain knowledge, and 3) to determine if the cohort identification task can be completely automated.</p><p>Accordingly, we used Essie, a domain specific search engine that cannot be easily decoupled from its knowledge, and Lucene, to which we added the same knowledge embedded in the document structure and query formulation and expansion. Towards our second goal, we compared the "off-the-shelf" Lucene runs with the Lucene knowledge augmented runs.</p><p>Finally, we focused on manually modifying queries until the top 10 visits looked relevant for the most part. The final queries and top ten results (without eliminating the obviously irrelevant documents that could not be eliminated with query modifications) were reviewed by two MDs (SA and DDF). In total we submitted six runs as described in Table <ref type="table" coords="8,349.29,154.77,3.76,8.96" target="#tab_0">2</ref>. Manual queries translated to the Lucene query language EssieAuto (not judged)</p><p>Automatic Essie queries described in Section 6.1 NLMAutoLuc (not judged) Automatic Lucene knowledge-based queries described in Section 6.2 NLMLucene (not judged)</p><p>Baseline 'out-of-the-box' Lucene retrieval over original documents NLMLucenePS (not judged)</p><p>Baseline 'out-of-the-box' Lucene retrieval over non-negated sections</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Results</head><p>To our surprise, our baseline Lucene run was statistically (Wilcoxon signed rank test) as good as our Lucene manual run and the Essie automatic run. See Table <ref type="table" coords="8,132.60,343.14,3.76,8.96" target="#tab_1">3</ref>. Our manual Essie run was significantly better than all our other runs on all three reported metrics. The Essie automatic, Lucene manual and Lucene baseline runs were significantly better than the two automatic knowledge-enhanced Lucene runs, with the NLMLucenePS run being the worst compared to all other conditions.</p><p>Although we manually verified that most of the top ten documents in the NLMManual run were relevant, on three topics <ref type="bibr" coords="8,425.15,400.76,80.83,8.96">(110, 123, and 128)</ref> this run performed worse than the median (See Figure <ref type="figure" coords="8,513.80,412.16,3.63,8.96" target="#fig_0">3</ref>). An in-depth analysis of whether this is caused by the differences in the evaluators' opinions or technical errors is underway. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Discussion</head><p>The preliminary answers to our questions indicate that depending on the nature of the task, an "out of the box" search engine might be quite sufficient for clinical record retrieval. One such task would amount to finding enough patients for a study in a very large clinical database -in this case, relatively high precision demonstrated by the automatic runs will ensure that a sufficient number of the found patients are eligible for inclusion, and the size of the database will ensure the needed number of patients is found. Unfortunately, we cannot judge the quality of the recall in this evaluation.</p><p>For our knowledge question, the answer seems to be that blindly adding knowledge to a general-purpose search engine significantly hurts its performance, but a domain specific engine is more powerful, especially when used by domain experts. We have to note that the significantly weaker performance of the Lucene manual run might be partially due to running queries developed specifically for Essie. As much as we tried to preserve the gist of the queries, some of it might have been lost in translation.</p><p>Finally, the third of the original cohort identification topics needed significant modifications and, in some cases, significant time spent on finding the right terms by domain experts. So the answer to our complete automation question is probably no, and maybe we should next focus on better presentation of the results for quick evaluation and simplification of the query syntax.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,72.00,339.64,468.09,8.96;9,71.99,351.16,324.55,8.96;9,72.48,72.48,466.68,258.24"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Differences in P@10 between the NLM runs and the judged median results per topic (EM = NLMManual, EA = EssieAuto, LucM = NLMManualLuc, LucA = NLMLucene)</figDesc><graphic coords="9,72.48,72.48,466.68,258.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,123.84,186.84,364.20,160.32"><head></head><label></label><figDesc></figDesc><graphic coords="6,123.84,186.84,364.20,160.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,71.97,172.53,458.12,62.07"><head>Table 2 NLM runs submitted to the Medical Records Track</head><label>2</label><figDesc></figDesc><table coords="8,72.00,190.24,458.09,44.36"><row><cell>Run</cell><cell>Description</cell></row><row><cell>NLMManual (judged)</cell><cell>Manual queries generated using the Essie user interface, padded with the lossy</cell></row><row><cell></cell><cell>expansion of the original topics</cell></row><row><cell>NLMManualLuc (judged)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,360.90,206.10,98.71"><head>Table 3 Evaluation results</head><label>3</label><figDesc></figDesc><table coords="8,72.00,378.64,206.10,80.96"><row><cell>Run</cell><cell>P@10</cell><cell>bpref</cell><cell>R-prec</cell></row><row><cell>NLMManual</cell><cell cols="2">0.7265 0.6583</cell><cell>0.4999</cell></row><row><cell>NLMManualLuc</cell><cell cols="2">0.5147 0.5126</cell><cell>0.3567</cell></row><row><cell>EssieAuto</cell><cell cols="2">0.4971 0.4822</cell><cell>0.3369</cell></row><row><cell>NLMAutoLuc</cell><cell cols="2">0.2294 0.3671</cell><cell>0.1911</cell></row><row><cell>NLMLucene</cell><cell cols="2">0.4382 0.4781</cell><cell>0.3367</cell></row><row><cell>NLMLucenePS</cell><cell cols="2">0.1735 0.3317</cell><cell>0.1285</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="4,77.76,687.76,141.00,8.96"><p>http://general-medicine.jwatch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,75.24,699.16,208.76,8.96;4,72.00,710.68,130.86,8.96"><p>http://www.iom.edu/Reports/2009/ComparativeEffe ctivenessResearchPriorities.aspx</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,329.76,710.68,123.01,8.96"><p>http://www.nlm.nih.gov/mesh/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,323.99,474.53,215.96,8.96;9,323.99,486.05,215.95,8.96;9,323.99,497.58,215.86,8.96;9,323.99,508.98,215.99,8.96;9,323.99,520.50,216.04,8.96;9,323.99,532.03,209.01,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,323.99,486.05,215.95,8.96;9,323.99,497.58,211.70,8.96">Automatic extraction of lexico-syntactic patterns for detection of negation and speculation scopes</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Apostolova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tomuro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,323.99,508.98,215.99,8.96;9,323.99,520.50,216.04,8.96;9,323.99,532.03,93.82,8.96">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="283" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,323.99,555.07,215.92,8.96;9,323.99,566.60,215.83,8.96;9,323.99,578.00,176.00,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,428.53,555.07,111.38,8.96;9,323.99,566.60,166.61,8.96">An overview of MetaMap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,497.51,566.60,42.31,8.96;9,323.99,578.00,52.60,8.96">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010-05">2010 May-Jun</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,323.98,601.05,216.07,8.96;9,323.98,612.57,215.91,8.96;9,323.98,624.10,199.53,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,484.66,601.05,55.39,8.96;9,323.98,612.57,215.91,8.96;9,323.98,624.10,36.15,8.96">Evaluation of PICO as a knowledge representation for clinical questions</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,366.82,624.10,96.44,8.96">AMIA Annu Symp Proc</title>
		<imprint>
			<biblScope unit="page" from="359" to="363" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,323.98,647.03,215.97,8.96;9,323.97,658.55,215.93,8.96;9,323.97,670.07,216.05,8.96;9,323.97,681.60,108.08,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,532.76,647.03,7.19,8.96;9,323.97,658.55,215.93,8.96;9,323.97,670.07,67.92,8.96">A Concept Based Search Engine for Structured Biomedical Text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Loane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Essie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,401.37,670.07,108.77,8.96">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2007-05">2007 May-June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,74.20,215.91,8.96;10,72.00,85.72,19.50,8.96;10,107.28,85.72,29.41,8.96;10,152.64,85.72,48.97,8.96;10,217.56,85.72,7.19,8.96;10,240.59,85.72,47.33,8.96;10,72.00,97.24,213.68,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,141.60,74.20,146.31,8.96;10,72.00,85.72,19.50,8.96;10,107.28,85.72,29.41,8.96;10,152.64,85.72,48.97,8.96;10,217.56,85.72,7.19,8.96;10,240.59,85.72,47.33,8.96;10,72.00,97.24,50.67,8.96">Automatic Indexing of Documents from Journal Descriptors: A Preliminary Investigation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Humphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,129.12,97.24,70.89,8.96">J Am Soc Inf Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="661" to="674" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.99,120.17,215.85,8.96;10,71.98,131.70,215.85,8.96;10,71.98,143.22,36.57,8.96;10,132.94,143.22,28.05,8.96;10,185.37,143.22,8.34,8.96;10,218.13,143.22,25.48,8.96;10,267.93,143.22,20.10,8.96;10,71.98,154.74,215.09,8.96;10,71.98,166.27,117.55,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,71.98,131.70,215.85,8.96;10,71.98,143.22,36.57,8.96;10,132.94,143.22,24.04,8.96">Generating Typed Dependency Parses from Phrase Structure Parses</title>
		<author>
			<persName coords=""><forename type="first">M-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/pubs/LREC06_dependencies" />
	</analytic>
	<monogr>
		<title level="m" coord="10,218.13,143.22,25.48,8.96">LREC</title>
		<imprint>
			<date type="published" when="2006-08-16">2006. August 16, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.98,189.20,215.84,8.96;10,71.98,200.72,215.94,8.96;10,71.98,212.24,215.95,8.96;10,71.98,223.77,215.93,8.96;10,71.98,235.29,168.08,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,222.83,212.24,65.10,8.96;10,71.98,223.77,152.52,8.96">Extracting Rx information from clinical narrative</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,235.76,223.77,52.15,8.96;10,71.98,235.29,52.60,8.96">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="536" to="539" />
			<date type="published" when="2010-09">2010 Sep-Oct</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.98,258.22,215.80,8.96;10,71.98,269.74,215.98,8.96;10,71.98,281.27,149.73,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,184.89,258.22,102.90,8.96;10,71.98,269.74,180.91,8.96">Using the RxNorm web services API for quality assurance purposes</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,261.34,269.74,26.62,8.96;10,71.98,281.27,71.48,8.96">AMIA Annu Symp Proc</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="591" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.98,304.31,215.94,8.96;10,71.98,315.72,215.93,8.96;10,71.97,327.24,216.05,8.96;10,71.96,338.76,78.09,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,94.30,315.72,193.61,8.96;10,71.97,327.24,101.74,8.96">The well-built clinical question: a key to evidence-based decisions</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Hayward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,182.38,327.24,49.98,8.96">ACP J Club</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="1995-11">1995 Nov-Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,71.96,361.81,216.06,8.96;10,71.96,373.22,215.90,8.96;10,71.96,384.74,215.94,8.96;10,71.96,396.26,154.04,8.96" xml:id="b9">
	<analytic>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Csirik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.88,373.22,159.99,8.96;10,71.96,384.74,211.78,8.96">The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>Suppl 11):S9</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
