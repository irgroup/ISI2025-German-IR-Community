<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.80,115.90,267.76,12.90;1,155.72,133.83,303.92,12.90;1,199.22,151.77,216.91,12.90">University of Glasgow (qirdcsuog) at TREC Crowdsourcing 2011: TurkRank -Network-based Worker Ranking in Crowdsourcing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.26,189.97,63.33,8.64"><forename type="first">Stewart</forename><surname>Whiting</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.47,189.97,88.03,8.64"><forename type="first">Jesus</forename><surname>Rodriguez Perez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.10,189.97,55.14,8.64"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.35,189.97,90.87,8.64"><forename type="first">Teerapong</forename><surname>Leelanupab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.44,201.93,64.48,8.64"><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.80,115.90,267.76,12.90;1,155.72,133.83,303.92,12.90;1,199.22,151.77,216.91,12.90">University of Glasgow (qirdcsuog) at TREC Crowdsourcing 2011: TurkRank -Network-based Worker Ranking in Crowdsourcing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B880A1FC9E59ABF6A15048204A97D3F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For TREC Crowdsourcing 2011 (Stage 2) we propose a networkbased approach for assigning an indicative measure of worker trustworthiness in crowdsourced labelling tasks. Workers, the gold standard and worker/gold standard agreements are modelled as a network. For the purpose of worker trustworthiness assignment, a variant of the PageRank algorithm, named TurkRank, is used to adaptively combine evidence that suggests worker trustworthiness, i.e., agreement with other trustworthy co-workers and agreement with the gold standard. A single parameter controls the importance of co-worker agreement versus gold standard agreement. The TurkRank score calculated for each worker is incorporated with a worker-weighted mean label aggregation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>In the TREC 2011 Crowdsourcing track we worked on the Stage 2 Task to extract single relevance labels from an aggregated set of crowdsourced document relevance labels. In this report we describe our approach to the problem using a network-based algorithm to assign worker trustworthiness.</p><p>Crowdsourcing is a broad term used to describe any technique of obtaining data from multiple people on a large-scale. Whilst the concept of crowdsourcing has undoubtedly existed for a long time, web-based platforms such as Amazon Mechanical Turk<ref type="foot" coords="1,153.68,527.61,3.69,6.39" target="#foot_0">1</ref> (AMT) or CrowdFlower <ref type="foot" coords="1,261.06,527.61,3.69,6.39" target="#foot_1">2</ref> have become a popular tool to facilitate markets in which geographically-dispersed workers complete the tasks of requesters for financial reward. Such platforms allow requesters to package their work as relatively small tasks (commonly known as Human Intelligence Tasks, or, HITs) and instantly advertise them to the pre-recruited worker population, with satisfactory completion attracting a reward and often bonus payment.</p><p>However, whilst the use of crowdsourcing may seem very favourable for repetitive labour-intensive tasks such as labelling document relevance to a query, the reliability and quality of the collected labels has been strongly questioned. With the incentive of maximising earnings, some workers carelessly random-click in the hope of work acceptance. Likewise, automated systems (i.e. bots) setup to repeatedly submit random data have also become increasingly common with the increased financial incentives. To combat these issues, a number of approaches to improve data quality have been proposed and researched in the context of labelling tasks.</p><p>Aside from pre-screening workers through qualification requirements (e.g. previous work acceptance rates) or tests, intelligent HIT design can provide a first-line defence against casual random-clickers and bots. Validation can be through CAPTCHAs, whereby workers are asked to answer a simple question (such as 6 + 5 = ?) or repeat a string of characters shown in a distorted image (to avoid optical character recognition). Similarly, many have proposed task-specific validation questions, the answers for which can be found within the content of the task itself <ref type="bibr" coords="2,359.80,251.06,10.58,8.64" target="#b1">[2]</ref>. Alternatively, rigidly controlling the workflow with submission control timers and strict input validation can also be used.</p><p>Multiple redundant labels can be sourced for each judgement, and following data collection, aggregration can determine the most likely label. Majority voting is the simplest method, however, obtaining multiple labels for each HIT can be expensive, and even then, not necessarily yield the correct aggregated label if there are multiple low-quality labels. Additional judgements can be crowdsourced (or low-quality ones removed) to improve certainty by using statistics such as Fleiss' or Cohen's Kappa coefficient to measure the inter-worker agreement, and the chance agreement occurred randomly. There has been a significant amount of work to apply machine learning (ML) strategies to detecting poor quality workers <ref type="bibr" coords="2,331.50,382.81,10.58,8.64" target="#b2">[3]</ref>. Meanwhile, some have suggested heuristics, such as work time or reward level, approximately correlate with work quality, whereas others have reported no such connection.</p><p>Similar to previous work <ref type="bibr" coords="2,253.43,418.91,10.58,8.64" target="#b0">[1]</ref>, our approach provides a score of worker quality, or, trustworthiness. We model workers, the gold standard and worker/gold standard agreements as a network. Using a variant of the PageRank algorithm, named TurkRank, we combine evidence that suggests worker trustworthiness, i.e., agreement with other trustworthy co-workers as well as agreement with the gold standard. The TurkRank assigned to each worker is then used to weight the importance of the worker's label during aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>We consider worker trustworthiness to be a measure of the likelihood that a worker is performing valuable work: providing correct labels. Evidence for worker trustworthiness can be incrementally accumulated through co-worker label agreement and gold standard label agreement; with the availability of either source of evidence determining its importance. The more a worker agrees with other worker labels, the more trustworthy they are likely to be. However, particularly important is the trustworthiness of the agreeing worker (whether it is another crowdsourced worker, or a gold standard NIST assessor). A transitive relationship where workers agree with other workers and so on suggests an implicit network of trust propagation. Extensively studied in many applications is the PageRank (PR) <ref type="bibr" coords="2,255.88,656.44,11.62,8.64" target="#b3">[4]</ref> family of algorithms to measure relative importance within a network (e.g. web page authority in a link graph). We make use of the extended PageRank with Priors (PRwP) <ref type="bibr" coords="3,257.21,131.27,11.62,8.64" target="#b5">[6]</ref> algorithm to variably combine the evidence provided by both co-worker and gold standard agreement, naming our approach TurkRank.</p><p>TurkRank models crowdsourced workers, the gold standard and label agreements as a graph. Vertices are crowdsourced workers or the gold standard worker (i.e. NIST assessor). Edges are undirected positive label agreements between workers (binary, with weight = 1). An example network can be seen in Figure <ref type="figure" coords="3,360.16,191.22,3.74,8.64" target="#fig_0">1</ref>.</p><p>The PR algorithm is effective for discovering nodes with a high relative importance in a network, in this scenario, workers who have strong work agreement with other workers. To combine both worker and gold standard agreement in this estimation of relative importance, PRwP extends traditional PR by including vertex priors. The PageRank π (i.e. measure of trust) for a worker vertex v at iteration i is therefore calculated as:</p><formula xml:id="formula_0" coords="3,212.92,284.78,267.67,28.33">π(v) (i+1) = (1 -β) d in (v) ∑ u=1 p(v|u)π (i) (u) + βp v<label>(1)</label></formula><p>Priors influence the likelihood of the random walker jumping to a given vertex when teleporting, if the probability of teleporting is &gt; 0. PRwP has a single parameter regulating teleport probability, β.</p><p>Rather than teleporting to any vertex with equal probability (as in traditional PR), the gold standard vertex is assigned a 100% prior probability. Therefore, when β = 1, the random walker will always teleport to the gold standard worker, and so only it will accumulate trust. Conversely, when β = 0, the random walker will never teleport and so will move using edge-based co-worker agreements only, excluding the gold standard. With 0 &lt; β &lt; 1, worker agreement and gold standard agreement will be combined. With a higher β, greater trust will be given to those workers who agree with the gold standard, whereas, a lower β will give more trust to those workers who agree more with similarly agreeing workers. The sum of the TurkRanks assigned to all workers is always = 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST-Assessed Gold Standard Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Worker Label Aggregation</head><p>A weighted mean is used to incorporate the worker TurkRank in label aggregation, thus emphasising label contributions from more trustworthy workers. t is the set of worker trust ranks and l is the set of binary worker relevance labels (0/1, non-relevant/relevant).</p><formula xml:id="formula_1" coords="4,283.48,174.83,197.11,26.54">l = ∑ n i=1 t i l i ∑ n i=1 t i<label>(2)</label></formula><p>l is rounded to the nearest integer, such that ≤ 0.49 is considered non-relevant and ≥ 0.5 relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>Evaluation was performed using the TREC 2011 Crowdsourcing Stage 2 dataset. The dataset contains 19,033 topic-document pairs, with judgement labels made by 762 workers, who produced a total of 89,624 binary relevance judgments. 2,275 of the topic-document judgements have prior "gold" relevance judgements by trusted NIST assessors (1,000 non-relevant, 1,275 relevant).</p><p>To investigate the effect of different β settings (i.e. importance of the gold standard agreement versus co-worker agreement) we vary 0 ≥ β ≤ 0.9 at 0.1 intervals. For simplicity we report β = 0, 0.2, 0.4, 0.6, 0.8 only in this paper. Our primary TREC submission run for the track (as team "qirdcsuog") was with β = 0.4. We submitted both binary relevance labels and a relevance ranking order, based on the descending order of weighted means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>Figure <ref type="figure" coords="4,163.63,453.20,4.98,8.64">2</ref> shows the quantity of relevant labels produced after weighted mean aggregation for each β setting. A linear relationship is indicated with decreasing relevant judgements as the importance of agreement with the gold standard is increased in TurkRank assignment.</p><p>We are not certain of the cause of this relationship. One speculation posed, based on the design of the HIT form in the data collection study <ref type="bibr" coords="4,367.34,512.97,11.62,8.64" target="#b4">[5]</ref> is that poor workers may simply click the first available relevance selection (i.e. "Very Relevant" in this case) before submission. As these poor workers are filtered out with an increasing β for more gold standard reliance, the value of their previously majority judgements are reduced by the weighted mean.</p><p>Figure <ref type="figure" coords="4,177.96,572.75,4.98,8.64">3</ref> shows the logarithmic distribution of worker TurkRanks for each β setting. Noticeable is the increasing decline in TurkRank distribution towards the lower-ranked workers, particularly at higher β levels (e.g. 0.8).</p><p>Figure <ref type="figure" coords="4,180.27,608.62,4.98,8.64">4</ref> presents the raw weighted means calculated for each topic-document judgement, before binary rounding. There are clearly substantially more definitely relevant topic-document judgements than definitely non-relevant judgements. A large quantity of the weighted means are marginally above or below the cut-off 0.5 threshold. For these, additional labels should be sought to make a more reliable binary judgement.  Figure <ref type="figure" coords="6,180.12,357.90,4.98,8.64" target="#fig_2">5</ref> presents the effect of the β parameter setting on recall and precision (against the Gold 1000 ground truth). There is relatively little variance in precision, however, recall does increase as β is reduced. Table <ref type="table" coords="6,175.43,547.65,4.98,8.64" target="#tab_1">1</ref> shows the preliminary evaluation results for our primary submitted run, with β = 0.4. The baseline is provided by the consensus for all examples: 16785 judgements, 1000 judgements and the 1000 gold set judgements. With the exception of recall, TurkRank at β = 0.4 reduces performance compared to the baseline. Recall is enhanced, however at the cost of precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Preliminary evaluation results suggest that while TurkRank may be ranking workers based on their trust, optimal effectiveness (and possible baseline improvement) is very dependent on the β parameter setting. An arbitrary β of 0.4 is clearly not adequate in the evaluation scenario. A deeper analysis is required to achieve optimisation, in particular on the behaviour of β at different levels, given different scenarios with varying levels of gold standard and redundant worker labelling evidence. Similarly, the weighted mean approach to aggregating worker TurkRank and label evidence may not be the best approach to incorporating a worker trustworthiness measure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,495.19,345.83,8.12;3,134.77,506.50,104.10,7.77"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example network model, showing label agreements (edges) between co-workers and the gold standard NIST assessor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,147.96,155.16,319.44,8.23"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig.2. Effect of varying β on the number of "relevant" labels produced after aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,140.16,126.29,335.04,8.23"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Relevance label recall and precision at 0 ≤ β &lt; 1 using Gold 1000 ground truth labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,149.49,456.67,318.40,156.25"><head></head><label></label><figDesc>Fig. 4. Distribution of TurkRank-weighted mean for all topic-document judgements (before binary label rounding) for all β settings.</figDesc><table coords="5,149.49,456.67,318.40,156.25"><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Weighted</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beta0 beta0.2 beta0.4</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beta0.6</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beta0.8</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell cols="2">596 1191</cell><cell cols="3">1786 2381 2976 3571</cell><cell cols="2">4166 4761 5356 5951 6546</cell><cell cols="2">7141 7736 8331 8926 9521 10116 10711 11306 11901 12496 13091 13686 14281 14876 15471 16066 16661 17256 17851 18446</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topic-Document Judgement</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell cols="2">24</cell><cell>47 70 93</cell><cell cols="2">116 139</cell><cell cols="2">162 185 208 231 254</cell><cell>277 300 323 346</cell><cell>369 392 415 438 461</cell><cell>484 507 530 553 576</cell><cell>599 622 645 668</cell><cell>691 714 737 760</cell></row><row><cell></cell><cell cols="2">0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PageRank Worker</cell><cell cols="2">0.0001 0.001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beta0 beta0.2 beta0.4 beta0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beta0.8</cell></row><row><cell></cell><cell cols="2">0.00001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.000001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">0.0000001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Worker</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,163.36,413.60,288.64,104.75"><head>Table 1 .</head><label>1</label><figDesc>Preliminary results for our primary submitted run, with β = 0.4.</figDesc><table coords="6,163.36,435.30,288.64,83.05"><row><cell cols="6">Run Accuracy Recall Precision Specificity Log Loss KL-Div. RMSE MAP NDCG</cell></row><row><cell>Consensus 16785</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Consensus</cell><cell>76.9% 80.0%</cell><cell>83.0%</cell><cell>71.6%</cell><cell cols="2">4630.0 8384.0 0.0% 99.9% 99.9%</cell></row><row><cell>qirdcsuog</cell><cell>72.0% 89.8%</cell><cell>72.6%</cell><cell cols="3">41.3% 43168.9 50317.7 40.4% 76.3% 91.8%</cell></row><row><cell>Consensus 1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Consensus</cell><cell>77.0% 80.1%</cell><cell>83.3%</cell><cell>71.5%</cell><cell>275.6</cell><cell>496.3 0.0% 19.1% 47.3%</cell></row><row><cell>qirdcsuog</cell><cell>70.3% 88.9%</cell><cell>71.6%</cell><cell>37.2%</cell><cell cols="2">2735.5 3050.2 41.2% 10.6% 37.7%</cell></row><row><cell>Gold 1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Consensus</cell><cell>61.7% 73.3%</cell><cell>59.5%</cell><cell>50.2%</cell><cell>647.1</cell><cell>647.1 47.0% 16.3% 41.2%</cell></row><row><cell>qirdcsuog</cell><cell>52.9% 82.4%</cell><cell>51.8%</cell><cell>23.4%</cell><cell cols="2">4338.1 4338.2 68.6% 8.6% 32.6%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,645.79,81.61,7.77"><p>http://www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,657.08,104.57,7.77"><p>http://www.crowdflower.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.13,474.91,342.46,7.77;7,146.47,485.70,334.12,7.93;7,146.47,496.82,151.18,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,298.22,474.91,179.03,7.77">Quality management on amazon mechanical turk</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,156.13,485.70,298.30,7.93">Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP &apos;10</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation, HCOMP &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,507.78,342.46,7.77;7,146.47,518.58,333.59,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,347.89,507.78,132.70,7.77;7,146.47,518.74,211.36,7.77">Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Milic-Frayling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,374.47,518.58,20.54,7.73">SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,529.54,342.46,7.93;7,146.47,540.50,290.47,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,187.05,529.70,220.72,7.77">On Quality Control and Machine Learning in Crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,426.54,529.54,54.05,7.73;7,146.47,540.50,209.68,7.73">Proceedings of the 3rd Human Computation Workshop (HCOMP) at AAAI</title>
		<meeting>the 3rd Human Computation Workshop (HCOMP) at AAAI</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,551.62,342.46,7.77;7,146.47,562.58,334.12,7.77;7,146.47,573.54,87.74,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,316.91,551.62,163.68,7.77;7,146.47,562.58,35.44,7.77">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>SIDL-WP-1999-0120</idno>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,138.13,584.33,342.46,7.93;7,146.47,595.29,268.77,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,240.37,584.50,200.12,7.77">Semi-supervised consensus labeling for crowdsourcing</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,461.93,584.33,18.66,7.73;7,146.47,595.29,242.58,7.73">ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,606.25,342.46,7.93;7,146.47,617.21,334.12,7.73;7,146.47,628.17,268.48,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,235.95,606.41,209.85,7.77">Algorithms for estimating relative importance in networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,464.56,606.25,16.03,7.73;7,146.47,617.21,334.12,7.73;7,146.47,628.17,79.88,7.93">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;03</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
