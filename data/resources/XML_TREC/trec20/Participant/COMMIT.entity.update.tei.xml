<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,189.45,83.69,230.81,17.82">Team COMMIT at TREC 2011</title>
				<funder ref="#_7uy8gtq">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">CIP ICT-PSP</orgName>
				</funder>
				<funder ref="#_4ts3EQP #_7YbHV78 #_XsCh6zP #_AcAXFpc">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ySR4S3d">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_svWvEKs">
					<orgName type="full">CLARIN-nl program</orgName>
				</funder>
				<funder ref="#_TBxurZW">
					<orgName type="full">Hyperlocal Service Platform</orgName>
				</funder>
				<funder ref="#_wwu43Mt">
					<orgName type="full">PROMISE Network of Excellence</orgName>
				</funder>
				<funder ref="#_nR7G3uP">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_4TCA3Dy">
					<orgName type="full">Service Innovation &amp; ICT program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.26,116.24,56.34,12.37"><forename type="first">Marc</forename><surname>Bron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.55,116.24,58.77,12.37"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.27,116.24,114.18,12.37"><forename type="first">Maria-Hendrike</forename><surname>Peetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.94,136.64,82.71,12.37"><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.59,136.64,90.19,12.37"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,189.45,83.69,230.81,17.82">Team COMMIT at TREC 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A98B6806861C10EA4373443FB588AA9A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of Team COMMIT in this year's Microblog and Entity track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Team COMMIT participated in two tracks this year: the Microblog track and the Entity track.</p><p>In our participation in the Microblog track, we used a feature-based approach. Specifically, we pursued a precision oriented recency-aware retrieval approach for tweets. Amongst others we used various types of external data. In particular, we examined the potential of link retrieval on a corpus of crawled content pages and we use semantic query expansion using Wikipedia. We also deployed pre-filtering based on query-dependent and query-independent features. Our main finding is that cutting-off the result list is difficult and crucial for good results. We also found that using external data helps with recall and precision.</p><p>For our participation in this year's Entity track we focused on the Entity List Completion (ELC) task. We experimented with a text-based and a link-based approach to retrieving entities from Linked Data (LD). Additionally, we experimented with selecting candidate entities from a web corpus. Our hypothesis is that entities occurring on pages with many of the example entities are more likely to be good candidates than entities that do not. Due to the absence of evaluation results at the time of writing, we have no preliminary conclusions yet.</p><p>The remainder of the paper consists of two largely independent sections, one for each of the tracks in which we participated, plus a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Microblog Track</head><p>Our approach to the Microblog track assembles a list of feature values for each tweet and uses learning to rank to combine them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features</head><p>We have two classes of features: query-dependent and query-independent features. Query-dependent features aim at capturing different aspects of relevancy of the tweet to the query. Query-independent features target at encoding the usefulness of a tweet in the information seeking process, and can be thought as information quality indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Query-dependent features.</head><p>We consider the following six query-dependent features: temporal query modeling, semantic query expansion, link retrieval, HITS scores, boolean, and Indri retrieval scores. We look at each feature in turn.</p><p>Our first feature, temporal query modeling, aims at capturing the dynamics of topics in Twitter <ref type="bibr" coords="1,479.63,395.47,10.79,9.54" target="#b2">[3]</ref>; the probability of a term given a query, P(t|θ Q ), is given by the weighted mixture of the original query Q and the expanded query Q, controlled by parameter α. To construct the expanded query, we rank terms according to Eq. 1, and select the top k terms. This model tries to take into account the dynamic nature of microblogging platforms: while a topic evolves, the language usage around it is expected to evolve as well. Consequently, selecting terms temporally closer to query time could result in terms that are more relevant for that point in time. Term scoring becomes a function of time:</p><formula xml:id="formula_0" coords="1,318.83,534.23,237.09,46.56">score(t, Q) = (1) log |N c | |{d : t ∈ d, d ∈ N c }| • ∑ {d∈N c :q∈Q and t,q∈d} e -β(c-c d ) ,</formula><p>where c is query submission time, c d is post d's publication time, N c is the set of posts that are posted before time c and q. The parameter β controls the contribution of each post to the score of term t based on their posted time. We select only those terms as candidate expansion terms, that occur in more than ϕ posts. In our experiments we set k = 10, β = 0.01, ϕ = 10. The feature value is the retrieval score score(t, Q).</p><p>As a second query modeling feature, we use semantic query modeling (SQM) <ref type="bibr" coords="1,411.30,686.66,14.12,9.54">[? ]</ref>. SQM leverages the anchor texts of incoming hyperlinks to Wikipedia articles, including not only "normal" hyperlinks, but also redirects, and alternative titles of pages. It does so in two steps. First, the anchor texts are used to identify, and score relevant Wikipedia articles for all possible term n-grams in a query. Then, for each of these articles, we again use the incoming anchor texts. In this step, however, we use them to determine the parameters of a language model for each article. The language models are subsequently combined for all n-grams in the query, yielding as end result a semantically-informed language model of the query, i.e., a semantic query model. The feature value is the retrieval score for the expanded query and the tweet.</p><p>Our third feature is link retrieval. We extracted, and crawled all links from the tweets after resolving the link's original URL if it was shortened. Some links were not accessible either due to URL "unshortening" services blocking pages with doubtful content, or because the link was simply broken. For those links we ended up having content for, we indexed them using Indri. We use language modeling and SQM to model queries, and fire them against this link index. The retrieved links are mapped back to the tweets that contain them, producing a ranking of tweets. The feature value score for a tweet is the retrieval score for the link.</p><p>We also consider the retrieval scores of different rankers, such as strict boolean matching of the query terms, and Indri's retrieval model.</p><p>Our sixth, and final, query-dependent feature is based on HITS scores. We start with retrieving a set of tweets for a query, using standard language modeling. Then, we create a weighted directed graph, with the nodes being the authors of the retrieved tweets. The edges denote the number of times a user retweeted another user's tweet. Based on this graph, we calculate the HITS authority and hub scores for every user in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Query-independent features.</head><p>Next we discuss our query-independent features: textual features and Twitter-specific features.</p><p>We use textual features to decide the potential interestingness of a tweet <ref type="bibr" coords="2,116.88,519.28,10.58,9.54" target="#b3">[4]</ref>. Following <ref type="bibr" coords="2,178.79,519.28,10.58,9.54" target="#b4">[5]</ref>, we believe that the ratio of capital to lowercase letters in a tweet is important: tweets with only capital letters (shouting) and all lowercased tweets indicate a different author profile than tweets with correct casing. Another measure of interestingness can be the language density in a tweet, namely, the sum of tf • idf values of non-stopwords, divided by the number of stopwords they are apart, squared <ref type="bibr" coords="2,112.83,602.97,10.58,9.54" target="#b1">[2]</ref>. We trained a spam classifier using semimanually selected spam tweets (making use of the @spam hashtag) and assigned each tweet a spam value. We also included features that encode whether a tweet is a question or an exclamation, and the number of links in the tweet.</p><p>Our second set of query-independent features is Twitterspecific features. Some inherent characteristics of tweets include the use of hashtags, the mention of usernames in a tweet and the possibility to retweet a tweet. We encode these characteristics into features, such as whether a tweet is a direct message, the number of hashtags, and the number of usernames in a tweet. We also exploit Twitter's network properties. In particular, we set up a graph similar to the HITS graph mentioned above using all tweets in the collection. We compute the PageRank scores of the nodes (authors) and use them as a feature for tweets. These scores denote the importance of a user by their likelihood of being retweeted. Other network features we take into account include the number of friends and followers, and the number of tweets a user uttered before the current tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Combination of features.</head><p>Query dependent features (except for the HITS feature) were combined together using a learning to rank (LR) method, trained on an in-house developed set; see below. The query independent features (including the HITS feature) were used for learning over a set of relevant tweets, useful for filtering out low quality tweets before retrieval. Here, we used the relevance assessments for our training set to create a set of tweets that were labeled as relevant and non-relevant. Based on that, we trained a Random Forest classifier, and kept only the tweets in the collection classified as relevant. Given each ranked result list of tweets, we calculated the z-score of the scores. All tweets with a z-score greater than ζ were in the final result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Runs</head><p>We submitted four runs, as listed in Table <ref type="table" coords="2,483.51,399.73,3.74,9.54" target="#tab_0">1</ref>. For all runs, we use language identification [? ] to identify, and keep only the English language tweets in the collection. Duplicate tweets are removed, and the oldest tweet in the set of duplicates is kept. Retweets are also removed, however, we store the retweet information. In ambiguous cases, e.g. where comments were added to a retweet, the tweet is kept. Hashtags remain in the tweet as simple words, i.e. we simply removed the leading hashtag. Finally, we performed punctuation and stop word removal, based on a collection based stop word list. To prevent future information from leaking into our collection, we created separate indexes for every query.</p><p>For training we developed an in-house dataset capturing the period January 1st, 2011 to March 15, 2011. The dataset consists of 39 queries, with their timestamps for a dataset of 12.7 million tweets. For each query, we selected tweets published before the timestamp of the query. Each query had 100 annotated tweets. To prevent leaking information, the sizes of the training sets vary between 2,363 and 1,422 tweets, depending query time .</p><p>Our baseline (COMMITbase) uses temporal query expansion and returns only tweets that contain a link. The second run (COMMITexp) uses learning to rank with all querydependent features, except for the link mapping: temporal query expansion, semantic query modeling, boolean matching, and language modeling. The third run (COMMITlinks) uses the same features as in COMMITexp, however it in- cludes the link mapping using SQM, and a standard language model on the link corpus. In our final run (COM-MITfilter) we applied pre-filtering. The ranked lists from all runs are further curated. We normalize the retrieval scores using z-scoring, and discard tweets with low z-score. Based on manual inspection of the outcomes of our preliminary experiments on our training queries, we set ζ = 0.6. All runs except the baseline use external data. While P@30 is the official metric, we also look at other metrics to better understand the performance of our systems. We report on MAP and precision at 5 (P@5) and 30 (P@30). We use the Student's t-test to evaluate statistical significance, and denote statistically significant increase in performance with and (p &lt; 0.01 and p &lt; 0.05 respectively). Likewise, and denote a statistically significant decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>In this section, we report on the performance of our runs, and we discuss two main points of interest for our participation in the Microblog track: a) whether retrieving links of a tweet as a feature increases performance, and b) the effect of prefiltering of tweets. Table <ref type="table" coords="3,88.55,423.64,4.98,9.54" target="#tab_1">2</ref> lists the results for our four runs, for two relevant sets: a) all relevant, and b) highly relevant. A general comment is that COMMITexp, COMMITlinks, and COM-MITfilter show similar performance in all metrics. Compared to the baseline (COMMITbase) all show marginally higher P@5 (not statistically significant), and lower P@30 and MAP (statistically significant). Looking at the reasons of this performance differences, we find that all runs return on average less tweets than the baseline (20 vs. 135). By cutting off very early, we do not retrieve enough tweets. Essentially, with the scores of the runs other than the baseline are clustered around the mean and do not have such a high variance. In future work, we aim at defining ζ as a function of the scores shape distribution.</p><p>Looking at the results of our three runs, we find that in very tight cut-off thresholds, using more corpora has no effect on P@5, but query expansion such as SQM, does not decrease recall. COMMITfilter shows that filtering in tight cutoff thresholds does has a negative effect in performance. In some cases there are less that 5 tweets returned, and further filtering on those small sets is likely to decrease P@5 and recall. We can however see a small improvement (though not significant) from COMMITexp to COMMITlinks in MAP. A possible explanation is the large number of tweets in the evaluation set with at least one link (70% and 80% of rele-vant and highly relevant tweets, respectively), which shows to help increase recall. Retrieving highly relevant tweets is more difficult. COMMITlinks has a higher precision than the baseline, however not for two queries. Indeed, for those two queries, we failed to retrieve the one or two relevant tweets. Obviously, the cut-off procedure is too strict again: returning more tweets would increase recall and thus MAP. We believe P@30 may not be the right metric for the task, given the small number of average highly relevant tweets (11). Our runs show an increase in P@5 and P@10, stemming from a strict cut-off criterion aiming at selecting highly relevant tweets. Similar to retrieving all tweets, we have an improvement using link mapping, indeed a higher percentage of highly relevant tweets in the ground truth have links (∼80%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entity Track</head><p>In this year's edition of the Entity Track we focus on the Entity List Completion (ELC) task. In the ELC task the goal is to find entities in structured data given a source entity, relation, target type and example entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity List Completion Approach</head><p>The corpus consists of a new Linked Data (LD) crawl. To process and index the data code was provided for two methods of indexing: entity centric (ED) and document centric (DE). In our experiments we use the entity centric index. The LD crawl consists of RDF triples, where an RDF triple consists of a "subject," "predicate" and an "object." A subject is always a URI and represents a "thing" (in our case: an entity). Subject URIs serve as unique identifiers for entities. An object is either a URI referring to another "thing" or a string , holding a literal value. Predicates are also always URIs and represent relations between subjects and objects. The entity centric representation assumes an entity to be represented by the set of all triples that have the same URI as subject.</p><p>Entity resolution. Another change this year is that the URIs of the example entities are no longer provided in the topics. Instead the homepage and the name of example entities are given. To resolve entity names to URIs we use the anchor text in Wikipedia. Given an entity name (e) we find the set of all Wikipedia pages (W P) that have e as anchor Text-based approach. The text-based approach follows the intuition that entities with links to objects that have overlapping terms with the narrative and source entity should be ranked higher than entities that do not. To represent entities we use the terms present in the entity centric representation.</p><p>Entities are indexed based on the terms occurring in the literals and URIs in each entity representation. We rank entities according to the similarity between the entity's textual representation (e d ) and a query (Q) containing the source entity name (E text ) and relation (R). The similarity is calculated using the vector space model with standard tf-idf term weighting:</p><formula xml:id="formula_1" coords="4,101.11,518.85,144.48,31.25">sim text (e, Q) = V (e d ) • V (E text R) | V (e d )| • | V (E text R)| ,</formula><p>where the numerator is the dot product of the vectors, | V | is the length of V and V (E text R) is the term vector containing the terms from E text and R.</p><p>We additionally filter entities retrieved by the text-based approach on type. The type is given in the topic, e.g., Company. We construct a URI by appending the type to http://dbpedia.org/ontology/. Only entities that have this URI as an object in their representation are kept, otherwise they are removed from the ranking.</p><p>Link-based approach. The link-based approach follows the intuition that candidate entities that are more similar to the example entities should be ranked higher than entities that are less similar. In this setting we use the links in the entity centric representation, i.e., an entity representation consists of all RDF triples which have the entity's URI as subject (i.e., outlinks) or object (i.e., inlinks). Together these triples form the link-based representation of an entity (e l = {r 1 ,. . . ,r m }, where r i is an RDF triple).</p><p>Under this representation, entities consist of sets of triples. The set of example entities becomes a set of sets of triples (X = {x 1 , . . . , x n } and x i = {r 1 ,. . . ,r k }). We rank entities according to the similarity between the entity's link-based representation e l and a query (Q) containing the set of example entities (X). The similarity is calculated according to the weighted Jaccard similarity between the triples of an entity and the triples of the example entities:</p><formula xml:id="formula_2" coords="4,367.28,322.71,134.21,28.65">sim link (e, Q) = ∑ r∈ x∈X (x∩e l ) w(r) ∑ r∈ x∈X (x∪e l ) w(r)</formula><p>, where x∈X (x ∩ e l ) is the union of the triples that the examples in X and e l have in common, x∈X (x ∪ e l ) is the union of all the triples in X and e l , and w(r) is a weight function that determines the importance of a link. We set the weight proportional to the number of times a triple occurs in the representation of the example entities:</p><formula xml:id="formula_3" coords="4,380.33,446.82,104.18,22.85">w(r) = max 1, ∑ x∈X n(r, x)</formula><p>Here n(r, x) is 1 if a triple r occurs in the representation of example x and 0 otherwise.</p><p>Candidate selection from a web corpus. Both the text and link-based approaches consider all entities in the LD crawl as candidate entities and rank them either based on the textual representation or overlap with the example entities. Given the sparse nature of LD, differentiating between candidates becomes more difficult when the number of candidates increases. To limit the set of candidates we propose to only consider entities that occur on webpages associated with the source entity.</p><p>To find webpages associated with the source entity we look at three types of pages: pages that are part of the homepage of the source entity (HP); Wikipedia pages that link to the homepage of the source entity (W P l ); and Wikipedia pages that are relevant to a query (W P q ), where the query consists of the source entity (E text ) and the relation (R).</p><p>To obtain candidate entities we apply a named entity recognizer to these pages. To further limit the set of candidate entities we rank each page in the HP and W P l set based on the number of example entities that occur on the page: rank(page) = |{e : e ∈ C page ∩ e ∈ X names }|, where X names is the set of names of the example entities and C page is the set of entities that occur on page. To rank the W P q pages we use the query likelihood, i.e., the probability that the query q (E text and R) is generated by a Wikipedia page: P(q|θ wp ) = ∏ t∈q P(t|θ wp ) n(t,q) , where θ wp is the language model representation of wp, and n(t, q) is the number of times t occurs in q. We chose the top 5 highest ranked pages from each source and use the entities in those pages as candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Runs and Results</head><p>We submitted the following four runs: ilpsTextFilt This run is created using the text-based method to find entities, i.e., querying the entity centered index with the relation and source entity. Entities are filtered based on the DBpedia type specified in the topics.</p><p>ilpslinkcand This run is created using the link-based method; entities are ranked based on the link overlap with the example entities.</p><p>ilpslinkOL This run also uses the link-based method to rank entities. Candidate entities are harvested from 3 sources: the homepage domain of the source entity, pages linking to the source entity homepage domain, and DBpedia pages relevant to the narrative and source entity.</p><p>ilpsPMIcMNZ Our final run combines the ranked lists from the text and the link based runs. Instead of using standard COMBMNZ we weigh candidates found by each of the methods by co-occurrence in the homepage.</p><p>At the time of writing of these working notes no evaluation results are available yet for the ELC runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we reported on our participation in the Microblog and Entity tracks. For the Microblog track we found that a simple cut-off based on the z-score is not sufficient: for differently distributed scores, this can decrease recall.</p><p>A well set cut-off parameter can however significantly increase precision, especially if there are few highly relevant tweets. Filtering based on query-independent filtering does not help for already small result list. With a high occurrence of links in relevant tweets, we found that using link retrieval helps improving precision and recall for highly relevant and relevant tweets. Future work should focus on a score-distribution dependent selection criterion. For the Entity track there are no analyses or conclusions to report yet; at the time of writing no evaluation results are available for the Entity track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,116.77,59.01,373.68,81.24"><head>Table 1 :</head><label>1</label><figDesc>Overview of our Microblog runs.</figDesc><table coords="3,116.77,59.01,118.77,9.54"><row><cell>Name</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,59.01,410.42,301.33"><head>Table 2 :</head><label>2</label><figDesc>Results for our runs. text. We then pick the Wikipedia page that is most commonly referred to by a link with anchor text e: |L e,wp | ∑ wp |L e,wp | , and |L e,wp | indicates the number of times entity name e is linked to Wikipedia page wp. To map the URL of a Wikipedia page to a URI in the LD crawl we replace the http://en.wikipedia.org/wiki/ part of the URL by http://dbpedia.org/resource/. In the following we only consider example or candidate entities that have been successfully mapped to a DBpedia URI.</figDesc><table coords="4,53.80,59.01,410.42,200.91"><row><cell></cell><cell></cell><cell></cell><cell>All relevant</cell><cell></cell><cell cols="2">Highly relevant</cell></row><row><cell></cell><cell>Run</cell><cell>P@30</cell><cell>MAP</cell><cell>P@5</cell><cell>P@30</cell><cell>MAP</cell><cell>P@5</cell></row><row><cell></cell><cell cols="2">COMMITbase 0.4279</cell><cell>0.2746</cell><cell>0.5551</cell><cell>0.0952</cell><cell cols="2">0.1422 0.1633</cell></row><row><cell></cell><cell>COMMITexp</cell><cell cols="3">0.3034 0.1502 0.5633</cell><cell cols="3">0.0626 0.1071 0.1755</cell></row><row><cell></cell><cell cols="4">COMMITlinks 0.3082 0.1519 0.5633</cell><cell cols="3">0.0646 0.1081 0.1755</cell></row><row><cell></cell><cell cols="4">COMMITfilter 0.2946 0.1465 0.5551</cell><cell cols="3">0.0592 0.1035 0.1755</cell></row><row><cell>arg max</cell><cell cols="2">COMMONNESS(e, wp),</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wp∈W P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">where COMMONNESS(e, wp) is defined as:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">COMMONNESS(e, wp) =</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was partially supported by the <rs type="funder">European Union</rs>'s <rs type="programName">ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme</rs>, <rs type="funder">CIP ICT-PSP</rs> under grant agreement nr 250430, the <rs type="funder">PROMISE Network of Excellence</rs> co-funded by the 7th <rs type="programName">Framework Programme</rs> of the <rs type="funder">European Commission</rs> under grant agreement nr 258191, the <rs type="projectName">LiMoSINe</rs> project co-funded by the 7th <rs type="programName">Framework Programme</rs> of the <rs type="funder">European Commission</rs> under grant agreement nr 288024, the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project nrs <rs type="grantNumber">612.061.-814</rs>, <rs type="grantNumber">612.061.815</rs>, <rs type="grantNumber">640.004.802</rs>, <rs type="grantNumber">380-70-011</rs>, <rs type="grantNumber">727.011.005</rs>, the <rs type="institution">Center for Creation, Content and Technology (CCCT)</rs>, the <rs type="funder">Hyperlocal Service Platform</rs> project funded by the <rs type="funder">Service Innovation &amp; ICT program</rs>, the <rs type="projectName">WAHSP</rs> project funded by the <rs type="funder">CLARIN-nl program</rs>, under <rs type="projectName">COMMIT</rs> project <rs type="projectName">Infiniti</rs> and by the <rs type="programName">ESF Research Network Program ELIAS</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7uy8gtq">
					<orgName type="program" subtype="full">ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_wwu43Mt">
					<orgName type="program" subtype="full">Framework Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_ySR4S3d">
					<orgName type="project" subtype="full">LiMoSINe</orgName>
					<orgName type="program" subtype="full">Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_nR7G3uP">
					<idno type="grant-number">612.061.-814</idno>
				</org>
				<org type="funding" xml:id="_4ts3EQP">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_7YbHV78">
					<idno type="grant-number">640.004.802</idno>
				</org>
				<org type="funding" xml:id="_XsCh6zP">
					<idno type="grant-number">380-70-011</idno>
				</org>
				<org type="funding" xml:id="_TBxurZW">
					<idno type="grant-number">727.011.005</idno>
				</org>
				<org type="funded-project" xml:id="_4TCA3Dy">
					<orgName type="project" subtype="full">WAHSP</orgName>
				</org>
				<org type="funded-project" xml:id="_svWvEKs">
					<orgName type="project" subtype="full">COMMIT</orgName>
				</org>
				<org type="funded-project" xml:id="_AcAXFpc">
					<orgName type="project" subtype="full">Infiniti</orgName>
					<orgName type="program" subtype="full">ESF Research Network Program ELIAS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,333.41,407.90,222.51,9.54;5,328.43,419.86,227.48,9.54;5,328.43,431.81,17.43,9.54" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,436.19,407.90,119.72,9.54;5,328.43,419.86,88.50,9.54">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,426.18,423.34,76.77,5.95">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,451.74,222.50,9.54;5,328.43,463.69,227.49,9.54;5,328.43,475.65,227.49,9.54;5,328.43,487.60,227.49,9.54;5,328.43,499.56,227.49,9.54;5,328.43,511.51,143.24,9.54" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,395.94,475.65,159.97,9.54;5,328.43,487.60,227.49,9.54;5,328.43,499.56,29.70,9.54">Siteq: Engineering high performance qa system using lexico-semantic pattern matching and shallow nlp</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B.-K</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,392.82,503.04,163.10,5.95;5,328.43,515.00,45.56,5.95">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,531.44,222.51,9.54;5,328.43,543.39,227.49,9.54;5,328.43,555.35,227.49,9.54;5,328.43,570.79,227.49,5.95;5,328.43,579.26,149.16,9.54" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,427.79,543.39,128.13,9.54;5,328.43,555.35,210.51,9.54">Incorporating Query Expansion and Quality Indicators in Searching Microblog Posts</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Massoudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,328.43,570.79,227.49,5.95;5,328.43,582.75,34.98,5.95">ECIR 2011: 33rd European Conference on Information Retrieval</title>
		<meeting><address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,599.18,222.50,9.54;5,328.43,611.14,227.49,9.54;5,328.43,623.09,227.48,9.54;5,328.43,638.54,198.39,5.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,360.63,611.14,195.29,9.54;5,328.43,623.09,98.67,9.54">Bad news travel fast: A content-based analysis of interestingness on twitter</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gottron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kunegis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Alhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,445.08,626.58,110.83,5.95;5,328.43,638.54,194.28,5.95">WebSci &apos;11: Proceedings of the 3rd International Conference on Web Science</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,654.97,222.51,9.54;5,328.43,666.93,227.49,9.54;5,328.43,678.88,227.49,9.54;5,328.43,690.84,227.49,9.54;5,328.43,702.80,71.96,9.54" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,496.42,654.97,59.50,9.54;5,328.43,666.93,127.97,9.54">Credibility improves topical blog post retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,473.22,670.42,82.70,5.95;5,328.43,682.37,33.21,5.95">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="923" to="931" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
