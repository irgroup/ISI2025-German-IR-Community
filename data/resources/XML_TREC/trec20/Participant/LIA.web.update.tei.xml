<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.52,71.79,416.50,12.90;1,245.30,87.73,106.94,12.90">LIA at TREC 2011 Web Track: Experiments on the Combination of Online Resources</title>
				<funder ref="#_U6kbVdc">
					<orgName type="full">French Agency for Scientific Research (Agence Nationale de la Recherche)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,105.54,125.98,87.70,10.75"><forename type="first">Romain</forename><surname>Deveaud</surname></persName>
							<email>romain.deveaud@univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIA -CERI</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<settlement>Avignon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.15,125.98,69.24,10.75"><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
							<email>eric.sanjuan@univ-avignon.fr</email>
							<affiliation key="aff1">
								<orgName type="department">LIA -CERI</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<settlement>Avignon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.52,125.98,69.27,10.75"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@lsis.org</email>
							<affiliation key="aff2">
								<orgName type="institution">LSIS Aix-Marseille University</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.52,71.79,416.50,12.90;1,245.30,87.73,106.94,12.90">LIA at TREC 2011 Web Track: Experiments on the Combination of Online Resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0EC1C2378ABC8AA4445CF064385DDD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report the experiments we conducted for our participation to the TREC 2011 Web Track. The experiments we conducted this year aim at discovering how the combination of specific external resources in a language modeling fashion can help web search. We use Wikipedia and Google as external resources for different search contexts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When searching for a specific information, users query the retrieval system with a list of keywords, a question, a declarative sentence or maybe a long description of the search topic. However, this often does not fully describe the user information need, which may harm retrieval performance.</p><p>One way to better outline the topic of the search without the help of the user is to enrich the query with additional information. Such query expansion techniques have shown to significantly improve the effectiveness of retrieval systems in many TREC tracks before.</p><p>This year we experimented with the combination of two external and online resources for improving web search. Terms related to the information need are extracted from these resources and appended to the query, following a weighting scheme that reflects the relevance of each term to the initial query. We experimented expansions with Wikipedia, Google and both. When using only Wikipedia, we modeled the thematic links between the encyclopedic pages in order to generate a thematic graph. We used this graph to increase the thematic coverage and to expand the initial query with more terms linked to the topic.</p><p>The ClueWeb09 collection includes the English version of Wikipedia and is composed of approximately 504 million of English documents. Considering Google indexes roughly 45 billion web pages<ref type="foot" coords="1,332.12,303.07,3.99,6.91" target="#foot_0">1</ref> , we can assume that Google includes the ClueWeb09 collection itself. In the case of using Wikipedia as an external resource, we expand the query using a thematic graph extracted from an encyclopedic subset of the collection. In the case of using Google, we expand the query using terms extracted from a set of documents that include the collection. Our goal with these experiments is to compare both intra-collection and extra-collection approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval system</head><p>This year we used a language modeling approach to retrieval. We follow the work done by <ref type="bibr" coords="1,486.53,488.48,39.01,9.46;1,307.28,502.02,147.28,9.46" target="#b2">Diaz and Metzler (Diaz and Metzler, 2006)</ref> who provided a framework allowing to interpolate relevance models computed using external collections with the maximum likelihood query estimate. This approach highlighted significant improvements over query likelihood alone when performing retrieval on news and web data with expansion terms extracted from different news and web collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequential Dependence Model</head><p>The sequential dependence model (SDM) is a special case of the Markov Random Field model for Information Retrieval introduced by Metzler and Croft <ref type="bibr" coords="1,356.99,680.31,119.63,9.46" target="#b4">(Metzler and Croft, 2005)</ref>, and was used by several teams in previous Web Track editions <ref type="bibr" coords="1,333.47,707.40,111.69,9.46" target="#b0">(Bendersky et al., 2011;</ref><ref type="bibr" coords="1,451.27,707.40,74.28,9.46" target="#b3">He et al., 2010;</ref><ref type="bibr" coords="1,307.28,720.95,91.74,9.46">Smucker et al., 2010)</ref>. The sequential dependence instantiation of MRF aims to model dependencies between adjacent query terms.</p><p>The SDM provides two feature functions for two types of term dependence involving query bigrams.</p><p>The f O (q i , q i+1 , D) feature function considers ordered matches of two adjacent query terms and is denoted by the O subscript. The second one is denoted by the U subscript and considers unordered matches within a window of 8 terms. Here, c(#1(q i , q i+1 ), D) is the number of occurrencies of the bigram (q i , q i+1 ) in the document D. On the other side, c(#uw8(q i , q i+1 ), D) is the number of occurrence of the two query terms q i and q i+1 within an unordered window composed of 8 terms in the document D.</p><p>Finally, the query-document score using the above feature functions defined by the sequential dependence model is:</p><formula xml:id="formula_0" coords="2,74.10,352.29,216.17,116.92">score SDM (Q, D) = λ T q∈Q f T (q, D) + λ O |Q|-1 i=1 f O (q i , q i+1 , D) + λ U |Q|-1 i=1 f U (q i , q i+1 , D) (1)</formula><p>where λ T , λ O and λ U are free parameters, and f T (q, D) is a maximum likelihood estimate of term q in a document D computed over the target collection with a Dirichlet smoothing. We will further refer to the SDM scoring function defined in Equation (1) as SDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Online Resources Combination</head><p>After defining the basic retrieval models we use, we can now detail how external resources are incorporated to the SDM ranking function defined in Section 2.1. We incorporate terms extracted from different resources as feature functions in the model. The external resources we use in this work are Wikipedia and Google Search. Considering W a sequence of words extracted from a set of Wikipedia pages and G a sequence of words extracted from a set of Google pages, we rank docu-ments according to the following scoring function:</p><formula xml:id="formula_1" coords="2,310.65,88.10,214.89,70.73">score(Q, D) = score SDM (Q, D) + λ W w∈W H W (w) • f T (w, D) + λ G g∈G H G (g) • f T (g, D) (2)</formula><p>where λ W and λ G are fixed parameters (λ W = λ G = 1 in our experiments).</p><p>The weights H W (•) and H G (•) are the entropy measures of words computed over the different resource sets. Considering W a sequence of words extracted from a set of Wikipedia pages, the entropy measure H W we use is defined as follows:</p><formula xml:id="formula_2" coords="2,334.58,273.63,163.65,22.26">H W = - w∈W p(w|W) • log p(w|W)</formula><p>where word appearance probabilities p W (•) are computed within the whole set of Wikipedia pages. We chose an entropy measure to weigh the selected terms in order to reflect their relative informativeness within the set of pages they belong to. This measure behaves the same when using Google as an external resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Term extraction process</head><p>As described in Section 2.2, we use terms extracted from external resources as features in the ranking function. We detail in this section the processes involved in selecting informative terms from these resources for a given query. First, we explain how relevant documents are selected for each of the resources. Then, we give details about how terms are extracted and from the previously selected relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Page Selection</head><p>The purpose of this general query expansion is to associate documents issued from external resources to a single query. The underlying principle is that adding knowledge related to the search topic will help to better understand the user's information need. In this work we use two different resources for query expansion, namely Wikipedia and the Google search engine. No query reformulation are made for page selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Wikipedia search API</head><p>Wikipedia pages are retrieved using the online API tool provided by the free encyclopedia 2 . We use the query terms to query the API, which returns results of Lucene search engine 3 . The results given by the API consist of URLs of Wikipedia articles. When we ran our experiments during July 2011, the online version of Wikipedia had a total 3,641,203 encyclopedic articles. Given a query, the specific tool we developed automatically:</p><p>1. queries the API, 2. collects the resulting URLs, 3. gets the articles from URLs and concatenates them,</p><p>4. strips all HTML fields and filters stopwords, 5. computes frequencies of the words.</p><p>Then, we have a list of Wikipedia words W related to the user's query, with their frequencies in the selected pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Google search engine</head><p>When using Google, we query the search engine using the strict &lt;query&gt; text and collect the URLs of the top retrieved results. Again, we get the pages linked by the URLs, strip HTML tags and filter stopwords in order to obtain a list of words G which have been extracted from web pages retrieved by Google.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Term Extraction</head><p>We detailed in Section 3.1 how relevant documents from external resources are selected. Now we need to extract highly informative terms from the pages in order to add them to the query.</p><p>Each set of documents W or G is considered as a bag of terms that contain no stopwords. An entropy measure is computed for each term within its own set of documents. This measure allows to reflect the informativeness of each term considering the context of the user's search. Then, an informative weight is associated to each term. The terms are sorted by decreasing informativeness and the top-ranked ones are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Wikipedia Thematic Graphs</head><p>In the previous methods we expanded the query with words selected from pages directly related to the query. For our last run, we wanted to select broader and more general words, that could 3 http://lucene.apache.org/ stretch topic coverage, at the risk of being too general. Considering that we can retrieve up to 10,000 documents for each query, we expect that extending topic coverage in large part will lead to better results. The main idea is to generate a thematic graph between Wikipedia pages in order to generate a set of articles that (ideally) completely covers the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Thematic anchor texts</head><p>For this purpose we use anchor texts and their associated hyperlinks in the first Wikipedia page associated to the query. We keep the term extraction process detailed in Section 3 for selecting a Wikipedia page highly relevant to the query. We extract informative words from this page using the exact same method as above. But we also extract all anchor texts in this page.</p><p>The words selected with the entropy measure are considering as a set T W , as well as each anchor text. We then compute and intersection between set T W and each anchor text set. If the intersection is not null, we consider that the Wikipedia article that is linked with the anchor text is thematically relevant to the first retrieved Wikipedia article. We sum the previously computed entropies of the words in common between the anchors and the expansion words, which gives a relevance score to each anchor.</p><p>This method relies on the fact that anchor texts in Wikipedia are written by experienced users that are confident about the topic. Indeed, on the web hyperlinks are sometimes randomly constructed by robots, and some web pages can be linked together even if they do not share the same topic. Authors also have different writing styles that can affect the definition of anchor texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Building a complete weighted graph</head><p>We can iterate and construct a directed graph of Wikipedia articles linked together. Children node pages (or sub-articles) are weighted half that of their parents in order to minimize a potential topic drift. We avoid loops in the graph (i.e. a children node can not be linked to one of his elder) because it brings no additional information. It also could change weights between linked articles. Informative words are then extracted from the sub-articles and incorporated to the query as another resource, as described in Section 2.2.</p><p>The complete process of generating the weighted graph, from querying Wikipedia through its API to weighting the edges of the graph, is depicted on Figure <ref type="figure" coords="4,161.13,280.15,4.09,9.46" target="#fig_0">1</ref>. Here, the Wikipedia API ranks the article Lymphoma in Animals first for the query lymphoma in dogs, which is highly relevant. Informative words like "Cancer", "Lymph" or "Gastrointestinal" are extracted from this first article in order to expand the query. The algorithm also looks at the anchor texts and sees that they contain the previously selected words. If an article provides additional information about a concept or a topic that is used to expand the query, we can assume that the whole article is likely to be relevant. Hence we follow the hyperlinks and build the oriented graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We indexed the whole ClueWeb09 collection with Indri and two servers of 16 cores and 32GB of memory each.</p><p>We used the embedded stoplist along with the standard Krovetz stemmer.</p><p>We queried Wikipedia and Google simultaneously on July 2011. We used a plain Mozilla user profile to query anonymously Google, while we used the standard API for querying Wikipedia, as described in Section 3.1.1.</p><p>All the runs we submitted involve query expansion. We did not use any spam filter for all these runs. When expanding the query with a resource, with always select the top 20 words ranked according to their entropy measure. These entropies are also used to weigh the words to reflect their relative informativeness.</p><p>We submitted three runs for the Ad Hoc task:</p><p>liaQEWikiA Expanding the query with words extracted from the first Wikipedia page given by its API for a query. Document retrieval is performed over the full ClueWeb09 collection (category A).</p><p>liaQEWikiGoA Expanding the query with words extracted from the first Wikipedia page and the first Google page given by their APIs for a query. Document retrieval is also performed over the category A.</p><p>liaQEWikiGoo Same run as the previous one, but only retrieving category B documents. Considering that all our other runs are on category A, we will not discuss this run.</p><p>In the meantime we submitted one run for the Diversity task:</p><p>liaQEWikiAnA This run generates a thematic graph as described in Section 4. The two subarticles whose anchor texts contain the most expansion words are considering for building the graph. Hence, expansion words are taken from three sources: the first Wikipedia page and two children. Document retrieval is performed over the category A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results</head><p>Results are reported in Table <ref type="table" coords="4,445.36,633.92,4.09,9.46">1</ref>. We use a Sequential Dependence Model (SDM) as a competitive baseline. We set the weights in (1) as recommended by Metzler and Croft in <ref type="bibr" coords="4,468.75,674.57,56.79,9.46;4,307.28,688.12,52.09,9.46" target="#b4">(Metzler and Croft, 2005)</ref>: λ T = 0.85, λ O = 0.1 and λ = 0.05.</p><p>We observe that the combination of terms extracted from Wikipedia and Google achieves the best results in terms of MAP and early nDCG and precision. However using Google alone as a source of external information achieves slightly  <ref type="table" coords="5,98.76,148.97,4.24,9.46">1</ref>: Comparison of the retrieval performance of three of the four submitted runs and two additional runs. We use two sided paired wise Wilcoxon test ( * : p &lt; 0.1; * * : p &lt; 0.05; * * * : p &lt; 0.01) to determine significant differences with baseline.</p><p>better results in terms of ERR. Using Wikipedia alone for expanding the query performs significantly better than the baseline, as well as our thematic graph approach. It is important to note that this Wikipedia thematic graph run outperforms the "standard" use of Wikipedia for all measures excepting MAP. There was no significant differences between these two runs though.</p><p>The average P@10 and P@20 scores of our best run (liaQEWikiGoA) is slightly lower than the average of median participant score for each topic, but differences are not significant (p-value= 0.76 using wilcoxon test), meanwhile these differences are significant for runs using only one resource (pvalue between 0.9 and 0.1).</p><p>It appears that each resource significantly improves the baseline on its own. MAP, P@10 and P@20 scores for GooA are significantly higher (p-value&lt; 0.001). The same significant improvement exists for P@10 and P@20 scores considering the liaQEWikiA run (p-value&lt; 0.001), even though the improvement in MAP is a bit less significant (p-value&lt; 0.1). Using Wikipedia alone performs lower than using Google alone. Apart from the PageRank effects, the fact that Google englobes the ClueWeb09 collection plays a major role. However the inherent quality of Wikipedia helps the retrieval of relevant documents at early ranks (P@20= 0.2500). More surprisingly, when combining these two external resources using a language model, the resulting run liaQEWikiGoA significantly outperforms both GooA and liaQEWikiA runs considering MAP and P@20 measures (p-value&lt; 0.1 using Wilcoxon test). We also constructed a run that optimally combines the two resources (i.e. it selects the best resource for each topic). We observe that its MAP score (0.1700) is significantly higher (p-value&lt; 0.1) than the MAP of our automatic combination based on the language model (li-aQEWikiGoA). This opens perspectives for future improvements. This optimal combination could not highlight significant improvements in terms of early precision nor graded metrics, but it however achieves the best results for almost all metrics (ERR@20= 0.0790, nDCG@20= 0.2102, P@20= 0.2810).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Spam filtering</head><p>All the runs we submitted did not have any policy concerning spam documents. We wanted to see if the combination of information extracted from different resources could automatically filter this spam. Indeed, the quality of Wikipedia's content is very good because it is edited by contributors and reviewed by moderators. On the other side, one of Google's search engine last improvement consists of lowering the ranks of "low-quality" sites such as content farmers.</p><p>We used the "Fusion" set of spam scores for the ClueWeb09 provided by <ref type="bibr" coords="5,417.97,465.86,100.36,9.46" target="#b1">(Cormack et al., 2010)</ref>  <ref type="foot" coords="5,518.33,463.82,3.99,6.91" target="#foot_2">4</ref> . For each document in the collection, the spam list contains a percentile score, which indicates the percentage of the documents in the corpus that are "spammier". The authors recommend to label the documents with a percentile score below 70 as spam, and the others as non-spam. We followed these indication and pruned the spammed documents from the output of our submitted runs.</p><p>The results of our runs without any spam are reported in Table <ref type="table" coords="5,387.91,601.35,4.09,9.46" target="#tab_1">2</ref>. It is important to note that, contrary to the runs that contain spam, there is no significant difference for all the six metrics. This behavior was also noted by other participants during the workshop: it was indeed difficult to highlight significant differences between runs that performed over non-spammed documents. The SDM baseline performs very well for all metrics and achieves the best results in terms of MAP. The best results in terms of nDCG, MAP and early precision are achieved when using both Wikipedia On the other side, using Google alone seems to favor the diversity of retrieved documents. The run NoSpamGooA achieves the best results in terms of α-nDCG@20 and its performance in ERR-IA@20 are very close to the best run. When looking again at Table <ref type="table" coords="6,108.59,347.19,4.09,9.46">1</ref>, we see that the run GooA that only uses Google achieves the best diversity results by far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we detailed the runs we submitted and the experiments we conducted for the TREC 2011 Web Track. We experimented with the combination of two external resources available online, namely Wikipedia and Google, for improving web search. For this purpose we select words in first top ranked Wikipedia and Google pages in a pseudo-relevance feedback fashion and expand the original query. We also proposed a method generating thematic graphs using anchor texts and hyperlinks of Wikipedia pages. Results highlight significant improvements over a competitive baseline when searching over the entire and spammed collection, despite some effects can be attributed to the internal adjustments of the online resources we used. We also see that using several Wikipedia pages thematically linked together for selecting expansion terms helps retrieval. When applying a spam filter that removes 70 percent of the documents that are judged spams, our approaches performs better than the (strong) baseline. However these results are not statistically significant.</p><p>The results of this resource combination opens some perspectives for the future. First, we aim to do without online resources and reproduce the results with local indexes. Then we want to explore the use of more resources in order to help several search contexts and scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.00,219.81,453.55,9.46;4,72.00,233.36,128.16,9.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Generating a partial thematic graph (stopping at level 2) using anchor texts of Wikipedia pages. Query comes from topic 111.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,67.55,453.55,207.69"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the retrieval performance of the runs presented in Table 1 with all spammed documents pruned from the result list.</figDesc><table coords="6,72.00,67.55,446.64,207.69"><row><cell>Run</cell><cell>Resources</cell><cell>ERR@20</cell><cell>nDCG@20</cell><cell>MAP</cell><cell>P@20</cell><cell cols="2">ERR-IA@20 α-nDCG@20</cell></row><row><cell>NoSpamSDM</cell><cell>-</cell><cell>0.1216</cell><cell>0.2390</cell><cell>0.1651</cell><cell>0.3370</cell><cell>0.4184</cell><cell>0.5218</cell></row><row><cell>NoSpamWikiA</cell><cell>Wiki</cell><cell>0.1121</cell><cell>0.2425</cell><cell>0.1488</cell><cell>0.3440</cell><cell>0.3848</cell><cell>0.4765</cell></row><row><cell>NoSpamGooA</cell><cell>Google</cell><cell>0.1185</cell><cell>0.2426</cell><cell>0.1574</cell><cell>0.3280</cell><cell>0.4315</cell><cell>0.5433</cell></row><row><cell cols="3">NoSpamWikiGoA Wiki + Google 0.1230</cell><cell>0.2635</cell><cell>0.1628</cell><cell>0.3600</cell><cell>0.4345</cell><cell>0.5319</cell></row><row><cell cols="4">and Google as external resources. This can be</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">explained by the fact that these two valuable and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">clean resources bring redundant information. The</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">terms used for expanding the queries are often</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">very closely related, if not synonyms. Hence a lot</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of similar documents are retrieved, improving top-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ical relevance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,323.42,757.97,166.77,6.31"><p>http://www.worldwidewebsize.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,323.42,757.97,215.19,6.31"><p>http://www.mediawiki.org/wiki/API:Search</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,323.42,757.13,183.72,7.77"><p>http://plg.uwaterloo.ca/ gvcormac/clueweb09spam/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This work was supported by the <rs type="funder">French Agency for Scientific Research (Agence Nationale de la Recherche)</rs> under <rs type="projectName">CAAS</rs> project (<rs type="grantNumber">ANR 2010 CORD 001 02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_U6kbVdc">
					<idno type="grant-number">ANR 2010 CORD 001 02</idno>
					<orgName type="project" subtype="full">CAAS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,307.28,338.12,218.27,8.64;6,318.19,349.08,207.36,8.64;6,318.19,359.87,207.36,8.81;6,318.19,370.83,207.36,8.58;6,318.19,381.79,76.64,8.58" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,345.05,349.08,180.49,8.64;6,318.19,360.04,170.63,8.64">UMass at TREC 2010 Web Track: Term Dependence, Spam Filtering and Quality Bias</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,507.73,359.87,17.81,8.58;6,318.19,370.83,207.36,8.58;6,318.19,381.79,72.35,8.58">Proceedings of the Nineteenth Text REtrieval Conference (TREC 2010)</title>
		<meeting>the Nineteenth Text REtrieval Conference (TREC 2010)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,401.88,218.27,8.64;6,318.19,412.84,207.36,8.64;6,318.19,423.80,207.36,8.64;6,318.19,434.59,89.66,8.81" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,408.15,412.84,117.40,8.64;6,318.19,423.80,203.33,8.64">Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<idno>CoRR, abs/1004.5168</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,454.69,218.27,8.64;6,318.19,465.64,207.36,8.64;6,318.19,476.43,207.36,8.81;6,318.19,487.39,207.36,8.58;6,318.19,498.35,195.52,8.81" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,483.63,454.69,41.91,8.64;6,318.19,465.64,207.36,8.64;6,318.19,476.60,44.17,8.64">Improving the estimation of relevance models using large external corpora</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,383.82,476.43,141.73,8.58;6,318.19,487.39,207.36,8.58;6,318.19,498.35,191.58,8.81">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;06</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,518.45,218.27,8.64;6,318.19,529.41,207.36,8.64;6,318.19,540.36,207.36,8.64;6,318.19,551.15,207.36,8.58;6,318.19,562.11,87.34,8.58" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,489.02,529.41,36.52,8.64;6,318.19,540.36,189.95,8.64">Heuristic Ranking and Diversification of Web Documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,318.19,551.15,207.36,8.58;6,318.19,562.11,28.95,8.58">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2009">2010. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,582.21,218.27,8.64;6,318.19,593.00,207.36,8.81;6,318.19,603.96,207.36,8.58;6,318.19,614.92,207.36,8.58;6,318.19,625.87,124.84,8.81" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,484.38,582.21,41.17,8.64;6,318.19,593.17,169.20,8.64">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,507.73,593.00,17.81,8.58;6,318.19,603.96,207.36,8.58;6,318.19,614.92,207.36,8.58;6,318.19,625.87,120.90,8.81">Proceedings of the 28th annual international ACM SI-GIR conference on Research and development in information retrieval, SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SI-GIR conference on Research and development in information retrieval, SIGIR &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,307.28,645.97,218.27,8.64;6,318.19,656.93,207.36,8.64;6,318.19,667.72,207.36,8.81;6,318.19,678.68,207.36,8.58;6,318.19,689.63,55.90,8.58" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,399.08,656.93,126.46,8.64;6,318.19,667.89,149.59,8.64">Experiments with ClueWeb09: Relevance Feedback and Web Tracks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,489.48,667.72,36.06,8.58;6,318.19,678.68,207.36,8.58">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2009">2010. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
