<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.98,78.46,291.11,15.11">PKU_ICST at TREC 2011 Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.05,103.66,49.18,10.03"><forename type="first">Feng</forename><surname>Liang</surname></persName>
							<email>liangfeng@pku.edu.cn</email>
						</author>
						<author>
							<persName coords="1,260.33,103.66,62.35,10.03"><forename type="first">Runwei</forename><surname>Qiang</surname></persName>
							<email>qiangrunwei@pku.edu.cn</email>
						</author>
						<author>
							<persName coords="1,330.07,103.66,58.89,10.03"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
							<email>yangjw@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">http://www.lemurproject.org/lemur.php</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.98,78.46,291.11,15.11">PKU_ICST at TREC 2011 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63C8CF8F763EDDFD8F7BF144DAD8E311</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the PKU_ICST participation in the TREC 2011 Microblog track. In the first year of Microblog track, we designed a group of experiments to verify whether external resources and future resources would improve the performance of our system. Moreover, given that microblog track is a real---time adhoc task, we explored an approach making use of the temporal evidence. To obtain a better performance, we employed different strategies to generate final results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we describe our entry into the TREC 2011 Microblog track. This year, Microblog track focuses be represented by a query at a specific time. In particular, user wishes to see the most recent but relevant information to the query. Hence, the system should answer a query by providing a list of relevant tweets ordered from latest to earliest, starting from the time the query was issued. In the first run of Microblog track, we employed different methods to verify whether external resources and future resources would help our system improve its performance. Then, we incorporated temporal evidence into baseline method to meet the real---time demand. Lastly, we explored both static and dynamic methods to choose the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Microblog Realtime Adhoc Task</head><p>In this chapter, we describe our approach for the Realtime Adhoc Task in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">System Overview</head><p>Our system architecture is shown in Figure <ref type="figure" coords="1,290.94,540.21,4.02,10.66" target="#fig_0">1</ref>. We dealt with corpus and query in parallel. For Tweet2011 Corpus, we did necessary preprocessing including non---English word filtering and retweet elimination. In the next step, we employed two optional methods, and processing the hashtag information to original corpus, to generate three candidate corpuses to be retrieved. For the test query, we explored four approaches, namely retrieving NYTimes (New York Times) news articles, making use of WordNet relationship, combining both NYTimes and WordNet resource, selecting top tweet as relevance feedback, to expand the original query. In addition, both stemming and stop words elimination were applied to corpus and query processing. After the retrieval step with the help of Lemur1 IR 1 toolkit, a relevance score is assigned to each tweet. Next, we incorporated the temporal evidence into the normalized score. Finally, we developed two strategies to select results and re---ranked the result list according to tweet id.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preprocessing</head><p>The size of the corpus is approximately 16 million tweets over a period of 2 weeks. We crawled HTML version corpus directly from Twitter, using a provided tool. In the preprocessing step, we dealt with Tweet2011 Corpus in following ways:</p><p>Non---English tweets filter: we filtered out all tweets that have words encoded with non---ASCII code.</p><p>Simple retweet elimination: we with the consideration that these tweets are simple retweets without any other additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Retrieval Models</head><p>In the baseline stage, we employed four baselines just using original query as follows:</p><p>Boolean OR Model as baseline1 System returned the most recent 1000 tweets that contain any of the query terms.</p><p>Probability OR Model as baseline2 System firstly calculated the probability of a given query, using Indri Retrieval Model <ref type="bibr" coords="2,496.12,680.64,12.07,10.65" target="#b0">[1]</ref> which is based on a combination of the language modeling and inference network. We estimated the probability by the following formula:</p><formula xml:id="formula_0" coords="2,222.41,741.54,199.33,24.03">= 1 1 =1<label>(1)</label></formula><p>Where p(termi) is the probability of a query term. For instance, query MB004 is represented by Indri query language as follows:</p><p>After obtaining a list of tweets with relevance scores, we selected top _ tweets and re---ranked them according to their tweet ids. Note that _ is the parameter discussed in section 2.7.</p><p>Probability OR Model with the addition of simple rules as baseline3 In baseline3, we incorporated following two simple rules into baseline2: Rule1: contents within Quotation marks must be conjunctive, and this part should be assigned to a higher weight, i.e. 2.5. For example, query MB014 is represented by Indri language as follows: Rule2: conjunctive capitalized words must be contained within a fixed---length, i.e. 5, text window, and this part should receive a higher weight, i.e. 2.0. For instance, query MB001 is represented by Indri language as follows:</p><p>Language Model using KL---divergence method as baseline4 We employed language model <ref type="bibr" coords="3,244.73,509.01,14.88,10.65" target="#b1">[2]</ref> choosing KL---divergence method with the help of Lemur toolkit, to estimate the likelihood of a given query and tweets. To avoid the zero---probability problem and overcome data sparsity in Twitter in which most tweets usually contain a few words, we applied different smoothing methods <ref type="bibr" coords="3,450.46,555.81,13.14,10.65" target="#b2">[3]</ref>, namely Jelinek---Mercer, Dirichlet prior, and Absolute discounting. As we did in baseline2, we then selected top _ tweets and re---ranked them based on their tweet id, from latest to earliest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Future Resource vs. Present Resource</head><p>We obtained three corpuses, i.e. oriCorpus, hashtagCorpus and linkCorpus, after the document expansion approach discussed in section 2.5.2. Then, we built a unified index as well as an incremental index on all corpuses. Unified index is considered as a future resource because it uses the information that would not have been available to the system at the timestamp of the query, while incremental index is a present resource for it only takes advantages of tweets available at the time the query is issued. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">External Resource vs. Local Resource</head><p>In this year s track, external resources, such as NYTime news articles and WordNet, were used in the query expansion and document expansion process. Also, we took advantages of local resources to improve system s performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Query Expansion (QE)</head><p>We explored four query expansion approaches, namely NYTimes news---based QE, WordNet---based QE, mixture QE of NYTimes news and WordNet, and pseudo---relevance feedback QE, of which the first three methods made use of external resource, while the pseudo---relevance feedback QE approach expanded the original query by making the most of local resource.</p><p>NYTimes news---based QE: As news article and microblog are both time---sensitive, it not hard to consider of making use of news articles as external resources to expand original query <ref type="bibr" coords="4,157.63,282.79,11.95,10.65" target="#b3">[4]</ref>. In order to obtain a parallel news corpus, we chose New York Times as our external resource of news articles. We crawled news articles through NYTimes s advanced search page<ref type="foot" coords="4,192.05,313.95,3.24,6.54" target="#foot_0">2</ref> observing the following steps: 1) Set up the date ranging from 24th January 2011 to 8th February. 2) Treat the whole test query as the news search query.</p><p>3) Only keep nouns, capitalized words, and phrases in within Quotation marks as the news search query. 4) Only keep person s name, place name, capitalized words and words indicating time as the news search query. Note that at the end of step 2 to 4, we checked whether the number of retrieved news articles is greater than 10, if so, we stopped the crawling process. Otherwise at the end of step 4, the number of retrieved news articles is less than 10, we then did not expand this query. Term relationships were extracted from the articles we crawled from NYTimes search page. We used a co---occurrence analysis, which assumes that terms that often co---occur in the news articles are related. In particular, we consider a text window of fixed size, i.e. terms whose distance is not larger than that size are considered to co---occur, and this add 1 into the global count of their co---occurrences. To filter out noise, we used a threshold of 10 on the global count, i.e. we only retained the co---occurrences whose count is at least 10. For each query term, we retained a set of strongest co---occurrences as its expanded words. Note that the windows size and the number of expanded words are set up as parameters.</p><p>WordNet---based QE: We just considered of expanding noun query terms by using their synonyms relationship in WordNet. For each query term, we simply selected the strongest synonyms as its expanded words. In particular, we kept a global count for each query term, and for each sense we added every word s count which indicates how often this word appears in the specific sense into the global count. From a set of candidate words, we chose 10 strongest words as query term s expanded words.</p><p>Mixture QE of NYTimes news and WordNet: In this mixture query expansion approach, we retained 10 terms as expanded words, of which 5 words were selected from 5 strongest co---occurrences in NYTimes news---based QE while the other 5 words were obtained based on WordNet---based QE.</p><p>Pseudo---Relevance feedback QE: Pseudo---relevance feedback <ref type="bibr" coords="5,404.95,87.76,14.88,10.66" target="#b4">[5]</ref> is widely used in query expansion process. In general, pseudo---relevance process obtains a subset of documents to create a new query with the assumption that top retrieved documents are relevance. In our pseudo---relevance approach, however, we retrieved the first tweet from the subset composed of the most recent 100 tweets and rebuilt the query by adding tweet s content into the original query for the following reasons: 1) Given that twitter posts are updated fast, the tweet which is close to query s timestamp could represent user s information better. That s why we chose the most recent 100 tweets as the candidate pseudo---relevance document. 2) Our previous experiments based on the 12 example queries showed that retrieval results were sensitive to the number of words added into the original query. That s why we combined only one tweet s content with the original query. Notice that we ranked tweets by the probability that the given query could be generated by the tweet language model. We estimated this probability by calculating the unigram language model as follows:</p><formula xml:id="formula_1" coords="5,198.17,335.90,220.34,57.58">| = | =1 (2) | = 1 , | | + | |<label>(3)</label></formula><p>Where , is the number of times a query term occurs in the tweet, is the number of times a query term occurs in the collection of the most recent 100 tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">Document Expansion (DE)</head><p>Given that the length of tweet is limited by 140 characters, it is necessary to expand the document, i.e. tweet to overcome the vocabulary---mismatch problem. In this year s run, external resources were used in document expansion approach by add the content extracted from title field of outer link. Also, we made use of local resources to generate expanded terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outer link DE:</head><p>We extracted outer links from tweets as an external resource to expand document, i.e. tweet. For each outer link, we extracted the content wrapped in title tag by analyzing html code. Then we mixed original tweet with the extracted words as the new document. Note that in this year s track, we have not downloaded the whole collection of outer links because of Great Fire Wall in China.</p><p>Hashtag DE: Hashtag plays an import role in Twitter, signaling aspects of a tweet s meaning such as its topic or its intended audience <ref type="bibr" coords="5,413.83,626.04,13.05,10.65" target="#b5">[6]</ref>. For instance, #bbcworldservicecut ostensibly marks tweets related to affair about BBC world service staff cut. We created a lexicon by collecting all terms in 50 queries except stop words. When processing a tweet, we put a term into the original document if it is a sub---string of the hashtag with the tweet. For example, if a tweet has a hashtag #bbcworldservicecut, then we will put BBC, world, service and cut into it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Temporal Evidence</head><p>Considering that the system prefers latest tweets, we should normalize the score by taking temporal factor into consideration:</p><formula xml:id="formula_2" coords="6,165.62,90.71,312.44,11.26">( _ ) = ( _ ) Ã— ( _ ) (4)</formula><p>Where ( _ ) is the preliminary score and ( _ ) is a decreasing quadratic function, which penalizes tweets that are far away from the time query is issued smoothly. We suppose that the temporal factor ranges from to 1. Let denote the penalty factor which equals to 1 . Therefore ( _ ) should be computed as follows:</p><formula xml:id="formula_3" coords="6,126.14,187.80,352.25,36.51">_ = ( _ _ ) 2 + 1 = 1 ( _ _ ) 2 = ( _ _ ) 2 5</formula><p>Where (0,1)</p><p>As shown in Eq.5, a higher will increase the impact of time, so that the latest tweet will be assigned a higher weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Generate Final Results</head><p>Two strategies are used to generate final results from the scores which have been revised by the temporal evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static strategy Simply choose</head><p>_ highest---score results as the final results. Note that _ is a parameter to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic strategy</head><p>To begin with, we normalized the score ranging from 0 to M, which was an integer tuned from 1 to 20. Different from the static strategy, we suppose that the number of final result should be dynamic since the number of the relevant tweets varies from query to query. So we propose a tricky method to select final results dynamically. 1) Cut the scores into (M+1) intervals: [0, 1), <ref type="bibr" coords="6,293.46,466.65,10.71,10.65" target="#b0">[1,</ref><ref type="bibr" coords="6,304.17,466.65,14.28,10.65" target="#b1">2)</ref>, , [M, M+1) 2) Count the number of scores in each interval: 0,1 , 1,2 , , , +1 3) Compute the absolute difference between each adjacent number such as 0,1</p><p>Then find the maximum difference. Suppose that 1, and , +1 make the maximum absolute difference. 4) Any result whose score is higher than t will be chosen to the final results list. Finally, we sort the final results list according to the tweet id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Result Analysis</head><p>In this section, we analyze the result of our approaches. In this year s track, all 184 submitted runs were pooled to depth 30 according to the retrieval scores indicated in each run. Among 50 topics, query 50 did not have any relevant tweets in the pool, and only 33 topics have tweets judged to be highly relevant. Thus, the evaluation includes two separate sets of scores, first of which considers all relevant and highly relevant tweets as relevant and is over 49 topics, while the second considers only highly relevant tweets, and is over 33 topics. All submitted runs adopted pseudo---relevance feedback query expansion method, original corpus with incremental index and language model using KL---divergence retrieval model. Furthermore, temporal evidence has been integrated into relevance score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis of official runs</head><p>The only difference among the four submitted runs is the result selection strategy. In particular, the PKUICST run applied dynamic strategy to generate its result list, while the PKUICST2, PKUICST3, PKUICST4 applied static strategy with the parameter _ set as 30, 100, 300 respectively. From the evaluation result, we can see that the run with dynamic result number, i.e. PKUICST achieved a good P@30 value and better MAP and R---Prec values, compared with the runs that have static result number, which indicates that the dynamic strategy performs quite stably. Besides, PKUICST2 s value of P@30 outperformed others. Given that the average number of dynamic result is about 75, we could infer that the less the result number is, the better the retrieval result is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of unofficial runs</head><p>In addition to the submitted runs, we also did some complementary experiments for comparison. Notice that all the results are produced after parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Model Comparison</head><p>To compare different models, we employed baseline1 to baseline 4, utilizing the original query and corpus with incremental index. Both allrel (on the left side) and highrel (on the right side) evaluation results are shown in Figure <ref type="figure" coords="7,340.60,525.57,3.92,10.66">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: results of model comparison</head><p>From figure <ref type="figure" coords="7,158.41,723.24,5.22,10.66">2</ref>, we can find that among four baselines, language model using KL---divergence outperforms other models. We compared the results that original query runs on original corpus with different kinds of index. As shown in Figure <ref type="figure" coords="8,240.38,87.76,5.22,10.66">3</ref>, we could figure out that unified index which takes advantages of future resources yields a better performance than incremental index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Future Resource vs. Present Resource</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: results of index comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">External Resource vs. Local Resource</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Expansion Comparison</head><p>In section 2.4, we describe our four query expansion approaches and the results of different query expansion comparison are present in Figure <ref type="figure" coords="8,370.07,360.79,4.18,10.66">4</ref>. As shown in Figure <ref type="figure" coords="8,482.97,360.79,4.18,10.66">4</ref>, we could see that first three query expansions which made use of external resources did not increase the performance of system, compared with original query without any query expansion. However, pseudo---relevance feedback approach which only utilized the local resource yields a better performance, increasing the P@30 value by 8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: results of query expansion comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Expansion Comparison</head><p>Note that pseudo---relevance feedback query expansion approach obtain excellent evaluation values, we adopted it to our document expansion experiment. From the results shown in Figure <ref type="figure" coords="8,215.25,672.84,5.28,10.66" target="#fig_3">5</ref>, we could draw a conclusion that external resources in document expansion approach, i.e. outer links, improve the performance dramatically. However, local resources such as hashtag decrease the retrieval result slightly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future work</head><p>In this paper, we present our system for Microblog Real---Time Retrieval Task. We employed four various query expansion methods to overcome the vocabulary---mismatch problem. In addition, we developed two different approaches to expand the original document. Then, we incorporated temporal evidence into relevance score calculated by preliminary retrieval model, which made a trade---off between recentness and relevance.</p><p>To generate the final result, we explored two selection strategies based on the score revised by temporal evidence and then re---ranked the result list according to tweet id.</p><p>From the evaluation results, we can see that our system have achieved competitive results. In the future work, we will complement the outer link set to generate a more comprehensive corpus. Besides, we will build a more general and robust retrieval model with combination of relevance and recentness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgement</head><p>The work reported in this paper was supported by the National Natural science Foundation of China Grant 60875033. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,233.21,416.19,131.66,9.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,102.62,115.56,46.25,9.45;3,102.62,131.16,121.28,9.45;3,102.62,146.64,52.01,9.45;3,101.30,302.07,46.25,9.45;3,101.30,317.67,167.48,9.45;3,101.30,333.27,98.24,9.45;3,101.30,348.87,52.01,9.45;3,101.42,422.91,46.25,9.45;3,101.42,438.51,219.46,9.45;3,101.42,454.13,161.72,9.45;3,101.42,469.73,52.01,9.45"><head></head><label></label><figDesc>&lt;query&gt; #or(Mexico drug war) &lt;/query&gt; &lt;query&gt; 1.0 #or(release of the rite) 2.5 #1(the rite) &lt;/query&gt; &lt;query&gt; 1.0 #or(bbc world service staff cuts) 2.0 #uw5(bbc world service) &lt;/query&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,195.65,244.56,206.78,9.86"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: results of query expansion comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,97.32,205.05,19.91,8.90;9,97.32,179.42,19.91,8.90;9,97.32,153.79,19.91,8.90;9,97.32,128.13,19.91,8.90;9,97.32,102.50,19.91,8.90;9,97.32,76.87,19.91,8.90;9,136.75,220.99,24.15,8.90;9,183.34,220.99,19.43,8.90;9,225.84,220.99,23.18,8.90;9,268.34,103.40,25.36,7.19;9,268.34,120.32,26.48,7.19;9,268.34,137.21,19.92,7.19;9,323.71,207.00,5.05,8.90;9,311.14,185.76,17.66,8.90;9,316.20,164.54,12.61,8.90;9,311.14,143.30,17.66,8.90;9,316.20,122.09,12.61,8.90;9,311.14,100.85,17.66,8.90;9,316.20,79.63,12.61,8.90;9,347.57,222.93,24.15,8.90;9,393.24,222.93,19.43,8.90;9,433.13,222.93,26.20,8.90;9,479.28,106.25,25.36,7.19;9,479.28,121.92,26.48,7.19;9,479.28,137.60,19.92,7.19;10,89.90,71.99,415.06,10.00;10,89.90,88.03,163.71,9.43"><head></head><label></label><figDesc>conference on Research and development in information retrieval (Geneva, Switzerland, 2010), 787-788.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,89.90,747.48,418.12,26.25"><head>Table 1 and</head><label>1</label><figDesc>Table 2 show the allrel and highrel performance values of our submitted four runs.</figDesc><table coords="7,105.02,72.65,387.80,97.89"><row><cell>Run_ID</cell><cell>P@30 MAP R---Prec</cell><cell>Run_ID</cell><cell>P@30 MAP R---Prec</cell></row><row><cell cols="2">PKUICST 0.3796 0.2757 0.3492</cell><cell cols="2">PKUICST 0.1394 0.2555 0.2637</cell></row><row><cell cols="2">PKUICST2 0.3905 0.2221 0.2725</cell><cell cols="2">PKUICST2 0.1414 0.2380 0.2563</cell></row><row><cell cols="2">PKUICST3 0.2830 0.2478 0.3134</cell><cell cols="2">PKUICST3 0.0980 0.2273 0.2262</cell></row><row><cell cols="2">PKUICST4 0.2177 0.2207 0.2490</cell><cell cols="2">PKUICST4 0.0848 0.2126 0.1931</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,89.90,183.65,417.77,8.41"><head>Table 1 : The allrel performance of our submitted runs Table 2: The highrel performance of our submitted runs</head><label>1</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,93.14,765.83,251.96,8.05"><p>http://query.nytimes.com/search/advanced?srchst=m</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,106.46,571.68,398.81,9.43;9,89.90,586.86,209.55,10.00" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,331.72,571.68,156.56,9.43">Indri at TREC 2004: Terabyte track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In proceeding of the 2004 Text Retrieval Conf</note>
</biblStruct>

<biblStruct coords="9,105.86,602.88,399.73,9.43;9,89.90,618.09,416.24,10.00;9,89.90,633.69,248.57,10.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,241.45,602.88,264.14,9.43;9,89.90,618.51,90.58,9.43">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,191.57,618.09,314.57,10.00;9,89.90,633.69,169.98,10.00">CIKM 2001: Proceedings of the tenth international conference on Information and knowledge management</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.38,649.71,400.03,9.43;9,89.90,664.89,382.88,10.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,237.77,649.71,267.64,9.43;9,89.90,665.31,100.41,9.43">A Study of Smoothing Methods for Language Models Applied to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,197.09,664.89,183.74,10.00">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.46,680.91,399.04,9.43;9,89.90,696.09,378.61,10.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,206.35,680.91,294.45,9.43">An empirical comparison of topics in Twitter and traditional media</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,89.90,696.09,95.94,10.00">Technical Paper Series</title>
		<imprint/>
		<respStmt>
			<orgName>Singapore Management University School of Information System</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.98,712.11,399.45,9.43;9,89.90,727.71,415.67,9.43;9,89.90,743.31,205.23,9.43" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,274.67,712.11,230.76,9.43;9,89.90,727.71,23.00,9.43">Automatic Retrieval with Locality Information Using Smart</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,140.88,727.71,203.38,9.43">The First Text REtrieval Conference (TREC-1)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="59" to="72" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,106.22,758.48,399.98,10.00" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,180.59,758.91,215.53,9.43">Hashtag retrieval in a microblogging environment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,404.83,758.48,101.37,10.00">Proceeding of the 33rd</title>
		<meeting>eeding of the 33rd</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
