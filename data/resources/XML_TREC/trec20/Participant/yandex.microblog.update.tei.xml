<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.49,164.85,272.26,15.12">Yandex at TREC 2011 Microblog track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.27,197.33,101.76,10.48"><forename type="first">Zlata</forename><surname>Obukhovskaya</surname></persName>
						</author>
						<author>
							<persName coords="1,304.86,197.33,113.12,10.48"><forename type="first">Konstantin</forename><surname>Pervyshev</surname></persName>
						</author>
						<author>
							<persName coords="1,220.32,211.28,77.24,10.48"><forename type="first">Andrey</forename><surname>Styskin</surname></persName>
						</author>
						<author>
							<persName coords="1,306.55,211.28,84.38,10.48;1,286.60,225.23,38.04,10.48"><forename type="first">Pavel</forename><forename type="middle">Serdyukov</forename><surname>Yandex</surname></persName>
						</author>
						<title level="a" type="main" coord="1,169.49,164.85,272.26,15.12">Yandex at TREC 2011 Microblog track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ACA92EF6A5B966C8D6C484235384FE4A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building microblog search is a challenging task in many ways. One of its most exciting aspects is the creation of reliable search engine architecture. However in our work we focused on tweet retrieval, utilizing relevance signals coming from various sources related to a tweet and learning to rank.</p><p>For ranking-related tasks we decided to rely on the following data sources:</p><p>• Text sources: tweet itself and the title of the linked URL</p><p>• Crowd activity (retweets)</p><p>We assumed that the number of retweets should be a good indicator of social interest to the particular document and the retweet prediction would be a useful feature for ranking. Also we encountered a problem with low recall of TF-based retrieval on small-sized twitter documents. This prompted us to use flexible term weighting models and query expansion to avoid missing relevant results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Search Framework</head><p>For our experiments we developed an offline search system that operated on MapReduce cluster. On the way we faced various problems related to inverted indexing and search. First, we had to calculate query-specific, time-specific and other statistics without re-indexing the entire data each time. So we tried to speed up the whole cycle of search and simplify the process of adding new features. As a result our search framework was able to calculate feature values for query-tweet pairs by a full scan of twitter corpus, aggregate them, if necessary, and re-use during any consequent full scan passes. So the search process represented a series of full scan and aggregation phases. All test topics were processed in parallel what took only 4 minutes on a cluster of 500 machines.</p><p>In this section we describe features and the details of ranking model. Experiments results are discussed in Section 4, Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document-Query Features and Query Expansion</head><p>TF-based textual features do not work well for twitter statuses since tweets are much shorter than regular web documents. In order to get as much signal from textual features as possible we had to use query expansion.</p><p>Our query expansion technique is based on pseudo-relevance feedback <ref type="bibr" coords="2,460.60,243.88,12.66,8.74" target="#b0">[1]</ref>. For each word in the results retrieved in the first search pass the relevance score is calculated. Relevance score is a probability of word being sampled from the same distribution as the words of query. Words are sorted by these scores in descending order. Then every word is added to the expansion while the ratio of the next word's score and the current word's score is less than some threshold. Which was empirically chosen to be 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Static Features</head><p>We used some external information for creating two groups of query-independent features. Most of the query-independent features mentioned below were implemented in related work <ref type="bibr" coords="2,231.81,564.33,13.53,8.74" target="#b1">[2]</ref> (marked with stars).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># ID Description 7</head><p>TweeLen   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ranking Function</head><p>In this section we describe how we trained ranking functions for TREC runs. We analyze improvements that we made after NIST assessments were distributed. Also we discuss the influence that our interestingness prediction (see Section 4.2) have over ranking quality. We used Gradient Boosted Decision Trees (GBDT) tool for training. Query expansion was used in all our runs.</p><p>The learning pool that we used was formed as follows. For each of the 12 example topics provided by NIST we selected about 80 tweets that had at least one of the text feature values greater than a certain threshold. All those tweets were assessed as either "relevant" or "not relevant".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Run 1 and Run 2</head><p>For both runs we trained regression on textual features (see Section 3.1). The first and the second attempt differed in the number of the iterations made by the learning algorithm. Results are in Table <ref type="table" coords="3,329.20,608.56,3.87,8.74" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Run 3 and Run 4</head><p>We used regression for run 3 and classification for run 4. In addition to textual features (see Section 3.1) we built extra features based on retweet prediction.</p><p>Our intuitions about using retweets for interestingness prediction are given in Section 5.1.1.</p><p>For creating retweet predictors we trained several classifiers with retweet classes as targets (0 for 0 retweets, 1 for 0-10 retweets, 2 for &gt;10 retweets) and combining various features sets and learning samples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quality Improvement</head><p>Having received the 50 topic tweet assessment provided by NIST we were able to improve our ranking function. Using positive examples from the assessment we increased P 30 from 0.2388 to 0.2923 for relevant tweets and from 0.0697 to 0.1011 for highly relevant ones. Using both positive and negative examples we managed raise P 30 even more.</p><p>Here, we discuss ranking function that was learned on random negative examples. This formula was trained as follows. From the 16 million tweet collection, we chose those tweets that contained at least one word from a topic extension. Of those tweets all relevant tweets (1800 tweets) and some randomly chosen were selected for our training pool.</p><p>For the resulting formula a 5-fold cross-validation evaluation on 50 topics was conducted. As can be seen, P 30 strongly depends on the number <ref type="bibr" coords="4,429.68,565.62,8.03,8.74">of</ref>  Our new ranking function has P 30 that is noticeably higher than before. The main reason for that is that our official runs were trained with much fewer negative examples (76%) than is optimal. Another explanation is that the new ranking function was trained with 40 topics, while our runs were trained with 12 topics only.</p><p>We conducted experiments to evaluate the dependence of P 30 on the number of topics in the pool. For 12 topics, the resulting P 30 equals to 0.2684 and noticeably depends on the particular topics of which the pool is formed. 5 Further Experiments</p><formula xml:id="formula_0" coords="5,227.18,234.31,6.78,8.74">P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments outline</head><p>For further experiments we reworked our search engine retrieval algorithm. In this section we ran experiments using simple search method and search with query and outer links expansions. Combinig features described in Section 3, and Section 5.1.1 we made a number of ranking functions using regression with GBDT tool.</p><p>The following sections describe details of the experiments and quality evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Artificially Created Features or Interestingness Predictors</head><p>The original lack of labels lead us to the idea of mining it from twitter statuses. The retweets seemed a good crowdsourced indicator of tweets interestingness. So we used retweets as targets while training retweet prediction funtion on the various sets of features (see Section 3). We called these artificially created features "interestingness predictors". The outcome of the predictors become new features values for training final ranking functions.</p><p>The natural consideration about using retweets as labels was that its interestingness signal may somehow be noisy. Shouldn't popular users be retweeted only because of popularity?</p><p>We tried to clear the signal by training predictors on the special tweet samples. Samples were created by splitting the whole corpus into parts relying on users social features.  We evaluated MAP, P 30 and BPREF measures with TREC evaluation tool using 5-fold cross-validation on 50 topics. All quality numbers in the following sections are averaged over folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Simple Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Textual Features</head><p>In this experiment we learned ranking function with textual (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Query &amp; Outer Link Expansion</head><p>For the following set of experiments we used search with pseudo-relevance feedback query expansion (see Section 3.1). Also we fetched titles of 20% of documents linked from tweets and made pseudo tweets out of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Textual Features</head><p>Considering strength of features (compare Table <ref type="table" coords="7,348.37,641.44,9.96,8.74" target="#tab_11">10</ref> to Table <ref type="table" coords="7,401.43,641.44,4.43,8.74" target="#tab_12">8</ref>) and number of relevant documents in search results (see Table <ref type="table" coords="7,343.07,653.40,9.22,8.74" target="#tab_11">11</ref>) it seems that query expansions do not help to find more relevant documents, though features based on</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,139.75,315.61,332.76,185.62"><head>Table 1 :</head><label>1</label><figDesc>Textual Features</figDesc><table coords="2,139.75,315.61,332.76,163.30"><row><cell></cell><cell>3.</cell></row><row><cell cols="2">Some of our textual features use expansions.</cell></row><row><cell># ID</cell><cell>Description</cell></row><row><cell>0 SumIDF Q</cell><cell>Sum of query words IDFs</cell></row><row><cell>1 SumIDF QEx</cell><cell>Sum of expanded query words IDFs</cell></row><row><cell>2 3Gram Q</cell><cell>Jaccard index for the sets of character-level trigrams of</cell></row><row><cell></cell><cell>document and query</cell></row><row><cell>3 3Gram QEx</cell><cell>Jaccard index for the sets of character-level trigrams of</cell></row><row><cell></cell><cell>document and expanded query</cell></row><row><cell>4 NWords Q</cell><cell>Number of words in query</cell></row><row><cell>5 NWords QEx</cell><cell>Number of words in expanded query</cell></row><row><cell cols="2">6 SumPRel QEx Sum of relevance scores for the words added by query</cell></row><row><cell></cell><cell>expansion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,139.75,233.96,332.72,129.09"><head>Table 2 :</head><label>2</label><figDesc>Informativness features</figDesc><table coords="3,139.75,270.23,332.72,92.82"><row><cell># ID</cell><cell>Description</cell></row><row><cell>18 NFollowers*</cell><cell>Number of user's followers</cell></row><row><cell>19 NTweets*</cell><cell>Number of user's tweets</cell></row><row><cell>20 NFollowees*</cell><cell>Number of user's followees</cell></row><row><cell>21 NListed*</cell><cell>Number of groups where user was added</cell></row><row><cell>22 Verified*</cell><cell>True if users's account is verified</cell></row><row><cell cols="2">23 FollowersRatio Followees to followers ratio</cell></row><row><cell>24 NLastStatRT</cell><cell>Number of the retweets of the most recent user's status</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,254.55,376.63,99.79,8.74"><head>Table 3 :</head><label>3</label><figDesc>Social features</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,133.77,268.98,307.92,103.91"><head>Table 4 :</head><label>4</label><figDesc>Retweet predictors</figDesc><table coords="4,133.77,297.12,307.92,75.77"><row><cell>4.3 TREC results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our results, according to NIST.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>run 1</cell><cell>run 2</cell><cell>run 3</cell><cell>run 4</cell></row><row><cell>P 30 (relevant)</cell><cell cols="4">0.2027 0.2156 0.2190 0.2388</cell></row><row><cell cols="5">P 30 (highly relevant) 0.0667 0.0697 0.0515 0.0646</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,244.45,386.19,119.03,8.74"><head>Table 5 :</head><label>5</label><figDesc>TREC runs results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="5,153.16,234.31,301.61,139.85"><head>Table 6 :</head><label>6</label><figDesc>Quality of ranking functions trained using NIST assessments</figDesc><table coords="5,180.30,234.31,250.65,117.53"><row><cell></cell><cell>10</cell><cell>P 30</cell><cell>P 50</cell><cell>P 100</cell></row><row><cell>run 1</cell><cell>0.3653</cell><cell>0.3122</cell><cell>0.2914</cell><cell>0.2308</cell></row><row><cell>run 2</cell><cell>0.4429</cell><cell>0.3469</cell><cell>0.2931</cell><cell>0.2276</cell></row><row><cell>run 3</cell><cell>0.2531</cell><cell>0.2252</cell><cell>0.2082</cell><cell>0.1749</cell></row><row><cell>run 4</cell><cell>0.2939</cell><cell>0.2483</cell><cell>0.2196</cell><cell>0.1796</cell></row><row><cell></cell><cell cols="4">NDCG 10 NDCG 30 NDCG 50 NDCG 100</cell></row><row><cell>run 1</cell><cell>0.3806</cell><cell>0.4004</cell><cell>0.4277</cell><cell>0.4625</cell></row><row><cell>run 2</cell><cell>0.4763</cell><cell>0.4529</cell><cell>0.4578</cell><cell>0.4865</cell></row><row><cell>run 3</cell><cell>0.2554</cell><cell>0.2860</cell><cell>0.3137</cell><cell>0.3530</cell></row><row><cell>run 4</cell><cell>0.3067</cell><cell>0.3228</cell><cell>0.3416</cell><cell>0.3753</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="6,133.77,446.37,259.64,38.14"><head>Table 7 :</head><label>7</label><figDesc>More interestingness predictors</figDesc><table coords="6,133.77,475.73,129.27,8.77"><row><cell>5.1.2 Quality Evaluation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="6,133.77,582.65,343.71,20.69"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="6,453.69,582.65,23.79,8.74"><row><cell>) fea-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="7,133.77,206.86,343.71,304.88"><head>Table 8 :</head><label>8</label><figDesc>No expansions, textual features</figDesc><table coords="7,133.77,234.38,343.71,277.36"><row><cell cols="3">5.2.2 Informativeness &amp; Social Features</cell></row><row><cell cols="4">This experiment was generally meant to measure static features (see Section 3.2)</cell></row><row><cell>strength.</cell><cell></cell><cell></cell></row><row><cell cols="2">Features set P 30</cell><cell>MAP</cell><cell>BPREF</cell></row><row><cell>7</cell><cell cols="3">0.1092 0.1190 0.0007</cell></row><row><cell>8</cell><cell cols="3">0.0333 0.0748 0.0007</cell></row><row><cell>9</cell><cell cols="3">0.0329 0.0818 0.0007</cell></row><row><cell>10</cell><cell cols="3">0.0333 0.0822 0.0007</cell></row><row><cell>11</cell><cell cols="3">0.2870 0.2656 0.0679</cell></row><row><cell>12</cell><cell cols="3">0.0639 0.0966 0.0014</cell></row><row><cell>13</cell><cell cols="3">0.0341 0.0793 0.0007</cell></row><row><cell>14</cell><cell cols="3">0.0192 0.0870 0.0003</cell></row><row><cell>15</cell><cell cols="3">0.0239 0.0675 0.0007</cell></row><row><cell>16</cell><cell cols="3">0.0412 0.0842 0.0007</cell></row><row><cell>17</cell><cell cols="3">0.0979 0.1240 0.0031</cell></row><row><cell>18</cell><cell cols="3">0.0364 0.0752 0.0010</cell></row><row><cell>19</cell><cell cols="3">0.0356 0.0740 0.0006</cell></row><row><cell>20</cell><cell cols="3">0.0247 0.0693 0.0007</cell></row><row><cell>21</cell><cell cols="3">0.0655 0.0895 0.0047</cell></row><row><cell>22</cell><cell cols="3">0.0652 0.0898 0.0011</cell></row><row><cell>23</cell><cell cols="3">0.0820 0.0950 0.0031</cell></row><row><cell>24</cell><cell cols="3">0.0382 0.0823 0.0007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="7,159.92,525.32,288.08,8.74"><head>Table 9 :</head><label>9</label><figDesc>No expansions, informativeness &amp; social features strength</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we shared our experience of creating microblog search. Query expansions didn't help to improve search recall, but textual features based on expansions improved ranking. Query-independent features based on document text itself and its author's profile gave desired quality increase. Using retweets for predicting interestingness also didn't help in our struggle for better ranking and retrieval. Though we inclined to believe that further research on correlation between various values mined from twitter (for example, correlation between number of followers and retweets) will give new insights on building microblog search.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,149.27,149.78,328.22,8.74;10,149.27,161.74,328.21,8.74;10,149.27,173.69,164.46,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,275.43,149.78,148.30,8.74">Relevance-Based Language Models</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Larvrenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Intelligent Information Retrieval, Department of Comupter Science, University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,193.62,328.21,8.74;10,149.27,205.57,202.18,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,321.44,193.62,151.67,8.74">Information Credibility on Twitter</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Poblete</surname></persName>
		</author>
		<imprint>
			<publisher>WWW 2011 Session: Information Credibility</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,225.50,328.21,8.74;10,149.27,237.45,127.29,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,298.30,225.50,179.18,8.74;10,149.27,237.45,50.28,8.74">Estimation Methods for Ranking Recent Information</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Golovchansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,219.90,237.45,25.54,8.74">SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,257.38,328.21,8.74;10,149.27,269.33,328.22,8.74;10,149.27,281.29,236.36,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,383.47,257.38,94.00,8.74;10,149.27,269.33,181.69,8.74">Searching microblogs: coping with sparsity and document quality</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gottron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kunegis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,350.04,269.33,127.45,8.74;10,149.27,281.29,171.60,8.74">20th ACM Conference on Information and Knowledge Management</title>
		<imprint>
			<publisher>CIKM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
