<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.76,76.22,460.21,18.61">Auto-Relevancy and Responsiveness Baseline II</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.76,163.42,60.54,9.19"><forename type="first">Cody</forename><surname>Bennett</surname></persName>
							<email>c_bennett@tcdi.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Improving Concept Search to Establish a Subset with Maximized Recall for Automated First Pass and Early Assessment Using Latent Semantic Indexing [LSI]</orgName>
								<address>
									<addrLine>Bigrams and WordNet 3.0 Seeding</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.76,76.22,460.21,18.61">Auto-Relevancy and Responsiveness Baseline II</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2427E78C4D8C4E7255781C388D740F9D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We experiment with manipulating the features at build time by indexing bigrams created from EDRM data and seeding the LSI index with thesaurus-like WordNet 3.0 strata. From experimentation, this produces fewer false positives and a smaller, more focused relevant set. The method allows concept searching using bigrams and WordNet senses in addition to singular terms increasing polysemous value and precision; steps towards a unification of Semantic and Statistical. Also, because of LSI and WordNet senses, WSD appears enhanced. We then apply an automated method for</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>selecting search criteria, query expansion and concept searching from Reviewer Guidelines and the original Request for Production thereby returning a search result with scores across the Enron corpus for each topic. The result of the normalized cosine distance score for each document in each topic is then shifted based on the foundation of primes, golden standard, and golden ratio. This results in 'best cutoff' using naturally occurring patterns in probability of expected relevancy with limit approaching .5. Submissions A1, A2, A3, and AF include similar combinations of the above. Although we did not submit a mopup run, we analyzed the mopups for post assessment. For each of the three topics, there were documents which TAs selected as relevant in contention with their other personal assessments. The defect percentage and potential impact to a semi/automated system will also be examined. Overall the influence of humans involved (TAs) was very minimal, as their assessments were not allowed to modify any rank or probability of documents. However, the identification of relevant documents by TAs at low LSI thresholds provided a feedback loop to affect the natural cutoff. Cutoffs for A1, A2, A3 were nearly -.04 (Landau) against the Golden and Poisson means and F was nearly +.04 <ref type="bibr" coords="1,254.50,509.92,24.82,7.40">(Apéry)</ref>. Since more work is required to decrease false positives, it is encouraging to find a natural relevancy cutoff that maximizes probable Recall of Responsiveness across differing topics. Automated concept search using a mechanically generated semantically derived feature set upon indexed bigram and WordNet sense terms in an LSI framework reduces false positives and produces a tighter cluster of potentially responsive documents. Further, since legal Productions are essentially binary (R/NR), work was done to argue for scoring supporting this view. Obtaining Recall =&gt;90% and Precision =&gt;90% with a high degree of success is a two step process<ref type="foot" coords="1,122.20,627.85,2.67,4.48" target="#foot_0">1</ref> , of which we test and discuss the first (maximization of Recall) for this study. Therefore, our focus will be heavily skewed on the probability of attaining high Recall for the creation of a subset of the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Experiment Methods</head><p>See the TREC website for details on the mock Requests for Production, Reviewer Guidelines per topic and other information regarding scoring and assessing. Team TCDI's participation will be discussed without the repetition of most of that information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Participation</head><p>TCDI's baseline submissions assume that by building a blind automated mechanism, the result is a distribution useful as a statistical snapshot, part of a knowledge and/or eDiscovery paradigm, and/or ongoing quality assurance and control within large datasets and topic training strata. Further, corporations' Information Management architectures currently deployed can offer hidden insights of relevancy when historically divergent systems</p><p><ref type="foot" coords="1,342.52,397.45,2.67,4.49" target="#foot_1">2</ref> are hybridized. For TREC Legal Track 2011, TCDI's baseline submission considers a hybridization of NLP, Semantic and LSI<ref type="foot" coords="1,376.36,415.93,2.67,4.49" target="#foot_2">3</ref> systems. 4 runs were submitted of 5 -we did not submit a "mopup" run. For runs A1, A2 and A3, some keyword filtering was tested. The Final run, AF used no keyword filtering. Multiple side experiments were performed, some discussed further. Steps for running the main experiment are listed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Build for Indexing</head><p>[STEP 0] Baselines were submitted to TREC Legal using:</p><p>• 685,592 de-duped Enron emails and attachments conceptually indexed 4</p><p>• Additional features per document beyond unigrams: o bigrams produced by a simple algorithm o small set of randomly selected WordNet 3.0 senses • 3 Topics Data inputs were the mock Requests for Production, Reviewer Guidelines, and phone conversations. Similar to some Web methods, the verbiage within the legal documents and discussions were expanded upon using a mixture of Natural Language Processing, WordNet sense non-linear distance, LSI 2/5 and term and document frequency. Outputs were relevancy and rank among other metadata described in TREC Legal Track requirements.</p><p>Runs A1 and AF were automatic with no intervention, no feedback loop and no previous TREC seed sets. Runs 2 and 3 were used as subtle tests to gauge human / machine learning, with focus on how the document movement based on 40 human generated responsiveness calls per topic, out of the 1000 per topic allowable.</p><p>This automation provides a repeatable system, typical of black box approaches. This year's black box used "concept search" to obtain high recall in comparison to 2010's categorization approach. The addition of bigram and WordNet senses A should add focus to the standard concept search, attempting to give the syntax of LSI more semantic value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Expansion</head><p>[STEP 1] By expanding on last year's methods, multiple inputs 5 were parsed and applied to the query expansion algorithm B , creating 3 simple queries 6 . Topics 401, 402, 403: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Of Natural Cutoffs, Mathematical Constants, and Golden Ratios C</head><p>We attempt to smooth quantify potentially relevant documents by applying approximations to the Golden Mean -"The desirable middle between two extremes". Further, the nature of LSI is reminiscent of fringe ideas similar to ideas from theories of Biolinguistics D .</p><p>[STEP 3] Result scores were modified as below:</p><formula xml:id="formula_0" coords="2,72.76,577.75,78.19,59.04">As θ = cos_sim() = € d • q d q</formula><p>• Probability is shifted based on ratio influence @ θ=0.</p><p>Lower numbers have higher influence causing a stricter threshold:</p><p>5 Using verbiage from Mock Request for Production and portions of the Reviewer Guidelines from available at http://trec-legal.umiacs.umd.edu 6 This is counterintuitive to how eDiscovery typically handles keyword expansion, human based analytics or other team efforts. However, it does not preclude these actions from improving the automated method's capability. By applying this threshold conversion, a binary classification recalculation of 0:1 to &lt;&gt;.5 is possible. Probability of returning Responsiveness / Relevancy is mandated by values greater than .5<ref type="foot" coords="2,457.00,244.57,2.67,4.49" target="#foot_4">8</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>TCDI's runs without TA influence (AF) had preliminary avg. ROC AUC and Recall @ 200k scores at or above last year's highest averages<ref type="foot" coords="2,374.20,315.37,2.67,4.49" target="#foot_5">9</ref> .</p><p>To graphically set the stage, superimposed Gain graphs from the Legal Track assessors<ref type="foot" coords="2,406.60,339.85,5.31,4.49" target="#foot_6">10</ref> of Automated and Technology Assisted (green is tcdinokaAF run, and red dashed is roughly the 30% corpora returned threshold) along with general comments are shown below:</p><p>Topic 401</p><p>• all runs appear to miss an unstated goal of &gt;~90% Recall &lt;=30% documents returned</p><p>• the topic and underlying relationships may be semantically heterogeneous, ambiguous Topic 402</p><p>• automated runs appear to do well Topic 403</p><p>• best illustration of the power of Technology Assisted Review (for this study) with automated systems following closely</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring for 2011 (as with 2010)</head><p>Scores for all TCDI submitted runs are listed [Figure <ref type="figure" coords="3,259.09,535.83,2.96,7.41">1</ref> Arguably, however, document similarity and semantic degrees of separation FG based on "likeness" push potentially responsive outliers from the initial query direction. So, even if false negatives fall just below threshold, additional misses are less likely than higher degrees of similarity. But, "there is always one more 11 " document which may be relevant and nowhere near similar due to semantic ambiguity. The most important documents to a case arguably may be those which are in this outlier area, and more expensive to obtain.</p><p>Using this as a bookend as well as the notion of "Recall at 30% documents returned", we sought to refit a result set to naturally "break" at the center threshold of .5, so that statistical methods could be later employed to obtain outlying data.</p><p>If this natural threshold is used as described previously in the algorithm <ref type="bibr" coords="3,348.37,248.55,3.73,7.41">[</ref>Step 3], the combinatorial linguistic features should expose highest probability of responsiveness / nonresponsiveness numerically / visually [Figure <ref type="figure" coords="3,473.78,267.03,3.32,7.41">2</ref>] with the noise falling to the left of .5 and the likely Responsive falling to the right of .5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 -Natural Cutoff (.5)</head><p>Other scores produced by the official TREC algorithm deal more with Precision. As an automated system, our Precision is baseline middle or lower as suggested by Hypothetical F1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Topic 401: 28.6%  sampling seems appropriate to further maximize possible Recall. The manual effort spent ascertaining initial sets with high Recall could arguably be spent on finding critical outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Work</head><p>Our Hypothetical F1 and other self-estimations need further work to automatically game TREC Legal Track's scoring algorithm. However, the automated runs' Recall appears to be successful with comparatively excellent AUC ROC and "Recall at 30% documents retrieved" [Figure <ref type="figure" coords="4,204.59,175.59,2.95,7.41">4</ref>]. Again, our goal is not to create an all encompassing document set for production, but to establish the best case subset with maximized Recall to pass to TAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4 -2010, 2011 Averages</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Posteriori Control</head><p>The control was the Final run since no keyword filters were used and a less restrictive (Apéry) threshold was employed.</p><p>The control AF outperformed A1, A2 and A3. A1 assumed that by adding complex concepts into the index, a higher order of coupling would occur semantically, and therefore the application of a strict threshold. A2 and A3 measured high noise during the 40 human assessments, but there were intermittent hits of responsiveness that caused AF to apply a less intense threshold.</p><p>A1, A2 and A3 Topics 401 and 402 had "AND NOT" keyword filters as:</p><p>• dinner, lunch*, interiew*, drug.test*, gllery.openings, internetshortcut, job.application, trading.meeting, promotion A1, A2 and A3 Topic 403 had "AND NOT" keyword filter as:</p><p>• dinner, lunch*, interiew*, drug.test*, gllery.openings, internetshortcut, job.application, trading.meeting, promotion, air.condition*, cont.air, us.air, air.force, dry.clean AF had no keyword filters, solely using "concepts" (unigram and bigram).</p><p>Since AF was the superior run, the effects of the keyword filter appear negligible, although extensive analysis has yet to be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Secondary Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSI Indexing Comparisons</head><p>One negative effect bigrams add to the LSI model, is an overall lowering of document scores. However, the removal of noise in 2 out of 3 topics may give weight to the usage of complex concepts as useful features.</p><p>For Topic 401 [Figure <ref type="figure" coords="4,391.95,193.59,2.96,7.41">5</ref>], LSI indexing "out of the box" <ref type="bibr" coords="4,505.77,193.59,32.22,7.41">[BoW+R]</ref> had less noise than an index using WordNet features and Bigrams [RWN+B+R] complex feature building.</p><p>For Topics 402 [Figure <ref type="figure" coords="4,395.95,227.19,3.33,7.41">6</ref>] and 403 [Figure <ref type="figure" coords="4,463.14,227.19,4.40,7.41">7</ref> and 8], noise was reduced using complex feature building <ref type="bibr" coords="4,454.71,236.31,41.59,7.41">[RWN+B+R]</ref>. And in the case of Topic 403, dramatically reduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Exploration</head><p>WordNet HI 3.0 was used to add Ontology-like features to the statistical LSI index. While most of this work is arguably proprietary, there appears to be statistically valid Word Sense Disambiguation capabilities when the two are combined. Further, word senses seem to offer interesting context when requesting term-&gt;document relationships and overall topic modeling 12 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The application of a hybrid feature approach / complex concepts to Latent Semantic Indexing using very simple automated parsing and query construction appears promising in generating a high Recall set based solely on initial topic modeling (Request for Production). By reinterpreting a well known concept search method's (LSI) scoring and applying smoothing and best fit techniques found in many disciplines (besides IR) 13 , automated runs across diverging topics can attain Actual Recall of ~90% with maximum documents returned at 30% of the corpus. Using this probabilistically predetermined rate of success, the subset of automatically accrued data can be sent downstream for further analysis, 12 Not quite Blei et. al with LDA, but closer to relationships manually attained with Upper Ontology. 13 We affectionately name this method "Blatant Semantic Indexing". applied to a feedback system to further improve Recall at an attempt to completely maximize full potential, and/or to a Technology Assisted Review workflow. In any of these cases, the target should be maximizing Precision while allowing for best of breed statistical sampling / QC to assure max P / R. This automated study is not about replacing the human intelligence required to successfully complete an end-to-end review. It is one part of a display of how automated and human assisted workflows can in tandem guide a historically expensive process into a realm of data proportionality and expectation.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.76,293.91,155.40,8.09;2,90.76,303.03,168.95,7.41;2,90.76,312.39,156.09,7.41;2,90.76,321.51,151.20,7.41;2,90.76,330.63,140.95,7.41;2,90.76,339.99,78.04,7.41;2,72.76,355.11,207.46,8.09;2,90.76,364.23,137.40,7.41;2,90.76,373.59,115.60,7.41;2,90.76,382.71,73.59,7.41;2,72.76,397.83,206.99,8.09;2,90.76,407.19,125.39,7.41;2,90.76,416.31,173.43,7.41;2,90.76,425.43,153.83,7.41;2,90.76,434.79,78.04,7.41;2,72.76,449.91,217.96,7.41;2,72.76,459.03,21.34,7.41"><head>•</head><label></label><figDesc>401 -enrononline financial.instruments derivative.instruments commodities.enrononline enrononline.swaps enrononline.transactions enrononline.trades enrononline.commodity trading.enrononline enrononline.training (10 = 1 uni, 9 bi) [.11] • 402 -otc.derivatives derivative.regulation regulate.otc botched.deregulation legal.instruments regulatory.instruments derivative (7 = 1 uni, 6 bi) [.17] • 403 -environment environmental disaster oil.spill epa emissions enron.strategies habitats environmental.pollution noise.pollution oil.leaking environmental.policy environmental.training (13 = 6 uni, 7 bi) [.86] [STEP 2] Queries from Step 1 were submitted to the concept index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,367.24,73.83,151.71,8.10;2,385.24,83.67,133.99,7.41;2,385.24,92.79,146.60,7.44;2,385.24,102.15,129.17,7.41;2,331.24,117.99,198.99,8.09;2,349.24,127.11,178.30,7.41;2,349.24,136.47,180.10,7.41;2,349.24,145.59,60.45,7.41;2,367.24,160.71,133.58,8.10;2,385.24,170.07,90.69,7.41;2,331.24,185.19,198.08,8.09;2,349.24,194.31,167.70,7.41;2,349.24,203.67,147.67,7.41"><head></head><label></label><figDesc>o ~.33 = (θ + .5) / (1 + L) -Landau [~.5] (MIN) 7 ~.37 = e -1 -Poisson, Euler [~2.71828] (MEAN) ~.38 = 1 -1 / ϕ -Golden Mean [~1.61803] (MEAN) ~.42 = (θ + .5) / A -Apéry [~1.20205] (MAX) • Conversion of 0:1 distribution to one approaching a lim of .5 based on loose rational approximations to the Golden Ratio using Landau for runs A1, A2, A3 and Apéry for AF o A1, A2, A3 = tcdicskwA1 = [MIN] AF = tcdinokaAF = [MAX] • The change from [MIN] to [MAX] was based in part on runs A2 and A3, and the amount of resulting responsive documents determined by TA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,313.24,406.00,58.68,7.40"><head>FigureFigure 8 -</head><label>8</label><figDesc>Figure 5 -Topic 401</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,313.24,188.79,222.69,7.41;5,313.24,197.91,195.06,7.41;5,313.24,207.27,210.41,7.41;5,313.24,225.51,223.33,7.41;5,313.24,234.63,166.27,7.41;5,313.24,253.11,217.73,7.41;5,313.24,271.35,63.46,7.41;5,313.24,289.83,187.59,7.41;5,313.24,298.95,108.49,7.41;5,313.24,317.43,208.59,7.41;5,313.24,326.55,198.38,7.41;5,313.24,335.67,165.21,7.41;5,313.24,354.15,191.70,7.41;5,313.24,363.27,162.64,7.41;5,313.24,386.31,197.33,7.41;5,315.06,395.43,150.96,7.41;5,313.24,413.67,218.86,7.41;5,313.24,423.03,80.91,7.41"><head>A</head><label></label><figDesc>Fausto Giunchiglia, Uladzimir Kharkevich, Ilya Zaihrayeu, Concept Search: Semantics Enabled Information Retrieval, University of Trento, Italy http://www.ulakha.com/pubs/concept-search-tech-report-2010.pdf, 2010. B Christopher D. Manning, Hinrich Schütze, Foundations of Statistical Natural Language Processing, MIT Press, Cambridge, MA, 1999. C Dr. Bekir Taner Dincer, Mugla University for thoughts on Fibonacci, 2010. D Dr. Juan Uriagereka E Gordon Cormack and his extensive work with Spam detectionhttp://plg.uwaterloo.ca/~gvcormac/jig/ F Advanced Data Mining and Applications: 6th International Conference, ADMA, Finding Potential Research Collaborators in Four Degrees of Separation -By Longbing Cao, Yong Feng, Jiang Zhong. G A. Budanitsky. Semantic Distance in WordNet: An Experimental, Application-oriented Evaluation of Five Measures, 2001. H George A. Miller (1995). WordNet: A Lexical Database for English. Communications of the ACM Vol. 38, No. 11: 39-41. I Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.76,72.01,463.33,644.27"><head></head><label></label><figDesc>]:</figDesc><table coords="3,72.76,551.44,217.95,140.36"><row><cell>R@30</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>AF</cell><cell>Avg</cell><cell>AF Actual</cell></row><row><cell>401</cell><cell>70.5</cell><cell>71.0</cell><cell>72.6</cell><cell>72.9</cell><cell>71.8</cell><cell>~91</cell></row><row><cell>402</cell><cell>94.2</cell><cell>93.8</cell><cell>92.2</cell><cell>97.1</cell><cell>94.3</cell><cell>~99</cell></row><row><cell>403</cell><cell>99.0</cell><cell>99.0</cell><cell>66.9</cell><cell>100.3</cell><cell>91.3</cell><cell>~99</cell></row><row><cell>Avg.</cell><cell>87.9</cell><cell>87.9</cell><cell>77.2</cell><cell>90.1</cell><cell>85.8</cell><cell>~96</cell></row><row><cell>ROC AUC</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>AF</cell><cell>Avg</cell><cell></cell></row><row><cell>401</cell><cell>80.0</cell><cell>80.1</cell><cell>80.5</cell><cell>80.1</cell><cell>80.2</cell><cell></cell></row><row><cell>402</cell><cell>91.8</cell><cell>91.9</cell><cell>91.4</cell><cell>95.4</cell><cell>92.6</cell><cell></cell></row><row><cell>403</cell><cell>87.7</cell><cell>87.8</cell><cell>66.0</cell><cell>91.7</cell><cell>83.3</cell><cell></cell></row><row><cell>Avg.</cell><cell>86.5</cell><cell>86.6</cell><cell>79.3</cell><cell>89.1</cell><cell>85.4</cell><cell></cell></row><row><cell cols="3">Figure 1 -Submitted Runs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="3,72.76,699.75,221.31,7.41;3,72.76,708.87,221.03,7.41;3,313.24,73.83,141.83,7.41;3,455.08,72.01,3.20,4.49;3,460.66,73.83,68.03,7.41;3,313.24,82.95,222.85,7.41"><p>"AF Actual Recall" scores are those which were determined by documents humans have actually assessed. Other scores are obtained from the algorithmic probability E that documents will be Responsive / Non-responsive, but not returned by a human.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.86,666.87,219.88,7.41;1,72.76,675.99,218.76,7.41;1,72.76,685.11,213.34,7.41;1,72.76,694.23,215.49,7.41;1,72.76,703.59,210.05,7.41;1,72.76,712.71,123.61,7.41"><p>During initial data assessment, automated maximization of Recall should be of highest value, since the Recall will carry over to human assisted systems such as Technology Assisted Review, and/or other search methodologies whose focus is to maximize Precision. In tandem, the approach will give a higher probability of attaining max P/R, and use hybridization techniques allowing for semi-/ automated capabilities.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,317.34,657.51,212.94,7.41;1,313.24,666.87,213.68,7.41"><p>Keyword vs. concept, concept vs. probabilistic, concept vs. semantic, etc. Esp. with IR systems, hybridization offers revitalization and ROI longevity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,317.34,675.99,212.99,7.41;1,313.24,685.11,218.47,7.41;1,313.24,694.23,214.47,7.41;1,313.24,703.59,85.34,7.41"><p>The semantic and conceptual systems could be considered plug and play for different approaches. The approach is considered modular as long as a topic model is available and exemplar data is available specifying relevant and non-relevant information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="2,317.34,651.51,207.91,7.41;2,313.24,660.87,61.61,7.41"><p>Similarly approached by Robertson and Spark Jones, 1976 although for weight normalization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="2,317.34,669.99,198.01,7.41;2,313.24,679.11,212.97,7.41"><p>During litigation productions, if the document is leaving the door, it is considered Responsive / Relevant to the request -a very binary situation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="2,317.34,688.23,126.91,7.41;2,313.24,697.59,186.55,7.41"><p>Using the top average scores from tables at http://plg.uwaterloo.ca/~gvcormac/legal10/legal10a.pdf -page 3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="2,319.62,706.71,160.43,7.41"><p>Gordon Cormack, See http://trec-legal.umiacs.umd.edu</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
