<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.51,111.25,387.23,15.12">University of Waterloo at TREC 2011: Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.76,148.72,78.26,10.48"><forename type="first">Adam</forename><surname>Roegiest</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.68,148.72,103.80,10.48"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.51,111.25,387.23,15.12">University of Waterloo at TREC 2011: Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B6524D47EB50B9CF0A34C2EEC5D3FF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the first year of the Microblog Track, a real time ad hoc search task was determined to be a suitable first task. The goal of the track is to return the most recent but also relevant tweets for a user's query. Participating runs will be officially scored using precision at 30. Other experimental scoring measures will be evaluated in parallel to the official measure.</p><p>As this was the first year for the Microblog Track, our primary goal was to create a baseline method and then attempt to improve upon the baseline. Since the only task was to perform a real time ad hoc search for the track, we decided that the task would be best suited by using a traditional search methodology. In doing so we used the Wumpus Search Engine 1 , which was developed by Stefan Büttcher while at the University of Waterloo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus</head><p>From May 27th to June 3rd, the provided HTML crawler 2 was run after which the tweets were repaired using the provided repair code. Once it was learned that the HTML crawler returned tweets with null text bodies, the supplied repair code was modified to refetch tweets with null bodies. The modified repair script was run six times until it was no longer feasible to continue trying to repair tweets. Table <ref type="table" coords="1,242.22,587.84,4.98,8.74" target="#tab_0">1</ref> shows the distribution of tweets in our copy of the corpus. We note that status code 200 denotes manually posted tweets, 302 denotes Twitter supported retweets, 403 denotes tweets the posting user has protected, and 404 denotes deleted tweets. We note that any tweet marked as 403 or 404 has no associated information as it is no longer available for public use. No further attempt was made to retrieve the remaining 25,585 tweets as we were unsure as to whether or not the tweets were still available, e.g. not protected or deleted. In addition, we desired to begin working with the corpus and did not believe the missing tweets would significantly affect our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Methodology</head><p>Our basic idea was to use a wisdom of the crowds approach for our search methodology. To do this for each topics we took 30 sets of search results, created using feature engineering and a variety of ranking methods, and combined them using reciprocal-rank fusion (RRF) <ref type="bibr" coords="1,382.78,511.27,10.52,8.74" target="#b4">[5]</ref> as a method of meta-ranking the results. The RRF formula used is as follows</p><formula xml:id="formula_0" coords="1,370.86,542.69,123.93,23.23">RRF score(t) = Σ i 1 60 + r i (t)</formula><p>where r i (t) is the rank of the tweet t in result set i.</p><p>Feature engineering was performed for each query by creating six different indexes for Wumpus, where the index stores each tweet's features. Table <ref type="table" coords="1,512.88,615.13,4.98,8.74" target="#tab_1">2</ref> briefly describes the features used for each index. Tweets were ranked using five different methods, which are outlined in Table <ref type="table" coords="1,397.96,651.00,3.87,8.74">3</ref>, on each index. These combinations produce our 30 sets of results. Once all the results were combined we took the 30 highest scoring tweets for each and returned those as our relevant tweets. We note that the actual query issued to Wumpus was not just the provided topic but each whitespace delimited word was treated as its own search term. If this was not done then Wumpus would attempt to look for tweets containing exactly the topic phrase and this is not generally a desirable behaviour for a search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Runs</head><p>In this section we describe the official and unofficial runs that we conducted. Runs 1 through 4 were the runs submitted to TREC for official evaluation and so we give the run ID for each of these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Run 1 -Baseline (waterlooa1)</head><p>Our baseline run performs exactly as outlined above with no additions or changes. That is, we use the 5 ranking methods on each of the 6 indexes and use RRF on the 30 sets of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Run 2 -HTTP 200 Only (waterlooa2)</head><p>This run is identical to our baseline except only those tweets with a HTTP status of 200 are used. The intent with this run was to boost our return of highly relevant tweets as explicit retweets could not be matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Run 3 -Query Expansion (waterlooa3)</head><p>For this run we decided to perform pseudo-relevance feedback. Using the GOV2 corpus (from the Terabyte Track) as a language model, we performed KLDbased <ref type="bibr" coords="2,347.28,119.60,10.52,8.74" target="#b2">[3]</ref> and Okapi-style <ref type="bibr" coords="2,435.60,119.60,10.52,8.74" target="#b1">[2]</ref> pseudo-relevance feedback for each query over the top 15 documents and returning 8 feedback terms. Due to some internal limitations, we did not use Cover-Density Ranking for this run. This resulted in a total of 48 result sets being used rather than the standard 30. From our use of RRF, these additional 18 result sets could have improved the final results returned, however, we did not believe any such improvement would be significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Run 4 -Recency Blending(waterlooa3)</head><p>This run takes the results of the first run and blends the results with respect to how recent the tweet is. The goal here to use the idea that the search request happens when the topic is (relatively) popular and so more recent tweets are more likely to be relevant to the topic. Namely, each tweet, t, is re-scored as follows:</p><formula xml:id="formula_1" coords="2,334.55,350.66,197.74,22.31">recscore(t) = tweetid(t) max-tweetid * run1-score(t)</formula><p>where max-tweetid is the upper most recent tweet ID by the time of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Run 5 -Conjunctive Query</head><p>This run modifies the baseline by creating a new set of query terms by taking the pairwise conjunction of the original query terms. The intent here is to return documents which contain more of the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Run 6 -Stopword Removal</head><p>This run removes all stopwords from the query terms (excepting those that are part of quoted terms) as a means to remove the possibility of Wumpus returning tweets that are relevant to stopwords. Our goal in doing so was to hopefully improve the quality of the search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.7">Run 7 -Tweet-based Query Expansion</head><p>This run builds upon the theory of run 3 by using the all the tweets up until the oldest query tweet time (e.g. the earliest query) and builds a Wumpus-style language model using those tweets. The intent here is to see how much effect the language model has on the results returned.</p><p>Table <ref type="table" coords="3,262.66,72.11,3.87,8.74">3</ref>: Ranking Methods used Method Description Okapi BM25 <ref type="bibr" coords="3,143.65,96.17,15.71,8.74" target="#b5">[6]</ref> BM25 is a bag-of-words retrieval method that ranks documents based upon how often query terms appear in the document and does not take into account any relationships between query terms in the document (e.g. proximity). Ponte-Croft Language Modelling <ref type="bibr" coords="3,233.56,144.39,13.42,8.74">[7]</ref> This method infers a nonparametric language model for each document and then ranks each document based on the query. That is, a document is ranked for a query using the product of the probability of producing query terms in the document and the probability of not producing other terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modelling with Dirichlet Priors[8]</head><p>A language model, in this method, is treated as a multinomial distribution and the Dirichlet distribution is used to smooth the probability of word occurrence. Cover-Density Ranking <ref type="bibr" coords="3,191.45,252.78,14.07,8.74" target="#b3">[4]</ref> Given a set of query terms Q1, ..., Qn, this method builds a boolean AND for all subsets (e.g. Q1^Q2^Q9^Qn) and ranks these subsets by the sum of their terms' Inverse Document Frequency values. Then all documents are ranked based upon the largest subset they contain. Divergence from Randomness <ref type="bibr" coords="3,218.00,312.96,15.00,8.74" target="#b0">[1]</ref> This method uses inverse term frequency, the ratio of two Bernoulli processes, and the hypothesis that the term frequency density is inversely related to the length to create a ranking formula for documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.8">Run 8 -Conjunctive Query and Stopword Removal</head><p>In an effort to boost the effectiveness of runs 5 and 6, we removed all stopwords before taking the pairwise conjunction as this would remove silly conjunctions, e.g. '(at^the)'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.9">Run 9 and Run 10 -Query Expansion using Okapi-style feedback only</head><p>These two runs redo runs 3 and 7, respectively, but remove the KLD-based queries, meaning that only 24 sets of results are used in the computation of the final ranking. The purpose of this run is to see how much effect was produced by using KLD queries, e.g. to determine how much affect the 18 additional queries had due to RRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation</head><p>Tweets were judged to be relevant by NIST assessors after the results from all participating teams were pooled. Assessors were asked to judge tweets as relevant and highly relevant with regards to the topic. In addition, only tweets that primarily contained the English language were allowed to be judged relevant.</p><p>According to the assessment of returned tweets, 33 topics were found to have highly relevant tweets and 49 topics were found to have relevant tweets. We did not augment the set of relevant tweets with any new tweets found by our unofficial runs. Runs were scored based upon precision at 30 for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>Tables <ref type="table" coords="3,351.31,491.13,4.98,8.74">4</ref> and<ref type="table" coords="3,381.70,491.13,4.98,8.74">5</ref> provide summary results for topics with highly relevant tweets only and topics with relevant tweets 3 for posterity. In addition, our results did not differ regardless of which set of qrels were used. From these summary results, our baseline run appears to have performed most of the time at or above median, which may indicate that it is a good baseline. Our second run performed not as well as hoped, however, we feel that this was due to the relative paucity of highly relevant tweets, which amounts to about a third of the tweets judged to be relevant. The improvement in run 3 was expected due to the nature of query expansion. We found that while recency blending did not extremely improve results over the baseline it did tend to improve the results and would seem to indicate that relevant tweets tend to occur closer to the query time as we predicted. We first consider whether any of the official runs improves substantially over the baseline. Run 3 appears to yield a significant improvement (p = 0.016) when only highly relevant tweets are scored, and also (p = 0.00004) when all relevant tweets are scored. Since three hypotheses were tested at once, we apply Bonferroni correction, and find that the both improvements are still significant (p = 0.048 and p = 0.00012, respectively).</p><p>Of the unofficial runs, Run 7 and Run 10 improve significantly on the baseline, after Bonferroni correction, both when highly relevant tweets and all tweets are considered. Run 9 appears also to improve on baseline; however, it is arguable whether the improvement when highly relevant tweets are scored is significant, after correcting for multiple hypothesis testing. According to a strict application of Bonferroni (which is known to be conservative) it is not.</p><p>The results would seem to indicate that both run 5 and 6 did not significantly improve upon the baseline. This unfortunately extends to run 8, which was the combination of methodologies for runs 5 and 6. Run 8 appears to have marginally improved upon run 6 with respect to all relevant tweets but has decreased performance when compared to run 5. Similarly, while run 8 appears to have returned more highly relevant tweets than either of its component runs it would appear that it did so at the expense of returning tweets at or above the median.</p><p>Our results would seem to indicate that the language model used for query expansion does matter. Namely, our tweet based language model for query expansion still does quite a bit better than our baseline and still appears to give some improvement over the initial query expansion run. This would indicate that a better tweet-based language model, e.g. consisting only of English tweets or larger size, may further improve retrieval results.</p><p>We noticed a strange phenomena when examining our summary tables. It appears that for all non-query expansion runs the percentage of topics that have relevant tweets returned at or above median is higher for highly relevant tweets than all relevant tweets. But this appears to be the opposite for all query expansion runs. Unfortunately, we have no good explanation as to why this is happening.</p><p>Furthermore, runs 9 and 10 would seem to indicate that the additional results provided by the KLDbased pseudo-relevance feedback did not seem to significantly affect the results. There is a minor drop in the average number of relevant results returned @ 30 but not nearly dramatic enough to indicate that the extra results significantly boosted the results. What is interesting is that run 9 has a higher percentage of topics with relevant tweets returned at or above the media than does run 3 but this appears to be the opposite for runs 7 and 10. We were unable to determine a reasonable explanation for this fact.</p><p>In an examination of the relevant tweets we came to a discomforting realization; that 12 of the relevant tweets were marked as HTTP status 403/404. As we had collected the data set fairly soon after it was released we had hoped that this would not happen. While it is likely that missing these 12 tweets did not terribly affect our performance, it is possible, however, that teams who collected the data set after we did would be penalized for doing so. Thus, a true comparison of performance may require that any HTTP status 403 or 404 tweets from any group be removed from the relevant set of tweets. But this could substantially affect the number of relevant tweets, which at this time seems rather low. In any future iterations of this track, it would be desirable if a stable data set could be used or enforced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Work</head><p>From the performance of run 7, we would like to investigate the utility of a better tweet-based language model. For example, we would like to use a tweetbased language model composed of English tweets only as we believe this will correspond better to the provided topics. In addition, we would like to use a dynamic language model, by which we mean a language model that scales to each topic's most recent tweet as opposed to our current method of using the oldest topic's most recent tweet as our cut-off. Finally, we would also like to investigate an idealized run wherein we use the whole corpus (or some other sizable collection of tweets) as the language model. Due to the increasing usage of hashtags as part of the Twitter experience, we initially thought to use hashtags as a query method. However, initial testing on the example topics showed an inconsistent pattern of usage, e.g. some topics had very high usage and some had little to no usage. Thus, we ruled out doing this automatically on the official topics. In examining the set of relevant tweets, we saw a similar pattern where some topics had decent hashtag usage and others did not. While this still prohibits an automatic search system, we will in the future conduct a manual run using hand selected hashtag(s) from those topics with adequate hashtag usage.</p><p>We had initially planned to crawl the URLs contained in tweets and use the first 10,000 bytes as an additional source of information, however, we were unable to accomplish this. As around 82% of the relevant tweets contain at least one URL, we feel that such an endeavour may have a non-trivial effect on the retrieval of tweets. Accordingly, if the track continues next year we will endeavour to use the linked content as an additional source of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This year the University of Waterloo created a baseline for our own measurement. While not all of our modifications to this baseline proved to be beneficial we did see some improvement over the baseline using various techniques. The best performing of these techniques appears to be query expansion, especially when a tweet-based language model is used. In addition, we performed well on a majority of topics and even achieved the highest number relevant documents returned @ 30 on a few topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,324.62,234.11,217.60,79.82"><head>Table 1 :</head><label>1</label><figDesc>Corpus distribution</figDesc><table coords="1,324.62,255.78,217.60,58.15"><row><cell cols="3">HTTP Status Num. of Tweets Num. null text</cell></row><row><cell>200</cell><cell>14,313,888</cell><cell>14</cell></row><row><cell>302</cell><cell>1,137,183</cell><cell>25,571</cell></row><row><cell>403</cell><cell>138,560</cell><cell>138,560</cell></row><row><cell>404</cell><cell>552,181</cell><cell>552,181</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,83.13,72.11,187.40,271.89"><head>Table 2 :</head><label>2</label><figDesc>Description of engineered features</figDesc><table coords="2,87.81,93.78,178.04,250.23"><row><cell cols="2">Index Description</cell></row><row><cell>1</cell><cell>This index includes stemmed and</cell></row><row><cell></cell><cell>unstemmed versions of words</cell></row><row><cell></cell><cell>present in the tweet. The stem-</cell></row><row><cell></cell><cell>ming is accomplished using a</cell></row><row><cell></cell><cell>Porter stemmer. In addition,</cell></row><row><cell></cell><cell>word bigrams are included in the</cell></row><row><cell></cell><cell>index.</cell></row><row><cell>2</cell><cell>Identical to Index 1 but with no</cell></row><row><cell></cell><cell>word bigrams present.</cell></row><row><cell>3</cell><cell>This index contains only un-</cell></row><row><cell></cell><cell>stemmed versions of words and</cell></row><row><cell></cell><cell>word bigrams.</cell></row><row><cell>4</cell><cell>Idetical to Index 3 but contains</cell></row><row><cell></cell><cell>no word bigrams.</cell></row><row><cell>5</cell><cell>Instead of words as before, char-</cell></row><row><cell></cell><cell>acter 3-grams are used in the in-</cell></row><row><cell></cell><cell>dex. Subsequently, stemming is</cell></row><row><cell></cell><cell>not used as it has no benefit.</cell></row><row><cell>6</cell><cell>Identical to Index 5 but uses</cell></row><row><cell></cell><cell>character 4-grams.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,333.87,690.72,214.34,6.99;3,318.63,700.18,178.59,6.99"><p>We note that in the TREC notebook version, the presented results for Run 7 for highly relevant tweets were</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information Processing and Management, pages 779-840, 2000.</p><p>[7] Jay M. Ponte and W. Bruce Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 275-281, 1998.</p><p>[8] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 334-342, 2001. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,77.53,344.62,214.09,8.74;5,77.54,356.58,214.09,8.74;5,77.54,368.53,214.09,8.74;5,77.54,380.49,214.09,8.74;5,77.54,392.44,22.69,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,99.37,356.58,192.25,8.74;5,77.54,368.53,214.09,8.74;5,77.54,380.49,16.47,8.74">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,101.43,380.49,95.16,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002-10">October 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.53,413.07,214.09,8.74;5,77.54,425.02,214.09,8.74;5,77.54,436.98,214.09,8.74;5,77.54,448.93,214.09,8.74;5,77.54,460.89,99.68,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,239.55,413.07,52.08,8.74;5,77.54,425.02,214.09,8.74;5,77.54,436.98,66.12,8.74">Questioning query expansion: an examination of behaviour and parameters</title>
		<author>
			<persName coords=""><forename type="first">Bodo</forename><surname>Billerbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,166.56,436.98,125.06,8.74;5,77.54,448.93,126.94,8.74;5,269.35,448.93,22.28,8.74;5,77.54,460.89,11.62,8.74">Proceedings of the 15th Australasian database conference</title>
		<meeting>the 15th Australasian database conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
	<note>ADC &apos;04</note>
</biblStruct>

<biblStruct coords="5,77.53,481.52,214.09,8.74;5,77.54,493.47,214.09,8.74;5,77.54,505.43,214.09,8.74;5,77.54,517.38,177.32,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,184.09,493.47,107.54,8.74;5,77.54,505.43,177.24,8.74">An information-theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renato</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,268.41,505.43,23.21,8.74;5,77.54,517.38,70.26,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001-01">January 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.53,538.01,214.09,8.74;5,77.54,549.97,214.09,8.74;5,77.54,561.92,120.52,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,179.87,549.97,111.76,8.74;5,77.54,561.92,90.38,8.74">Relevance ranking for one to three term queries</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tudhope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.53,582.55,214.09,8.74;5,77.54,594.51,214.09,8.74;5,77.54,606.46,214.09,8.74;5,77.54,618.42,214.09,8.74;5,77.54,630.37,214.09,8.74;5,77.54,642.33,214.09,8.74;5,77.54,654.28,63.65,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,166.45,594.51,125.17,8.74;5,77.54,606.46,214.09,8.74;5,77.54,618.42,34.68,8.74">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,132.17,618.42,159.46,8.74;5,77.54,630.37,214.09,8.74;5,77.54,642.33,181.80,8.74">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;09</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.53,674.91,214.09,8.74;5,77.54,686.86,214.09,8.74;5,77.54,698.82,214.09,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,77.54,686.86,214.09,8.74;5,77.54,698.82,190.12,8.74">A probabilistic model of information retrieval: development and comparative experiments</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
