<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.84,128.52,311.37,18.91;1,140.28,155.40,314.39,18.91">The University Carlos III of Madrid at TREC 2011 Crowdsourcing Track</title>
				<funder>
					<orgName type="full">Amazon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.72,189.52,64.40,10.38"><forename type="first">Julián</forename><surname>Urbano</surname></persName>
							<email>jurbano@inf.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,145.42,189.52,77.67,10.38"><forename type="first">Mónica</forename><surname>Marrero</surname></persName>
							<email>mmarrero@inf.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.23,189.52,61.89,10.38"><forename type="first">Diego</forename><surname>Martín</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.06,189.52,62.95,10.38"><forename type="first">Jorge</forename><surname>Morato</surname></persName>
							<email>jmorato@inf.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.36,189.52,66.12,10.38"><forename type="first">Karina</forename><surname>Robles</surname></persName>
							<email>krobles@inf.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,462.34,189.52,60.23,10.38"><forename type="first">Juan</forename><surname>Lloréns</surname></persName>
							<email>llorens@inf.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University Carlos III of Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.84,128.52,311.37,18.91;1,140.28,155.40,314.39,18.91">The University Carlos III of Madrid at TREC 2011 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7854CEB024066C98A9C38DD9AC3DFFA9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the uc3m team in both tasks of the TREC 2011 Crowdsourcing Track. For the first task we submitted three runs that used Amazon Mechanical Turk: one where workers made relevance judgments based on a 3-point scale, and two similar runs where workers provided an explicit ranking of documents. All three runs implemented a quality control mechanism at the task level based on a simple reading comprehension test. For the second task we also submitted three runs: one with a stepwise execution of the GetAnotherLabel algorithm and two others with a rule-based and a SVMbased model. According to the NIST gold labels, our runs performed very well in both tasks, ranking at the top for most measures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC 2011 Crowdsourcing Track was designed to investigate how to better use crowdsourcing platforms to evaluate Information Retrieval systems. The Track was divided in two tasks: obtaining topical relevance judgments from individual workers and computing consensus judgments from several workers. The Knowledge Reuse research group at the University Carlos III of Madrid put together a team of six people to participate in both tasks. We submitted three runs for each task, although most of our work was devoted to the first one. We focused on designing a practical and effective task template for gathering unconventional relevance judgments through Amazon Mechanical Turk, while studying quality control mechanisms at the task level for such heterogeneous documents as arbitrary HTML pages from the Web. In the second task, two of our runs used a rule-based and an SVM Machine Learning model, while the other one followed a stepwise execution of the GetAnotherLabel algorithm by <ref type="bibr" coords="1,188.39,489.37,87.61,9.60" target="#b10">Ipeirotis et al. [2010]</ref>.</p><p>The rest of the paper is organized as follows. Section 2 describes our submissions for the first task, detailing the HIT design, document preprocessing and quality control mechanism. Section 3 describes our submissions for the second task, and Section 4 summarizes the results in both tasks. Section 5 concludes with final remarks and lines for further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task I: Crowdsourcing Individual Judgments</head><p>For the first task we submitted three runs (see Table <ref type="table" coords="1,284.16,595.09,3.80,9.60">1</ref>), all of which used Amazon Mechanical Turk (AMT) as the crowdsourcing platform. The task was implemented with external HITs, that is, we hosted the templates and data in our own server, communicating with AMT via the API. This allowed us to have more control over the whole process, besides the possibility of gathering some additional data such as knowing when and for how long workers previewed our HITs or where they came from.  <ref type="foot" coords="1,181.56,723.32,3.32,5.78" target="#foot_0">1</ref>$87 ($8.7) $87 ($8.7) $87 ($8.7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Table <ref type="table" coords="1,224.63,740.35,3.68,8.25">1</ref>. Summary of the runs submitted for Task I. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HIT Design 2</head><p>We worked on the HIT template to make it as simple, functional and self-contained as possible. Given that all five documents in a set had to be judged by the same worker, we decided to include them all in a single HIT to make the assignment process easier and allow workers to see all five documents at once to make the ranking judgments easier. Thus, we had a total of 435 HITs for a total of 2175 judgments. We paid $0.20 plus fees per HIT, which makes it $0.04 per document. It was in our plans to try paying a little less, but given the tight deadlines we decided to stick with the $0.04 wage for the sake of time <ref type="bibr" coords="2,364.56,383.17,83.60,9.60" target="#b15">[Mason et al., 2009]</ref>: workers seemed to get interest in our task, and in previous trials we asked them if they thought the payment was fair, and they mostly thought so.</p><p>All three runs shared the same template except for the HIT description, instructions and the part where the actual relevance questions were asked (see Figure <ref type="figure" coords="2,279.48,435.97,3.80,9.60" target="#fig_0">1</ref>). The top left box contained the task instructions. For the uc3m.graded and uc3m.slider runs the focus of the task was on the quality control questions (see <ref type="bibr" coords="2,477.85,447.73,56.94,9.60">Section 2.4.3)</ref>, making the relevance question secondary (see <ref type="bibr" coords="2,259.93,459.97,49.34,9.60">Section 2.3)</ref>. For the uc3m.hterms run, we made it the other way around. We informed that the most accurate workers would get a bonus, while the least accurate would receive a 15% penalization <ref type="bibr" coords="2,135.24,483.85,78.43,9.60" target="#b19">[Shaw et al., 2011]</ref>. However, due to mere programming difficulties, we ended up paying bad workers too to avoid unnecessary conflicts for the time being. Some fragments of the instructions were rendered as hyperlinks (see Figure <ref type="figure" coords="2,173.89,507.37,3.84,9.60" target="#fig_0">1</ref>), and when clicked upon they triggered a highlighting effect on the part of the template they referred to, so that workers could immediately follow up while reading. For instance, when clicking on "what we were searching for" the box with the topic description would be highlighted blinking thrice.</p><p>The top right box contained the topic description, with the title in a bigger bold face. Underneath, two lines briefly described what good and bad results were considered for the topic. These descriptions were not directly taken as in the files distributed for the track; we made a simpler version, explaining concepts when appropriate.</p><p>The middle right area of the template contained five stacked boxes where workers had to give their answers, one box per document. First, we asked for the answer to the quality control question (see Section 2.4), which consisted in selecting one correct answer out of two options. Next, we asked the actual relevance question for the document selected. Unlike the quality control question, the relevance question was different in all three runs (see <ref type="bibr" coords="2,76.81,636.61,48.65,9.60">Section 2.3)</ref>.</p><p>The five documents in the set were displayed below the instructions and topic description using a tabbed design where workers could display one document or another by clicking on the corresponding tab header. This way, there is no need to follow external links or do long scrolls throughout the HIT to go from document to document. The tab and answer boxes corresponding to the document currently being displayed were rendered with a lighter color, while the others' where darker and with all options disabled, so that workers could easily know which of the five answer boxes belonged to the document and make no mistake. Tab headers were located in the upper right corner, while the upper left corner offered two different options to display documents:</p><p>With images. This mode kept CSS styles, layout, images, etc. However, we removed all scripts, embedded objects and all HTML elements not related with the page rendering (see Section 2.2). Therefore, workers saw the documents nearly as they would if surfing the Web themselves.</p><p>Just text. This mode rendered documents as in the images mode, but removing all images, colors, custom formatting and all other elements not related with the page layout. This results in a simple black and white view, while still maintaining the headers and layout scheme of the original page.</p><p>In early trials we asked workers which of the two display modes they preferred. Most of the times they chose the mode with images, so we made it the default one. However, some workers always selected the just text mode, so we decided to keep it anyway and let workers decide. The bottom of the HIT contained a simple box with a textbox to provide optional feedback (they rarely did), and in the bottom right corner we placed the submission button. When clicking this button, a script checked that workers answered all five relevance questions and all five quality control questions. If not, they were informed with a message displayed in red font right before the submission button, keeping the browser in the same page. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Preprocessing</head><p>Documents displayed to workers were the result of a previous cleanup process to ensure smooth loading and safe rendering, embedding all contents within a single HTML file so no additional requests were made to any server. Documents displayed in the mode with images were processed as follows:</p><p>documents to judge for that topic, and for every word with a matching stem we set the background color to yellow and font color to black, wrapping the word with an HTML span tag (see <ref type="bibr" coords="4,391.20,70.45,37.57,9.60">Figure 2,</ref><ref type="bibr" coords="4,431.04,70.45,32.34,9.60">bottom)</ref>.</p><p>All these document versions were stored in our server and sent on demand whenever a worker clicked on a document tab header. Runs uc3m.graded and uc3m.slider allowed workers to see documents with and without images, and run uc3m.hterms used those same two versions but with the topic key terms highlighted. Many workers preferred to use the just text version: 7 (24%) in uc3m.graded, 21 (24%) in uc3m.slider, and 12 (36%) in uc3m.hterms. Apparently, a larger proportion in the latter preferred this version, because the highlighted terms can be more easily glimpsed with a black and white document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Question</head><p>We explored two lines of work for obtaining the actual relevance judgments (see Figure <ref type="figure" coords="4,443.16,193.57,3.80,9.60" target="#fig_2">3</ref>). In uc3m.graded we tried to optimize for the binary relevance label, while for runs uc3m.slider and uc3m.htrems we tried to optimize for the ranking labels. Unanswered Answered uc3m.graded uc3m.slider uc3m.hterms </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">uc3m.graded</head><p>For the first run we used a traditional 3-point graded relevance scale where documents could be judged as bad, fair or good for the search topic (see <ref type="bibr" coords="4,221.78,347.05,38.53,9.60">Figure 3,</ref><ref type="bibr" coords="4,263.66,347.05,16.50,9.60">left)</ref>. Even though the Track asks for binary labels, using three levels provides more information to adjust for a probability-based binary label as well as for the ranking labels.</p><p>For the probabilistic binary labels we set the bad documents as 0, the good documents as 1, and then studied different probabilities for the fair documents. We tried values from 0 to 1, with increments of 0.1, and the results with the known judgments supported the choice of 1 for the fair documents too (see <ref type="bibr" coords="4,417.60,399.85,48.65,9.60">Section 4.1)</ref>. For the ranking labels, we ordered documents by their relevance level, as reported by workers. We broke ties ordering again by the number of failures in quality control (see Section 2.4) and then by the time spent on each document (descending). We thus assumed that, given two documents with the same relevance judgment, the one for which workers failed the quality control question or spent little time is less likely to be relevant. We tried other attributes and orderings, but they were marginally worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">uc3m.slider</head><p>For the second run we investigated how to have workers provide an explicit ranking of the five documents in the set without much trouble. Preference judgments would make more sense for a relatively large number of documents, but for just five documents we did not consider it a viable option because of the additional cost. We also thought about using a 5-point relevance scale without ties, but such an option would not be very practical for little over just five documents, and still we would not be able to obtain a clear boundary between the relevant and not relevant documents for the binary scale.</p><p>Instead, we gave workers a slider widget they could use to give each document a certain amount of relevance (see Figure <ref type="figure" coords="4,108.13,585.73,3.78,9.60" target="#fig_2">3</ref>, middle), from 0 to 1 with increments of 0.01, though workers could not see the actual value set. This would give us a complete ranking of the documents and something like a relevance distance between them: documents similarly relevant to the topic would have their slider handles close together, and documents too different in terms of relevance would have them far apart. Intuitively, workers would move the slider handle to the right for the relevant documents and to the left for nonrelevant ones, even form little clusters with the slider handles for documents of similar relevance. Using a simple clustering algorithm we could infer the binary relevance label out of these clusters.</p><p>In early trials the slider handle was shown in the middle from the beginning, and workers had to move it in either direction, even if they wanted it to be exactly in the middle. This had a clear advantage for binary labels: if the slider handle were moved to the right side, even if just a little, we could assign a binary label of 1 because the right end of the slider was labeled as good. Similarly, if moved to the left side we could assign a label of 0. The downside was that some workers moved the slider handle just 5-10 pixels, being hardly informative for the ranking labels. Most probably, these were careless workers trying to make easy money, but they could as well be honest workers that just wanted to indicate mid-relevant documents. In contrast, some others tended to place the slider handles at the end points (see Figure <ref type="figure" coords="4,268.33,755.89,3.78,9.60" target="#fig_3">4</ref>, left), providing little information for the ranking labels too. Probably they just did not understand the slider widget and just clicked on the end labels. We finally decided to hide the slider handle at the beginning, as an indication that no value was set (see <ref type="bibr" coords="5,418.78,58.69,38.17,9.60">Figure 3,</ref><ref type="bibr" coords="5,459.94,58.69,16.13,9.60">top)</ref>, avoiding any initial bias too. Workers had to click on the slider bar to set an initial value.</p><p>For the ranking label we just used the slider positions provided by workers, and broke ties by the number of failures in quality control and then by the time spent on each document (descending), just like in the uc3m.graded run. For the binary label we studied several options: Range-Normalized. The probabilistic binary label is just the slider value from 0 to 1.</p><p>Range-Normalized with Threshold. All documents with a slider value larger than or equal to a threshold t would be assigned a binary label of 1 and 0 otherwise. We tried threshold values from 0 to 1, with increments of 0.1.</p><p>Set-Normalized. The slider value is normalized between the minimum and maximum judgments in the set of five documents. This option could correct for worker bias, although it might suffer learning effects and it assumes there is a highly relevant document and a highly nonrelevant document in the set of five.</p><p>Set-Normalized with Threshold. The Set-Normalized value is computed, and if it is larger than or equal to a threshold t we assign a binary label of 1 and 0 otherwise. We tried values for t from 0 to 1 with increments of 0.1.</p><p>Worker-Normalized. We compute the range of values given by the worker for all documents (i.e. her global minimum and maximum), and then normalize the slider values within that range. This option could correct for possible biases per worker, although it makes the similarly unjustified assumption that all workers judge at least one highly relevant document and one highly nonrelevant. In addition, it could clearly suffer from learning effects.</p><p>Worker-Normalized with Threshold. The Worker-Normalized value is computed, and if it is larger than or equal to a threshold t we assign a binary label of 1 and 0 otherwise. Similarly, we tried values for t from 0 to 1 with increments of 0.1.</p><p>Cluster. Given the slider values for all five documents in the set, we ran a k-means clustering algorithm for 2 clusters: documents in the left cluster are assigned a binary label of 0, and those in the right side are assigned a label of 1. This again assumes there are both relevant and nonrelevant documents in the set.</p><p>These options were evaluated with the known labels of the test set provided for the Track, and we chose the Set-Normalized labels with Threshold t=0.4 (see Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">uc3m.hterms</head><p>For the third run we followed the same line as in the second run, but we decided to include another label in the slider widget marking the middle point as fair (see Figure <ref type="figure" coords="5,316.45,480.61,3.78,9.60" target="#fig_2">3</ref>, right). Our hope was that workers would better understand that they had a wide range of possible values to set, not just the bad and good extremes. However, they seemed to just set more values around the middle label as well (see Figure <ref type="figure" coords="5,396.22,504.13,3.78,9.60" target="#fig_3">4</ref>, right). It can be seen how they tended to click in the end points and, in the second case, also towards the center.</p><p>We tried the same methods to generate the binary labels, and the Set-Normalized method with Threshold t=0.4 yielded again the best results (see Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Quality Control</head><p>There is no warranty that the answers provided by workers are correct (to the extent there is an objective and correct answer at all in this task <ref type="bibr" coords="5,213.21,744.73,73.09,9.60" target="#b23">[Voorhees, 2000]</ref>). We thus decided to implement some quality control mechanisms, which work at different levels: Worker level, where only workers that meet certain demographic criteria are allowed to work in the task.</p><p>In the case of Amazon Mechanical Turk, they can be filtered by their percent of work accepted, geographic location, etc.</p><p>Task level, where the task template is modified to incorporate some additional information about the individual worker response that could indicate misbehavior. This information can be implicit, such as the time needed to complete the task <ref type="bibr" coords="6,235.55,123.37,70.16,9.60" target="#b24">[Zhu et al., 2010</ref><ref type="bibr" coords="6,305.70,123.37,96.17,9.60" target="#b22">][Urbano et al., 2010]</ref> or behavioral patterns such as mouse clicks, scrolling and key strokes <ref type="bibr" coords="6,265.19,135.13,114.20,9.60" target="#b17">[Rzeszotarski et al., 2011]</ref>. It can also be gathered explicitly, actually asking additional secondary questions with quantitative, unbiased and automatically computable answers <ref type="bibr" coords="6,124.92,158.53,84.40,9.60" target="#b12">[Kittur et al., 2008]</ref>. If giving correct answers to these questions required as much effort as answering the main question, we could assume that if the secondary answer is correct then we can trust the answer to the main question.</p><p>Process level, where the task assignment process is modified so that workers receive examples for which the correct answer to the main question is known beforehand (i.e. trap questions), allowing us to assess how well the worker responses fit the known answers <ref type="bibr" coords="6,333.72,223.21,105.97,9.60" target="#b18">[Sanderson et al., 2010]</ref> and follow a learning process accordingly <ref type="bibr" coords="6,172.09,234.85,63.32,9.60" target="#b13">[Le et al., 2010]</ref>.</p><p>The subset of known labels provided in the test set suggests the use of trap questions as the quality control mechanism at the Process level. However, we decided not to pursue this line for several reasons. First, these known labels would need to be generated in real evaluation settings, with the problem of choosing what topics (if not all of them) and what documents to judge, increasing the workload in a real evaluation experiment via crowdsourcing. Second, the labels would not necessarily be balanced across relevance levels like in the data used in the Track, so that many judgments could be needed beforehand to get a meaningful gold set to use as trap questions. Third, using these questions involve an overhead cost, whether adding just one document per set or having workers judge all five documents in a known set. Fourth, the assignment process is more complex, and most importantly, trap questions may ensure that workers understood and paid attention to those particular topics and documents, but not necessarily to all others.</p><p>Instead, we decided to investigate quality control mechanisms at the Task level. As an implicit measure we used the work time per document, taking into account both the HIT previewing and working times; and as an explicit measure we used a very simple reading comprehension test based on a selection of the keywords that best describe the documents. We also used the Worker level control provided by AMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Worker Level</head><p>We used some of the worker filters provided by Amazon Mechanical Turk. At first, we considered submitting different runs filtering by country, but given the tight deadlines we discarded this option in favor of having more workers. We did require all them to have a minimum of 100 total approved HITs, and a minimum percentage of approved HITs of 95% for the uc3m.graded and uc3m.slider runs, 98% for the uc3m.hterms run. Based on the demographics of AMT <ref type="bibr" coords="6,153.25,497.53,65.35,9.60" target="#b10">[Ipeirotis, 2010</ref><ref type="bibr" coords="6,218.60,497.53,87.00,9.60" target="#b16">][Mason et al., 2011]</ref>, we implemented these restrictions to avoid sporadic workers. In addition, we limited workers to contribute a maximum of 50 sets (250 judgments) to avoid superworkers. For the uc3m.hterms run we also requested workers to have Amazon's Categorization Master Qualification, but these workers were not attracted at all by our task, so we immediately removed this filter and started over. However, there is no easy way of knowing if any of the workers in the three runs does have this qualification anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Implicit Task Level: Work Time</head><p>Our HIT templates measured how much time workers spent on each document, rather than on all five altogether, summing the time spent with both display modes. As an implicit quality measure we counted the number of time failures, that is, the number of documents in the set for which workers spent less than 4.5 seconds. Previous trials with real AMT workers and colleagues indicated 4.5 seconds as a suitable threshold, even for documents that can be quickly identifiable as spam.</p><p>We did not use the work time in seconds reported by Amazon. In our experience, many workers like to preview HITs, and spend some time getting used to the particular contents of each one. Some workers solve HITs while previewing, and the time they spend after accepting the HIT is used just to select their answers and submit. As such, using only the work time can be very misleading, and some workers get very offended if their work is rejected on the basis of work time alone <ref type="bibr" coords="6,234.95,707.29,87.32,9.60" target="#b22">[Urbano et al., 2010]</ref>. Our HITs were implemented as external, that is, they were hosted in our own server. Thus, we knew when workers previewed HITs. We included a script in the HIT template to report back to our server, every 10 seconds, how much time was spent on each document so far. When computing the number of time failures, we added this preview time to the work time to have a better estimate of actual time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Explicit Task Level: Reading Comprehension</head><p>Our first approach for explicit quality measures was to modify documents by inserting a paragraph with syntactically correct yet nonsensical sentences, asking workers to detect where they are. However, some documents can be very easily identifiable as relevant or nonrelevant, so asking workers to spot the nonsensical paragraph would just be a waste of time and it would deteriorate worker performance (for example in a large Wikipedia article). In addition, automatically finding a suitable location to embed these paragraphs is certainly not an easy task. <ref type="bibr" coords="7,136.30,136.81,84.59,9.60" target="#b12">Kittur et al. [2008]</ref> asked for the number of references, sections and images in Wikipedia articles, but in our case documents are very heterogeneous HTML documents that seldom have a common structure, so we discarded these questions as well.</p><p>Our choice was to implement a very simple reading comprehension test where workers had to tell which one of two sets of keywords better described the document (call this the positive keyword set). We designed a previous experiment where subjects were given documents and they had to provide a list with 5 to 10 keywords that they thought best described the document. We ran four trials with real AMT workers: one with no qualification required, with more than 95% approval, with more than 1,000 total HITs approved and 95% approval, and another one with the Categorization Master Qualification required. Another trial was run with four colleagues with varying expertise in Information Retrieval. We found that virtually all subjects in all five trials provided the most and/or the second most frequent terms in the document (excluding stopwords and considering two terms as equal if they had the same stem), and those who did not were clearly spammers.</p><p>Given that all subjects recognized the most frequent terms, we included in the positive keyword set the 3 most frequent terms plus 2 random terms from the next 5 most frequent ones; and for the negative keyword set we randomly picked 5 terms out of the 25 least frequent ones (see Figure <ref type="figure" coords="7,387.73,312.97,3.78,9.60" target="#fig_0">1</ref>, right). In addition, terms within keyword sets were shuffled. We decided to pick terms randomly to some degree because many document sets had duplicate documents or documents for which the most frequent terms were the same, which would have made the task very easy. As an explicit quality measure we counted the number of keyword failures, that is, the number of documents in the set for which workers did not select the positive keyword set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Rejecting HITs and Blocking Workers</head><p>We used the number of time and keyword failures to decide whether to accept all five judgments in the HIT, and in case of rejecting it whether to block the worker or not. If a HIT was to be rejected, we extended it so that another worker could answer it, and when blocking a worker we did not reject all his previous work to be sure we met the deadline. Moreover, some documents might not have clearly important terms, so simply failing the keywords question would not be that rare. Table <ref type="table" coords="7,157.67,546.55,3.68,8.25">2</ref>. Maximum number of keyword and time failures allowed per HIT and worker.</p><p>As Table <ref type="table" coords="7,111.59,563.05,5.51,9.60">2</ref> shows, the uc3m.graded run was the most permissive of all three, and uc3m.slider was the most restrictive. For instance, in the first case we rejected a HIT if there were more than 1 keyword failures or more than 2 time failures, and if a worker accumulated more than 1 HIT rejected because of keyword failures or more than 2 HITs rejected because of time failures, we would block him. One of the consequences was that the uc3m.slider run took much more time to complete, as much more work was being rejected. In fact, workers seemed to keep failing in 7 of the HITs from the uc3m.slider run, so we decided to stop extending those HITs when three workers failed in them. The labels for those HITs were computed by simply averaging the labels of their three workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">uc3m.wordnet</head><p>For this submission we used the GetAnotherLabel algorithm<ref type="foot" coords="8,317.88,79.53,3.59,6.25" target="#foot_4">3</ref> (GAL) to manage redundant answers <ref type="bibr" coords="8,487.43,79.45,51.13,9.60;8,56.64,91.09,37.16,9.60" target="#b10">[Ipeirotis et al., 2010]</ref>, based on the expectation maximization algorithm proposed by <ref type="bibr" coords="8,371.17,91.09,103.06,9.60" target="#b9">Dawid and Skene [1979]</ref>. Given a set of examples with previously known answers and the labels given by workers to those and other examples, it can compute a certain probability for each unknown example to have a particular label based on the worker responses. In addition, it computes a confusion matrix and an overall expected quality for each worker. In early trials we found this algorithm to perform exceptionally well, but we did not make a submission just with it alone because it is not our work.</p><p>Instead, we decided to explore stepwise executions of the algorithm using various features. Let us assume a feature that can be used to categorize the topics. Our idea is that workers could be more reliable in documents from one category than another, so we ran the algorithm once per category, using only documents pertaining to it. We then compared the worker expected quality in each category with the overall expected quality using all documents, and if it was smaller we ignored that worker's judgments and executed the algorithm again. That is, we used only the best workers per category, and ran the GAL algorithm with them. The features we tried were: Topic category, as indicated in TREC's topic descriptions: closed or limited, advice, navigational, etc. Subject of the information need: politics, shopping, people, geography, etc.</p><p>Rareness of the topic. We looked up in Wordnet the terms in the topic descriptions, and if a topic had terms that did not appear in any glossary we categorized it as rare. That is, we assumed that some workers might have more difficulty with rare topics.</p><p>The binary labels are directly provided by the algorithm as the probability of having the relevant label, and the ranking labels can be assigned just by ordering documents according to these probabilistic binary labels. We tried the three features, and the topic rareness was marginally better than the others, so that is the one we used for our submission. However, when computing the labels based on the answers of the good workers, we only used their labels for that topic category, rather than their labels for all topic categories. Therefore, we lost much information for computing the consensus labels, which surely affected the quality of the results (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">uc3m.rule</head><p>For the second run we trained a rule-based model using several features regarding worker quality, with each topic-document as a different example. Besides other features, we used the output of the GAL algorithm using all topic-document pairs; in particular, the confusion matrix for each worker. These matrixes are defined with several variables of the form π ij w , that is, the probability that worker w assigned the label j to an example whose actual label is i. The complete set of features is as follows:</p><p>Relevant to nonrelevant ratio of the labels received for the topic-document. Using only this feature would be like using majority voting, possibly with bias correction. For all workers that labeled the topic-document, the average correct to incorrect ratio when they assigned a relevant label.</p><p>For all workers that labeled the topic-document, the average correct to incorrect ratio when they assigned a nonrelevant label.</p><p>For all workers that labeled the example, the average posterior probability of it being relevant, that is, π 11 w if they said relevant (they were right) and π 10 w if they said nonrelevant (they were wrong).</p><p>For all workers that labeled the example, the average posterior probability of it being nonrelevant, that is, π 00 w if they said nonrelevant (they were right) and π 01 w if they said relevant (they were wrong).</p><p>The model provided the binary labels out of the box, and the ranking labels were computed by the branch of the decision tree the topic-document fell under: the larger the positive to negative ratio of examples falling under that branch, the higher the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">uc3m.svm</head><p>For the third run we trained a SVM model using the same features as in the uc3m.rule run. The model allowed us to easily rank documents and assign binary labels: positive scores indicated relevant documents (negative scores for nonrelevant), and the larger the score the more relevant the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>A much debated aspect of the Track was how to actually evaluate runs. This year, the Track followed a different methodology than usual, because NIST assessors did not provide new relevance judgments to evaluate runs. The alternative consisted in computing the labels for each topic-document example through the vote of the majority, and using those labels as ground truth. One problem with this method is that the vote of the majority is too simple and it results in low quality labels when compared to other methods <ref type="bibr" coords="9,391.69,128.77,81.79,9.60" target="#b20">[Sheng et al., 2008]</ref>. Also, it is not clear to us whether the best results will be achieved by the actually best runs or by the average ones. In our experience, workers are biased toward the relevant label, probably because of the topic terms appearing in the documents. This can be observed when comparing UWaterlooMDS's run with the consensus labels for recall and specificity 4 [Lease et al., 2011]. As a result, if teams do not correct this bias somehow the vote of the majority will be biased too. Indeed, it is striking how much the numbers change when looking at the results against the vote of the majority and against the known NIST labels. All results we describe below are computed as per the available NIST gold, not the consensus gold. Also, we show our results only for the accepted work, ignoring rejected labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task I</head><p>Figure <ref type="figure" coords="9,87.13,255.61,5.51,9.60" target="#fig_5">5</ref> shows the expected pattern for different probabilistic labels for the fair labels in run uc3m.graded: the larger the value, the higher the recall and the lower the specificity. All measures vary between 0.6 and 0.8, but we decided to maximize recall and accuracy and let all fair judgments have a label of 1 for simplicity.   <ref type="bibr" coords="9,84.96,747.61,103.56,8.68">Worker-Normalized (right)</ref>. The points in the right side of each plot mark the scores without using a threshold. 4 Their run consisted in the judgments of one of the participants himself, which could somehow be considered as a NIST assessor judgments. In the case of the uc3m.slider and uc3m.hterms runs the patterns are more complex (see Figure <ref type="figure" coords="10,506.53,58.69,3.80,9.60" target="#fig_6">6</ref>). The expected result is again clear: the larger the threshold t, the lower the recall and the higher the specificity. As the sliders need to be more and more to the right for the document to be considered relevant (high values of t), the fewer relevant documents we will have altogether, missing actually relevant documents (low recall) and hitting actually nonrelevant ones (high specificity). Indeed, in the Range-Normalized plot (left), for t=0 recall is 1 and specificity is 0, reversed for t=1. The steep decay in precision is caused by a couple of documents with the sliders set at the very end, but that turned out to be nonrelevant.</p><p>In general, without thresholds involved, precision scores are a little over 0.8; and accuracy, recall and specificity move between 0.6 and 0.7. In addition, we can see that using thresholds can indeed improve the results over the base cases. In general, thresholds between 0.5 and 0.6 seem to work best, improving both recall and accuracy at the cost of lowering specificity. Another clear pattern can be observed with the slopes of the curves: the Set-Normalized curves have less variation, ensuring better results around the chosen threshold. We chose the Set-Normalized method with Threshold t=0.4, because it yielded the seemingly best results for uc3m.hterms and very good results for uc3m.slider without approaching steep decays and maximizing more measures. The cluster method, not shown here, resulted in worse results overall.</p><p>Table <ref type="table" coords="10,96.83,247.57,5.51,9.60" target="#tab_2">3</ref> summarizes the results of our runs compared to the best and median per measure. The median scores are computed between the other teams runs (according to the notebook overview paper <ref type="bibr" coords="10,436.07,259.33,79.20,9.60" target="#b14">[Lease et al., 2011]</ref>) and our main run (uc3m.hterms). The top scores are computed over all runs. In addition, we do not include the BUPT-WILDCAT team results because "workers could review the reference answers of the gold set as instructions for further HITs", yielding their exceptionally high overfitted scores (above 0.9) <ref type="bibr" coords="10,396.75,295.45,72.70,9.60" target="#b14">[Lease et al., 2011</ref> As the table shows, our runs performed better than the median in all cases, except for uc3m.slider in recall. In fact, the best overall results were achieved by our uc3m.graded and uc3m.hterms runs. In addition, we can see that the slider widget did indeed result in a better ranking of the documents in runs uc3m.slider and uc3m.hterms. We note that we did not use these known labels to train workers or as trap questions. However, we did use them to tune the threshold values. Nonetheless, the results ignoring the thresholds, and thus the known labels altogether, would have still been above the median for all measures and at the top most of the times. We thus discard overfitting effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task II</head><p>Table <ref type="table" coords="10,83.63,515.17,5.51,9.60" target="#tab_3">4</ref> summarizes the results of our runs compared to the best and median per measure. Again, the median scores are computed between the other teams runs (according to the notebook overview paper) and our main run (uc3m.rule). The top scores are computed over all runs. We can see how the uc3m.wordnet run did indeed perform poorly, probably because of our mistake when using only within-category judgments to compute the labels (see <ref type="bibr" coords="10,345.83,654.85,49.97,9.60">Section 3.1)</ref>. The other two runs performed above the median except for recall and the uc3m.svm run for the ranking measures. In fact, the best overall scores, except for recall, were achieved by our runs. We again discard any overfitting effects because these true labels were not known to participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper described the participation of the University Carlos III of Madrid (uc3m team) in the TREC 2011 Crowdsourcing Track. For the first task we submitted three runs that used Amazon Mechanical Turk: in one case we asked workers for typical 3-point judgments, and in the other two cases we asked for unconventional judgments based on a slider widget, thus focusing on the ranking of documents. We also worked on a simple task design, document rendering, and quality control based on a simple reading comprehension test. In general, our runs ranked at the top for most measures. In any case, all teams performed reasonably well: the median scores are about 0.75 for precision and recall. These numbers agree exceptionally well with Ellen Voorhees finding that NIST assessors agreed with each other to about 0.65 precision at 0.65 recall in the old TREC ad hoc tasks <ref type="bibr" coords="11,56.64,117.37,74.97,9.60" target="#b23">[Voorhees, 2000]</ref>. The results of the Crowdsourcing Track do therefore support the use of evaluation methodologies based on crowdsourcing.</p><p>For the second task we also submitted three runs: two of them were based on trained machine learning models, and the other one consisted in a stepwise execution of an expectation maximization algorithm. Our hypothesis was that bad workers are not necessarily bad for all kinds of topics. For instance, a worker could perform poorly with sports-related topics, but good with politics. However, we could not really study this hypothesis because of a programming bug. Nonetheless, the other two runs ranked at the top of the results for all measures but recall. Again, the average results across teams passed the 0.65 precision at 0.65 recall threshold.</p><p>A clear line for further work is the quality control mechanism at the explicit task level for arbitrary Web documents. Our results with and without rejected work do not differ significantly, so we tend to believe that our reading comprehension test was probably too easy. Two immediate changes are the maximum number of allowed mistakes and the frequency of the terms included in each set: perhaps having the 3 most frequent terms is too obvious, or maybe having five documents together for the same topic makes the test inherently easier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,217.08,280.57,161.08,8.68"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. HIT design for run uc3m.hterms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,94.44,518.05,406.36,8.68;3,73.68,366.36,226.80,129.60"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Document display modes for runs uc3m.graded and uc3m.slider (top) and uc3m.hterms (bottom).</figDesc><graphic coords="3,73.68,366.36,226.80,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,224.16,297.01,146.80,8.68"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Relevance question widgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,84.96,649.09,425.21,8.68;5,84.96,659.89,400.37,8.68"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Distribution of slider values assigned by workers in runs uc3m.slider (left) and uc3m.hterms (right).It can be seen how they tended to click in the end points and, in the second case, also towards the center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,84.96,431.17,425.31,8.68;9,84.96,442.09,131.92,8.68"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Accuracy, Recall, Precision and Specificity of the uc3m.graded run for various probabilistic label assignments to the fair judgments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,84.96,726.25,425.40,8.68;9,84.96,737.05,425.37,8.68;9,84.96,747.61,424.24,8.68"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Accuracy, Recall, Precision and Specificity of the uc3m.slider (top) and uc3m.hterms (bottom) runs for the different methods to compute binary labels: Range-Normalized (left), Set-Normalized (center) and Worker-Normalized (right). The points in the right side of each plot mark the scores without using a threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,122.16,295.45,356.52,95.43"><head>Table 3 .</head><label>3</label><figDesc>]. Results of our three runs for Task I compared to the best and median per measure.</figDesc><table coords="10,136.20,313.33,316.79,62.44"><row><cell></cell><cell>Accuracy</cell><cell>Recall</cell><cell cols="2">Precision Specificity</cell><cell>MAP</cell><cell>NDCG</cell></row><row><cell>Best</cell><cell>.75</cell><cell>.80</cell><cell>.86</cell><cell>.73</cell><cell>-</cell><cell>-</cell></row><row><cell>Median</cell><cell>.62</cell><cell>.74</cell><cell>.77</cell><cell>.54</cell><cell>-</cell><cell>-</cell></row><row><cell>uc3m.graded</cell><cell>.75</cell><cell>.80</cell><cell>.84</cell><cell>.63</cell><cell>.73</cell><cell>.78</cell></row><row><cell>uc3m.slider</cell><cell>.69</cell><cell>.72</cell><cell>.82</cell><cell>.61</cell><cell>.76</cell><cell>.80</cell></row><row><cell>uc3m.hterms</cell><cell>.73</cell><cell>.74</cell><cell>.86</cell><cell>.73</cell><cell>.81</cell><cell>.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,120.72,556.93,353.81,77.56"><head>Table 4 .</head><label>4</label><figDesc>Results of our three runs for Task II compared to the best and median per measure.</figDesc><table coords="10,133.44,556.93,322.19,62.44"><row><cell></cell><cell></cell><cell>Recall</cell><cell cols="2">Precision Specificity</cell><cell>MAP</cell><cell>NDCG</cell></row><row><cell>Best</cell><cell>.71</cell><cell>.91</cell><cell>.70</cell><cell>.68</cell><cell>.17</cell><cell>.42</cell></row><row><cell>Median</cell><cell>.65</cell><cell>.77</cell><cell>.63</cell><cell>.57</cell><cell>.11</cell><cell>.36</cell></row><row><cell>uc3m.rule</cell><cell>0.70</cell><cell>0.75</cell><cell>0.68</cell><cell>0.64</cell><cell>0.17</cell><cell>0.42</cell></row><row><cell>uc3m.svm</cell><cell>0.71</cell><cell>0.75</cell><cell>0.70</cell><cell>0.68</cell><cell>0.08</cell><cell>0.33</cell></row><row><cell>uc3m.wordnet</cell><cell>0.57</cell><cell>0.66</cell><cell>0.56</cell><cell>0.48</cell><cell>0.06</cell><cell>0.30</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,61.20,777.48,214.97,7.75"><p>This is the total cost if rejected work were not paid. See Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="1,277.92,777.48,18.21,7.75"><p>2.4.4.   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="2,61.32,777.60,334.16,7.75"><p>All HIT templates and gathered data can be downloaded at http://www.kr.inf.uc3m.es/trec2011/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="7,67.68,677.89,232.20,12.09;7,56.64,699.37,481.98,9.60;7,56.64,711.01,481.91,9.60;7,56.64,722.77,481.97,9.60;7,56.64,734.53,481.82,9.60;7,56.64,746.17,481.96,9.60;7,56.64,757.93,481.77,9.60;7,56.64,769.69,364.29,9.60"><p>Task II: Aggregating Multiple JudgmentsThe second task of the Track promoted research on quality control at the fourth level: Aggregation. These mechanisms modify the task assignment process so that questions are answered by multiple workers, resulting in redundant answers from which a consensus response can be computed<ref type="bibr" coords="7,379.31,722.77,78.55,9.60" target="#b21">[Snow et al., 2008]</ref>. There are simple mechanisms such as using the vote of the majority[Alonso et al., 2008]  or comparing the distributions of possible answers<ref type="bibr" coords="7,132.48,746.17,86.24,9.60" target="#b22">[Urbano et al., 2010]</ref>; and more complex alternatives such as weighting answers by the worker trustworthiness<ref type="bibr" coords="7,128.77,757.93,69.84,9.60" target="#b13">[Le et al., 2010]</ref> or using statistical models that try to maximize the effectiveness of workers<ref type="bibr" coords="7,56.64,769.69,76.92,9.60" target="#b20">[Sheng et al., 2008</ref><ref type="bibr" coords="7,133.56,769.69,75.73,9.60" target="#b21">][Snow et al., 2008</ref><ref type="bibr" coords="7,209.29,769.69,97.71,9.60" target="#b10">][Ipeirotis et al., 2010]</ref>. We followed the last line.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4" coords="8,61.20,777.48,444.92,7.75"><p>Actually, we implemented a C# port of the original Java software, available at http://code.google.com/p/get-another-label-dotnet/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We acknowledge the support received by <rs type="funder">Amazon</rs> for the accomplishment of these experiments.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,74.58,603.13,455.46,9.60" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="3,84.96,603.13,180.41,9.60">All hyperlinks were modified to point to &quot;#</title>
		<imprint/>
	</monogr>
	<note>so that clicking on them would not trigger any page loading</note>
</biblStruct>

<biblStruct coords="3,74.58,614.89,462.88,9.60" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="3,84.96,614.89,447.50,9.60">All CSS rules in external stylesheets were joined and put together in a single style tag within the document</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,74.58,627.01,247.51,9.60" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="3,84.96,627.01,232.11,9.60">All CSS rules unrelated to style or layout were removed</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,74.58,638.77,367.86,9.60" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,84.96,638.77,270.99,9.60">Unsafe or irrelevant HTML elements, such as scripts and applets</title>
		<imprint/>
	</monogr>
	<note>were removed too</note>
</biblStruct>

<biblStruct coords="3,74.58,650.53,278.84,9.60" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="3,84.96,650.53,263.43,9.60">All HTML attributes unrelated to style or layout were removed</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,74.58,662.17,353.33,9.60;3,70.80,680.41,377.77,9.60" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="3,84.96,662.17,342.95,9.60;3,70.80,680.41,373.59,9.60">All images were loaded and embedded within their a tag, using base64 encoding. Documents displayed in the mode with just text underwent three more processing steps</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,74.58,698.17,464.05,9.60;3,84.96,709.81,263.88,9.60" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="3,84.96,698.17,453.67,9.60;3,84.96,709.81,39.89,9.60">The source of all images was removed. That is, workers could see where images appeared in the document</title>
		<imprint/>
	</monogr>
	<note>but no image was shown at all (see Figure 2, right</note>
</biblStruct>

<biblStruct coords="3,74.58,721.57,361.96,9.60" xml:id="b7">
	<monogr>
		<title level="m" coord="3,84.96,721.57,347.45,9.60">HTML attributes related to style were removed, keeping the ones related to layout</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,74.58,733.33,455.22,9.60;3,70.80,751.09,467.77,9.60;3,56.64,763.21,481.97,9.60;11,56.64,354.25,64.48,12.09;11,56.64,375.49,475.46,9.14;11,70.80,386.53,37.60,9.14" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,84.96,733.33,444.84,9.60;3,70.80,751.09,467.77,9.60;3,56.64,763.21,378.33,9.60;11,207.95,375.49,161.61,9.14">Background colors were all set to white, font colors to black, and the font family set to Sans Serif of 11px. Both document versions were modified for the uc3m.hterms run to have key terms highlighted. We removed stopwords from each topic description, and obtained the stems of the remaining words</title>
	</analytic>
	<monogr>
		<title level="s" coord="11,379.80,375.91,67.80,8.71">ACM SIGIR Forum</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Alonso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Rose</surname></persName>
		</editor>
		<editor>
			<persName><surname>Stewart</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Crowdsourcing for Relevance Evaluation</note>
</biblStruct>

<biblStruct coords="11,56.64,397.69,450.62,9.14;11,70.80,408.85,274.71,9.14" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,172.94,397.69,325.79,9.14">Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,70.80,409.27,148.21,8.71">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,420.01,471.24,9.14;11,70.80,431.17,152.79,9.14" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,212.81,420.01,198.46,9.14">Quality Management on Amazon Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,421.92,420.43,105.97,8.71;11,70.80,431.59,80.27,8.71">ACM SIGKDD Workshop on Human Computation</title>
		<imprint>
			<biblScope unit="page" from="64" to="67" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,442.21,297.08,9.14;11,70.80,453.37,184.36,9.14" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,117.14,442.21,152.08,9.14">The Demographics of Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<ptr target="http://archive.nyu.edu/handle/2451/29585" />
		<imprint/>
	</monogr>
	<note>2010, available at</note>
</biblStruct>

<biblStruct coords="11,56.64,464.53,464.41,9.14;11,70.80,475.69,230.08,9.14" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,171.14,464.53,202.94,9.14">Crowdsourcing User Studies With Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,384.60,464.95,136.45,8.71;11,70.80,476.11,147.42,8.71">Annual ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,486.85,459.18,9.14;11,70.80,498.01,463.10,9.14;11,70.80,509.05,23.07,9.14" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,238.38,486.85,277.45,9.14;11,70.80,498.01,162.06,9.14">Ensuring Quality in Crowdsourced Search Relevance Evaluation: The Effects of Training Question Distribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,242.76,498.43,244.09,8.71">ACM SIGIR Workshop on Crowdsourcing for Search Evaluation</title>
		<imprint>
			<biblScope unit="page" from="17" to="20" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,520.21,456.48,9.14;11,70.80,531.37,111.15,9.14" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<title level="m" coord="11,151.63,520.21,361.49,9.14;11,70.80,531.79,81.49,8.71">Overview of the TREC 2011 Crowdsourcing Track (Conference Notebook)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Text REtrieval Conference Notebook</note>
</biblStruct>

<biblStruct coords="11,56.64,542.53,469.08,9.14;11,70.80,553.69,121.47,9.14" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,162.71,542.53,210.32,9.14">Financial Incentives and the &quot;Performance of Crowds</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,388.44,542.95,137.29,8.71;11,70.80,554.11,49.08,8.71">ACM SIGKDD Workshop on Human Computation</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,564.73,447.81,9.14;11,70.80,575.89,61.35,9.14" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,149.37,564.73,251.59,9.14">Conducting Behavioral Research on Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,411.60,565.15,92.85,8.71;11,70.80,576.31,31.62,8.71">Social Science Research Network</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,587.05,434.32,9.14;11,70.80,598.21,324.99,9.14" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,177.38,587.05,313.59,9.14;11,70.80,598.21,49.28,9.14">Instrumenting the Crowd: Using Implicit Behavioral Measures to Predict Task Performance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rzeszotarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,131.04,598.63,235.35,8.71">ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,609.37,461.90,9.14;11,70.80,620.41,442.48,9.14" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,290.27,609.37,219.87,9.14">Do User Preferences and Evaluation Measures Line Up?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,70.80,620.83,360.08,8.71">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="555" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,631.57,472.33,9.14;11,70.80,642.73,195.04,9.14" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,209.25,631.57,192.41,9.14">Designing Incentives for Inexpert Human Raters</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,411.72,631.99,117.26,8.71;11,70.80,643.15,111.64,8.71">ACM Conference on Computer Supported Cooperative Work</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,653.89,472.32,9.14;11,70.80,665.05,467.32,9.14" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,223.83,653.89,305.14,9.14;11,70.80,665.05,57.46,9.14">Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,138.48,665.47,316.91,8.71">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,676.09,479.48,9.14;11,70.80,687.25,446.68,9.14" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m" coord="11,247.46,676.09,288.66,9.14;11,70.80,687.25,364.14,9.14">Cheap and Fast-But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
	<note>Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct coords="11,56.64,698.41,443.61,9.14;11,70.80,709.57,384.03,9.14" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,252.01,698.41,248.24,9.14;11,70.80,709.57,62.51,9.14">Crowdsourcing Preference Judgments for Evaluation of Music Similarity Tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Urbano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Morato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marrero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martín</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,143.76,709.99,244.09,8.71">ACM SIGIR Workshop on Crowdsourcing for Search Evaluation</title>
		<imprint>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,720.73,456.62,9.14;11,70.80,731.89,248.79,9.14" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="11,122.37,720.73,333.21,9.14">Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,465.48,721.15,47.78,8.71;11,70.80,732.31,110.66,8.71">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,56.64,742.93,451.38,9.14;11,70.80,754.09,271.95,9.14" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="11,162.28,742.93,292.80,9.14">An Analysis of Assessor Behavior in Crowdsourced Preference Judgments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,465.96,743.35,42.06,8.71;11,70.80,754.51,199.81,8.71">ACM SIGIR Workshop on Crowdsourcing for Search Evaluation</title>
		<imprint>
			<biblScope unit="page" from="21" to="26" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
