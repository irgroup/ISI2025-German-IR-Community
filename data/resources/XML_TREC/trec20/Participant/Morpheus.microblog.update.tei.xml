<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.92,150.94,362.15,15.12">Online Topic Modeling for Real-time Twitter Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.85,184.84,76.17,10.48"><forename type="first">Christan</forename><surname>Grant</surname></persName>
							<email>cgrant@cise.ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science Gainesville</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Florida</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.51,184.84,77.25,10.48"><forename type="first">Clint</forename><forename type="middle">P</forename><surname>George</surname></persName>
							<email>cgeorge@cise.ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science Gainesville</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Florida</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.25,184.84,78.02,10.48"><forename type="first">Chris</forename><surname>Jenneisch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science Gainesville</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Florida</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.12,184.84,90.03,10.48"><forename type="first">Joseph</forename><forename type="middle">N</forename><surname>Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science Gainesville</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Florida</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.92,150.94,362.15,15.12">Online Topic Modeling for Real-time Twitter Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F4A2FB04F6BAE5C3DA1D57735669A95</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses the work done by a team at the University of Florida for the TREC 2011 Microblog Track. To build a real-time microblog search engine we rely on topic modeling for our search. To facilitate our algorithms we bundle similar tweets together in what we call supertweet generation. We perform online inference and offline inference depending on the time frame of the topical query. In this paper we discuss our techniques, challenges, future work, but not the evaluation of our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we discuss the task of real-time microblog search. Given a topic (a time-stamped query) our goal is to find the interesting and relevant microblog entries (tweets) in the data set. This is difficult for many reasons. First, each microblog entry is short, this means we don't have as much context as in most text documents. Second, many communications do not provide any useful information or are spam. Adding the scale of the data set and real-time operation to the equation creates an amalgam of difficulties.</p><p>Our system uses a topic modeling framework for querying in the large corpus (document collection) of tweets. Topic models represent documents as bags of words without considering word order as being of any importance. These models have the ability to represent large document collections with lower dimensional topics, which represent clusters of similarly behaving words. In addition, the document words are assumed to be generated from topic specific multinomials and the topic for a particular word is chosen from a document topic mixture. These topics are assumed to be generated over the corpus vocabulary from a Dirichlet distribution. <ref type="bibr" coords="1,458.24,595.25,45.75,9.57">Blei et al.</ref> gives a detailed description of this generative process and its assumptions <ref type="bibr" coords="1,463.19,608.80,10.91,9.57" target="#b1">[2]</ref>.</p><p>Topic models have a natural way to encode assumptions about observed data. Their analysis is dependent upon exploring the posterior distribution of model parameters and hidden variables conditioned on observed words. The model parameters are corpus-level topics or concepts-sets of words with corresponding probabilities-and document-level topic mixtures. In our approach, we use topic models to discover topics in the tweets and compare them with the estimated topics from the online query. The estimated topics are further used to rank relevant tweets in the corpus. We use both online and offline topic modeling to facilitate real-time search. If one considers the tweets as elements of a stream, we capture hourly batches of tweets and perform topic modeling on each batch independently. We perform online topic modeling for recent time intervals that have not yet been included in a batch.</p><p>Instead of looking at each tweet individually, we collect tweets into buckets of other similar tweet in a process called supertweet generation. We construct these large supertweet documents to help get around the problems associated with analysis of numerous small documents.</p><p>Given a query topic (a small set of search terms) and a time stamp we find the query topic distribution with respect to the documents before last hourly break point, and then the topic distribution for all the previous hourly tweet batches. We can then provide rankings for individual supertweets using Kullback-Leibler (KL) divergence of the topic distributions.</p><p>In the following sections we describe the supertweet generation process, inference techniques, and ranking procedures. Then we will discuss the approaches we take.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Supertweet generation</head><p>One of the problems with considering tweets as documents is that they contain a very small number of words being restricted to 140 characters. To tackle this problem we form supertweets from individual, similar tweets and use them for our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Similarity features</head><p>To group tweets into supertweets, we defined several similarity measures based on tweets' textual content after removing stop words. Our stop words list includes typical Internet stop words, foreign words and swear words. We identified a set of features found in tweets and associated each feature with a positive non-zero weight in order to prioritize them.</p><p>The weights for each feature were selected experimentally. The limited time period for this experiment precluded the development of a more principled approach to feature selection. Table <ref type="table" coords="3,236.57,406.33,4.65,9.57">2</ref>.1 lists the features and their weights.</p><p>The highest priority is assigned to hashtags as they represent tweets that humans have deemed to have common topics. Prior work supports the use of hash tags as a basis for aggregating tweets <ref type="bibr" coords="3,256.03,446.98,16.00,9.57" target="#b9">[10]</ref>. Web links are also used for tweet aggregation on the basis that identical links should have similar content. If a tweet is a retweet, a lower weight is assigned because we deem them as not being original. Bigrams (consecutive word pairs) are given the lowest rank as there is a high chance of getting a matching bigram sequence as compared to other features.</p><p>Based on the above five features, we define the similarity between two tweets t 1 and t 2 as:</p><formula xml:id="formula_0" coords="3,237.61,539.05,266.39,31.85">F (t 1 , t 2 ) = 5 i=1 W i * N i (t 1 , t 2 ) (1)</formula><p>where W i is the weight for feature i, and N i is the number of occurrences of common features between two tweets t 1 and t 2 .</p><p>We can also use this measure to find the similarity between a supertweet and tweet. In this case, using the average F value for the tweet with each of the tweets in the supertweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supertweet Generation</head><p>The supertweet generation process aggregates tweets that are similar to the tweets currently associated with a supertweet. We use a threshold on the similarity measure to decide whether a new tweet can be added to a supertweet.</p><p>Since there are many tweets, we cannot consider the whole corpus for forming supertweets. Instead, we use a sliding window approach in which we represent a sliding window as a priority queue Q. Q is a fixed length queue and contains supertweets in the order of their recency of formation. Whenever Q is full, we remove the oldest supertweet and insert a newly formed supertweet. The removed supertweet would be added to our final list of supertweets that we maintain in the database. Algorithm 1 describes our method for supertweet generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Supertweet Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic modeling and document analysis</head><p>Topic models such as latent Dirichlet allocation (LDA) <ref type="bibr" coords="4,382.81,595.25,11.52,9.57" target="#b2">[3]</ref> and hierarchical LDA <ref type="bibr" coords="4,108.00,608.80,16.97,9.57" target="#b10">[11]</ref> are well-known for exploratory and predictive analysis of text. They define topics as distributions over the words in a vocabulary and documents as being generated by mixtures of these topics. The words of individual documents are drawn independently from document topic mixtures (mixtures of topic multinomials) <ref type="bibr" coords="4,489.45,649.45,10.91,9.57" target="#b2">[3]</ref>. Topic models represent document (message) words in a bag-of-words format without considering word order to be of any particular importance. These models support powerful methods for dimensionality reduction of large, unstructured document collections. In addition, we can use the document posteriors for information retrieval and classification.</p><p>Topic models are conventionally designed for fixed <ref type="bibr" coords="5,378.47,152.16,11.52,9.57" target="#b2">[3]</ref> or varying numbers of topics <ref type="bibr" coords="5,141.01,165.71,16.97,9.57" target="#b10">[11]</ref> usually on discrete-time document collections. In the case of twitter conversations, the number of possible topics or concepts is unbounded. We can also assume that the topics associated with tweets emerge, evolve, and disappear over time. For this reason, we used the hierarchical Dirichlet process based LDA <ref type="bibr" coords="5,473.67,206.35,16.97,9.57" target="#b10">[11]</ref> to extract topics from the offline data store (which contains tweets from a two week time period).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Batch topic learning</head><p>Topic models provide a natural way to encode assumptions about observed data and their analysis is dependent on exploring the posterior distribution of model parameters and hidden variables conditioned on observed words. The model parameters are corpus-level topics or concepts, i.e., sets of words with corresponding probabilities, and document-level topic mixtures. Our twitter data analysis is largely based on these parameters. Performing maximum a-posteriori (MAP) estimation on the LDA model is intractable <ref type="bibr" coords="5,206.74,364.55,11.52,9.57" target="#b2">[3,</ref><ref type="bibr" coords="5,222.04,364.55,12.73,9.57" target="#b10">11]</ref>. Thus, people typically use relatively efficient sampling approaches and optimization approaches for inference. Sampling approaches are usually based on Markov Chain Monte Carlo (MCMC) methods, in which we define a Markov chain whose stationary distribution is the posterior of interest <ref type="bibr" coords="5,459.71,405.20,10.91,9.57" target="#b6">[7]</ref>. Most common optimization approaches for topic modeling are based on variational Bayes (VB), which optimizes a simplified parametric distribution close to the posterior on Kullback-Leibler divergence <ref type="bibr" coords="5,262.15,445.85,10.91,9.57" target="#b2">[3]</ref>. Variational Bayes estimation of the posterior usually introduces bias, where as MCMC-based Gibbs samplers generate independent samples from the posterior <ref type="bibr" coords="5,270.76,472.95,10.91,9.57" target="#b7">[8]</ref>. In this project, we used a Gibbs sampling based MCMC to estimate the parameters of the hierarchical LDA <ref type="bibr" coords="5,421.47,486.50,16.00,9.57" target="#b10">[11]</ref>. We used the hierarchical LDA package developed by Chong Wang<ref type="foot" coords="5,361.39,498.09,4.23,6.99" target="#foot_0">1</ref> in our batch topic learning.</p><p>Even though topic models are very useful in dimensionality reduction, clustering, and analysis of large document collections <ref type="bibr" coords="5,316.92,527.14,11.52,9.57" target="#b2">[3,</ref><ref type="bibr" coords="5,332.82,527.14,7.68,9.57" target="#b0">1]</ref>, their topic estimation process is computationally expensive and is currently inconceivable for huge collections <ref type="bibr" coords="5,484.00,540.69,16.00,9.57" target="#b12">[13]</ref>. In this project, we have a data set of ∼ 16 million tweets. It is nearly impossible to run topic inference for this entire data set in the time frame of our investigation. Our approach was to split the data set into the batches of tweets drawn from onehour time periods and perform hierarchical LDA on the individual batches. Also, we deleted those extracted topics that did not have a minimum threshold of word associations in the whole document collection.</p><p>There are some natural limitations that prevent us from directly applying topic modeling to the twitter data available. First, twitter messages are usually small (being restricted to 140 characters), which is substantially different from conventional information text retrieval and mining problems. Second, within this short text people communicate rich meanings. For example, for long URLs we use URL shortening services, and for specific events or topics we define # tags. We found that the restricted lengths of tweets prevents us from exploiting their full potential in a topic-modeling setting. Our experiments showed that aggregating tweets (section 2) to train the topic model can obtain an improved set of topics. The research work of Hong et al. <ref type="bibr" coords="6,180.74,206.35,11.52,9.57" target="#b8">[9]</ref> discusses similar observations on a different Twitter dataset. In addition, we consider special tags (e.g. #, @) and URLs as if they are individual words, allowing them to group into different topical clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online document inference</head><p>In this project, we used tweets' and super tweets' topic mixtures to find tweet similarities. By online document inference, we mean inferring topic mixtures during query time. Conventional topic learning algorithms <ref type="bibr" coords="6,360.78,310.36,11.52,9.57" target="#b2">[3,</ref><ref type="bibr" coords="6,376.44,310.36,13.94,9.57" target="#b10">11]</ref> are designed to run on collections of documents (i.e., in batch mode). In our recent work, we developed a system <ref type="bibr" coords="6,156.73,337.46,11.52,9.57" target="#b4">[5,</ref><ref type="bibr" coords="6,173.62,337.46,8.49,9.57" target="#b5">6]</ref> that can infer topic structure for newly encountered documents without retraining the estimated topic model. This model is based on a fast hybrid Metropolis search <ref type="bibr" coords="6,194.57,364.55,11.52,9.57" target="#b5">[6]</ref> and can use the learned models from any batch topic modeling algorithms.</p><p>To enable online processing for a given query, we first find out the corresponding hour batch to which it belongs by consulting its time stamp. Second, at query processing time we infer topic distributions for the tweets that belong to the query's batch and have time stamps earlier than the query time stamp, using the hierarchical LDA. Finally, we calculate the query topic mixtures based on all the batches' precomputed topic distributions using the hybrid Metropolis topic search. Once we have the query topic mixtures, we run our query matching algorithms that are explained in the next section to display relevant tweets to the end user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ranking</head><p>Our ranking method is a straightforward filtering method. For a query q we infer topic mixtures using the online document inference. We compute the top-k supertweets in each batch using the KL-divergence between the query topic distribution for the specific batch and each supertweet in the batch. Next, for each tweet we compute a weighted distribution over several parameters to calculate F (Eq. 1). These parameters all contribute to the idea of interestingness.</p><p>Supertweet score We keep the tweet's supertweet divergence. A high score here means there is some contextual relevance between the query and this tweet neighborhood.</p><p>Recency We give an exponential back-off score so that we reward more recent tweets. The recency is given by 1000/|t 1 -t 2 |, where t 1 is the time associated with the query and t 2 is the timestamp associated with the tweet.</p><p>Word Length We say a helpful tweet is one with about 20 words. We create a Gaussian function to give the most weight to tweets around this value. The word length function is given by</p><formula xml:id="formula_1" coords="7,303.60,185.01,65.41,23.27">1 √ 2πσ 2 e -(x-µ) 2 2σ 2</formula><p>where x is the number of tokens in the tweet, µ is 20, and σ is 5.</p><p>Important Words This function assigns higher weights to longer matching words in the candidate tweet.</p><p>Jaccard similarity coefficient We calculate the simple term matching methods.</p><p>Fuzzy Wuzzy Measure We implemented a string similarity metric developed by seat geek for matching short snippets of text<ref type="foot" coords="7,356.65,301.05,4.23,6.99" target="#foot_1">2</ref> . This algorithm is similar to the well known Jaro-Winkler algorithm.</p><p>These ranking function were implemented in Scala version 2.9.1 and were parallelized using the parallel collection libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We spent about two-thirds of our time obtaining and cleaning the dataset. The size of the dataset gave us trouble when attempting to download tweets across the network. We were finally able to obtain the full data set using the twitter-corpustools <ref type="foot" coords="7,130.79,453.02,4.23,6.99" target="#foot_2">3</ref> . Next, we removed all foreign language tweets from the data set. Because we used the HTML scrape of the data set, the language specified from the HTML document webpage could not be trusted. So, we created two Bloom filters to help us filter languages. We filled the first Bloom filter with with dictionaries of several different languages. We filled the second Bloom filter with terms from English and SMS short codes (e.g. LOL). Then we went through all tweets and removed any tweet that contained a word that was both in the foreign language bloom filter and not in the English Bloom filter (with the confidence of a particular threshold). We found this technique to be fast and accurate with a low number of false positives.</p><p>We faced several problems when testing our method on the evaluation queries. First, we found that the number of terms in each the evaluation query was too small to generate a statistically interesting topic distribution (e.g. Toyota Recall ). This prevented us from effectively exploiting similarity based on topic distributions with some queries. Our method was more successful with longer queries containing more diverse search terms.</p><p>Second, during the preprocessing step, we removed infrequently occurring words from the dataset. Our dataset pruning can remove search terms needed to satisfy queries associated with infrequently mentioned topics. In addition, we did not perform any stemming, lemmatization, or other type of dictionary-based preprocessing on the tweets or queries. Our operating principle was that similar meaning words would be clustered together by LDA during topic modeling based retrieval. This assumption may not hold for infrequently occurring words and word forms even if synonyms or alternate forms appear frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We presented a method to perform real-time search for the TREC 2011 Microblog Track. We used topic modeling to extract and rank tweets from a large dataset. Our method for this problem is novel but it will require further refinement to be effective.</p><p>Our first goal for future work is evaluation of the results and techniques. Our initial results were somewhat dissappointing but have led us to a better understand of some of the limitations of topic modeling. The topic multinomials we identified appeared to be quite reasonable. The choices we were forced to make in supertweet formation and feature weighting need to be considered more carefully. Second, we will look into evolving topics across time-varying batches. Other researchers have tracked topic over time <ref type="bibr" coords="8,218.10,400.57,16.00,9.57" target="#b11">[12]</ref>. Third, our features for ranking are few. In the future we will look into adding soft tf-idf scores and other parameters to increase accuracy <ref type="bibr" coords="8,489.45,414.12,10.91,9.57" target="#b3">[4]</ref>. To offset our shortcomings, we believe query expansion techniques will be helpful for matching queries. Finally, we plan to create a simulator so we can test our code in a real-time environment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,233.42,324.06,145.15,9.57;2,108.00,108.00,346.00,200.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Architecture</figDesc><graphic coords="2,108.00,108.00,346.00,200.00" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,124.59,670.17,212.90,7.86"><p>http://www.cs.princeton.edu/ chongw/resource.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,124.59,663.62,165.99,7.86"><p>https://github.com/seatgeek/fuzzywuzzy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,124.59,674.58,191.16,7.86"><p>https://github.com/lintool/twitter-corpus-tools</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,130.43,513.49,373.58,9.57;8,130.42,527.04,373.58,9.57;8,130.42,540.59,130.82,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,359.31,513.49,144.69,9.57;8,130.42,527.04,318.52,9.57">The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,461.56,527.04,35.33,9.57">J. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2010-02">February 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,130.43,563.10,373.58,9.57;8,130.42,576.65,178.80,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,328.75,563.10,122.99,9.57">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,461.57,563.10,42.43,9.57;8,130.42,576.65,52.91,9.57">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,130.43,599.17,373.58,9.57;8,130.42,612.72,117.61,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,335.24,599.17,124.04,9.57">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,470.66,599.17,26.67,9.57">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,130.43,635.23,373.57,9.57;8,130.42,648.78,373.57,9.57;8,130.42,662.33,24.85,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,359.26,635.23,144.74,9.57;8,130.42,648.78,28.13,9.57">Duplicate record detection: A survey</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,171.67,648.78,266.61,9.57">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.43,111.51,373.58,9.57;9,130.42,125.06,373.58,9.57;9,130.42,138.61,339.87,9.57" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,218.63,125.06,230.33,9.57">Product partition models for dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
		<idno>519</idno>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>The Dept of CISE</note>
</biblStruct>

<biblStruct coords="9,130.43,161.12,373.58,9.57;9,130.42,174.67,373.57,9.57;9,130.42,188.22,373.58,9.57;9,130.42,201.77,227.47,9.57" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,191.87,174.67,312.13,9.57;9,130.42,188.22,205.17,9.57">Topic learning and inference using dirichlet allocation product partition models and hybrid metropolis search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">N</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
		<idno>520</idno>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>The Dept of CISE</note>
</biblStruct>

<biblStruct coords="9,130.43,224.29,373.57,9.57;9,130.42,237.83,326.10,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,292.91,224.29,113.05,9.57">Finding scientific topics</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,417.36,224.29,86.64,9.57;9,130.42,237.83,141.64,9.57">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">Suppl. 1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.43,260.35,373.57,9.57;9,130.42,273.90,373.57,9.57;9,130.42,287.45,373.57,9.57;9,130.42,301.00,24.85,9.57" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,292.82,260.35,206.69,9.57">Online learning for latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,168.67,287.45,244.82,9.57">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.43,323.51,373.58,9.57;9,130.42,337.06,373.57,9.57;9,130.42,350.61,197.31,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,271.65,323.51,212.19,9.57">Empirical study of topic modeling in twitter</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,130.42,337.06,340.20,9.57">Proceedings of the First Workshop on Social Media Analytics, SOMA &apos;10</title>
		<meeting>the First Workshop on Social Media Analytics, SOMA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.42,373.13,373.58,9.57;9,130.42,386.68,373.58,9.57;9,130.42,400.23,373.58,9.57;9,130.42,413.77,287.37,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,288.98,373.13,215.02,9.57;9,130.42,386.68,214.88,9.57">Smoothing techniques for adaptive online language models: topic tracking in tweet streams</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,367.21,386.68,136.79,9.57;9,130.42,400.23,373.58,9.57;9,130.42,413.77,41.51,9.57">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="422" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.42,436.29,373.57,9.57;9,130.42,449.84,373.57,9.57;9,130.42,463.39,24.85,9.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,401.26,436.29,102.74,9.57;9,130.42,449.84,41.92,9.57">Hierarchical Dirichlet processes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,180.24,449.84,219.74,9.57">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.42,485.90,373.57,9.57;9,130.42,499.45,373.57,9.57;9,130.42,513.00,373.57,9.57;9,130.42,526.55,163.36,9.57" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,271.01,485.90,232.99,9.57;9,130.42,499.45,106.01,9.57">Topics over time: a non-markov continuous-time model of topical trends</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,257.21,499.45,246.79,9.57;9,130.42,513.00,295.20,9.57">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,130.42,549.07,373.57,9.57;9,130.42,562.62,373.57,9.57;9,130.42,576.17,373.57,9.57;9,130.42,589.72,86.37,9.57" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,324.34,549.07,179.66,9.57;9,130.42,562.62,204.31,9.57">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,361.48,562.62,142.52,9.57;9,130.42,576.17,265.20,9.57">Proceedings of the 15th ACM SIGKDD international conference on KDD, KDD &apos;09</title>
		<meeting>the 15th ACM SIGKDD international conference on KDD, KDD &apos;09<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
