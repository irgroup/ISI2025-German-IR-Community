<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.83,97.04,318.35,16.49">Microsoft Research at TREC 2011 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.98,130.91,76.89,11.45"><forename type="first">Bodo</forename><surname>Billerbeck</surname></persName>
						</author>
						<author>
							<persName coords="1,240.65,130.91,66.67,11.45"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
						</author>
						<author>
							<persName coords="1,315.36,130.91,71.88,11.45"><forename type="first">Dennis</forename><surname>Fetterly</surname></persName>
						</author>
						<author>
							<persName coords="1,394.61,130.91,61.41,11.45;1,282.43,155.66,47.15,11.45"><forename type="first">Marc</forename><forename type="middle">Najork</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main" coord="1,146.83,97.04,318.35,16.49">Microsoft Research at TREC 2011 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D2DF9D2A426E1B3517F43FBA1674A52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our entry into the TREC 2011 Web track. We extracted and ranked results from the ClueWeb09 corpus using a parallel processing pipeline that avoids the generation of an inverted file. We describe the components of the parallel architecture and the pipeline, how we ran the TREC experiments, and we present effectiveness results. * w0 * w1 * w2 * w3 * wS * wI * wW * wA * wB * wS * wI * wW * wA * wB * wS * wI * wW * wA * wB * wS * wI * wW * wA * wB</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our entry into the TREC 2011 Web track. Our team comprised members from Bing, Microsoft's search engine, and Microsoft Research, Silicon Valley. We used the DryadLINQ data-parallel processing system <ref type="bibr" coords="1,539.83,325.03,18.17,10.45" target="#b9">[10]</ref> on a cluster of about 220 machines to process the half-billion page English-language subset of the ClueWeb09 collection. We also used the Scalable Hyperlink Store <ref type="bibr" coords="1,294.52,352.13,12.72,10.45" target="#b7">[8]</ref> running on a cluster of 12 machines to compute SALSA scores for each topic. Our retrieval and ranking pipeline consisted of multiple stages. In the first stage we generated query reformulations, using three different rewriting techniques, all based on the anchor text of co-citing hyperlinks. In the second stage we retrieved results for the original topic, and separately for each reformulation, from the ClueWeb corpus. In the third stage, we computed five features (described below) for each candidate result. In the final stage we used a weighted linear combination to blend these features in each result set followed by a second weighted linear combination to blend the scores of the result sets associated with the original and rewritten topics. We performed four runs, and submitted two of them to both the ad-hoc and the diversity task, and the other two runs to a single task each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Processing Pipeline</head><p>The processing pipeline we used to prepare this year's entry consisted of the following steps:</p><p>• Parsing: We used DryadLINQ to parse the HTML web pages, tokenizing them into individual words and hyperlinks. We associated each title and content word occurrence with the document containing it; in addition, we associated each anchor word occurrence with the document referenced by the anchor's hyperlink.</p><p>• Query Reformulation We developed 3 rewriting techniques, which are numbered 1,2,3 and described in Section 3. The original queries can be seen as a "baseline" rewriting, with technique number 0.</p><p>• Document Frequencies: We computed document frequencies for the union of all terms of the queries we were considering. The queries included rewrites of this year's 50 topics, the 100 topics from TREC 2009 and TREC 2010.</p><p>• BM25F score: We computed a BM25F score for each query and each document in the ClueWeb09 collection, retaining the top 5000 results per query.</p><p>• SALSA score: Furthermore, we used SHS to run SALSA-SETR <ref type="bibr" coords="2,373.84,57.57,12.72,10.45" target="#b8">[9]</ref> on the top 5000 results of each query. SALSA-SETR is a variant of Lempel &amp; Moran's SALSA algorithm <ref type="bibr" coords="2,384.79,71.12,11.59,10.45" target="#b6">[7]</ref>, which in turn is a variant of Kleinberg's HITS algorithm <ref type="bibr" coords="2,182.59,84.67,11.59,10.45" target="#b5">[6]</ref>, one of the best-known link-based ranking algorithms.</p><p>• Inter-domain in-degree We also used SHS to compute the inter-domain in-degree of each result page, that is the number of hyperlinks from pages in different domains referring to the result page.</p><p>• Matching anchor count (MAC): We resolved all the symbolic hostnames in ClueWeb09 page and link URLs into IP addresses. Then, using DryadLINQ, we identified unique s, t, a triples, where s is the first three octets of the IP address of a document, t is the target URL of a link within that document and a is the anchor text of that link. Then we built the MAC ranking feature for each query-target pair q, t, counting the number of source IPs that link to target t with anchor a = q.</p><p>• Extraction: For each query we identified a pool of documents and collated our ranking features. The resulting "extraction" file could be used for training or for generating submitted runs.</p><p>• Spam scores: For each result, we extracted the "spam score" provided by the University of Waterloo <ref type="bibr" coords="2,519.31,256.03,13.79,10.45" target="#b0">[1]</ref>.</p><p>• Training: We combined evidence from the various features by applying ad-hoc transformations to each feature (identity or log, depending on the feature) and then performing a weighted linear combination of the transformed features. For the original queries, we trained the weights using an extraction of the 2009 and 2010 topics and their official judgments. Then, having scored each original and rewritten query using "optimal" weights, we performed a weighted linear combination of the scores of each rewriting technique, again training on the 2009 and 2010 judgments.</p><p>• Runs: Using an extraction of the fifty 2011 topics, we ran our trained model to produce our submitted runs.</p><p>This pipeline differed from the approach we took last year <ref type="bibr" coords="2,310.64,391.32,12.72,10.45" target="#b2">[3]</ref> in the following ways:</p><p>• Query reformulation: The main innovation of this year's entry compared to our previous entries is the addition of query rewriting techniques. Commercial engines can employ multiple data sources, including query logs, to drive such rewriting; however we did not want to utilize any proprietary data. The techniques are described in Section 3.</p><p>• Omitted and added features: We re-introduced the inter-domain in-degree feature that we employed in our 2009 entry <ref type="bibr" coords="2,130.51,490.55,11.59,10.45" target="#b1">[2]</ref>. Conversely, we removed the "span score" feature introduced in our 2010 entry since it was not effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Rewriting Techniques</head><p>Query rewriting transforms an input query such as 'obama family tree', into a rewrite query such as 'barack obama genealogy'. Our ranking model this year (Figure <ref type="figure" coords="2,275.87,586.57,4.54,10.45" target="#fig_0">1</ref>) combined signals from the original query and three rewrites. Using rewrites, rather than traditional query expansion, allowed us to make use of our MAC ranking feature. MAC is a phrase match between query and anchor, so is defined for a rewrite query, but would not be defined under blind feedback with a weighted bag-of-words expansion.</p><p>Following <ref type="bibr" coords="2,118.93,640.77,12.72,10.45" target="#b4">[5]</ref> we treated anchor text information as though it were search log data. We generated a bipartite click graph <ref type="bibr" coords="2,107.71,654.32,12.72,10.45" target="#b3">[4]</ref> from the ClueWeb99 anchors, using the anchor phrases as queries, which are connected the link target URLs as though they were 'clicked'. For clarity, we will continue to refer to the bipartite graph as being between anchors and URLs.</p><p>In rewriting techniques 1 and 2, we generated a set of simple phrase-level translation probabilities. Given that an anchor contains term A, the probability of translation A is the probability of finding another anchor in two steps  on the bipartite graph where A is replaced with A . For example, the probability of translation 'az' for the term 'arizona' is 0.00575 because 181, 900 anchors contain 'arizona' and 1046 of those phrases co-cite a URL with the corresponding 'az' query. To reduce noise, we eliminated translations that happened in fewer than 50 anchor pairs. In technique 1 we took the most probable translation for each query, so for 'arizona game and fish' would generate the rewrite 'az game and fish'. Another possible translation, from 'game and fish' to 'fish and game' was not applied because it is less probable under the simple translation model.</p><p>In technique 2, to further reduce noise we only considered translations where the most probable translation of A is A and the most probable translation of A is A. For each input query we then applied the translation with the highest score = max (strlen(A), strlen(A )). So translating 'game and fish' to 'fish and game' (13 characters) was chosen, rather than 'arizona' to 'az' (7 characters).</p><p>Technique 3 went back to the bipartite graph. To rewrite a query, there must be an anchor in the graph that matches exactly. We then took two steps on the graph to find rewrites, choosing the rewrite that was connected by the most two-step paths, which means having the most URLs in common. To reduce noise we eliminated rewrites with only one path. We also removed rewrites that were simply shortenings of the input query or contained noise terms 'free', 'wikipedia', 'www', 'click', 'here', 'com', 'org', 'site', 'website', 'more', 'link', 'amp', 'lt', 'gt', or 'nbsp'. Technique 3 yielded some of the most interesting rewrites, for example 'to be or not to be that is the question' is rewritten as 'hamlet s soliloquy'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Web track submissions</head><p>We submitted three runs to the ad-hoc task and three runs to the diversity task. In all six runs the ranker is a simple linear combination of features. Figure <ref type="figure" coords="3,222.02,675.10,5.45,10.45" target="#fig_0">1</ref> shows the ranking tree. The following table shows the weights used in each run. In addition to the six submitted runs, we also report on two "baseline" runs (named unofficial1 and unofficial2) that combine our five features but do not perform query rewriting. We evaluated training runs using multiple performance measures: for the adhoc task, ERR@20, nDCG@20, P@20, and MAP; for the diversity task, ERR-IA@20, α-nDCG@20, NRBP, and MAP-IA. Our training process left us with a pool of runs each of which excelled on a particular measure. We selected our submitted runs such that they did well on the primary measure of each task while also doing well on at least one alternate measure. Run msrsv2011a2 is identical to run msrsv2011d1 and run msrsv2011a3 is identical to run msrsv2011d3. The next table shows the performance of our six submitted runs and the two unofficial "baseline" runs compared to the median performance of all TREC 2010 web track entries, according to a variety of different performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>ERR@20 nDCG@20 P@10 P@20 msrsv2011a1 0. We can draw two observations from these performance numbers: First, all of our runs had better than median performance; and second, query rewriting did not lead to substantial performance improvements. The improvements are not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Feature Analysis</head><p>This section presents a brief analysis of MAC, which is one of our most important ranking features, to show that it performed differently this year than in previous years.</p><p>We first analyze how many documents per query have non-zero MAC (its coverage). As indicated in the table, the coverage dropped this year, with the median coverage dropping from hundreds of documents to 7 documents. The explanation for the coverage drop is a change in query sampling this year. In previous years, the average query length was around two words. This year all queries are 3 or more words, and the average length is 3.4 words. MAC is only defined if the query exactly matches an anchor. For longer queries it is less likely that such anchors exist, and therefore MAC has lower coverage. The second analysis is to confirm that MAC, with its lower coverage, was also relatively less useful as a ranking feature this year. We compare the adhoc performance of a MAC-only ranker to a BM25F-only ranker. We use 2010 and 2011 queries, omitting 2009 since assessors that year used different judging guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adhoc 2010</head><p>Adhoc 2011</p><p>NDCG@20 ERR@20 NDCG@20 ERR@20 BM25F 0. On the 2010 queries, where MAC had better coverage, it also performed relatively better than BM25F, around twice as well, whereas on the 2011 adhoc task the two features perform roughly equally well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In preparing our entry this year, we utilized the same computational infrastructure we employed in the previous two years: the DryadLINQ data-parallel processing platform and the Scalable Hyperlink Store. This year's main innovation was the introduction of query rewriting leveraging anchor texts. According to our preliminary performance results, our system performed competivily to other systems. Query rewriting, however, did not lead to substantial performance improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,248.55,351.57,114.89,10.45"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scoring pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,146.57,56.50,18.19,10.45;4,217.31,56.62,149.03,10.95;4,382.07,56.62,12.04,10.89;4,415.84,56.62,12.04,10.89;4,449.62,56.62,12.04,10.89;4,146.57,70.45,58.18,10.45;4,222.13,70.45,5.45,10.45;4,244.86,70.45,95.56,10.45;4,357.84,70.45,5.45,10.45;4,378.80,70.45,86.64,10.45;4,146.57,83.99,58.18,10.45;4,222.13,83.99,5.45,10.45;4,244.86,83.99,95.56,10.45;4,357.84,83.99,5.45,10.45;4,378.80,83.99,86.64,10.45;4,146.57,97.54,58.18,10.45;4,222.13,97.54,5.45,10.45;4,244.86,97.54,95.56,10.45;4,357.84,97.54,5.45,10.45;4,378.80,97.54,86.64,10.45;4,146.57,111.09,58.79,10.45;4,222.13,111.09,5.45,10.45;4,244.86,111.09,95.56,10.45;4,357.84,111.09,5.45,10.45;4,378.80,111.09,86.64,10.45;4,146.57,124.64,58.79,10.45;4,222.13,124.64,5.45,10.45;4,244.86,124.64,95.56,10.45;4,357.84,124.64,5.45,10.45;4,378.80,124.64,86.64,10.45;4,146.57,138.19,58.79,10.45;4,222.13,138.19,5.45,10.45;4,244.86,138.19,95.56,10.45;4,357.84,138.19,5.45,10.45;4,378.80,138.19,86.64,10.45;4,146.57,151.74,47.00,10.45;4,222.13,151.74,5.45,10.45;4,244.86,151.74,95.56,10.45;4,357.84,151.74,5.45,10.45;4,385.62,151.74,5.45,10.45;4,419.39,151.74,5.45,10.45;4,453.16,151.74,5.45,10.45;4,146.57,165.29,47.00,10.45;4,222.13,165.29,5.45,10.45;4,244.86,165.29,95.56,10.45;4,357.84,165.29,5.45,10.45;4,385.62,165.29,5.45,10.45;4,419.39,165.29,5.45,10.45;4,453.16,165.29,5.45,10.45"><head>Runw</head><label></label><figDesc>B w A w S w I w W w 0 w 1</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,77.63,430.45,480.37,10.45;5,77.63,443.53,331.48,10.92" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,288.19,430.45,269.81,10.45;5,77.63,444.00,54.03,10.45">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1004.5168" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,466.52,480.37,10.45;5,77.63,480.07,358.22,10.45" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,354.91,466.52,203.09,10.45;5,77.63,480.07,119.60,10.45">Microsoft Research at TREC 2009 -Web and Relevance Feedback Tracks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,217.55,480.15,186.24,10.24">Proc. of the 18th Text Retrieval Conference</title>
		<meeting>of the 18th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,502.58,480.37,10.45;5,77.63,516.13,122.44,10.45" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,239.74,502.58,204.41,10.45">Microsoft Research at TREC 2010 Web Tracks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,464.68,502.67,93.33,10.24;5,77.63,516.22,90.39,10.24">Proc. of the 19th Text Retrieval Conference</title>
		<meeting>of the 19th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,538.65,470.73,10.45" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,198.22,538.65,144.37,10.45">Random walks on the click graph</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,362.92,538.73,153.39,10.24">Proc. of the 30th SIGIR Conference</title>
		<meeting>of the 30th SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,561.16,480.37,10.45;5,77.63,574.71,79.89,10.45" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,172.87,561.16,166.45,10.45">Query reformulation using anchor text</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,358.71,561.25,199.30,10.24;5,77.63,574.80,47.83,10.24">Proc. of the 3rd Web Search and Data Mining Conference</title>
		<meeting>of the 3rd Web Search and Data Mining Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,597.23,480.37,10.45;5,77.63,610.77,255.03,10.45" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,154.54,597.23,223.91,10.45">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,400.57,597.31,157.43,10.24;5,77.63,610.86,151.90,10.24">Proc. of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,633.29,480.37,10.45;5,77.63,646.84,282.11,10.45" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,197.16,633.29,356.94,10.45">The stochastic approach for link-structure analysis (SALSA) and the TKC effect</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,77.63,646.93,168.97,10.24">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1-6</biblScope>
			<biblScope unit="page" from="387" to="401" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.63,669.36,480.37,10.45;5,77.63,682.90,84.84,10.45" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,129.88,669.36,122.11,10.45">The scalable hyperlink store</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,271.88,669.44,280.97,10.24">Proc of the 20th ACM Conference on Hypertext and Hypermedia</title>
		<meeting>of the 20th ACM Conference on Hypertext and Hypermedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.63,57.57,480.37,10.45;6,77.63,71.12,480.37,10.45;6,77.63,84.67,68.18,10.45" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,274.66,57.57,283.34,10.45;6,77.63,71.12,69.73,10.45">Less is More: Sampling the neighborhood graph makes SALSA better and faster</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,169.34,71.21,355.46,10.24">Proc of the 2nd ACM International Conference on Web Search and Data Mining</title>
		<meeting>of the 2nd ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,77.63,104.81,480.37,12.83;6,77.63,120.74,480.37,10.45;6,77.63,134.29,356.77,10.45" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,441.82,107.19,116.18,10.45;6,77.63,120.74,349.28,10.45">DryadLINQ: a system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ú</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.86,120.82,109.14,10.24;6,77.63,134.37,269.91,10.24">Proc. of the 8th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 8th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
