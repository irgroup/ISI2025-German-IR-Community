<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.44,72.44,396.73,16.59">GeAnn at the TREC 2011 Crowdsourcing Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.84,118.02,87.44,11.06"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<email>c.eickhoff@tudelft.nl</email>
						</author>
						<author>
							<persName coords="1,248.16,118.02,113.14,11.06"><forename type="first">Christopher</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
							<email>christopher-harris@uiowa.edu</email>
						</author>
						<author>
							<persName coords="1,400.20,118.02,101.57,11.06"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<country>TU Delft Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Iowa</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Arjen P. de Vries CWI</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Delft University of Tech-nology</orgName>
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.44,72.44,396.73,16.59">GeAnn at the TREC 2011 Crowdsourcing Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A9C1591FA2128092746AFC7E047FBD8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relevance assessments of information retrieval results are often created by domain experts. This expertise is typically expensive in terms of money or personal effort. The TREC 2011 crowdsourcing track aims to evaluate different strategies of crowdsourcing relevance judgements. This work describes the joint participation of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Ground truth relevance assessments for information retrieval benchmarking initiatives such as TREC have traditionally been created by professionals with substantial expertise in search and information science. Typically, the assessment is done either in a collective effort of the research community by pooling and redistributing the submitted runs to participants, or, through external experts, such as the assessors at NIST <ref type="bibr" coords="1,120.36,470.03,9.12,8.97" target="#b5">[5]</ref>. Recent work has shown the applicability of crowdsourcing for this use case <ref type="bibr" coords="1,210.36,480.47,9.12,8.97" target="#b1">[1]</ref>. Under the right conditions, a group of inexpensive workers could match the performance of professional NIST annotators for this task at significantly lower cost. However, the novel setting introduces a number of new challenges, previously unknown in the traditional controlled relevance assessment task. Concretely, there are frequent mentions of crowdsourcing workers cheating or delivering results of inferior quality <ref type="bibr" coords="1,269.04,553.67,9.64,8.97" target="#b3">[3,</ref><ref type="bibr" coords="1,283.20,553.67,6.42,8.97">6]</ref>. The TREC 2011 crowdsourcing track was set up to devise and compare different strategies of how to phrase relevance assessment tasks as crowdsourcing HITs. We suspect that there are fundamentally different motivations for offering workforce on a crowdsourcing platform <ref type="bibr" coords="1,280.68,605.99,9.12,8.97" target="#b4">[4]</ref>. Money-driven workers are mainly motivated by the financial reward that is being paid upon completion of the HIT. Entertainment-driven workers, on the other hand, primarily seek an appealing pastime while seeing the payment as a positive side effect rather than a central motivation. Due to these different underlying motivations, we expect to observe a different working behaviour. Financially driven workers may display a greater likelihood to cheat or take shortcuts that result in lower result quality. Our TREC participation aims at providing a more appealing and engaging rele-vance assessment environment by means of a term association game. The remainder of this working note is structured as follows: Section 2 describes our game-based approach to the assessment task and gives a detailed inspection of the obtained results. Section 3 describes and evaluates the trust aggregation method used for the consensus task. Section 4 discusses general observations about the track and its setting. Finally, Section 5 closes by proposing future modifications of the GeAnn game and its use for relevance assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ASSESSMENT TASK</head><p>The first task asked participants to collect binary relevance judgements for approximately 2100 topic/document pairs in typical TREC fashion. The effectiveness of the different strategies and HIT designs is evaluated in terms of the quality of the collected labels, the time taken to create those labels and the amount of money invested in the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach</head><p>A fundamental difference between typical relevance assessment tasks in the fashion of TREC and the game that we propose, resides in the fact that we do not judge the relevance of a document as a whole, but instead break down the global decision onto term level. In the game, the player is confronted with 4 buckets at the bottom of the screen, each of which represents a TREC query. From the top of the screen, a keyword (or image) slides down and the player is required to direct it into one of the query buckets that is most closely related to the term. One of the 4 buckets represents the original query from the q/d pair, 2 are randomly drawn TREC queries and a final bucket is labelled "None" to account for terms that are not related to any of the topics. The top left hand corner provides additional evidence by displaying the snippet of text in which the current term appeared on the original web page. Depending on the consensus with peers, the player is awarded points, that ultimately reward a position on the ladder board. As the game progresses, the speed increases, making decisions more difficult. The terms are aligned such on the screen, that without user input they will not fall into any of the buckets. As the player misses a bucket for the third time, the game ends. Figure <ref type="figure" coords="1,345.84,679.19,4.60,8.97" target="#fig_0">1</ref> shows a screen view of the annotation game.</p><p>Consequently, we are faced with a number of preprocessing steps before being able to begin the annotation: (1) Break up the document into a set of sentences S. (2) Rank every sentence s ∈ S according to an informativeness criterion c(s). In the present case, we used the averaged idf score across all constituent terms t ∈ s. (3) Use the top n sentences from the ranked list for assessment by means of our game. (4) For each of the selected sentences, identify the single most informative term and use it as a sliding keyword, while the original sentence is shown as context snippet in the top left hand corner.</p><formula xml:id="formula_0" coords="2,134.04,340.91,158.86,24.02">c(s) = 1 |s| t∈s idf (t)<label>(1)</label></formula><p>The main decision to influence the confidence in our pagewide relevance labels is the choice of n. High settings of this parameter result in a better coverage of the document's content in the assessment. To comply with the TREC deadline despite several delays in data distribution and game development, we were only able to judge a fixed number of n = 6 sentences per document. Optimally, concrete settings of n should depend on the document length, as well as the prior agreement rates on the document in question. In such a scheme, it would be possible to demand more assessments for ambiguous or long documents. Finally, in a post-processing step, we aggregate the sentence-level judgements that were made by individual players, and, subsequently, make a global decision l doc across all sentences, games and players. The aggregation is based on a uniform majority voting scheme mv across a set of labels L, in which the most frequent label is propagated.</p><formula xml:id="formula_1" coords="2,134.04,566.03,158.86,9.66">l (sent) = mv (Lgame )<label>(2)</label></formula><formula xml:id="formula_2" coords="2,137.40,588.47,155.50,9.66">l (doc) = mv (Lsent )<label>(3)</label></formula><p>The game was initialized in this fashion and the HITs were offered on Amazon Mechanical Turk via CrowdFlower. Workers were asked to play at least one round (10 terms from 2 document sets as provided by the TREC organizers) of the term association game and were offered a payment of 1 US cent. This was regarded to be more of an initial incentive for giving the game attention rather than an actual payment for the assessments. Moreover, to attract additional players, we advertised the game through various social networking sites. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>In this section, we will inspect the performance of our game-based relevance assessment approach along the three previously mentioned dimensions: (1) Result quality, (2) Time taken to acquire results, as well as (3) the financial effort put into the assessment. As mentioned before, we recruited players through word of mouth as well as an advertisement HIT with a very small payment. Throughout the evaluation section, we will pay careful attention to investigating whether there are significant differences in the observed assessment behaviour of paid vs. unpaid players. Overall, 47% of our 188 players that contributed to the TREC submission were recruited through the crowdsourcing HIT. The remaining players accessed the game directly from the Web. Table <ref type="table" coords="2,503.40,310.31,4.60,8.97" target="#tab_0">1</ref> presents an overview of a number of key statistics for the overall player population, as well as per subgroup.</p><p>Although no further payment was offered after the first round of 10 term associations in a game, we can see that a substantial number of players continued the game even without the prospect of an additional financial reward. The share of players recruited through MTurk to which this applies is slightly lower than for external players. The average game lasted between 5 and 8 rounds, with slightly shorter games being played by Turkers. On average, each player played 1.6 games, with shares of 20-28% of unique players returning for additional games after the first. For returning players, the average time between games was found to be 3.5 hours without major differences by player origin. The global distributions of rounds per game and games per player are shown in Figures <ref type="figure" coords="2,387.96,477.71,4.60,8.97" target="#fig_1">2</ref> and<ref type="figure" coords="2,413.40,477.71,3.56,8.97" target="#fig_2">3</ref>, respectively. These initial figures already hint towards a tendency that is not typically found for crowdsourcing settings: Workers performing more work than they were required and paid for. A possible reason could be the engaging nature of our relevance assessment game. We will revisit this observation when analysing the cost efficiency of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label quality</head><p>One of the key performance criteria for relevance assessments is the accuracy of the collected labels. In our main TREC run, we evaluated the pure quality of the labels produced by our method. We omitted any form of aggregation or majority voting across players. The result can be considered a conservative lower bound performance that would be equivalent to asking relevance assessments to only a single worker without redundancy. The official TREC evaluation results with respect to the global consensus across teams as well as prior gold standard judgements from NIST are shown in Table <ref type="table" coords="2,353.04,679.19,3.56,8.97" target="#tab_2">2</ref>. Even without any form of consensus, the use of which is typically considered mandatory for crowdsourcing, could we achieve substantial result quality. In relevance assessment scenarios, agreement rates of 60-70% are typically    to be expected from single judges in controlled lab environments. Being able to reach this quality level with single uncontrolled annotators was not to be expected.</p><p>To get more realistic insights into the potential of our method, we additionally aggregate majority vote labels across players. Table <ref type="table" coords="3,376.32,468.00,4.60,8.97" target="#tab_1">3</ref> reports the updated figures for this setting. We can note a consistent upwards tendency for all compared measures.</p><p>In order to create a competitive and challenging atmosphere that would motivate players to return to the game, we increase the game speed with each new round into which the player advances. This time pressure could have an influence on label quality as players have less time to make decisions in later rounds. Figure <ref type="figure" coords="3,418.56,551.76,4.60,8.97" target="#fig_3">4</ref> shows the accuracy (agreement with global majority label) of judgements as a function of the game round in which they were issued.</p><p>Given this general downwards tendency in result quality, we reconsider and expand our majority voting scheme by a round-based confidence parameter λ. As the game speeds up, we expect players to err more frequently and therefore put less trust in judgements from higher rounds. Starting at λ = 1.0, for each round after the first, we reduce it by 0.05 to a minimum of λ = 0.5. Table <ref type="table" coords="3,470.52,645.84,4.60,8.97" target="#tab_3">4</ref> shows the resulting performance gains of this scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessment speed</head><p>Another key criterion in the evaluation of crowdsourcing methods is the time required to produce a number of judgements. This global time between initially publishing the </p><formula xml:id="formula_3" coords="4,125.52,337.67,95.62,9.02">t batch = n(t uptake + t tasl )</formula><p>In reality, HITs are accessed by multiple workers in parallel, thus reducing the time per batch. Especially for short HITs with low values of t task , the batch run time is dominated by the uptake rate.</p><p>In the concrete case of our TREC participation, we collected 10,535 document-level judgements within 8 days. During this period, we observed a t uptake between games of 33 minutes. The overall distribution of games played is shown in Figure <ref type="figure" coords="4,82.56,437.51,3.56,8.97" target="#fig_4">5</ref>. The HIT uptake in the last 3 days is substantially lower as we were only issuing small batches by that time in order to fill in labels that had previously been begun in cancelled sessions. Due to the track's rules, in order to include a worker's judgement on a q/d pair, he has to judge all pair in a set. Therefore, we had to resubmit incomplete sessions.</p><p>The time per HIT in our case could not be evaluated statically as a game could run for a variable number of rounds. Instead, we measure the time taken per round of 10 judgements. The speed of the game imposes an upper bound on the available time per judgement, and, subsequently, per round. Concretely, we observed an average time between judgements of a round of 5.6 seconds, resulting in an average round duration of approximately one minute. As a conclusion to our temporal result analysis, we find our game environment to facilitate judgements in a fast, yet qualitypreserving manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessment cost</head><p>The final part of our analysis is concerned with the necessary cost involved in the collection of crowdsourced relevance labels. In the previous sections, we found game-based relevance assessments to be of good quality and collection speed. The real strength of our method, however, lies in giving workers an alternative motivation from the pure financial reward. The total overall cost involved in the collection of our 10,350 query/document labels, including the AMT service overhead, was $ 3.74. Even with respect to the generally low pay rates on crowdsourcing platforms, this result can be considered remarkable. It nicely shows how workers become players with a primary interest in the game experience rather than the hourly rate of only $ 0.23 (a total of $ 3.74 paid for 10350 labels, with the average assessment taking 5.6 seconds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CONSENSUS TASK</head><p>The previous task was concerned with the collection of labels using crowdsourcing. Task 2 assumes that this step has already been taken. Given a number of crowdsourced relevance labels for query document pairs, determine consensus between multiple labels on the same pair. The collection contains 19,033 unique query/document pairs for which 89,624 binary relevance had been collected. 3,275 pairs also contain gold standard NIST labels. For the final evaluation, 1000 additional gold labels were withheld. The document IDs were anonymized so that no further evidence beyond the set of labels could be collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>Our approach towards Task 2 is based on iterative computation of worker reliability in order to make non-uniform majority votes. A central component of our method is the reliability function rt that assigns each worker w a score between 0 and 1 at time t. Higher scores express higher prior reliability. At the beginning of our iterative scheme, r1.05(w) is initialized as the worker's accuracy accg(w) on the set of gold labels Gw, that he encountered. If no gold judgements are available for that worker, we assume the maximum reliability score of 1.</p><formula xml:id="formula_4" coords="5,103.92,299.64,137.68,21.45">r1.05(w) = accg(w) if |Gw| &gt; 0 1 else</formula><p>Now, we rank all query/document pairs in decreasing order of the ratio of agreement a in their labels. E.g., a pair for which all 5 workers assigned the same label (a = 1) would be ranked higher than one for which 2 relevant and 3 irrelevant judgements were issued (a = <ref type="formula" coords="5,175.56,370.16,3.65,5.98" target="#formula_1">2</ref>3 ). At this point, the preparation is complete. Now, we start the iteration process by computing majority voting labels lt(p) for all pairs p with an agreement a &gt;= t, where t ranges from 1 to 0.5 in steps of 0.05 per iteration. The majority label, finally, is computed as the weighted average label, based on the individual worker labels l(w, p) and the previous worker reliability. The last step in each iteration is to update each worker's reliability scores by the accuracy of all previous votes (against both gold and consensus). Once this is achieved, we lower the threshold agreement and start the following iteration. Figure <ref type="figure" coords="5,82.44,530.51,4.60,8.97" target="#fig_5">6</ref> illustrates the work flow of our method graphically. rt(w) = acct(w)</p><formula xml:id="formula_5" coords="5,118.32,464.96,47.53,9.72">lt(p) = w∈W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>The official evaluation of Task 2 was conducted based on the overall consensus across groups as well as the 1000 heldout gold labels from NIST. Table <ref type="table" coords="5,198.96,604.19,4.60,8.97" target="#tab_4">5</ref> gives an overview of the achieved performance. Inspecting these numbers, we find, that while being able to aggregate worker performance, our method was not among the most competitive ones. We can additionally note a significant disparity between performance as evaluated against consensus and gold labels. This tendency is repeated for all participating groups. In a number of cases this drastically changes the ranking of teams between the two evaluation methods. We will discuss this property of employing consensus labels for evaluation purposes in greater detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>Following the performance analysis of our proposed method, we will now proceed to discussing a number of central observations made during the label collection process. Traditionally, especially in Web retrieval settings, relevance is assumed to be distributed sparsely in the document collection. For any reasonable query, we can expect the vast majority of documents to be irrelevant. Typically initiatives such as TREC resort to capturing a biased sample of retrieval results that gives a more even split between relevant and irrelevant pages than a random sample of the Web would. Especially for crowdsourcing purposes such an approach appears sensible in order to ward off workers who try to learn the underlying label distribution in order to cheat on subsequent tasks. The data set constructed for the crowdsourcing, however, makes an exception in the opposite direction, here, 68% of the provided NIST labels belong to the relevant classes. Also, the resulting crowdsourced consensus labels show a collection-wide average relevance of 0.55, that significantly surpasses an even split between classes. A global share of 57% of all labels belong to the relevant class. Figure <ref type="figure" coords="5,345.24,279.24,4.60,8.97" target="#fig_7">7</ref> shows the distribution of relevance in the consensus labels across all teams. As we can see, the bias resides on the highly relevant pages which seem to be over-represented in the collection. Such an imbalanced setting bears significant dangers of over training the worker population towards giving relevance-biased answers. This problem gains additional impact in consensus-based settings. As the majority across a collection of labels is used to evaluate subsets of the collection, one has to be very careful to avoid any form of crowd training where possible. As soon as large parts of the crowd suspect a biased underlying label distribution, global consensus may not represent a valid means of evaluation any more. This is additionally aggravated by the fact that majority voting across teams with homogeneous numbers of submissions is not necessarily an objective measure of result quality. Teams with a high number of submitted labels that were previously curated to follow an internal majority can greatly bias the global decision. As a consequence, the consensus label can be gamed by submitting more than other teams. To further understand whether this happened in the present evaluation, it would be good to investigate the correlation between the number of submitted labels and the various resulting team performance scores. Since these figures are not openly available, we could not provide the sketched analysis. A final aspect to be covered by this discussion is crowd diversity. The submitted runs show a substantial variance in the number of unique workers who contributed to the submitted labels. The different teams employed worker pools ranging from 1 to 503 individuals with an average of 128 workers per team. Using a Spearman rank correlation test, we analysed team performance in dependency with the size of their worker pool. The result can be found in Table <ref type="table" coords="5,500.76,613.91,3.56,8.97">6</ref>. We can notice a strong inverse correlation between the size of worker pools and the achieved performance in terms of accuracy and precision. While this observation does not necessarily imply a causality between small worker groups and superior performance, it certainly raises the question how comparable the employed settings were across teams and how well methods based on 30 workers scale to larger problems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION &amp; FUTURE DIRECTIONS</head><p>In this work, we described GeAnn, a game-based approach towards phrasing relevance judgement tasks in an engaging and entertaining way. By breaking up web documents into phrases and asking players to associate those with a number of topics we create relevance judgements in an efficient manner. The evaluation of the TREC 2011 crowdsourcing track has shown that our method delivers good quality at extremely low cost. Making annotations more entertaining served for an alternative motivation besides the financial reward. As a side-effect of our judgement scheme, we produce passage-level relevance judgements from which we derive a holistic decision per document. The main challenge resides in the fact that the task being carried out in our game (associating terms and topics) is not identical to the one being ultimately evaluated (page-wide relevance assessments between queries and documents). This disparity may introduce additional noise in the resulting judgements. There are a number of aspects to be addressed and improved upon in the future in order to further improve the performance of game-based annotations. (1) Currently, there was no limit to the number of rounds for which a game could last. This confused players and shifted the aim of the game to "surviving" through as many rounds as possible instead of producing as high-quality labels as possible within a fixed number of rounds. An updated version of the game now has a fixed number of ten rounds after which the game ends. (2) currently, we extracted a fixed number of phrases per document. By doing so, we may over-represent short documents while having insufficient coverage of large documents. In the future, we will take a different approach that takes document lengths into consideration when extracting phrases. With respect to this, the ideal degree of document coverage has to be determined. (3) Currently, the only element of competition lies in ranking players on a ladder board. For subsequent versions of the game we would like to emphasize this point to further increase player engagement. This has been previously shown to be beneficial for result quality <ref type="bibr" coords="6,543.60,516.84,9.12,8.97" target="#b2">[2]</ref>. Concretely, we aim to introduce a multi player setting, in which the direct competition between peers will be enabled. <ref type="bibr" coords="6,316.80,548.16,11.74,8.97" target="#b4">(4)</ref> In this work, we exclusively focused on textual resources. However, images often convey a significant amount of meaning, as well. In the future we aim to also use images to replace some of the game elements (e.g., bucket labels, sliding terms or text blocks). This may introduce more variation in the game, thus additionally motivating players. We also suspect moving image content to be easier to discern than text (a few players commented on the sliding terms being hard to read). We can conclude that there are many potential alleyways towards making game-based relevance assessments a superior alternative to both, standard expert assessments as well as crowdsourced tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,73.68,215.24,199.38,8.08;2,66.41,54.07,213.60,147.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Screen view of the GeAnn game.</figDesc><graphic coords="2,66.41,54.07,213.60,147.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,56.76,347.12,233.34,8.08;3,53.88,80.88,252.00,252.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of rounds played per game.</figDesc><graphic coords="3,53.88,80.88,252.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,56.28,682.88,234.30,8.08;3,53.88,416.52,252.00,252.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of games played per player.</figDesc><graphic coords="3,53.88,416.52,252.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,340.32,386.12,191.94,8.08;3,316.92,119.88,252.00,252.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Label accuracy by game round.</figDesc><graphic coords="3,316.92,119.88,252.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,321.00,390.92,230.82,8.08;4,316.92,124.68,252.00,252.00"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of games played over time.</figDesc><graphic coords="4,316.92,124.68,252.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,93.72,215.60,159.30,8.08;5,53.86,53.94,244.08,147.31"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Reliability-based voting.</figDesc><graphic coords="5,53.86,53.94,244.08,147.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,168.00,458.00,59.04,5.98;5,161.40,478.04,16.21,3.51;5,179.88,471.08,35.40,5.98"><head>r</head><label></label><figDesc>t+0.05 (w) * l(w,p) w∈W r t+0.05 (w)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,60.60,653.24,225.54,8.08;6,53.88,387.00,252.00,252.00"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Distribution of relevance in consensus.</figDesc><graphic coords="6,53.88,387.00,252.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,335.52,63.68,201.78,73.92"><head>Table 1 :</head><label>1</label><figDesc>Game-based assessment behaviour</figDesc><table coords="2,346.44,72.00,179.83,65.61"><row><cell></cell><cell cols="3">All Turk Web</cell></row><row><cell cols="4">Games with 2+ rounds 52% 45% 58%</cell></row><row><cell>Rounds per game</cell><cell>6.7</cell><cell>5.3</cell><cell>7.9</cell></row><row><cell>Games per player</cell><cell>1.6</cell><cell>1.5</cell><cell>1.7</cell></row><row><cell>Returners</cell><cell cols="3">24% 20% 28%</cell></row><row><cell>Time to return</cell><cell cols="3">3.5h 3.4h 3.7h</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,354.96,63.68,162.82,42.72"><head>Table 3 :</head><label>3</label><figDesc>Aggregated Task 1 results</figDesc><table coords="3,356.76,73.80,159.12,32.61"><row><cell>Source</cell><cell>Accuracy</cell><cell>R</cell><cell>P</cell></row><row><cell>Consensus</cell><cell>69.4%</cell><cell cols="2">79.7% 69.2%</cell></row><row><cell>Gold</cell><cell>63.1%</cell><cell cols="2">75.9% 73.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,122.04,63.68,365.67,41.04"><head>Table 2 :</head><label>2</label><figDesc>Official Task 1 results</figDesc><table coords="4,122.04,72.00,365.67,32.73"><row><cell>Source</cell><cell>Accuracy</cell><cell>R</cell><cell>P</cell><cell cols="4">Specificity Log Loss KL Divergence RMSE</cell></row><row><cell>Consensus</cell><cell>65.0%</cell><cell cols="2">76.9% 66.8%</cell><cell>45.4%</cell><cell>376.3</cell><cell>358.8</cell><cell>51.4%</cell></row><row><cell>Gold</cell><cell>62.3%</cell><cell cols="2">74.8% 72.4%</cell><cell>26.5%</cell><cell>94.6</cell><cell>94.6</cell><cell>52.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,53.76,133.88,239.32,186.12"><head>Table 4 :</head><label>4</label><figDesc>Discounted Task 1 resultsThe uptake time t uptake expresses the mean interval between workers starting new games. Depending on how appealing the HIT looks and how competitive the offered pay level is, the uptake time can vary greatly. Previous work has shown this factor to be subject to external influences such as the size of the HIT batch (in our case 50 HITS per batch) or its position on the overview page from which workers select their tasks. (2) The task time t task represents the actual time a worker spends per task. A naive estimate of an upper bound on the expected runtime per batch is therefore:</figDesc><table coords="4,53.76,142.32,239.20,73.17"><row><cell>Source</cell><cell>Accuracy</cell><cell>R</cell><cell>P</cell></row><row><cell>Consensus</cell><cell>70.5%</cell><cell cols="2">80.1% 71.3%</cell></row><row><cell>Gold</cell><cell>64.3%</cell><cell cols="2">76.3% 74.8%</cell></row><row><cell cols="4">HIT and collecting the results is controlled by two central</cell></row><row><cell>elements: (1)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,53.76,63.68,433.95,201.36"><head>Table 5 :</head><label>5</label><figDesc>Official Task 2 results</figDesc><table coords="6,53.76,72.00,433.95,193.05"><row><cell></cell><cell>Source</cell><cell cols="2">Accuracy</cell><cell>R</cell><cell>P</cell><cell cols="2">Specificity Log Loss KL Divergence RMSE</cell></row><row><cell cols="2">Consensus</cell><cell></cell><cell>73.6%</cell><cell cols="2">81.5% 78.0%</cell><cell>60.1%</cell><cell>5992.5</cell><cell>12911.1</cell><cell>15.2%</cell></row><row><cell></cell><cell>Gold</cell><cell></cell><cell>57.7%</cell><cell cols="2">73.5% 55.8%</cell><cell>41.8%</cell><cell>1150.4</cell><cell>1150.5</cell><cell>51.3%</cell></row><row><cell cols="7">Table 6: Team performance correlated to the size of</cell></row><row><cell cols="2">employed worker pools</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell># Workers</cell><cell></cell><cell></cell></row><row><cell>Acc</cell><cell cols="3">1.00 0.95 0.19</cell><cell>-0.71</cell><cell></cell><cell></cell></row><row><cell>P</cell><cell></cell><cell cols="2">1.00 -0.05</cell><cell>-0.76</cell><cell></cell><cell></cell></row><row><cell>R</cell><cell></cell><cell></cell><cell>1.00</cell><cell>-0.14</cell><cell></cell><cell></cell></row><row><cell># Workers</cell><cell></cell><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,58.25,55.55,96.71,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,68.04,70.20,197.89,8.97;7,68.04,80.64,185.78,8.97;7,68.04,91.08,183.86,8.97;7,68.04,101.52,223.95,8.97;7,68.04,112.08,58.71,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,177.48,70.20,88.45,8.97;7,68.04,80.64,185.78,8.97;7,68.04,91.08,41.70,8.97">Can we get rid of trec assessors? using mechanical turk for relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,128.52,91.38,123.38,8.65;7,68.04,101.82,166.31,8.65">Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<meeting>the SIGIR 2009 Workshop on the Future of IR Evaluation</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,68.04,123.48,216.42,8.97;7,68.04,133.92,201.40,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,155.52,123.78,128.94,8.65;7,68.04,134.22,120.17,8.65">Finding flow: The psychology of engagement with everyday life</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Csikszentmihalyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,68.04,145.44,207.63,8.97;7,68.04,155.88,185.77,8.97;7,68.04,166.62,223.74,8.65;7,68.04,177.06,204.03,8.65;7,68.04,187.19,218.79,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,184.32,145.44,91.35,8.97;7,68.04,155.88,42.03,8.97">How crowdsourcable is your task?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,125.64,156.18,128.17,8.65;7,68.04,166.62,223.74,8.65;7,68.04,177.06,204.03,8.65;7,68.04,187.50,135.49,8.65">Proceedings of the Workshop on Crowdsourcing for Search and Data Mining (CSDM) at the Fourth ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the Workshop on Crowdsourcing for Search and Data Mining (CSDM) at the Fourth ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,68.04,198.71,214.35,8.97;7,68.04,209.15,224.09,8.97;7,68.04,219.90,183.87,8.65;7,68.04,230.03,153.27,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,91.68,209.15,160.90,8.97">Geann -games for engaging annotations</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,271.32,209.46,20.82,8.65;7,68.04,219.90,183.87,8.65;7,68.04,230.34,125.16,8.65">Proc. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR&apos;11)</title>
		<meeting>ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,68.04,241.55,186.38,8.97;7,68.04,251.99,224.82,8.97;7,68.04,262.43,203.91,8.97;7,68.04,272.87,45.28,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,215.16,252.30,77.70,8.65;7,68.04,262.74,155.74,8.65">TREC: Experiment and evaluation in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MIT press USA</publisher>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards, and Technology (US</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,68.04,284.39,218.11,8.97;7,68.04,294.83,198.52,8.97;7,68.04,305.27,201.83,8.97;7,68.04,316.02,219.91,8.65;7,68.04,326.27,116.68,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,243.72,284.39,42.43,8.97;7,68.04,294.83,198.52,8.97;7,68.04,305.27,108.14,8.97">How much spam can you take? an analysis of crowdsourcing results to increase accuracy</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vuurens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,194.76,305.58,75.11,8.65;7,68.04,316.02,219.91,8.65;7,68.04,326.58,34.80,8.65">Proc. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR&apos;11)</title>
		<meeting>ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
