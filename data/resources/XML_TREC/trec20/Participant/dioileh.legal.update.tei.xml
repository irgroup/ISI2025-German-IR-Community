<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,204.76,115.96,205.84,12.62;1,153.47,133.89,308.41,12.62;1,254.54,151.82,106.27,12.62">Helioid at TREC-Legal 2011: Learning to Rank from Relevance Feedback for e-Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.42,189.51,96.74,8.74"><forename type="first">Peter</forename><surname>Lubell-Doughtie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Helioid Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,326.86,189.51,81.08,8.74"><forename type="first">Kenneth</forename><surname>Hamilton</surname></persName>
							<email>kenneth@helioid.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Helioid Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,204.76,115.96,205.84,12.62;1,153.47,133.89,308.41,12.62;1,254.54,151.82,106.27,12.62">Helioid at TREC-Legal 2011: Learning to Rank from Relevance Feedback for e-Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">41B5790D12447C8C55A5B97645119738</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the results of applying a learning to rank algorithm to the 2011 TREC Legal dataset. The learning to rank algorithm we use was designed to maximize NDCG, MAP, and AUC scores. We therefore examine our results using the AUC and hypothetical F1 scores. We find query expansion and learning to rank improve scores beyond standard language model retrieval, however learning to rank does not outperform query expansion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our approach begins with language modeling and then, as feedback from the user is received, we combine relevance feedback and learning to rank on the query level to improve result rankings using information from user interaction. This approach is based on the work in <ref type="bibr" coords="1,302.23,432.77,9.96,8.74" target="#b4">[5]</ref>, which showed that by using relevance feedback in conjunction with learning to rank, result ordering could be improved beyond what is possible with only relevance feedback. We use learning to rank in order to automatically tune the amount and nature of relevance feedback specifically to the circumstance of a particular query. We apply learning to rank by first constructing a feature representation of our relevance feedback, and then using a pair-wise learning to rank algorithm which reorders documents based on their features and the features of judged documents.</p><p>In this paper we present experiments applying learning to rank with relevance feedback to the 2011 TREC Legal learning task. The framework our system used was designed to maximize AUC scores and we will therefore compare our system system using the AUC and hypothetical F1 scores, the same metric used in 2010 TREC Legal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most commonly used query expansion method is the Rocchio algorithm <ref type="bibr" coords="1,467.30,632.21,9.96,8.74" target="#b7">[8]</ref>. It is based on the vector space representation of queries and documents and aims to select expansion terms from the relevant documents to create a query that can optimally distinguish between relevant and non-relevant documents. Typically, implementations of the Rocchio algorithm ignore non-relevant documents because excluding terms has been found to be problematic <ref type="bibr" coords="2,399.34,142.90,14.61,8.74" target="#b15">[16]</ref>. However, <ref type="bibr" coords="2,465.09,142.90,15.50,8.74" target="#b15">[16]</ref> find that negative feedback can be beneficial, especially for difficult queries. Our method avoids the problems of modeling negative feedback by using non-relevant documents in reordering, not query expansion.</p><p>Problems with sensitivity to parameter settings have led to substantial research in adaptive relevance feedback, where parameters such as the number of expansion terms, or the relative weight of expanded and original query terms are tuned using machine learning methods <ref type="bibr" coords="2,327.44,227.19,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,341.99,227.19,7.01,8.74" target="#b8">9]</ref>. Previous work considers the task of tuning these settings across queries (e.g., learning to predict the optimal weights for interpolation based on query and result set characteristics). Our work is complementary to these approaches, as it focuses on learning to rank for individual queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Additional Research</head><p>In other approaches, <ref type="bibr" coords="2,224.18,330.77,10.52,8.74" target="#b2">[3]</ref> apply latent semantic indexing to reduce dimensions, and <ref type="bibr" coords="2,134.77,342.72,15.50,8.74" target="#b9">[10]</ref> use kernel density estimation and logistic regression to model the training data. <ref type="bibr" coords="2,160.30,354.68,15.50,8.74" target="#b11">[12]</ref> use relevance feedback based upon the seed documents, and <ref type="bibr" coords="2,444.41,354.68,10.52,8.74" target="#b1">[2]</ref> use a Naive Bayes classifier to model the seed documents. The Naive Bayes classifier is known to have problems with unbalanced datasets <ref type="bibr" coords="2,374.57,378.59,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="2,394.20,378.59,11.62,8.74" target="#b14">15]</ref>. In contrast, we use a learning to rank algorithm which computes an optimized ranking using stochastic gradient descent. With this algorithm, called Ranking SVM, we are able to avoid problems of dataset balance.</p><p>The methods which have been successfully used to address the TREC Legal track are similar to those used in the TREC-CHEM track, which involves a prior art search task and is similarly recall oriented <ref type="bibr" coords="2,336.30,450.92,9.96,8.74" target="#b5">[6]</ref>. Given this overlap, we expect that applying our research to patent search will be a fruitful avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The learning to rank from relevance feedback system learns from user feedback to build a model with which it reorders documents to better serve the user's information needs. In the interactive setting our approach models the retrieval task as a series of interactions between the retrieval system and the user. Figure <ref type="figure" coords="2,134.77,584.39,4.98,8.74" target="#fig_0">1</ref> presents a high level overview of our search system. The user first submits a query, with which the system retrieves results. The system then returns these results to the user, who judges a subset of them. Next the system uses these judged documents to expand the query and reorder results through learning to rank. Finally, the reordered results are returned to the user, who is either satisfied and ends the search, or judges more results and continues the retrieval process. However, in the TREC Legal Learning task there is no interaction. In this case we use the complete set of judged documents as the relevance feedback set, and will compare this to performing retrieval when we have no relevance feedback. The main components of our search system are:</p><p>1. a retrieval function that ranks documents, 2. an expansion function that builds updated queries from judged documents, 3. a learning function that learns a document ranking using judged documents.</p><p>In the first step we index and retrieve documents with the Lemur (Indri 5.0)<ref type="foot" coords="3,476.12,440.44,3.97,6.12" target="#foot_0">1</ref> search engine using language modeling. We run queries under the default retrieval model <ref type="bibr" coords="3,195.12,465.93,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="3,213.94,465.93,11.62,8.74" target="#b12">13]</ref>, which uses Dirichlet smoothing with µ = 2500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query expansion</head><p>We learn from judged documents on two levels by using query expansion (second step) in concert with learning to rank (third step). In query expansion we use the judged documents, D q , to build a modified query:</p><formula xml:id="formula_0" coords="3,247.47,575.04,233.13,20.28">q = αq 0 + (1 -α) dj ∈Dq d j ,<label>(1)</label></formula><p>where q 0 is the original query and α is an interpolation parameter. We retrieve documents with the expanded query q, which provides us with a retrieval score per document. We will use these retrieval scores as a feature in learning to rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning method</head><p>To learn a reordering of our retrieved documents we begin by projecting the judged documents into different feature spaces to create a feature vector x i = Φ(d i , q), where Φ extracts features from document d i with respect to query q.</p><p>We use document judgements to build a set of preference pairs P such that (i, j) ∈ P iff document d i is preferred to d j . We then minimize the objective function:</p><formula xml:id="formula_1" coords="4,229.80,202.32,246.55,27.27">1 2 ||w|| 2 + C (i,j)∈P (w x i -w x j ), (<label>2</label></formula><formula xml:id="formula_2" coords="4,476.35,209.06,4.24,8.74">)</formula><p>where C is a parameter that allows trading-off margin size against training error and is an appropriate loss function, in our case it is the squared hinge loss: (t) = max(0, 1 -t)<ref type="foot" coords="4,223.53,257.56,3.97,6.12" target="#foot_1">2</ref>  <ref type="bibr" coords="4,232.08,259.13,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="4,246.67,259.13,7.01,8.74" target="#b3">4]</ref>. Minimizing Equation <ref type="formula" coords="4,358.12,259.13,4.98,8.74" target="#formula_1">2</ref>gives us w, the weighting vector over the feature space which minimizes the number of incorrectly ordered pairs of judged documents. 2  We then project the top m documents from the corpus-as ranked by query q-into the same feature space and use w to classify these documents:</p><formula xml:id="formula_3" coords="4,284.64,324.10,46.08,8.77">y = w X,</formula><p>where X = [x 1 , . . . , x m ] are feature vectors for documents [d 1 , . . . , d m ], and each value y i ∈ y is the predicted relevance of document d i . We sort the documents by their predicted relevance, as given by y, and append the not-reordered documents ranked greater than m, to produce a reordering of the entire corpus.</p><p>We build features using the relevance feedback documents in multiple retrieval runs with different queries constructed by varying the number of expansion terms. The feature extractor Φ takes P q , the vector of judged documents from relevance feedback, r different expanded queries {q 0 • t 1 • • • • • t lj |j ≤ r}, and the retrieval scores when using these expanded queries:</p><formula xml:id="formula_4" coords="4,165.40,451.92,137.64,36.27">Φ     {q 0 • t 1 • • • • • t l1 , P q , s<label>(1)</label></formula><p>1 , . . . , s</p><p>n } . . .</p><formula xml:id="formula_6" coords="4,189.86,451.92,290.73,47.62">{q 0 • t 1 • • • • • t lr , P q , s (r) 1 , . . . , s (r) n }     =     s (1) 1 , . . . , s (1) m . . . s (r) 1 , . . . , s (r) m     ,<label>(3)</label></formula><p>where {l j |j ≤ r} indicates the number of terms used in expansion, e.g. if l j = 10 we expand the query with 10 terms and s (j) 1 , . . . , s (j) n are the retrieval scores when using this expanded query. These features are formed from the same judged document set, and thus the specific set of judged documents has a significant influence on the feature space and reordering, this implies that it is a more exploitative strategy.</p><p>However, this method has the practical disadvantage that it requires a retrieval run for each feature. For example, to test re-ranking with r features each iteration requires r retrieval runs: one to generate each additional feature for each expanded query. In practice this is not a problem because the retrieval runs can be parallelized.</p><p>We use the judged documents in 2011 TREC Legal to evaluate three scenarios:</p><p>1. lm -standard language model retrieval using the original query. 2. expansion -query expansion with 20 terms 3. learning -our approach using learning to rank with r = 4 and l = {5, 10, 15, 20}</p><p>In the expansion scenario we chose to use 20 expansion terms because experiments with the 2010 TREC Legal data found this setting to perform best. We further note that the system we used for evaluation is based on the 2010 TREC Legal dataset, in which the AUC score was the basis for measurement. Therefore we will present results of our learning to rank system based on both hypothetical F1 scores and AUC scorse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Table <ref type="table" coords="5,161.67,337.32,4.98,8.74" target="#tab_0">1</ref> presents the hypothetical F1 scores for all queries and Table <ref type="table" coords="5,430.81,337.32,4.98,8.74" target="#tab_1">2</ref> shows the AUC scores. In the first columns we see that the baseline, language model retrieval, achieves an average score over all queries of 9.8 and 80.6 for hypothetical F1 and AUC scores respectively. Next, expansion outperforms lm with hypothetical F1 and AUC scores of 15.0 and 82.5 respectively. Our method, learning, also outperforms lm and is competitive with the relevance feedback run (expansion), with hypothetical F1 and AUC scores of 14.9 and 82.5, respectively.</p><p>Per query, the performance across all methods varies substantially. All our retrieval methods are based upon language modeling and we therefore expect that this variation is because some queries contain keywords that are more helpful for retrieval than others. From a cursory examination of the queries, we note that query 401, for which we achieve the highest scores across methods, concerns online services and contains the term enrononline, which is likely to be quite informative. However, query 403, on which performance is worst across methods, concerns the environmental impact of Enrons activities, a subject which the company would perhaps have an incentive to obscure and may therefore be less likely to be explicitly mentioned in email communications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to Previous Research</head><p>Our results show that applying learning to rank does not perform substantially better than using query expansion alone. These results are in contrast to previous research using the TREC 2011 data, in which learning to rank substantially outperformed query expansion <ref type="bibr" coords="6,256.17,284.85,9.96,8.74" target="#b4">[5]</ref>. However, previous work used the more common information retrieval metrics of normalized discounted cumulative gain (NDCG) and mean average prevision (MAP), whereas the 2011 TREC Legal evaluation does not calculate these metrics.</p><p>There are also differences in the dataset. There are fewer, 3 versus 8, queries in the 2011 dataset compared to the 2010 dataset. Perhaps more significantly, there are a greater number of judged documents per query for the 2011 dataset in comparison with the 2010 dataset. On average there are about twice as many judged documents in 2011 and less variation in the number of judged documents per query. It is possible that the larger number of judged documents correlates with less stringent requirements on the degree of (non-)relevance necessary for a document to be marked as (non-)relevant. If this is the case, it would dilute the feature space and require adjustments in our learning to rank method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have described our application of learning to rank from relevance feedback to the 2011 TREC Legal dataset. Interestingly, using this dataset learning to rank is not able to substantially outperform query expansion alone, although both learning to rank and query expansion outperform an approach based solely on language modeling. We speculate that learning to rank is not able to improve results because the judged documents are of a lower quality in the 2011 dataset as compared to the 2010 dataset, in virtue of the larger number of judged documents.</p><p>More work is in order to investigate why learning to rank does not outperform query expansion. We also note that the original research evaluated learning to rank with a number of different feature representations <ref type="bibr" coords="6,378.31,620.25,9.96,8.74" target="#b4">[5]</ref>, and in this research we used only one feature representation. Future work applying additional feature representations to the the 2011 TREC Legal data can provide a clearer picture of the differences in performance between this dataset and the 2010 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,205.05,298.34,205.27,7.89;3,152.06,115.83,311.26,167.73"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A high level overview of our search system.</figDesc><graphic coords="3,152.06,115.83,311.26,167.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,159.36,561.98,296.64,75.69"><head>Table 1 .</head><label>1</label><figDesc>Hypothetical F1 scores per query and averaged over all queries.</figDesc><table coords="5,229.11,582.76,153.92,54.91"><row><cell cols="3">Query lm expansion</cell><cell>learning</cell></row><row><cell cols="2">401 18.3</cell><cell>24.6</cell><cell>24.6</cell></row><row><cell>402</cell><cell>7.8</cell><cell>11.1</cell><cell>11.0</cell></row><row><cell>403</cell><cell>3.2</cell><cell>9.0</cell><cell>9.3</cell></row><row><cell cols="2">AVG 9.8</cell><cell>15.0</cell><cell>14.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,182.08,115.91,251.20,75.69"><head>Table 2 .</head><label>2</label><figDesc>AUC scores per query and averaged over all queries.</figDesc><table coords="6,229.11,136.68,153.92,54.91"><row><cell cols="2">Query lm expansion</cell><cell>learning</cell></row><row><cell>401 72.5</cell><cell>76.0</cell><cell>76.0</cell></row><row><cell>402 88.6</cell><cell>89.0</cell><cell>89.0</cell></row><row><cell>403 80.7</cell><cell>82.6</cell><cell>82.5</cell></row><row><cell>AVG 80.6</cell><cell>82.5</cell><cell>82.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.44,127.10,7.47"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,645.84,335.87,8.12;4,144.73,656.80,278.15,8.12"><p>We use SGD-SVM as implemented in sofia-ml, https://code.google.com/p/ sofia-ml/, which solves equation 2 with stochastic gradient descent.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,154.09,141.59,326.51,7.86;7,154.08,151.55,155.11,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,280.77,141.59,167.97,7.86">Efficient algorithms for ranking with svms</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,457.17,141.59,23.43,7.86;7,154.08,151.55,65.78,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,161.52,326.51,7.86;7,154.08,171.48,326.51,7.86;7,154.08,181.44,326.51,7.86;7,154.08,191.40,326.51,7.86;7,154.08,201.37,326.51,7.86;7,154.08,211.33,20.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,417.18,161.52,63.41,7.86;7,154.08,171.48,69.92,7.86">IT-Discovery at TREC 2010 Legal</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cordover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Borden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Strickland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,419.25,171.48,61.34,7.86;7,154.08,181.44,326.51,7.86;7,154.08,191.40,326.51,7.86;7,154.08,201.37,299.43,7.86">The Nineteenth Text REtrieval Conference Proceedings (TREC 2010). National Institute of Standards and Technology (NIST) the Defense Advanced Research Projects Agency (DARPA) and the Advanced Research and Development Activity (ARDA)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,221.29,326.50,7.86;7,154.08,231.26,326.51,7.86;7,154.08,241.22,326.51,7.86;7,154.08,251.18,326.51,7.86;7,154.08,261.14,326.51,7.86;7,154.08,271.11,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,282.94,221.29,197.64,7.86;7,154.08,231.26,73.82,7.86">Applying Latent Semantic Indexing on the TREC 2010 Legal Dataset</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kontostathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,419.57,231.26,61.02,7.86;7,154.08,241.22,326.51,7.86;7,154.08,251.18,326.51,7.86;7,154.08,261.14,299.43,7.86">The Nineteenth Text REtrieval Conference Proceedings (TREC 2010). National Institute of Standards and Technology (NIST) the Defense Advanced Research Projects Agency (DARPA) and the Advanced Research and Development Activity (ARDA)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,281.07,326.51,7.86;7,154.08,291.03,326.51,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,330.61,281.07,145.81,7.86">The Elements of Statistical Learning</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,154.08,291.03,109.48,7.86">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer New York Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,300.99,326.50,7.86;7,154.08,310.96,125.32,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,310.99,300.99,169.60,7.86;7,154.08,310.96,56.18,7.86">Learning to rank from relevance feedback for e-discovery</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lubell-Doughtie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,229.68,310.96,20.52,7.86">ECIR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,320.92,326.50,7.86;7,154.08,330.88,298.39,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,318.11,320.92,162.48,7.86;7,154.08,330.88,124.99,7.86">Trec-chem: large scale chemical information retrieval evaluation at trec</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,286.75,330.88,53.49,7.86">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2009-12">December 2009</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,340.84,326.51,7.86;7,154.08,350.81,101.89,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,231.31,340.84,206.42,7.86">Adaptive relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,455.13,340.84,25.46,7.86;7,154.08,350.81,11.09,7.86">CIKM &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,360.77,326.50,7.86;7,154.08,370.73,171.02,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,350.40,360.77,130.19,7.86;7,154.08,370.73,24.37,7.86">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.09,380.70,326.51,7.86;7,154.08,390.66,218.53,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,280.22,380.70,200.37,7.86;7,154.08,390.66,86.16,7.86">Exploiting session context for information retrieval -a comparative study</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luxenburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,259.87,390.66,20.52,7.86">ECIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="652" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.08,400.62,326.52,7.86;7,154.08,410.58,326.51,7.86;7,154.08,420.55,326.51,7.86;7,154.08,430.51,326.51,7.86;7,154.08,440.47,326.51,7.86;7,154.08,450.43,118.77,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,401.02,400.62,79.58,7.86;7,154.08,410.58,157.83,7.86">DUTH does Probabilities of Relevance at the Legal Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">S</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,186.14,420.55,294.45,7.86;7,154.08,430.51,326.51,7.86;7,154.08,440.47,326.51,7.86;7,154.08,450.43,67.64,7.86">The Nineteenth Text REtrieval Conference Proceedings (TREC 2010). National Institute of Standards and Technology (NIST) the Defense Advanced Research Projects Agency (DARPA) and the Advanced Research and Development Activity (ARDA)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.08,460.40,326.51,7.86;7,154.08,470.36,135.08,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,211.69,460.40,225.07,7.86">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Amherst, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="7,154.08,480.32,326.52,7.86;7,154.08,490.28,326.51,7.86;7,154.08,500.25,326.51,7.86;7,154.08,510.21,326.51,7.86;7,154.08,520.17,230.98,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,210.83,480.32,230.36,7.86">Learning Task Experiments in the TREC 2010 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,309.27,490.28,171.32,7.86;7,154.08,500.25,326.51,7.86;7,154.08,510.21,326.51,7.86;7,154.08,520.17,179.85,7.86">The Nineteenth Text REtrieval Conference Proceedings (TREC 2010). National Institute of Standards and Technology (NIST) the Defense Advanced Research Projects Agency (DARPA) and the Advanced Research and Development Activity (ARDA)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.08,530.13,326.52,7.86;7,154.08,540.10,222.95,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,214.93,530.13,233.03,7.86">Evaluation of an inference network-based retrieval model</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Turtle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,459.15,530.13,21.45,7.86;7,154.08,540.10,149.51,7.86">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="187" to="222" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.08,550.06,326.52,7.86;7,154.08,560.02,326.51,7.86;7,154.08,569.99,74.80,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,445.11,550.06,35.48,7.86;7,154.08,560.02,66.50,7.86">Learning to Rank QA data</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raaijmakers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Theijssen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Boves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,237.87,560.02,238.44,7.86">Proceedings of the Learning to Rank workshop at SIGIR 2009</title>
		<meeting>the Learning to Rank workshop at SIGIR 2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.08,579.95,326.51,7.86;7,154.08,589.91,288.91,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,454.33,579.95,26.26,7.86;7,154.08,589.91,166.55,7.86">Learning to Rank for Why-Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Raaijmakers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Theijssen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Boves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,329.18,589.91,85.91,7.86">Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,154.08,599.87,326.52,7.86;7,154.08,609.84,184.01,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,301.39,599.87,179.20,7.86;7,154.08,609.84,33.00,7.86">A study of methods for negative relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,206.81,609.84,40.49,7.86">SIGIR &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
