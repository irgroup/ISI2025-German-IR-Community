<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.76,155.44,289.73,14.93">USC/ISI at TREC 2011: Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.94,187.75,75.36,10.37"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.55,187.75,66.76,10.37"><forename type="first">Congxing</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California Marina del Rey</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.76,155.44,289.73,14.93">USC/ISI at TREC 2011: Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFD63DDB4DACE3B8E1C0AE67D498F340</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the search system we developed for the inaugural TREC 2011 Microblog Track. Our system makes use of best-practice ranking techniques, including term, phrase, and proximity-based text matching via the Markov random field model, pseudo-relevance feedback using Latent Concept Expansion, and a feature-based ranking model that uses a simple, but effective learningto-rank model. We adapted each of these approaches to the specifics of the microblog search task, giving rise to a highly effective end-to-end search system. The official results from the TREC evaluation suggest that pseudorelevance feedback and learning-to-rank yield significant improvements in precision at early rank under different evaluation scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microtexts represent a relatively new, but nearly ubiquitous communication medium that has begun replacing traditional "long" forms of communications, such as email and blogs. Popular examples of microtexts include SMS text messages, status updates, and microblog messages. Such texts exhibit a number of unique properties that necessitate the development of novel ranking techniques and evaluation methodologies. In order to catalyze research along these directions, a new TREC track, called the Microblog Track, was run as part of TREC 2011.</p><p>The Microblog Track search task is technically defined as temporally-biased ad hoc search over a stream of microblog (Twitter) messages. That is, information needs are defined in terms of a keyword query and a temporal reference point. It is assumed that the user issued the keyword at the temporal reference point and is looking for microblog posts that contain information that is both recent and relevant.</p><p>This search task differs from previous TREC ad hoc search tasks (e.g., news search, Web search, etc.) in a number of important ways, thereby giving rise to a number of interesting research problems. We specifically focused on the following task-specific research challenges while developing our system:</p><p>• Very short documents. Microblog messages are short, by their very definition. For example, Twitter messages are limited to 140 characters of content. This hard limit, along with other contextual factors, causes users to make heavy use of abbreviations, phonetically shorten terms, drop vowels, etc <ref type="bibr" coords="1,346.47,500.31,10.58,8.64" target="#b3">[4]</ref>. Therefore, microblog messages exhibit a great deal of lexical variation that only exacerbates the vocabulary mismatch problem that plagues information retrieval systems. To help overcome this issue, we use pseudo-relevance feedback to build an expanded, lexically richer query representation.</p><p>• Highly varied document quality. User generated content, including microblog messages, vary greatly in terms of content quality. While some authors pride themselves in producing high quality content, others create content that is barely decipherable. Since it is unlikely that low quality messages will yield much valuable information, we developed a number of features that quantify the quality of microblog content.</p><p>• Language identification issues. Microblogs are inherently multi-lingual. However, for the purpose of this track, all non-English messages were considered non-relevant. Therefore, accurate language identification is important, not only for this track, but for real microblog search systems, as well. Instead of imposing a hard filtering of the messages, we used the confidence score output by a highly effective SVM-based English language classifier as a feature within our ranking functions.</p><p>• Temporally-biased queries. The track required that results satisfy two temporal conditions. First, only tweets that were issued before the query's temporal reference point could be returned. Second, the results must be returned in reverse chronological order (i.e., most recent first). To handle these requirements, we made use of a modified version of the Indri search engine <ref type="bibr" coords="2,165.81,345.96,15.27,8.64" target="#b11">[12]</ref>. Our modifications also ensured that all collection statistics were computed using only "past" information.</p><p>• Retrieval metrics. Recency and relevance are both critical factors when evaluating microblog search results. However, given the insufficiency of existing metrics, the official evaluation metric of this year's track was precision at rank 30 (P@30). This metric focuses entirely on relevance and completely ignores recency. Therefore, all of our models were optimized for P@30. However, it is important to note that the learning framework we use to tune our models is flexible enough to easily optimize for other metrics, such as those that combine relevance and recency.</p><p>• Lack of training data. Since this is the first year that the Microblog Track was run, no existing training data was available to help tune the parameters of our ranking functions. To overcome this issue, we recruited a few volunteers to help us construct a small training set of 15 queries and 346 binary judgments. Although small, the training set was successfully used to learn effective learning-to-rank models.</p><p>The remainder of this paper describes the details of our system and how we addressed each of these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Given that this was the first year that the Microblog Track was run, we focused almost exclusively on establishing a highly effective baseline system that leverages existing state-of-the-art retrieval approaches and upon which more advanced capabilities and improvements can be built moving forward. Along these lines, the following approaches were used:</p><p>• The Markov random field retrieval model <ref type="bibr" coords="2,527.63,241.80,11.62,8.64" target="#b8">[9]</ref> forms the basis of our text-based scoring functions (Section 2.1).</p><p>• Latent Concept Expansion <ref type="bibr" coords="2,453.52,285.43,16.60,8.64" target="#b9">[10]</ref> is used to help overcome the vocabulary mismatch problem (Section 2.2).</p><p>• A simple, but effective learning-to-rank model <ref type="bibr" coords="2,527.63,329.06,11.62,8.64" target="#b5">[6]</ref> is used to combine evidence from multiple features (Section 2.3).</p><p>Our four official runs represent different combinations of these basic approaches. We now provide a brief description of each approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Scoring</head><p>Our text-based scoring function makes use of the Markov random field retrieval model <ref type="bibr" coords="2,428.20,455.46,10.58,8.64" target="#b8">[9]</ref>. The model can capture dependencies between terms and provides a formal and highly effective framework for combining scores from term, phrase, and proximity-based text matching features.</p><p>Specifically, in this work, we use the full dependence variant of the MRF model, which assumes that all terms are dependent on each other <ref type="bibr" coords="2,425.15,527.19,10.58,8.64" target="#b8">[9]</ref>. Rather than describe the technical details of the model, we provide an example of how the model is applied to the query moscow airport bombing (Topic 36). It can be shown the following query, expressed in the Indri query language <ref type="bibr" coords="2,468.65,575.01,15.27,8.64" target="#b11">[12]</ref>, ranks documents according to the full dependence model <ref type="bibr" coords="2,496.82,586.97,10.79,8.64" target="#b8">[9]</ref>: #weight( 0.8 #combine(moscow airport bombing) 0.1 #combine(#1(airport bombing) #1(moscow airport) #1(moscow airport bombing)) 0.1 #combine(#uw8(airport bombing) #uw8(moscow bombing) #uw8(moscow airport) #uw12(moscow airport bombing)))</p><p>where #weight and #combine are Indri query language operators that combine scores from term matches, phrase matches (#1), and proximity matches (#uwN). It has been empirically shown that this particular way of combining text matching evidence is highly effective for a variety of tasks <ref type="bibr" coords="3,142.03,233.01,10.58,8.64" target="#b7">[8]</ref>.</p><p>Readers who may be familiar with the MRF model will note that most previous studies have used the sequential dependence variant of the model, since it typically achieves comparable effectiveness to the full dependence variant, but is substantially more efficient at runtime. However, our preliminary experiments suggested that the full dependence model yielded superior results compared to the sequential dependence model. We hypothesize this is the case because the full dependence model promotes tweets that match as many query terms and as many exact matching subsequences of query terms as possible, whereas the sequential dependence model is less aggressive since it only looks at phrase and proximity features defined over adjacent query terms. While the sequential dependence model may suffice for long documents, our findings suggest that the full dependence model is better for ranking short noisy documents. In the future, we plan to undertake a more detailed empirical evaluation to develop a better understanding of this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Expansion</head><p>As we described in the introduction, the fact that Twitter messages are so short only exacerbates the so-called vocabulary mismatch problem. The most common approaches for overcoming the lexical gap between queries and documents (tweets) are query expansion and document expansion. In this work, we focus our attention on query expansion. To expand queries, we make use of Latent Concept Expansion (LCE), an effective pseudorelevance feedback technique developed specifically for the MRF retrieval framework <ref type="bibr" coords="3,193.48,618.07,15.27,8.64" target="#b9">[10]</ref>. LCE is a generalization of relevance-based language models <ref type="bibr" coords="3,238.49,630.02,11.62,8.64" target="#b4">[5]</ref> that permits dependencies between terms to be modeled and arbitrary features to be used for pseudo-relevance feedback.</p><p>To illustrate the potential benefits of query expansion within the microblog domain, consider the query oprah winfrey half sister (Topic 13), which concerns the revelation that media mogul Oprah Winfrey had a half-sister she never knew about. For this query, the top 10 expansion terms returned by LCE are:</p><p>"oprah", "winfrey", "she", "secret", "AP" (Associated Press), "family", "harvey", "reveal", "watching", "announce"</p><p>To understand why these terms are chosen by LCE, consider what a typical tweet on this subject may look like. For example, it is likely to have the form "Oprah Winfrey [announced | revealed] that she has a half-sister (AP news)". Therefore, these terms are capturing the most salient terms (named entities, pronouns, news sources, verbs, etc.) that are used when describing this particular topic. By expanding the original query with these terms, it is possible to identify tweets that may not contain all (or any) of the original query terms, but are still relevant. All of our LCE-based runs use 100 feedback tweets, 10 expansion terms, and weight the contribution of the original query and expansion terms equally. Based on a preliminary analysis of our results, we believe that the LCE parameters used were far from optimal, which likely limited the potential gains of the technique. As part of future work we will undertake a more detailed tuning of the parameters to unlock the full potential of the approach.</p><p>We also hypothesize that external sources of expansion evidence, such as Wikipedia, query logs, and temporallyaligned news corpora, would also be useful for constructing expanded query representations. We also believe that temporally-biased expansion models, such as the one proposed by Massoudi et al., could prove to be effective <ref type="bibr" coords="3,525.14,511.81,10.58,8.64" target="#b6">[7]</ref>. These are areas of potentially fruitful future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning-to-Rank</head><p>Up until this point, we have focused exclusively on approaches for computing highly effective text matching scores for Twitter messages. However, as we described in the introduction, tweets exhibit a high variance in content quality and are written in a variety of languages beyond English. Therefore, it is necessary to combine multiple sources of evidence (e.g., text scores, content quality scores, language identification confidences, etc.) to facilitate effective microblog ranking.</p><p>There are many different ways to combine evidence, including result set fusion <ref type="bibr" coords="4,169.78,140.23,10.58,8.64" target="#b2">[3]</ref>, inference networks <ref type="bibr" coords="4,264.72,140.23,15.27,8.64" target="#b12">[13]</ref>, and learning-to-rank approaches <ref type="bibr" coords="4,188.40,152.19,10.58,8.64" target="#b5">[6]</ref>. In this work, we use a simple, but effective learning-to-rank approach for combining evidence from multiple features. Another reason for choosing this particular paradigm is because learningto-rank approaches have been shown to be effective for Twitter search in the past <ref type="bibr" coords="4,174.41,211.97,10.58,8.64" target="#b1">[2]</ref>.</p><p>The remainder of this section describes the learning-torank model we used, our feature set, and how the model parameters are estimated, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Model</head><p>We utilize a simple linear learning-to-rank model. Given a query Q and a tweet D, the model computes a relevance score s(Q, D) according to:</p><formula xml:id="formula_0" coords="4,132.08,340.46,108.48,30.32">s(Q, D) = N i λ i f i (Q, D)</formula><p>where N is the total number of features, f i (Q, D) is a feature function, and λ i is a model parameter. Given a query Q, tweets D are ranked in descending order of their relevance score s(Q, D).</p><p>To instantiate the model, we must define a set of features (f (Q, D)) and estimate the model parameters (λ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Features</head><p>As we will describe in more detail in the next section, the Microblog Track required participants to crawl their own data set. Two versions of the crawler were available. One version required special access to theTwitter APIs and provided a feature-rich JSON representation of each tweet, while the other crawler downloaded a barebones HTML representation of the tweets. Since we did not have special access to the Twitter API, we download the basic HTML version of the data, which limited the types of features we could use within our model. We used the following set of features, some of which were inspired by the work of Duan et al. <ref type="bibr" coords="4,235.08,621.68,10.58,8.64" target="#b1">[2]</ref>, while others are novel:</p><p>• text score(Q,D) -Text matching feature. For runs that do not use LCE, this is the MRF model's full dependence score computed for the query Q and tweet D. For runs that use LCE, this is the score of the expanded query with respect to tweet D.</p><p>• tdiff(Q,D) -Time difference feature. The difference in time, as measure in seconds, between the query Q's temporal reference point and tweet D's timestamp.</p><p>• has hashtag(D) -Does the tweet D contain a hashtag? (binary valued)</p><p>• has url(D) -Does the tweet D contain a URL? (binary valued)</p><p>• length(D) -The length (number of terms) of the tweet D.</p><p>• oov pct(D) -The percentage of terms in tweet D that are out-of-vocabulary (OOV). We use the English Aspell dictionary as our vocabulary.</p><p>• is reply(D) -Is the the tweet D a reply to another tweet? (binary valued)</p><p>• english prob(D) -The confidence score of our English language classifier. The classifier is a Support Vector Machine (SVM) model trained using a small set of manually labeled tweets. It uses character trigrams and average word length as features, and achieves an accuracy of around 93%.</p><p>Due to time restrictions, we did not use the number of times a tweet was re-tweeted or any user-specific information (e.g., authority scores) as features within our model. As part of future work, we are interested in expanding our feature set and developing a better understanding of the relative importance of different types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Parameter Estimation</head><p>Since our model only has 8 features, there is no need to resort to overly sophisticated learning-to-rank parameter estimation strategies. Indeed, for such simple models, it is likely best to keep things as simple as possible. In the spirit of simplicity, all of our learning-to-rank models are learned using using a simple coordinate-level ascent approach that directly optimizes P@30, the official Microblog Track metric <ref type="bibr" coords="4,396.85,666.26,15.27,8.64" target="#b10">[11]</ref>. Since this is the first time that the TREC Microblog Track was run, we did not have access to any data from which we could train our models. To alleviate this problem, we recruited a few volunteers from within our organization to help us construct a small set of training data. Each volunteer was asked to issue one or more queries (without any knowledge of the TREC Microblog test queries) to a prototype microblog search engine, which ranked tweets based on the text score alone. The volunteers were then asked to annotate the relevance of the top 30 results returned for each query. All relevance judgments were binary and users were allowed to skip (i.e., not judge) results that they were unsure of. This yielded a total of 15 training queries and 346 judgments. Although this training data set is extremely small, it contained enough signal to distill a relatively effective learning-torank model, as we will show in the next section.</p><p>Using this small training set, we learned the linear ranking function presented in Table <ref type="table" coords="5,208.52,497.19,3.74,8.64" target="#tab_0">1</ref>. We use this ranking function for all of our learning-to-rank runs. By inspecting the weights of the model, we see that the text score feature provides the most positive evidence in favor of relevance. Other positive indicators of relevance include the has url (which was also observed by Duan et al. <ref type="bibr" coords="5,120.58,568.92,10.45,8.64" target="#b1">[2]</ref>), english prob, and has hashtag. On the other hand, the learned model suggests that oov pct and is reply are negative indicators of relevance. We found these weights to be intuitive and to match our expectations.</p><p>It is worth noting that two features, tdiff and length were assigned a weight of 0, which suggests that they do not provide strong evidence in favor of or against relevance. We suspect that if the metric being optimized for included a recency component, then the tdiff feature would play a more important role in the model. As for the length feature, it is likely that most tweets are more or less the same length, and hence length on its own is not a strong relevance signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>This section describes our experiences with downloading the Twitter corpus, our experimental methodology, and an overview of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Methodology</head><p>As mentioned earlier, the Microblog Track was unique because it required participants to crawl/download the data set on their own. This was necessary to abide by Twitter's terms of service. The track organizers provided participants with a list of user and tweet IDs that make up the data set. They also provided helper scripts for downloading the data set. If a participating group had special access to Twitter's APIs, then they could download the data in a feature-rich JSON format. All other participants had to download the data in a bare-bones HTML format that contains less information than the JSON format. Since we do not have elevated access to Twitter's APIs, we downloaded an HTML version of the data set.</p><p>Our crawl was performed in late May 2011 on an Intel i7 processor, 16GB of RAM, running Fedora 12 with a 10GBps ethernet connection. We used the provided HTML crawler, which downloaded tweets at a rate of approximately 1 block of 10k tweets every 8 minutes.</p><p>Figure <ref type="figure" coords="5,351.87,642.35,4.98,8.64" target="#fig_0">1</ref> shows the distribution of HTTP response codes returned during crawling. The response codes are summarized as follows: • 200 (OK) -a successfully downloaded tweet.</p><p>• 302 (Found) -a successfully downloaded re-tweet (via a redirect).</p><p>• 403 (Forbidden) -the user has disabled public sharing of their tweets.</p><p>• 404 (Not found) -the user account no longer exists.</p><p>Based on the download statistics, we see that a vast majority (over 97%) of the tweets were successfully downloaded, while only a small fraction were no longer accessible. Table <ref type="table" coords="6,120.52,474.19,4.98,8.64" target="#tab_1">2</ref> shows a detailed count of each type, as well as the total number of searchable tweets (15,736,358) in our version of the corpus. A set of 50 test topics was released for evaluation purposes. Each topic consists of a keyword query and a temporal reference point, which acts as a query timestamp. NIST employed a pooling strategy to obtain ground truth for evaluation purposes. There were no relevant items for Topic 50 and it was therefore eliminated from the topic pool. Furthermore, only 33 topics had tweets that were judged to be highly relevant.</p><p>All of our submissions were run using the Indri search engine <ref type="bibr" coords="6,102.16,618.44,15.27,8.64" target="#b11">[12]</ref>. Indri was used because it was particularly suitable for the specifics of the task. In particular, we used Indri's numeric field support to annotate each tweet with its timestamp. At query time, we could then issue queries of the form: We utilized the following procedure to rank tweets in response to a query according to the Microblog Track guidelines. First, we retrieve the 1000 most relevant tweets according to the text score feature. Second, we filter out all re-tweets (i.e., tweets with HTML status code 302 or those that begin with the string "RT"). Then, if this is a learning-to-rank run, we re-order the non-filtered tweets based on the learned model (Table <ref type="table" coords="6,528.46,354.57,3.60,8.64" target="#tab_0">1</ref>). Next, we truncate the (relevance-ordered) ranked and retain only the top 30 results. Finally, we return the top 30 results in reverse chronological order.</p><p>It is very important to note that our system only returns at most 30 results per query. This was part of our strategy to optimize every aspect of our system for the official evaluation metric (P@30). Hence, it makes little-to-no sense to evaluate our runs based on measures like MAP. We chose this particular strategy because it is optimal for precision at rank 30. Indeed, if our system had returned more than 30 results per query, it would face the risk of introducing non-relevant (or less relevant) documents into the top 30 during the process of sorting the tweets in reverse chronological order. If future evaluations use a different metric of interest, such as one that combines relevance and recency, then this particular strategy would unlikely yield satisfactory results.</p><p>Table <ref type="table" coords="6,348.27,570.34,4.98,8.64" target="#tab_2">3</ref> summarizes our official runs and the approaches used by each. If a run ID contains "FD" then it makes use of the MRF full dependence model, if the ID contains "RM" then it makes use of Latent Concept Expansion, and if it contains "L" then it uses learning-torank.</p><p>Finally, we note that none of our runs made use of external or future data. The only data that may be construed as "external", depending on your point of view, are the relevance judgments obtained for training our learning-torank models and the Aspell dictionary for detecting out of vocabulary terms. However, we view these more as "basic" resources than "external" ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We now describe the results from our four official runs.</p><p>The results are summarized in Table <ref type="table" coords="7,220.59,415.18,3.74,8.64" target="#tab_3">4</ref>. The evaluation is broken down into two separate sets of metrics. The first considers all queries that had at least one tweet judged relevant (denoted "AllRel."). The second only considers highly relevant tweets as relevant, and is averaged over the 33 topics that had such judgments (denoted "HighRel" ). The metric reported is precision at 30. If we consider all queries, then we see that the isiFDL run, which makes use of the MRF retrieval model and learning-to-rank performs better than all of the other runs. Interestingly, while pseudo-relevance feedback showed some improvements, it did not perform as well as the isiFDL run. The difference between isiFDL and isiFD represents the only statistically significant difference amongst all pairs of runs.</p><p>When we only consider highly relevant judgments, our findings are substantially different. Under this scenario, the isiFDRM run, which combines the MRF and LCE outperforms all other methods, including those that make use of learning-to-rank. Here, only the difference between isiFDRM and isiFD is statistically significant amongst all pairs of runs.</p><p>Therefore, the results are mixed and not altogether conclusive. We had originally hypothesized that both learning-to-rank and pseudo-relevance feedback would be effective on their own, which does indeed turn out to be true. However, we had also hypothesized that the gains would be additive and that isiFDRML would consistently outperform all of the other runs. However, this was simply not the case.</p><p>There are a number of possible explanations for these findings. First, the relevance judgments were obtained using isiFD as a base ranking function. Therefore, the judgments may not be suitable for training pseudorelevance feedback-based models. Second, we may not have had enough judgments to adequately train a learningto-rank model in the first place. Third, it may be that pseudo-relevance feedback is not useful for finding "somewhat relevant" tweets, but is actually quite useful for finding "highly relevant" tweets. It would be valuable to understand this better from a risk/reward tradeoff point of view <ref type="bibr" coords="7,342.93,355.84,10.58,8.64" target="#b0">[1]</ref>. Finally, the "Highly Relevant" results are averaged over 33 queries, which is a very small sample size, and hence it is inappropriate to draw strong conclusions from the results. We intend to dig deeper into these issues in the future to develop a better understanding the pros and cons of the various approaches employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our experiments at the inaugural TREC Microblog Track focused on developing a strong baseline system, based on best-practice approaches, upon which we can build novel, effective approaches in the future. In particular, we made use of the Markov random field model (full dependence variant) for text scoring, Latent Concept Expansion for pseudo-relevance feedback, and learning-to-rank to combine evidence a variety of features (text, content quality, language identification, etc.).</p><p>Our experimental results showed that both learningto-rank and pseudo-relevance feedback approaches were effective. However, we did not observe additive gains across the two approaches. As future work, we plan to understand the relationship between pseudo-relevance feedback and learning-to-rank, to develop novel microblogspecific features, and to investigate unsupervised methods for automatically training microblog ranking functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,73.31,295.92,226.03,8.64;6,118.61,153.85,165.48,101.99"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of HTML crawler response codes.</figDesc><graphic coords="6,118.61,153.85,165.48,101.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,310.61,129.22,45.16,7.01;6,318.08,140.23,106.54,8.64;6,318.08,152.19,178.09,8.64;6,363.24,164.14,125.79,8.64;6,429.32,176.10,54.19,8.64;6,429.32,188.06,85.44,8.64;6,363.24,200.01,137.74,8.64;6,429.32,211.97,78.41,8.64;6,429.32,223.92,66.14,8.64;6,429.32,235.88,110.01,8.64"><head></head><label></label><figDesc>#filreq( #less(time 1297191104) #weight(0.8 #combine(2022 fifa soccer) 0.1 #combine(#1(fifa soccer) #1(2022 fifa) #1(2022 fifa soccer)) 0.1 #combine(#uw8(fifa soccer) #uw8(2022 soccer) #uw8(2022 fifa) #uw12(2022 fifa soccer))))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,87.93,127.08,196.79,126.42"><head>Table 1 :</head><label>1</label><figDesc>Learning-to-rank model feature weights.</figDesc><table coords="5,130.32,127.08,112.01,104.68"><row><cell>Feature</cell><cell>Weight</cell></row><row><cell>text score</cell><cell>0.5549</cell></row><row><cell>tdiff</cell><cell>0</cell></row><row><cell>has hashtag</cell><cell>0.0203</cell></row><row><cell>has url</cell><cell>0.1218</cell></row><row><cell>length</cell><cell>0</cell></row><row><cell>oov pct</cell><cell>-0.1218</cell></row><row><cell>is reply</cell><cell>-0.1218</cell></row><row><cell cols="2">english prob 0.0593</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,310.61,127.08,228.64,102.90"><head>Table 2 :</head><label>2</label><figDesc>Tweets2011 corpus summary statistics (HTML version, crawled in May 2011).</figDesc><table coords="5,360.83,127.08,128.19,69.21"><row><cell>Tweet Type</cell><cell>Count</cell></row><row><cell>200 (OK)</cell><cell>14,579,587</cell></row><row><cell>302 (Found)</cell><cell>1,156,771</cell></row><row><cell>404 (Not found)</cell><cell>289,886</cell></row><row><cell>403 (Forbidden)</cell><cell>115,568</cell></row><row><cell cols="2">Total (searchable) 15,736,358</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,127.08,228.64,132.81"><head>Table 3 :</head><label>3</label><figDesc>Summary of the approaches used by our four official runs.</figDesc><table coords="7,77.98,127.08,218.08,132.81"><row><cell cols="2">Run ID</cell><cell cols="2">Approaches Used</cell><cell></cell></row><row><cell cols="2">isiFD</cell><cell></cell><cell>MRF</cell><cell></cell></row><row><cell cols="2">isiFDL</cell><cell cols="2">MRF + learning-to-rank</cell><cell></cell></row><row><cell cols="2">isiFDRM</cell><cell></cell><cell>MRF + LCE</cell><cell></cell></row><row><cell cols="5">isiFDRML MRF + LCE + learning-to-rank</cell></row><row><cell>Criteria</cell><cell cols="4">isiFD isiFDL isiFDRM isiFDRML</cell></row><row><cell>AllRel</cell><cell>.4361</cell><cell>.4551</cell><cell>.4476</cell><cell>.4442</cell></row><row><cell>HighRel</cell><cell>.1384</cell><cell>.1434</cell><cell>.1566</cell><cell>.1556</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.00,278.07,228.64,20.59"><head>Table 4 :</head><label>4</label><figDesc>Precision at rank 30 for each official run under the two relevance criteria.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Dirk Hovy</rs> for providing access to his Twitter English language classifier. We would also like to thank the volunteers that helped us construct a small set of training data.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,93.58,250.52,207.06,8.64;8,93.58,262.30,207.06,8.82;8,93.58,274.25,207.06,8.59;8,93.58,286.21,166.49,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,182.31,250.52,118.34,8.64;8,93.58,262.48,174.53,8.64">Accounting for stability of retrieval algorithms using risk-reward curves</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,93.58,274.25,207.06,8.59;8,93.58,286.21,82.32,8.59">Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation</title>
		<meeting>the SIGIR 2009 Workshop on the Future of IR Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="27" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,309.85,207.06,8.64;8,93.58,321.81,207.06,8.64;8,93.58,333.58,207.06,8.59;8,93.58,345.72,87.44,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,93.58,321.81,191.28,8.64">An empirical study on learning to rank of tweets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,93.58,333.58,203.21,8.59">Proc. 23rd Intl. Conf. on Computational Linguistics</title>
		<meeting>23rd Intl. Conf. on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="295" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,369.18,207.06,8.64;8,93.58,380.95,207.06,8.82;8,93.58,393.09,87.44,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,201.73,369.18,98.91,8.64;8,93.58,381.13,32.21,8.64">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,147.82,380.95,148.45,8.59">Proc. 2nd Text REtrieval Conference</title>
		<meeting>2nd Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,416.55,207.06,8.64;8,93.58,428.50,207.06,8.64;8,93.58,440.28,207.07,8.82;8,93.58,452.24,207.07,8.82;8,93.58,464.37,207.06,8.64;8,93.58,476.33,32.94,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,268.69,416.55,31.95,8.64;8,93.58,428.50,202.59,8.64">Contextual bearing on linguistic variation in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.99,440.28,194.66,8.59;8,93.58,452.24,93.65,8.82">Proceedings of the Workshop on Languages in Social Media, LSM &apos;11</title>
		<meeting>the Workshop on Languages in Social Media, LSM &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="8,93.58,499.79,207.06,8.64;8,93.58,511.56,207.06,8.82;8,93.58,523.52,207.06,8.59;8,93.58,535.47,128.80,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,215.47,499.79,85.17,8.64;8,93.58,511.74,53.39,8.64">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,167.07,511.56,133.57,8.59;8,93.58,523.52,207.06,8.59;8,93.58,535.47,34.98,8.59">Proc. 24th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>24th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,559.11,207.06,8.64;8,93.58,570.89,207.06,8.59;8,93.58,583.02,43.99,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,134.53,559.11,162.60,8.64">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,93.58,570.89,203.18,8.59">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.58,606.49,207.06,8.64;8,93.58,618.44,207.06,8.64;8,93.58,630.40,207.06,8.64;8,93.58,642.17,207.06,8.59;8,93.58,654.13,207.06,8.82;8,93.58,666.26,113.33,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,159.29,618.44,141.35,8.64;8,93.58,630.40,189.05,8.64">Incorporating query expansion and quality indicators in searching microblog posts</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Massoudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,93.58,642.17,207.06,8.59;8,93.58,654.13,66.68,8.82">Proc. 33rd European Conf. on Information Retrieval, ECIR&apos;11</title>
		<meeting>33rd European Conf. on Information Retrieval, ECIR&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,332.18,128.10,207.07,8.82;8,332.18,140.06,207.06,8.82;8,332.18,152.19,22.42,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,383.08,128.10,156.17,8.59;8,332.18,140.06,34.98,8.59">A Feature-Centric View of Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,379.94,140.23,114.92,8.64">Information Retrieval Series</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,332.18,172.11,207.06,8.64;8,332.18,183.89,207.06,8.82;8,332.18,195.85,207.06,8.59;8,332.18,207.80,207.07,8.82;8,332.18,219.94,22.42,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,461.73,172.11,77.52,8.64;8,332.18,184.07,139.71,8.64">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,496.28,183.89,42.97,8.59;8,332.18,195.85,207.06,8.59;8,332.18,207.80,138.37,8.59">Proc. 28th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>28th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,332.18,239.86,207.06,8.64;8,332.18,251.64,207.06,8.82;8,332.18,263.59,207.06,8.59;8,332.18,275.55,145.96,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,450.09,239.86,89.15,8.64;8,332.18,251.82,128.81,8.64">Latent concept expansion using Markov random fields</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,477.46,251.64,61.79,8.59;8,332.18,263.59,207.06,8.59;8,332.18,275.55,117.17,8.59">Proc. 30th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>30th Ann. Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,332.18,295.65,207.06,8.64;8,332.18,307.43,207.07,8.82;8,332.18,319.38,117.46,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,455.99,295.65,83.26,8.64;8,332.18,307.61,130.52,8.64">Linear feature-based models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,474.02,307.43,65.24,8.59;8,332.18,319.38,24.81,8.59">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="274" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,332.18,339.49,207.06,8.64;8,332.18,351.44,207.06,8.64;8,332.18,363.22,207.07,8.82;8,332.18,375.17,196.83,8.82" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,332.18,351.44,207.06,8.64;8,332.18,363.40,65.88,8.64">Indri: A language model-based serach engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,424.22,363.22,115.03,8.59;8,332.18,375.17,167.89,8.59">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,332.18,395.28,207.06,8.64;8,332.18,407.05,207.07,8.82;8,332.18,419.01,180.70,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,435.79,395.28,103.46,8.64;8,332.18,407.23,120.76,8.64">Evaluation of an inference network-based retrieval model</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,463.84,407.05,75.42,8.59;8,332.18,419.01,92.32,8.59">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="222" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
