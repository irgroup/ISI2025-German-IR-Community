<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.82,117.24,301.71,13.16;1,136.72,138.05,341.90,9.17;1,252.43,155.99,109.39,9.17">WISTUD at TREC 2011 Microblog Track: Exploiting Background Knowledge from DBpedia and News Articles for Search on Twitter</title>
				<funder ref="#_GYgnUbu">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ZxgM2jw">
					<orgName type="full">European Union Seventh Framework Programme</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,228.60,190.26,29.81,9.52"><forename type="first">Ke</forename><surname>Tao</surname></persName>
							<email>k.tao@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Web Information Systems</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.51,190.26,51.39,9.52"><forename type="first">Fabian</forename><surname>Abel</surname></persName>
							<email>f.abel@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Web Information Systems</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.87,190.26,60.89,9.52"><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
							<email>c.hauff@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Web Information Systems</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.82,117.24,301.71,13.16;1,136.72,138.05,341.90,9.17;1,252.43,155.99,109.39,9.17">WISTUD at TREC 2011 Microblog Track: Exploiting Background Knowledge from DBpedia and News Articles for Search on Twitter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">712C201924AEF90AB89B814E62397AC5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>These working notes describe the system developed by the WISTUD team for the Microblog track. We evaluated the suitability of semantic technologies for the search task, in particular, query expansion with named entities that are deduced by means of a topic-based profiling process. The results indicate the feasibility of the approach: for half of the topics, at P@30, our top performing automatic method based on semantic profiling yields better results than the median over all submitted runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In TREC 2011, we participated in the Microblog track, which was run this year for the first time. A corpus of sixteen million Twitter messages (so-called tweets) was released together with 50 search topics.</p><p>The search task is defined as retrieving the interesting and relevant tweets for a given topic and a given time frame. In this task, we consider the language gap between the topics and the Twitter messages as a core challenge. Since tweets are limited to a maximum of 140 characters, Twitter users often rely on abbreviations and memes to convey their message <ref type="bibr" coords="1,362.69,465.60,9.96,9.52" target="#b0">[1]</ref>. Given a query such as "Roger Federer Wimbledon 2009", we expect to find very few tweets that contain all query concepts. We found this indeed to be the case in this year's search topics: only 18 of the provided 50 search topics yield ten or more result tweets in the corpus when used as conjunctive queries. To overcome this problem, we implemented approaches that expand the original topics with keywords, that are more in line with the type of language people use on Twitter. Our solutions are inspired by Twitter-based user modeling methods <ref type="bibr" coords="1,391.81,549.29,10.51,9.52" target="#b1">[2]</ref> and aim to create a semantically rich profile for a topic that is then translated into a weighted keyword query. Therefore, each query is modelled as a list of weighted concepts. The concepts, e.g. FIFA, Brazil, Barack Obama, Qatar, Sport, and Eurosport, are automatically derived with a Named-Entity-Recognition service from (i) news articles by mainstream media outlets, and (ii) from a set of phrases commonly used in tweets, so-called memes. It should be stressed that although we used a news corpus for query expansion, due to time constraints, we did not consider any of the web pages linked to in Twitter messages as evidence. Apart from the automatic runs submitted based on these topic modelling strategies, we also created one manual run based on searches performed by a human annotator.</p><p>While our automatic runs -as expected -performed less well than the manual run, we found our best automatic run to outperform the median result in 50% of the topics (P@30). The remainder of the paper is organized as follows: the search framework is described in detail in Section 2. The experimental setup is described in Section 3, followed by the results (Section 4). Section 5 discusses the results and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Search Framework</head><p>We interpret the interestingness of a tweet as the informativeness and coverage from different informations sources, such as news media, experts, and politicians. For each of the search topics, we derive a topic profile, which contains the concepts important to the topic and their corresponding weights. The concepts are derived from the enrichment process. The final query (the topic profile concepts and their weights) is submitted to the underlying retrieval engine<ref type="foot" coords="2,413.38,427.91,3.97,6.39" target="#foot_0">1</ref> . Returned is a ranked list of tweets, together with their corresponding probability scores of relevance. Since the result tweets -by definition of the task -are ranked according to their time stamp instead of their relevance scores (as is the case in standard ad-hoc retrieval tasks), we define a rank threshold K. Tweets with a relevance rank below K are discarded from the result list. Finally, the tweets remaining in the result list are ordered according to their time stamp from newest to oldest. Since up to 1000 tweets could be submitted for each topic, we appended the truncated result list with additional tweets which were older than the oldest tweet in the top K results.</p><p>An overview of our search framework is shown in Figure <ref type="figure" coords="2,397.76,548.69,3.87,9.52" target="#fig_0">1</ref>. It contains three types of components: main components (in blue), assisting components (yellow) and data (red). In the following sections, we describe the corpus preparation, the topic profiling and finally the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus Pre-Processing</head><p>As the corpus is distributed as a list of Twitter message IDs, we first had to crawl 2 the tweets. Apart from the message text itself, we also gain some important meta-data such as geographic locations, the number of times a tweet was re-tweeted, the users' self-introduction, etc.</p><p>In the pre-processing phase, we filter out all tweets of the corpus that are not identified as English since -by definition of the task -only English tweets will be judged for their relevance. This is a text categorization problem that can be solved with an N-gram based method <ref type="bibr" coords="3,342.15,179.54,9.96,9.52" target="#b2">[3]</ref>. The N-gram based method generates language profiles from a number of training documents of which the languages are already known. Each profile is a list of the most frequent Ngrams in the documents. By measuring the similarity (or distance) between the language profiles and the N-grams of each tweet (URLs were removed from the tweets, letters occurring in sequence 4+ times were replaced with a single letter), the most likely language of each tweet can be found. For our experiments, we relied on an existing language detection library<ref type="foot" coords="3,355.24,262.00,3.97,6.39" target="#foot_2">3</ref> . If a tweet was classified as being in English with a probability of at least 0.85, it was included in the reduced English-only corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Modeling</head><p>We model each topic as a topic profile which should provide an accurate and comprehensive summary of the topic. When users submit a query to search for information, they are often not able to include the specific information nugget they are looking for. For example, a user's intention behind submitting a query such as "2022 FIFA Soccer" may be to learn which country has won the bid for the 22 nd FIFA World Cup. If the user was searching for the result of the bid, he would obviously not have added the concept "Qatar" to his query. Our solution framework sets out to solve problems such as this one.</p><p>Topic profiles are supposed to be expandable so that more information can be integrated when available. To specify the importance of an element in the profile, a weight is assigned to each element (see Definition 1).</p><p>Definition 1 (Topic Profile). The profile P of a topic T is a set of weighted concepts t, determined by a set of strategies. Each concept has weight w s (t, T ), which is determined by the strategy s, and the topic T .</p><formula xml:id="formula_0" coords="3,239.10,518.71,137.17,16.60">P s (T ) = {(t, w s (t, T ))|t ∈ s(T )}</formula><p>In the topic profile, the knowledge gained from named-entity recognition, tweets, and news articles will be merged with varying importance weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Enrichment of Topics</head><p>The search topics are short and consist on average of 3.5 words. They contain abbreviations, part of names, and nicknames. One example (see Table <ref type="table" coords="3,243.30,590.62,4.42,9.52" target="#tab_0">1</ref>) is the first name "Jintao" (in example topic Exam-ple002: "Jintao visit US") which refers to the President of the People's Republic of China. However, in tweets he may also be referred to as "President Hu", "Chinese President", etc. If these variants of a person's name and titles are considered when building a topic profile, a wider variety of tweets can be found. We utilize the well-known named-entity recognition (NER) service DBPedia Spotlight<ref type="foot" coords="4,463.82,209.62,3.97,6.39" target="#foot_3">4</ref> to identify names and their synonyms from text snippets. A snippet can be a topic, a tweet or a Web page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Enrichment Based on Tweets &amp; News</head><p>The information from other sources, not just the original topic string, can also be utilized to enhance the topic profile. The named entities identified in the topics can be used to search in corpora for related entities. One obvious choice is of course the Twitter corpus itself: the entities identified in the topics are used to find related tweets, they are NER processed and the additional entities are added to the profile (selective automatic query expansion). Since the provided TREC example topics are often related to current news, we also consider an external corpus of news articles as additional source of topic enrichment. We collected the titles and abstracts of news articles written in the time frame of the Twitter corpus. For each topic, we extracted the headlines and abstracts that contained entities identified in the topic. Applying NER yielded another set of new entities.</p><p>Topic Profile Aggregation Having identified additional named entities to enrich the original topic, we aggregate the entities into the final profile. Different weights are attached to different sources. For example, a greater weight may be assigned to the given topics than the additional entities extracted from tweets and related news articles. We expect such a weighting scheme to reduce the amount of query drift considerably.</p><p>Topic Profile to Query Translation Since we extracted the formal names of the entities mentioned in the topics, related tweets, and news articles, the query can be expanded to a more comprehensive version. Different strategies are used when we consider to integrate the information from these different sources.</p><p>To reduce the risk of query drift, we performed the following steps:</p><p>-Each word in the topic is assigned weight w.</p><p>-If the annotated text in the topic contains more than one word, the name of the entity is appended with weight w. Otherwise it (i) is dropped, or (ii) it is appended with weight w 2 .</p><p>-Each word in the names of the extracted entities is appended with half the weight of the entities. This is useful when the name is not fully mentioned (see Table <ref type="table" coords="4,179.46,615.34,3.87,9.52" target="#tab_0">1</ref>). For example, "Jintao" is the first name of the Chinese president, but the name of the entity is "Hu Jintao". Sometimes he is also mentioned with his last name "Hu".</p><p>After expanding the query with the entities extracted from the topic profile, we submit the query to the search system and retrieve related tweets or related news articles, which are prerequisites for the further expansion of the query.</p><p>Using Top Entities in Related Tweets. We integrate the information from related tweets in order to learn how users talk about a topic, which words they are using, which people they are referring to and so on.</p><p>The related tweets are the tweets we retrieve by searching the corpus with the query expanded with entities extracted from the topic profile. We select the top entities as the source of query enrichment. The possible concepts in them are considered to be appended to the query string. The total weight of these concepts can be adjusted. We apply similar weighting strategies as already described earlier:</p><p>-If the annotated text in the topics contains more than one word, we append the name of the entity with weight w as a whole concept. Otherwise it is appended with weight w 2 .</p><p>-Append each word in the multiple-word concept as concepts and assign them half the weight of the original multiple-word concept.</p><p>Using top entities in related news articles Often, users write postings on Twitter about what is going on at the moment in the world. News articles contain details about the most important events. We expect to be able to benefit from searching for related news articles to find additional entities that can be included in the topic profiles. The related news articles can be retrieved by searching a news corpus. We select the top entities from news articles that were published four days before and after the query time. Again, we use the same strategy to compute the weight of concepts derived from each entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Indri Query Language</head><p>In the last step, the topic profile is converted to the Indri query language which supports the assignment of weights to query concepts. Entities consisting of more than one concept are treated as a phrase (Indri's #1(....) operator). As an example, consider the Indri query below, which was enriched with named entities derived from news and related tweets (topic MB002 "2022 FIFA soccer"):</p><p>#weight( 0.21265 FIFA 0.00838 Brazil 0.00838 #1(The Bahamas) 0.00838 #1(FIFA Beach Soccer World Cup) 0.13932 #1(Association football) 0.00599 Eurosport 0.00599 #1(West Bromwich Albion FC) 0.00599 #1(2010 Commonwealth Games) 0.00539 Italy 0.00479 #1(Nassau Bahamas) 0.00329 Bahamas 0.00299 Basel 0.00269 Yahoo 0.00240 #1(Frank Purdy Lahm) 0.00240 Futsal 0.00240 #1(FC Basel) 0.00240 Sport 0.00210 Beach 0.00210 Soccer 0.01213 World 0.04015 #1(FIFA World Cup) ... 0.03333 game 0.03333 series ) 3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Twitter Corpus</head><p>The corpus consists of approximately 16 million tweets, posted over a period of 2 weeks (January 24 until February 8th, inclusive). Since over time, less tweets are available for public access, we were only able to crawl fifteen million tweets (crawled in June/July), of which nearly five million tweets were detected to be written in English. Employing NER on the English tweets resulted in a total over six million named entities among which we found approximately 0.14 million distinct entities. The external news corpus was derived by extracting articles from 62 RSS feeds of prominent news media such as BBC, CNN or New York Times. A precise overview of the numbers can be found in Table <ref type="table" coords="6,343.26,436.27,3.87,9.52" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted Runs</head><p>We submitted four runs: basicWISTUD (automatic, no external or future evidence), manualWISTUD (manual), dbpWISTUD (automatic, external and future evidence), and mulnewWISTUD (automatic, external and future evidence). They are described in turn. basicWISTUD A separate index was created for each topic that included all tweets up to the topic's time stamp only (to avoid dealing with corpus statistics that are computed over future tweets). Apart from filtering out non-English tweets, we also removed re-tweets, tweets with less than 100 characters, tweets with less than 50 characters if URLs are ignored and tweets with words that contain a single letter three or more times in sequence (e.g., "oooooooooh" or "aaaaaaah"). Language modeling with relevance model RM2 <ref type="bibr" coords="6,403.89,603.65,10.51,9.52" target="#b3">[4]</ref> was employed. Up to 1000 results were returned per topic. No additional processing was performed to take the time-based ordering of the task into account.</p><p>manualWISTUD The manual run was created by processing each topic for 5 minutes as follows: one of the paper's authors formulated queries, submitted them to a MySQL engine<ref type="foot" coords="7,250.26,118.53,3.97,6.39" target="#foot_4">5</ref> and scanned the results of the English-tweet corpus sorted in descending order of tweet time. Tweets the annotator deemed relevant were marked. Tweets being duplicates of already marked tweets were subsequently ignored. The annotator did not follow hyperlinks mentioned in tweets, only the tweet text itself was considered (though it was sometimes possible to determine the potential informativeness based on the URL itself, e.g. a link http://www.bbc.co.uk/.../ was considered more informative than http://bit.ly/rrxSt9). No external source (e.g. news articles) were used to learn more about a topic, potential new query concepts were learnt while scanning the tweets. Once the 5 minutes were up, the next topic was processed. On average, 20.8 tweets were marked as relevant. The minimum was 0 (topic MB047) and the maximum was 42 (MB039). The tweet time of the oldest manually marked tweet was recorded and one more query (also created by the annotator) was submitted; all tweets retrieved this way with a time stamp older than the manually selected ones were appended until a maximum of 1000 tweets was reached.</p><p>dbpWISTUD This run integrated background knowledge from DBpedia and applied the semantic enrichment strategies described in Section 2.2. Therefore, we extracted entities from the topic description and the top related tweets by means of different NER services (DBpedia Spotlight, Alchemy API, and Zemanta). Only those entities for which we could identify a DBpedia URI were taken into account while profiling the topic. Each topic was therefore first represented via a set of weighted DBpedia concepts and then transformed into a set of weighted terms by exploiting the labeling information from DBpedia. The entire process including the weighting is described in Section 2.2. The final topic profile was constructed as an accummulation of the initial profile (inluence = 80%), which was generated based on the topic description, and the expanded profile (inluence = 20%), which was generated based on the related tweets. We then translated the topic profile into the query according to Indri's syntax and retrieved the top 100 tweets via the Inri search engine using relevance model RM2 <ref type="bibr" coords="7,415.30,474.08,9.96,9.52" target="#b3">[4]</ref>. Finally, the retrieved tweets were then ranked according to their recency. On average, 34.9 tweets were marked as relevant. The minimum was 0 (MB033) and the maximum was 122 (MB 020). Across all topics, the precision at 30 documents (P@30) is 0.301. The best four performing topics achieved P @30 = 0.833 (MB020, MB021, MB022 and MB030).</p><p>mulnewWISTUD Besides potentially related entities from tweets, this run also took the related news into account. The topic profiling was as same as in the dbpWISTUD run, but we issued the search query not only to the Twitter corpus but also the news articles corpus. Then, besides the entities retrieved from the tweets, we also aggregated the entities of the top ranked news articles into the final topic profile. The expanded parts for news and tweets were both weighted with 20%. On average, 30.7 tweets were marked as relevant. The minimum was Table <ref type="table" coords="8,165.08,117.59,4.13,8.21">3</ref>. Results of the official runs (revised relevance assessments). Reported are precision at 30 tweets (P@30) and mean average precision (MAP) for the full set of 49 topics (all) and the 33 topics with highly relevant tweets (high). The last two columns show the number of topics for which the P@30/MAP performance was better than the median P@30/MAP across the runs submitted by all track participants. 0 (MB016, MB033 and MB048) and the maximum was 123 (MB020). Here, P @30 = 0.196 across all topics. The two top performing topics achieved P @30 = 0.833 (MB022 and MB030).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results of our official runs are reported in Table <ref type="table" coords="8,362.63,405.06,3.87,9.52">3</ref>. The evaluation measures are P@30 and mean average precision (MAP). The results for the original topic set, consisting of 49 topics<ref type="foot" coords="8,248.33,427.74,3.97,6.39" target="#foot_5">6</ref> are shown in rows marked with all, while the subset of 33 topics that contain highly relevant tweets are shown in rows marked high. Shown are the results of the revised relevance assessment process (v2.0). The final two columns list the number of topics for which the P@30/MAP performance of our runs improved over the median P@30/MAP derived from all runs submitted by the participants of this task.</p><p>The manual run outperforms the automatic runs on all measures by a considerable margin. This is not surprising, as the manual run involved a great amount of human effort. In 42 of the 49 all relevant topics, the manual result outperforms the median P@30 across all submitted runs. The fraction of topics that improve over the median decreases when considering only the topic set with highly relevant tweets, a result which can be explained by the fact that the human annotator only considered the tweet text and possibly the URL string, but did not follow the hyperlinks present in the tweets. As will be described below, many relevant tweets do contain URLs. Among the automatic runs, the topic enrichment process with DBPedia Spotlight performs best, with P @30 = 0.3 (all) and P @30 = 0.1 (high). Extracting related entities from news articles has a less positive effect which may be caused by query drift.</p><p>Relevance Judgments Analysis Our initial analysis of the relevance judgments 7 revealed a number of interesting characteristics. The average length (number of characters) of the relevant tweets is 109.4, the minimum being 16. Thus, by filtering out short tweets, we are, inadvertently, already removing a number of relevant tweets in the pre-processing step. As already stated, in neither our manual nor our automatic runs were tweeted hyperlinks followed. This is a necessary step however, as 81.9% of the relevant tweets contain URLs. When only considering the highly relevant tweets this percentage rises further: 95.3% of them contain URLs. In both judgment sets -all relevant and highly relevant only -the minimum length of extra information besides the URL was found to be 0 (the average being 87.0 for all relevant and 85.34 for the highly relevant judgments), which means that some relevant tweets actually contain only a URL and no further information.</p><p>The DBPedia entity extraction was found to perform well, among the relevant tweets, in 86.8% (86.0% for the highly relevant) one or more named entities could be extracted. On average, relevant tweets contain 2.8 entities.</p><p>Finally, the language detection process we performed as a pre-processing step also removed a small number of tweets from the pool of relevant tweets: 94.8% of the relevant tweets (95.5% of the highly relevant tweets) were detected to be in English; tweets consisting only of a URL were not considered to be in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Three topics for which our runs dbpWISTUD and mulnewWISTUD performed worse than the median are: MB 001 8 , MB 003 9 , and MB 043 10 . For these topics, Table <ref type="table" coords="9,161.99,402.05,4.98,9.52" target="#tab_3">4</ref> shows the top entities and their assigned weights. After an initial investigation, we found a number of reasons for the poor effectiveness of our system in these case:</p><p>-The NER services sometimes fail. For example, the word "staff" in MB 001 was recognized as an entity which is related to a class of stick-shaped objects.</p><p>The word "olive" in MB 043 was annotated with an entity which was linked to a music band. One solution to this problem is to limit the types of entities that we adopt while employing NER on topics. The context of the topics should also be considered in order to filter out those obviously non-relevant entities. -The multiple-word entities received weights which were disproportionate: other useful information was not appropriately weighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have presented the search framework we created for the Microblog track 2011. We investigated the suitability of semantic enrichment tech-7 Note, that this analysis was conducted on the initially released relevance assessments. niques to improve the retrieval effectiveness. The fact that our run, enriched with DBPedia knowledge, performs better than the median across all submitted runs for half of the topics, leads us to the conclusion that semantic enrichment is a technique which should be investigated further in this setting. Future work will focus on the extraction of content from hyperlinks present in tweets, as our analysis has shown that the majority of relevant tweets contain URLs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,222.24,209.90,170.88,8.57"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the search framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,141.75,116.73,331.85,64.45"><head>Table 1 .</head><label>1</label><figDesc>Example of named-entity recognition and possible concepts in the topic.</figDesc><table coords="4,173.81,140.69,264.63,40.49"><row><cell>Topic (Example 002)</cell><cell cols="2">Jintao visits US</cell></row><row><cell cols="3">Name of the Entity Annotated Text Possible Concepts</cell></row><row><cell>Hu Jintao</cell><cell>Jintao</cell><cell>Hu, Jintao, Hu Jintao</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,116.73,345.82,137.51"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the Twitter corpus, the external news sources and the extracted named entities.</figDesc><table coords="6,187.02,150.26,238.25,103.98"><row><cell>Corpus</cell><cell>#Elements</cell></row><row><cell>Crawled Twitter 2011 Corpus</cell><cell>14,958,450</cell></row><row><cell>English Twitter Corpus</cell><cell>4,766,901</cell></row><row><cell>RSS News Feeds</cell><cell>62</cell></row><row><cell>News Articles</cell><cell>13,959</cell></row><row><cell>Entities extracted from Twitter Corpus</cell><cell>6,193,060</cell></row><row><cell>Entities extracted from News Articles</cell><cell>357,559</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,133.85,634.21,131.03,31.95"><head>Table 4 .</head><label>4</label><figDesc>8 BBC World Service Staff cuts 9 Haiti Aristide return 10 Kucinich olive pit lawsuit Receognized entities and their assigned weights for three example topics.</figDesc><table coords="10,159.84,132.54,292.60,122.80"><row><cell>Topic</cell><cell>MB001</cell><cell>MB003</cell><cell>MB043</cell></row><row><cell></cell><cell>BBC World Service</cell><cell cols="2">Haiti Dennis Kucinich</cell></row><row><cell></cell><cell>0.30124</cell><cell>0.37274</cell><cell>0.28660</cell></row><row><cell></cell><cell cols="2">BBC Jean Bertrand Aristide</cell><cell>Olive (band)</cell></row><row><cell>Entity and Weight</cell><cell>0.20963 Staff (stick) 0.07143 Cuts</cell><cell>0.13277 Return Transnistria 0.10000 Jean</cell><cell>0.18750 Kucinich 0.11852 Dennis</cell></row><row><cell></cell><cell>0.07143</cell><cell>0.06212</cell><cell>0.11852</cell></row><row><cell></cell><cell>World</cell><cell>Bertrand</cell><cell>band</cell></row><row><cell></cell><cell>0.03960</cell><cell>0.05819</cell><cell>0.09375</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,646.63,190.12,8.57"><p>Indri, http://http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.59,295.76,8.57"><p>Twitter search API, https://dev.twitter.com/docs/api/1/get/search</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,657.59,291.72,8.57"><p>Language detection, http://code.google.com/p/language-detection/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,657.59,217.41,8.57"><p>DBpedia Spotlight, http://spotlight.dbpedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,144.73,646.63,335.86,8.57;7,144.73,657.59,254.50,8.57"><p>Regular expressions were used, e.g. "%bbc%cut%" for searching the corpus for tweets relevant to the topic "BBC World Service staff cuts" (MB001).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,144.73,657.59,177.75,8.57"><p>One topic was dropped from the evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_6" coords="10,144.73,658.36,113.47,7.65"><p>http://imreal-project.eu</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements We would like to thank <rs type="person">Wen Li</rs> from the <rs type="institution">Multimedia Information Retrieval Lab at TU Delft</rs> who kindly helped us with crawling the Twitter corpus. The research leading to these results has received funding from the <rs type="funder">European Union Seventh Framework Programme</rs> (<rs type="programName">FP7/2007-2013)</rs> under grant agreement no ICT <rs type="grantNumber">257831</rs> (<rs type="projectName">ImREAL</rs> project <rs type="grantNumber">11</rs> ).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZxgM2jw">
					<idno type="grant-number">257831</idno>
					<orgName type="project" subtype="full">ImREAL</orgName>
					<orgName type="program" subtype="full">FP7/2007-2013)</orgName>
				</org>
				<org type="funding" xml:id="_GYgnUbu">
					<idno type="grant-number">11</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,468.99,321.06,8.57" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Der Zee</surname></persName>
		</author>
		<title level="m" coord="10,215.16,468.99,159.43,8.57">Twitter triumphs. Index on Censorship</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,479.95,342.27,8.57;10,146.91,490.91,333.67,8.57;10,146.91,501.87,333.68,8.57;10,146.91,512.82,333.67,8.57;10,146.91,523.78,316.88,8.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,328.37,479.95,152.24,8.57;10,146.91,490.91,166.19,8.57">Analyzing User Modeling on Twitter for Personalized News Recommendations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,239.55,501.87,241.04,8.57;10,146.91,512.82,200.00,8.57">Proceedings of the 19th International Conference on User Modeling, Adaption and Personalization (UMAP)</title>
		<title level="s" coord="10,146.91,523.78,140.31,8.57">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Conejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Marzo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</editor>
		<meeting>the 19th International Conference on User Modeling, Adaption and Personalization (UMAP)<address><addrLine>Girona, Spain; Girona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="volume">6787</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,534.74,342.24,8.57;10,146.91,545.70,268.15,8.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,273.89,534.74,134.91,8.57">N-gram-based text categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,433.95,534.74,46.64,8.57;10,146.91,545.70,204.16,8.57">Symposium On Document Analysis and Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,556.66,342.24,8.57;10,146.91,567.62,333.67,8.57;10,146.91,578.58,204.01,8.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,262.35,556.66,134.61,8.57">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,421.52,556.66,59.07,8.57;10,146.91,567.62,333.67,8.57;10,146.91,578.58,140.02,8.57">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;01</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval. SIGIR &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
