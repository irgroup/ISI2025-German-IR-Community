<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.00,79.96,308.48,12.54">RMIT at the Crouwdsourcing Track of TREC 2011</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.80,125.93,57.18,8.64"><forename type="first">Matthias</forename><surname>Petri</surname></persName>
							<email>matthias.petri@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">RMIT University School of CS&amp;IT</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.94,125.93,66.57,8.64"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<email>mark.sanderson@rmit.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">RMIT University School of CS&amp;IT</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.82,125.93,51.49,8.64"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<email>falk.scholer@rmit.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">RMIT University School of CS&amp;IT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.00,79.96,308.48,12.54">RMIT at the Crouwdsourcing Track of TREC 2011</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E583DECF3D91162E88537CBBBF7957D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our submission to the crowdsourcing track of TREC 2011. We first describe our crowdsourcing environment. Next we evaluate our approach and discuss our results. We conclude with a discussion of problems encountered during our participation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Crowdsourcing has become a useful tool that is used by the research community to parallelize human interaction tasks. Many experiments performed by university affiliated IR researchers tend to only include very few participants, usually students, due to the limit amount of funds available in comparison to companies like Google. Crowdsourcing experiments, on the other hand, can be performed on a larger scale, with a very diverse group of participants and at a reasonable cost.</p><p>The purpose of our participation in the 2011 crowdsourcing track is to evaluate crowdsourcing as an alternative to traditional user-focused IR experiments. In this paper we first describe our experimental setup under the constraints of the trec guidelines. Next we evaluate our submitted results and conclude with a discussion of problems encountered during our participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPERIMENTAL DESIGN</head><p>The TREC 2011 Crowdsourcing Track consisted of two tasks: an assessment task, and a consensus task. RMIT University participated in the first of these tasks, the aim of which was to evaluate the effectiveness of crowdsourding to collect relevance judgements. The assessments were made on documents from a subset of the ClueWeb test collection, a 25TB crawl of the World Wide Web in 2009. To avoid the possibility of harmful code, documents were rendered as image files.</p><p>In our experiment we used CROWDFLOWER (CF) to create and manage our assessment tasks. We used the CF internal markup language to define our interface. The basic design of a single human intelligence task (HIT) is shown in Figure <ref type="figure" coords="1,95.33,667.55,3.77,8.64" target="#fig_0">1</ref>. Query terms, description and narrative were extracted from the TREC topic descriptions and displayed above each image. When possible, the "shortened" version of the website image was used to decrease loading times. Instead of showing the complete image directly we used the crowdflower webservers to provide a clickable, cached thumbnail of size 800x400. Note that CF's validator functionality was used to ensure all workers select either relevant or not relevant in the voting widget.</p><p>The TREC guidelines require every worker to judge all 5 documents of a given topic-document set. However, we defined one HIT to only consist of one image relevance judgement. To ensure that individual workers perform all image relevance judgements in a topic set we specified, in the CF job interface, that 5 HITs per page should be displayed at the same time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quality Assurance</head><p>We used the CF internal gold data facilities to control the quality of our workers. We manually created 23 gold standard image/topic sets containing 5 images each which were chosen to be clearly identifiable as either relevant or non-relevant. For a given topic, we selected documents which were either judged relevant to a different topic or could easily be identified as not relevant or relevant by simple visible features.</p><p>A worker is periodically presented with a gold standard set of 5 image relevance judgement tasks for which we predefined the correct answer. The worker gets notified if he incorrectly answers any of the test tasks. If an individual worker fails to answer several of these gold standard questions we no longer present him with additional HITs.</p><p>Crowdflower categorizes workers into two categories: trusted and and "not trusted". Trusted workers are shown gold standard answers sets less frequently than "not trusted" workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pricing</head><p>We gathered 2 judgements per document set. In total we gathered 866 judgements over 456 topic -document sets at a cost of $133.43.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION</head><p>The task was "ordered" at 9pm AEST and finished at roughly 11pm on the same day. Overall 1376 trusted judgements and 424 untrusted judgements were gathered. Judgements were collected from 44 workers. The judgement/worker distribution can be seen in Figure <ref type="figure" coords="2,82.57,454.95,3.70,8.64">2</ref>. Note that 15 workers performed 50% of all tasks. CF only reports the time taken to complete 1 HIT (5 document judgements) in minutes. We can therefore only give rough estimates on the time it took workers to complete on set of documents: 28% of the sets were completed in less than one minute. 50% were completed in 1 -2 minutes. 12% were completed in 2 -3 minutes. The other 6% of all judgements were completed in 3 -22 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Worker quality</head><p>The median average worker trust calculated by CF was 0.9. Only 12% of our gold standard test sets were answered incorrectly. We first use consensus based evaluation to judge worker quality. Overall our worker quality is similar to that of other groups using crowdsourcing. We however always perform better than the mean consensus score over all submitted runs. The runs using crowdsourcing tend to perform worse than a submitted single worker run (UWaterlooMDS).</p><p>Next we evaluate worker quality by comparing to existing TREC gold data. Team BUPT-WILDCAT outperforms the other teams on all metrics. Our run performs worse than the mean of all submitted runs. There is a large discrepancy between the performance of our run in the consensus evaluation and in the gold standard evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>We performed binary relevance judgements of images using CrowdFlower. We used gold standard data to control worker quality. However, the gathered judgements are only 76.4% accurate even though our measured worker quality through CF is very high. We conjecture that our gold standard data was not "good enough" to filter all nontrustworthy workers. Overall, judgements obtained using crowdsourcing are of similar quality to that of a single high quality worker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,306.72,529.72,235.72,6.91;1,306.72,538.69,184.09,6.91"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Layout of one HIT unit consisting of query terms, description, narrative, image thumbnail and relevance voting widget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,102.66,420.42,137.40,6.91"><head></head><label></label><figDesc>Figure 2. Worker judgement distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.69,466.90,189.65,115.22"><head></head><label></label><figDesc>Table I shows the country worker distribution.</figDesc><table coords="2,106.69,488.98,130.83,93.14"><row><cell>Country</cell><cell>% of judgements</cell></row><row><cell>India</cell><cell>77%</cell></row><row><cell>Serbia</cell><cell>5%</cell></row><row><cell>Philippines</cell><cell>4%</cell></row><row><cell>Japan</cell><cell>3%</cell></row><row><cell>Thailand</cell><cell>2%</cell></row><row><cell>USA</cell><cell>2%</cell></row><row><cell>Other</cell><cell>6%</cell></row><row><cell></cell><cell>Table I</cell></row><row><cell cols="2">JUDGEMENT COUNTRY DISTRIBUTION.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
