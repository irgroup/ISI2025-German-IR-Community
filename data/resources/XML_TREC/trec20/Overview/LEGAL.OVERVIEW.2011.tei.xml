<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.39,112.60,285.22,14.93">Overview of the TREC 2011 Legal Track</title>
				<funder ref="#_EyeA2kw">
					<orgName type="full">Business Intelligence Associates, Inc.</orgName>
				</funder>
				<funder ref="#_CwbxfKC">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">IE Discovery, Inc.</orgName>
				</funder>
				<funder>
					<orgName type="full">NIST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,122.99,144.71,141.90,10.57"><roleName>Wachtell</roleName><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
						</author>
						<author>
							<persName coords="1,272.79,144.71,78.71,10.29"><forename type="first">Rosen</forename><forename type="middle">&amp;</forename><surname>Lipton</surname></persName>
						</author>
						<author>
							<persName coords="1,354.49,144.71,50.17,10.57"><forename type="first">New</forename><surname>Katz</surname></persName>
						</author>
						<author>
							<persName coords="1,407.65,144.91,46.49,10.37"><roleName>NY</roleName><surname>York</surname></persName>
						</author>
						<author>
							<persName coords="1,140.85,158.86,93.54,10.37"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
						</author>
						<author>
							<persName coords="1,141.60,187.47,80.51,10.37"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Bruce Hedin</orgName>
								<address>
									<postCode>H5</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.39,112.60,285.22,14.93">Overview of the TREC 2011 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E92BCDA850932B8654B855A0EDADF13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC 2011 Legal Track consisted of a single task: the learning task, which captured elements of both the TREC 2010 learning and interactive tasks. Participants were required to rank the entire corpus of 685,592 documents by their estimate of the probability of responsiveness to each of three topics, and also to provide a quantitative estimate of that probability. Participants were permitted to request up to 1,000 responsiveness determinations from a Topic Authority for each topic. Participants elected either to use only these responsiveness determinations in preparing automatic submissions, or to augment these determinations with their own manual review in preparing technologyassisted submissions. We provide an overview of the task and a summary of the results. More detailed results are available in the Appendix to the TREC 2011 Proceedings. * The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are concerned with the identification of responsive documents as part of the e-discovery process, for which the objective is to identify as nearly as practicable all documents from a collection that are responsive to a request for production in civil litigation, while minimizing the number of unresponsive documents that are identified.</p><p>The learning task models the scenario in which a senior attorney -the Topic Authority -is charged with interpreting the request for production, communicating that interpretation to a review team, and producing responsive documents to the requesting party. TREC participants play the role of the review team.</p><p>At the outset, the Topic Authority reviews the request and a sample of potentially responsive documents, and prepares a set of coding guidelines. The production request and the guidelines are provided to participants, and an initial kick-off call allows interested participants to ask the Topic Authority questions about his or her interpretation of the request for production.</p><p>Over the course of several weeks, each participant is entitled to request feedback from the Topic Authority on a number of documents from the collection. This feedback consists of a simple binary responsiveness determination: participants are informed whether the Topic Authority determines each document to be responsive or not. No other communication with the Topic Authority is permitted.</p><p>Teams from ten different organizations participated in the 2011 Legal Track; the names of the teams, as well as the prefix used to label each team's results, are shown in Table <ref type="table" coords="1,308.35,563.36,3.74,8.64" target="#tab_0">1</ref>. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Collection</head><p>The document collection used for the TREC 2011 Legal Track was identical to that used for TREC 2010. <ref type="bibr" coords="1,515.01,619.26,6.09,8.64">It</ref>  Track coordinators, and hosted by EDRM <ref type="bibr" coords="2,241.11,247.98,10.58,8.64" target="#b0">[1]</ref>. The EDRM dataset consists of 1.3 million email messages captured by the Federal Energy Regulatory Commission ("FERC") from Enron, in the course of its investigation of Enron <ref type="bibr" coords="2,510.63,259.93,10.58,8.64" target="#b1">[2]</ref>. ZL independently acquired the dataset from Lockheed Systems (formerly Aspen Systems) who captured and maintain the dataset on behalf of FERC. The EDRM dataset is available in two formats: EDRM XML and PST. The EDRM XML version contains a text rendering of each email message and attachment, as well as the original native format. The PST version contains the same messages in a Microsoft proprietary format used by many commercial tools. Both versions of the dataset approach 100GB in size, presenting an obstacle to some participants. Furthermore, there are a large number of duplicate email messages in the dataset that were captured more than once by Lockheed. For TREC, a list of 455,449 distinct messages were identified as canonical; all other messages duplicate one of the canonical messages. These messages contain 230,143 attachment files; together, these messages plus attachments form the 685,592 documents of the TREC 2010/2011 Legal Track collection. Text and native versions of these documents were made available to participants, along with a mapping from the EDRM XML and PST files to their canonical counterparts in the TREC collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Responsiveness Assessments</head><p>In order to measure the efficacy of TREC participants' efforts, it is necessary to compare their results to a gold standard indicating whether or not each document in the collection is responsive to a particular discovery request. The learning task had three distinct topics, each representing a distinct request for production.</p><p>Ideally, a gold standard would indicate the responsiveness of each document to each topic. Because it would be impractical to use human assessors to render these two million assessments, a sample of documents was identified for each topic, and assessors were asked to code only the documents in the sample as responsive or not. Since errors in the gold standard can have substantial impact on evaluation, redundant independent assessments were made for the majority of the sampled documents, and disagreements were adjudicated by the Topic Authority.</p><p>A total of 16,999 documents -about 5,600 per topic -were selected and assessed to form the gold standard. The documents that were selected met one or more of the following four criteria:</p><p>1. All documents that were identified by the Track coordinators to be potentially responsive in the course of developing the topics before the start of the task;</p><p>2. All documents submitted by any team for responsiveness determination;</p><p>3. All documents ranked among the 100 most probably responsive by any submission;</p><p>4. A uniform random sample of the remaining documents.</p><p>11,612 documents (referred to as the 100 stratum) were selected according to one or more of the first three criteria; 5,387 documents (referred to as the 1000 stratum) were sampled according to the fourth. All documents in the 100 stratum were assessed, regardless of whether or not a responsiveness determination had been previously rendered by the Topic Authority. Each document in the 1000 stratum was given to two assessors; that is, each sampled document was assessed twice. The learning task assessments were rendered by four professional review companies, who volunteered their services pro bono (although, to our knowledge, the reviewers themselves were paid for their services). Three of the companies used a Web-based platform developed by the Track coordinators to view scanned documents and to record their responsiveness judgments. To avoid problems with local rendering software on each assessor's workstation, the assessors made their judgments based on pdf-formatted versions of the documents, as opposed to the original native format documents. The fourth review company downloaded the pdf documents and conducted the review on their own platform. All review companies were asked to employ their established commercial practice, including their quality assurance procedures.</p><p>Assessors were provided with orientation and detailed guidelines created by a Topic Authority. The review platform included a "seek assistance" link which assessors were encouraged to use to request that the Topic Authority resolve any uncertainties. Assessors were instructed to make a responsiveness judgment of responsive ("R"), not responsive ("N"), or broken ("B") for every document assigned to them for review. The latter code reflects the fact that a small percentage of documents from the EDRM dataset are malformed and therefore could not be assessed.</p><p>Once the preliminary assessments were complete, quality assurance was conducted by having the Topic Authority adjudicate conflicting assessments, which occurred in one of two cases:</p><p>1. For documents selected according to criterion 2 above, the Topic Authority's initial responsiveness determination and the assessor's responsiveness judgment differed; or 2. For documents selected according to criterion 4 above (i.e., the 1000 stratum), the two assessors' judgments differed.</p><p>The Topic Authority adjudicated all conflicting documents together, with no indication of which documents had been subject to a previous responsiveness determination, or what that determination had been.</p><p>The gold standard consists of:</p><p>• The assessor's judgment, for documents without conflicting assessments; and,</p><p>• The Topic Authority's final judgment, for documents with conflicting assessments.</p><p>The gold standard, along with the toolkit used for the evaluation, may be found on the web: http://plg1.uwaterloo.ca/trec11-assess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Task</head><p>The learning task models the use of automated or semi-automated methods to guide review strategy for a multi-stage document review effort, organized as follows:</p><p>1. Initial search and assessment. The responding party analyzes the production request. Using ad hoc methods, the team identifies a seed set of potentially responsive documents, and assesses each as responsive or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning by example.</head><p>A learning method is used to rank the documents in the collection from most to least likely to be responsive to the production request, and to estimate the likelihood of responsiveness for each document. The input to the learning method consists of the seed set, the assessments for the seed set, and the unranked collection; the output is a ranked list consisting of the document identifier and a probability of responsiveness for each document in the collection.</p><p>The two learning objectives -ranking and estimating the likelihood of responsiveness -may be accomplished by the same method or by different methods. Either may be automated or manual. For example, ranking may be done using an information retrieval method or by human review using a five-point scale. Estimation may be done in the course of ranking or, for example, by sampling and reviewing documents at representative ranks.</p><p>3. Review process. A review process may be conducted, with strategy guided by the ranked list. One possible strategy is to review documents in order, from most likely to least likely to be responsive, thereby discovering as many responsive documents as possible for a given amount of effort. Another possible strategy is triage: to review only mid-ranked documents, deeming, without further review, the top-ranked ones to be responsive, and the bottom-ranked ones to be non-responsive.</p><p>Review strategy may be guided not only by the order of the ranked list, as outlined above, but also by the estimated effectiveness of various alternatives. Consider the strategy of reviewing the top-ranked documents.</p><p>Where should a cut be made so that documents above the cut are reviewed and documents below are not? For triage, where should the two necessary cuts be made?</p><p>Practically every review strategy decision boils down to the question, Of this particular set of documents, how many are responsive and how many are not?</p><p>This question itself could be answered by first answering the more detailed question, What is the probability of each document in the set being responsive?</p><p>Given an answer to the second question, the answer to the first is simply the sum of the probabilities. For this reason, participants in the learning task were required to provide an estimate of the probability of responsiveness for each document in the collection. This probability estimate serves a dual purpose:</p><p>1. The documents may be sorted by this probability in order to rank them from the most likely to the least likely to be responsive. It stands to reason that the set of documents ranked 1 through c is likely to contain more of the responsive documents (i.e., to achieve higher recall, precision, and F 1 ) than some other set of c documents <ref type="bibr" coords="4,520.91,350.45,15.27,8.64" target="#b12">[13]</ref>.</p><p>2. The probabilities of the top-ranked c documents may be summed to yield an estimate of the number of responsive documents that they contain, Rel c . Furthermore, the probabilities of all documents may be summed to yield an estimate of the number of responsive documents in the collection, Rel. From these estimates we may derive estimates of recall ( Rel c Rel ), precision ( Rel c c ), and F 1 (</p><formula xml:id="formula_0" coords="4,297.75,404.00,26.13,17.72">2 Rel Relc + c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relc</head><p>). These estimates, if they are accurate, may be used to inform the selection of the cutoff value c to account for the tradeoffs among recall, precision, effort, and size of production.</p><p>Task participants were therefore required to submit, for each document in the collection and for each topic, an estimate of the probability that the document was responsive to the topic. The participants' objectives in supplying these estimates were twofold:</p><p>1. To yield a good ranking of documents: for any given cutoff c, the number of responsive documents among the top-ranked c documents should be as large as possible.</p><p>2. To yield good effectiveness estimates: for any given cutoff c, the estimate Rel c should be as close as possible to the actual number of responsive documents, so that the estimates of recall, precision, and F 1 at cutoff c are also as close as possible to their true values.</p><p>These objectives are consistent with the requirement in civil litigation to produce as nearly as practicable all and only the documents that are responsive to the request for production, independent of their evidentiary value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submission Phases</head><p>For each topic, teams were required to submit an initial set of probability estimates prior to requesting any responsiveness determinations from the Topic Authority. Following the initial submission, teams were entitled to receive up to 100 responsiveness determinations before being required to submit an interim set of probability estimates. After submitting the first interim results, teams were entitled to receive up to 200 further responsiveness determinations before submitting a second interim set of results. Thereafter, teams were entitled to receive up to 700 additional responsiveness determinations. In total, each team was allowed to request at most 1,000 responsiveness determinations per topic, subject to submitting the required initial and interim results.</p><p>Each team was required to submit a final set of probability estimates once it had received all the responsiveness determinations requested by the team. In a final mopup phase, all responsiveness determinations requested by all teams were distributed to all teams, who had the opportunity to submit a mopup set of probability estimates. Thus, the final submission used only relevance determinations for documents specified by the submitting team, while the mopup submission used relevance determinations for documents specified by all teams. Table <ref type="table" coords="5,426.08,247.98,4.98,8.64" target="#tab_1">2</ref> shows the total number of responsiveness determinations given to the teams for the mopup phase.</p><p>In this Overview, we report results for the final and mopup submissions. The run identifiers for the various phases may be distinguished by their final symbol: final submissions end in "F"; and mopup submissions end in "M". In the Appendix to the proceedings <ref type="bibr" coords="5,204.67,295.80,15.27,8.64" target="#b14">[15]</ref>, we provide all results, including those for the initial and interim submissions,<ref type="foot" coords="5,535.82,293.87,3.69,6.39" target="#foot_0">2</ref> whose run identifiers end in "1", "2", and "3".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Participation Categories</head><p>Participants were asked to declare each run to be automatic or technology-assisted. Automatic runs were allowed to use manual query formulation, but human review of the document collection (other than that provided by TREC via responsiveness determinations) was not permitted. Technology-assisted runs were allowed to avail themselves of any amount of human review. Participants were asked to state the number of hours spent -configuring the system, searching the dataset, reviewing documents, and analyzing the results -as summarized in Table <ref type="table" coords="5,459.84,402.77,3.74,8.64" target="#tab_2">3</ref>. The participation category of a run is specified by the penultimate character in its name: "A" for automatic; and "T" for technologyassisted. For example, the run named "gggxxxAF" is a final run, automatic participation, by the group whose run prefix is ggg, with the letters xxx chosen by the submitting team to distinguish among its submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topics</head><p>The learning task used three topics: 401, 402 and 403.</p><p>• Topic 401 (Topic Authority: Kevin F. Brady, Eckert, Seamans, Cherin &amp; Mellott LLC.)</p><p>All documents or communications that describe, discuss, refer to, report on, or relate to the design, development, operation, or marketing of enrononline, or any other online service offered, provided, or used by the Company (or any of its subsidiaries, predecessors, or successors-in-interest), for the purchase, sale, trading, or exchange of financial or other instruments or products, including but not limited to, derivative instruments, commodities, futures, and swaps.</p><p>• Topic 402 (Topic Authority: Brendan M. Schulman, Kramer Levin Naftalis &amp; Frankel LLP.) All documents or communications that describe, discuss, refer to, report on, or relate to whether the purchase, sale, trading, or exchange of over-the-counter derivatives, or any other actual or contemplated financial instruments or products, is, was, would be, or will be legal or illegal, or permitted or prohibited, under any existing or proposed rule(s), regulation(s), law(s), standard(s), or other proscription(s), whether domestic or foreign. • Topic 403 (Topic Authority: Robert Singleton, Squire, Sanders &amp; Dempsey (US) LLP.) All documents or communications that describe, discuss, refer to, report on, or relate to the environmental impact of any activity or activities undertaken by the Company, including but not limited to, any measures taken to conform to, comply with, avoid, circumvent, or influence any existing or proposed rule(s), regulation(s), law(s), standard(s), or other proscription(s), such as those governing environmental emissions, spills, pollution, noise, and/or animal habitats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>In contrast to the interactive tasks of TREC 2008 through 2010, for 2011 the Track coordinators did not compose a new mock complaint to provide context for the three topics. Topics 401 and 402 were cast as supplemental requests relating to the 2009 complaint <ref type="bibr" coords="7,195.58,267.90,10.58,8.64" target="#b2">[3]</ref>, while topic 403 was cast as a supplemental request relating to the 2010 complaint <ref type="bibr" coords="7,72.00,279.86,10.58,8.64" target="#b3">[4]</ref>. Table <ref type="table" coords="7,112.75,291.81,4.98,8.64" target="#tab_3">4</ref> shows the estimated number of responsive documents for each topic, with 95% confidence intervals, calculated using 100 bootstrap samples to estimate the standard error of measurement <ref type="bibr" coords="7,415.65,303.77,10.58,8.64" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Each submission was evaluated according to how well it met the objectives of the task: ranking and estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ranking</head><p>For representative values of the cutoff value c, representing the number of top-ranked documents to be considered for production, Tables <ref type="table" coords="7,149.39,418.81,46.81,8.64" target="#tab_6">5 through 7</ref> show recall, precision, and F 1 for each run with respect to each topic. For any given combination of topic and cutoff, higher recall, precision, and F 1 indicate better ranking and hence greater retrieval effectiveness. It follows from the definitions of recall, precision, and F 1 that if one submission is superior to another for a given topic and cutoff, it will be superior on all three measures. The best measures for each combination of topic and cutoff -that is, the best measures in each column -are shown in bold font.</p><p>Each row of Tables 5 through 7 illustrates the recall-precision tradeoff inherent in the choice of cutoff. At low cutoff values, precision is generally high while recall is low. At high cutoff values, recall is high while precision is low. F 1 is low at both low and high cutoff values, and peaks somewhere in between.</p><p>For the purpose of guiding review strategy, recall conveys completeness as a function of cutoff much more directly than the other measures, answering the question, "If we were to examine the top-ranked c documents, what fraction of the responsive ones would be found?" Precision provides a measure of the efficiency with which a review can be conducted; F 1 sheds no additional light. Precision and F 1 are in fact mathematically redundant, as they may be calculated from recall, given c and Rel. Gain curves, shown in Figures <ref type="figure" coords="7,357.48,562.27,44.96,8.64">1 through 3</ref>, plot recall as a function of cutoff for each of the three topics. Gain curves allow the reader to see at a glance the absolute and relative effectiveness of the submissions, at various cutoff levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Estimation</head><p>The recall-precision-F 1 tables and gain curves detailed in the previous section indicate the effectiveness of participants' approaches at various cutoff levels. During the course of an actual review effort, if the gain curves were known, it would be a simple matter to pick the value of c that best captured the desired tradeoff between effort and recall. But the gain curves presented here are not known; they are the result of an extensive evaluation effort that could not reasonably Cutoff (# docs) be conducted within the context of a single document review. Instead, the task required participants to estimate their own gain curves in the form of a probability estimate for each document and topic. Tables <ref type="table" coords="8,116.57,509.33,53.67,8.64" target="#tab_7">8 through 10</ref> show the participants' estimates of recall for each combination of cutoff and topic. For comparison, the gold-standard estimates (from Tables <ref type="table" coords="8,292.25,521.29,46.34,8.64" target="#tab_6">5 through 7</ref>) are shown, as well as the difference. A positive difference indicates that the participant's estimate was too high; a negative difference indicates that the participant's estimate was too low. All estimates and differences are rounded to the closest integer, so the rounded integer difference shown in the table is not always equal to the difference between the rounded integer estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summary measures</head><p>No single measure can fully characterize how well a system ranks documents and estimates the probability of responsiveness. Nevertheless, it is useful to have summary measures that roughly capture the effectiveness of the various approaches. As measures of ranking effectiveness, without regard to the accuracy of the participant's estimates, we use Hypothetical F 1 and Area Under the Receiver Operating Characteristic Curve ("AUC"). As a measure to combine ranking effectiveness and estimation accuracy, we use Actual F 1 . These summary results are shown in Tables 11 through <ref type="bibr" coords="8,105.48,664.12,8.30,8.64" target="#b12">13</ref>.</p><p>Note from Tables 5 through 7 that F 1 depends on the cutoff c. Each submission, for each topic, has 685,592 possible F 1 scores -one for each possible value of c. Hypothetical F 1 is simply the best of these 685,592 possible scores, calculated by enumerating all possible values of c and calculating F 1 for each. It is called "Hypothetical" because it is achieved only when the optimal cutoff is used, and there is no way to determine this cutoff from the submission itself (i.e., without the gold standard). Hypothetical F 1 is the F 1 score that could have been achieved, had this optimal value c been known to the participants when submitting their results.</p><p>Actual F 1 relies on the submitted probability estimates to choose c. As with Hypothetical F 1 , all possible values of c are enumerated, but instead of choosing after the fact the one that maximizes F 1 , we choose c that maximizes the participant's estimate of F 1 , which is known beforehand. Actual F 1 is the actual value of F 1 (i.e., computed using the gold standard) based on c chosen to maximize the estimated value of F 1 (i.e., computed using the participant's probability estimates). Thus, Actual F 1 is a summary measure that captures both the effectiveness of the ranking and the accuracy of the estimates, and, as such, provides the most informative gauge of the effectiveness of an approach at meeting the retrieval challenge in a real-world scenario (when an after-the-fact gold standard would not be available). Both ranking and estimation must be good in order to achieve a high Actual F 1 score.</p><p>AUC is a summary measure for ranking effectiveness (regardless of estimation accuracy) derived from signal detection theory <ref type="bibr" coords="9,140.90,242.85,10.58,8.64" target="#b7">[8]</ref>. Although its name implies a geometric quantity, AUC has a particularly simple probabilistic meaning: AUC is the probability that a responsive document will be ranked higher than a non-responsive document. It is easily estimated by enumerating all pairs of responsive and non-responsive documents and computing the fraction of pairs for which the relevant document has a higher rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The TREC 2011 Legal Track evaluated the efficacy of various review techniques and tools chosen and implemented by the participating teams. Some participants may have conducted an all-out effort to achieve the best possible results, while others may have conducted experiments to illuminate selected aspects of document review technology. It is inappropriate -and forbidden by the TREC participation agreement -to claim that the results presented here show that one participant's system or approach is generally better than another's. It is also inappropriate to compare the results of TREC 2011 with the results of past TREC Legal Track exercises, as the test conditions as well as the particular techniques and tools employed by the participating teams are not directly comparable. One TREC 2011 Legal Track participant was barred from future participation in TREC for advertising such invalid comparisons.</p><p>One may see from the results presented in this Overview that some particular techniques and tools achieved good results in this exercise, and therefore show promise that they might also achieve good results in other document review efforts. The efficacy of the participants' efforts are characterized by the quality of ranking and the accuracy of recall estimates. Efficacy must be interpreted in light of effort, which is characterized by the number of relevance determinations sought from the Topic Authority, as well as by the amount of manual effort employed by the participating team (see Table <ref type="table" coords="9,114.91,490.02,3.60,8.64" target="#tab_2">3</ref>).</p><p>The quality of ranking is illustrated in Tables <ref type="table" coords="9,275.15,501.98,46.29,8.64" target="#tab_6">5 through 7</ref>, the gain curves in Figures <ref type="figure" coords="9,437.87,501.98,46.29,8.64">1 through 3</ref>, and the Hypothetical F 1 and AUC summary measures shown in Tables <ref type="table" coords="9,323.95,513.93,57.05,8.64" target="#tab_2">11 through 13</ref>. The gain curves convey at a glance the tradeoff between recall and cutoff. Figure <ref type="figure" coords="9,261.53,525.89,4.98,8.64">1</ref> shows that, for Topic 401, four submissions (otL11FTM, rec03TF, UWALINAM, and mlblrnTM) achieve about 70% recall when only the top-ranked 75,000 documents (11% of the collection) are considered. Assuming that each of these 75,000 documents is reviewed by a human, examining only the top 11% represents a nine-fold saving in review effort, compared to a manual review of the entire collection. Somewhat higher recall may be achieved with more effort, but it is unclear whether improvements in recall measures above 70% are meaningful, given the inherent uncertainties arising from sampling and human assessment of responsiveness <ref type="bibr" coords="9,108.15,597.62,10.79,8.64" target="#b8">[9,</ref><ref type="bibr" coords="9,122.30,597.62,11.83,8.64" target="#b9">10]</ref>. Figure <ref type="figure" coords="9,173.10,597.62,4.98,8.64">2</ref> shows that, for Topic 402, one run (rec03TF) achieves a recall of more than 70% when only the top-ranked 20,000 documents (3% of the collection) are considered. Another run (UWALINAM) achieves similar recall when about 40,000 documents (6% of the collection) are considered. It is worth noting that the former is a technology-assisted final run, while the latter is an automatic mopup run. Figure <ref type="figure" coords="9,413.02,633.48,4.98,8.64">3</ref> shows that two runs (rec03TF and UWALINAM) achieve 70% recall when only the top-ranked 5,000 documents (less than 1% of the collection) are considered. For all three topics, the achievement of 70% recall at the cutoffs noted above reveals relatively low levels of precision; nonetheless, even at these levels of precision, the savings gained by reviewing the top c documents (rather than the entire collection) would be substantial. The recall, precision, and F 1 measures in Tables <ref type="table" coords="9,469.07,681.31,4.98,8.64" target="#tab_4">5</ref>  precisely quantify these observations, and the Hypothetical F 1 and AUC measures in Tables 11 through 13 provide a rough estimate of overall ranking effectiveness.</p><p>In practice, a high-quality ranking offers the promise of a review effort that examines only a fraction of the collection -whether 11%, 3%, or 1% -while still achieving substantial recall. To achieve this promise, it is essential to determine, at review time, exactly what fraction of the collection must be reviewed to achieve this end: Is it 1%, 3%, 11%, or some other number? Tables <ref type="table" coords="10,246.16,425.64,52.85,8.64" target="#tab_7">8 through 13</ref> show the participants' estimates of the recall they thought they achieved for various fractions of the collection. The results are not encouraging. Most runs for most topics dramatically overestimated recall at all cutoff levels. Such an overestimate might lead the manager of a review effort to terminate the review prematurely, due to the false belief that a high level of recall had been achieved. Two participants (Recommind and the University of Waterloo) underestimated recall by a relatively small amount for Topics and 402, and by a much larger amount for Topic 403. Overall, while teams occasionally achieved Actual F 1 scores that came close to the Hypothetical scores (e.g., on Topic 401, one team (Recommind) achieved an Actual F 1 score of 54%, which is reasonably close to their corresponding Hypothetical F 1 score of 58%), no team was able to estimate recall consistently enough to achieve, for all topics, Actual F 1 scores near the Hypothetical F 1 scores that could have been achieved, were their estimates accurate. Overall, consistent recall estimation continues to be a challenge worthy of investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The 2011 TREC Legal Track was the sixth since the Track's inception in 2006, and the third that has used a collection based on Enron email (see <ref type="bibr" coords="10,180.34,613.04,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="10,193.77,613.04,12.45,8.64" target="#b13">14,</ref><ref type="bibr" coords="10,208.86,613.04,12.45,8.64" target="#b11">12,</ref><ref type="bibr" coords="10,223.94,613.04,12.45,8.64" target="#b10">11,</ref><ref type="bibr" coords="10,239.03,613.04,7.05,8.64" target="#b5">6]</ref>). From 2008 through 2011, the results show that the technology-assisted review efforts of several participants achieve recall scores that are about as high as might reasonably be measured using current evaluation methodologies. These efforts require human review of only a fraction of the entire collection, with the consequence that they are far more cost-effective than manual review. There is still plenty of room for improvement in the efficiency and effectiveness of technology-assisted review efforts, and, in particular, the accuracy of intra-review recall estimation tools, so as to support a reasonable decision that "enough is enough" and to declare the review Cutoff (# docs) complete. Commensurate with improvements in review efficiency and effectiveness is the need for improved external evaluation methodologies that address the limitations of those used in the TREC Legal Track and similar efforts. How can we construct a gold standard with reasonable effort, or, in the alternative, measure review effectiveness without a gold standard? How best can we measure recall and precision values that are beyond the limit of what can be measured with reference to a single assessor? How can we better control for the amount of effort expended in conducting document review?</p><p>The TREC 2011 coordinators determined that it would be not be worthwhile to pursue these research objectives further using the Enron email collection, and endeavored to build a new collection for TREC 2012 and beyond. At the time of writing, the collection was not available, and as a result the TREC Legal Track will not be run in 2012. Work on preparing the collection continues. When complete, the collection will be made available to interested researchers subject to a usage agreement. Further evaluation efforts -whether under the auspices of TREC or a different organization -will be able to use this collection.</p><p>Interested researchers may obtain the Enron collection, the Tobacco collection used in the TREC Legal Track from 2006 through 2009, as well as the submissions and evaluation results for the six years of the TREC Legal Track. These collections may be used to reproduce the results reported here, or to conduct new experiments to address the many outstanding research questions that remain.</p><p>Cutoff (# docs) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,72.00,619.26,468.00,20.59"><head>Table 1 :</head><label>1</label><figDesc>was derived from the EDRM Enron Dataset, version 2, prepared by ZL Technologies in consultation with the 2010 Legal Organizations participating in the TREC 2011 Legal Track.</figDesc><table coords="2,173.55,74.28,264.90,128.59"><row><cell>Participating Organization</cell><cell>Run Prefix</cell></row><row><cell cols="2">Beijing University of Posts and Telecommunications pri</cell></row><row><cell>Helioid</cell><cell>HEL</cell></row><row><cell>Indian Statistical Institute</cell><cell>ISI</cell></row><row><cell>OpenText</cell><cell>ot</cell></row><row><cell>Recommind</cell><cell>rec</cell></row><row><cell>TCDI</cell><cell>tcd</cell></row><row><cell>University of Melbourne</cell><cell>mlb</cell></row><row><cell>University of South Florida</cell><cell>USF</cell></row><row><cell>University of Waterloo</cell><cell>UW</cell></row><row><cell>Ursinus College</cell><cell>URS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,146.68,74.28,318.64,66.60"><head>Table 2 :</head><label>2</label><figDesc>Number of Topic Authority Relevance Determinations for mopup runs.</figDesc><table coords="5,236.31,74.28,139.39,44.90"><row><cell cols="3">Topic Resp. Nonresp. Total</cell></row><row><cell>401</cell><cell>1,040</cell><cell>1,460 2,500</cell></row><row><cell>402</cell><cell>238</cell><cell>1,864 2,102</cell></row><row><cell>403</cell><cell>245</cell><cell>1,954 2,199</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,139.68,468.00,485.03"><head>Table 3 :</head><label>3</label><figDesc>Self-reported effort (in hours) to configure the participants' systems, search for documents, review documents, and analyze results prior to submission.</figDesc><table coords="6,180.83,139.68,252.84,451.38"><row><cell></cell><cell cols="4">Setup Search Review Analysis Total</cell></row><row><cell>HELclrAM</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>HELq20rAM</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>ISIFuSAM</cell><cell>10</cell><cell>10</cell><cell>0</cell><cell>5</cell></row><row><cell>ISIFUSAM</cell><cell>10</cell><cell>5</cell><cell>0</cell><cell>10</cell></row><row><cell>ISILRFTF</cell><cell>40</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>ISILrFTF</cell><cell>40</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>ISIRoTAM</cell><cell>10</cell><cell>10</cell><cell>0</cell><cell>10</cell></row><row><cell>ISIROTAM</cell><cell>20</cell><cell>10</cell><cell>0</cell><cell>10</cell></row><row><cell>ISIROTTF</cell><cell>40</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>ISIRoTTF</cell><cell>40</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>ISITrFAM</cell><cell>10</cell><cell>10</cell><cell>0</cell><cell>5</cell></row><row><cell>ISITRFAM</cell><cell>20</cell><cell>10</cell><cell>0</cell><cell>15</cell></row><row><cell>ISITrFTF</cell><cell>40</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>ISITRFTF</cell><cell>40</cell><cell>10</cell><cell>20</cell><cell>10</cell></row><row><cell>mlbclsAF</cell><cell>10</cell><cell>1</cell><cell>0</cell><cell>3</cell></row><row><cell>mlblrnTF</cell><cell>10</cell><cell>1</cell><cell>10</cell><cell>3</cell></row><row><cell>mlblrnTM</cell><cell>10</cell><cell>0</cell><cell>0</cell><cell>5</cell></row><row><cell>otL11BTM</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>1</cell></row><row><cell>otL11FTM</cell><cell>10</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>otL11HTM</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>1</cell></row><row><cell>priindAM</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>4</cell></row><row><cell>rec03TF</cell><cell>20</cell><cell>15</cell><cell>500</cell><cell>120</cell></row><row><cell>rec04TM</cell><cell>20</cell><cell>30</cell><cell>150</cell><cell>30</cell></row><row><cell>tcdAF</cell><cell>40</cell><cell>1</cell><cell>0</cell><cell>20</cell></row><row><cell>URS205AM</cell><cell>400</cell><cell>28</cell><cell>20</cell><cell>20</cell></row><row><cell>USFDSETF</cell><cell>20</cell><cell>20</cell><cell>48</cell><cell>48</cell></row><row><cell>USFEOLTF</cell><cell>80</cell><cell>5</cell><cell>36</cell><cell>24</cell></row><row><cell>USFMOPTF</cell><cell>10</cell><cell>5</cell><cell>4</cell><cell>7</cell></row><row><cell>UWABASA4</cell><cell>7</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>UWABASAF</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWABASAM</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWALINA4</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWALINAF</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWALINAM</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWASNAA4</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWASNAAF</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>UWASNAAM</cell><cell>40</cell><cell>0</cell><cell>0</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,171.55,132.24,268.90,8.64"><head>Table 4 :</head><label>4</label><figDesc>Estimated number of responsive documents for each topic.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,72.00,74.28,468.00,389.79"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table coords="8,80.64,74.28,450.72,356.13"><row><cell></cell><cell></cell><cell>2,000</cell><cell></cell><cell></cell><cell>5,000</cell><cell></cell><cell></cell><cell>20,000</cell><cell></cell><cell></cell><cell>50,000</cell><cell></cell><cell></cell><cell>100,000</cell><cell>200,000</cell></row><row><cell>Run</cell><cell>R</cell><cell cols="2">P F 1</cell><cell>R</cell><cell cols="2">P F 1</cell><cell>R</cell><cell cols="2">P F 1</cell><cell>R</cell><cell cols="2">P F 1</cell><cell>R</cell><cell cols="2">P F 1</cell><cell>R P F 1</cell></row><row><cell>HELclrAM</cell><cell cols="2">4 42</cell><cell>7</cell><cell cols="12">6 24 10 20 20 20 33 14 19 50 10 17 65 7 12</cell></row><row><cell>HELq20rAM</cell><cell cols="2">4 42</cell><cell>8</cell><cell cols="12">6 25 10 20 20 20 33 14 19 50 10 17 65 7 12</cell></row><row><cell>ISIFuSAM</cell><cell cols="3">10 98 17</cell><cell cols="7">8 32 13 11 11 11 10</cell><cell>4</cell><cell cols="2">6 11</cell><cell>2</cell><cell>4 25 3</cell><cell>5</cell></row><row><cell>ISILRFTF</cell><cell cols="2">4 35</cell><cell cols="8">7 15 61 24 17 17 17 16</cell><cell>6</cell><cell cols="2">9 18</cell><cell>4</cell><cell>6 33 3</cell><cell>6</cell></row><row><cell>ISIRoTAM</cell><cell cols="2">5 50</cell><cell>9</cell><cell cols="2">5 21</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>3</cell><cell>4</cell><cell>8</cell><cell>2</cell><cell>3 25 3</cell><cell>5</cell></row><row><cell>ISIROTTF</cell><cell cols="10">10 98 18 10 38 15 12 12 12 11</cell><cell>5</cell><cell cols="2">7 12</cell><cell>2</cell><cell>4 27 3</cell><cell>5</cell></row><row><cell>ISITrFAM</cell><cell cols="15">7 70 13 13 54 21 53 51 52 65 25 36 69 14 23 71 7 13</cell></row><row><cell>ISITRFTF</cell><cell cols="15">7 68 12 15 65 25 42 43 42 56 22 32 69 14 23 71 7 13</cell></row><row><cell>mlbclsAF</cell><cell cols="2">2 17</cell><cell>3</cell><cell cols="2">6 23</cell><cell cols="8">9 18 18 18 29 12 17 45</cell><cell cols="2">9 15 53 5 10</cell></row><row><cell>mlblrnTF</cell><cell cols="2">3 33</cell><cell>6</cell><cell cols="7">9 35 14 14 14 14 14</cell><cell>6</cell><cell cols="2">8 13</cell><cell>3</cell><cell>4 18 2</cell><cell>3</cell></row><row><cell>mlblrnTM</cell><cell cols="15">10 99 18 21 88 34 41 43 42 70 29 41 76 16 26 89 9 17</cell></row><row><cell>otL11BTM</cell><cell cols="10">6 66 12 15 61 24 25 25 25 23</cell><cell cols="3">9 13 24</cell><cell>5</cell><cell>8 34 3</cell><cell>6</cell></row><row><cell>otL11FTM</cell><cell cols="15">7 74 13 22 99 36 37 39 38 66 26 37 79 15 26 95 9 17</cell></row><row><cell>otL11HTM</cell><cell cols="15">7 69 13 16 67 26 39 41 40 66 26 37 80 15 26 96 9 17</cell></row><row><cell>priindAM</cell><cell cols="2">5 50</cell><cell>9</cell><cell cols="2">5 20</cell><cell>8</cell><cell>5</cell><cell>5</cell><cell cols="2">5 10</cell><cell>4</cell><cell cols="2">6 18</cell><cell>4</cell><cell>6 28 3</cell><cell>5</cell></row><row><cell>rec03TF</cell><cell cols="15">7 68 12 18 75 29 50 51 50 66 26 37 82 16 27 89 9 17</cell></row><row><cell>rec04TM</cell><cell cols="15">8 77 14 18 75 29 48 48 48 60 23 33 82 16 27 91 9 17</cell></row><row><cell>tcdAF</cell><cell cols="3">9 99 17</cell><cell cols="2">6 23</cell><cell cols="8">9 11 11 11 29 12 17 42</cell><cell cols="2">9 14 77 8 14</cell></row><row><cell>URS205AM</cell><cell cols="2">4 41</cell><cell cols="13">7 17 68 27 39 37 38 52 21 30 54 11 18 58 6 11</cell></row><row><cell>USFDSETF</cell><cell cols="2">3 27</cell><cell>5</cell><cell cols="2">3 13</cell><cell cols="10">5 49 48 48 54 21 30 58 12 19 65 7 12</cell></row><row><cell>USFEOLTF</cell><cell cols="13">9 86 16 17 62 27 30 29 30 36 14 21 44</cell><cell cols="2">9 15 51 5</cell><cell>9</cell></row><row><cell>USFMOPTF</cell><cell cols="3">6 61 11</cell><cell cols="12">9 37 15 43 40 41 57 22 32 60 12 20 61 6 11</cell></row><row><cell>UWABASAF</cell><cell cols="3">9 97 17</cell><cell cols="12">9 37 15 24 24 24 46 18 26 54 11 18 58 6 11</cell></row><row><cell>UWABASAM</cell><cell cols="2">4 45</cell><cell cols="13">8 15 62 24 30 31 31 50 20 28 62 12 21 74 7 13</cell></row><row><cell>UWALINAF</cell><cell cols="2">1 12</cell><cell>2</cell><cell cols="2">5 20</cell><cell cols="5">8 10 10 10 21</cell><cell cols="3">9 12 42</cell><cell cols="2">8 14 67 7 12</cell></row><row><cell>UWALINAM</cell><cell cols="15">10 89 17 20 82 33 41 43 42 64 25 36 77 16 26 83 9 16</cell></row><row><cell>UWASNAAF</cell><cell cols="3">6 64 12</cell><cell cols="10">8 31 12 22 21 22 38 15 22 38</cell><cell cols="2">8 13 38 4</cell><cell>7</cell></row><row><cell>UWASNAAM</cell><cell cols="3">6 65 12</cell><cell cols="7">9 37 15 18 18 18 23</cell><cell cols="3">9 13 31</cell><cell cols="2">6 10 68 7 12</cell></row></table><note coords="8,108.63,443.31,431.36,9.74;8,72.00,455.43,114.61,8.64"><p>Topic 401 Recall (%), Precision (%), and F 1 at representative document review cutoffs. The best result for each cutoff is shown in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,477.22,681.31,62.78,8.64"><head>Table 6 :</head><label>6</label><figDesc>Topic 402 Recall (%), Precision (%), and F 1 at representative document review cutoffs. The best result for each cutoff is shown in bold.</figDesc><table coords="9,477.22,681.31,62.78,8.64"><row><cell>through 7 more</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,72.00,212.24,468.00,341.97"><head>Table 7 :</head><label>7</label><figDesc>Topic 403 Recall (%), Precision (%), and F 1 at representative document review cutoffs. The best result for each cutoff is shown in bold.</figDesc><table coords="11,80.84,212.24,447.14,308.31"><row><cell></cell><cell></cell><cell>2,000</cell><cell></cell><cell></cell><cell>5,000</cell><cell></cell><cell>20,000</cell><cell></cell><cell>50,000</cell><cell></cell><cell>100,000</cell><cell></cell><cell>200,000</cell></row><row><cell>Run</cell><cell>R</cell><cell cols="2">P F 1</cell><cell>R</cell><cell cols="2">P F 1</cell><cell cols="2">R P F 1</cell><cell cols="2">R P F 1</cell><cell cols="2">R P F 1</cell><cell>R P F</cell></row><row><cell>HELclrAM</cell><cell>11</cell><cell>7</cell><cell cols="2">8 16</cell><cell>4</cell><cell cols="2">7 28 2</cell><cell>3</cell><cell>62 2</cell><cell>3</cell><cell>64 1</cell><cell>2</cell><cell>66 0</cell></row><row><cell>HELq20rAM</cell><cell>11</cell><cell>7</cell><cell cols="2">9 17</cell><cell>4</cell><cell cols="2">7 28 2</cell><cell>3</cell><cell>62 2</cell><cell>3</cell><cell>64 1</cell><cell>2</cell><cell>66 0</cell></row><row><cell>ISIFUSAM</cell><cell cols="4">26 16 20 28</cell><cell cols="3">7 11 28 2</cell><cell>3</cell><cell>29 1</cell><cell>1</cell><cell>36 0</cell><cell>1</cell><cell>38 0</cell></row><row><cell>ISILrFTF</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>7 0</cell><cell>1</cell><cell>9 0</cell><cell>0</cell><cell>23 0</cell><cell>1</cell><cell>27 0</cell></row><row><cell>ISIROTAM</cell><cell cols="4">20 12 15 20</cell><cell>5</cell><cell cols="2">8 21 1</cell><cell>2</cell><cell>22 1</cell><cell>1</cell><cell>31 0</cell><cell>1</cell><cell>33 0</cell></row><row><cell>ISIRoTTF</cell><cell>6</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>2</cell><cell>3</cell><cell>8 0</cell><cell>1</cell><cell>9 0</cell><cell>0</cell><cell>23 0</cell><cell>1</cell><cell>27 0</cell></row><row><cell>ISITRFAM</cell><cell cols="7">21 13 16 52 12 20 63 4</cell><cell>7</cell><cell>96 2</cell><cell>5</cell><cell>99 1</cell><cell>2</cell><cell>99 1</cell></row><row><cell>ISITrFTF</cell><cell>8</cell><cell>5</cell><cell cols="2">6 12</cell><cell>3</cell><cell cols="2">5 47 3</cell><cell>5</cell><cell>49 1</cell><cell>2</cell><cell>60 1</cell><cell>1</cell><cell>63 0</cell></row><row><cell>mlbclsAF</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>1</cell><cell>3 0</cell><cell>0</cell><cell>4 0</cell><cell>0</cell><cell>4 0</cell><cell>0</cell><cell>4 0</cell></row><row><cell>mlblrnTM</cell><cell>8</cell><cell>5</cell><cell>6</cell><cell>8</cell><cell>2</cell><cell>3</cell><cell>8 1</cell><cell>1</cell><cell>9 0</cell><cell>0</cell><cell>10 0</cell><cell>0</cell><cell>16 0</cell></row><row><cell>otL11BTM</cell><cell cols="4">28 18 22 32</cell><cell cols="3">8 13 64 4</cell><cell>8</cell><cell>65 2</cell><cell>3</cell><cell>67 1</cell><cell>2</cell><cell>68 0</cell></row><row><cell>otL11FTM</cell><cell cols="7">60 37 46 66 17 27 68 4</cell><cell>8</cell><cell>98 2</cell><cell>5</cell><cell>98 1</cell><cell>2</cell><cell>98 1</cell></row><row><cell>otL11HTM</cell><cell cols="7">30 18 23 62 15 24 69 4</cell><cell cols="2">8 100 2</cell><cell cols="2">5 100 1</cell><cell cols="2">2 100 1</cell></row><row><cell>priindAM</cell><cell cols="4">20 12 15 20</cell><cell>5</cell><cell cols="2">8 20 1</cell><cell>2</cell><cell>20 0</cell><cell>1</cell><cell>23 0</cell><cell>1</cell><cell>25 0</cell></row><row><cell>rec03TF</cell><cell cols="8">31 19 23 95 26 41 97 7 12</cell><cell>99 3</cell><cell>5</cell><cell>99 1</cell><cell cols="2">2 100 1</cell></row><row><cell>rec04TM</cell><cell cols="8">27 17 21 87 23 36 92 7 13</cell><cell>93 3</cell><cell>5</cell><cell>94 1</cell><cell>2</cell><cell>96 1</cell></row><row><cell>tcdAF</cell><cell>13</cell><cell cols="3">8 10 22</cell><cell>5</cell><cell cols="2">9 31 2</cell><cell>4</cell><cell>37 1</cell><cell>2</cell><cell>70 1</cell><cell>2</cell><cell>99 1</cell></row><row><cell>URS205AM</cell><cell cols="4">17 11 13 18</cell><cell>4</cell><cell cols="2">7 50 3</cell><cell>6</cell><cell>50 1</cell><cell>3</cell><cell>49 1</cell><cell>1</cell><cell>49 0</cell></row><row><cell>UWABASAF</cell><cell cols="4">17 11 13 22</cell><cell>6</cell><cell cols="2">9 59 4</cell><cell>7</cell><cell>62 2</cell><cell>3</cell><cell>61 1</cell><cell>1</cell><cell>62 0</cell></row><row><cell cols="8">UWABASAM 60 44 51 62 18 28 62 4</cell><cell>7</cell><cell>65 2</cell><cell>3</cell><cell>64 1</cell><cell>2</cell><cell>66 0</cell></row><row><cell>UWALINAF</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>1</cell><cell>5 0</cell><cell>1</cell><cell>38 1</cell><cell>2</cell><cell>46 1</cell><cell>1</cell><cell>55 0</cell></row><row><cell>UWALINAM</cell><cell cols="7">50 36 42 76 20 31 82 5</cell><cell>9</cell><cell>88 2</cell><cell>4</cell><cell>95 1</cell><cell>2</cell><cell>98 1</cell></row><row><cell>UWASNAAF</cell><cell>14</cell><cell cols="6">9 11 46 11 18 53 3</cell><cell>6</cell><cell>53 1</cell><cell>3</cell><cell>59 1</cell><cell>1</cell><cell>64 0</cell></row><row><cell cols="8">UWASNAAM 45 28 35 52 12 20 60 4</cell><cell>7</cell><cell>60 2</cell><cell>3</cell><cell>62 1</cell><cell>2</cell><cell>66 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,72.00,556.85,468.00,20.59"><head>Table 8 :</head><label>8</label><figDesc>Topic 401 Participant-Estimated Recall (%), Actual Recall (%), and Error in Estimate. "+" indicates an overestimate; "-" indicates an underestimate. Topic 401 Gain Curves. For each participant, the run with the best AUC score is shown. Topic 402 Gain Curves. For each participant, the run with the best AUC score is shown.</figDesc><table coords="13,93.23,192.65,422.18,387.64"><row><cell></cell><cell></cell><cell></cell><cell cols="4">Topic 401 Recall vs. Documents Reviewed</cell><cell></cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall (%)</cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">otL11FTM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">mlblrnTM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UWALINAM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">rec03TF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tcdAF</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HELq20rAM USFDSETF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ISITRFTF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">URS205AM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">priindAM</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell><cell>700</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Thousands of Documents Reviewed</cell><cell></cell></row><row><cell></cell><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="16,72.00,323.93,468.00,20.59"><head>Table 9 :</head><label>9</label><figDesc>Topic 402 Participant-Estimated Recall (%) and Error in Estimate. "+" indicates an overestimate; "-" indicates an underestimate.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="5,86.35,652.53,453.65,6.91;5,72.00,661.99,128.82,6.91"><p>Due to logistical challenges, the Track coordinators were unable to enforce the initial and interim submission requirements, with the consequence that these results are incomplete.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The Legal Track would not have been possible without the efforts of a great many people. Our sincere thanks go to the Topic Authorities (<rs type="person">Kevin F. Brady</rs>, <rs type="person">Brendan M. Schulman</rs>, and <rs type="person">Robert Singleton</rs>) for their diligent efforts in interpreting the topics, providing participating teams with assessments, and adjudicating the results of the sample review. We are grateful to the assessors (and, especially, to the four firms that made the assessors available: <rs type="institution">ACT Litigation Services, Inc.</rs>; <rs type="funder">Business Intelligence Associates, Inc.</rs> ("<rs type="projectName">BIA</rs>"); Daegis; and <rs type="funder">IE Discovery, Inc.</rs>) for conducting the first-pass review of the evaluation samples. Finally, we are indebted to <rs type="person">Ellen Voorhees</rs> and <rs type="person">Ian Soboroff</rs> of <rs type="funder">NIST</rs> for their patient and thoughtful guidance in sorting out the many issues that arise in conducting an evaluation of this sort.</p></div>
			</div>
			<div type="funding">
<div> †  <p>This work has been supported in part by the <rs type="funder">National Science Foundation</rs> under grant <rs type="grantNumber">IIS-1065250</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. 1 Track coordinator <rs type="person">Gordon V. Cormack</rs> did not participate in the <rs type="institution">University of Waterloo</rs>'s TREC 2011 Legal Track effort.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_EyeA2kw">
					<orgName type="project" subtype="full">BIA</orgName>
				</org>
				<org type="funding" xml:id="_CwbxfKC">
					<idno type="grant-number">IIS-1065250</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Curve ("AUC"), as percentages with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Hypothetical F 1 (%) Actual F 1 (%) AUC (%) HELclrAM </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="20,91.92,96.36,250.38,7.77" xml:id="b0">
	<monogr>
		<ptr target="http://edrm.net/projects/dataset" />
		<title level="m" coord="20,91.92,96.36,85.19,7.77">EDRM Data Set Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,111.31,448.08,7.77;20,91.92,122.27,41.33,7.77" xml:id="b1">
	<monogr>
		<ptr target="http://www.ferc.gov/industries/electric/indus-act/wec/enron/info-release.asp" />
		<title level="m" coord="20,91.92,111.31,158.34,7.77">Information released in Enron investigation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,137.21,166.36,7.77;20,91.92,148.17,293.74,7.77" xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Trec-</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/topics/LT09_Complaint_J_final.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="20,138.00,137.21,95.00,7.77">Legal Track -Complaint J</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,163.11,169.35,7.77;20,91.92,174.07,333.07,7.77" xml:id="b3">
	<monogr>
		<ptr target="http://trec-legal.umiacs.umd.edu/topics/LT10_Complaint_K_final-corrected.pdf" />
		<title level="m" coord="20,91.92,163.11,165.31,7.77">TREC-2010 Legal Track -Complaint K, 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,188.86,448.08,7.93;20,91.92,199.82,65.66,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="20,303.59,189.02,124.09,7.77">TREC 2006 Legal Track overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,447.34,188.86,92.67,7.72;20,91.92,199.82,39.31,7.72">Proc. 15th Text REtrieval Conference</title>
		<meeting>15th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,214.92,448.08,7.77;20,91.92,225.73,158.51,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="20,378.52,214.92,146.98,7.77">Overview of the TREC 2010 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,91.92,225.73,132.16,7.72">Proc. 19th Text REtrieval Conference</title>
		<meeting>19th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,240.67,448.08,7.93;20,91.92,251.63,448.08,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="20,257.58,240.82,196.43,7.77">Statistical precision of information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,472.31,240.67,67.70,7.72;20,91.92,251.63,385.41,7.72">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,277.53,351.56,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="20,143.97,277.68,116.48,7.77">An introduction to ROC analysis</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,267.14,277.53,93.08,7.72">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,292.63,448.08,7.77;20,91.92,303.44,356.93,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="20,266.25,292.63,273.75,7.77;20,91.92,303.59,139.77,7.77">Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,238.57,303.44,150.09,7.72">Richmond Journal of Law and Technology</title>
		<imprint>
			<biblScope unit="volume">XVII</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,318.53,448.08,7.77;20,91.92,329.34,205.48,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="20,261.33,318.53,278.67,7.77;20,91.92,329.49,90.39,7.77">Inconsistent responsiveness determination in document review: Difference of opinion or human errror?</title>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,186.07,329.34,61.02,7.72">Pace Law Review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,344.28,448.08,7.93;20,91.92,355.24,137.00,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,357.26,344.43,146.82,7.77">Overview of the TREC 2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,520.73,344.28,19.27,7.72;20,91.92,355.24,110.65,7.72">Proc. 18th Text REtrieval Conference</title>
		<meeting>18th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,370.18,448.08,7.93;20,91.92,381.14,137.00,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,357.26,370.34,146.82,7.77">Overview of the TREC 2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,520.73,370.18,19.27,7.72;20,91.92,381.14,110.65,7.72">Proc. 17th Text REtrieval Conference</title>
		<meeting>17th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,396.09,405.45,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,175.34,396.24,138.22,7.77">The probability ranking principle in IR</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,320.52,396.09,92.55,7.72">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,411.18,448.08,7.77;20,91.92,421.99,158.51,7.93" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,374.50,411.18,149.61,7.77">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,91.92,421.99,132.16,7.72">Proc. 16th Text REtrieval Conference</title>
		<meeting>16th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.92,436.93,448.08,7.93;20,91.92,448.05,54.54,7.77" xml:id="b14">
	<monogr>
		<title level="m" coord="20,270.26,436.93,269.74,7.93;20,91.92,448.05,28.11,7.77">Proc. 20th Text REtrieval Conference (TREC 2011) Proceedings. NIST SP 500-295</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>20th Text REtrieval Conference (TREC 2011) eedings. NIST SP 500-295</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
