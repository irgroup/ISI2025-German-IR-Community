<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.40,114.69,277.25,12.31">Overview of the TREC 2011 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,99.84,148.42,105.76,10.97"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
						</author>
						<author>
							<persName coords="1,237.36,148.42,70.08,10.97;1,248.40,162.46,48.10,10.97"><forename type="first">Nick</forename><forename type="middle">Craswell</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName coords="1,423.48,148.42,93.77,10.97"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Ian Soboroff NIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.40,114.69,277.25,12.31">Overview of the TREC 2011 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83CEF1F064D7CEFC3CCBC48956584CD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Web Track explores and evaluates Web retrieval technology over large collections of Web data. In its current incarnation, the Web Track has been active since TREC 2009, where it included both a traditional adhoc retrieval task and a new diversity task <ref type="bibr" coords="1,441.48,276.70,10.91,10.91" target="#b3">[4]</ref>. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For TREC 2010 the track introduced a new Web spam task and Web-style, six-level relevance assessment for the adhoc task <ref type="bibr" coords="1,466.80,317.26,10.91,10.91" target="#b4">[5]</ref>. For TREC 2011, as recommended by participants at the track planning session held during TREC 2010, we dropped the spam task but continued the other tasks essentially unchanged. As we did for TREC 2009 and TREC 2010, we based our TREC 2011 experiments on the billion-page ClueWeb09<ref type="foot" coords="1,535.08,356.86,4.23,5.47" target="#foot_0">1</ref> collection created by the Language Technologies Institute at Carnegie Mellon University.</p><p>The tasks use a common topic set, differing only in their evaluation methodology. Topics are created from the logs of a commercial search engine, with the aid of tools developed at Microsoft Research <ref type="bibr" coords="1,117.72,412.18,10.91,10.91" target="#b8">[9]</ref>. Given a target query, these tools extract and analyze groups of related queries, using co-clicks and other information, to identify clusters of queries that highlight different aspects and interpretations of the target query. These clusters are employed by NIST for topic development. Each resulting topic is structured as a representative set of subtopics, each related to a different user need. The selection of subtopics attempts to reflect a mix of genuine user requirements for the topic.</p><p>For the adhoc task documents are judged with respect to the topic as a whole. Relevance levels are similar in structure to the levels used in commercial Web search, including a spam/junk level. Moreover, the top two levels of the assessment structure are closely related to the homepage finding and topic distillation tasks appearing in older Web Tracks. For the diversity task, documents are judged with respect to the subtopics, as well as with respect to the topic as a whole.</p><p>For TREC 2011, the topic selection process was modified slightly from previous years. For TREC 2009 and 2010, topics were chosen to be of medium-to-high frequency. TREC 2011 attempts to work with more obscure topics, which may still be underspecified (i.e., faceted) but may be less ambiguous. Search engines have difficulty with queries of this type, since they can rely less on click/anchor information, and popularity signals like PageRank. With these new tough topics we hope to work in an area of Web retrieval that has received relatively little attention. Given the smaller number of pages that may be relevant for these tough topics, we may potentially be able to create a more reusable collection, with sufficiently complete judgments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Category A and B Collections</head><p>The billion-page ClueWeb09 collection was crawled from the general Web during January and February 2009, and consists of 25TB of uncompressed data (5TB compressed) in multiple languages. Since some participants were not able to work with the full collection, the track accepted runs based on the smaller "Category B" subset of the full "Category A" collection. This Category B data set comprises about 50 million English-language pages, including the entirety of the English-language Wikipedia. Nonetheless, we strongly encouraged participants to use the full Category A data set, if possible. Results reported in this paper are labeled by their collection category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>NIST created and assessed 50 new topics for the track. Figure <ref type="figure" coords="2,373.80,394.18,5.45,10.91" target="#fig_0">1</ref> provides two examples.</p><p>Each topic contains a query field, a description field, and several subtopic fields. The query is intended to represent the text a user might enter into a Web search engine, if they were seeking the information indicated by the description field or by any of the subtopics. For the adhoc task, relevance is judged on the basis of the description. For the diversity task, relevance is judged separately with respect to each subtopic. Initially, only the query field was released to track participants. The full topics were not released until the participants had submitted their runs.</p><p>Each topic is assigned one of two types. Topics with ambiguous queries, such as topic 140 in figure <ref type="figure" coords="2,116.40,502.54,4.21,10.91" target="#fig_0">1</ref>, have several unrelated interpretations. One of these interpretations is chosen for the description, while a wider range of interpretations appear in the subtopics. Topics with faceted queries, such as topic 114 in the figure, have one primary interpretation, reflected in the description field. For these queries, the subtopics address various aspects of the broader topic. In all topics, the description field and the first subtopic field are identical.</p><p>Each subtopic is assigned one of two types. Navigational subtopics (with type "nav") assume the user is seeking a specific page or site. Navigational subtopics may often have only a single relevant page. Informational subtopics (with type "inf") assume the user is seeking information without regard to its source, provided that the source is reliable. Informational subtopics may often have a large number of relevant pages. Subtopics were chosen to be roughly balanced in terms of popularity. Strange and unusual aspects and interpretations were avoided as much as possible.</p><p>All topics are expressed in English. Non-English documents are never considered relevant, even if the assessor understands the language of the document and the document would be relevant in that language. &lt;topic number="114" type="faceted"&gt; &lt;query&gt;adobe indian houses&lt;/query&gt; &lt;description&gt; How does one build an adobe house? &lt;/description&gt; &lt;subtopic number="1" type="inf"&gt; How does one build an adobe house? &lt;/subtopic&gt; &lt;subtopic number="2" type="inf"&gt; information about Indian tribes that used adobe houses &lt;/subtopic&gt; &lt;subtopic number="3" type="nav"&gt; I'd like to order books or videos/CDs about how to construct adobe buildings. &lt;/subtopic&gt; &lt;/topic&gt; &lt;topic number="140" type="ambiguous"&gt; &lt;query&gt;east ridge high school&lt;/query&gt; &lt;description&gt; demographics of East Ridge High School in Lick Creek, Kentucky &lt;/description&gt; &lt;subtopic number="1" type="inf"&gt; demographics of East Ridge High School in Lick Creek, Kentucky &lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt; home page for East Ridge High School in Chattanooga, Tennessee &lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt; information about the sports program at East Ridge High School in Clermont, Florida &lt;/subtopic&gt; &lt;subtopic number="4" type="inf"&gt; description of the sports facilities at East Ridge High School in Woodbury, MN &lt;/subtopic&gt; &lt;/topic&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology and Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adhoc Task</head><p>An adhoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. The goal of an adhoc task is to return a ranking of the documents in the collection in order of decreasing probability of relevance. The probability of relevance of a document is considered independently of other documents that appear before it in the result list.</p><p>For the adhoc task, documents are judged on the basis of the description field using a five-point scale, defined as follows:</p><p>1. Nav: This page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site. (relevance grade 3)</p><p>2. Key: This page or site is dedicated to the topic; authoritative and comprehensive, it is worthy of being a top result in a web search engine. (relevance grade 2)</p><p>3. Rel: The content of this page provides information on the topic; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page. (relevance grade 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Non:</head><p>The content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query.</p><p>(relevance grade 0)</p><p>5. Junk: This page does not appear to be useful for any reasonable purpose; it may be spam or junk. (relevance grade -2)</p><p>After each description, we list the relevance grade assigned to that level for the purpose of calculating graded effectiveness measures. Please note that the original track guidelines for TREC 2011 incorrectly listed a six-point scale. The primary effectiveness measure for the adhoc task is expected reciprocal rank (ERR) as defined by Chapelle et al. <ref type="bibr" coords="4,200.88,485.38,10.91,10.91" target="#b1">[2]</ref>. We also report a variant of nDCG <ref type="bibr" coords="4,393.36,485.38,10.91,10.91" target="#b7">[8]</ref>, as well as standard binary measures, including mean average precision (MAP) and precision at rank k (P@k). We compute ERR at rank k (ERR@k) as follows:</p><formula xml:id="formula_0" coords="4,217.08,536.27,322.83,32.00">ERR@k = k i=1 R(g i ) i i-1 j=1 (1 -R(g i )),<label>(1)</label></formula><p>where R(g) = 2 g -1</p><p>16 and g 1 , g 2 , ..., g k are the relevance grades associated with the top k documents. We compute nDCG@k as DCG@k ideal DCG@k , where</p><formula xml:id="formula_1" coords="4,238.80,625.67,296.50,32.12">DCG@k = k i=1 2 g i -1 log 2 (1 + i) . (<label>2</label></formula><formula xml:id="formula_2" coords="4,535.30,635.74,4.61,10.91">)</formula><p>For the binary relevance measures, we treat grades 1-4 as relevant and grade 0 as non-relevant. We apply trec_eval to compute the binary measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diversity Task</head><p>The diversity task is similar to the adhoc retrieval task, but differs in its judging criteria and evaluation measures. The goal of the diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For this task, the probability of relevance of a document is conditioned on the documents that appear before it in the result list.</p><p>For the diversity task, documents are judged on the basis of the subtopics. For each subtopic, a binary judgment indicates whether or not a document satisfies the information need associated with that subtopic. For the TREC 2011 track, assessors made actually made graded judgments. To apply evaluation measures, we mapped these graded judgments to binary judgments by treating values &gt; 0 as relevant and values ≤ 0 as not relevant. However, the graded judgments are available in the TREC data repository for the use of interested participants.</p><p>The primary effectiveness measure for the adhoc task is a variant of intent-aware expected reciprocal rank (ERR-IA) as defined by Chapelle et al. <ref type="bibr" coords="5,332.40,257.98,10.91,10.91" target="#b1">[2]</ref>. We also report a number of other intent aware measures appearing in the literature, including α-nDCG@k <ref type="bibr" coords="5,388.56,271.54,10.91,10.91" target="#b6">[7]</ref>, NRBP <ref type="bibr" coords="5,441.48,271.54,10.91,10.91" target="#b5">[6]</ref>, and MAP-IA <ref type="bibr" coords="5,525.48,271.54,10.82,10.91" target="#b0">[1]</ref>. Clarke et al. <ref type="bibr" coords="5,135.48,285.10,11.54,10.91" target="#b2">[3]</ref> provide a detailed description and analysis of the novelty and diversity measures employed in the TREC Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pooling and Judging</head><p>For each topic, participants in the adhoc and diversity tasks submitted a ranking of the top 10,000 documents for that topic. All submitted runs were included in the pool for judging. This year, a common pool was used for both tasks, and all runs were judged to depth 25 using both the adhoc and diversity judging criteria. In this paper, we report results only for runs explicitly submitted to one task or the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Cat ERR@20 nDCG@20 P@20 MAP Chinese Acad. of Sciences </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table" coords="6,101.52,470.02,5.45,10.91" target="#tab_1">2</ref> presents the top adhoc task results ordered by ERR@20. Table <ref type="table" coords="6,411.60,470.02,5.45,10.91" target="#tab_2">3</ref> presents the top diversity task results ordered by ERR@20. The figures mix results for both Category A and B runs. All runs submitted to the adhoc and diversity tasks were judged according to the judging criteria of both tasks, even runs that were not submitted to both tasks. This additional judging allows us to make direct comparisons between runs optimized for the two tasks, supporting efforts to determine if the different judging criteria and evaluation measures identify genuine differences. For example, figure <ref type="figure" coords="6,149.40,551.26,5.45,10.91" target="#fig_2">2</ref> provides a scatter plot comparing the performance of the runs under ERR@20 and ERR-IA@20, the primary effectiveness measures for the adhoc and diversity tasks respectively. While the values are correlated, there are clear differences in the relative performance of runs under the two measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Plans</head><p>Given the drop in participants, we believe that we must develop new tasks and ideas for the track to go forward for another year. We should not simply repeat the current tasks on the same collection. Some possible ideas were proposed during a workshop on Diversity in Document Retrieval held at ECIR 2011. Breakout groups at the workshop identified a number of possible extensions to the current task set, which we hope to explore during our planning session at TREC. These extensions include: 1) a query suggestion task, 2) a subtopic mining task, and 3) a snippet generation task. Jamie Callan's research group at CMU, who created the ClueWeb09 collection, is considering the creation of a new collection. This collection will be of a similar size to ClueWeb09, but will address some of its known problems. If this new collection is available in time for TREC 2012 tasks, we plan to switch to it, providing researchers with a set of topics and judgments over the new collection.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,176.64,598.78,253.70,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of TREC 2011 Web track topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,156.41,465.67,3.89,6.48;9,150.57,428.82,9.73,6.48;9,150.57,391.97,9.73,6.48;9,150.57,355.07,9.73,6.48;9,150.57,318.22,9.73,6.48;9,150.57,281.37,9.73,6.48;9,150.57,244.52,9.73,6.47;9,163.53,472.67,3.89,6.48;9,235.51,472.67,13.62,6.48;9,314.36,472.67,9.73,6.48;9,389.26,472.67,13.62,6.48;9,468.06,472.67,9.73,6.48;9,133.07,381.91,6.47,38.61;9,133.07,354.30,6.47,25.66;9,133.07,326.68,6.48,25.67;9,133.07,295.18,6.47,29.56;9,263.31,483.17,109.80,6.48"><head></head><label></label><figDesc>@20 (primary diversity measure) ERR@20 (primary adhoc measure)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,82.68,513.10,441.74,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of runs under the primary adhoc and diversity effectiveness measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,73.87,467.91,132.50"><head>Table 1 :</head><label>1</label><figDesc>Participation in the TREC 2011 Web track</figDesc><table coords="2,72.00,73.87,467.91,132.50"><row><cell cols="4">Task Adhoc Diversity Total</cell></row><row><cell>Groups</cell><cell>14</cell><cell>9</cell><cell>16</cell></row><row><cell>Runs</cell><cell>37</cell><cell>25</cell><cell>62</cell></row><row><cell cols="4">Table 1 summarizes participation in the TREC 2011 Web Track. A total of 16 groups partic-</cell></row><row><cell cols="4">ipated in the track this year, a substantial decrease from last year, when 23 groups participated,</cell></row><row><cell cols="2">and from TREC 2009, when 26 groups participated.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,88.30,507.05,287.15"><head>Table 2 :</head><label>2</label><figDesc>Top adhoc task results ordered by ERR@20. Only the best run from each group is included in the ranking.</figDesc><table coords="6,78.00,88.30,501.05,287.15"><row><cell cols="2">(ICT) ICTNET11ADR3</cell><cell>A</cell><cell>0.157</cell><cell>0.283</cell><cell>0.345</cell><cell>0.175</cell></row><row><cell>Univ. of Glasgow (Terrier)</cell><cell>uogTrA45Vm</cell><cell>A</cell><cell>0.149</cell><cell>0.305</cell><cell>0.347</cell><cell>0.203</cell></row><row><cell>Univ. of Waterloo (MDS)</cell><cell>UWatMDSqlt</cell><cell>A</cell><cell>0.144</cell><cell>0.242</cell><cell>0.307</cell><cell>0.135</cell></row><row><cell>Microsoft Research</cell><cell>msrsv2011a3</cell><cell>A</cell><cell>0.143</cell><cell>0.273</cell><cell>0.327</cell><cell>0.172</cell></row><row><cell>Srchvrs</cell><cell>srchvrs11b</cell><cell>B</cell><cell>0.131</cell><cell>0.233</cell><cell>0.298</cell><cell>0.110</cell></row><row><cell>Univ. of Ottawa</cell><cell>DFalah11</cell><cell>B</cell><cell>0.122</cell><cell>0.204</cell><cell>0.275</cell><cell>0.079</cell></row><row><cell>Univ. of Amsterdam (Kamps)</cell><cell>UAmsM705tiLS</cell><cell>B</cell><cell>0.119</cell><cell>0.202</cell><cell>0.273</cell><cell>0.085</cell></row><row><cell>Univ. of Waterloo (Clarke)</cell><cell>uwBAadhoc</cell><cell>A</cell><cell>0.119</cell><cell>0.172</cell><cell>0.230</cell><cell>0.079</cell></row><row><cell>Group</cell><cell>Run</cell><cell cols="5">Cat ERR-IA@20 α-nDCG@20 NRBP</cell></row><row><cell>Univ. of Glasgow (Terrier)</cell><cell>uogTrA45Nmx2</cell><cell>A</cell><cell>0.528</cell><cell>0.630</cell><cell></cell><cell>0.487</cell></row><row><cell>Microsoft Research</cell><cell>msrsv2011d1</cell><cell>A</cell><cell>0.499</cell><cell>0.608</cell><cell></cell><cell>0.456</cell></row><row><cell>Univ. of Waterloo (MDS)</cell><cell>UWatMDSqltsr</cell><cell>A</cell><cell>0.494</cell><cell>0.595</cell><cell></cell><cell>0.457</cell></row><row><cell>Chinese Acad. of Sciences (ICT)</cell><cell>ICTNET11DVR3</cell><cell>A</cell><cell>0.476</cell><cell>0.582</cell><cell></cell><cell>0.432</cell></row><row><cell>Univ. of Amsterdam (Kamps)</cell><cell>UAmsM705tFLS</cell><cell>B</cell><cell>0.438</cell><cell>0.522</cell><cell></cell><cell>0.407</cell></row><row><cell>Univ. of Waterloo (Clarke)</cell><cell>uwBA</cell><cell>A</cell><cell>0.399</cell><cell>0.492</cell><cell></cell><cell>0.359</cell></row><row><cell>Univ. of Delaware (Fang)</cell><cell>UDCombine2</cell><cell>B</cell><cell>0.375</cell><cell>0.458</cell><cell></cell><cell>0.340</cell></row><row><cell cols="2">Centrum Wiskunde + Informatica CWIcIA2t5b1</cell><cell>A</cell><cell>0.349</cell><cell>0.432</cell><cell></cell><cell>0.304</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,399.10,467.98,24.47"><head>Table 3 :</head><label>3</label><figDesc>Top diversity task results ordered by ERR-IA@20. Only the best run from each group is included in the ranking.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.56,678.52,171.27,6.22"><p>boston.lti.cs.cmu.edu/Data/clueweb09.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Again this year, we extend our thanks to <rs type="person">Jamie Callan</rs>, <rs type="person">Mark Hoy</rs>, and the <rs type="institution">Language Technologies Institute at Carnegie Mellon University</rs>, who created and continue to distribute the ClueWeb09 collection. The track could not operate without this valuable resource.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,88.92,99.10,450.89,10.91;8,88.92,112.66,450.86,10.91;8,88.92,126.22,111.86,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,449.64,99.10,90.17,10.91;8,88.92,112.66,29.84,10.91">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">Sreenivas</forename><surname>Rakesh Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,143.40,113.03,332.08,10.05">2nd ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,148.78,451.07,10.91;8,88.92,162.34,450.98,10.91;8,88.92,175.90,99.26,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,419.16,148.78,120.83,10.91;8,88.92,162.34,95.36,10.91">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,209.88,162.71,324.23,10.05">18th ACM Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,198.34,451.05,10.91;8,88.92,211.90,451.18,10.91;8,88.92,225.46,208.82,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,411.84,198.34,128.13,10.91;8,88.92,211.90,208.04,10.91">A comparative analysis of cascade measures for novelty and diversity</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,325.80,212.27,214.30,10.05;8,88.92,225.83,114.52,10.05">th ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,248.02,450.86,10.91;8,88.92,261.58,317.66,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,352.20,248.02,183.06,10.91">Overview of the TREC 2009 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,102.60,261.95,148.46,10.05">18th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,284.02,451.12,10.91;8,88.92,297.58,432.14,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,463.44,284.02,76.60,10.91;8,88.92,297.58,105.06,10.91">Overview of the TREC 2010 web track</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,217.20,297.95,148.46,10.05">19th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,320.14,451.03,10.91;8,88.92,333.70,451.01,10.91;8,88.92,347.26,198.98,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,402.96,320.14,136.99,10.91;8,88.92,333.70,180.18,10.91">An effectiveness measure for ambiguous and underspecified queries</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,294.36,334.07,245.57,10.05;8,88.92,347.63,91.37,10.05">2nd International Conference on the Theory of Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,369.70,451.22,10.91;8,88.92,383.26,450.99,10.91;8,88.92,396.82,451.09,10.91;8,88.92,410.38,262.94,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,281.04,383.26,258.87,10.91;8,88.92,396.82,17.39,10.91">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Ashkann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,131.04,397.19,408.97,10.05;8,88.92,410.75,101.69,10.05">31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,432.82,450.98,10.91;8,88.92,446.38,312.62,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,291.72,432.82,243.39,10.91">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,88.92,446.75,206.48,10.05">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.92,468.94,450.94,10.91;8,88.92,482.50,451.10,10.91;8,88.92,496.06,24.62,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,342.36,468.94,197.50,10.91;8,88.92,482.50,45.53,10.91">Inferring query intent from reformulations and clicks</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,157.20,482.87,227.78,10.05">19th International World Wide Web Conference</title>
		<meeting><address><addrLine>Raleigh, North Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
