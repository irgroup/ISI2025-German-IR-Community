<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.72,71.87,374.28,16.84">Overview of the TREC-2011 Microblog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.22,116.90,57.82,11.06"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.84,116.90,90.24,11.06"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.87,116.90,53.62,11.06"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff1">
								<address>
									<settlement>Twitter, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.21,116.90,65.13,11.06"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
							<affiliation key="aff3">
								<orgName type="institution">NIST</orgName>
								<address>
									<settlement>Gaithersburg</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.72,71.87,374.28,16.84">Overview of the TREC-2011 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C8D3F35D6502239C72CA4AA049073187</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Microblog track examines search tasks and evaluation methodologies for information seeking behaviours in microblogging environments such as Twitter. It was first introduced in 2011, addressing a real-time adhoc search task, whereby the user wishes to see the most recent but relevant information to the query. In particular, systems should respond to a query by providing a list of relevant tweets ordered from newest to oldest, starting from the time the query was issued.</p><p>For TREC 2011, we used the newly-created Tweets2011 corpus. The corpus is comprised of 16M tweets over approximately two weeks, sampled courtesy of Twitter. The corpus is designed to be a reusable, representative sample of the twittersphere. As the reusability of a test collection is paramount in a TREC track, these sampled tweets can be obtained at any point in time (subjected to some caveats, discussed below). To accomplish this, the TREC Microblog track introduced a novel methodology whereby participants sign an agreement for the ids of the tweets in the corpus. Tools are then provided that permit the downloading of the corpus from the Twitter website.</p><p>The first Microblog track in TREC 2011 has been a remarkable success. A total of 59 groups participated in the track from across the world, with 184 submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TWEETS2011 CORPUS</head><p>The corpus was obtained using a donation of the unique identifiers of a sample of tweets from Twitter. Creating a sharable reference collection of tweets is difficult, because Twitter's terms of service forbids the redistribution of tweets. Among other reasons for this, Twitter users can delete their tweets (and indeed their entire account) or restrict their tweets to followers only, and these states can change during and outside the corpus epoch. We devised a novel methodology whereby participants obtain a list of identifiers pointing to the tweets in the corpus after signing a usage agreement. These identifiers are of the form (screen name, tweet id). Each identifier can be mapped to a URL at twitter.com which, when resolved, contains the tweet, delivered by Twitter according to their terms of service.</p><p>We developed a set of tools to generate a copy of the corpus given the list of tweet ids, as well as sample indexing and searching code. <ref type="foot" coords="1,335.98,208.49,2.99,5.18" target="#foot_0">1</ref> Participants and others obtaining the Tweets2011 collection agree not to redistribute the tweets in the collection, but anyone can obtain a substantially identical tweet set using the ids and tools. The set is only "substantially" identical because tweets may have been deleted or made private in the intervening time, and also some tweets may be unavailable due to transitory network failures.</p><p>The resulting corpus, called Tweets2011, consists of an approximately 1% sample (after some spam removal) of tweets from January 23, 2011 to February 7, 2011 (inclusive), totaling approximately 16 million tweets. Major events that took place within this time frame include the massive democracy demonstrations in Egypt as well as the Super Bowl in the United States. Each day of the corpus is split into files called blocks, each of which contains about 10,000 tweets compressed using gzip. Each tweet is in JSON format, similar (but not identical) to the format used by the Twitter streaming hoses. Within the corpus, tweets are ordered by tweet ids, which are roughly chronologically ordered for our purposes. The sample of tweets and the corresponding tools were released to the TREC participating groups on 16th May 2011.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REAL-TIME SEARCH TASK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>In TREC 2011, the Microblog track addressed one single pilot task, entitled the real-time search task, where the user wishes to see the most recent but relevant information to the query. The realtime search task can be summarised as: At time t, find tweets about topic X. This task is akin to adhoc search on Twitter, where a user's information need is represented by a query at a specific time. Participants were asked to rank the relevant tweets by time. One possible interpretation of the task is to rank all tweets up to time t, keep all interesting tweets, and then discard non-relevant tweets. Interestingness is subjective, but the issuer of a query might interpret it as providing added value with respect to the query topic. It is of note that for TREC 2011, the novelty between tweets was not considered.</p><p>NIST created 50 new topics based on the Tweets2011 collection, each representing an information need at a specific point in time. Figure <ref type="figure" coords="1,342.99,613.33,4.48,7.77" target="#fig_0">1</ref> shows an example topic. The &lt;querytime&gt; tag contains the timestamp of the query in a human and machine readable ISO standard form, while the &lt;querytweettime&gt; tag contains the timestamp of the query in terms of the chronologically nearest tweet id within the corpus. Moreover, while no narrative and description tags were provided to participants during the evaluation (as with earlier TREC adhoc topics), the topic developer created a clearly defined information need for later use during assessment. For assessing the tweets, the assessors judged the relevance of a tweet after reading it, and also by following any URLs linked from the tweet. Tweets were judged on the basis of the defined information need using a three-point scale:</p><p>Not Relevant. The content of the tweet does not provide any useful information on the topic, or is either written in a language other than English, or is a retweet.</p><p>Relevant. The tweet mentions or provides some minimally useful information on the topic.</p><p>Highly Relevant. A highly relevant tweet will either contain highly informative content, or link to highly informative content.</p><p>All assessments were conducted by NIST assessors. The primary evaluation measure was precision at rank 30 cutoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pooling and Judging</head><p>Participating groups were permitted to submit up to four runs to the real-time adhoc search task. At least one compulsory automatic run that does not use any external or future sources of evidence was also requested. For the purposes of the task, we defined external and future evidence as follows:</p><p>External Evidence: Evidence beyond the Tweets2011 corpus -for instance, this encompasses other tweets or information from Twitter, as well as other corpora, e.g., Wikipedia or the web.</p><p>Future Evidence: Information that would not have been available to the system at the timestamp of the query. For example, IDF scores computed using tweets not already posted at the timestamp of the query.</p><p>The participating groups were encouraged to rank their submitted runs by preference. For comparison purposes, the track requested at least one compulsory automatic run that abides by realtime and external resource constraints; beyond this, the participating groups were at liberty to submit manual, external and untimely runs, which could be useful to improve the quality of the test collection. TREC received 184 runs in total, from 59 participating groups. All runs were pooled to depth 30, according to the ranking indicated in each run. We later determined that this pooling process was problematic, but the problems did not affect the evaluation results reported here. We elaborate on the problems in the Discussion section below.</p><p>Simple retweets were removed from the pools (as they were de facto assumed to be non-relevant). Tweets were clustered to bring near-duplicates close together in the pools, using shingling <ref type="bibr" coords="2,276.72,644.45,9.52,7.77" target="#b1">[1]</ref>. <ref type="foot" coords="2,289.42,642.59,2.99,5.18" target="#foot_1">2</ref>We believe this sorting supported consistent judgments because the assessor would judge lexicographically similar tweets together, but we did not measure the effect on assessor consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head><p>All </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We first report evaluation results of the 59 participating groups with 49 topics. Topic 50 did not have any relevant tweets in the pool, and it was therefore dropped from the evaluation. As mentioned in Section 3.1, the primary measure for retrieval effectiveness was precision at rank 30 (P@30), but we also report mean average precision (MAP). Table <ref type="table" coords="2,432.66,256.41,4.48,7.77" target="#tab_0">1</ref> shows the per-topic best and medians of the submitted real-time search task runs. Since only 33 topics have tweets judged to be highly relevant in the pool, the table shows two separate sets of scores. The first considers all relevant and highly relevant tweets as relevant and is over 49 topics. The second considers only highly relevant tweets, and is over 33 topics. From the results, it appears that the real-time search task is challenging when we focus on only the highly-relevant tweets.</p><p>In the next analysis, we focus on the evaluation results using all 49 topics, where all relevant and highly relevant tweets are considered as relevant. Table <ref type="table" coords="2,398.75,361.02,4.48,7.77">2</ref> shows the best submitted compulsory runs from each participating group, ranked by P@30. Although this condition was required, not all groups followed the requirement; the 14 groups which did not submit compulsory runs are omitted. Mean Average Precision (MAP) is also reported in the table. The correlation between the ranking of groups by MAP and P@30 is high but not without noticable differences (Spearman's ρ = 0.82). Using a bootstrap test for discriminative power <ref type="bibr" coords="2,490.98,434.25,9.52,7.77" target="#b5">[5]</ref>, differences in P@30 or MAP of less than 0.07 have a run swap probability of greater than 5%, and thus are not deemed to be meaningful.</p><p>Table <ref type="table" coords="2,348.46,465.63,4.48,7.77" target="#tab_1">3</ref> shows the best performing run from each participating group, regardless of the run type, and the extent to which it abides by the real-time and external resources constraints. In contrast to Table <ref type="table" coords="2,339.72,497.01,3.36,7.77">2</ref>, all 59 groups are present. We note the presence of five manual runs. Yet, the overall ranking of groups is not markedly different with the relaxing of the run constraints -we observe a correlation of ρ = 0.93 between the ranking of groups by best compulsory run and best run (among those groups that submitted a compulsory run).</p><p>Finally, Table <ref type="table" coords="2,378.01,559.77,4.48,7.77" target="#tab_2">4</ref> shows the best submitted compulsory run from each participating group, ranked by P@30 calculated using only highly relevant tweets. This ranking of groups is nearly the same as when all relevant tweets are counted (table 2, ρ = 0.95), although in some cases a group's best run was not the same under the two conditions. Only 33 topics have highly relevant tweets, and differences of less than 0.03 P@30 and 0.08 MAP are not meaningful according to the bootstrap discriminative power test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>As mentioned above, tweets were pooled from participating runs down to rank 30, following the rank field of the run. This was problematic for two reasons. The first reason is that this is itself an unusual pooling approach for TREC; runs are traditionally pooled by the document score (retrieval status value, also known as the 'sim' field) with ties in score broken by document identifier (in other words, randomly with respect to relevance). The runs were pooled by rank following the premise that because the task concerned real-time retrieval, the rank order in the runs was significant. In fact, this premise itself revealed the second problem, which is that the task was underspecified. Some participants computed scores to correspond to the order of tweet ids. Some participants adjusted ranks to boost documents, without changing scores. Some participants submitted "traditional" output with ranks coerced to increase with decreasing retrieval status values. In short, the task did not define the semantics of the run submission sufficiently for us to make comparisons between different systems. Comparisons of runs within a group are valid as long as the run ranking has the same meaning, but comparisons between systems in different groups are less valid without further investigation.</p><p>These issues could have two practical effects. One is that by pooling to a depth different than the depth semantics of the run, we may not have judged an equal number of tweets from each run. While pooling to equal depths is not necessary (see for example <ref type="bibr" coords="5,78.07,256.68,9.41,7.77" target="#b2">[2]</ref>), this is usually done in TREC in the spirit of fairness to all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,56.44,113.71,233.83,8.06"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Topic MB01 from the TREC 2011 Microblog track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,55.68,239.11,112.62"><head>Table 1 :</head><label>1</label><figDesc>Summary of results from the TREC 2011 Microblog track evaluation: per-topic best and medians for the 49 topics where all relevant and highly relevant tweets were considered relevant (denoted All Relevant), and the 33 topics where only highly relevant tweets were considered relevant (denoted Highly Relevant).</figDesc><table coords="2,347.36,55.68,178.01,39.55"><row><cell></cell><cell></cell><cell>Relevant</cell><cell cols="2">Highly Relevant</cell></row><row><cell></cell><cell>Best</cell><cell>Median</cell><cell>Best</cell><cell>Median</cell></row><row><cell>P@30</cell><cell cols="4">0.6116 0.2575 0.2646 0.0687</cell></row><row><cell>MAP</cell><cell cols="4">0.5127 0.1426 0.4740 0.1377</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,61.66,68.04,486.39,635.66"><head>Table 3 :</head><label>3</label><figDesc>Ranked runs, 1 per group; ranked by P@30 where tweets judged highly or minimally relevant are considered relevant.</figDesc><table coords="3,96.85,68.04,416.01,614.90"><row><cell>Group</cell><cell>Run</cell><cell>Auto.</cell><cell cols="2">Corpus Real-time Linked Ext. Res. P@30</cell><cell>MAP</cell></row><row><cell>isi</cell><cell>isiFDL</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4551 0.1892</cell></row><row><cell>FUB</cell><cell>DFReeKLIM30</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4401 0.2316</cell></row><row><cell>PRIS</cell><cell>PRISrun1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4388 0.3302</cell></row><row><cell>KobeU</cell><cell>ri</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4265 0.2203</cell></row><row><cell>CLARITY DCU</cell><cell>clarity1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4211 0.2109</cell></row><row><cell>FASILKOMUI</cell><cell>FASILKOM02</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4197 0.1904</cell></row><row><cell>waterloo</cell><cell>waterlooa3</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4095 0.2082</cell></row><row><cell>ICTIR</cell><cell>run2</cell><cell></cell><cell>HTML</cell><cell cols="2">0.4075 0.2953</cell></row><row><cell>Purdue IR</cell><cell>myrun2</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3993 0.1977</cell></row><row><cell>HIT LTRC</cell><cell>hitWIt</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3973 0.3157</cell></row><row><cell>wis tudelft</cell><cell>WISTUD</cell><cell cols="2">manual HTML</cell><cell cols="2">0.3946 0.2719</cell></row><row><cell>PKU ICST</cell><cell>PKUICST2</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3905 0.2196</cell></row><row><cell>CIIR</cell><cell>ciirRun2</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3646 0.2274</cell></row><row><cell>SEEM CUHK</cell><cell>WiseFifthRun</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3578 0.1687</cell></row><row><cell>NUSIS</cell><cell>relevanceRun</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3517 0.1862</cell></row><row><cell>syles</cell><cell>sylesNoRes</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3476 0.2114</cell></row><row><cell>KAUST</cell><cell>KAUSTRerank</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3456 0.1699</cell></row><row><cell>DUTIR</cell><cell>dutirMixFb</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3408 0.2902</cell></row><row><cell>IRSI</cell><cell>Google1GNO</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3401 0.2265</cell></row><row><cell>gslisUIUC</cell><cell>gut</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3218 0.1233</cell></row><row><cell>QCRI</cell><cell>QCRIwTagOrg</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3177 0.1230</cell></row><row><cell>RMIT</cell><cell>RMITMRR</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3163 0.2311</cell></row><row><cell>SienaCLTeam</cell><cell>SienaCL1B</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3082 0.1635</cell></row><row><cell>udel</cell><cell>udelIndri</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3082 0.1230</cell></row><row><cell>COMMIT</cell><cell>COMMITlinks</cell><cell></cell><cell>JSON</cell><cell cols="2">0.3027 0.1703</cell></row><row><cell>Udel Fang</cell><cell>UDMicroIDF</cell><cell></cell><cell>HTML</cell><cell cols="2">0.3027 0.1842</cell></row><row><cell>DLDE</cell><cell>omarRun</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2932 0.0874</cell></row><row><cell>UPorto</cell><cell>baseline2</cell><cell></cell><cell>JSON</cell><cell cols="2">0.2925 0.1239</cell></row><row><cell>UIowaS</cell><cell>UIowaS3</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2918 0.1403</cell></row><row><cell>UoW</cell><cell>PL2NoQeSd</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2823 0.1561</cell></row><row><cell>PolyU</cell><cell>LJQO5</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2639 0.1633</cell></row><row><cell>xmuPRC</cell><cell>RunPure</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2639 0.1145</cell></row><row><cell>IRIT SIG</cell><cell>iritfd1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2605 0.2115</cell></row><row><cell>kwcenter</cell><cell>2</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2578 0.1905</cell></row><row><cell>UniMelbLT</cell><cell>melblt</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2565 0.1409</cell></row><row><cell>UICIR</cell><cell>uicir1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2524 0.0916</cell></row><row><cell>yandex</cell><cell>ya4</cell><cell></cell><cell>other</cell><cell cols="2">0.2381 0.0822</cell></row><row><cell>L3S</cell><cell>qHtagBaseRun</cell><cell></cell><cell>HTML</cell><cell cols="2">0.2190 0.1154</cell></row><row><cell>QUT1</cell><cell>run3a</cell><cell></cell><cell>other</cell><cell cols="2">0.2034 0.0663</cell></row><row><cell>uogTr</cell><cell>uogTrUB2</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1939 0.1014</cell></row><row><cell>WeST</cell><cell>WESTfilext</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1776 0.1071</cell></row><row><cell>Vitalie Scurtu</cell><cell>scurtuRun1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1762 0.1453</cell></row><row><cell cols="2">NEMIS ISTI CNR runNeMISext</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1714 0.1186</cell></row><row><cell>FDUMED</cell><cell>FDUNLP</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1510 0.1411</cell></row><row><cell>Elly</cell><cell>Basic</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1463 0.0943</cell></row><row><cell>SIEL IIITH</cell><cell>sielrun4</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1265 0.0569</cell></row><row><cell>GUCAS</cell><cell>IDEAACTQE</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1190 0.1106</cell></row><row><cell>Morpheus</cell><cell>MorpheusRun1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.1150 0.0206</cell></row><row><cell>UGLA D</cell><cell>tfTP01</cell><cell></cell><cell>JSON</cell><cell cols="2">0.1007 0.1166</cell></row><row><cell>UCSC</cell><cell>run3</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0939 0.1416</cell></row><row><cell>monash</cell><cell>MONASH1NEW</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0823 0.1144</cell></row><row><cell>ikm101</cell><cell>ikmRun1</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0612 0.0433</cell></row><row><cell>ICTNET</cell><cell>ICTNET11MBR3</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0490 0.1000</cell></row><row><cell>TUD DMIR</cell><cell>EMAX</cell><cell></cell><cell>JSON</cell><cell cols="2">0.0435 0.0301</cell></row><row><cell>ULuga</cell><cell>baselineBM25</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0415 0.0292</cell></row><row><cell>KapeReunion</cell><cell>kapeRun</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0401 0.0553</cell></row><row><cell>utwente</cell><cell>UTWngFuture</cell><cell></cell><cell>other</cell><cell cols="2">0.0245 0.0246</cell></row><row><cell>uiuc</cell><cell>uiucsf</cell><cell></cell><cell>HTML</cell><cell cols="2">0.0075 0.0007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,316.81,627.64,239.11,39.44"><head>Table 4 :</head><label>4</label><figDesc>Automatic runs abiding by the real-time and external resources constraints, 1 per group; ranked by P@30, where only tweets judged to be highly relevant are considered relevant.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.30,702.51,234.62,7.77;1,316.81,711.47,140.21,7.77"><p>http://twittertools.cc/, which redirected at the time of writing to https://github.com/lintool/twitter-tools/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,58.28,693.54,234.62,7.77;2,53.80,702.51,239.11,7.77;2,53.80,711.47,22.93,7.77"><p>Usually in TREC, pools are sorted by document identifier. The goal in pool sorting is to sort without respect to run, score, or relevance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="5,62.77,304.00,230.14,7.77;5,53.80,314.46,239.11,7.77;5,53.80,324.92,239.11,7.77;5,53.80,335.38,239.11,7.77;5,53.80,345.84,239.11,7.77;5,53.80,356.30,239.11,7.77;5,53.80,366.77,217.91,7.77"><p>The Microblog track ran for the first time at TREC 2011, addressing a real-time adhoc search task. The creation of the corpus, which followed a novel methodology for TREC, has been a major success. With 59 groups participating, this is the largest TREC track/task ever in terms of participating groups. The evaluation results show that the real-time search task is far from being a solved problem. The Microblog track will run again in TREC 2012.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,321.30,55.51,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.23,71.07,194.15,7.77;5,336.23,81.38,171.10,7.93;5,336.23,91.84,204.96,7.72;5,336.23,102.46,129.50,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,497.01,71.07,33.37,7.77;5,336.23,81.54,74.08,7.77">Syntactic clustering of the Web</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,426.79,81.38,80.54,7.72;5,336.23,91.84,201.10,7.72">Proceedings of the 6th International World Wide Web Conference (WWW 1997)</title>
		<meeting>the 6th International World Wide Web Conference (WWW 1997)<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.23,113.91,203.11,7.77;5,336.23,124.22,210.86,7.93;5,336.23,134.68,193.92,7.72;5,336.23,145.14,189.49,7.72;5,336.23,155.76,101.36,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,508.68,113.91,30.66,7.77;5,336.23,124.37,128.65,7.77">Efficient construction of large test collections</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,480.26,124.22,66.83,7.72;5,336.23,134.68,193.92,7.72;5,336.23,145.14,185.63,7.72">Proceedings of the 21st Annual ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998)</title>
		<meeting>the 21st Annual ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.23,167.21,216.29,7.77;5,336.23,177.67,211.29,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,535.08,167.21,17.44,7.77;5,336.23,177.67,81.12,7.77">Blog track research at TREC</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,424.62,177.67,47.34,7.77">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="75" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.23,189.13,188.69,7.77;5,336.23,199.59,186.19,7.77;5,336.23,209.90,212.93,7.93;5,336.23,220.51,20.17,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,379.84,199.59,129.51,7.77">Overview of TREC-2006 Blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,336.23,209.90,96.18,7.72">Proceedings of TREC 2006</title>
		<meeting>TREC 2006<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,336.23,231.97,187.18,7.77;5,336.23,242.28,209.80,7.93;5,336.23,252.74,207.22,7.72;5,336.23,263.20,200.88,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,369.94,231.97,153.48,7.77;5,336.23,242.43,32.05,7.77">Evaluating evaluation metrics based on the bootstrap</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,383.31,242.28,162.73,7.72;5,336.23,252.74,207.22,7.72;5,336.23,263.20,76.56,7.72">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
