<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.72,72.36,374.28,16.84">Overview of the TREC-2014 Microblog Track</title>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.62,115.20,52.12,14.80"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.03,115.20,57.80,14.80"><forename type="first">Miles</forename><surname>Efron</surname></persName>
							<email>mefron@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.62,115.20,53.19,14.80"><forename type="first">Yulu</forename><surname>Wang</surname></persName>
							<email>ylwang@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.36,115.20,90.59,14.80"><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
							<email>gsherma2@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.72,72.36,374.28,16.84">Overview of the TREC-2014 Microblog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A0C3B4EF039FA281966F8681BBFB60A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This year represents the fourth iteration of the TREC Microblog track, which has been running since 2011. The track continued using the "evaluation as a service" model <ref type="bibr" coords="1,283.18,243.88,9.73,7.92" target="#b8">[8,</ref><ref type="bibr" coords="1,53.80,254.35,6.49,7.92">7]</ref>, in which participants had access to the document collection only through an API. In addition to the temporallyanchored ad hoc retrieval task, which has been running since the inception of the track, we introduced a new task called tweet timeline generation (TTG), where the goal is to produce concise "summaries" about a particular topic for human consumption.</p><p>Although this overview covers both tasks, more emphasis is placed on the tweet timeline generation task, which necessitated the development of a new evaluation methodology. We refer the reader to previous track overview papers <ref type="bibr" coords="1,268.79,358.95,9.73,7.92" target="#b8">[8,</ref><ref type="bibr" coords="1,281.12,358.95,11.78,7.92" target="#b12">12,</ref><ref type="bibr" coords="1,53.80,369.41,7.17,7.92" target="#b9">9]</ref> for details on the setup of the ad hoc task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASK DESCRIPTION</head><p>One assumption of the Cranfield Paradigm <ref type="bibr" coords="1,235.79,407.77,9.22,7.92" target="#b4">[4]</ref>, which provides the basis of most TREC evaluations, is that researchers can acquire the document collection under study. What if this were not possible? Tweets represent an example of such collections: Twitter's terms of service forbid redistribution of tweets, and thus it would not be permissible for an organization to host a collection of tweets for download.</p><p>The evaluation-as-a-service (EaaS) model was introduced in the TREC 2013 Microblog track as one solution to this challenge <ref type="bibr" coords="1,94.16,501.92,9.73,7.92" target="#b8">[8,</ref><ref type="bibr" coords="1,107.82,501.92,6.49,7.92">7]</ref>. Under this model, we (as the track organizers) gathered a collection of tweets centrally, but, instead of distributing the tweets themselves, we provided a service API (built on the open-source Lucene search engine) through which participants could access the tweets to complete the task.</p><p>The collection and service API used in this year's track was the same as last year's <ref type="bibr" coords="1,170.91,575.14,9.22,7.92" target="#b8">[8]</ref>. The collection, known as Tweets2013, consists of 243 million tweets crawled from the public Twitter sample stream between February 1 and March 31, 2013 (inclusive). This level of access is available to anyone with a Twitter account and does not require any special authorization. The API provides basic search access and returns tweet content as well as various metadata fields. More details about the API specification are provided in last year's track overview paper <ref type="bibr" coords="1,141.16,658.83,9.22,7.92" target="#b8">[8]</ref>.</p><p>Experience from last year's track suggested that the evaluation-as-a-service model provided a satisfactory solution to the collection redistribution issues. The level of participation (one of the most popular tracks at TREC) seems to indicate that the transition to API-based access was not overly burdensome, and that the API provided sufficient flexibility for participants to pursue their own research ideas <ref type="bibr" coords="1,522.97,220.47,13.52,7.92" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporally-Anchored Ad Hoc Retrieval</head><p>The putative user model for the ad hoc retrieval task is as follows: "At time T , give me the most relevant tweets about an information need expressed as query Q." Although the task definition has not substantively changed since the first Microblog track in TREC 2011, we have in recent years gained a more refined understanding of how the task is operationalized in TREC <ref type="bibr" coords="1,411.30,313.62,13.52,7.92" target="#b17">[17]</ref>.</p><p>The above user model can actually describe two slightly different scenarios: Consider a scenario where a journalist is investigating a sports scandal that has been brewing for the past several weeks. She just got news of a breaking development, and turns to searching tweets to find out more details: the scandal's major facts, reactions from fellow athletes, commentary from analysts, etc. Since this particular news story has been developing for several weeks, any keyword search involving the athlete's name would likely bring up results from many different points in time. In this case, the journalist specifically wants to see the most recent tweets about the event. For convenience, we refer to this as the "real-time search" scenario.</p><p>Consider another scenario where a journalist is searching an archive of tweets as part of writing a retrospective piece about the impact of social media on the Egyptian revolution. The topic is temporally-anchored in the sense that political events may not have played out fully, and the journalist is interested in perspectives at a particular point in time T . In this case, she might want to search for results that accurately reflect the volume of discourse on the topic (not necessarily biased toward more recent tweets prior to time T ). For convenience, we refer to this as "archive search".</p><p>Although the original conception of the task was closer to the real-time search scenario, NIST assessors indicate that topic development and relevance judgments followed a model closer to the archive search scenario. Given that the document collection was indeed retrospective, archive search represented a more natural operationalization of the ad hoc task.</p><p>From a technical perspective, these two alternative scenarios hold implications for the design of the search infrastructure. The search API is built on an index of the entire collection; the temporal constraint T is satisfied by retrieving from the entire collection, but then discarding tweets that occur after T . One concern with this approach is that the service is taking advantage of term statistics of tweets that occur "in the future" (if one wishes to simulate the real-time scenario). However, we have confirmed in a recent study that such inclusion of future term statistics does not substantively affect ranking results <ref type="bibr" coords="2,197.41,78.50,13.52,7.92" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tweet Timeline Generation</head><p>This year, we introduced a new task called tweet timeline generation (TTG), motivated by the observation that for many queries, search systems often return many tweets that are duplicates, near-duplicates, or contain the same (or highly-similar) information-a user is unlikely to want to see an enumeration of all these tweets. Instead, it would be desirable if the system produced a "summary" timeline about the topic. In the task definition this year, a summary is operationalized as a list of non-redundant, chronologically ordered tweets. Thus, the putative user model is as follows: "At time T , I have an information need expressed by query Q, and I would like a summary that captures relevant information." It is imagined that the user would consume the entire summary (unlike a ranked list, where the user might stop reading at any time).</p><p>The tweet timeline generation task supplements the standard challenges of ad hoc retrieval with issues from topic detection and tracking (TDT) and multi-document summarization. In our conception, systems need to address two additional challenges (beyond ad hoc retrieval):</p><p>• Detect (and eliminate) redundant tweets. This is equivalent to saying that systems must detect novelty.</p><p>• Determine how many results to return. Some topics have more relevant and non-redundant tweets than others and a system must be able to automatically infer this. Systems can make different precision/recall tradeoffs along these lines.</p><p>Redundancy is operationalized as follows: for every pair of tweets, if the chronologically later tweet contains substantive information that is not present in the earlier tweet, the later tweet is considered novel; otherwise the later tweet is redundant with respect to the earlier one. In our definition, redundancy and novelty are antonyms, and so we use them interchangeably, but in opposite contexts.</p><p>Note that because of the temporal constraint, redundancy is not symmetric. If tweet A precedes tweet B and tweet B contains substantively similar information found in tweet A, then B is redundant with respect to A, but not the other way around. Finally, we assume transitivity. Suppose A precedes B and B precedes C: if B is redundant with respect to A and C is redundant with respect to B, then by definition C is redundant with respect to A. In this task setup, redundancy boils down to the definition of the binary relation "contains substantively similar information". This is more precisely defined as part of our evaluation methodology, which is described in Section 3.2.</p><p>We imagined that participants would tackle the TTG task in a pipelined architecture that begins with ad hoc retrieval followed by summary generation. Therefore, participants in the TTG task were also required to participate in the ad hoc retrieval task (i.e., submit runs).</p><p>The tweet timeline generation task represents a natural extension of classic ad hoc retrieval and shares similarities with previous tasks such as aspect retrieval <ref type="bibr" coords="2,235.35,690.21,13.52,7.92" target="#b10">[10]</ref>, sub-topic retrieval <ref type="bibr" coords="2,91.16,700.67,13.52,7.92" target="#b18">[18]</ref>, and the notion of "information nuggets" in TREC question answering evaluations <ref type="bibr" coords="2,216.70,711.13,14.34,7.92" target="#b15">[15,</ref><ref type="bibr" coords="2,236.00,711.13,6.49,7.92" target="#b3">3]</ref>. From the perspective of search result diversification, Tao et al. <ref type="bibr" coords="2,541.58,57.58,14.34,7.92" target="#b13">[13]</ref> recently explored multi-aspect retrieval in the tweet context. Previous work along these lines suggest that this task is not so difficult as to preclude meaningful progress toward its solution, which is an important consideration in developing TREC tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporally-Anchored Ad Hoc Retrieval</head><p>For TREC 2014, NIST assessors developed a total of 55 new topics. The ad hoc retrieval task was evaluated using a standard pooling methodology by NIST assessors. Judgment pools were created using depth 100 across all submitted ad hoc runs, plus a random selection of 100 tweets per topic from each TTG run. Although we envisioned a system architecture consisting of ad hoc retrieval followed by summary generation, the inclusion of TTG results in the pool was explicitly designed to reduce missing judgments for TTG runs in cases where participants' systems did not include an explicit ad hoc retrieval stage.</p><p>These judgment pools were further reduced by removing retweets (declared not relevant by track fiat) and then clustered so that textually similar tweets are presented to assessors in close proximity (to enhance judgment consistency). Tweets were judged on a three-way scale of "not relevant", "relevant", and "highly relevant".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tweet Timeline Generation</head><p>The TTG definition of redundancy and the assumption of transitivity means that the task can be viewed as semantic clustering-that is, we wish to group relevant tweets into clusters in which all tweets share substantively similar information. Within each cluster, the earliest tweet is novel; all other tweets in the cluster are redundant with respect to all earlier tweets.</p><p>Our annotation methodology to generate judgments for evaluation builds on exactly this idea. For a topic, we begin with the list of relevant tweets, ordered chronologically, from earliest to latest. These tweets are presented, one at a time, to a human assessor. For each tweet, the assessor can add it to an existing cluster if she thinks the tweet contains substantively similar information with respect to tweets in the existing cluster, or she can create a new cluster for the tweet. We have developed a JavaScript-based annotation interface to help assessors accomplish this task-a screenshot is shown in Figure <ref type="figure" coords="2,393.86,543.76,3.58,7.92" target="#fig_0">1</ref>.</p><p>In the interface, the next tweet to be processed is shown at the bottom of the screen. The assessor can either add the tweet to an existing cluster by clicking the "Add" button next to the cluster or create a new cluster by hitting the space bar. At any time, the assessor can expand a cluster to show all tweets contained in it, or collapse the cluster, in which only the first tweet is shown. The interface also implements an undo feature that allows the assessor to reverse the action taken and go back to the previous tweet.</p><p>The TTG evaluation methodology boils down to this central question: what exactly does "substantively similar information" mean? Like document relevance in ad hoc retrieval, assessors make the final determination and we expect natural variations among humans. However, pilot studies helped us devise a set of guidelines, which were provided as instructions to the assessors. We told them: A good rule of thumb is that if two tweets "say the same thing", then they're substantively similar. To speed up the clustering process, the annotators were asked not to consider external content (e.g., follow links in the tweets).</p><p>To provide further guidance, we devised a number of questions that the assessor might consider in determining whether two tweets should be in the same cluster:</p><p>• If I had already seen the first tweet, would I have missed out on some information if I didn't see the second tweet?</p><p>• If two tweets are similar but the second contains an addition to or endorsement of the first, is the addition or endorsement important enough that I would be interested in seeing both tweets?</p><p>• Sometimes two tweets look similar but actually narrate the development of an event. Are the tweets different enough from each other such that I would want to see both tweets to understand how an event develops or unfolds?</p><p>The annotation process proceeded concurrently at the University of Maryland and the University of Illinois, using relevant documents from the NIST judgment pools as the starting point. Due to resource constraints, NIST assessors were not able to perform the clustering, and thus a weakness of our setup is that the individual with the information need was not the one who created the clusters. The two assessors at the University of Maryland were graduate students in computer science (both male). The two assessors at the University of Illinois were graduate students in library and information science (one male, one female).</p><p>Assessors were first trained in the laboratory: the session included an introduction to the task and an overview of the annotation interface. After that, assessors were free to perform annotations at their own pace on their laptops, at locations of their choosing (this was possible because the annotation interface was implemented in JavaScript and hence accessible over the web). All assessors began with a throwaway "practice topic" (although they were not aware of the throwaway nature) and then proceeded to annotate topics in batches (roughly ten topics per batch). Topics were grouped into batches of roughly equal size (in terms of the number of relevant documents). When an assessor completed a batch, he or she could request another batch to work on. In order to preserve consistency across topics, we opted to have fewer annotators each working on more topics, as opposed to many annotators each processing only a single batch.</p><p>Each site annotated the topic batches in the opposite order, and when a covering set for all topics had been obtained, we designated those clusters to be the "official" judgments. However, the annotation process continued until both sites had processed all topics, giving us alternate judgments. Thus, both the official and alternate clusters contained annotations generated from both sites.</p><p>The output of the human annotation process is an ordered list of tweet clusters. Within each cluster, the tweets are sorted by temporal order (earliest to latest). The clusters themselves are sorted by the temporal order of their earliest tweet. Following the heuristic of using the most straightforward metric when defining a new task <ref type="bibr" coords="3,516.38,704.40,4.61,7.92">(</ref> subsequently refining the metric as needed), we decided to measure cluster-based precision and recall. The measure is cluster-based in the sense that systems only receive credit for returning one tweet from each cluster-that is, once a tweet is retrieved, all other tweets in the cluster are automatically considered not relevant. From this, we can compute precision, recall, and F-score in the usual way (lacking any basis for setting the β parameter, we simply computed F1). Since the user model assumes that a searcher will consume the entire summary, set-based metrics seemed appropriate and straightforward.</p><p>The only additional refinement is that we computed both weighted and unweighted variants of recall. In weighted recall, each cluster is assigned a weight proportional to the sum of the relevance grades from every tweet in the cluster (relevant tweets receive a weight of one and highly-relevant tweets receive a weight of two). This weighting scheme implements the heuristic that larger clusters and those containing more highly-relevant tweets are more important, and the denominator in the weighted recall computation is the sum of clusters' weights. In unweighted recall, all clusters are considered equally important, and the denominator is simply the total number of clusters.</p><p>Note that this setup gives equal credit to retrieving any tweet from a cluster. Intuitively, however, this seems overly simplistic-users would certainly prefer seeing certain tweets over others, even if they contain substantively similar information. For example, users might prefer to see the earliest tweet, a tweet from the most "authoritative" user (e.g., a verified news account), or a tweet from someone close by in their network (e.g., a tweet from someone they follow). We currently do not have sufficient understanding to accu-rately model such preferences, and thus explicitly made the decision not to tackle this challenge.</p><p>The evaluation metrics for TTG were derived from previous work referenced in Section 2.2: aspect recall <ref type="bibr" coords="4,518.20,410.80,13.52,7.92" target="#b10">[10]</ref>, subtopic recall <ref type="bibr" coords="4,365.48,421.27,13.52,7.92" target="#b18">[18]</ref>, and the "nugget pyramid" approach from the TREC question answering evaluations <ref type="bibr" coords="4,492.45,431.73,9.22,7.92" target="#b6">[6]</ref>. Alternative metrics we had considered include those based on gain <ref type="bibr" coords="4,546.18,442.19,9.73,7.92" target="#b2">[2]</ref> and the extension of mean average precision to graded relevance judgments <ref type="bibr" coords="4,391.30,463.11,13.52,7.92" target="#b11">[11]</ref>. After careful consideration, for this initial evaluation we decided to stick with the simpler setbased metrics, but we will consider different metrics in the future based on lessons learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Temporally-Anchored Ad Hoc Retrieval</head><p>For the temporally-anchored ad hoc retrieval task, NIST received a total of 75 runs from 21 groups. Table <ref type="table" coords="4,510.56,567.06,4.61,7.92" target="#tab_0">1</ref> shows the run with the highest mean average precision (MAP) from each group. Precision at rank 30 (P30) and R-precision are also shown. Rows in the table are sorted by MAP. In computing these metrics, both the "relevant" and "highly relevant" grades are considered relevant.</p><p>For reference, we provided two baselines, also shown in Table <ref type="table" coords="4,342.28,640.28,3.58,7.92" target="#tab_0">1</ref>: The "baseline" condition is simply the raw output of the API with queries as bags of words. The "RM3" condition is our reference implementation of the RM3 variant of relevance models <ref type="bibr" coords="4,398.43,671.67,9.73,7.92" target="#b5">[5,</ref><ref type="bibr" coords="4,411.39,671.67,6.49,7.92" target="#b1">1]</ref>. <ref type="foot" coords="4,421.12,669.92,3.65,5.28" target="#foot_0">1</ref> For RM3, we extracted the top 20 feedback terms from the top 50 tweets, which is interpolated with the original query model with a weight of 0.5.  Both baselines were submitted as normal runs and hence were part of the judgment pool. These results show that participants are submitting effective runs overall. Using the baselines to calibrate, there are many more teams beating the baselines than in previous years. This suggests that the ad hoc retrieval task is perhaps becoming "too easy".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tweet Timeline Generation</head><p>In total, 13 groups submitted 50 runs to the tweet timeline generation task. Each topic averaged 194 relevant documents. In the official clusters, assessors formed an average of 89 clusters per topic (and each cluster averaged 2.2 tweets). Results based on these annotations are shown in Table <ref type="table" coords="5,78.22,648.37,3.58,7.92" target="#tab_1">2</ref>, which contains the run with the highest weighted F1 score from each group. The columns show unweighted recall, weighted recall (indicated by the w superscript), precision, F1 with unweighted recall, and F1 with weighted recall (indicated by the w superscript). Rows are sorted by weighted F1 score. In computing these metrics, both the "relevant" and "highly relevant" grades are considered relevant.</p><p>Recognizing that systems make different choices with respect to balancing precision and recall, it is illustrative to visualize the tradeoffs in a scatter plot. Figure <ref type="figure" coords="5,506.30,517.16,4.61,7.92" target="#fig_1">2</ref> shows precision vs. unweighted recall (left) and precision vs. weighted recall (right) for all runs. Iso-F1 contours are plotted in blue; points on the same contour line have the same F1 score, but with different precision/recall tradeoffs.</p><p>What is the effect of adding the cluster weights? Figure <ref type="figure" coords="5,551.30,569.46,4.61,7.92" target="#fig_2">3</ref> shows a scatter plot of the weighted F1 score vs. the unweighted F1 score for all submitted runs (where each point represents a run). We see that for most runs, the two scores are highly correlated, but there are a number of runs where the weighted F1 score is quite a bit higher than the unweighted F1 score.</p><p>To verify the stability and reliability of the evaluation, we performed two sets of analyses: the first concerns the number of missing judgments in evaluating the TTG runs. Recall that our cluster annotation workflow starts with relevant tweets from the NIST judgment pools, created via the process described in Section 3.1.</p><p>In total, the TTG runs returned a combined 81,726 unique   tweets. Of these, 37,449 (45.8%) were absent from the judgment pools and therefore lacked explicit judgments. However, these missing tweets were concentrated in a relatively small number of runs: this is shown in Figure <ref type="figure" coords="6,237.34,554.22,3.58,7.92" target="#fig_3">4</ref>, which plots the fraction of tweets that are missing judgments (averaged across topics). For 18 runs (out of 50), we did not observe any missing judgments. Of the runs with missing judgments, 19 were missing less than 5%. Note that these bars indicate the fraction of missing judgments, which obscures the absolute number since some systems return answers that are on average longer. For example, the run that was most severely impacted had an average of 412 missing judgments per topic (out of an average of 1000 tweets returned).</p><p>There are two possible interpretations of these results: the first is that systems are building summaries without an explicit ad hoc retrieval stage (whose outputs can contribute to the ad hoc judgment pools); the second is that the systems are generating summaries that are so long that missing judgments are the result of a shallow pool depth. <ref type="bibr" coords="6,233.96,711.13,15.88,7.92">The</ref>  with the largest numbers of unjudged results appear to have been generated without corresponding ad hoc runs. These runs are also very verbose for the most part. On the whole, it is the case that the number of missing judgments is positively correlated with run length, but the fraction of results that are unjudged does not show this relationship. From this, there appears to be evidence in favor of the hypothesis that the shallow pool depth contributed to missing judgments. However, without explicit knowledge of each team's operational relationship between its ad hoc and TTG runs, it is hard to draw firmer conclusions.</p><p>Our second analysis focuses on the stability of the evaluation with respect to assessor differences. The judgment of whether two tweets contain "substantively similar information" is likely to have high variability across assessors, which would yield substantially different clusters. We would like to determine to what extent this impacts our ability to make system comparisons, i.e., that system X is more effective than system Y <ref type="bibr" coords="6,381.04,386.85,13.52,7.92" target="#b14">[14]</ref>. Note that as is standard in IR metaevaluation, differences in absolute scores are not worrisome, so long as system comparisons are stable.</p><p>As part of our annotation process, we have obtained two independent sets of cluster annotations for all topics. Figure <ref type="figure" coords="6,333.05,439.15,4.61,7.92">5</ref> shows scores based on the official judgments and the alternate judgments for each of the five metrics in Table <ref type="table" coords="6,548.75,449.61,3.58,7.92" target="#tab_1">2</ref>. Results are sorted by scores based on the official judgments. The official set averaged 89 clusters per topic, while the alternate judgments averaged 73 clusters per topic, which indicates that humans perform the clustering task at different levels of granularity. We see that the rankings produced by both sets of judgments are highly correlated, with the exception of weighted F1, which shows a number of runs that yield different comparisons with respect to the two sets of judgments. Furthermore, except for the weighted variants, the absolute values of the metrics are also quite similar.</p><p>In Table <ref type="table" coords="6,364.21,564.68,3.58,7.92" target="#tab_3">3</ref>, we tally the number of rank swaps for each metric. A rank swap is a pairwise comparison where according to one set of judgments, run A scores higher than run B, but according to another set of judgments, run B scores higher than run A. There are a total of (50 × 49)/2 = 1225 pairwise comparisons. Table <ref type="table" coords="6,437.92,616.99,4.61,7.92" target="#tab_3">3</ref> also shows the Kendall's τ correlation between rankings induced by the two different sets of judgments. With the exception of weighted F1, we do not observe many rank swaps, as confirmed by the high rank correlations.</p><p>Finally, we show histograms of the rank swaps for unweighted and weighted F1 in Figure <ref type="figure" coords="6,469.47,679.75,4.61,7.92">6</ref> binned by absolute score differences. Such an analysis is informative because we are less concerned with rank swaps in which the absolute score differences between the two conditions are small. For  space considerations, we only show these histograms for the two metrics with the Kendall's τ correlations. We do see a number of rank swaps with large score differences, which correspond to the jagged portion of the blue line in the weighted F1 plot in Figure <ref type="figure" coords="7,179.36,585.96,3.58,7.92">5</ref>. For the other histograms, most of the rank swaps occur with small absolute score differences (which are not of major concern).</p><p>Based on these analyses, we can conclude that the evaluation methodology for TTG is stable with respect to assessor differences. Evaluation using independent cluster annotations give rise to system comparisons that are consistent, which strengthens our confidence in the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>The tweet timeline generation task this year represents a serious attempt in the TREC Microblog track to move be-yond ad hoc retrieval. In the design of the track, we have explicitly taken a conservative approach in changing only one aspect of the evaluation at a time (last year, it was the introduction of the evaluation-as-a-service model; this year, the introduction of TTG). We believe that this approach provides continuity and allows participants to build on lessons learned in previous years in an incremental fashion. The track will continue in TREC 2015, where we look forward to continue pushing the state of the art in information retrieval techniques applied to microblogs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,53.80,319.00,502.12,8.02;3,53.80,329.46,502.12,8.02;3,53.80,339.92,502.12,8.02;3,53.80,350.38,502.12,8.02;3,53.80,360.84,308.69,8.02;3,104.01,53.80,401.69,251.06"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Screenshot of the clustering interface for assessors. Tweets are presented one at a time in chronological order. The next tweet to be processed is shown at the bottom of the screen: the assessor can either add the tweet to an existing cluster (by clicking the "Add" button next to the cluster) or create a new cluster (by hitting the space bar). At any time, the assessor can expand a cluster to show all tweets contained in it, or collapse the cluster, in which case only the first tweet is shown.</figDesc><graphic coords="3,104.01,53.80,401.69,251.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,53.80,456.04,502.11,8.02;5,53.80,466.50,194.10,8.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scatter plots showing precision vs. unweighted recall (left) and precision vs. weighted recall (right) for all runs, overlaid with iso-F1 contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,53.80,247.27,239.10,8.02;6,53.80,257.73,150.91,8.02"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter plot showing weighted F1 score vs. unweighted F1 score for all runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,53.80,470.93,239.10,8.02;6,53.80,481.39,239.10,8.02;6,53.80,491.85,239.11,8.02"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of missing judgments. Bars show, for each run, the fraction of returned tweets missing explicit judgments (averaged across topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,53.80,309.96,502.11,8.02;7,53.80,320.42,502.11,8.02;7,53.80,330.88,197.52,8.02"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Comparison between scores based on the official judgments and the alternate judgments for precision, unweighted recall, weighted recall, unweighted F1, and weighted F1. Runs are sorted by score based on the official judgments in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,520.99,704.40,34.92,7.92"><head>Table 1 : Results of the temporally-anchored ad hoc retrieval task, showing the run with the highest mean average</head><label>1</label><figDesc></figDesc><table coords="3,520.99,704.40,34.92,7.92"><row><cell>and then</cell></row></table><note coords="4,91.70,339.22,464.22,8.02;4,53.80,349.68,92.34,8.02"><p>precision (MAP) from each group. Precision at rank 30 (P30) and R-precision are also shown. Rows are sorted by MAP.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.80,53.63,502.12,383.86"><head>Table 2 : Results of the tweet timeline generation (TTG) task, showing the run with the highest weighted F1 score from each group. Columns show recall (unweighted and weighted), precision, and F1 (unweighted and weighted); the w superscript indicates the weighted variant of the metric. Rows are sorted by weighted F1 score.</head><label>2</label><figDesc></figDesc><table coords="5,105.96,53.63,397.81,147.81"><row><cell></cell><cell>Group</cell><cell cols="2">recall recall w precision</cell><cell>F1</cell><cell>F w 1</cell><cell>Type</cell></row><row><cell>TTGPKUICST2</cell><cell>PKUICST</cell><cell>0.3698 0.5840</cell><cell cols="3">0.4571 0.3540 0.4575 automatic</cell></row><row><cell>EM50</cell><cell>QCRI</cell><cell>0.2867 0.4779</cell><cell cols="3">0.4150 0.2546 0.3815 automatic</cell></row><row><cell>hltcoeTTG1</cell><cell>hltcoe</cell><cell>0.4029 0.5915</cell><cell cols="3">0.3407 0.2760 0.3702 automatic</cell></row><row><cell cols="2">QUTmpDecayTTgCL QU</cell><cell>0.3277 0.5167</cell><cell cols="3">0.3236 0.2440 0.3300 automatic</cell></row><row><cell>PrisTTG2014b</cell><cell cols="2">BUPT_PRIS 0.4231 0.6137</cell><cell cols="3">0.2730 0.2461 0.3093 automatic</cell></row><row><cell>SRTD</cell><cell>HU_DB</cell><cell>0.0868 0.2764</cell><cell cols="3">0.5798 0.1324 0.2907 automatic</cell></row><row><cell>3unique0</cell><cell>uog_twteam</cell><cell>0.2522 0.4374</cell><cell cols="3">0.2558 0.1957 0.2744 automatic</cell></row><row><cell>udelRunTTG1</cell><cell>udel</cell><cell>0.1873 0.3645</cell><cell cols="3">0.2793 0.1774 0.2669 automatic</cell></row><row><cell>UDInfoMMRWC5</cell><cell>udel_fang</cell><cell>0.0900 0.2191</cell><cell cols="3">0.5709 0.1338 0.2577 automatic</cell></row><row><cell>wistudt2bd</cell><cell>wistud</cell><cell>0.1111 0.3075</cell><cell cols="3">0.2827 0.1251 0.2305 automatic</cell></row><row><cell>SCIAI3cm4aTTG</cell><cell>SCIAITeam</cell><cell>0.0655 0.1941</cell><cell cols="3">0.4992 0.0921 0.2171 manual</cell></row><row><cell>ICTNETAP4</cell><cell>ICTNET</cell><cell>0.2528 0.4836</cell><cell cols="3">0.1702 0.1559 0.2072 automatic</cell></row><row><cell>JufeLdkeSum2</cell><cell>LDKE</cell><cell>0.4294 0.6156</cell><cell cols="3">0.0861 0.1180 0.1307 automatic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,252.36,55.34,303.56,663.71"><head>Table 3 : Count of rank swaps and Kendall's τ corre- lation based on the official and alternate judgments for each metric.</head><label>3</label><figDesc></figDesc><table coords="6,252.36,711.13,40.54,7.92"><row><cell>TTG runs</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,321.42,711.93,108.27,7.37"><p>http://twittertools.cc/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">National Science Foundation</rs> under IIS-1217279 and IIS-1218043. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsor. We are grateful to <rs type="person">Ellen Voorhees</rs> and the assessors at <rs type="institution">NIST</rs> for making TREC possible.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,321.30,568.40,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,584.61,193.41,7.92;7,335.63,595.07,182.20,7.92;7,335.63,605.53,197.25,7.92;7,335.63,615.99,207.73,7.92;7,335.63,626.45,200.35,7.92;7,335.63,636.91,152.29,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,494.39,605.53,38.49,7.92;7,335.63,615.99,130.40,7.92">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,486.66,615.99,56.70,7.92;7,335.63,626.45,167.17,7.92">Proceedings of the Thirteenth Text REtrieval Conference</title>
		<meeting>the Thirteenth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,335.63,648.37,172.76,7.92;7,335.63,658.83,181.69,7.92;7,335.63,669.29,208.21,7.92;7,335.63,679.75,218.03,7.92;7,335.63,690.21,205.65,7.92;7,335.63,700.67,204.22,7.92;7,335.63,711.13,157.57,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,397.06,669.29,146.78,7.92;7,335.63,679.75,75.94,7.92">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,430.14,679.75,123.52,7.92;7,335.63,690.21,205.65,7.92;7,335.63,700.67,204.22,7.92;7,335.63,711.13,20.95,7.92">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,57.58,202.00,7.92;8,72.62,68.04,213.96,7.92;8,72.62,78.50,204.83,7.92;8,72.62,88.97,210.71,7.92" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,210.59,57.58,64.03,7.92;8,72.62,68.04,149.64,7.92">Overview of the TREC 2006 question answering track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,240.69,68.04,45.88,7.92;8,72.62,78.50,204.83,7.92;8,72.62,88.97,20.95,7.92">Proceedings of the Fifteenth Text REtrieval Conference (TREC 2006)</title>
		<meeting>the Fifteenth Text REtrieval Conference (TREC 2006)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="99" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,100.42,220.28,7.92;8,72.62,110.88,117.49,7.92" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,123.21,100.42,131.62,7.92">Information Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,122.34,190.17,7.92;8,72.62,132.80,207.97,7.92;8,72.62,143.26,205.65,7.92;8,72.62,153.72,204.22,7.92;8,72.62,164.18,213.60,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,197.32,122.34,65.47,7.92;8,72.62,132.80,64.56,7.92">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,156.13,132.80,124.46,7.92;8,72.62,143.26,205.65,7.92;8,72.62,153.72,204.22,7.92;8,72.62,164.18,20.95,7.92">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2001)</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2001)<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,175.64,213.51,7.92;8,72.62,186.10,202.98,7.92;8,72.62,196.56,217.29,7.92;8,72.62,207.02,163.40,7.92;8,72.62,217.48,196.85,7.92;8,72.62,227.94,175.19,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,206.95,175.64,79.17,7.92;8,72.62,186.10,93.26,7.92">Will pyramids built of nuggets topple over?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,181.49,186.10,94.11,7.92;8,72.62,196.56,217.29,7.92;8,72.62,207.02,73.64,7.92">Proceedings of the 2006 Human Language Technology Conference of the North American Chapter</title>
		<meeting>the 2006 Human Language Technology Conference of the North American Chapter<address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,239.40,193.44,7.92;8,72.62,249.86,218.63,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,159.54,239.40,106.51,7.92;8,72.62,249.86,82.00,7.92">Evaluation as a service for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,161.26,249.86,53.49,7.92">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,261.32,201.89,7.92;8,72.62,271.78,218.74,7.92;8,72.62,282.24,168.22,7.92;8,72.62,292.70,124.08,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,159.54,261.32,114.97,7.92;8,72.62,271.78,64.41,7.92">Overview of the TREC-2013 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,155.85,271.78,135.51,7.92;8,72.62,282.24,164.02,7.92">Proceedings of the Twenty-Second Text REtrieval Conference (TREC 2013)</title>
		<meeting>the Twenty-Second Text REtrieval Conference (TREC 2013)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,304.16,192.57,7.92;8,72.62,314.62,198.20,7.92;8,72.62,325.08,177.52,7.92;8,72.62,335.54,209.05,7.92;8,72.62,346.00,20.99,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,72.62,314.62,182.45,7.92">Overview of the TREC-2011 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,72.62,325.08,177.52,7.92;8,72.62,335.54,101.77,7.92">Proceedings of the Twentieth Text REtrieval Conference (TREC 2011)</title>
		<meeting>the Twentieth Text REtrieval Conference (TREC 2011)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,357.46,217.42,7.92;8,72.62,367.92,206.81,7.92;8,72.62,378.38,124.08,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,108.86,357.46,105.97,7.92">TREC-6 interactive report</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,233.34,357.46,56.70,7.92;8,72.62,367.92,202.62,7.92">Proceedings of the Sixth Text REtrieval Conference (TREC 1997)</title>
		<meeting>the Sixth Text REtrieval Conference (TREC 1997)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.62,389.84,185.46,7.92;8,72.62,400.30,194.07,7.92;8,72.62,410.76,183.49,7.92;8,72.62,421.22,205.65,7.92;8,72.62,431.68,204.22,7.92;8,72.62,442.14,200.13,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,72.62,400.30,194.07,7.92;8,72.62,410.76,39.67,7.92">Extending average precision to graded relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,131.29,410.76,124.82,7.92;8,72.62,421.22,205.65,7.92;8,72.62,431.68,204.22,7.92;8,72.62,442.14,20.95,7.92">Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2010)</title>
		<meeting>the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2010)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="603" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,57.58,192.57,7.92;8,335.63,68.04,198.20,7.92;8,335.63,78.51,190.00,7.92;8,335.63,88.97,209.05,7.92;8,335.63,99.43,20.99,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,335.63,68.04,182.45,7.92">Overview of the TREC-2012 Microblog Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.63,78.51,190.00,7.92;8,335.63,88.97,44.34,7.92">Proceedings of the Twenty-First Text REtrieval Conference</title>
		<meeting>the Twenty-First Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,110.88,193.33,7.92;8,335.63,121.34,209.48,7.92;8,335.63,131.81,199.18,7.92;8,335.63,142.27,203.01,7.92;8,335.63,152.73,66.32,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,487.11,110.88,41.85,7.92;8,335.63,121.34,194.31,7.92">Building a microblog corpus for search result diversification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G.-J</forename><surname>Houben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.63,131.81,199.18,7.92;8,335.63,142.27,135.79,7.92">Proceedings of the 9th Asia Information Retrieval Societies Conference (AIRS 2013)</title>
		<meeting>the 9th Asia Information Retrieval Societies Conference (AIRS 2013)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,164.18,203.87,7.92;8,335.63,174.64,199.66,7.92;8,335.63,185.11,203.91,7.92;8,335.63,195.57,209.13,7.92;8,335.63,206.03,211.80,7.92;8,335.63,216.49,112.22,7.92" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,403.42,164.18,136.08,7.92;8,335.63,174.64,184.45,7.92">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.63,185.11,203.91,7.92;8,335.63,195.57,209.13,7.92;8,335.63,206.03,144.59,7.92">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998)</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,227.94,199.00,7.92;8,335.63,238.41,213.96,7.92;8,335.63,248.87,204.83,7.92;8,335.63,259.33,152.29,7.92" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,470.61,227.94,64.02,7.92;8,335.63,238.41,149.64,7.92">Overview of the TREC 2005 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,503.70,238.41,45.88,7.92;8,335.63,248.87,204.83,7.92;8,335.63,259.33,20.95,7.92">Proceedings of the Fifteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fifteenth Text REtrieval Conference (TREC 2005)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,270.78,186.47,7.92;8,335.63,281.24,219.72,7.92;8,335.63,291.71,174.26,7.92;8,335.63,302.17,179.74,7.92;8,335.63,312.63,211.80,7.92;8,335.63,320.85,114.87,7.92" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,492.90,270.78,29.21,7.92;8,335.63,281.24,141.19,7.92">On run diversity in &quot;evaluation as a service</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,498.65,281.24,56.70,7.92;8,335.63,291.71,174.26,7.92;8,335.63,302.17,179.74,7.92;8,335.63,312.63,144.59,7.92">Proceedings of the 37th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2014)</title>
		<meeting>the 37th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2014)<address><addrLine>Gold Coast, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="959" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,332.30,192.54,7.92;8,335.63,342.76,211.58,7.92;8,335.63,353.23,211.26,7.92;8,335.63,363.69,209.93,7.92;8,335.63,374.15,94.03,7.92" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,421.64,332.30,106.54,7.92;8,335.63,342.76,136.36,7.92">The impact of future term statistics in real-time tweet search</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,490.51,342.76,56.70,7.92;8,335.63,353.23,211.26,7.92;8,335.63,363.69,90.16,7.92">Proceedings of the of the 36th European Conference on Information Retrieval (ECIR 2014)</title>
		<meeting>the of the 36th European Conference on Information Retrieval (ECIR 2014)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,385.60,192.14,7.92;8,335.63,396.06,192.59,7.92;8,335.63,406.53,205.55,7.92;8,335.63,416.99,219.77,7.92;8,335.63,427.45,209.40,7.92;8,335.63,437.91,209.92,7.92" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,497.69,385.60,30.09,7.92;8,335.63,396.06,192.59,7.92;8,335.63,406.53,115.40,7.92">Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,469.19,406.53,71.99,7.92;8,335.63,416.99,219.77,7.92;8,335.63,427.45,209.40,7.92;8,335.63,437.91,54.11,7.92">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
