<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.98,172.72,232.30,15.12">TREC 2014 Web Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-02-18">February 18, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,168.85,206.62,130.19,10.48"><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
						</author>
						<author>
							<persName coords="1,341.15,206.62,88.11,10.48"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
						</author>
						<author>
							<persName coords="1,151.04,240.37,67.95,10.48"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
						</author>
						<author>
							<persName coords="1,269.56,240.37,74.79,10.48"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
						</author>
						<author>
							<persName coords="1,380.13,240.37,93.86,10.48"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.98,172.72,232.30,15.12">TREC 2014 Web Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-02-18">February 18, 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">AB6BE155E02052D95CDC7C14D56410A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the TREC Web track over the past few years has been to explore and evaluate innovative retrieval approaches over large-scale subsets of the Web -currently using ClueWeb12, on the order of one billion pages. For TREC 2014, the sixth year of the Web track, we implemented the following significant updates compared to 2013. First, the risk-sensitive retrieval task was modified to assess the ability of systems to adaptively perform risk-sensitive retrieval against multiple baselines, including an optional selfprovided baseline. In general, the risk-sensitive task explores the tradeoffs that systems can achieve between effectiveness (overall gains across queries) and robustness (minimizing the probability of significant failure, relative to a particular provided baseline). Second, we added query performance prediction as an optional aspect of the risk-sensitive task. The Adhoc task continued as for TREC 2013, evaluated using both adhoc and diversity relevance criteria.</p><p>This year, experiments by participating groups again used the ClueWeb12 Web collection, a successor to the ClueWeb09 dataset that comprises about one billion Web pages crawled between Feb-May 2012. 1 The crawling and collection process for ClueWeb12 included a rich set of seed URLs based on commercial search traffic, Twitter and other sources, and multiple measures for flagging undesirable content such as spam, pornography, and malware.</p><p>For consistency with last year's Web track, topic development was done using a very similar process to the one used in 2013. A common topic set of 50 additional new topics was developed and used for both the Adhoc and Risk-sensitive tasks. In keeping with the goal of reflecting authentic Web retrieval problems, the Web track topics were again developed from a pool of candidate topics based on the logs and data resources of commercial search engines. The initial set of candidates developed for the 2013 track was large enough that candidate topics not used in 2013 were used as the pool for the 2014 track. We kept the distinction between faceted topics, and unfaceted (single-facet) topics. Faceted topics were more like "head" queries, and structured as having a representative set of subtopics, with each subtopic corresponding to a popular subintent of the main topic. The faceted topic queries had subintents that were likely to be most relevant to users. Unfaceted (single-facet) topics were intended to be more like "tail" queries with a clear question or intent. For faceted topics, query clusters were developed and used by NIST for topic development. Only the base query was released to participants initially: the topic structures containing subtopics and single-vs multi-faceted vs. topic type were only released after runs were submitted. This was done to avoid biases that might be caused by revealing extra information about the information need that may not be available to Web search systems as part of the actual retrieval process.</p><p>The Adhoc task judged documents with respect to the topic as a whole. Relevance levels are similar to the levels used in commercial Web search, including a spam/junk level. The top two levels of the assessment structure are related to the older Web track tasks of homepage finding and topic distillation. Subtopic assessment was also performed for the faceted topics, as described further in Section 3.</p><p>Table <ref type="table" coords="2,171.62,472.02,5.45,9.57" target="#tab_0">1</ref> summarizes participation in the TREC 2014 Web Track. Overall, we received 42 runs from 9 groups: 30 ad hoc runs and 12 risk-sensitive runs. The number of participants in the Web track decreased over 2013 (when 15 groups participated, submitting 61 runs). Seven runs were categorized as manual runs (4 adhoc, 3 risk), submitted across 2 groups: all other runs were automatic with no human intervention. All submitted runs used the main Category A corpus: none used the Category B subset of ClueWeb12.</p><p>The submitting groups were: Three teams submitted at least one run with an associated Query Performance Prediction file.</p><p>In the following, we recap on the corpus (Section 2), and topics (Section 3) used for TREC 2014. Section 4 details the pooling and evaluation methodologies applied for Adhoc and Risk-Sensitive tasks, as well as the results of the participating groups. Section 5 examines sources of variation across submitted runs using Principal Components Analysis. Section 6 details the efforts of participants on the query performance sub-task. Concluding remarks follow in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ClueWeb1Category A and B corpus</head><p>As with ClueWeb09, the ClueWeb12 corpus comes with two datasets: Category A, and Category B. The Category A dataset is the main corpus and contains about 733 million documents (27.3 TB uncompressed, 5.54 TB compressed). The Category B dataset is a sample from Category A, containing about 52 million documents, or about 7% of the Category A total. Details on how the Category A and B corpora were created may be found on the Lemur project website<ref type="foot" coords="3,265.74,611.68,4.23,6.99" target="#foot_0">2</ref> . We strongly encouraged participants to use the full Category A data set if possible. All of the results in this overview paper are labeled by their corpus category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>NIST created and assessed 50 new topics for the TREC 2014 Web track. As with TREC 2013, the TREC 2014 Web track included a significant proportion of more focused topics designed to represent more specific, less frequent, possibly more difficult queries. To retain the Web flavor of queries in this track, we kept the notion that some topics may be multi-faceted, i.e. broader in intent and thus structured as a representative set of subtopics, each related to a different potential aspect of user need. Examples are provided below. For topics with multiple subtopics, documents were judged with respect to each of the subtopics. For each subtopic, NIST assessors made a six-point judgment scale as to whether or not the document satisfied the information need associated with the subtopic. For those topics with multiple subtopics, the set of subtopics was intended to be representative, not exhaustive.</p><p>Subtopics were based on information extracted from the logs of a commercial search engine, based on a pool of remaining topic candidates created but not sampled for the 2013 Web track. Topics having multiple subtopics had subtopics selected roughly by overall popularity, which was achieved using combined query suggestion and completion data from two commercial search engines. In this way, the focus was retained on a balanced set of popular subtopics, while limiting the occurrence of strange and unusual interpretations of subtopic aspects. Single-facet topic candidates were developed based on queries extracted from search log data that were low-frequency ('tail-like') but issued by multiple users; less than 10 terms in length; and relatively low effectiveness scores across multiple commercial search engines (as of January 2013).</p><p>The topic structure was similar to that used for the TREC 2009 topics. An example of a single-facet topic: &lt;topic number="293" type="single"&gt; &lt;query&gt;educational advantages of social networking sites&lt;/query&gt; &lt;description&gt; What are the educational benefits of social networking sites? &lt;/description&gt; &lt;/topic&gt; An example of a faceted topic: &lt;topic number="289" type="faceted"&gt; &lt;query&gt;benefits of yoga&lt;/query&gt; &lt;description&gt;What are the benefits of yoga for kids?&lt;/description&gt; &lt;subtopic number="1" type="inf"&gt;What are the benefits of yoga for kids?&lt;/subtopic&gt; &lt;subtopic number="2" type="inf"&gt;Find information on yoga for seniors.&lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt;Does yoga help with weight loss?&lt;/subtopic&gt; &lt;subtopic number="4" type="inf"&gt;What are the benefits of various yoga poses?&lt;/subtopic&gt; &lt;subtopic number="5" type="inf"&gt;What are the benefits of yoga during pregnancy?&lt;/subtopic&gt; &lt;subtopic number="6" type="inf"&gt;How does yoga benefit runners?&lt;/subtopic&gt; &lt;subtopic number="7" type="inf"&gt;Find the benefits of yoga nidra.&lt;/subtopic&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/topic&gt;</head><p>The initial release of topics to participants included only the query field, as shown in the excerpt here: As shown in the above examples, those topics with a clear focused intent have a single subtopic. Topics with multiple subtopics reflect underspecified queries, with different aspects covered by the subtopics. We assume that a user interested in one aspect may still be interested in others. Each subtopic was informally categorized by NIST as being either navigational ("nav") or informational ("inf"). A navigational subtopic usually has only a small number of relevant pages (often one). For these subtopics, we assume the user is seeking a page with a specific URL, such as an organization's homepage. On the other hand, an informational query may have a large number of relevant pages. For these subtopics, we assume the user is seeking information without regard to its source, provided that the source is reliable.</p><p>For the adhoc task, relevance is judged on the basis of the description field. Thus, the first subtopic is always identical to the description sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology and Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pooling and Judging</head><p>For each topic, participants in the adhoc and risk-sensitive tasks submitted a ranking of the top 10,000 results for that topic. All submitted runs were included in the pool for judging (with the exception of 2 runs from 1 group that were marked as lowest judging priority and exceeded the per-team task limit in the guidelines). A common pool was created from the runs submitted to both tasks, which were pooled to rank depth 25.</p><p>For the risk-sensitive task, versions of ndeval and gdeval supporting the risk-sensitive versions of the evaluation measures (described below) were provided to NIST. These versions were identical to those used in last year's track except for a minor adjustment in output formatting.</p><p>All data and tools required for evaluation, including the scoring programs ndeval and gdeval as well as the baseline runs used in computation of the risk-sensitive scores are available in the track's github distribution <ref type="foot" coords="6,442.13,339.28,4.23,6.99" target="#foot_1">3</ref> .</p><p>The relevance judgment for a page was one of a range of values as described in Section 4.2. All topic-aspect combinations this year had a nonzero number of known relevant documents in the ClueWeb12 corpus. For topics that had a single aspect in the original topics file, that one aspect is used. For all other topics, aspect number 1 is the single aspect. All topics were judged to depth 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adhoc Retrieval Task</head><p>An adhoc task in TREC provides the basis for evaluating systems that search a static set of documents using previously-unseen topics. The goal of an adhoc task is to return a ranking of the documents in the collection in order of decreasing probability of relevance. The probability of relevance for a document is considered independently of other documents that appear before it in the result list. For the adhoc task, documents are judged on the basis of the description field using a six-point scale, defined as follows:</p><p>1. Nav: This page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site.</p><p>(relevance grade 4)</p><p>2. Key: This page or site is dedicated to the topic; authoritative and comprehensive, it is worthy of being a top result in a web search engine.</p><p>(relevance grade 3)</p><p>3. HRel: The content of this page provides substantial information on the topic. (relevance grade 2)</p><p>4. Rel: The content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page. (relevance grade 1)</p><p>5. Non: The content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query. (relevance grade 0) 6. Junk: This page does not appear to be useful for any reasonable purpose; it may be spam or junk (relevance grade -2).</p><p>After each description we list the relevance grade assigned to that level as they appear in the judgment (qrels) file. These relevance grades are also used for calculating graded effectiveness measures, except that a value of -2 is treated as 0 for this purpose.</p><p>The primary effectiveness measure for the adhoc task was intent-aware expected reciprocal rank (ERR-IA) which is a diversity-based variant of ERR as defined by Chapelle et al. <ref type="bibr" coords="7,277.97,417.82,11.52,9.57" target="#b0">[1]</ref> that accounts for faceted topics. For single-facet topics, ERR-IA simply becomes ERR. We also report an intentaware version of nDCG, α-nDCG <ref type="bibr" coords="7,285.28,444.92,10.91,9.57" target="#b2">[3]</ref>, and novelty-and rank-biased precision (NRBP) <ref type="bibr" coords="7,169.15,458.47,10.91,9.57" target="#b1">[2]</ref>. Table <ref type="table" coords="7,218.36,458.47,5.45,9.57">2</ref> presents the (diversity-aware) performance of the participating groups in the Adhoc task, ranked by ERR-IA@20 and selecting each group's highest performing run among those they submitted to the Adhoc task. The applied measures, ERR-IA@20, α-nDCG@20, and NRBP, take into account the multiple possible subintents underlying a given topic, and hence measure if the participants systems would have performed effective retrieval for such multi-faceted queries. Of note, the highest performing run was a manual run. Moreover, while category B runs were permitted, no participanting groups chose to submit category B runs.</p><p>We also report the standard (non-diversity-based) ERR@20 and nDCG@20 effectiveness measures for the Adhoc task in Table <ref type="table" coords="7,375.97,593.96,4.24,9.57" target="#tab_3">3</ref>. We note that these rankings exhibit some differences from Table <ref type="table" coords="7,349.25,607.51,4.24,9.57">2</ref>, demonstrating that some systems may focus upon single dominant interpretations of a query, without trying to uncover other possible interpretations.</p><p>Finally, Figure <ref type="figure" coords="7,217.80,648.16,5.45,9.57">1</ref> visualizes the per-topic variability in ERR@20 across all submitted runs. For many topics, there was relatively little difference Table 2: Top ad-hoc task results (diversity-based measures), ordered by ERR-IA@20. Only the best automatic run according to ERR-IA@20 from each group is included in the ranking. Only one team submitted a manual run that outperformed automatic -the highest manual run from that team (udel fang) is included as well. between the top runs and the median, according to some of the effectiveness measures (e.g. ERR@20 and some diversity measures). As a result, a small number of topics tended to contribute to most of the variability observed between systems. In particular, topics 298, 273, 253, 293, 269 had especially high variability across systems. In comparing the Indri and Terrier baselines used for risk-sensitive evaluation: absolute difference in ERR@20 between the baselines was greater than 0.10 for 17 topics, and greater than 0.20 for 7 topics. Expressed as a relative percentage gain/loss, there were 18 topics for which the Terrier baseline ERR@20 was at least 50% higher than the Indri baseline, and 6 topics where the Indri baseline was at least 50% higher than the Terrier baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Risk-sensitive Retrieval Task</head><p>The risk-sensitive retrieval task for Web evaluation rewards algorithms that not only achieve improvements in average effectiveness across topics (as in the adhoc task), but also maintain good robustness, which we define as minimizing the risk of significant failure relative to a given baseline. Search engines use increasingly sophisticated stages of retrieval in their quest to improve result quality: from personalized and contextual re-ranking to automatic query reformulation. These algorithms aim to increase retrieval   effectiveness on average across queries, compared to a baseline ranking that does not use such operations. However, these operations are also risky since they carry the possibility of failure -that is, making the results worse than if they had not been used at all. The goal of the risk-sensitive task is two-fold: 1) To encourage research on algorithms that go beyond just optimizing average effectiveness in order to effectively optimize both effectiveness and robustness, and achieve effective tradeoffs between these two competing goals; and 2) to explore effective risk-aware evaluation criteria for such systems. The risk-sensitive retrieval track is related to the goals of the earlier TREC Robust Track <ref type="bibr" coords="10,229.47,537.10,59.10,9.57">(TREC 2004</ref><ref type="bibr" coords="10,297.34,537.10,19.39,9.57">(TREC , 2005</ref>), <ref type="foot" coords="10,326.43,535.15,4.23,6.99" target="#foot_2">4</ref> which focused on increasing retrieval effectiveness for poorly-performing topics using evaluation measures such as geometric MAP that focused on maximizing the average improvement on the most difficult topics. The risk-sensitive retrieval track can be thought of as a next step in exploring more general retrieval objectives and evaluation measures that (a) explicitly account for, and can differentiate systems based on, differences in variance or other risk-related statistics of the win/loss distribution across topics for a single run, (b) the quality of the curve derived from a set of tradeoffs between effectiveness and robustness achievable by systems, measured across multiple runs at different average effectiveness levels, and (c) computing (a) and (b) by accounting for the effectiveness of a competing baseline (both standard, and participant-supplied) as a factor in optimizing retrieval performance.</p><p>Two runs were provided as standard baselines, derived from the Indri<ref type="foot" coords="11,479.72,199.09,4.23,6.99" target="#foot_3">5</ref> and Terrier<ref type="foot" coords="11,179.97,212.64,4.23,6.99" target="#foot_4">6</ref> retrieval engines. For Indri, we used a pseudo-relevance feedback run as implemented by the Indri retrieval engine. Specifically, for each query, we used 10 feedback documents, 20 feedback terms, and a linear interpolation weight of 0.60 with the original query. For the Terrier standard baseline, documents were ranked using the DPH weighting model from the Divergence from Randomness framework. For both systems, we provided baselines with and without application of the Waterloo spam classifier, where the filtered runs removed all documents with a percentile-score less than 70 from the rankings <ref type="foot" coords="11,270.89,321.03,4.23,6.99" target="#foot_5">7</ref> .</p><p>As with the adhoc task, we use Intent-Aware Expected Reciprocal Rank (ERR-IA) as the basic measure of retrieval effectiveness, and per-query retrieval delta is defined as the absolute difference in effectiveness between a contributed run and the above standard baseline run, for a given query. A positive delta means a win for the system on that query, and negative delta means a loss. For single runs, the following will be the main risk-sensitive evaluation measure. Let ∆(q) = R A (q) -R BASE (q) be the absolute win or loss for query q with system retrieval effectiveness R A (q) relative to the baseline's effectiveness R BASE (q) for the same query. We categorize the outcome for each query q in the set Q of all N queries according to the sign of ∆(q), giving three categories: Hurt Queries (Q -) have ∆(q) &lt; 0; Unchanged Queries (Q 0 ) have ∆(q) = 0; Improved Queries (Q + ) have ∆(q) &gt; 0. The risk-sensitive utility measure U RISK (Q) of a system over the set of queries Q is defined as:</p><formula xml:id="formula_0" coords="11,178.65,550.73,301.17,11.37">U RISK (Q) = 1/N • [Σ q∈Q + ∆(q) -(α + 1)Σ q∈Q -∆(q)] (<label>1</label></formula><formula xml:id="formula_1" coords="11,479.81,550.73,4.65,9.57">)</formula><p>where α is the risk-aversion parameter. In words, this rewards systems that maximize average effectiveness, but also penalizes losses relative to the baseline results for the same query, weighting losses α + 1 times as heavily as successes. For example, when the risk aversion parameter α is large, this rewards systems that are more conservative and able to avoid large losses relative to the baseline. The adhoc task objective, maximizing only average effectiveness across queries, corresponds to the special case α = 0. Table <ref type="table" coords="12,171.52,173.94,5.45,9.57">5</ref> summarizes the results for all runs submitted for the risk-sensitive retrieval task, according to the U RISK evaluation measure with α = 5, using each of the Indri and Terrier baselines. The average U RISK across both baselines is also reported. Notably, the best average and Terrier-relative U RISK was achieved by manual runs (in the top 3 places). However, the best Indrirelative U RISK was achieved by an automatic run. Also, the relative ranking of runs by U RISK changes considerably with the choice of baseline. This may indicate that some systems were tuned to optimize U RISK for one baseline but not the other -or for no baseline at all.</p><p>In last year's risk-sensitive task, participants were asked to submit a set of runs that were optimized for different levels of risk aversion, e.g. by optimizing for U RISK using different pre-specified values of α. However, not enough groups attempted this to allow for meaningful analysis. As a result, we adjusted the task this year so that participants were requested to submit risk-sensitive runs each of which was optimized against one of two different baselines. Participants also had the opportunity to include their own self-defined baseline runs. The goal was to reward systems that could successfully adapt to multiple baselines. Table <ref type="table" coords="12,355.27,404.28,5.45,9.57" target="#tab_4">4</ref> gives the complete set of U RISK results (α = 5) for the official and self-defined baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of variation in effectiveness across runs</head><p>Using Principal Component Analysis (PCA) on topic retrieval effectiveness scores across all submitted runs can help identify underlying factors that account for variation across systems. Figure <ref type="figure" coords="12,337.38,503.65,5.45,9.57" target="#fig_3">2</ref> shows a biplot based on PCA of standardized retrieval effectiveness scores (ERR@20 and nDCG@20) for all 2014 topics and runs. Runs are plotted as circles, with selected runs labeled with TREC run name. Topics are plotted as plus signs. (Details on PCA plots for IR can be found in Dinçer <ref type="bibr" coords="12,325.16,557.84,11.27,9.57" target="#b4">[5]</ref>.)</p><p>Topics and runs near the origin have a mean effectiveness score close to the group mean. Topics and runs far from the origin along one or both component dimensions are those that have more influence on the group ranking (based on average ERR@20 or nDCG@20 respectively). Topics that are close together in the plot are those with similar effectiveness profiles across runs. Likewise, runs that are close together had similar effectiveness profiles across topics.  More than 80% of the overall variation in retrieval effectiveness was captured by a single (first) principal component, for both ERR@20 (80.9%) and nDCG@20 (86.6%). A subset of about 10 topics had large coordinate values (greater than 1) associated with this first principal component, and thus had a large effect on the overall ranking of systems by average retrieval effectiveness. Examples of such topics include topic 267 (feliz navidad lyrics) for ERR@20 and topic 297 (altitude sickness) for NDCG@20. (These results are in accord with those of Figure <ref type="figure" coords="14,302.54,228.14,5.45,9.57">1</ref> that visualizes per-topic variability across runs.)</p><p>The second principal component accounted for much less remaining variation for both ERR@20 (6.1%) and nDCG@20 (2.8%). A set of 3-5 topics (ERR@20 vs nDCG@20) were strongly associated with the second component. These included topic 269 (marshall county schools) and topic 298 (medical care and jehovah's witnesses). We can see from the biplot that some runs (e.g. UDInfoWebRiskTR) did well on the first-component topics (and these were typically highly-ranked overall) but less so on the secondcomponent topics, while other runs (e.g. utexact and ICTNET14ADR1) did well on the second-component topics.</p><p>The separation between first-and second-component runs could be due to different choices of text representation or feature weighting. For example, similarities between the runs that did well on second-component topics (utexact and ICTNET14ADR1) might be due to their focus on anchor text. Better understanding of these factors is a topic for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Query Performance Prediction</head><p>Determining the effectiveness of a retrieval, be it a baseline or a treatment, is one of the fundamental subtasks in risk-aware ranking. As such, in addition to the core document ranking tasks, the web track included a query performance prediction subtask, where participants were asked to rank topics according to the predicted performance of the baseline and the treatment.</p><p>The format of the evaluation followed the TREC 2004 Robust Retrieval Track with minor variations <ref type="bibr" coords="14,258.38,571.39,10.91,9.57" target="#b6">[7]</ref>. Participants were asked to output a score for each topic: (a) a prediction score for the absolute effectiveness of the results for baseline used for risk-sensitive run, (b) a prediction score for the absolute effectiveness of the results for the risk-sensitive run, and (c) a relative gain or loss prediction score (the difference in effectiveness between the risk-sensitive run and the baseline run). We evaluated the ability of participants to predict the rank ordering of per-topic performance and, therefore, measured the Kendall's τ correlation between the system performance prediction scores and the actual topic performance in terms of ERR@20.</p><p>We present summary results in Table <ref type="table" coords="15,333.85,160.39,5.45,9.57">6</ref> and scatterplots of per-topic prediction scores versus ERR@20 in Figures <ref type="figure" coords="15,339.86,173.94,20.60,9.57">3, 4,</ref> and<ref type="figure" coords="15,385.31,173.94,4.24,9.57">5</ref>.</p><p>We found that participant performance predictors were weakly correlated with the actual topic performance. On an absolute scale, the values of Kendall's τ are low, with an mean τ of &lt; 0.1 across all prediction tasks. The Kendall's τ values tend to lie between 0.1 and 0.5 for web topics [6, Table <ref type="table" coords="15,155.74,241.69,4.24,9.57" target="#tab_4">4</ref>] and 0.3 and 0.5 for non-web tasks [4, Table <ref type="table" coords="15,377.81,241.69,3.84,9.57" target="#tab_0">1</ref>], suggesting that the 2014 topics may be more confusing to systems than previous collections.</p><p>For both baseline and risk runs in isolation, participant predictions were more weakly correlated (τ baseline = 0.04, τ riskrun = 0.03) compared to predictions of relative improvements (τ relative = 0.07). This observation suggests that detecting within-topic performance may be easier and, as a result, supports risk-sensitive retrieval as a research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This is the last year for the Web track at TREC in its current form. Over the past 6 years, the Web track has developed resources including 300 topics with associated relevance judgments, across two separate corpora: ClueWeb09 and ClueWeb12.</p><p>Particular areas that we believe that have benefited from the TREC Web track include approaches for learning-to-rank, diversification and the efficiency/effectiveness tradeoff, as well as risk-sensitive models and evaluation. In particular, we believe that there is much further scope for promising research in the area of risk-sensitive retrieval, building upon the resources created by the TREC Web track. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,125.80,669.34,358.65,9.57;9,125.80,682.89,358.66,9.57;9,125.80,696.44,358.65,9.57;9,125.80,709.99,77.00,9.57"><head>( b )Figure 1 :</head><label>b1</label><figDesc>Figure1: Boxplots for TREC 2014 Web topics, showing variation in ERR@20 effectiveness across all submitted runs. Topics are sorted by decreasing Terrier baseline ERR@20 (blue bar) with Indri baseline also shown (light pink bar).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,259.46,401.30,91.33,7.86;13,255.20,662.74,99.84,7.86"><head></head><label></label><figDesc>(a) Measure: ERR@20 (b) Measure: NDCG@20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="13,125.80,684.15,358.65,9.57;13,125.80,697.70,275.96,9.57;13,299.67,691.60,10.91,9.57;13,152.71,421.67,304.84,235.54"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Principal components biplot of TREC 2014 Web Track runs and topics, based on ERR@20 (top) and nDCG@20 (bottom). 13</figDesc><graphic coords="13,152.71,421.67,304.84,235.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,182.49,131.82,259.31,215.87"><head>Table 1 :</head><label>1</label><figDesc>TREC 2014 Web Track participation.</figDesc><table coords="3,182.49,131.82,259.31,215.87"><row><cell cols="4">Task Adhoc Risk Total</cell></row><row><cell>Groups</cell><cell>9</cell><cell>5</cell><cell>9</cell></row><row><cell>Runs</cell><cell>30</cell><cell>12</cell><cell>42</cell></row><row><cell cols="4">Carnegie Mellon University and Ohio State University</cell></row><row><cell cols="2">Chinese Academy of Sciences</cell><cell></cell><cell></cell></row><row><cell cols="3">Delft University of Technology</cell><cell></cell></row><row><cell cols="3">Medical Informatics Laboratory</cell><cell></cell></row><row><cell cols="3">University of Delaware (Carterette)</cell><cell></cell></row><row><cell cols="3">University of Delaware (Fang)</cell><cell></cell></row><row><cell cols="4">University of Glasgow (Terrier Team)</cell></row><row><cell cols="3">University of Massachusetts Amherst</cell><cell></cell></row><row><cell>University of Twente</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,125.80,131.82,358.66,245.82"><head>Table 3 :</head><label>3</label><figDesc>Top ad-hoc task results (non-diversity-based) ordered by ERR@20. Only</figDesc><table coords="10,125.80,144.42,358.66,233.22"><row><cell cols="6">the best automatic run according to ERR@20 from each group is included in the</cell></row><row><cell cols="6">ranking. Only one team submitted a manual run that outperformed automatic -</cell></row><row><cell cols="5">the highest manual run from that team (udel fang) is included as well.</cell><cell></cell></row><row><cell>Group</cell><cell>Run</cell><cell>Cat</cell><cell>Type</cell><cell cols="2">ERR@20 nDCG@20</cell></row><row><cell>udel fang</cell><cell>UDInfoWebRiskTR</cell><cell>A</cell><cell>manual</cell><cell>0.233</cell><cell>0.325</cell></row><row><cell>ICTNET</cell><cell>ICTNET14ADR1</cell><cell>A</cell><cell>auto</cell><cell>0.208</cell><cell>0.261</cell></row><row><cell>udel fang</cell><cell>UDInfoWebAX</cell><cell>A</cell><cell>auto</cell><cell>0.207</cell><cell>0.307</cell></row><row><cell>Group.Xu</cell><cell>Terran</cell><cell>A</cell><cell>auto</cell><cell>0.204</cell><cell>0.294</cell></row><row><cell>uogTr</cell><cell>uogTrDwl</cell><cell>A</cell><cell>auto</cell><cell>0.195</cell><cell>0.324</cell></row><row><cell>Organizers1</cell><cell>TerrierBase</cell><cell>A</cell><cell>auto</cell><cell>0.189</cell><cell>0.260</cell></row><row><cell>udel</cell><cell>udelCombCAT2</cell><cell>A</cell><cell>auto</cell><cell>0.179</cell><cell>0.261</cell></row><row><cell>SNUMedinfo</cell><cell>SNUMedinfo12</cell><cell>A</cell><cell>auto</cell><cell>0.176</cell><cell>0.270</cell></row><row><cell>BUW</cell><cell>webisWt14axAll</cell><cell>A</cell><cell>auto</cell><cell>0.174</cell><cell>0.258</cell></row><row><cell>wistud</cell><cell>wistud.runB</cell><cell>A</cell><cell>auto</cell><cell>0.174</cell><cell>0.291</cell></row><row><cell>ut</cell><cell>utexact</cell><cell>A</cell><cell>auto</cell><cell>0.172</cell><cell>0.226</cell></row><row><cell>Organizers2</cell><cell>IndriBase</cell><cell>A</cell><cell>auto</cell><cell>0.153</cell><cell>0.243</cell></row><row><cell>UMASS CIIR</cell><cell>CiirAll1</cell><cell>A</cell><cell>auto</cell><cell>0.153</cell><cell>0.250</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="20,125.80,165.43,358.66,457.23"><head>Table 4 :</head><label>4</label><figDesc>Risk-sensitive effectiveness of submitted risk runs relative to official and self-defined baselines.</figDesc><table coords="20,209.88,199.19,190.49,423.47"><row><cell>Run</cell><cell>Baseline</cell><cell>U RISK , α = 5</cell></row><row><cell>ICTNET14RSR1</cell><cell>indri</cell><cell>-0.01702</cell></row><row><cell>ICTNET14RSR1</cell><cell>terrier</cell><cell>-0.25635</cell></row><row><cell>ICTNET14RSR1</cell><cell>ICTNET14ADR1</cell><cell>-0.32634</cell></row><row><cell>ICTNET14RSR1</cell><cell>ICTNET14ADR2</cell><cell>-0.31050</cell></row><row><cell>ICTNET14RSR1</cell><cell>ICTNET14ADR3</cell><cell>-0.32254</cell></row><row><cell>ICTNET14RSR2</cell><cell>indri</cell><cell>-0.11972</cell></row><row><cell>ICTNET14RSR2</cell><cell>terrier</cell><cell>-0.18145</cell></row><row><cell>ICTNET14RSR2</cell><cell>ICTNET14ADR1</cell><cell>-0.32083</cell></row><row><cell>ICTNET14RSR2</cell><cell>ICTNET14ADR2</cell><cell>-0.30409</cell></row><row><cell>ICTNET14RSR2</cell><cell>ICTNET14ADR3</cell><cell>-0.31601</cell></row><row><cell>ICTNET14RSR3</cell><cell>indri</cell><cell>-0.15928</cell></row><row><cell>ICTNET14RSR3</cell><cell>terrier</cell><cell>-0.20986</cell></row><row><cell>ICTNET14RSR3</cell><cell>ICTNET14ADR1</cell><cell>-0.02661</cell></row><row><cell>ICTNET14RSR3</cell><cell>ICTNET14ADR2</cell><cell>-0.01318</cell></row><row><cell>ICTNET14RSR3</cell><cell>ICTNET14ADR3</cell><cell>-0.01554</cell></row><row><cell>udelCombCAT2</cell><cell>indri</cell><cell>-0.11092</cell></row><row><cell>udelCombCAT2</cell><cell>terrier</cell><cell>-0.27713</cell></row><row><cell>udelCombCAT2</cell><cell>udel itu</cell><cell>-0.37582</cell></row><row><cell>udelCombCAT2</cell><cell>udel itub</cell><cell>-0.32152</cell></row><row><cell>UDInfoWebRiskAX</cell><cell>indri</cell><cell>-0.09497</cell></row><row><cell>UDInfoWebRiskAX</cell><cell>terrier</cell><cell>-0.13282</cell></row><row><cell>UDInfoWebRiskAX</cell><cell>UDInfoWebAX</cell><cell>-0.09389</cell></row><row><cell>UDInfoWebRiskAX</cell><cell>UDInfoWebENT</cell><cell>-0.09006</cell></row><row><cell>UDInfoWebRiskAX</cell><cell>UDInfoWebLES</cell><cell>-0.06893</cell></row><row><cell>UDInfoWebRiskRM</cell><cell>indri</cell><cell>-0.07929</cell></row><row><cell>UDInfoWebRiskRM</cell><cell>terrier</cell><cell>-0.12730</cell></row><row><cell>UDInfoWebRiskRM</cell><cell>UDInfoWebAX</cell><cell>-0.09723</cell></row><row><cell>UDInfoWebRiskRM</cell><cell>UDInfoWebENT</cell><cell>-0.08083</cell></row><row><cell>UDInfoWebRiskRM</cell><cell>UDInfoWebLES</cell><cell>-0.07408</cell></row><row><cell>UDInfoWebRiskTR</cell><cell>indri</cell><cell>-0.05661</cell></row><row><cell>UDInfoWebRiskTR</cell><cell>terrier</cell><cell>-0.10552</cell></row><row><cell>UDInfoWebRiskTR</cell><cell>UDInfoWebAX</cell><cell>-0.08702</cell></row><row><cell>UDInfoWebRiskTR</cell><cell>UDInfoWebENT</cell><cell>-0.07484</cell></row><row><cell>UDInfoWebRiskTR</cell><cell>UDInfoWebLES</cell><cell>-0.06485</cell></row><row><cell>uogTrBwf</cell><cell>indri</cell><cell>-0.13225</cell></row><row><cell>uogTrBwf</cell><cell>terrier</cell><cell>-0.22992</cell></row><row><cell>uogTrBwf</cell><cell>uogTrDuax</cell><cell>-0.21253</cell></row><row><cell>uogTrBwf</cell><cell>uogTrDwl</cell><cell>-0.26402</cell></row><row><cell>uogTrBwf</cell><cell>uogTrIwa</cell><cell>-0.12952</cell></row><row><cell>uogTrDwsts</cell><cell>indri</cell><cell>-0.12092</cell></row><row><cell>uogTrDwsts</cell><cell>terrier</cell><cell>-0.26885</cell></row><row><cell>uogTrDwsts</cell><cell>uogTrDuax</cell><cell>-0.26911</cell></row><row><cell>uogTrDwsts</cell><cell>uogTrDwl</cell><cell>-0.27401</cell></row><row><cell>uogTrDwsts</cell><cell>uogTrIwa</cell><cell>-0.20727</cell></row><row><cell>uogTrq1</cell><cell>indri</cell><cell>-0.12489</cell></row><row><cell>uogTrq1</cell><cell>terrier</cell><cell>-0.22741</cell></row><row><cell>uogTrq1</cell><cell>uogTrDuax</cell><cell>-0.19293</cell></row><row><cell>uogTrq1</cell><cell>uogTrDwl</cell><cell>-0.22614</cell></row><row><cell>uogTrq1</cell><cell>uogTrIwa</cell><cell>-0.18079</cell></row><row><cell>wistud.runD</cell><cell>indri</cell><cell>-0.15582</cell></row><row><cell>wistud.runD</cell><cell>terrier</cell><cell>-0.23495</cell></row><row><cell>wistud.runD</cell><cell>wistud.runA</cell><cell>-0.04875</cell></row><row><cell>wistud.runD</cell><cell>wistud.runB</cell><cell>-0.18761</cell></row><row><cell>wistud.runD</cell><cell>wistud.runC</cell><cell>-0.18761</cell></row><row><cell>wistud.runE</cell><cell>indri</cell><cell>-0.17293</cell></row><row><cell>wistud.runE</cell><cell>terrier</cell><cell>-0.35354</cell></row><row><cell>wistud.runE</cell><cell>wistud.runA</cell><cell>-0.21114</cell></row><row><cell>wistud.runE</cell><cell>wistud.runB</cell><cell>-0.19061</cell></row><row><cell>wistud.runE</cell><cell>wistud.runC</cell><cell>-0.19061</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,142.39,662.46,202.42,7.47"><p>http://lemurproject.org/clueweb12/specs.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,142.39,663.68,189.78,7.47"><p>http://github.com/trec-web/trec-web-2014</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="10,142.39,654.27,240.07,7.47"><p>http://trec.nist.gov/data/robust/04.guidelines.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="11,142.39,637.61,160.05,7.47"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="11,142.39,648.57,84.73,7.47"><p>http://terrier.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="11,142.39,659.53,235.37,9.21"><p>http://www.mansci.uwaterloo.ca/ ~msmucker/cw12spam/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p>We thank <rs type="person">Jamie Callan</rs>, <rs type="person">David Pane</rs> and the <rs type="institution">Language Technologies Institute at Carnegie Mellon University</rs> for creating and distributing the ClueWeb12 collection. This track could not operate without this valuable resource. We also thank <rs type="person">Nick Craswell</rs> and <rs type="person">Charlie Clarke</rs> for their many valuable suggestions and feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table" coords="21,155.95,586.24,4.24,9.57">6</ref><p>: Kendall's τ between system predicted value of per-topic ERR@20 and observed ERR@20 for baseline and risk runs. The third column presents τ between the system predicted value of relative ERR@20 and observed relative ERR@20.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.77,157.64,341.69,9.57;16,142.77,171.19,341.69,9.57;16,142.77,184.74,341.69,9.57;16,142.77,198.29,163.36,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,391.11,157.64,93.34,9.57;16,142.77,171.19,121.27,9.57">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,289.61,171.19,194.85,9.57;16,142.77,184.74,262.69,9.57">Proceedings of the 18th ACM conference on Information and knowledge management, CIKM &apos;09</title>
		<meeting>the 18th ACM conference on Information and knowledge management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.77,220.81,341.68,9.57;16,142.77,234.36,341.68,9.57;16,142.77,247.91,341.69,9.57;16,142.77,261.45,341.69,9.57;16,142.77,275.00,339.42,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,362.37,220.81,122.08,9.57;16,142.77,234.36,198.58,9.57">An effectiveness measure for ambiguous and underspecified queries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,142.77,261.45,196.37,9.57">Advances in Information Retrieval Theory</title>
		<title level="s" coord="16,420.23,261.45,64.22,9.57;16,142.77,275.00,97.65,9.57">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5766</biblScope>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.77,297.52,341.69,9.57;16,142.77,311.07,341.69,9.57;16,142.77,324.62,341.68,9.57;16,142.77,338.17,341.69,9.57;16,142.77,351.72,292.74,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,306.64,311.07,177.82,9.57;16,142.77,324.62,88.75,9.57">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,251.68,324.62,232.78,9.57;16,142.77,338.17,341.69,9.57;16,142.77,351.72,46.89,9.57">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;08</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.77,374.23,341.69,9.57;16,142.77,387.78,341.68,9.57;16,142.77,401.33,341.68,9.57;16,142.77,414.88,192.15,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,184.64,374.23,247.13,9.57">Performance prediction using spatial autocorrelation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,453.51,374.23,30.94,9.57;16,142.77,387.78,341.68,9.57;16,142.77,401.33,260.81,9.57">SIGIR &apos;07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="583" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.77,437.39,341.69,9.57;16,142.77,450.94,341.69,9.57;16,142.77,464.49,156.10,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,211.69,437.39,272.76,9.57;16,142.77,450.94,44.81,9.57">Statistical principal components analysis for retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Dinçer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,196.33,450.94,288.13,9.57;16,142.77,464.49,49.90,9.57">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="560" to="574" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.77,487.01,341.68,9.57;16,142.77,500.56,341.68,9.57;16,142.77,514.11,341.69,9.57;16,142.77,527.66,163.36,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,362.41,487.01,122.04,9.57;16,142.77,500.56,104.06,9.57">Improved query difficulty prediction for the web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,271.86,500.56,212.60,9.57;16,142.77,514.11,259.15,9.57">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.77,550.17,341.68,9.57;16,142.77,563.72,341.68,9.57;16,142.77,577.27,88.55,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,224.73,550.17,240.03,9.57">Overview of the TREC 2004 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,142.77,563.72,271.13,9.57">Proceedings of the Thirteenth Text REtrieval Conference</title>
		<meeting>the Thirteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
