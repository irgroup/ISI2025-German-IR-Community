<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.95,75.27,397.78,9.13;1,90.95,88.47,187.25,9.13">Evaluating Stream Filtering for Entity Profile Updates in TREC 2012, 2013, and 2014 (KBA Track Overview, Notebook Paper)</title>
				<funder ref="#_73EaCaD">
					<orgName type="full">Open Science Grid, Amazon</orgName>
				</funder>
				<funder>
					<orgName type="full">MKW</orgName>
				</funder>
				<funder>
					<orgName type="full">TREC KBA</orgName>
				</funder>
				<funder>
					<orgName type="full">DAR thank the Fannie and John Hertz Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.95,101.67,62.71,9.13"><forename type="first">John</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">KBA Organizers</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,162.92,101.67,92.75,9.13"><forename type="first">Max</forename><surname>Kleiman-Weiner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">KBA Organizers</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.92,101.67,78.29,9.13"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">KBA Organizers</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,90.95,114.87,67.71,9.13"><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,167.94,114.87,54.45,9.13"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.95,75.27,397.78,9.13;1,90.95,88.47,187.25,9.13">Evaluating Stream Filtering for Entity Profile Updates in TREC 2012, 2013, and 2014 (KBA Track Overview, Notebook Paper)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">155081EB2D1A669F7C2AE3864E1686A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Information Filtering</term>
					<term>H.3.m [Information Storage and Retrieval]: Miscellaneous -Test Collections</term>
					<term>I.2.7 [Natural Language Processing] Text analysis -Language parsing and understanding Experimentation, Measurement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Knowledge Base Acceleration (KBA) track ran in TREC 2012TREC  , 2013TREC  , and 2014   as an entitycentric filtering evaluation. This track evaluates systems that filter a time-ordered corpus for documents and slot fills that would change an entity profile in a predefined list of entities. Compared with the 2012 and 2013 evaluations, the 2014 evaluation introduced several refinements, including high-quality community metadata from running Raytheon/BBN's Serif named entity recognizer, sentence parser, and relation extractor on 579,838,246 English documents in the corpus. We also expanded the query entities to be primarily long-tail entities that lacked Wikipedia profiles. We simplified the SSF scoring, and also added a third task component for highlighting creative systems that used the KBA data. A successful KBA system must do more than resolve the meaning of entity mentions by linking documents to the KB: it must also distinguish novel "vitally" relevant documents and slot fills that would change a target entity's profile. This combines thinking from natural language understanding (NLU) and information retrieval (IR). Filtering tracks in TREC have typically used queries based on topics described by a set of keyword queries or short descriptions, and annotators have generated relevance judgments based on their personal interpretation of the topic. For TREC 2014, we selected a set of filter topics based on people, organizations, and facilities in the region between Seattle, Washington, and Vancouver, British Columbia: 86 people, 16 organizations, and 7 facilities. Assessors judged ~30k documents, which included most documents that mention a name from a handcrafted list of surface form names of the 109 target entities. TREC teams were provided with all of the ground truth data divided into training and evaluation data. We present peak macro-averaged F_1 scores for all run submissions. High scoring systems used a variety of approaches, including feature engineering around linguistic structures, names of related entities, and various types of classifiers. Top scoring systems achieved F_1 scores in the high-50s. We present results for a baseline system that performs in the low-40s. We discuss key lessons learned that motivate future tracks at the end of the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This overview paper describes the progression of the KBA evaluation over the three years from <ref type="bibr" coords="1,90.95,628.95,100.04,9.13">2012, 2013, and 2014.</ref> TREC KBA is a stream filtering task focused on entity-level events in large volumes of data. Many large knowledge bases, such as Wikipedia, are maintained by small workforces of humans who cannot manually monitor all relevant content streams. As a result, most entity profiles lag far behind current events. KBA aims to help these scarce human resources by driving research on automatic systems for filtering streams of text for new information about entities. We refer to such novel information as "vital," which has a technical meaning as the highest relevance level in the assessor-generated graded relevance data in KBA.</p><p>KBA focuses on the boundary between IR and natural language understanding (NLU). Entities are widely used concept in NLU, which focuses on fully automatic algorithms. KBA transports the concept of entities into IR and uses entities as queries for an end user task. Entities are a special subset of general topics. Entities have strongly typed attributes and relationships, such as a name, birth date, father, hometown, employer, profession, which information retrieval (IR) systems can exploit to surface novel information. We refer readers to the TREC KBA 2013 Overview Paper for a more in-depth discussion of the goals of the TREC KBA track and its relation to other evaluations the data generated by and for the track, and the evaluation metrics. This paper is organized as follows: Section 1 describes data assets generated by the evaluation. Section 2 discusses the scores from TREC KBA 2014. Section 3 concludes with three lessons learned from KBA, which motivate possible future directions that build on the KBA experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Assets</head><p>In addition to the three hundred run submissions from diverse systems, KBA produced a unique corpus and three sets of ground truth from human assessors on portions of that corpus. This data is available through NIST and also at trec-kba.org</p><p>The KBA corpus is the largest stream corpus ever released for open evaluations. It consists of 1.2 billion documents from a contiguous span of 19 months (13,663 hours) of news, blog, and Web content with several special substreams, including the Internet arXiv pre-print server. In 2014, we deployed BBN's Serif NLP system on the English and likely-English documents, which make up more than half the corpus. For 2014, we expanded the corpus with two more months of Spinn3r data. Figure <ref type="figure" coords="2,187.12,396.15,5.47,9.13" target="#fig_0">1</ref> illustrates the large scale of the corpus relative to smaller set filtered out for humans working on the particular query entities used in the evaluation.   The primary task in KBA is identifying documents that contain vital new information that would not have be in an up-to-profile at the time the document entered the stream. The difference between a vital update and background biographical info can be subjective in several ways. One particular aspect of subjectivity involves judging whether an event that changed the entity is being mentioned way after the fact. For example, a text might explain when an entity was born --fifty years after the fact. Such reporting is obviously biographical (therefore "useful") and not sufficiently timely to be vital. Borderline examples exist. For example, should we consider this passage timely? "Sara helped start NXIVM in 2007." (reporting date 2011) No, that's not timely enough. That's useful, not vital. We refer the reader to assessor guidelines included with the truth data, which can be obtained through NIST.</p><p>Assessors must also decide what to include in an entity's profile. For example, for an entity with a Wikipedia article, a profile might include why the person is noteworthy. If the person is less noteworthy, the profile might simply describe how they spend their time.</p><p>Figure <ref type="figure" coords="4,122.06,517.59,5.47,9.13" target="#fig_1">2</ref> illustrates interassessor agreement. The most important line is "Vital" colored red. This is the highest relevance level. Human agreement on the notion of vital varies across entities and domains. TREC KBA 2014 focused on less noteworthy entities than the many moderately famous people in TREC KBA 2013, and this increased subjectivity contributed to the lower interassessor agreement.</p><p>Top scoring systems in KBA 2014 captured linguistic signals in the sentence or short passage surrounding entity mentions. The precision gains found via these structures suggest that assessors' judgments of vitality are correlated with particular linguistic patterns, such as verbs, see MSR_KMG slides about action patterns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCR Queries and Training Data</head><p>For 2014, an initial round of assessing was conducted in June using name match heuristics to find candidate documents for assessors to judge for each entity. Efforts were made to have one assessor complete the judging for one entity, although this was not always practical. The assessors are identified in the truth data by unique strings.</p><p>In July, track participants were provided with all the truth data divided into two sets: ~20% for training, and ~80% for evaluation. The boundary between training and evaluation was set separately for each entity at the hour by which 20% of the true positives had appeared in the stream. The remainder was used to measure the F_1 accuracy and scaled utility of these systems.</p><p>After run submissions were sent to NIST in September, we conducted an additional week of assessing to boost recall. The final scores are computed using this expanded set of truth data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCR Metrics (see SSF Metrics below)</head><p>The metrics for CCR are F_1 and Scaled Utility(SU) and are shown in Figure <ref type="figure" coords="5,432.76,508.23,4.15,9.13" target="#fig_3">4</ref>. Most systems had an SU below 0.333, which corresponds to a run with no output. The F_1 score is the harmonic mean of the macro-averaged precision and macro-averaged recall. In this context, macro-averaging means using the confidence cutoff for which the F_1 is highest for the system under study, and summing the precision (or recall) scores from all of the query entities and dividing by the number of entities. Some of track participants have invented a time-sensitive metric <ref type="bibr" coords="5,120.94,587.67,12.41,9.13">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCR Results</head><p>Figure <ref type="figure" coords="5,122.06,626.07,5.47,9.13" target="#fig_3">4</ref> ranks the teams by their highest scoring system using the maximum F_1 or maximum Scaled Utility using two different retrieval objectives. The primary ranking is F_1 on the vital-only retrieval objective. This is a very hard task, as illustrated by the plateau of high ranking systems with scores similar to the baseline. Figure <ref type="figure" coords="7,124.27,75.27,4.42,9.13">5</ref>: macro-averaged recall versus precision with curves of constant F_1 for "vital" CCR and also "vital+useful," which is equivalent to document-level to coreference resolution. The precision and recall values correspond to the maximum F_1 as a function of confidence cutoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCR Baseline Systems</head><p>After considering several baseline systems to characterize the task, we decided to keep the official baseline that assigns a "vital" rating to every document that matches a surface form name of an entity and assigning a confidence score based on the number of matches of tokens in the name. See code in github <ref type="bibr" coords="7,209.93,199.11,10.39,9.13">[6]</ref>. macro-P=0.316, macro-R=0.520, macro-F=0.393, SU=0.3334 This is shown in the official score plots below as "baseline." In contrast to the so-called "oracle baseline" used in 2013, the baseline used in 2014 uses only the entity's canonical name instead of hand-picked variants. We also tested a similar oracle baseline that included string matches for all of the slot fills strings gathered by assessors had minimal impact on the baseline's score, and it scored almost the same as the non-oracle baseline.</p><p>As shown in Figure <ref type="figure" coords="7,178.73,284.31,4.15,9.13">5</ref>, systems had lower precision than recall. The baseline system's precision of 0.316 sets a threshold (vertical red line). Systems with precision above this line found signals that correlated with novelty. Both LSIS and UW reported improved runs shortly after the evaluation deadline, which are described in their TREC papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Streaming Slot Filling (SSF):</head><p>Compared with 2013, we significantly simplified the SSF evaluation process in 2014. Instead of detecting changes to particular slots, we simply asked systems to fill as many slots as possible. A summary of the most common slot types is displayed in  To evaluate SSF systems, we considered the token overlap between the slot fills found by human assessors and those found by a system. To make compute this overlap comparison, we constructed bags of words from each systems' output and also from the truth data. Each bag of words is a count vector in word space. Four comparison functions for bags of words are:</p><p>• dot product = &lt;a, b&gt; = sum_i a_i b_i • cosine(a,b) = &lt;a, b&gt; / |a| / |b| • c_TT(a,b) = sum_i 1 if a_i&gt;0 and b_i &gt; 0, which is similar to dot product with all counts set to one; often called "boolean overlap" • sokalsneath(a, b) = c_TT / (c_TT + 2 (c_TF + c_FT)), which is form of normalization for c_TT. c_FT means the count of items that are in the right-side vector but not the left, and c_TF is the converse. The dot product and c_TT metrics do not penalize systems for precision errors, so a naive system that outputs all of the text in the corpus will get a high score. The normalizations in cosine and sokalsneath penalize for precision errors, and makes them better metrics. By ignoring counts, the sokalsneath metric has the largest spread between systems. However, the familiar cosine metric properly penalizes the low-precision baseline system, which outputs entire sentences surrounding name-matching tokens. For this evaluation, cosine is the better metric.</p><p>See Table <ref type="table" coords="8,139.28,255.99,5.47,9.13">3</ref> in the Appendix for SSF scores. TREC KBA has established a foundation for temporally driven IR tasks at the entity level. This foundation opens up many new avenues of research. When we first proposed the track in the fall of 2011, we cited large knowledge bases and large data streams as key motivators. We also defined the track in contrast to the Knowledge Base Population track in the Text Analytics Conference (TAC KBP). Both evaluations aim to create knowledge bases from text mining. KBP evaluates fully automatic approaches and has consistently shown that key aspects of natural language understanding are beyond the state of the art. In contrast, KBA evaluates machineassisted, human-driven knowledge base curation. In addition to the many lessons learned about different technical approaches, the KBA experience also taught us about how to structure evaluations for systems on the boundary between IR and NLU. We summarize three salient aspects of this intersection:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lessons Learned and</head><p>1. While top performing KBA systems often use ranking signals generated by NLU algorithms that depend on fixed ontologies, such rigid schemas capture only a fraction of vitally relevant events. NLU systems often generate entity attributes and relations from fixed ontologies of strongly typed properties. This approach facilitates machine learning and enables sophisticated logical inference in automatically constructed knowledge bases. However, we observed that only half of the vitally relevant documents in KBA 2013 were associated with a change to a value in one of the schematized slots.</p><p>Even when such a change occurred, it often touched only tangentially on the salient information in the event. Despite the subtlety and complexity of strongly typed ontologies from evaluations such as ACE and TAC KBP, these knowledge representations evidently lack expressive power to fully capture events like this example: "M.I.A.," the rapper, tweeted that her daughter's father is using New York's child custody laws to perpetrate a human rights violation. This information is not easily schematized into slots.</p><p>2. When vital events trigger updates to a KB profile, they can also drive deeper research by the KB curator into the entity's past. In real KBs, this background research also updates the profile. KBA's emphasis on vital events is motivated by real phenomena in the stream of content, see spikes in Figure <ref type="figure" coords="9,384.33,127.11,4.15,9.13" target="#fig_2">3</ref>. Streams of content have a periodic rhythm illustrated in Figure <ref type="figure" coords="9,285.37,138.63,4.15,9.13" target="#fig_0">1</ref>. These temporal structures deeply affect humans curating knowledge bases and systems designed to support them. KBA implemented a key aspect of this by instructing assessors to define vital documents relative to an "already up-to-date profile." This combines with a short, and subjective, time window of one to three days of vitality after a sudden event. A more natural user pattern expands the search task beyond that time window to find other information that is also missing from the profile. Updating entity profiles requires human editors to both monitor the stream and explore the past. For example, current events in the Kalispel Tribe build upon a long history that also needs to get into the KB.</p><p>3. The KBA StreamCorpus captures the head of a long-tailed distribution of websites that change with varying frequency. The KBA StreamCorpus contains many salient events for noteworthy people. Most web domains are less focused on current events, and therefore typically not crawled in the feed aggregations used to make the KBA StreamCorpus. As such, the KBA corpus skims the surface of many complex networks of related entities. Exploring these complex networks will require deeper, more targeted content harvesting to expand the corpus. All document collections are gathered at a particular time, and thus are inherently "stream corpora." The StreamCorpus processing tools [7] developed while assembling the KBA corpus are in active use building more data sets and provide a foundation for this further expansion.</p><p>We hope to address these issues in a successor track to KBA, called TREC Dynamic Domain. TREC DD starts in 2015 and builds on the foundation of TREC KBA. See trec-dd.org</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.95,428.55,418.47,9.13;2,90.95,441.75,158.82,9.13"><head>Figure 1</head><label>1</label><figDesc>Figure 1 depicts the corpus statistics over time, and Table 1 describes the truth data. See also powerpoint slides from trec-kba.org</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.95,331.59,298.30,9.13;4,90.95,92.17,432.00,229.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inter-Assessor Agreement Scores across all three years.</figDesc><graphic coords="4,90.95,92.17,432.00,229.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,90.95,247.59,407.80,9.13;5,90.95,260.79,402.28,9.13;5,90.95,273.99,392.37,9.13;5,90.95,287.19,376.75,9.13;5,41.55,72.95,540.75,164.25"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: KBA 2013's 141 query entities and their vital document counts per hour. Several spikes are visible. Most spikes are echos in the blogosphere that reverberate after a single event, which is often characterized by changes to a few strongly typed entity attributes or relationships, such as death or the breakdown of a corporate or spousal relationship.</figDesc><graphic coords="5,41.55,72.95,540.75,164.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,90.95,349.35,415.48,9.13;6,90.95,362.55,406.28,9.13;6,90.95,375.75,219.50,9.13;6,90.95,392.73,404.95,324.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Official scores using "vital" and "vital+useful" as the classification objective. The red points represent the baseline system. In both 2013 and 2014, teams that built SSF systems consistently performed below the median in CCR.</figDesc><graphic coords="6,90.95,392.73,404.95,324.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,196.51,277.83,85.45,9.13;8,90.95,296.79,390.11,9.13;8,90.95,320.55,190.42,9.13;8,90.95,339.51,3.44,9.13;8,113.86,339.51,163.29,9.13;8,90.95,357.03,3.44,9.13;8,113.86,357.03,93.85,9.13;8,90.95,374.55,3.44,9.13;8,113.86,374.55,167.23,9.13;8,90.95,392.07,3.44,9.13;8,113.86,392.07,109.96,9.13;8,90.95,409.35,3.44,9.13;8,113.86,409.35,124.39,9.13;8,320.29,320.31,122.11,9.13;8,317.51,337.83,3.44,9.13;8,340.42,337.83,163.29,9.13;8,317.51,355.35,3.44,9.13;8,340.42,355.35,134.97,9.13;8,317.51,372.63,3.44,9.13;8,340.42,372.63,146.09,9.13;8,317.51,390.15,3.44,9.13;8,340.42,390.15,138.82,9.13;8,317.51,407.67,3.44,9.13;8,340.42,407.67,124.41,9.13;8,90.95,432.87,34.37,9.13"><head></head><label></label><figDesc>Future DirectionsKBA sits at the boundary between two different paradigms for exploiting textual content:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,90.95,72.95,422.60,287.99"><head></head><label></label><figDesc></figDesc><graphic coords="3,90.95,72.95,422.60,287.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.95,428.55,418.47,22.33"><head>org Figure 1: KBA</head><label></label><figDesc>Table 1 describes the truth data. See also powerpoint slides from trec-kba.'s StreamCorpus and assessing data. (depicted using the 2013 data)</figDesc><table coords="3,95.51,387.51,426.33,299.29"><row><cell></cell><cell>2012</cell><cell>2013</cell><cell>2014</cell></row><row><cell></cell><cell>7 months (4,973 hours)</cell><cell cols="2">17 months (11,948 hours) 19 months (13,663 hours)</cell></row><row><cell>Corpus [1]</cell><cell>&gt;400M documents 40% English</cell><cell>&gt;1B documents 60% English or unknown</cell><cell>1.2B documents 60% English or unknown</cell></row><row><cell></cell><cell>Oc 2011 to Apr 2012.</cell><cell>Oct 2011 to Feb 2013</cell><cell>Oct 2011 to April 2013</cell></row><row><cell></cell><cell></cell><cell>98 people, 19</cell><cell></cell></row><row><cell></cell><cell></cell><cell>organizations, and 24</cell><cell></cell></row><row><cell>Queries (entities)</cell><cell>27 people, 2 organizations, all from Wikipedia</cell><cell>facilities. Fourteen inter-related communities of entities, such small towns like Danville, KY, and Fargo, ND, and academic</cell><cell>86 people, 16 organizations, and 7 facilities all from the geographic region between Seattle and Vancouver.</cell></row><row><cell></cell><cell></cell><cell>communities like Turing</cell><cell></cell></row><row><cell></cell><cell></cell><cell>award winners.</cell><cell></cell></row><row><cell>Assessing</cell><cell>70% agreement on "central"</cell><cell>3198 hours have &gt;0 vitals, 76% agreement on "vital" (replaced "central")</cell><cell>2503 hours have &gt;0 vitals, 68% agreement on "vital"</cell></row><row><cell cols="2">Submissions 11 teams, 40 runs</cell><cell>13 teams, 140 runs</cell><cell>11 teams, 118 runs</cell></row><row><cell>Metrics</cell><cell>F_1, Scaled Utility</cell><cell>F_1, Scaled Utility</cell><cell>F_1, Scaled Utility [2]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,74.63,385.35,472.70,267.41"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="7,381.59,385.35,5.49,9.13"><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.95,663.75,37.10,9.13"><head>Table 2</head><label>2</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements:</head><p>The KBA organizers are grateful to all of the excellent teams who participated in making KBA such a great learning experience (see appendix). We also thank <rs type="person">Boyan Onyshkevych</rs> and <rs type="person">Alan Goldschen</rs> for ideas and helpful discussions. JRF, <rs type="funder">MKW</rs>, <rs type="funder">DAR thank the Fannie and John Hertz Foundation</rs> for support. <rs type="person">Ian Soboroff</rs> and <rs type="person">Ellen Voorhees</rs> at TREC have been instrumental in designing and organizing KBA. We also wish to thank Diffeo, <rs type="institution">MIT, University of Wisconsin</rs>, the <rs type="funder">Open Science Grid, Amazon</rs>, <rs type="grantNumber">Spinn3r</rs>, and the arXiv for their generous support of <rs type="funder">TREC KBA</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_73EaCaD">
					<idno type="grant-number">Spinn3r</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
