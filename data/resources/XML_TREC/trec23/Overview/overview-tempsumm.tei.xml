<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.02,164.85,307.21,15.12;1,273.37,186.77,64.51,15.12">TREC 2014 Temporal Summarization Track Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-02-17">February 17, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.53,219.25,65.26,10.48"><forename type="first">Javed</forename><surname>Aslam</surname></persName>
						</author>
						<author>
							<persName coords="1,234.25,219.25,74.79,10.48"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
						</author>
						<author>
							<persName coords="1,333.52,219.25,133.21,10.48"><forename type="first">Matthew</forename><surname>Ekstrand-Abueg</surname></persName>
						</author>
						<author>
							<persName coords="1,207.56,233.20,99.01,10.48"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
						</author>
						<author>
							<persName coords="1,341.75,233.20,61.94,10.48"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
						</author>
						<author>
							<persName coords="1,270.15,247.15,70.95,10.48"><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
						</author>
						<title level="a" type="main" coord="1,152.02,164.85,307.21,15.12;1,273.37,186.77,64.51,15.12">TREC 2014 Temporal Summarization Track Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-02-17">February 17, 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">AFC25BD6DE545B6C094B5B613DB0E020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>News events such as protests, accidents or natural disasters represent a unique information access problem where traditional approaches fail. For example, immediately after an event, the corpus may be sparsely populated with relevant content. Even when, after a few hours, relevant content becomes available, it is often inaccurate or highly redundant. At the same time, crisis events demonstrate a scenario where users urgently need information, especially if they are directly affected by the event.</p><p>The goal of this track is to develop systems for efficiently monitoring the information associated with an event over time. Specifically, we are interested in developing systems which can broadcast short, relevant, and reliable sentencelength updates about a developing event. The track has the following four main aims:</p><p>• To develop algorithms which detect sub-events with low latency,</p><p>• To model information reliability in the presence of a dynamic corpus,</p><p>• To understand and address the sensitivity of text summarization algorithms in an online, sequential setting, and</p><p>• To understand and address the sensitivity of information extraction algorithms in dynamic settings.</p><p>The remainder of this overview is structured as follows. Section 2 describes the temporal summarization task in detail. In Section 3, we discuss the corpus of documents from which the summaries are produced, while in Section 4, we discuss how temporal summarization systems are evaluated within the track. Section 5 details the process via which sentence updates were assessed. Finally, &lt;event&gt; &lt;id&gt;1&lt;/id&gt; &lt;title&gt;2012 Buenos Aires rail disaster&lt;/title&gt; &lt;description&gt;...&lt;/description&gt; &lt;start&gt;1329910380&lt;/start&gt; &lt;end&gt;1330774380&lt;/end&gt; &lt;query&gt;buenos aires train crash&lt;/query&gt; &lt;type&gt;accident&lt;/type&gt; &lt;/event&gt; in Section 6, we summarize the performance of the participant systems to the 2014 track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The aim of this task is to emit a series of sentence updates over time about a named event given a high volume stream of input documents. In particular, the temporal summarization task focuses on large events with a high impact, such as protests, accidents or natural disasters. Each event is represented by a topic description, providing a textual query representing that event, along with start and end timestamps defining a period of time within which to track that event. An example topic description is illustrated in Figure <ref type="figure" coords="2,365.78,431.35,3.87,8.74" target="#fig_0">1</ref>.</p><p>For an event, participant systems process a stream of Web documents from the tracking period as defined in the topic in temporal order. The aim is to select sentences from those documents to emit as updates describing that event. The set of sentences emitted form a summary of that event over time. An optimal summary is one that covers all of the essential information about the event with no redundancy, where each new piece of information was added to the summary as soon as it became available.</p><p>For the 2014 task, participants produced temporal summaries for 15 different events, spanning accidents, natural disasters, storms, shootings and protests. Table <ref type="table" coords="2,161.17,550.90,4.98,8.74" target="#tab_0">1</ref> summarizes these 15 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus</head><p>The 2014 Temporal Summarization track used documents from the TREC KBA 2014 Stream Corpus. This corpus consists of a set of timestamped documents from a variety of news and social media sources covering the time period October 2011 through April 2013. Each document contains a set of sentences, each with a unique identifier.</p><p>Each event topic defines a subset of the time period covered by this corpus, representing the period to track that event. Participant systems had two options available when working with the corpus: 1. Extract the topic time periods from the TREC KBA 2014 Stream Corpus and process all documents from these time periods. This was the only option available to partatipants during the 2013 track.</p><p>2. Use a pre-filtered version of the TREC KBA 2014 Stream Corpus, denoted TREC-TS-2014F, which only contains documents from the 2014 event topic time periods. TREC-TS-2014F was also subject to pre-filtering such that it focuses on documents that are more likely to contain relevant sentences.</p><p>Each document within the TREC KBA 2014 Stream Corpus contains zero or more sentences (the sentence boundaries are pre-defined) and a timestamp representing when that document was crawled. Participants return a list of sentences extracted from the KBA corpus documents for each event. Each sentence is identified by the combination of a document identifier (which document the sentence came from) and a sentence identifier (the position of the sentence within the document). Additionally, when a sentence is emitted, the participant system also records the time with respect to the underlying document stream of that emission. If the participant system is making immediate binary emit/ignore decisions on a per sentence basis, then this timestamp will correspond to crawl-time of the document. However, some participant systems opted to delay the emission of sentences to collect more information before issuing updates -in these cases the timestamps recorded reflect the additional latency of these systems.</p><p>Participants were allowed to include runs that use information external to the KBA corpus. The use of external data had the following requirements:</p><p>• External data must have existed before the event start time, or • External data must be time-aligned with the KBA corpus and no information after the simulation decision time can be used.</p><p>Similarly, supporting statistical models or auxiliary programs were subject to the same requirements. For example, participants were not to use a statistical model trained on data that existed after the event end time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate runs according to their relevance, coverage, novelty, and latency of the updates.</p><p>• The relevance or precision of the summary with respect to the event topic, i.e. the degree to which the updates within the summary are on-topic and novel. This is measured by the (normalized) Expected Gain metric (nEG(S)). • The coverage of the summary with respect to all of the essential information that could have been retrieved for the event. This is measured by the Comprehensiveness metric (C(S)).</p><p>• The degree to which the information contained within the updates is outdated. This is measured by the Expected Latency metric (E[Latency]).</p><p>We also report the performance of all of the participant runs under a combined measure (that incorporates Expected Gain and Comprehensiveness with Latency included), i.e. the Harmonic Mean of normalized Expected Latency Gain (nEG τ (S)) and Latency Comprehensiveness (C τ (S)), denoted H. This is the official target metric for the 2014 task. Detailed descriptions of metrics and how they are calculated can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Judging</head><p>The evaluation process occurred in two phases: The first phase defined the space of relevant information for the queries. In particular, this involves the creation of a set of 'information nuggets' about each event that represent all of of the essential information that a good summary should contain. This phase also associates each information nugget with a timestamp representing approximately when that information became public knowledge. The second phase generates a matching between updates provided by the participants to the information nuggets. It is this matching that forms the basis for evaluating a system's accuracy and coverage. A detailed description of these phases of judging can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We present an overview of the performance of the participant systems (runs) in Table <ref type="table" coords="5,161.18,242.50,3.87,8.74">3</ref>. The last column in Table <ref type="table" coords="5,286.65,242.50,4.98,8.74">3</ref> reports the H of each participant run and the TREC average. From Table <ref type="table" coords="5,280.43,254.46,3.87,8.74">3</ref>, we observe that the 2APSal run by cunlp was the best performing overall, closely followed by the BJUT and uogTr runs.</p><p>To examine why these runs are the best performing, we next report performance under both Expected Gain and Comprehensiveness separately. Note that we would expect there to be some degree of trade-off between these two metrics. The average performance of participating system runs under (normalized) Expected Gain and Comprehensiveness are reported in Table <ref type="table" coords="5,404.06,326.19,3.87,8.74">3</ref>. From Table <ref type="table" coords="5,469.73,326.19,3.87,8.74">3</ref>, we observe that in terms of (normalized) Expected Gain, the three BJUT runs and the 2APSal run by cunlp produced the most precise summaries, i.e. they returned the least content not matching one or more nuggets (that were not previously covered). Meanwhile, the systems that produced the most Comprehensive summaries (those that matched the most unique information nuggets) were the three uogTr systems, the cunlp 3AP run and the BUPT PRIS Cluster1 approach. Figure <ref type="figure" coords="5,214.78,409.88,4.98,8.74" target="#fig_2">2</ref> illustrates the distribution of all submitted runs in terms of (normalized) Expected Gain and Comprehensiveness. As we can see from Figure <ref type="figure" coords="5,166.46,433.79,3.87,8.74" target="#fig_2">2</ref>, the performance of the submitted runs varies greatly with respect to Expected Gain and Comprehensiveness, indicating that participants applied very different approaches to select sentences for inclusion into the summaries.</p><p>Next, we examine the performance of participating systems in terms of Latency. The fifth column of Table <ref type="table" coords="5,275.74,481.61,4.98,8.74">3</ref> reports the average Expected Latency weight observed for each system. Somewhat contrary to its name, a higher latency weight is better, above one means that on average, the system has found the matched information before it was added to Wikipedia. From Table <ref type="table" coords="5,437.25,517.47,3.87,8.74">3</ref>, we observe that in general, there is not a large correlation between the latency of a system and its combined metric score (H). In fact, the lowest performing run also has the best overall latency. On the other hand, the best performing runs have a good overall, but relatively average Expected Latency. We can compare any two runs to visualize the latency of their individual updates. To illustrate this, Figure <ref type="figure" coords="5,188.47,589.20,4.98,8.74" target="#fig_3">3</ref> shows the times after the event began at which the first 75 updates were issued by the BJUT Q1 and uogTr2A runs, for event topic 13 (2013 Eastern Australia Floods). From Figure <ref type="figure" coords="5,315.72,613.11,3.87,8.74" target="#fig_3">3</ref>, we see that the uogTr2A runs began issuing updates very close to the beginning of the event (+3 hours from the event start), while the first update by the BJUT Q1 run was two days later (+86 hours from the event start). As expected, the overall latency of the uogTr2A run is higher. However, the BJUT Q1 run outperformed the uogTr2A run by a large margin under the combined measure (0.0992 vs 0.0526) for this topic. From this, we can conclude that the evaluation metrics are much less sensitive to latency considerations in comparison to Expected Gain or Comprehensiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In general, the best performing runs submitted to the 2014 track balanced the need for high precision and novelty with topic coverage. Systems that focused on one or other components were less effective overall. From the scale of the results, it appears that attaining high precision is more difficult than achieving recall for this task, and hence it is here that further research is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Metrics</head><p>To evaluate the performance of the summaries produced by participant systems, we define the concept of explicit sub-events or 'nuggets', each with a precise timestamp and text describing the sub-event. An effective summary should cover as many of these nuggets as possible, while minimizing redundancy.</p><p>A sentence update is a timestamped short text string. We generally denote an update as the pair (string, time): u = (u.string, u.t). For example u = ("The hurricane was upgraded to category 4", 1330169580) represents an update describing the hurricane category, now 4, pushed out by system S at UNIX time 1330169580 (i.e. 1330169580 seconds after 0:00 UTC on January 1, 1970). In this year's evaluation, the update string is chosen from the set of segmented sentences in the corpus as defined in the guidelines.  Table <ref type="table" coords="8,162.05,532.04,3.87,8.74">3</ref>: For each run, the number of submitted updates and the number of pooled updates (with std over topics), averaged over topics. Pooling takes into account the number of updates per run, and the confidence stated for each update per run. The number of pooled updates per run includes the automatic matched updates in the average.</p><p>Two updates are semantically comparable using a text similarity measure or a manual annotation process applied to their string components; if two updates u and u refer to the same information (semantically matching), then we write this as u ≈ u , irrespective of their timestamps. Because two systems might deliver the same update string at different times, it is generally not the case that u.string = u .string implies u.t = u .t.</p><p>Given an event, our manual annotation process generates a set of gold standard updates called nuggets, extracted from wikipedia event pages and timestamped according to the revision history of the page. Editorial guidelines recommend that nuggets be a very short sentence, including only a single sub-event, fact, location, date, etc, associated with topic relevance. We refer to the canonical set of updates as N . This manual annotation process is retrospective and subject to error in the precision of the timestamp. As a result we might encounter situations where the timestamp of the nugget is later than the earliest matching update.</p><p>In response to an event's news coverage, a system/run broadcasts a set of timestamped updates generated in the manner described in the Guidelines. We refer to a system's set of updates as S. The set of updates received before time τ is,</p><formula xml:id="formula_0" coords="9,256.60,361.81,220.88,9.65">S τ = {u ∈ S : u.t &lt; τ } (1)</formula><p>Our goal in this evaluation is to measure the precision, recall, timeliness, and novelty of updates provided by a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preliminaries</head><p>Our evaluation metrics are based on the following auxiliary functions.</p><p>• Nugget Relevance. Each nugget n ∈ N has an associated relevance/importance grade,</p><formula xml:id="formula_1" coords="9,287.10,486.06,190.39,8.77">R : N → [0, 1] (2)</formula><p>R(n) measures the importance of the content (information) in the nugget. Nugget importance was provided on a 0-3 scale by assessors (no importance to high importance). For graded relevance, we normalize on an exponential scale, since high importance nuggets are described as "of key importance to the query", whereas low importance nuggets are "of any importance to the query". When binary relevance is needed, everything of any relevance is relevant (0 is the only non-relevant grade). The actual relevance functions used are presented below; n.i denotes the nugget importance as assigned by the assessor.</p><formula xml:id="formula_2" coords="9,196.12,616.68,281.36,53.09">R graded (n) = e n.i e max n ∈N n .i Graded relevance (3) R binary (n) = 1 iff n.i &gt; 0 0 otherwise Binary relevance (4)</formula><p>Note that for graded relevance, returning exactly the nugget set as the system output updates and nothing more ("perfect system"), would usually not result in an expected gain of 1. However, using binary relevance, the perfect system would score an expected gain of 1.</p><p>The relevance can be discounted in time or in size, hence the following discounting functions.</p><p>• Latency Discount. Given a reference timestamp of a matching nugget, t * , a latency penalty function L(t * , t) is a monotonically decreasing function of t -t * . A system may return an update matching Wikipedia information before the Wikipedia information exists; thus we use a function that is smooth and decays on both the positive and negative sides.</p><p>The actual function used is presented below with arguments the nugget Wikipedia time (wiki-edit timestamp) n.t, and the update time u.t as indicated by the system. Current parameters allow the latency discount factor to vary from 0 to 2 (1 means nugget time equal to update time), and flattens at around one day(± 24 hours). Note that as a result, gain and expected gain can be greater than 1.</p><formula xml:id="formula_3" coords="10,176.02,328.79,301.47,22.31">L(n.t, u.t) = 1 - 2 π arctan( u.t -n.t α ) latency-discount<label>(5)</label></formula><p>• Verbosity Normalization. The task definition assumes that a user receives a stream of updates from the system. Consequently, we want to penalize systems for including unreasonably long updates, since these easily lead to significantly higher reading effort. The verbosity can be defined as a string length penalty function, monotonically increasing in the number of words of the update string. We will refer to this normalization function as V(u).</p><p>For the actual verbosity implementation, we approximate the number of extra nuggets worth of information in a given update. This is done by finding all text which did not match a nugget (as defined by the assessors), and dividing the number of words in the text by the average number of words in a nugget for that query.</p><formula xml:id="formula_4" coords="11,201.29,172.11,276.19,52.46">V(u) = 1 + |all words u | -|nuggetmatching words u | AV G n |words n | (7) = 1 + |u| -| ∪ n∈M -1 (u,S) M(n, S)| avg n∈N |n|<label>(8)</label></formula><p>where |u|, |n| are the length (in number of words) of the update u, and nugget n.</p><p>Note that if an update has all its words being part of some match to a nugget, the verbosity is V (u)=1; otherwise V (u)-1 is an approximation of the "extra non-matching words" in terms of equivalent number of nuggets.</p><p>• Update-Nugget Matching. We also define a key earliest matching function between a nugget and an update set,</p><formula xml:id="formula_5" coords="11,251.13,340.62,226.35,10.62">M(n, S) = argmin {u∈S:n≈u} u.t<label>(9)</label></formula><p>or ∅ if there is no matching update for n. M(n, S) should be interpreted as "given n, the earliest matching update in S."</p><p>We also define the set of nuggets for which u is the earliest matching update as,</p><formula xml:id="formula_6" coords="11,237.89,421.98,239.59,10.81">M -1 (u, S) = {n ∈ N : M(n, S) = u}<label>(10)</label></formula><p>Note that an update can be the earliest matching update for more than one nugget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Metrics</head><p>Using the previously defined notion of relevance, latency, verbosity, and matching we can define several measures of interest for Temporal Summarization. Given an update u and a matching nugget n (i.e. u ≈ n), we can define the discounted gain as,</p><formula xml:id="formula_7" coords="11,227.80,561.68,249.68,8.77">g(u, n) = R(n) × discounting factor (11)</formula><p>Given the previously defined discounts, we have the following family of discounted gains,</p><formula xml:id="formula_8" coords="11,171.86,617.22,305.62,24.63">g F (u, n) = R(n) discount-free gain (12) g L (u, n) = R(n) × L(n.t, u.t)</formula><p>latency-discounted gain (13)</p><p>Since an update can be the earliest to match several nuggets (u ≈ n), we define the gain of an update with respect to a system (or participant run) S as the sum of [latency-discounted] relevance of the nuggets for which it is the earliest matching update:</p><formula xml:id="formula_9" coords="12,243.07,163.24,234.41,20.64">G(u, S) = n∈M -1 (u,S) g(u, n)<label>(14)</label></formula><p>where the gain can be either of the discounted gains described earlier. Note that for an appropriate discounting function, G(u, S) ∈ [0, 1], although for the latency-discounted gain, given the imperfect nature of model timestamps,</p><formula xml:id="formula_10" coords="12,133.77,232.18,74.43,9.68">G L (u, S) ∈ [0, 2].</formula><p>One way to evaluate a system is to measure the expected gain for a system update. This is similar to traditional notions of precision in information retrieval evaluation. Over a large population of system updates, we can estimate this measure reliably. The computation of the expected update gain for system S by time τ is the average of the gain per update:</p><formula xml:id="formula_11" coords="12,204.82,311.30,272.66,90.18">nEG(S) = 1 Z|S| u∈S G(u, S) (15) = 1 Z|S| u∈S n∈M -1 (u,S) g(u, n) = 1 Z|S| {n∈N :M(n,S) =∅} g(M(n, S), n) (<label>16</label></formula><formula xml:id="formula_12" coords="12,473.05,380.95,4.43,8.74">)</formula><p>where Z is the maximum obtainable expected gain per topic (similar to DCG normalization). Additionally, we may penalize "verbosity" by normalizing not by the number of system updates, but by the overall verbosity of the system</p><formula xml:id="formula_13" coords="12,180.89,457.72,296.59,27.27">nEG V (S) = 1 u∈S V(u) 1 Z {n∈N :M(n,S) =∅} g(M(n, S), n)<label>(17)</label></formula><p>Our definition of g is such that it:</p><p>• does not penalize for large a update matching several nuggets, as opposed to a few small updates each matching a nugget, due to verbosity weighting,</p><p>• penalizes for late updates (against matched nugget reference timestamp), and</p><p>• penalizes "verbosity" of updates text not matching any nuggets.</p><p>Furthermore, we have that G(u, S τ ) ∈ [0, 1] if all update timestamps are at or after matching model timestamps. Over a set of events, the mean expected gain is defined as,</p><formula xml:id="formula_14" coords="12,251.94,644.29,221.11,26.80">MEG = 1 |E| ∈E EG(S ) (<label>18</label></formula><formula xml:id="formula_15" coords="12,473.05,651.03,4.43,8.74">)</formula><p>where E is the set of evaluation events and S is the system submission for event .</p><p>Because a user interest may be concentrated immediately after an event and because a system's performance (in terms of gain) may be dependent on the time after an event, we will also consider a time-sensitive expected gain for the first τ seconds,</p><formula xml:id="formula_16" coords="13,263.31,209.09,214.16,9.68">EG τ (S) = EG(S τ )<label>(19)</label></formula><p>with MEG τ defined similarly.</p><p>In addition to good expected gain, we are interested in a system providing a comprehensive set of updates. That is, we would like the system to cover as many nuggets as possible. This is similar to traditional notions of recall in information retrieval evaluation. Given a set of system updates, S, we define the comprehensiveness (and latency-comprehensiveness) of the system as:</p><formula xml:id="formula_17" coords="13,195.43,319.09,282.05,91.39">C(S) = 1 n∈N R(n) {n∈N :M(n,S) =∅} g(M(n, S), n) (20) = 1 n∈N R(n) u∈S n∈M -1 (u,S) g(u, n) = 1 n∈N R(n) u∈S G(u, S)<label>(21)</label></formula><p>We also define a time-sensitive notion of comprehensiveness,</p><formula xml:id="formula_18" coords="13,271.57,442.56,205.91,9.68">C τ (S) = C(S τ )<label>(22)</label></formula><p>with an aggregated measure defined as, which measures how quickly a system captures nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Judging B.1 Gold Nugget Extraction</head><p>In this first phase, assessors were asked to read all edits of the Wikipedia article for each query, manually extracting text perceived as relevant and novel for that edit. Additionally, assessors assigned an importance grade to every text fragment, or nugget, as well as noted any dependencies in the information. An example portion of the extraction interface can be seen in Figure <ref type="figure" coords="13,420.77,642.03,3.87,8.74" target="#fig_6">4</ref>.</p><p>In order to simplify later matching, assessors were told to extract nuggets such that they were atomic pieces of information relevant to the query. However, knowing that information can be highly contextual, we allow for the notion of dependencies between nuggets: a nugget may be relevant to a query if and only if another nugget is also present. For evaluation purposes, a nugget was considered unmatched if it had unmatched nuggets on which it depended.</p><p>Additionally, we provided a method for assessors to track updates to pieces of information. This was primarily used to allow them to collate their work and reduce redundancy, but it was also used in the matching phase to help the assessor find the closest piece of information to a match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Update-Nugget Matching</head><p>Once submissions were received, we performed a variant of depth-pooling in order to sample updates for evaluation. We sampled the top 60 updates per query and run as sorted by the provided confidence scores (highest first). Additionally, we performed near-duplicate detection among update text to increase the covered set. This resulted in sampled update counts as per Table <ref type="table" coords="14,446.06,497.60,3.87,8.74" target="#tab_1">2</ref>. One note here is that not all runs contained 60 updates per query; for the run-query pairs with less than 60 updates, all updates were sampled.</p><p>The sampled updates were presented in an interface similar to the one for extraction. Assessors examined and matched updates to nuggets by selecting portions of updates which matched a given nugget, as nuggets are atomic but updates are not. An assessor was allowed to break a nugget into two or more new nuggets to improve atomicity if desired. Note that a nugget may match multiple updates, and an update may match multiple nuggets. An example view of the matching interface can be seen in Figure <ref type="figure" coords="14,342.68,605.20,3.87,8.74" target="#fig_7">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Automatic matches for unpooled updates</head><p>The participant updates that did not make it to the pool for manual matching form the set of "unpooled updates". We performed an automatic exact match between these unpooled updates and the known relevant pooled updates (manually matched); the updates that matched a known relevant pooled update are also considered relevant and included as matching nuggets for evaluation purposes. All updates, both pooled and unpooled, that do not match any nugget (manual) or other relevant update (automatic), are considered nonrelevant for evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,133.77,251.05,343.71,8.74;2,133.77,263.01,26.65,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example topic description for the topic '2012 Buenos Aires Rail Disaster'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,133.77,314.80,343.71,8.74;6,133.77,326.75,36.69,8.74;6,161.62,124.80,288.00,164.92"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Participant run plot of (normalized) Expected Gain vs. Comprehensiveness.</figDesc><graphic coords="6,161.62,124.80,288.00,164.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,133.77,307.41,343.71,8.74;7,133.77,319.37,282.85,8.74;7,161.62,126.23,288.01,156.11"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of update issue times for the BJUT Q1 run and the uogTr2A run for event topic 13 (2013 Eastern Australia Floods).</figDesc><graphic coords="7,161.62,126.23,288.01,156.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,133.77,276.04,343.72,8.74;14,133.77,287.99,71.18,8.74;14,133.77,124.80,343.70,136.12"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Extraction interface used by assessors to extract nuggets from Wikipedia edits.</figDesc><graphic coords="14,133.77,124.80,343.70,136.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,135.10,248.41,341.06,8.74;15,133.77,124.80,343.71,108.49"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Matching interface used by assessors to match updates and nuggets.</figDesc><graphic coords="15,133.77,124.80,343.71,108.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,133.77,127.74,346.73,227.28"><head>Table 1 :</head><label>1</label><figDesc>TS2014 topics, with number of gold nuggets extracted by assessors, and number of participant updates pooled for matching.</figDesc><table coords="4,139.75,127.74,340.75,183.60"><row><cell>topic</cell><cell>type</cell><cell>#gold</cell><cell>#pooled</cell></row><row><cell></cell><cell></cell><cell>nuggets</cell><cell>updates</cell></row><row><cell>Costa Concordia disaster and recovery</cell><cell>accident</cell><cell>226</cell><cell>1008</cell></row><row><cell>Early 2012 European cold wave</cell><cell>natural disaster</cell><cell>73</cell><cell>654</cell></row><row><cell>2013 Eastern Australia Floods</cell><cell>storm</cell><cell>68</cell><cell>570</cell></row><row><cell>Boston Marathon bombings</cell><cell>shooting</cell><cell>76</cell><cell>984</cell></row><row><cell>Port Said Stadium riot</cell><cell>protest</cell><cell>47</cell><cell>813</cell></row><row><cell>2012 Afghanistan Quran burning protests</cell><cell>protest</cell><cell>75</cell><cell>759</cell></row><row><cell>In Amenas hostage crisis</cell><cell>hostage</cell><cell>48</cell><cell>768</cell></row><row><cell>2011-13 Russian protests</cell><cell>protest</cell><cell>89</cell><cell>916</cell></row><row><cell>2012 Romanian protests</cell><cell>protest</cell><cell>100</cell><cell>758</cell></row><row><cell>2012-13 Egyptian protests</cell><cell>protest</cell><cell>35</cell><cell>612</cell></row><row><cell>Chelyabinsk meteor</cell><cell>natural disaster</cell><cell>126</cell><cell>919</cell></row><row><cell>2013 Bulgarian protests against the Borisov</cell><cell>protest</cell><cell>117</cell><cell>608</cell></row><row><cell>2013 Shahbag protests</cell><cell>protest</cell><cell>138</cell><cell>723</cell></row><row><cell>February 2013 nor'easter</cell><cell>storm</cell><cell>105</cell><cell>951</cell></row><row><cell>Christopher Dorner shootings and manhunt</cell><cell>shooting</cell><cell>88</cell><cell>701</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,139.75,341.41,340.89,329.91"><head>Table 2 :</head><label>2</label><figDesc>Performance metrics per participant run, averaged over topics.</figDesc><table coords="7,139.75,341.41,340.89,308.05"><row><cell>TeamID</cell><cell>RunID</cell><cell>nEG(S)</cell><cell>C(S)</cell><cell>E[Latency]</cell><cell>H</cell></row><row><cell>cunlp</cell><cell>2APSal</cell><cell>0.0631</cell><cell>0.3220</cell><cell>1.2068</cell><cell>0.1162</cell></row><row><cell>BJUT</cell><cell>Q1</cell><cell>0.0657</cell><cell>0.4088</cell><cell>1.1491</cell><cell>0.1110</cell></row><row><cell>BJUT</cell><cell>Q2</cell><cell>0.0632</cell><cell>0.3979</cell><cell>1.1669</cell><cell>0.1091</cell></row><row><cell>BJUT</cell><cell>Q0</cell><cell>0.0632</cell><cell>0.3979</cell><cell>1.1669</cell><cell>0.1091</cell></row><row><cell>uogTr</cell><cell>uogTr2A</cell><cell>0.0467</cell><cell>0.4453</cell><cell>1.2322</cell><cell>0.0986</cell></row><row><cell>uogTr</cell><cell>uogTr4AC</cell><cell>0.0347</cell><cell>0.4539</cell><cell>1.2751</cell><cell>0.0793</cell></row><row><cell>uogTr</cell><cell>uogTr4ARas</cell><cell>0.0387</cell><cell>0.3691</cell><cell>1.2328</cell><cell>0.0772</cell></row><row><cell>IRIT</cell><cell>KW30H5NW3600</cell><cell>0.0383</cell><cell>0.3521</cell><cell>1.2221</cell><cell>0.0723</cell></row><row><cell>IRIT</cell><cell>KW30H5NW300</cell><cell>0.0378</cell><cell>0.3538</cell><cell>1.2208</cell><cell>0.0714</cell></row><row><cell>uogTr</cell><cell>uogTr4A</cell><cell>0.0281</cell><cell>0.4733</cell><cell>1.2522</cell><cell>0.0677</cell></row><row><cell>average</cell><cell></cell><cell>0.0327</cell><cell>0.3615</cell><cell>1.2943</cell><cell>0.0620</cell></row><row><cell>IRIT</cell><cell>KW80H5NW3600</cell><cell>0.0289</cell><cell>0.3764</cell><cell>1.2191</cell><cell>0.0604</cell></row><row><cell>IRIT</cell><cell>KW30H10NW300</cell><cell>0.0298</cell><cell>0.3780</cell><cell>1.2617</cell><cell>0.0602</cell></row><row><cell>cunlp</cell><cell>1APSalRed</cell><cell>0.0325</cell><cell>0.3058</cell><cell>1.1507</cell><cell>0.0602</cell></row><row><cell>IRIT</cell><cell>KW80H5NW300</cell><cell>0.0285</cell><cell>0.3806</cell><cell>1.2164</cell><cell>0.0596</cell></row><row><cell>ICTNET</cell><cell>run3</cell><cell>0.0531</cell><cell>0.1081</cell><cell>0.7004</cell><cell>0.0530</cell></row><row><cell cols="2">BUPT PRIS Cluster4</cell><cell>0.0155</cell><cell>0.2692</cell><cell>1.9140</cell><cell>0.0508</cell></row><row><cell>IRIT</cell><cell>KW80H10NW300</cell><cell>0.0225</cell><cell>0.4012</cell><cell>1.2621</cell><cell>0.0503</cell></row><row><cell cols="2">BUPT PRIS Cluster3</cell><cell>0.0115</cell><cell>0.3380</cell><cell>1.9165</cell><cell>0.0407</cell></row><row><cell>cunlp</cell><cell>3AP</cell><cell>0.0174</cell><cell>0.4265</cell><cell>1.3689</cell><cell>0.0403</cell></row><row><cell>ICTNET</cell><cell>run2</cell><cell>0.0418</cell><cell>0.0934</cell><cell>0.6266</cell><cell>0.0311</cell></row><row><cell cols="2">BUPT PRIS Cluster2</cell><cell>0.0059</cell><cell>0.3728</cell><cell>1.9170</cell><cell>0.0222</cell></row><row><cell>ICTNET</cell><cell>run4</cell><cell>0.0079</cell><cell>0.4070</cell><cell>1.2364</cell><cell>0.0178</cell></row><row><cell>ICTNET</cell><cell>run1</cell><cell>0.0070</cell><cell>0.4090</cell><cell>1.2314</cell><cell>0.0160</cell></row><row><cell cols="2">BUPT PRIS Cluster1</cell><cell>0.0033</cell><cell>0.4369</cell><cell>1.9175</cell><cell>0.0127</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
