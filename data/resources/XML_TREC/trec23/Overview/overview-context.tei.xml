<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.28,110.91,401.45,15.12">Overview of the TREC 2014 Contextual Suggestion Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.58,148.71,86.97,10.48"><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.51,148.71,105.90,10.48"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.42,148.71,63.95,10.48"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.10,182.46,68.18,10.48;1,228.91,196.40,36.58,10.48"><forename type="first">Paul</forename><forename type="middle">Thomas</forename><surname>Csiro</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Ellen Voorhes NIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.28,110.91,401.45,15.12">Overview of the TREC 2014 Contextual Suggestion Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A76643196D76816CE25D6707A1D15BA7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Summary for Previous Participants</head><p>For participants familiar with the 2013 Contextual Suggestion Track we have provided a list of the main changes to this year's track:</p><p>• Assessors were recruited only from a crowdsourcing service (Mechanical Turk) and not from any student bodies. • Only CSV formatted files were available for profiles, contexts, and suggestions.</p><p>• Two seed cities were used instead of one (Chicago, IL and Santa Fe, NM) and the target cities were also changed. • The number of ratings provided in profiles was changed from 50 to 70 or 100 (depending on the profile).</p><p>• 31 runs were submitted from 17 groups, 6 of these were ClueWeb12 runs and 25 were open web runs.</p><p>If you are already familiar with this track you can skip to Section 5 which gives an overview of the approaches participants used and Section 6 which contains the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Task Description</head><p>The contextual suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. For example, imagine an information retrieval researcher with a November evening to spend in Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse, dinner at the Flaming Pit, or even a trip into Washington on the metro to see the National Mall. The primary goal of this track is to develop evaluation methodologies for such systems.</p><p>This track ran for the third time as part of TREC 2014 after a positive response in previous years. This year participants were again given, as input, a set of profiles and set of geographical contexts. The task was to take these profiles and contexts and to produce a list of up to 50 ranked suggestions for each profile-context pair. Participants could choose to gather suggestions from either the open web or the ClueWeb12 dataset.</p><p>Each profile corresponds to a single assessor and indicates that assessor's preference with respect to each sample suggestion. For example, if one sample suggestion is a beer at the Dogfish Head Alehouse, the profile might indicate a negative preference to that suggestion. Each suggestion includes a title, short description, and an associated URL. Each context corresponds to a particular location at the granularity of a city. For example, a context might be Gaithersburg, Maryland.</p><p>As with previous years each groups was allowed to submit up to two runs. A total of 17 groups submitting 31 runs participated in the track this year. 6 of these runs comprised suggestions from the ClueWeb12 dataset, the other 25 runs comprised suggestions from the open web.</p><p>Profiles and contexts were distributed to participants in CSV formatted files. For this track we generated 299 profiles and 50 contexts. Below, we describe how these were generated. An experimental run consists of a single suggestion (CSV) file generated automatically from the profile and context files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Profiles</head><p>Profiles indicate an assessor's preferences to a list of 70-100 example suggestions within Chicago, IL and Santa Fe, NM. These profiles are built by conducting a survey advertised to crowdsourcing workers. These workers will be the same ones we use as assessors during evaluation.</p><p>Profiles are split into two files: examples2014.csv, profiles2014-70.csv, and profiles2014-100.csv. exam-ples2014.csv contains a list of 100 suggestions which each consist of an id, a title, a description, and a url. Below are two suggestions from the file: The profiles contain ratings given by assessors for suggestions within examples2014.csv. Some assessors only gave 70 suggestions (all profiles with 70 ratings are the same subset of 70) and some gave all 100. Profiles with 70 ratings are in profiles2014-70.csv and profiles with 100 ratings are in profiles2014-100.csv. Below are a few lines from profiles-70.csv: 8 4 9 , 1 0 1 , 3 , 4 8 4 9 , 1 0 2 , 2 , 3 8 4 9 , 1 0 5 , 2 , 2 . . . 8 4 9 , 1 8 4 , 1 , 1 8 4 9 , 1 8 5 , 2 , 3 8 4 9 , 1 8 7 , 3 , 3</p><p>Listing 1: An excerpt from profiles2014-70.csv.</p><p>The first line means that the assessor with id 849 gave example suggestion number 101 a description rating of 3 and a website rating of 4 (on a scale from 0 to 4, see section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Generating Example Suggestions</head><p>First we need to generate example suggestion which will rated by assessors in a survey. A suggestion consists of a title, short description, and a website URL. These 100 example suggestions were all from Chicago, IL and Santa Fe, NM. Example suggestions were selected from a commercial online listing of points-of-interest; example suggestions were chosen so that there was diversity in the types of attractions in our set.</p><p>Figure <ref type="figure" coords="3,229.27,232.65,3.87,8.74">1</ref>: Screenshots of survey seen by assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Gathering Attraction Preferences</head><p>Profiles distributed to participants indicated assessor's preference towards the example suggestions. In order to form the profiles, workers were recruited from Mechanical Turk and were asked to complete an online survey. In the survey sample suggestions were presented to assessors in a random order. Assessors were asked to give two 5-point ratings for each attraction, one for how interesting the attraction seemed to the assessor based on its description and one for how interesting the attraction seemed to the assessor based on its website. The survey interface, can be seen in figure <ref type="figure" coords="3,313.64,350.27,3.87,8.74">1</ref>. In total 299 crowdsourced assessors responded to the survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contexts</head><p>Contexts describe which city a user is currently located in. There were 50 cities chosen randomly from the list of primary cities in metropolitan areas in the United States (which are not part of a larger metropolitan area) excluding cities used as contexts last year and the seed cities. The list of metropolitan areas was taken from Wikipedia<ref type="foot" coords="3,138.97,454.75,3.97,6.12" target="#foot_0">1</ref> .</p><p>Contexts are distributed to participants in the file contexts2014.csv.</p><p>. . . 1 1 7 , Cumberland ,MD, 3 9 . 6 5 2 8 7 , -7 8 . 7 6 2 5 2 1 1 8 , S h r e v e p o r t , LA, 3 2 . 5 2 5 1 5 , -9 3 . 7 5 0 1 8 1 1 9 , Los Angeles ,CA, 3 4 . 0 5 2 2 3 , -1 1 8 . 2 4 3 6 8 1 2 0 , Portland ,OR, 4 5 . 5 2 3 4 5 , -1 2 2 . 6 7 6 2 1 1 2 1 , Grand Rapids , MI, 4 2 . 9 6 3 3 6 , -8 5 . 6 6 8 0 9 . . . Listing 2: An excerpt from contexts2014.csv.</p><p>Here the first line means that context id 117 represents Cumberland (city), MD (state) with a latitude of 39.65287 and a longitude of -78.76252. For contexts the latitude and longitude are provided as a convenience and are synonymous with the city. They are not meant to represent the exact position of the user. Contexts represent locations at the granularity of a city-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Collections</head><p>Participants were able to gather suggestions from either the open web, ClueWeb12<ref type="foot" coords="4,442.61,96.27,3.97,6.12" target="#foot_1">2</ref> , ClueWeb12 B13, or ClueWeb12 CS. ClueWeb12 and ClueWeb12 B13 are datasets prepared by Jamie Callan's research group at CMU. ClueWeb12 CS was prepared for the track by the track organizers.</p><p>The ClueWeb12 CS subcollection was created by issuing a variety of queries for each context location against a commercial search entire. Returned results that had URLs which matched documents in ClueWeb12 were grouped by context and included in the subcollection. URLs were normalized before they were matched, for example forward-slashes were removed from the end of URLs. In total the subcollection contains 30 144 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Submitted Suggestions</head><p>Each submitted run consists of up to 50 ranked suggestions for each profile-context pair. Similarly to the example suggestions, profiles consist of a title, description, and URL that correspond to an attraction. The URL can be substituted with a ClueWeb12 DocID. Suggestions also contain a group id, run id, profile id, context id, and rank.</p><p>In order to generate suggestions participants were allowed to use whatever resources they wished to use, for example review websites such as Yelp. The goal was that each suggestion should be tailored to the profile and located within the context that was being targeted. Ideally, the description of the suggestion would be tailored to reflect the preferences of the user.</p><p>Here are two of the suggestions we received: </p><formula xml:id="formula_0" coords="4,85.81,374.73,126.55,32.68">• Group ID RAMA Run ID RAMARUN2 Profile ID</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Judging</head><p>Judging was split up into two tasks. Suggestions were judged with respect to their profile relevance by crowdsourced assessors and with respect to the contextual relevance by accessors at NIST as well as crowdsourced assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Profile Relevance</head><p>In order to judge the relevance of suggestions with respect to profiles a second survey was conducted, which was similar to the first one. Assessors were invited back to give ratings for the attraction descriptions and websites of the top 5 ranked suggestions for each run for their profile and one, two, or three randomly chosen contexts.</p><p>The judgements given were one of:</p><p>- Here the last line means the the assessor (843) was strongly interested (4) in the point-of-interest based on both the description provided by run BJUTa from context 109 and the website. The last two numbers mean that the assessor took 19 sec. to make the judgement based on the description and 58 sec. to make the judgement based on the website. A -1 means that no timing data is available. This timing data is not used as part of the scoring calculations for runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geographical Relevance</head><p>In order to judge the geographical relevance of suggestions assessors were asked, during the survey, whether the attraction was in the city it was submitted for or not. Additionally accessors at NIST where also asked to make the same judgement for attractions.</p><p>-2,- Here the first line means that for context 102 the website http://www.adixiontours.com is not geographically appropriate (0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Measures</head><p>Three measures are used to rank runs. Our main measure, Precision at Rank 5 (P@5), is supplemented by Mean Reciprocal Rank (MRR) and a modified version of Time-Biased Gain (TBG) <ref type="bibr" coords="6,428.65,250.43,14.37,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">P@5</head><p>An attraction is considered relevant for P@5 if it has a geographical relevance of 1 or 2 and if the user reported that both the description and document were found to be interesting (3) or strongly interesting (4).</p><p>A P@5 score for a particular topic (a profile-context pair) is determined by how many of the top 5 ranked attractions are relevant, divided by 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MRR</head><p>For MRR, an attraction is considered relevant using the same criteria used for P@5. A MRR score is calculated as 1 k , where k is the rank of the first relevant attraction found. If there are no relevant attractions in the first 5 attractions in the ranked list a score of 0 is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TBG</head><p>In an effort to develop a metric better suited to evaluating this task the organizers of this track developed a metric based on TBG metric introduced by Smucker and Clarke <ref type="bibr" coords="6,340.02,492.62,12.46,8.74" target="#b1">[2]</ref>. The modified version of TBG is calculated by the equation described by Dean-Hall, et al. <ref type="bibr" coords="6,272.23,504.58,10.20,8.74" target="#b0">[1]</ref>:</p><formula xml:id="formula_1" coords="6,232.19,530.54,147.12,30.55">5 k=1 D(T (k))A(k)(1 -Θ) k-1 j=1 Z(j)</formula><p>• D is a decay function.</p><p>• T(k) is how long it took the user to reach rank k, calculated using the following two rules:</p><p>-The user reads every description which takes time T desc .</p><p>-If the description judgement is 2 or above then the user reads the document which takes time T doc .</p><p>• A(k) is 1 if the user gives a judgement of 2 or above to the description and 3 or above to the document, otherwise it is 0. • Z(k) is 1 if the user gives a judgement of 1 or below to either the description or the document, otherwise it is 0.</p><p>Note that, for this metric, the user always gives a rating of 0 to the document if the document has a geographical rating of 0. The four parameters for this metric are taken from Dean-Hall et al. <ref type="bibr" coords="6,485.64,702.37,9.96,8.74" target="#b0">[1]</ref>: Θ = 0.5, T desc = 7.45s, and T doc = 8.49s, and the half-life for the decay function H = 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participant Approaches</head><p>17 groups participated in the Contextual Suggestion track this year. An overview of the approaches used by groups that used the open web as their main datasource or ClueWeb12 as their main datasource is given in this section. More details about the approaches used by these teams are available in the individual team TREC reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Open Web Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">BJUT</head><p>This group used a combination of venue categories and venue ranking to make suggestions. Using the profile data they determined the probability the user would like the venue based on which of five categories it was in (attraction, activities, restaurant, shopping, and nightlife). The number of venues in the top 50 suggestions from each category is determined by this probability. These 50 suggestions are then ordered by either rank-only (BJUTa) or by both rank and user preferences (BJUTb).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">BUW</head><p>This group used a ranking of venues that came from a commercial web service. Then then compared the difference between using a description generated from a different commercial web service (webis 1) and that same description with a sentence from an averagely ranked review prepended to it (webis 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">eindhoven</head><p>This group used learning to rank techniques in order to make suggestions using two algorithms, RankNet (tueNet) and Random Forest (tueRforest). The set of attractions in the profile was used as training data. They used distance, scores from commercial web services, categories, description keywords and review keywords as features for their algorithm. Additionally they considered the impact of distance between training and testing venues and which commercial web services provided the most useful data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Group.Xu</head><p>This team based their suggestions heavily on the venue's category. They used category information returned by a commercial web service. When multiple categories were returned they picked a core category using idf based heuristics. Here the core category is the one that appears in fewer venues. In addition to determining which categories each user liked they also determined which categories were more appropriate depending upon the population of the city suggestions were being made for. This team only had one run (dixlticmu).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">ICTNET</head><p>This team compared generating feature vectors for users manually (cat) to generating them using LDA topic modeling (lda). In the first approach they manually annotate profiles and score candidate suggestions based on how many of these annotation appear in the venue information. In the second approach they use cosine similarity between the profile and candidate suggestion along with the ratings to score candidate suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6">RAMA</head><p>This team used three components to make up the scores given to candidate suggestions. They used a general user interest score based solely on the categories the user appeared to be interested in. They used a specific user interest score based on nouns extracted from venue titles and descriptions. Finally, they used a context score based on the user's distance from the venue, number of reviews, and average rating of the venue. These three scores were linearly combined. In both runs the context score was given a low weight, in one run (RAMARUN2) the general interest score was given a high weight and in the other run (RUN1) the specific interest score was given a high weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.7">udel</head><p>This team generated terms that users were interested in based upon their profiles. A commercial web service is then queried with these terms, it returns venues which are incorporated into the final results. In contrast to a similar technique used by the group last year, this year the group ordered terms based on the number of likes were associated with each term. The run, run DwD, didn't use this term prioritization whereas the second run, run FDWD did.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.8">udel fang</head><p>This team uses positive and negative venue reviews to build positive and negative user models. Various representation of these models are experiments with: using full text, using unique terms, using only frequent terms (both unique and not unique), using only nouns, and using summaries generated using the Opinosis algorithm. Last year this team used linear interpolation of similarity scores between both positive and negative profiles and candidate reviews. This year they compared the approach with a learning to rank approach (using LambdaMART). The UDInfoCS2014 1 run uses this learning to rank approach and UDInfoCS2014 2 uses linear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.9">uogTr</head><p>This team used three different approaches to making suggestions. The first approach (uogTrCsLtr) used learning to rank (LambdaMART) to rerank venues that were already personalized using a technique similar to the one that this team used last year. This algorithm was passed 64 features including category information, city features, user features, and venue features. The second approach (uogTrBunSum) bundled venues together using venue popularity and similarity to the profiles as features. In order to ensure diversity only the most central venues in each bundle were suggested. The final approach was not submitted as a run and is similar to the approach this team used last year (which the learning to rank approach builds upon). It serves as a baseline for this team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.10">waterloo clarke</head><p>This team clustered all the venues in a hierarchical fashion, so that at the top of the tree are the most general clusters of venues and lower in the tree are more specific clusters of venues. Terms from venue webpages and descriptions and normalized with K-L divergence and used as features for the clustering. In addition categories are also used as features. The clusters are then ordered by how many liked example attraction exist in them and attractions are picked from each cluster one at a time. The first run, waterlooA, used only positive ratings in the clustering ranking algorithm, and the second run, waterlooB, used both positive and negative ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ClueWeb12 Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">CWI</head><p>This groups used standard vector space model techniques to compare the similarity between candidate venues and the user's profile. This similarity was used to rank the candidate venues being suggested. This group compared different techniques for generating the list of candidate suggestions being ranked using this technique. The first technique used to generate a set of candidate suggestion was the GeoFiltered technique where only venues that mentioned City, State (in that format) were considered (CWI CW12 Full). The second technique, TouristFiltered, used a list of hand-picked domains that were geared towards tourist information and included all venues that were part of documents in those domains or documents that were linked to from documents in those domains (CWI CW12.MapWeb). The third technique, that was not submitted as a run, used venues returned commercial web services to generate a set of candidate suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">TJU CS IR</head><p>This team gathered attractions from Wikitravel and created vector representations of all the venues based on their titles and descriptions. In order to generate user profiles the ratings users gave for the example attractions along with the created vectors that represent each sample attractions are combined and passed to the Softmax algorithm. This outputs a user model which is then used to rank all candidate suggestions to porvide the final ranking. The output of this technique (RunA) is compared with using KNN instead of the Softmax algorithm (RunB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">UAmsterdam</head><p>This team scores venues based on three components: the prior probability of being relevant which is based on the PageRank algorithm (ensuring high quality pages are suggested), the probability of the suggestion being in the context, and the probability of the suggestion being in the user profile. Two approaches are used to develop the languages models to estimate these probabilities. The first approach (Model0) uses the full text of the documents in the index. The second approach (Model1) limits the terms in the documents to only terms that appear in anchor text linking to the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" coords="9,97.59,517.15,4.98,8.74" target="#tab_4">1</ref> lists the scores for all open web runs for all three metrics and table 2 lists the scores for all ClueWeb12 runs. Both of these tables are sorted by their P@5 rankings (our main metric). We do not compare open web and ClueWeb12 runs against each other as part of this track. Figure <ref type="figure" coords="9,378.51,541.06,4.98,8.74" target="#fig_1">2</ref> compares the three metrics against each other for all runs, note that there is a high amount of agreement between the three metrics. Also note that the best two performing runs are ranked the same regardless of the metric used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We are currently considering continuing this track as part of TREC 2015. The basic task will be similar to the one this year. However there are areas we will focus on improving next year.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,96.29,207.89,8.65,8.54;11,126.73,207.89,8.65,8.54;11,157.12,207.89,8.65,8.54;11,187.44,207.89,8.65,8.54;11,217.75,207.89,8.65,8.54;11,248.21,207.89,8.65,8.54;11,278.51,207.89,8.65,8.54;11,176.94,215.87,29.28,8.54;11,90.14,203.08,8.65,8.54;11,90.26,182.83,8.65,8.54;11,90.38,162.58,8.65,8.54;11,90.49,142.34,8.65,8.54;11,90.18,122.09,8.65,8.54;11,90.30,101.84,8.65,8.54;11,90.20,81.59,8.65,8.54;11,79.83,149.96,8.54,11.28;11,79.83,132.86,8.54,15.37;11,136.05,235.15,104.77,7.86;11,333.63,207.89,8.65,8.54;11,364.08,207.89,8.65,8.54;11,394.47,207.89,8.65,8.54;11,424.79,207.89,8.65,8.54;11,455.10,207.89,8.65,8.54;11,485.55,207.89,8.65,8.54;11,515.86,207.89,8.65,8.54;11,414.29,215.87,29.28,8.54;11,327.63,203.08,8.65,8.54;11,327.67,185.72,8.65,8.54;11,327.56,168.37,8.65,8.54;11,327.43,151.01,8.65,8.54;11,327.60,133.66,8.65,8.54;11,327.47,116.30,8.65,8.54;11,327.59,98.95,8.65,8.54;11,327.49,81.59,8.65,8.54;11,317.13,149.49,8.54,12.26;11,317.13,132.40,8.54,15.37;11,372.34,235.15,106.89,7.86;11,213.85,382.58,8.65,8.54;11,244.29,382.58,8.65,8.54;11,274.72,382.58,8.65,8.54;11,305.15,382.58,8.65,8.54;11,335.36,382.58,8.65,8.54;11,365.79,382.58,8.65,8.54;11,396.11,382.58,8.65,8.54;11,294.99,390.56,28.37,8.54;11,207.85,377.77,8.65,8.54;11,207.89,360.41,8.65,8.54;11,207.78,343.06,8.65,8.54;11,207.65,325.70,8.65,8.54;11,207.82,308.35,8.65,8.54;11,207.69,290.99,8.65,8.54;11,207.81,273.64,8.65,8.54;11,207.71,256.28,8.65,8.54;11,197.35,324.18,8.54,12.26;11,197.35,307.09,8.54,15.37;11,251.89,409.84,108.23,7.86"><head></head><label></label><figDesc>MRR vs TBG τ = 0.84</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,185.43,430.78,241.14,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparisons between P@5, MRR, and TBG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,85.81,398.67,455.33,231.09"><head></head><label></label><figDesc>Title Portland Art Museum Description Portland Art Museum is a museum. HERE ARE THE DESCRIPTIONS FROM ITS WEB SITE:The portland art museum is the seventh oldest museum in the united states and the oldest in the pacific northwest. HERE ARE REVIEWS FROM OTHER PEOPLE:The layout is excellent with certain special exhibits sectioned off to themselves. Each floor has just the right size exhibit space, which is perfect to appreciate the art presented. URL/Doc ID http://portlandartmuseum.org/</figDesc><table coords="4,85.81,489.20,141.62,56.59"><row><cell>•</cell><cell>Group ID udel fang</cell></row><row><cell></cell><cell>Run ID UDInfoCS2014 2</cell></row><row><cell></cell><cell>Profile ID 843</cell></row><row><cell></cell><cell>Context ID 120</cell></row><row><cell></cell><cell>Rank 1</cell></row></table><note coords="4,165.73,398.67,14.94,8.74;4,110.72,410.59,77.13,8.77;4,110.72,422.55,36.54,8.77;4,110.72,434.50,157.15,8.77;4,110.72,446.46,430.42,8.77;4,110.72,458.44,348.22,8.74;4,110.72,470.37,392.43,9.05"><p>843 Context ID 118 Rank 1 Title Shreveport Railroad Museum Description The Shreveport Railroad Museum is on the grounds of the Shreveport Water Works Museum just outside of the Central Business District. Run and staffed by the... URL/Doc ID http://www.yelp.com/biz/shreveport-railroad-museum-shreveport</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,70.87,643.65,470.27,44.60"><head>Table 1 :</head><label>1</label><figDesc>Firstly, ClueWeb12 provides a reusable test collection however most groups use the open web. Providing a reusable test collection that is more appropriate for this task and that participants are willing to use is important. Secondly, we are considering moving to suggestion and evaluation of itineraries rather than just individual points-of-interest. P@5, TBG, and MRR rankings for all open web runs.</figDesc><table coords="10,78.32,121.45,455.37,528.16"><row><cell>Run</cell><cell cols="4">P@5 Rank P@5 Score TBG Rank</cell><cell cols="2">TBG Score MRR Rank</cell><cell>MRR Score</cell></row><row><cell>UDInfoCS2014 2</cell><cell>1</cell><cell></cell><cell cols="2">0.5585 1 (-)</cell><cell cols="2">2.7021 1 (-)</cell><cell>0.7482</cell></row><row><cell>RAMARUN2</cell><cell>2</cell><cell></cell><cell cols="2">0.5017 2 (-)</cell><cell cols="2">2.3718 2 (-)</cell><cell>0.6846</cell></row><row><cell>BJUTa</cell><cell>3</cell><cell></cell><cell>0.5010</cell><cell>4 (Down 1)</cell><cell>2.2209</cell><cell>4 (Down 1)</cell><cell>0.6677</cell></row><row><cell>BJUTb</cell><cell>4</cell><cell></cell><cell>0.4983</cell><cell>5 (Down 1)</cell><cell>2.1949</cell><cell>6 (Down 2)</cell><cell>0.6626</cell></row><row><cell>uogTrBunSumF</cell><cell>5</cell><cell></cell><cell>0.4943</cell><cell>7 (Down 2)</cell><cell>2.1526</cell><cell>3 (Up 2)</cell><cell>0.6704</cell></row><row><cell>RUN1</cell><cell>6</cell><cell></cell><cell>0.4930</cell><cell>3 (Up 3)</cell><cell>2.2866</cell><cell>5 (Up 1)</cell><cell>0.6646</cell></row><row><cell>webis 1</cell><cell>7</cell><cell></cell><cell>0.4823</cell><cell>6 (Up 1)</cell><cell cols="2">2.1700 7 (-)</cell><cell>0.6479</cell></row><row><cell>simpleScoreImp</cell><cell>8</cell><cell></cell><cell>0.4602</cell><cell>10 (Down 2)</cell><cell cols="2">1.9795 8 (-)</cell><cell>0.6408</cell></row><row><cell>webis 2</cell><cell>9</cell><cell></cell><cell>0.4569</cell><cell>8 (Up 1)</cell><cell>2.1008</cell><cell>12 (Down 3)</cell><cell>0.5980</cell></row><row><cell>simpleScore</cell><cell>10</cell><cell></cell><cell>0.4535</cell><cell>9 (Up 1)</cell><cell>1.9804</cell><cell>9 (Up 1)</cell><cell>0.6394</cell></row><row><cell>run FDwD</cell><cell>11</cell><cell></cell><cell>0.4348</cell><cell>13 (Down 2)</cell><cell>1.7684</cell><cell>13 (Down 2)</cell><cell>0.5916</cell></row><row><cell>waterlooB</cell><cell>12</cell><cell></cell><cell cols="2">0.4308 12 (-)</cell><cell>1.8379</cell><cell>10 (Up 2)</cell><cell>0.6244</cell></row><row><cell>waterlooA</cell><cell>13</cell><cell></cell><cell>0.4167</cell><cell>14 (Down 1)</cell><cell>1.7364</cell><cell>11 (Up 2)</cell><cell>0.6021</cell></row><row><cell>UDInfoCS2014 1</cell><cell>14</cell><cell></cell><cell>0.4080</cell><cell>15 (Down 1)</cell><cell cols="2">1.6435 14 (-)</cell><cell>0.5559</cell></row><row><cell>dixlticmu</cell><cell>15</cell><cell></cell><cell>0.3980</cell><cell>16 (Down 1)</cell><cell cols="2">1.5110 15 (-)</cell><cell>0.5366</cell></row><row><cell>uogTrCsLtrF</cell><cell>16</cell><cell></cell><cell>0.3906</cell><cell>11 (Up 5)</cell><cell cols="2">1.9164 16 (-)</cell><cell>0.5185</cell></row><row><cell>run DwD</cell><cell>17</cell><cell></cell><cell cols="2">0.3177 17 (-)</cell><cell>0.9684</cell><cell>19 (Down 2)</cell><cell>0.3766</cell></row><row><cell>tueNet</cell><cell>18</cell><cell></cell><cell>0.2261</cell><cell>19 (Down 1)</cell><cell cols="2">0.9224 18 (-)</cell><cell>0.3820</cell></row><row><cell>choqrun</cell><cell>19</cell><cell></cell><cell>0.2254</cell><cell>21 (Down 2)</cell><cell>0.7372</cell><cell>23 (Down 4)</cell><cell>0.3412</cell></row><row><cell>tueRforest</cell><cell>20</cell><cell></cell><cell>0.2227</cell><cell>18 (Up 2)</cell><cell cols="2">0.9293 20 (-)</cell><cell>0.3604</cell></row><row><cell>cat</cell><cell>21</cell><cell></cell><cell>0.2087</cell><cell>23 (Down 2)</cell><cell cols="2">0.6120 21 (-)</cell><cell>0.3496</cell></row><row><cell>BUPT PRIS 01</cell><cell>22</cell><cell></cell><cell>0.1452</cell><cell>20 (Up 2)</cell><cell>0.7453</cell><cell>17 (Up 5)</cell><cell>0.4475</cell></row><row><cell>BUPT PRIS 02</cell><cell>23</cell><cell></cell><cell>0.1425</cell><cell>22 (Up 1)</cell><cell>0.6601</cell><cell>22 (Up 1)</cell><cell>0.3467</cell></row><row><cell>gw1</cell><cell>24</cell><cell></cell><cell cols="2">0.1024 24 (-)</cell><cell cols="2">0.3646 24 (-)</cell><cell>0.1649</cell></row><row><cell>lda</cell><cell>25</cell><cell></cell><cell cols="2">0.0843 25 (-)</cell><cell cols="2">0.2461 25 (-)</cell><cell>0.1564</cell></row><row><cell>Run</cell><cell cols="7">P@5 Rank P@5 Score TBG Rank TBG Score MRR Rank MRR Score</cell></row><row><cell cols="2">CWI CW12.MapWeb</cell><cell>1</cell><cell cols="2">0.1445 1 (-)</cell><cell cols="2">0.6078 1 (-)</cell><cell>0.2307</cell></row><row><cell>Model1</cell><cell></cell><cell>2</cell><cell cols="2">0.0903 2 (-)</cell><cell cols="2">0.3411 2 (-)</cell><cell>0.1979</cell></row><row><cell>Model0</cell><cell></cell><cell>3</cell><cell cols="2">0.0582 3 (-)</cell><cell cols="2">0.1994 3 (-)</cell><cell>0.1023</cell></row><row><cell>runA</cell><cell></cell><cell>4</cell><cell cols="2">0.0482 4 (-)</cell><cell cols="2">0.1647 4 (-)</cell><cell>0.0856</cell></row><row><cell>CWI CW12 Full</cell><cell></cell><cell>5</cell><cell cols="2">0.0468 5 (-)</cell><cell cols="2">0.1256 5 (-)</cell><cell>0.0767</cell></row><row><cell>runB</cell><cell></cell><cell>6</cell><cell cols="2">0.0254 6 (-)</cell><cell cols="2">0.0614 6 (-)</cell><cell>0.0552</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,164.32,665.31,283.36,8.74"><head>Table 2 :</head><label>2</label><figDesc>P@5, TBG, and MRR rankings for all ClueWeb12 runs.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,86.11,661.13,321.80,6.64"><p>http://en.wikipedia.org/wiki/List_of_metropolitan_areas_of_the_United_States</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,86.11,716.26,143.97,6.64"><p>http://lemurproject.org/clueweb12/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,86.36,490.06,454.77,8.74;11,86.36,502.01,22.69,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,395.25,490.06,141.55,8.74">Evaluating contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">Adriel</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,86.36,521.94,454.77,8.74;11,86.36,533.89,454.77,8.74;11,86.36,545.85,115.45,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,266.05,521.94,205.80,8.74">Time-based calibration of effectiveness measures</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,491.48,521.94,49.65,8.74;11,86.36,533.89,450.93,8.74">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
