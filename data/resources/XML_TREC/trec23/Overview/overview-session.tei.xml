<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.25,114.94,295.50,15.12">Overview of the TREC 2014 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.07,153.34,76.57,10.48;1,185.64,151.73,1.41,6.99"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.85,153.34,102.07,10.48;1,309.91,151.73,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google, Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.11,153.34,52.34,10.48;1,384.46,151.73,1.88,6.99"><forename type="first">Mark</forename><surname>Hall</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Edge Hill University</orgName>
								<address>
									<settlement>Ormskirk</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.06,153.34,63.57,10.48;1,484.63,151.73,1.88,6.99"><forename type="first">Paul</forename><surname>Clough</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Information School</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.25,114.94,295.50,15.12">Overview of the TREC 2014 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1EFE395A2A07A84C64E40243DECF8A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Session track ran for the fourth time in 2014. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions.</p><p>The experimental design of the track was similar to that of the previous three years <ref type="bibr" coords="1,476.42,320.90,11.52,9.57" target="#b4">[5,</ref><ref type="bibr" coords="1,491.58,320.90,8.49,9.57" target="#b5">6,</ref><ref type="bibr" coords="1,503.70,320.90,7.68,9.57" target="#b0">1]</ref>:</p><p>• sessions were real user sessions with a search engine that include queries, retrieved results, clicks, and dwell times;</p><p>• retrieval tasks were designed to study the effect of using session data in retrieval for only the mth query in a session.</p><p>For the 2014 track, sessions were obtained from workers on Amazon's Mechanical Turk. As a result, the 2014 data includes far more sessions than previous years-1,257 unique sessions as compared to around 100 for each of the previous three years. Apart from that, there is little different from the 2013 track <ref type="bibr" coords="1,144.15,462.70,10.91,9.57" target="#b0">[1]</ref>.</p><p>This overview is organized as follows: in Section 2 we describe the tasks participants were to perform. In Section 3 we describe the corpus, topics, and sessions that comprise the test collection. Section 4 gives some information about submitted runs. In Section 5 we describe relevance judging and evaluation measures, and Sections 6 present evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Tasks</head><p>We use the word "session" to mean a sequence of reformulations along with any user interaction with the retrieved results in service of satisfying a specific topical information need. The primary goal for participants of the 2013 track was to provide the best possible results for the last query in a session given data from the user interactions leading up to it.</p><p>We provided participants with a set of 1,257 sessions of varying length (described in more detail in Section 3). 1,021 of these were targeted for evaluation; the remaining 236 could be used for training, as could any data from the previous two years of the track. Each of the 1,021 evaluated sessions, numbered i = 1..1021, consists of:</p><p>• m i blocks of user interactions (the length of the session);</p><p>• the final query in the session q m i , referred to as the current query;</p><p>• m i -1 blocks of user interactions in the session prior to the current query:</p><p>1. the set of past queries in the session, q 1 , q 2 , ..., q m i -1 ;</p><p>2. the ranked list of URLs seen by the user for each of those queries;</p><p>3. the set of clicked URLs/snippets.</p><p>In addition, each user action is accompanied by a time relative to the start of the session. The remaining 236 sessions consist of the same data except all of them have length m i = 1 and thus they have no current/final query.</p><p>Participants were to run the 1,021 current queries against their search engines under each of the following three conditions separately:</p><p>RL1 ignoring the session prior to this query RL2 considering all the items (1), ( <ref type="formula" coords="2,254.75,362.72,4.65,9.57">2</ref>) and ( <ref type="formula" coords="2,291.94,362.72,4.65,9.57">3</ref>) above for the current session, i.e the queries prior to the current, the ranked lists of URLs and the corresponding web pages, the clicked URLs and the time spent on any interaction RL3 considering all data in the entire session log (in particular, other user sessions on the same topic)</p><p>Comparing the retrieval effectiveness in (RL1) with the retrieval effectiveness in (RL2)-(RL3), one can evaluate the effectiveness of different algorithms for incorporating session information into retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Collection</head><p>Like most IR test collections, ours consists of a corpus, a set of topics, and relevance judgments (described in the next section). Unlike most test collections, ours also includes a set of sessions of user interactions (including query reformulations). A single topic can have more than one session associated with it, since two different users could go about satisfying the same information need in very different ways and with different degrees of success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head><p>The track used the ClueWeb12 collection. The full collection consists of roughly 730 million Englishlanguage web pages, comprising approximately 5TB of compressed data. The dataset was crawled from the Web during February and March 2012.</p><p>Participants were encouraged to use the entire collection, however submissions over the smaller "Category B" collection of about 35 million documents were accepted. Note that Category B submissions was evaluated as if they were Category A submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics</head><p>For the 2014 track, we re-used topics that had been defined for the 2012 and 2013 tracks. Both of these topic sets were defined with an attempt to control two facets of search tasks as defined by Li and Belkin <ref type="bibr" coords="3,142.23,187.36,10.91,9.57" target="#b6">[7]</ref>: "product" and "goal quality". The "product" facet represents the end goal of the task, which is either intellectual-to produce new ideas or findings (e.g. learn about a topic or make decisions based on information collected)-or factual-locating facts, data, or other information items. An example of such variation (from the 2012 Session track topics) can be viewed below.</p><p>Query: dehumidifiers The "goal quality" facet reflects specific goal(s) and amorphous goal(s). This is very similar to the dimension that Ingwersen and Järvelin <ref type="bibr" coords="3,256.24,443.35,11.52,9.57" target="#b3">[4]</ref> proposed as well-defined and ill-defined information need.</p><p>Tasks with specific goals have a well-defined information need, while in tasks with amorphous goals, the information need is ill-defined. Tasks with amorphous goals might require users to redefine the topic or identify specific aspects of the subject themselves. An example (again from the 2012 track) can be viewed below:</p><p>Query: Swahili dishes</p><p>• Session topic: 38 "Goal" facet: specific goals Description: What are some traditional Swahili dishes? What ingredients do they use to cook them? Are swahili people using any particular herb in their dishes? Could you find these ingredients in your country? Are there any recipes you can find online?</p><p>• Session topic: 40 "Goal" facet: amorphous goals Description: One of your friends from Kenya invited you to attend a party in his house and have a taste of traditional swahili dishes. You would like to search and find some information about Swahili dishes.</p><p>Combining "product" and "goal quality" facets generates four classes of topics, characterized as follows: a factual task with specific goals is known-item search; a factual task with amorphous goals is known-subject search; an intellectual task with specific goals is interpretive search; and an intellectual task with amorphous goals is exploratory search. A complete example of all four combinations can be viewed below:</p><p>Query: depression symptoms</p><p>• Task: Known-item search Description: What is depression? What are the major symptoms of depression? What medications, therapies and other treatments can be used to treat depression symptoms? Who performs therapy and what are the costs? Does health insurance pay for any of the treatments?</p><p>• Task: Known-subject search Description: You think that one of your friends may have depression, and you want to search information about the depression symptoms and possible treatments.</p><p>• Task: Interpretive search Description: Depression is a loaded word in our culture. What are the symptoms that could differentiate depression from having just a bad month of excessive emotions? When should one seek help and what kind?</p><p>• Task: Exploratory search Description: A friend has been complaining for months that she is unhappy with her life. She has also mentioned that she can't easily sleep at nights. You think that she may be suffering from depression. You want to understand if this is the case and how you could assist her in getting some help from medical professionals.</p><p>For the 2014 track, we selected topics from 2012 and 2013 such that there would be the same number of topics in each of the four categories. We biased selection to topics that had longer user sessions and more clicks, i.e. more user interaction overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sessions</head><p>As describe above, a session is a series of actions, including queries and clicks on ranked results, that a user performs in the process of trying to satisfy the information need represented by the topic. The topics were presented to actual users, who were then able to use a fully-functional custom search engine for ClueWeb12 in order to satisfy the information need described by the topic. .</p><p>The search system used an indri index of ClueWeb12 as a backend. Each of the 20 ClueWeb12 segments (ClueWeb12 00 through ClueWeb12 19) was indexed using the Krovetz stemmer and no stopword list. The indexes searched contained only text from title fields, anchor text from incoming links ("inlink" text), and page URLs. We chose to index these fields only in an effort to get faster response times. Each query was plugged into an indri query language template as exemplified below for the query "quitting smoking":</p><p>#combine(#weight( <ref type="formula" coords="5,160.92,76.87,5.23,8.30">1</ref>#combine(quitting smoking) 1 #weight(1 #combine(quitting.title smoking.title) 1 #uw(quitting smoking).title) 50 #weight(1 #combine(quitting.inlink smoking.inlink) 1 #uw(quitting smoking).inlink) 0.5 #weight(1 #combine(quitting.url smoking.url) 1 #uw(quitting smoking).url)))</p><p>The retrieval score is computed from four component models: a basic query-likelihood model for the full document representation and three weighted combinations of basic query-likelihood field models with unordered-window within-field models. The "inlink" model was weighted 50 times higher than the title model, and 100 times higher than the URL model. This query template is the product of manual search and investigation of retrieved results.</p><p>The search interface connected to our indri backend to retrieve the top 50 results (which were filtered further so that at most two documents from any domain would be shown). Users were shown these results in pages of 10 along with a snippet produced by indri's built-in snippet generator. They could click results to see the current version of the page, which of course could be different from the version in the index. We asked users to search for a minimum of 3 minutes.</p><p>Our full indri search engine can be quite slow, requiring 30 seconds or more to respond to some queries. We cached results for all queries entered and served results from the cache when available.</p><p>In order to ensure a relatively fast response for all queries, we also built a smaller index consisting of documents in cached results. Then, if no cached results were available, the query was submitted to both indexes, which were given a maximum of 6 seconds to respond. If neither had responded after 6 seconds, results were assembled in an ad hoc way by fusing cached results from previous queries that used the same terms and phrases.</p><p>The system recorded the user's interactions with the retrieval system, including the queries issued, query reformulations, and items clicked in the results page. When data collection was complete, we had acquired a set of candidate sessions to go with the candidate topics we defined above. Each session consists of a topic, a set of queries actual users posed about the topic, the retrieved results, and the user interactions with the retrieved results.</p><p>Session data is provided in an XML file. Part of an example session is shown on page 6.</p><p>The released data comprised 1,257 full sessions. 1,021 of these have at least one reformulation (i.e. at least two queries, of which the second is the currentquery); these were the ones targeted for evaluation. Of these, 185 have at least three reformulations, 145 have at least four, 107 have at least five, 17 have at least 10, and there is one session with 15 reformulations. On average there are 4.33 queries per session, and the median session length is 2 queries. These are substantially shorter sessions than 2013, most likely reflecting the different user base. The median length of time spent on a session was about 2.8. minutes. The data includes 1,685 total clicks across 1,257 sessions, 1.34 per session on average and about one click every other user query.</p><p>&lt;session num="10" starttime="0"&gt; &lt;topic num="12"&gt; &lt;desc&gt;Your friend would like to quit smoking. You would like to provIde him with relevant information about: the different ways to quit smoking, programs available to help quit smoking, benefits of quitting smoking, second effects of quitting smoking, using hypnosis to quit smoking, using the cold turkey method to quit smoking&lt;/desc&gt; &lt;/topic&gt; &lt;interaction num="1" starttime="8. <ref type="bibr" coords="6,281.21,168.38,26.15,8.30">30123</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions</head><p>Participating sites were permitted to submit up to three runs. Each submitted run includes up to three separate ranked result lists for all 1,022 sessions. Files were named "runTag.RLn", where "runTag" is a unique identifier for the site and the particular submission, and "RLn" is RL1, RL2, or RL3, depending on the experimental condition.</p><p>The track received 27 runs from 11 groups, for a total of 74 ranked lists for each session. They are listed in Table <ref type="table" coords="7,143.88,177.00,4.24,9.57" target="#tab_2">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Session Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relevance Judgments</head><p>Judging was done by assessors at NIST. As described above, each topic was the subject of one or more sessions. Thus pools for judging were formed by topic, not by session.</p><p>For each topic, a pool was formed from the ranked results produced by our indri system for the past queries q 1 ...q m-1 and the current query q m , along with the top 10 ranked documents from the submitted runs on the current query q m .Because there was such a large number of sessions, only the first 100 (by ID) contributed to the pool. These sessions include 51 of the 60 topics, so there are 9 that received no relevance judgments this year.</p><p>The NIST assessors judged each document in the pool with respect to the topic description. URLs were sorted by domain prior to judging so that assessors would see all pages from the same domain before moving to another one.</p><p>The qrels produced have the following format:</p><p>&lt;topic-id&gt; 0 &lt;doc-id&gt; &lt;judgment&gt; Judgment values are: -2 for spam document (i.e. the page does not appear to be useful for any reasonable purpose; it may be spam or junk.); 0 for not relevant (i.e. the content of this page does not provide useful information on the topic, but may provide useful information on other topics, including other interpretations of the same query); 1 for relevant (i.e. the content of this page provides some information on the topic, which may be minimal; the relevant information must be on that page, not just promising-looking anchor text pointing to a possibly useful page); 2 for highly relevant (i.e. the content of this page provides substantial information on the topic); 3 for key, (i.e. the page or site is dedicated to the topic; authoritative and comprehensive, worthy of being a top result in a web search engine; typically, key pages are more comprehensive, have higher quality, and are from more trustworthy sources than the merely highly relevant page); and 4 for navigational (i.e. this page represents a home page of an entity directly named by the query; the user may be searching for this specific page or site; there is often at most one page that deserves a Navigational judgment for an aspect).</p><p>Relevance judgments were eventually transformed to relevance grades with spam and non-relevant documents assigned a grade of 0, relevant assigned a grade of 1, highly relevant assigned a grade of 2, key assigned a grade of 3, and navigational assigned a grade of 4.</p><p>A total of 16,949 pages were judged. Out of these 16,949 pages, 31 were judged as navigational, 148 as key, 1,160 as highly relevant, 3,536 as relevant, 11,277 as nonrelevant, and 797 as spam.</p><p>On average there were 121 nonrelevant (or spam) and 49 relevant (across all types of relevance) documents per session. Only 5 of the topics had at least one "key" document, and 24 had at least one "nav" document. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Measures</head><p>Based on the qrels provided by NIST and the decisions described above, we evaluated submitted runs by eight measures:</p><p>• Expected Reciprocal Rank (ERR) <ref type="bibr" coords="8,265.67,455.85,11.52,9.57" target="#b2">[3]</ref> • ERR@10</p><p>• ERR normalized by the maximum ERR per query (nERR)</p><p>• nERR@10</p><p>• nDCG</p><p>• nDCG@10</p><p>• Average Precision (AP)  <ref type="table" coords="9,101.98,463.66,4.24,9.57">2</ref>: All results by nDCG@10 for the current query in the first 100 sessions for each condition (sorted in decreasing order of RL1 nDCG@10). Boldface indicates the highest nDCG@10 in the condition. ↑, ↓ indicate positive or negative differences from the prior condition. ⇑, ⇓ indicate statistically significant (p &lt; 0.05 by a paired two-sided t-test) positive or negative differences from the prior condition. ↔ indicates no difference from the prior condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>Table <ref type="table" coords="9,101.32,579.91,5.45,9.57">2</ref> shows all results (by nDCG@10) for all submitted runs in all three experimental conditions. If RL1 (no information about the session) is the baseline, most (18 of 24 that submitted an RL2) of the submitted runs were able to improve on that using information about the session prior to the last query (RL2 results). Seven of those were statistically significant improvements. A smaller set of systems (14 of 22 that submitted an RL3) were further able to improve by making use of the full log (RL3 results), though only two of these were statistically significant over RL2. Figure <ref type="figure" coords="10,106.72,487.55,5.45,9.57" target="#fig_0">1</ref> shows changes in nDCG@10 from the RL1 baseline (left) or with increasing information (right). The plots going down the left column show changes in nDCG@10 from using no previous data (RL1) to using greater and greater amounts of previous data. The dashed line is a difference in nDCG of zero; points above that line represent systems that saw an improvement from using the additional data while points below it represent systems that were hurt with the additional data. The 95% confidence intervals give a rough idea of whether the results are significant.</p><p>On the right-hand side, Figure <ref type="figure" coords="10,221.48,573.54,5.45,9.57" target="#fig_0">1</ref> shows changes in nDCG@10 with increasing amounts of previous data: going from RL1 to RL2, then RL2 to RL3. Only a few systems see improvement at every step, and the improvements are not significant.</p><p>Figure <ref type="figure" coords="10,105.85,618.89,5.45,9.57" target="#fig_1">2</ref> shows the same information for unnormalized ERR. The two measures are well-correlated, with Kendall's τ correlation of 0.83 between the nDCG@10 difference from RL1 to RL2 and the ERR difference from RL1 to RL2.</p><p>Table <ref type="table" coords="10,102.47,664.23,5.45,9.57">3</ref> shows results for all sessions that each run ranked documents for. Some groups chose to return results for only the first 240 sessions, while others returned results for all 1021 sessions-the split is about even. Note that since only the first 100 sessions were used to form pools, evaluation measures in this table have many missing judgments. We have not yet analyzed the extent to which that affects results. However, we observe that the Kendall's τ rank correlation between the RL1 evaluation in Table <ref type="table" coords="12,167.04,526.68,5.45,9.57">2</ref> and Table <ref type="table" coords="12,227.25,526.68,5.45,9.57">3</ref> is only 0.69, suggesting that the missing judgments might make a big difference.</p><p>7 Analysis </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Topic reuse</head><p>As mentioned in Section ??, topics for 2014 were taken from the 2012 and 2013 Session tracks. Since 2013 also used ClueWeb12, we can compare 2013 and 2014 data for these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">User queries</head><p>We looked at similarity between user queries from 2013 to 2014. First, for each topic we estimate two unigram language models: one for queries issued for 2013 topics and one for queries issued for 2014 topics. Model parameters are estimated as follows:</p><p>Examples of topic language models are shown in Table ??. The two examples show two extreme cases: high agreement about query terms (topic 14) and low agreement about query terms (topic 32). Topic 32 in particular is an example of how difference in prior knowledge can influence search behavior: the topic asks for good places in the US to travel with a lot to do in a 150-mile radius. 2013 users immediately thought of specific cities in densely-populated areas like Philadelpha and New York City, while 2014 users use the much more vague phrase "best cities in united states".</p><p>On average, the difference in topic models is greatest for "exploratory" topics, followed by "interpretive" topics, followed by "known-subject" topics, followed by "known-item" topics (though these differences are not significant, as the sample size is small). This confirms our intuition. Since topics are the same, and we made no effort to remove documents judged for 2013 from the pools for 2014, it is likely that documents judged for 2013 were re-judged for 2014. This allows us to calculate agreement between assessors. 27 of the 60 topics for 2014 were taken from the 2013 data. Of these 27, 24 have relevance judgments.</p><p>In 2013, a total of 8,125 documents were judged for those 27 topics. In 2014, a total of 7,894 documents were judged for the 24 matching topics. That is a total of 16,019 documents judged. Of these, only 933 were judged both years. That is, only 6% of documents judged for these topics were judged both years. This is remarkably low and speaks poorly to the reusability of the collections (see below). Table <ref type="table" coords="14,102.15,444.64,4.48,9.57" target="#tab_6">7</ref>.1.3 shows agreement on binary relevance (converting all judgments ≥ 1 to 1 and all judgmend ≤ 0 to 0) among these 933. Overall agreement is 67%. Agreement on documents that at least one assessor judged relevant is 47%, which is in line with previous studies on disagreement. Interestingly, assessors were much more likely to say a document judged nonrelevant in 2013 was relevant in 2014 than vice versa. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,72.00,441.00,468.00,9.57;10,72.00,454.55,468.00,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Changes in nDCG@10 from RL1 to (from top to bottom) RL2 and RL3. Right: Changes in nDCG@10 from RL1 to RL2 and RL2 to RL3. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,72.00,441.00,468.00,9.57;12,72.00,454.55,404.27,9.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Changes in ERR from RL1 to (from top to bottom) RL2 and RL3. Right: Changes in ERR from RL1 to RL2 and RL2 to RL3. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,77.98,177.00,459.38,215.80"><head>Table 1 :</head><label>1</label><figDesc>. Groups participating in the 2014 Sessions Track.</figDesc><table coords="7,77.98,210.79,60.17,9.57"><row><cell>num group</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="14,72.00,517.08,338.37,136.34"><head>Table 7 .</head><label>7</label><figDesc>1.3  shows agreement on relevance grades.</figDesc><table coords="14,192.02,548.24,218.35,105.19"><row><cell>P 2013 P P</cell><cell>P</cell><cell>P P 2014 4 3 P P P nav -4 1 0</cell><cell>2 0</cell><cell>1 0</cell><cell>0 0</cell><cell>-2 0</cell></row><row><cell></cell><cell></cell><cell>key -3 0 1</cell><cell>2</cell><cell>7</cell><cell>4</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>hi -2 0 4</cell><cell cols="2">28 52</cell><cell>14</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell cols="3">rel -1 1 12 75 89</cell><cell>64</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>not -0 4 5</cell><cell cols="4">50 161 337 11</cell></row><row><cell></cell><cell></cell><cell>junk -2 0 0</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,88.59,669.34,451.41,7.86"><p>Somewhat strangely, two topics had more than 10 "key" documents, and one had more than 35 "nav" documents.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table" coords="11,102.59,564.82,4.24,9.57">3</ref><p>: All results by nDCG@10 for the current query in all ranked sessions for each condition (sorted in decreasing order of RL1 nDCG@10).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="15,88.97,104.34,451.03,9.57;15,88.97,117.89,215.30,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,413.18,104.34,126.82,9.57;15,88.97,117.89,58.69,9.57">Overview of the trec 2013 session track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,170.69,117.89,98.30,9.57">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,88.97,140.41,451.03,9.57;15,88.97,153.96,451.03,9.87;15,88.97,168.29,134.76,9.09" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<ptr target="http://ir.cis.udel.edu/ECIR11Sessions" />
		<title level="m" coord="15,423.98,140.41,116.02,9.57;15,88.97,153.96,300.19,9.57">Proceedings of the ECIR 2011 Workshop on Information Retrieval Over Query Sessions</title>
		<meeting>the ECIR 2011 Workshop on Information Retrieval Over Query Sessions</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,88.97,190.02,451.04,9.57;15,88.97,203.57,451.03,9.57;15,88.97,217.12,113.83,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,362.65,190.02,177.35,9.57;15,88.97,203.57,41.75,9.57">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,166.65,203.57,373.35,9.57;15,88.97,217.12,79.33,9.57">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,88.97,239.64,451.03,9.57;15,88.97,253.19,451.03,9.57;15,88.97,266.73,53.94,9.57" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="15,237.54,239.64,302.46,9.57;15,88.97,253.19,210.22,9.57">The Turn: Integration of Information Seeking and Retrieval in Context (The Information Retrieval Series)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,88.97,289.25,451.03,9.57;15,88.97,302.80,451.03,9.57;15,88.97,316.35,451.03,9.57;15,88.97,330.68,104.64,9.09" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,406.99,289.25,128.10,9.57">Session track 2011 overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec20/papers/SESSION.OVERVIEW.2011.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="15,103.04,302.80,324.14,9.57">The Twentieth Text REtrieval Conference Proceedings (TREC 2011</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="15,88.97,352.41,451.03,9.57;15,88.97,365.96,240.75,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,440.37,352.41,99.63,9.57;15,88.97,365.96,84.15,9.57">Overview of the trec 2012 session track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,196.14,365.96,98.30,9.57">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,88.97,388.48,451.03,9.57;15,88.97,402.03,244.40,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,210.33,388.48,325.00,9.57">A faceted approach to conceptualizing tasks in information seeking</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,88.97,402.03,101.09,9.57">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1822" to="1837" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
