<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,64.68,72.44,480.25,16.59">Overview of the TREC 2014 Federated Web Search Track</title>
				<funder>
					<orgName type="full">Folktales As Classifiable Texts</orgName>
					<orgName type="abbreviated">FACT</orgName>
				</funder>
				<funder>
					<orgName type="full">Ghent University -iMinds in Belgium</orgName>
				</funder>
				<funder ref="#_y9JQWS7">
					<orgName type="full">Dutch</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,84.24,118.02,104.90,11.06"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
							<email>tdmeeste@intec.ugent.be</email>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -iMinds</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.04,118.02,84.89,11.06"><forename type="first">Dolf</forename><surname>Trieschnigg</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.92,118.02,72.77,11.06"><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
							<email>d.nguyen@utwente.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.56,118.02,44.69,11.06"><forename type="first">Ke</forename><surname>Zhou</surname></persName>
							<email>kezhou@yahoo-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Yahoo Labs London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,435.12,118.02,86.21,11.06"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<email>d.hiemstra@utwente.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,64.68,72.44,480.25,16.59">Overview of the TREC 2014 Federated Web Search Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D5D8ED2EDC9A4DF889E374B85B737D1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Federated Web Search track facilitates research on federated web search, by providing a large realistic data collection sampled from a multitude of online search engines. The FedWeb 2013 Resource Selection and Results Merging tasks are again included in FedWeb 2014, and we additionally introduced the task of vertical selection. Other new aspects are the required link between the Resource Selection and Results Merging tasks, and the importance of diversity in the merged results. After an overview of the new data collection and relevance judgments, the individual participants' results for the tasks are introduced, analyzed, and compared.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>When Sergey Brin and Larry Page wrote their seminal "The Anatomy of a Large-Scale Hypertextual Web Search Engine" <ref type="bibr" coords="1,88.20,396.72,9.64,8.97" target="#b2">[3]</ref> they added an appendix about the scalibility of Google in which they argued that its scalability is limited by their choice for a single, centralized index. While these limitations would decrease over time, following Moore's law, a truly scalable solution would require a drastic redesign. They write the following: "Of course a distributed systems like Gloss or Harvest will often be the most efficient and elegant technical solution for indexing, but it seems difficult to convince the world to use these systems because of the high administration costs of setting up large numbers of installations. Of course, it is quite likely that reducing the administration cost drastically is possible. If that happens, and everyone starts running a distributed indexing system, searching would certainly improve drastically." (Brin and Page 1998 <ref type="bibr" coords="1,238.32,572.88,9.88,8.97" target="#b2">[3]</ref>) When we started to crawl results from independent web search engines of all kinds, we hoped it would inspire researchers to come up with elegant and efficient solutions to distributed search. However, the crawl can be used for many other research goals as well, including scenarios that resemble the aggregated search approaches implemented by most general web search engines today.</p><p>The TREC federated web search track provides a test collection consisting of search result pages of 149 internet search TREC 2014 Gaithersburg, USA engines. The track aims to answer research questions like: "What is the best search engine for this query?", "What is the best medium, topic or genre, for this query?" and "How do I combine the search results of a selection of the search engines into one coherent ranked list?" The research questions are addressed in the following three tasks: Resource Selection, Vertical Selection, and Results Merging:</p><formula xml:id="formula_0" coords="1,321.24,292.76,125.29,8.08">Task 1: Resource Selection</formula><p>The goal of resource selection is to select the right resources (search engines) from a large number of independent search engines given a query. Participants have to rank the given 149 search engines for each test topic without having access to the corresponding search results. The FedWeb 2014 collection contains search result pages for many other queries, as well as the HTML of the corresponding web pages. These data could be used by the participants to build resource descriptions. Participants may also use external sources such as Wikipedia, ODP, or WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 2: Vertical Selection</head><p>The goal of vertical selection is to classify each query into a fixed set of 24 verticals, i.e. content dedicated to either a topic (e.g. "finance"), a media type (e.g. "images") or a genre (e.g. "news"). Each vertical contains several resources, for example, the "image" vertical contains resources such as Flickr and Picasa. With this task, we aim to encourage vertical (domain) modeling from the participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 3: Results Merging</head><p>The goal of results merging is to combine the results of several search engines into a single ranked list. After the deadline for Task 1 passed, the participants were given the search result pages of 149 search engines for the test topics. The result pages include titles, result snippets, hyperlinks, and possibly thumbnail images, all of which were used by participants for reranking and merging.</p><p>The official FedWeb track guidelines can be found online 1 . This overview paper is organized as follows: Section 2 describes the FedWeb 2014 collection; Section 3 describes the process of gathering relevance judgements for the track; Section 4 presents our online system for validation and preliminary evaluation of runs. Sections 5, 6 and 7 describe the results for the vertical selection task, the resource selection </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FEDWEB 2014 COLLECTION</head><p>Similar to last year the collection for the FedWeb track consisted of a sample crawl and a topic crawl for a large number of online search engines. The sample crawl consists of sampled search engine results (i.e. the snippets from the first 10 results) and downloads of the pages these snippets refer to. The snippets and pages can be used to create a resource description for each search engine, to support vertical and resource selection. The topic crawl is used for evaluation and consists of only the snippets for a number of topic queries. In contrast to last year, in which also the pages of the topic queries were available, we provided only the snippets of the topics to make the tasks more realistic.</p><p>Where possible we reused the list of search engines from the 2013 track, ending up with a list of 149 search engines which were still available for crawling. We doubled the number of sample queries to 4000, to allow for more precise resource descriptions. Similar to last year the first set of 2000 queries were based on single words sampled from different frequency bins from the vocabulary of the ClueWeb09-A collection. These correspond to the sample queries issued in 2013. The second set of 2000 queries is different for each engine and consists of random words sampled from the language model obtained from the first 2000 snippets.</p><p>Table <ref type="table" coords="2,87.84,652.31,4.60,8.97" target="#tab_0">1</ref> lists the number of resources (search engines) per vertical. Appendix A lists the engines used this year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELEVANCE ASSESSMENTS</head><p>In this section, we describe how the test topics were chosen and how the relevance judgments were organized. We also visualize the distribution of relevant documents over the different test topics, and over the various verticals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test Topics</head><p>We started from the 506 topics gathered for FedWeb 2013 <ref type="bibr" coords="2,338.04,118.07,9.12,8.97" target="#b4">[5]</ref>, leaving out the 200 topics provided to the FedWeb 2013 participants. From the remaining 306 topics, we selected 75 topics as follows. We first assigned labels of the most likely vertical intents to each of the topics (based on intuition and query descriptions). We then manually selected these 75 topics such, that most of the topics would potentially target other verticals than just general web search engines, where even the smallest verticals had at least one dedicated topic (e.g., Jokes, or Games), and with more emphasis on the larger verticals (see Appendix A). The pages from all resources were entirely judged for 60 topics, randomly chosen among the 75 selected ones. The first 10 fully annotated topics were used for the online evaluation system (see Section 4), and the remaining 50 are the actual test topics (see Appendix B).</p><p>For the previous (2013) edition of the track, we had the top 3 snippets from each search engine for each of the candidate topics judged first, on which we based the choice of evaluation topics, and which provided the starting point for writing out the narratives providing the annotation context. This year, we decided not to do any snippet judgments, and instead, to spend our resources on judging 10 extra topics. We manually created the narratives by quickly going through the results, and in consultation with the assessors. An example of one of the test topics is given below, with the query terms, description, and narrative, which were all shown to the assessors. Each topic was judged by a single assessor, in a random order, where we had contributions from 10 hired assessors. The assessors are all students in various fields, such that we had the liberty of assigning specialized queries to specialized assessors. For example, the topic given below was entirely judged by a medical student. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relevance Levels</head><p>The same graded relevance levels were used as in the Fed-Web 2013 edition, taken over from the TREC Web Track<ref type="foot" coords="2,549.24,636.00,3.65,5.30" target="#foot_0">2</ref> : Non (not relevant), Rel (minimal relevance), HRel (highly relevant), Key (top relevance), and Nav (navigational). Based on the User Disagreement Model (UDM), introduced in <ref type="bibr" coords="2,543.60,668.51,9.12,8.97" target="#b3">[4]</ref>, the following weights are assigned to these relevance levels: wNon = 0.0 w Rel = 0.158 w HRel = 0.546 wKey = 1.0 wNav = 1.0 These were estimated from a set of double annotations for the FedWeb 2013 collection, which has, by construction, comparable properties to the FedWeb 2014 dataset.</p><p>For evaluating the quality of a set of 10 results as returned by the resources in response to a test topic, we use the relevance weights listed above to calculate the Graded Precision (introduced by <ref type="bibr" coords="3,115.80,204.71,14.20,8.97" target="#b10">[11]</ref> as the generalized precision). This measure amounts to the sum of the relevance weights associated with each of the results, divided by 10 (also for resources that returned less than 10 results).</p><p>We now provide some insights into how the most relevant documents are distributed, depending on the test topics and among the different verticals. Fig. <ref type="figure" coords="3,201.96,267.47,4.60,8.97">1</ref> shows, for each test topic, the highest graded precision as found among all resources. The figure can thus be interpreted as a ranking of the topics from 'easy' to 'difficult', with respect to the set of resources in the FedWeb 2014 system. For example, for the leftmost topic 7252, one resource managed to return 10 Key results (not taking into account duplicate results). The query welch corgi targeted broad information, including pictures and videos, on Welsh corgi dogs. For the rightmost topic 7222, no Key results were returned, although a number of HRel results were. The query route 666 appeared to be rather ambiguous, and the narrative specified a specific need only (reviews/summaries of the movie).</p><p>Next, we selected for each topic the best resource (i.e., with highest graded precision) within each of the verticals, and created a boxplot by aggregating over the verticals. The result is shown in Fig. <ref type="figure" coords="3,152.40,434.87,3.56,8.97" target="#fig_1">2</ref>. We see that the best resource (depending on the queries) from the General search engines achieves the highest number of relevant results (and/or the results with the highest levels of relevance), followed by the Blogs, Kids, and Video verticals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PRELIMINARY ONLINE EVALUATION</head><p>During the last couple of weeks before the submission deadline for the different tasks, we opened up an online platform where participants could test their systems under preparation. By submitting a preliminary run to this system, the runs were validated by checking if they adhere to the TREC format, and the main evaluation metrics were returned. The evaluation metrics returned were based on 10 test queries, i.e., as described above, those 10 that were fully annotated but not used for the actual evaluation. Figure <ref type="figure" coords="3,288.36,595.55,4.60,8.97" target="#fig_0">3</ref> shows a screenshot of the online system.</p><p>Multiple participants indeed used this system, and we kept track of the different submitted runs. More than 500 runs were validated and tested online before the official submission deadline. Figure <ref type="figure" coords="3,144.12,647.87,4.60,8.97">4</ref> shows the main evaluation metrics (F1 for Vertical Selection, and nDCG@20 for both Resource Selection and Results Merging) for the valid runs among the online trial submissions. These metrics are the results with respect to the 50 evaluation topics, not including the 10 test topics for which the participants received the intermediate results (and towards which their systems might have been tuned). We did not try to link trial runs to specific participants, although we noticed that the same team often submitted consecutive runs to the system, either for a range of different techniques, or maybe to determine suitable values for model hyperparameters. For the Vertical Selection task, there is an overall increase in effectiveness of the systems, although the last runs seem to perform worse. For the Resource Selection task, the best run was found early on in the chronological order. For the Results Merging tasks more than half of the runs perform almost equally well, around nDCG@20≈0.3, although few runs perform better, which might be explained by the fact that participants over-trained their systems on the 10 test queries of the online system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">VERTICAL SELECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation</head><p>We report the precision, recall and F-measure (primary metric) of the submitted vertical selection runs in Table <ref type="table" coords="3,548.76,480.23,3.56,8.97" target="#tab_4">2</ref>. The primary vertical selection evaluation metric is the Fmeasure (based on our own implementation). The methodology of how we obtain the vertical relevance can be referred to the (GMR + II) approach described in <ref type="bibr" coords="3,482.16,522.00,13.37,8.97" target="#b17">[18]</ref>. Basically, the relevance of a vertical for a given query is determined by the best performing resource (search engine) within this vertical. More specifically, the relevance is represented by the maximum graded precision of its resources. For the final evaluation, the binary relevance of a vertical is determined by a threshold: a vertical for which the maximum graded precision is 0.5 or higher, is considered relevant. This threshold was determined based on data analyses, such that for most queries there is a small set of relevant verticals. If for a given query, no verticals have exceeded this threshold, we use the top-1 vertical with the maximal relevance as the relevant vertical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>Seven teams participated in the vertical selection task, with a total of 32 system runs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Participant Approaches</head><p>Chinese Academy of Sciences (ICTNET) <ref type="bibr" coords="4,496.68,636.68,14.10,9.86" target="#b7">[8]</ref> For the task of Vertical Selection, ICTNET submitted a number of high-scoring runs, including the overall best performing run (ICTNETVS07). Several strategies were proposed. For ICTNETVS1, they calculated a term frequency based similarity score between queries and verticals. They also explored using random forest classification to score verticals </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Merging</head><p>Figure <ref type="figure" coords="5,86.88,393.92,4.11,8.08">4</ref>: Main metrics per task, for the trial runs, in the order as submitted to the online evaluation system.</p><p>(run ICTNETVS02), whereby expanded query representations based on results from the Google Custom Search API were used. They further used a model to calculate the similarity between a vertical (represented by a small portion of the available documents) and the expanded query representation, based on Latent Semantic Indexing (LSI) to score verticals (with run ICTNETVS03). They also submitted a number of runs with variations and/or combinations of these methods (ICTNETVS04, ICTNETVS05, ICTNETVS06). For ICT-NETVS07, the best run for this task, they used a borda fuse combination of 3 methods, based on frequent term ranks in the given documents.</p><p>East China Normal University (ECNUCS) <ref type="bibr" coords="5,242.16,559.39,19.62,9.86" target="#b9">[10]</ref> East China Normal University introduces the Search Engine Impact Factor (SEIF), a query-independent measure of a search engine's impact, estimated in two different ways: 1) using external data from comScore, a company providing marketing data and analytics to web pages of many enterprises and publishers; and 2) using the TREC 2013 dataset and its relevance judgments. University of Delaware (udel) <ref type="bibr" coords="5,450.84,422.36,14.10,9.86" target="#b0">[1]</ref> Both submitted runs are based on the resource selection run udelftrsbs, whereby the baseline udelftvql ranks verticals according to the number of resources in the corresponding resource selection run, and for udelftvqlR some verticalspecific rule-based modifications were done (e.g., to require the presence of interrogative words for the Q&amp;A vertical), resulting in a significant increase in the F-measure.</p><p>Drexel University (dragon) <ref type="bibr" coords="5,438.48,537.91,19.62,9.86" target="#b15">[16]</ref> Drexel University's approach for vertical selection was based on their resource selection methodology. To select only a subset of verticals from the vertical ranking, they set a fixed cut-off threshold 0.01 on the normalized vertical score. This fixed threshold also resulted in high recall and low precision while the CRCS approach (drexelVS1) performed the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University of Stavanger (NTNUiS) [2]</head><p>Their vertical selection runs were directly based on their resource selection runs. In particular, they applied a threshold on the relevance scores of the individual resources and selected all verticals containing a resource that passed a threshold. The NTNUiSvs2 run, based on their best performing resource selection run, performed best. University of Lugano (ULugano) <ref type="bibr" coords="7,201.00,56.24,14.10,9.86" target="#b6">[7]</ref> The vertical selection runs they submitted were simply a direct derivation from their resource selection runs. Basically, for each of the resource selection run, they simply aggregated the resource selection scores of the resources within each vertical and did not set any thresholds on the number of selected verticals. Therefore, this resulted in the high recall and low precision of all their vertical selection runs.</p><p>of Padova (UPD) <ref type="bibr" coords="7,181.08,152.84,14.10,9.86" target="#b5">[6]</ref> The University of Padova's participation aimed at the investigation of the effectiveness of the TWF.IRF weighting algorithm in a Federated Web search setting. TWF.IRF, Term Weighted Frequency times Inverse Resource Frequency, is a recursive weighting scheme originally proposed for hybrid hierarchical peer-to-peer networks. The University of Padova looked into the influence of stemming and stopwords. Their results indicate that stemming has no significant effect on TWF.IRF effectiveness, and that overall the TWF.IRF approach is not highly effective for vertical selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESOURCE SELECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation</head><p>We report the nDCG@20 (primary metric), nDCG@10, nP@1 and nP@5 of the submitted resource selection runs in Table <ref type="table" coords="7,92.52,343.56,3.56,8.97" target="#tab_5">3</ref>. The primary evaluation metric is nDCG@20 (using the implementation of ndcg_cut.20 in trec_eval). The relevance of a resource for a given query is obtained by calculating the graded precision (see Section 3.2) on the top 10 results. These values are used as the nDCG gain values, for convenience with trec_eval scaled by a factor of 1000. Thus, this metric takes the ranking of resources into account and the graded relevance of the documents in the top 10 of each resource, but not the ranking of documents within the resources.</p><p>We also report nP@1 and nP@5 (normalized graded precision at k=1 and k=5 ). Introduced in the FedWeb 2013 track <ref type="bibr" coords="7,53.76,469.08,9.12,8.97" target="#b4">[5]</ref>, the normalized graded precision represents the graded precision of the top ranked k resources, normalized by the graded precision of the best possible k resources for the given topic. Compared to nDCG, this metrics ignores the ranking of the resources within the top k. For example, nP@1 denotes the graded precision of the highest ranked resource, divided by the highest graded precision by any of the resources for that topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Analysis</head><p>This year, 10 teams participated in the resource selection task, with a total of 44 runs. The four best performing runs based on nDCG@20 (ecomsvz, ecomsv, eseif and ecomsvt) were all submitted by East China Normal University (EC-NUCS). These runs only make use of result snippets, and their ranking strategies are based for an important part on the Search Engine Impact Factor. In addition, three of these runs (ecomsvz, ecomsv and ecomsvt) make use of external resources (Google Search, data from KDD 2005). Interestingly, their eseif run is a static, query-independent ranking based on data from the Fedweb TREC 2013 task. The top 5 resources of their static run are: Yahoo Screen, Yahoo Answers, AOL Video, Kidrex and Ask. The second team, info ruc, used query extension based on Google, and matched queries with resources, based on a topic model representation, whereby a snippet-based topic model proved consistently better than one based on full web documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Participant Approaches</head><p>East China Normal University (ECNUCS) <ref type="bibr" coords="7,505.08,116.11,19.62,9.86" target="#b9">[10]</ref> Their resource selection runs outperform the runs from other participants by a big margin. For their best run (ecomsvz), several techniques were combined to score resources for each query. The Search Engine Impact Factor (see ECNUCS' vertical selection submissions) has the biggest contribution to performance improvements, besides the vertical selection results, tf-idf features, and a semantic similarity score. The indivitual contributions from these methods are explored in the other submitted runs.</p><p>Renmin University of China (info_ruc) <ref type="bibr" coords="7,489.24,232.51,19.62,9.86" target="#b14">[15]</ref> The team info ruc used two different LDA topic distributions for its resource selection runs. For the runs FW14DocsX (X=50, 75, 100), they performed an LDA analysis over the whole set of sampled documents, after which the topic distribution of each resource was determined as the average distribution of its documents. For the runs FW14SearchX, they merged all sampled snippets into one big document, and used these to infer LDA topics from. X represents the number of topics used. Each query was expanded using the Google Search API, and its topic distribution vector was determined, after which the similarity between the query and resource representation was used to rank resources. The results show that all snippet based runs FW14SearchX outperform the sample documents based runs FW14DocsX, and resulted in the overall second best set of runs for this task (after the ECNUCS runs). For the snippets, 50 topics were the better choice, against 100 topics for the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chinese Academy of Sciences (ICTNET) [8]</head><p>ICTNET used various approaches for this task. For their first run (ICTNETRS01), they used a straightforward IR setup, based on indexing the provided sample documents, to score a resource, thereby giving more weight to higher ranked results. This run performed very low, but augmenting the method with the (highly successful) vertical selection results, resulted in a much better effectiveness (runs ICTNETRS02 and ICTNETRS07). Further runs use a text classification strategy (ICTNETRS03) and LSI (ICTNETRS04), including the resources' pagerank for the latter. These approaches are similar to the corresponding vertical selection approaches (including the query expansion part). ICTNET's most successful resource selection runs use the LSI model (with pagerank), together with the vertical selection results (ICTNETRS05 and ICTNETRS06).</p><p>Drexel University (dragon) <ref type="bibr" coords="7,438.48,611.72,19.62,9.86" target="#b15">[16]</ref> In total 7 runs were submitted and the aim was to evaluate a variety of existing resource selection approaches from the existing literatures, namely ReDDE, ReDDE.top, CRCSLinear, CRCSExp, CiSS, CiSSAprox, SUSHI. All those resource selection approaches are based on the central sampled index (CSI) while the differences of those approaches are how they reward each resource based on the retrieved documents from the CSI. Ultimately, they found that the SUSHI approach (drexelRS7) performed the best. University of Illinois (uiucGSLIS) <ref type="bibr" coords="9,204.72,56.24,19.62,9.86" target="#b13">[14]</ref> The team from Illinois submitted 2 runs. The first (uiucGSLISf1) ranks resources by their query clarity (defined as the KL-divergence between the query and collection language models). The second (uiucGLSISf2) uses the 'collection frequency -inverse document frequency' score, with slightly better results.</p><p>University of Delaware (udel) <ref type="bibr" coords="9,187.80,141.32,14.10,9.86" target="#b0">[1]</ref> The udel team selected resources for a particular query, based on their contribution to those 100 results that were ranked highest according to the query-likelyhood model for the given query. By repeating the experiment based on an index of snippets (with the run udelftrssn), and one based on sampled pages (udelftrsbs), the best performance was reached for the one based on full sampled pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University of Stavanger (NTNUiS) [2]</head><p>In the previous edition of the track, NTNUiS experimented with two approaches: Collection-Centric and Document-Centric models. This year, they explored learning to rank to combine these strategies. A learning to rank model trained on data from Fedweb'13 (run NTNUiSrs2) performed best. However, a model trained on data from both Fedweb'12 and Fedweb'13 performed worse, achieving even a lower performance than their baseline approach (NTNUiSrs1) that only uses a document-centric model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University of Twente (ut) [9]</head><p>The run UTTailyG2000 was based on the Taily system, originally designed for efficient shard selection for centralized search.</p><p>of Padova (UPD) <ref type="bibr" coords="9,181.08,406.75,14.10,9.86" target="#b5">[6]</ref> Besides vertical selection, the University of Padova also investigated the TWF.IRF scheme for resource selection. They showed that stemming has no significant influence on the effectiveness, whereas stop-word removal does improve the TWF.IRF ranking.</p><p>University of Lugano (ULugano) <ref type="bibr" coords="9,201.00,481.28,14.10,9.86" target="#b6">[7]</ref> Their resource selection runs followed approaches that combine relevance and opinion. The relevance of the resource were calculated by the ReDDE resource selection method on the sampled representation of the resources while the opinion mining was based on counting the number of sentiment terms (defined by the external resource SentiWordNet) appearing in documents of each resource. They ultimately submitted three runs, among which ULuganoDFR only utilized a traditional resource selection approach, whereas the other two runs (ULuganoColL2 and ULuganoDocL2) utilized different ways to re-rank based on opinions. However, in the experiments, the opinions do not seem to improve the resource selection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RESULTS MERGING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluation</head><p>An important new condition in the Results Merging task, as compared to the analogous FedWeb 2013 task, is the requirement that each Results Merging run had to be based on a particular Resource Selection run. More in particular, only results from the top 20 highest ranked resources in the selection run were allowed in the merging run. Additionally, participants were asked to submit at least one run based on the Resource Selection baseline run provided by the organizers. The evaluation results for the results merging task are shown in Table <ref type="table" coords="9,397.20,109.31,4.60,8.97" target="#tab_6">4</ref> (runs based on provided baseline) and Table <ref type="table" coords="9,345.24,119.75,4.60,8.97" target="#tab_7">5</ref> (runs based on participants own resource selection runs), displaying for a number of metrics the average per run over all topics.</p><p>Different evaluation measures are shown:</p><p>1. nDCG@20 (official RS metric), with the gain of duplicates set to zero (see below), and where the reference covers all results over all resources.</p><p>2. nDCG@100: analogous.</p><p>3. nDCG@20 dups: analogous to nDCG@20, but without penalizing duplicates.</p><p>4. nDCG@20 loc: again an nDCG@20 measure, with duplicate penalty, whereby all results not originating from the top 20 resources of the chosen selection run, are considered non-relevant.</p><p>5. nDCG-IA@20: intent-aware nDCG@20 (see <ref type="bibr" coords="9,535.44,305.16,13.54,8.97" target="#b18">[19]</ref>), again with duplicate penalty and possibly relevant results from all resources, where each vertical intent is weighted by the corresponding intent probability.</p><p>Penalizing duplicates means that after the first occurrence of a particular result in the merged list for a query, all consecutive results that refer to the same web page as that first result, receive the default relevance level of non-relevance. The goal of reporting the nDCG@20 loc measure is to allow comparing reranking strategies only, not influenced by the quality of the corresponding resource selection run, and where an ideal ranking leads to a value of 1. The other reported nDCG@20 values measure the total effectiveness of both the selection and the merging strategies. For ideal ranking, given a selection run, the highest possible value may well be below one, as the denominator can contain contributions from resources outside of the considered 20. The vertical intent probabilities for the nDCG-IA@20 measure are calculated as follows: (i) the quality of each vertical is quantified by the maximum score of the resource the vertical contains, where the score of each resource is measured by the graded precision of the top retrieved documents in the resource, and (ii) the vertical intent probability is obtained by normalizing the vertical score obtained in (i) across all the verticals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Analysis</head><p>The top 5 performing runs overall are by ICTNET (ICTNETRM06, ICTNETRM07, ICTNETRM04, ICTNETRM05, ICT-NETRM03). These runs were based on the official baseline, which the organizers has chosen as ICTNET's ICTNETRS06 run.</p><p>Interestingly, the higest ranked run ICTNETRM06 (according to the official metric) was obtained by removing duplicates from the already high-scoring run ICTNETRM05, with a resulting increase in nDCG@20 of 5%. Note that the score from ICTNETRM06 according to the official metric remains almost constant, compared to the metric nDCG@20 dups that does include the gain from duplicates, whereas ICTNETRM05 would be rated 14% higher. This Task 3: Results Merging Group ID Run ID nDCG@20 nDCG@100 nDCG@20 dups nDCG@20 loc nDCG@100 loc nDCG-IA@20 confirms the intuitive idea that among the highly relevant (and hence top ranked) results, there are many duplicates (most likely returned by different resources).</p><p>The teams SCUTKapok (SCUTKapok6, SCUTKapok7) and dragon (FW14basemR) perform well as well, based on variations on round robin merging, and normalizing document scores based on the resource selection results, respectively.</p><p>We further note that the ranking of all submitted runs based on the intent-aware metric nDCG-IA@20 highly correlates with the nDCG@20-based ranking (rank correlation ρ = 0.95). Also, despite the clear absolute benefit of removing duplicates (with regard to the official metric nDCG@20), the rank correlation between systems scored on nDCG@20 vs. nDCG@20 dups is high, too (ρ = 0.89). The metric nDCG@20 loc, only measuring the reranking capabilities of the proposed methods, independent of the quality of the underlying resource selection baseline, highly correlates with nDCG@20 as well (ρ = 0.91). It can also be observed that the correlation when comparing the rank order of runs for nDCG@20 with nDCG@100 is less strong (ρ = 0.66).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Participant Approaches</head><p>Chinese Academy of Sciences (ICTNET) <ref type="bibr" coords="11,233.64,294.20,14.10,9.86" target="#b7">[8]</ref> ICTNET proposed various methods for this task, as in the vertical selection and resource selection tasks. Their lowest performant run (ICTNETRM01) is based on IR heuristics, but they also submitted a variant with duplicates filtered out (ICTNETRM02), scoring significantly higher. They again used the resources' pagerank and the LSI model (runs ICTNETRM03 and ICTNETRM04). Their most successful runs however (also the overall best performing runs), were obtained by combining these methods using an ensemble method (ICTNETRM05, ICTNETRM06, ICTNETRM07), whereby the run without duplicates scores best (ICTNETRM06).</p><p>South China University of Technology (SCUTKapok) <ref type="bibr" coords="11,284.64,431.11,19.62,9.86" target="#b16">[17]</ref> The team from South China University of Technology has investigated various alterations to the basic round robin method, with significant improvements by taking into account the resource selection baseline, the verticals the resources belong to, and removing duplicates.</p><p>Drexel University (dragon) <ref type="bibr" coords="11,175.44,505.28,19.62,9.86" target="#b15">[16]</ref> Their result merging runs were based on normalizing the document score based on the resource score by a simple multiplication. The resource score was determined by the resource selection approach (based on either the raw score or the resource ranking position). On the other side, the document score was based on its reciprocal rank of the selected resource. Ultimately, the rank based resource score combined with the document score on the RS baseline provided by the FedWeb team performed the best (drexelRS7mW).</p><p>East China Normal University (ECNUCS) <ref type="bibr" coords="11,242.16,621.20,19.62,9.86" target="#b9">[10]</ref> The ECNUCS results merging run (basedef) simply returns the output of the official FedWeb resource selection baseline.</p><p>Carnegie Mellon University (CMU_LTI) <ref type="bibr" coords="11,234.48,664.04,19.62,9.86" target="#b12">[13]</ref> They only participated in the results merging task and submitted several runs based on the baseline. For their baseline run, they used language modeling with Dirichlet smoothing by indexing the search result snippets using the Indri toolkit. In addition, they experimented with a sequential dependence model (sdm5) where the similarity is not only based on individual terms, but also on bigrams (exact match and unordered window). They also explored query expansion using word-vector representations released by Google (googUniform7 and googTermWise7). While the SDM model performed best on the FedWeb13 dataset, the query expansion strategies performed slightly better on the FedWeb14 dataset.</p><p>University of Lugano (ULugano) <ref type="bibr" coords="11,464.04,159.08,14.10,9.86" target="#b6">[7]</ref> The four submitted runs were intended to experiment whether diversifying the final merged result list to cover different sentiments, namely positive, negative and neutral, would be helpful. Therefore, both relevance and opinion scores of documents were considered when conducting result merging and a retrieval-interpolated diversification approach was utilized. The differences of the four submitted runs were based on whether they included sentiment diversification or not, and which resource selection baseline they utilized. However, opinion diversification did not boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>In FedWeb 2014, the second and final edition of the TREC Federated Web Search Track, 12 teams participated in one or more of the challenges Vertical Selection, Resource Selection, and Results Merging, with a total of 106 submitted system runs. We introduced an online evaluation system for system preparations, which turned out a success and in our opinion led to an increased effort into composing wellperforming runs. This year's most effective methods are in general more complicated, as compared to the FedWeb 2013 submissions, with the appearance of a number of machine learning methods, besides more traditional information retrieval methods.</p><p>We discussed the creation of the FedWeb 2014 dataset and relevance judgments, analyzed the relevance distributions over the test topics and different verticals in our system of 149 online search engines, and for each of the main tasks, listed the performance of the submitted runs, as a set of several evaluation measures. With the individual descriptions of the participants' approaches, this overview paper also provides insights into which methods are best suited for the different tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. FEDWEB 2014 SEARCH ENGINES</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,316.80,247.40,239.21,8.08;3,316.80,257.96,20.82,8.08;3,316.83,53.92,238.91,179.33"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screen shot of the online evaluation system.</figDesc><graphic coords="3,316.83,53.92,238.91,179.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,81.36,608.24,446.94,8.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Highest graded relevance among all resources within a vertical, over all 50 test topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,317.28,709.44,121.11,9.97"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="2,111.96,57.60,122.79,264.57"><row><cell>Vertical</cell><cell># Resources</cell></row><row><cell>Academic</cell><cell>17</cell></row><row><cell>Video</cell><cell>11</cell></row><row><cell cols="2">Photo/Pictures 11</cell></row><row><cell>Health</cell><cell>11</cell></row><row><cell>Shopping</cell><cell>10</cell></row><row><cell>News</cell><cell>10</cell></row><row><cell>General</cell><cell>8</cell></row><row><cell>Encyclopedia</cell><cell>8</cell></row><row><cell>Sports</cell><cell>7</cell></row><row><cell>Kids</cell><cell>7</cell></row><row><cell>Q&amp;A</cell><cell>6</cell></row><row><cell>Games</cell><cell>6</cell></row><row><cell>Tech</cell><cell>5</cell></row><row><cell>Recipes</cell><cell>5</cell></row><row><cell>Jobs</cell><cell>5</cell></row><row><cell>Blogs</cell><cell>4</cell></row><row><cell>Software</cell><cell>3</cell></row><row><cell>Social</cell><cell>3</cell></row><row><cell>Entertainment</cell><cell>3</cell></row><row><cell>Travel</cell><cell>2</cell></row><row><cell>Jokes</cell><cell>2</cell></row><row><cell>Books</cell><cell>2</cell></row><row><cell>Audio</cell><cell>2</cell></row><row><cell>Local</cell><cell>1</cell></row></table><note coords="1,317.28,709.44,3.65,5.30;1,321.36,711.78,117.03,7.62;2,154.08,336.20,80.38,8.08;2,53.76,363.96,239.20,8.97;2,53.76,374.40,205.48,8.97"><p>1 http://snipdex.org/fedweb Vertical statistics task and the results merging task, respectively; Section 8 gives a summary of this year's track main findings.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.80,689.64,239.08,29.97"><head></head><label></label><figDesc>The four best performing runs based on the F-measure (ICTNETVS07, esevsru, esevs and ICTNETVS02) were submitted by East China Normal</figDesc><table coords="4,65.41,66.68,477.31,514.19"><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>7252</cell><cell>7092</cell><cell>7176</cell><cell>7111</cell><cell>7211</cell><cell>7261</cell><cell>7044</cell><cell>7307</cell><cell>7299</cell><cell>7137</cell><cell>7441</cell><cell>7236</cell><cell>7197</cell><cell>7274</cell><cell>7250</cell><cell>7491</cell><cell>7486</cell><cell>7448</cell><cell>7215</cell><cell>7293</cell><cell>7239</cell><cell>7431</cell><cell>7235</cell><cell>7123</cell><cell>7205</cell><cell>7242</cell><cell>7207</cell><cell>7045</cell><cell>7263</cell><cell>7328</cell><cell>7161</cell><cell>7185</cell><cell>7303</cell><cell>7249</cell><cell>7167</cell><cell>7194</cell><cell>7320</cell><cell cols="2">7216</cell><cell>7146</cell><cell>7174</cell><cell>7173</cell><cell>7015</cell><cell>7501</cell><cell>7212</cell><cell>7072</cell><cell>7265</cell><cell>7200</cell><cell>7326</cell><cell>7230</cell><cell>7222</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>grad. prec. of best resource</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell cols="2">General</cell><cell cols="2">Blogs</cell><cell cols="2">Kids</cell><cell cols="2">Video</cell><cell cols="2">Encyclopedia</cell><cell cols="2">Q&amp;A</cell><cell cols="2">Photo/Pictures</cell><cell cols="3">Shopping</cell><cell cols="2">Books</cell><cell cols="2">News</cell><cell cols="2">Social</cell><cell cols="2">Academic</cell><cell cols="2">Health</cell><cell cols="2">Tech</cell><cell cols="2">Recipes</cell><cell cols="2">Travel</cell><cell cols="2">Entertainment</cell><cell cols="3">Software</cell><cell cols="3">Sports</cell><cell cols="2">Audio</cell><cell cols="2">Games</cell><cell cols="2">Jobs</cell><cell cols="2">Local</cell><cell>Jokes</cell></row></table><note coords="4,66.89,187.09,8.48,21.59;4,66.89,164.02,8.48,20.02;4,66.89,153.38,8.48,7.47;4,66.89,133.56,8.48,17.27;4,66.89,95.97,8.48,35.18;4,119.52,285.32,370.62,8.08"><p><p>grad. prec. of best resource</p>Figure 1: Graded relevance of the best resource per topic, for all 50 test topics.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,59.16,191.76,503.34,393.25"><head>Table 2 :</head><label>2</label><figDesc>Results for the Vertical Selection task.</figDesc><table coords="6,59.16,191.76,503.34,371.13"><row><cell cols="2">Task 2: Vertical Selection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Group ID Run ID</cell><cell cols="4">Precision Recall F-measure Resources Used</cell></row><row><cell></cell><cell>ekwma</cell><cell>0.054</cell><cell>0.120</cell><cell>0.069</cell><cell>snippets, wordnet</cell></row><row><cell></cell><cell>esevs</cell><cell>0.398</cell><cell>0.586</cell><cell>0.438</cell><cell>snippets, trec 2013 dataset, kdd 2005</cell></row><row><cell>ECNUCS</cell><cell>esevsru</cell><cell>0.388</cell><cell>0.598</cell><cell>0.440</cell><cell>snippets, trec 2013 dataset, kdd 2005</cell></row><row><cell></cell><cell>esvru</cell><cell>0.276</cell><cell>0.439</cell><cell>0.297</cell><cell>snippets, kdd 2005, google search</cell></row><row><cell></cell><cell>svmtrain</cell><cell>0.338</cell><cell>0.425</cell><cell>0.338</cell><cell>snippets, kdd 2005, google search</cell></row><row><cell></cell><cell>ICTNETVS02</cell><cell>0.292</cell><cell>0.790</cell><cell>0.401</cell><cell>documents, Google API, WEKA</cell></row><row><cell></cell><cell>ICTNETVS03</cell><cell>0.276</cell><cell>0.410</cell><cell>0.298</cell><cell>snippets, documents, Google API, NLTK, GENSIM</cell></row><row><cell></cell><cell>ICTNETVS04</cell><cell>0.427</cell><cell>0.392</cell><cell>0.377</cell><cell>snippets, documents, Google API, NLTK, GENSIM, WEKA</cell></row><row><cell>ICTNET</cell><cell>ICTNETVS05</cell><cell>0.423</cell><cell>0.365</cell><cell>0.359</cell><cell>snippets, documents, Google API, NLTK, GENSIM, WEKA</cell></row><row><cell></cell><cell>ICTNETVS06</cell><cell>0.258</cell><cell>0.673</cell><cell>0.344</cell><cell>documents, Google API, WEKA</cell></row><row><cell></cell><cell>ICTNETVS07</cell><cell>0.591</cell><cell>0.545</cell><cell>0.496</cell><cell>documents</cell></row><row><cell></cell><cell>ICTNETVS1</cell><cell>0.230</cell><cell>0.638</cell><cell>0.299</cell><cell>snippets, documents</cell></row><row><cell>NTNUiS</cell><cell>NTNUiSvs2 NTNUiSvs3</cell><cell>0.157 0.145</cell><cell>0.406 0.281</cell><cell>0.205 0.177</cell><cell>snippets, documents snippets, documents</cell></row><row><cell></cell><cell>ULuganoCL2V</cell><cell>0.117</cell><cell>0.983</cell><cell>0.197</cell><cell>documents, SentiWordNet Lexicon</cell></row><row><cell>ULugano</cell><cell>ULuganoDFRV</cell><cell>0.117</cell><cell>0.983</cell><cell>0.197</cell><cell>documents</cell></row><row><cell></cell><cell>ULuganoDL2V</cell><cell>0.117</cell><cell>0.983</cell><cell>0.197</cell><cell>documents, SentiWordNet Lexicon</cell></row><row><cell></cell><cell>UPDFW14v0knm</cell><cell>0.076</cell><cell>1.000</cell><cell>0.138</cell><cell>documents</cell></row><row><cell></cell><cell>UPDFW14v0nnm</cell><cell>0.076</cell><cell>1.000</cell><cell>0.138</cell><cell>documents</cell></row><row><cell>UPD</cell><cell>UPDFW14v0pnm UPDFW14v1knm</cell><cell>0.076 0.076</cell><cell>1.000 1.000</cell><cell>0.138 0.138</cell><cell>documents documents</cell></row><row><cell></cell><cell>UPDFW14v1nnm</cell><cell>0.076</cell><cell>1.000</cell><cell>0.138</cell><cell>documents</cell></row><row><cell></cell><cell>UPDFW14v1pnm</cell><cell>0.076</cell><cell>1.000</cell><cell>0.138</cell><cell>documents</cell></row><row><cell></cell><cell>drexelVS1</cell><cell>0.240</cell><cell>0.506</cell><cell>0.284</cell><cell>documents</cell></row><row><cell></cell><cell>drexelVS2</cell><cell>0.159</cell><cell>0.824</cell><cell>0.233</cell><cell>documents</cell></row><row><cell></cell><cell>drexelVS3</cell><cell>0.134</cell><cell>0.960</cell><cell>0.212</cell><cell>documents</cell></row><row><cell>dragon</cell><cell>drexelVS4</cell><cell>0.134</cell><cell>0.960</cell><cell>0.212</cell><cell>documents</cell></row><row><cell></cell><cell>drexelVS5</cell><cell>0.163</cell><cell>0.824</cell><cell>0.244</cell><cell>documents</cell></row><row><cell></cell><cell>drexelVS6</cell><cell>0.171</cell><cell>0.729</cell><cell>0.251</cell><cell>documents</cell></row><row><cell></cell><cell>drexelVS7</cell><cell>0.189</cell><cell>0.732</cell><cell>0.271</cell><cell>documents</cell></row><row><cell>udel</cell><cell>udelftvql udelftvqlR</cell><cell>0.167 0.236</cell><cell>0.852 0.680</cell><cell>0.257 0.328</cell><cell>documents documents</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,59.16,115.56,512.34,545.65"><head>Table 3 :</head><label>3</label><figDesc>Results for the Resource Selection task.</figDesc><table coords="8,59.16,115.56,512.34,523.53"><row><cell cols="2">Task 1: Resource Selection</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Group ID</cell><cell>Run ID</cell><cell cols="3">nDCG@20 nDCG@10 nP@1 nP@5 resources used</cell></row><row><cell></cell><cell>ecomsv</cell><cell>0.700</cell><cell>0.601</cell><cell>0.525 0.579 snippets, Google search, KDD 2005</cell></row><row><cell></cell><cell>ecomsvt</cell><cell>0.626</cell><cell>0.506</cell><cell>0.273 0.491 snippets, Google search, KDD 2005</cell></row><row><cell>ECNUCS</cell><cell>ecomsvz eseif</cell><cell>0.712 0.651</cell><cell>0.624 0.623</cell><cell>0.535 0.604 snippets, Google search, KDD 2005 0.306 0.546 snippets</cell></row><row><cell></cell><cell>esmimax</cell><cell>0.299</cell><cell>0.261</cell><cell>0.222 0.265 snippets, Google search</cell></row><row><cell></cell><cell>etfidf</cell><cell>0.157</cell><cell>0.113</cell><cell>0.093 0.113 snippets</cell></row><row><cell></cell><cell>ICTNETRS01</cell><cell>0.268</cell><cell>0.226</cell><cell>0.163 0.193 documents</cell></row><row><cell></cell><cell>ICTNETRS02</cell><cell>0.365</cell><cell>0.322</cell><cell>0.289 0.324 Google API, NLTK, GENSIM</cell></row><row><cell></cell><cell>ICTNETRS03</cell><cell>0.400</cell><cell>0.340</cell><cell>0.160 0.351 documents, Google API, NLTK, GENSIM, WEKA</cell></row><row><cell>ICTNET</cell><cell>ICTNETRS04</cell><cell>0.362</cell><cell>0.306</cell><cell>0.116 0.290 documents, Google API, NLTK, GENSIM</cell></row><row><cell></cell><cell>ICTNETRS05</cell><cell>0.436</cell><cell>0.391</cell><cell>0.489 0.377 documents, Google API, NLTK, GENSIM</cell></row><row><cell></cell><cell>ICTNETRS06</cell><cell>0.428</cell><cell>0.372</cell><cell>0.521 0.345 documents, Google API, NLTK, GENSIM</cell></row><row><cell></cell><cell>ICTNETRS07</cell><cell>0.373</cell><cell>0.334</cell><cell>0.267 0.334 documents, Google API, NLTK, GENSIM</cell></row><row><cell></cell><cell>NTNUiSrs1</cell><cell>0.306</cell><cell>0.225</cell><cell>0.148 0.195 documents</cell></row><row><cell>NTNUiS</cell><cell>NTNUiSrs2</cell><cell>0.348</cell><cell>0.281</cell><cell>0.206 0.257 snippets, documents</cell></row><row><cell></cell><cell>NTNUiSrs3</cell><cell>0.248</cell><cell>0.205</cell><cell>0.202 0.189 snippets, documents</cell></row><row><cell></cell><cell>ULuganoColL2</cell><cell>0.297</cell><cell>0.189</cell><cell>0.148 0.158 documents, SentiWordNet</cell></row><row><cell>ULugano</cell><cell>ULuganoDFR</cell><cell>0.304</cell><cell>0.193</cell><cell>0.137 0.164 documents</cell></row><row><cell></cell><cell>ULuganoDocL2</cell><cell>0.301</cell><cell>0.193</cell><cell>0.137 0.160 documents, SentiWordNet</cell></row><row><cell></cell><cell>UPDFW14r1ksm</cell><cell>0.292</cell><cell>0.209</cell><cell>0.148 0.180 documents</cell></row><row><cell></cell><cell>UPDFW14tiknm</cell><cell>0.278</cell><cell>0.209</cell><cell>0.118 0.191 documents</cell></row><row><cell></cell><cell>UPDFW14tiksm</cell><cell>0.310</cell><cell>0.223</cell><cell>0.126 0.188 documents</cell></row><row><cell>UPD</cell><cell>UPDFW14tinnm</cell><cell>0.281</cell><cell>0.212</cell><cell>0.134 0.201 snippets, documents</cell></row><row><cell></cell><cell>UPDFW14tinsm</cell><cell>0.306</cell><cell>0.221</cell><cell>0.153 0.197 documents</cell></row><row><cell></cell><cell>UPDFW14tipnm</cell><cell>0.280</cell><cell>0.212</cell><cell>0.115 0.191 snippets, documents</cell></row><row><cell></cell><cell>UPDFW14tipsm</cell><cell>0.311</cell><cell>0.226</cell><cell>0.123 0.187 documents</cell></row><row><cell></cell><cell>drexelRS1</cell><cell>0.389</cell><cell>0.348</cell><cell>0.222 0.318 documents</cell></row><row><cell></cell><cell>drexelRS2</cell><cell>0.328</cell><cell>0.227</cell><cell>0.125 0.180 documents</cell></row><row><cell></cell><cell>drexelRS3</cell><cell>0.333</cell><cell>0.229</cell><cell>0.125 0.179 documents</cell></row><row><cell>dragon</cell><cell>drexelRS4</cell><cell>0.333</cell><cell>0.229</cell><cell>0.125 0.180 documents</cell></row><row><cell></cell><cell>drexelRS5</cell><cell>0.342</cell><cell>0.241</cell><cell>0.135 0.211 documents</cell></row><row><cell></cell><cell>drexelRS6</cell><cell>0.382</cell><cell>0.284</cell><cell>0.201 0.250 documents</cell></row><row><cell></cell><cell>drexelRS7</cell><cell>0.422</cell><cell>0.359</cell><cell>0.293 0.314 documents</cell></row><row><cell></cell><cell>FW14Docs100</cell><cell>0.444</cell><cell>0.337</cell><cell>0.165 0.239 documents</cell></row><row><cell></cell><cell>FW14Docs50</cell><cell>0.419</cell><cell>0.292</cell><cell>0.174 0.203 documents, Google API</cell></row><row><cell>info ruc</cell><cell>FW14Docs75 FW14Search100</cell><cell>0.422 0.505</cell><cell>0.306 0.425</cell><cell>0.106 0.198 documents, Google API 0.278 0.384 snippets, Google API</cell></row><row><cell></cell><cell>FW14Search50</cell><cell>0.517</cell><cell>0.426</cell><cell>0.271 0.404 snippets, Google API</cell></row><row><cell></cell><cell>FW14Search75</cell><cell>0.461</cell><cell>0.366</cell><cell>0.256 0.345 snippets, Google API</cell></row><row><cell>udel</cell><cell>udelftrsbs udelftrssn</cell><cell>0.355 0.216</cell><cell>0.272 0.174</cell><cell>0.166 0.255 documents 0.147 0.149 snippets</cell></row><row><cell>uiucGSLIS</cell><cell>uiucGSLISf1 uiucGSLISf2</cell><cell>0.348 0.361</cell><cell>0.249 0.274</cell><cell>0.101 0.212 documents 0.179 0.213 documents</cell></row><row><cell>ut</cell><cell>UTTailyG2000</cell><cell>0.323</cell><cell>0.251</cell><cell>0.143 0.224 documents</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,59.16,190.44,506.80,396.21"><head>Table 4 :</head><label>4</label><figDesc>Results for the Results Merging task based on the official baseline run.</figDesc><table coords="10,59.16,190.44,506.80,396.21"><row><cell></cell><cell></cell><cell cols="2">googTermWise7</cell><cell>0.286</cell><cell>0.319</cell><cell>0.320</cell><cell>0.395</cell><cell>0.632</cell><cell>0.102</cell></row><row><cell>CMU LTI</cell><cell></cell><cell cols="2">googUniform7 plain</cell><cell>0.285 0.277</cell><cell>0.318 0.316</cell><cell>0.322 0.312</cell><cell>0.389 0.379</cell><cell>0.628 0.623</cell><cell>0.101 0.098</cell></row><row><cell></cell><cell></cell><cell>sdm5</cell><cell></cell><cell>0.276</cell><cell>0.315</cell><cell>0.315</cell><cell>0.379</cell><cell>0.623</cell><cell>0.096</cell></row><row><cell>ECNUCS</cell><cell></cell><cell>basedef</cell><cell></cell><cell>0.289</cell><cell>0.300</cell><cell>0.336</cell><cell>0.397</cell><cell>0.593</cell><cell>0.095</cell></row><row><cell></cell><cell></cell><cell>ICTNETRM01</cell><cell></cell><cell>0.247</cell><cell>0.307</cell><cell>0.361</cell><cell>0.338</cell><cell>0.599</cell><cell>0.080</cell></row><row><cell></cell><cell></cell><cell>ICTNETRM02</cell><cell></cell><cell>0.309</cell><cell>0.305</cell><cell>0.314</cell><cell>0.362</cell><cell>0.512</cell><cell>0.095</cell></row><row><cell>ICTNET</cell><cell></cell><cell>ICTNETRM03 ICTNETRM04</cell><cell></cell><cell>0.348 0.381</cell><cell>0.311 0.271</cell><cell>0.350 0.386</cell><cell>0.405 0.451</cell><cell>0.522 0.456</cell><cell>0.111 0.121</cell></row><row><cell></cell><cell></cell><cell>ICTNETRM05</cell><cell></cell><cell>0.354</cell><cell>0.354</cell><cell>0.492</cell><cell>0.497</cell><cell>0.706</cell><cell>0.123</cell></row><row><cell></cell><cell></cell><cell>ICTNETRM06</cell><cell></cell><cell>0.402</cell><cell>0.338</cell><cell>0.407</cell><cell>0.473</cell><cell>0.571</cell><cell>0.132</cell></row><row><cell></cell><cell></cell><cell>ICTNETRM07</cell><cell></cell><cell>0.386</cell><cell>0.331</cell><cell>0.390</cell><cell>0.451</cell><cell>0.557</cell><cell>0.123</cell></row><row><cell></cell><cell></cell><cell>SCUTKapok1</cell><cell></cell><cell>0.313</cell><cell>0.293</cell><cell>0.316</cell><cell>0.367</cell><cell>0.492</cell><cell>0.097</cell></row><row><cell></cell><cell></cell><cell>SCUTKapok2</cell><cell></cell><cell>0.319</cell><cell>0.316</cell><cell>0.361</cell><cell>0.442</cell><cell>0.624</cell><cell>0.106</cell></row><row><cell></cell><cell></cell><cell>SCUTKapok3</cell><cell></cell><cell>0.314</cell><cell>0.294</cell><cell>0.317</cell><cell>0.367</cell><cell>0.491</cell><cell>0.097</cell></row><row><cell cols="2">SCUTKapok</cell><cell>SCUTKapok4</cell><cell></cell><cell>0.318</cell><cell>0.299</cell><cell>0.320</cell><cell>0.370</cell><cell>0.497</cell><cell>0.099</cell></row><row><cell></cell><cell></cell><cell>SCUTKapok5</cell><cell></cell><cell>0.320</cell><cell>0.321</cell><cell>0.344</cell><cell>0.442</cell><cell>0.629</cell><cell>0.102</cell></row><row><cell></cell><cell></cell><cell>SCUTKapok6</cell><cell></cell><cell>0.323</cell><cell>0.298</cell><cell>0.325</cell><cell>0.377</cell><cell>0.497</cell><cell>0.101</cell></row><row><cell></cell><cell></cell><cell>SCUTKapok7</cell><cell></cell><cell>0.322</cell><cell>0.320</cell><cell>0.361</cell><cell>0.446</cell><cell>0.627</cell><cell>0.107</cell></row><row><cell>ULugano</cell><cell></cell><cell cols="2">ULugFWBsNoOp ULugFWBsOp</cell><cell>0.251 0.224</cell><cell>0.296 0.273</cell><cell>0.304 0.271</cell><cell>0.355 0.314</cell><cell>0.588 0.545</cell><cell>0.083 0.072</cell></row><row><cell>dragon</cell><cell></cell><cell>FW14basemR FW14basemW</cell><cell></cell><cell>0.322 0.260</cell><cell>0.318 0.298</cell><cell>0.361 0.312</cell><cell>0.446 0.367</cell><cell>0.626 0.592</cell><cell>0.107 0.086</cell></row><row><cell cols="3">Task 3: Results Merging</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Group ID Run ID</cell><cell cols="6">nDCG@20 nDCG@100 nDCG@20 dups nDCG@20 loc nDCG@100 loc nDCG-IA@20</cell></row><row><cell>ULugano</cell><cell cols="2">ULugDFRNoOp ULugDFROp</cell><cell>0.156 0.146</cell><cell>0.204 0.195</cell><cell>0.157 0.149</cell><cell>0.193 0.180</cell><cell>0.362 0.346</cell><cell>0.035 0.033</cell></row><row><cell></cell><cell cols="2">drexelRS1mR</cell><cell>0.219</cell><cell>0.298</cell><cell>0.222</cell><cell>0.264</cell><cell>0.491</cell><cell>0.059</cell></row><row><cell></cell><cell cols="2">drexelRS4mW</cell><cell>0.144</cell><cell>0.244</cell><cell>0.148</cell><cell>0.177</cell><cell>0.420</cell><cell>0.036</cell></row><row><cell>dragon</cell><cell cols="2">drexelRS6mR</cell><cell>0.198</cell><cell>0.270</cell><cell>0.194</cell><cell>0.232</cell><cell>0.443</cell><cell>0.050</cell></row><row><cell></cell><cell cols="2">drexelRS6mW</cell><cell>0.196</cell><cell>0.270</cell><cell>0.193</cell><cell>0.231</cell><cell>0.444</cell><cell>0.049</cell></row><row><cell></cell><cell cols="2">drexelRS7mW</cell><cell>0.250</cell><cell>0.305</cell><cell>0.249</cell><cell>0.318</cell><cell>0.535</cell><cell>0.070</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,109.32,600.68,391.02,8.08"><head>Table 5 :</head><label>5</label><figDesc>Results for the Results Merging task not based on the official baseline run.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,321.36,702.78,211.23,7.62;2,316.80,711.78,66.51,7.62"><p>http://research.microsoft.com/en-us/projects/ trec-web-2013/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">ACKNOWLEDGMENTS</head><p>This work was funded by the <rs type="funder">Folktales As Classifiable Texts (FACT)</rs> project in The Netherlands, by the <rs type="funder">Dutch</rs> national project <rs type="projectName">COMMIT</rs>, and by <rs type="funder">Ghent University -iMinds in Belgium</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_y9JQWS7">
					<orgName type="project" subtype="full">COMMIT</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,335.64,635.39,210.88,8.97;11,335.64,645.83,205.10,8.97;11,335.64,656.27,167.32,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,335.64,645.83,149.96,8.97">University of delaware at TREC 2014</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sabhnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zengin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,504.48,646.14,36.26,8.65;11,335.64,656.58,138.27,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,335.64,667.79,213.07,8.97;11,335.64,678.23,209.07,8.97;11,335.64,688.67,60.03,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,376.92,667.79,171.79,8.97;11,335.64,678.23,47.19,8.97">NTNUiS at the TREC 2014 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.16,678.54,143.55,8.65;11,335.64,688.98,30.99,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,335.64,700.07,201.98,8.97;11,335.64,710.63,215.31,8.97;12,72.60,57.30,191.56,8.65;12,72.60,67.56,69.64,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,421.44,700.07,116.18,8.97;11,335.64,710.63,124.90,8.97">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,479.16,710.94,71.79,8.65;12,72.60,57.30,191.56,8.65;12,72.60,67.56,42.09,8.97">Proceedings of the 7th International World Wide Web Conference, WWW &apos;98</title>
		<meeting>the 7th International World Wide Web Conference, WWW &apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,78.96,192.15,8.97;12,72.60,89.40,196.20,8.97;12,72.60,99.84,176.11,8.97;12,72.60,110.40,220.29,8.97;12,72.60,121.14,204.02,8.65;12,72.60,131.28,198.39,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,207.48,89.40,61.32,8.97;12,72.60,99.84,176.11,8.97;12,72.60,110.40,89.64,8.97">Exploiting user disagreement for web search evaluation: An experimental approach</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,180.96,110.70,111.93,8.65;12,72.60,121.14,204.02,8.65;12,72.60,131.58,89.23,8.65">Proceedings of the 7th ACM International Conference on Web Search and Data Mining (WSDM 2014)</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining (WSDM 2014)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,142.68,187.27,8.97;12,72.60,153.24,217.63,8.97;12,72.60,163.68,210.75,8.97;12,72.60,174.12,82.12,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,127.56,153.24,162.67,8.97;12,72.60,163.68,47.19,8.97">Overview of the trec 2013 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,138.12,163.98,145.23,8.65;12,72.60,174.42,53.95,8.65">The 22nd Text Retrieval Conference (TREC 2013)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,185.52,217.18,8.97;12,72.60,195.96,217.46,8.97;12,72.60,206.52,167.31,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,195.72,185.52,94.06,8.97;12,72.60,195.96,162.87,8.97">University of padova at TREC 2014: Federated web search track</title>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Buccio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Melucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,253.92,196.26,36.14,8.65;12,72.60,206.82,138.27,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,217.92,220.27,8.97;12,72.60,228.36,215.08,8.97;12,72.60,238.80,220.15,8.97;12,72.60,249.36,107.68,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,246.48,217.92,46.39,8.97;12,72.60,228.36,215.08,8.97;12,72.60,238.80,105.99,8.97">Opinions in federated search: University of lugano at TREC 2014 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,196.92,239.10,95.83,8.65;12,72.60,249.66,78.63,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,260.76,187.87,8.97;12,72.60,271.20,201.78,8.97;12,72.60,281.63,218.55,8.97;12,72.60,292.20,20.80,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,116.28,271.20,158.10,8.97;12,72.60,281.63,16.64,8.97">ICTNET at federated web search track 2014</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,108.12,281.94,177.87,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,303.60,215.32,8.97;12,72.60,314.03,217.47,8.97;12,72.60,324.48,188.20,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,174.48,303.60,113.44,8.97;12,72.60,314.03,183.50,8.97">U. twente at trec 2014 -two selfless contributions to web search evaluation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,274.56,314.34,15.51,8.65;12,72.60,324.78,159.15,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,335.99,218.95,8.97;12,72.60,346.43,209.83,8.97;12,72.60,356.87,201.66,8.97;12,72.60,367.31,146.67,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,151.80,335.99,139.75,8.97;12,72.60,346.43,209.83,8.97;12,72.60,356.87,126.08,8.97">Simple may be best -a simple and effective method for federated web search via search engine impact factor estimation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,217.32,357.18,56.94,8.65;12,72.60,367.62,117.63,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,378.83,181.87,8.97;12,72.60,389.27,209.45,8.97;12,72.60,400.02,202.22,8.65;12,72.60,410.15,145.23,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,198.96,378.83,55.51,8.97;12,72.60,389.27,160.86,8.97">Using Graded Relevance Assessments in IR Evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,240.72,389.58,41.34,8.65;12,72.60,400.02,202.22,8.65;12,72.60,410.46,41.92,8.65">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,421.67,204.75,8.97;12,72.60,432.11,195.03,8.97;12,72.60,442.55,68.31,8.97" xml:id="b11">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,190.68,421.67,86.68,8.97;12,72.60,432.11,95.61,8.97">Kdd cup-2005 report: Facing a great challenge</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,454.07,209.43,8.97;12,72.60,464.51,184.99,8.97;12,72.60,474.95,107.68,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,190.92,454.07,91.11,8.97;12,72.60,464.51,70.09,8.97">Query transformations for result merging</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palakodety</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,161.64,464.82,95.95,8.65;12,72.60,475.26,78.63,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,486.47,213.42,8.97;12,72.60,496.91,213.19,8.97;12,72.60,507.35,202.75,8.97;12,72.60,517.79,107.68,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,227.52,486.47,58.50,8.97;12,72.60,496.91,213.19,8.97;12,72.60,507.35,88.04,8.97">The university of illinois&apos; graduate school of library and information science at TREC 2014</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Willis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,179.52,507.66,95.83,8.65;12,72.60,518.10,78.63,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,529.31,209.56,8.97;12,72.60,539.75,215.22,8.97;12,72.60,550.19,146.67,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,197.64,529.31,84.52,8.97;12,72.60,539.75,139.21,8.97">RUC at TREC 2014: Select resources using topic models</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,230.88,540.06,56.94,8.65;12,72.60,550.50,117.63,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,561.59,212.11,8.97;12,72.60,572.15,180.07,8.97;12,72.60,582.59,107.67,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,156.12,561.59,128.59,8.97;12,72.60,572.15,65.79,8.97">Drexel at TREC 2014 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,156.84,572.46,95.83,8.65;12,72.60,582.90,78.63,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,593.99,213.42,8.97;12,72.60,604.43,206.78,8.97;12,72.60,614.99,167.31,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,234.12,593.99,51.90,8.97;12,72.60,604.43,152.19,8.97">SCUTKapok at the TREC 2014 federated web task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,243.24,604.74,36.14,8.65;12,72.60,615.30,138.27,8.65">The 23rd Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,626.39,217.15,8.97;12,72.60,636.83,211.22,8.97;12,72.60,647.27,219.13,8.97;12,72.60,658.14,199.17,8.65;12,72.60,668.27,48.99,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,136.44,636.83,147.38,8.97;12,72.60,647.27,62.89,8.97">Aligning vertical collection relevance with user intent</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,153.72,647.58,138.01,8.65;12,72.60,658.14,199.17,8.65;12,72.60,668.58,20.83,8.65">ACM International Conference on Information and Knowledge Management (CIKM 2014)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.60,679.67,197.71,8.97;12,72.60,690.11,202.02,8.97;12,72.60,700.67,202.51,8.97;12,335.64,57.30,172.71,8.65;12,335.64,67.55,136.00,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,119.64,690.11,154.98,8.97;12,72.60,700.67,106.62,8.97">On the Reliability and Intuitiveness of Aggregated Search Metrics</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,198.00,700.98,77.11,8.65;12,335.64,57.30,172.71,8.65;12,335.64,67.86,107.83,8.65">ACM International Conference on Information and Knowledge Management (CIKM 2013)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
