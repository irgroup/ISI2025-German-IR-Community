<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.76,115.96,313.83,12.62;1,256.58,133.89,102.19,12.62">NovaSearch at TREC 2014 Clinical Decision Support Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.53,171.56,61.29,8.74"><forename type="first">André</forename><surname>Mourão</surname></persName>
							<email>a.mourao@campus.fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia Universidade Nova de Lisboa Caparica</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.32,171.56,64.71,8.74"><forename type="first">Flávio</forename><surname>Martins</surname></persName>
							<email>flaviomartins@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia Universidade Nova de Lisboa Caparica</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.72,171.56,69.11,8.74"><forename type="first">João</forename><surname>Magalhães</surname></persName>
							<email>jm.magalhaes@fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia Universidade Nova de Lisboa Caparica</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.76,115.96,313.83,12.62;1,256.58,133.89,102.19,12.62">NovaSearch at TREC 2014 Clinical Decision Support Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">65FA894A80CAA36C67FBA80BE083A605</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the NovaSearch group at TREC Clinical Decision Support 2014. As this is the first edition of the track, we decided to assess the performance of multiple information retrieval techniques: retrieval functions, re-ranking, query expansion and classification of medical articles into question categories. The best performing run was based on an ensemble of state-of-the-art retrieval algorithms combined with unsupervised fusion. Our best run was based on the late fusion of runs using MeSH query expansion, pseudo-relevance feedback with terms from top retrieved results and multiple retrieval functions (BM25L, BM25+, TF-IDF and Language Models) combined with RRF fusion algorithm. We also tested an algorithm to measure article relevance to the target medical questions (diagnosis, test and treatment articles), based on the frequency of words to some categories. An additional experiment was based on pseudo relevance feedback based on each article's journal reputation. Although some techniques did not increase our baseline performance, we are satisfied with our global performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC Clinical Decision Support Track is a new track focused on the "retrieval of biomedical articles relevant for answering generic clinical questions about medical records." <ref type="foot" coords="1,209.91,528.72,3.97,6.12" target="#foot_0">1</ref> These medical records consist on cases summarizing patients medical records and conditions, presented on well-formed natural English text. For each record, there are two formulations: descriptions, containing a detailed account of the patients' visits and summaries, simplified versions of the descriptions with less irrelevant and negative information.</p><p>Our participation on this track follows our work on the textual component of our ImageCLEF Med 2013 system <ref type="bibr" coords="1,297.62,602.02,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,309.80,602.02,7.01,8.74" target="#b3">4]</ref>, adapted to answer the specific types of questions on this track. Section 2 details our usage of general and domain specific IR techniques. Section 3 describes how we measured article relevance to the Generic Clinical Question types. Section 4 describes our pseudo-relevance feedback algorithm based on journal reputation. Section 5 contains the results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Medical retrieval system</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text indexing and retrieval</head><p>This section summarizes the indexing and retrieval techniques applied in our medical retrieval system. For a more detailed explanation, see the text and fusion sections of <ref type="bibr" coords="2,185.56,228.47,9.96,8.74" target="#b3">[4]</ref>. The retrieval function BM25L <ref type="bibr" coords="2,342.09,228.47,10.52,8.74" target="#b0">[1]</ref> is our systems baseline. We experimentally found it to be the best model in the medical domain <ref type="bibr" coords="2,454.08,240.43,9.96,8.74" target="#b1">[2]</ref>, in particular because it handles long documents (i.e. article full text) better than other retrieval functions. We indexed and searched the full document text (all chapters including image captions), abstract and title. We ran pseudo-relevance feedback using the top 3 results retrieved using the initial query. We added a maximum of 25 new query terms to the initial query.</p><p>At query time, we expanded the initial query with preferred and alternative terms sourced from a SKOS formatted version of MeSH using Lucene-SKOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rank fusion</head><p>One of the hypothesis we wanted to test was that some retrieval functions can give better results for certain queries, and that an unsupervised combination of multiple functions can improve the results from individual functions. Rank fusion aims at combining ranked document lists (ranks) from multiple sources into a single (combined) ranked list. Previous works explored the effectiveness of different types of unsupervised rank fusion. and we have chosen RRF, as it generally achieves good, all-round performance. Due to a limited number (5) of submissions, we only submitted two combinations:</p><p>a combination using only one fusion algorithm (RRF) and a retrieval function set (TF-IDF, BM25L, BM25+ and Language Models with Dirichlet priors), with the techniques described in the previous section; no weighted PRF nor query type weighting; a combination of the baseline run and the query type weighting run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answering medical questions with relevant articles</head><p>Other hypothesis we wanted to test was: can we measure article relevance to questions (question type weighting, QTW) using word frequency patterns (e.g. articles relevant to the diagnosis questions would have more diseases or symptoms names, treatment articles would have medications name, ...).</p><p>For each category, we empirically selected the terms from top level MeSH hierarchy:</p><p>-Diagnosis: B03, B04, C III -Test: E01 -Treatment: D02, D04, D06, D26, D27, E02, E04 At indexing time and for each category, we assigned each document a weight derived from the percentage of total words from that category. For example, an abstract with 100 words containing 10 words from the "diagnosis" category would receive an unnormalized score of 0.1. These scores were then Min-Max normalized in relation to other scores for the category. Each document was weighted for each category (meaning each document has 3 weights). At retrieval time, document score (as returned by the retrieval function) was multiplied by the weight of that document for that question's category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reputation based re-ranking</head><p>We tested an alternative of weighing articles for PRF, weighted PRF, that computed an article keyword weight based on its journal's "Impact Factor" from Thomson Reuters' Journal Citation Reports 2013<ref type="foot" coords="3,351.66,311.62,3.97,6.12" target="#foot_1">2</ref> . Instead of using terms from the top 3 documents, we extracted the terms from the top 10 documents, giving higher weights to terms from documents with higher impact factor. Our best performing run was based on the combination of multiple retrieval functions with query expansion and pseudo-relevance feedback, Table <ref type="table" coords="3,443.95,589.89,3.87,8.74" target="#tab_2">2</ref>. QTW and W-PRF runs performed worse than the baseline. In our experiments, we found that relevant documents were being penalized excessively, due to normalization and weight tunning. Regarding W-PRF, we also performed additional experiments with the documents in the relevance judgments and found that there are many articles from journals that did not appear on the JCR list, and thus, receiving zero weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,392.24,323.87,8.74"><head>Table 1</head><label>1</label><figDesc>contains run summaries detailing the techniques used in each run.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,144.89,423.57,322.51,124.57"><head>Table 1 .</head><label>1</label><figDesc>NovaSearch run summary</figDesc><table coords="3,144.89,447.56,322.51,100.59"><row><cell cols="6">Run id Retrieval functions MeSH PRF W-PRF QTW Notes</cell></row><row><cell>1</cell><cell>BM25L</cell><cell>×</cell><cell>×</cell><cell></cell><cell>Baseline</cell></row><row><cell>2</cell><cell>BM25L</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>Question Type Weighting experiment</cell></row><row><cell>3</cell><cell>BM25L</cell><cell>×</cell><cell>×</cell><cell cols="2">* RRF of NS1 and NS2</cell></row><row><cell>4</cell><cell>BM25L, BM25+, TF-IDF, LM</cell><cell>×</cell><cell>×</cell><cell></cell><cell>RRF of runs with multiple ret. functions</cell></row><row><cell>5</cell><cell>BM25L</cell><cell>×</cell><cell>×</cell><cell></cell><cell>Journal reputation weighting experiment</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,134.77,115.91,345.82,102.60"><head>Table 2 .</head><label>2</label><figDesc>TREC CDS 2014 results for NovaSearch runs. Bold values represent best result for summary-based manual runs</figDesc><table coords="4,226.59,150.86,159.11,67.66"><row><cell cols="3">Run id infAP infNDCG R-prec P@10</cell></row><row><cell>4</cell><cell cols="2">0.0757 0.2631 0.2165 0.3900</cell></row><row><cell>1</cell><cell>0.0727 0.2504</cell><cell>0.1971 0.3667</cell></row><row><cell>3</cell><cell>0.0686 0.2418</cell><cell>0.1906 0.3333</cell></row><row><cell>2</cell><cell>0.0669 0.2360</cell><cell>0.1843 0.3333</cell></row><row><cell>5</cell><cell>0.0579 0.2101</cell><cell>0.1691 0.3033</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,113.47,7.47"><p>http://www.trec-cds.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.44,241.07,7.47"><p>http://thomsonreuters.com/journal-citation-reports/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,315.84,342.24,7.86;4,146.91,326.80,321.92,8.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,219.68,315.84,178.56,7.86">When documents are very long, BM25 fails</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2009916.2010070" />
	</analytic>
	<monogr>
		<title level="m" coord="4,420.03,315.84,40.36,7.86">SIGIR &apos;11</title>
		<imprint>
			<date type="published" when="2011-07">Jul 2011</date>
			<biblScope unit="page" from="1103" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,337.76,342.24,7.86;4,146.91,348.72,333.68,7.86;4,146.91,359.68,213.10,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,320.31,337.76,160.28,7.86;4,146.91,348.72,66.82,7.86">Query expansion using open web-based skos vocabularies</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Haslhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,233.35,348.72,247.24,7.86;4,146.91,359.68,138.16,7.86">ACM SIGIR Workshop on Health Search and Discovery: Helping Users and Advancing Medicine</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="35" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,370.64,342.24,7.86;4,146.91,381.60,290.23,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,311.09,370.64,145.40,7.86">NovaSearch on medical ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,160.99,381.60,205.40,7.86">CLEF 2013 Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,392.55,342.24,7.86;4,146.91,403.51,333.68,7.86;4,146.91,414.47,333.68,8.12;4,146.91,425.43,281.45,8.12" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,327.57,392.55,153.02,7.86;4,146.91,403.51,149.58,7.86">Multimodal medical information retrieval with unsupervised rank fusion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0895611114000664" />
	</analytic>
	<monogr>
		<title level="j" coord="4,303.54,403.51,177.05,7.86;4,146.91,414.47,10.29,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>medical visual information analysis and retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
