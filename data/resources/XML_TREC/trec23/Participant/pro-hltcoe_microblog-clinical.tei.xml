<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.31,80.99,411.38,12.90">HLTCOE at TREC 2014: Microblog and Clinical Decision Support</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.12,132.36,37.77,10.75"><forename type="first">Tan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.05,132.36,77.90,10.75"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University HLTCOE</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,414.88,132.36,88.24,10.75"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.31,80.99,411.38,12.90">HLTCOE at TREC 2014: Microblog and Clinical Decision Support</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A6D63BAD42D37D9A4562E0B9CDC332D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our team submitted runs for both the Microblog and Clinical Decision Support tracks. For the Microblog track, we participated in both the temporally anchored adhoc search and the tweet timeline generation subtasks. On the Clinical Decision support task, our efforts were time limited, and our main contribution was to investigate controlling for morphological variation in this technical domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We participated in two tracks for TREC 2014, Microblog and Clinical Decision Support. In this paper we describe our work for each in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Microblog</head><p>The Microblog track has been conducted for four consecutive years. In this year's evaluation, in addition to the traditional temporally anchored adhoc search task, a new Tweet Timeline Generation (TTG) task was introduced with the purpose of providing users a more succinct list of search results. The TTG task targets a form of automatic summarization, and single tweets are grouped into topical clusters. Therefore, the expected outcome is a list of tweets that cover as many clusters as possible (high aspectual recall) with as low a number of off-topic or topically redundant clusters as possible (high cluster precision) and as few topically unrelated tweets in a cluster (high item precision). Definitional question answering has been evaluated at TREC using similar means.</p><p>Ad-hoc search is an obvious pre-processing step for timeline generation because: (1) it helps to prefilter topically irrelevant tweets; and (2) the retrieval score provides a meaningful ranking of tweets as input. Thus, we begin our Microblog track work with the temporally anchored ad-hoc search task, as described in section 2.1. We then describe our process for the TTG task in section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporally-Anchored Ad-hoc Search</head><p>This task requires us to return at most 1000 relevant tweets published no later than time T , given query Q, which is expressed as a few topic-related keywords. Standard ranked retrieval measures are used to evaluate effectiveness, including: mean average precision (MAP), precision at rank 30 (P@30) and R-precision. This suggests two interrelated research questions: (1) what features can be used to determine a tweet's position in a query-focused relevance-ranked list; and (2) given these relevanceoriented features, which ranking functions generate better ranked lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Expansion</head><p>When working with a bag-of-words text representation, as we do, one challenge confronting microblog retrieval is the vocabulary sparsity issue. This sparsity arises not only because of the brevity of tweets, and the use of subjective language, but also because the queries are short (typically 2-3 keywords). We therefore tried both query expansion and tweet expansion. The last three years of Microblog track papers have shown substantial, consistent, and significant improvements in retrieval effectiveness from the use of expansion.</p><p>Specifically, the following three expansion tech-niques are applied in our retrieval system, with the parameters tuned by using topics 1-110 from the Tweets 2011 collection:</p><p>• Query expansion with the Google custom search engine (GSE). As reported by <ref type="bibr" coords="2,265.06,140.85,33.74,9.46;2,93.82,154.40,83.99,9.46" target="#b1">(El Din and Magdy, 2012;</ref><ref type="bibr" coords="2,183.87,154.40,110.23,9.46" target="#b2">El-Ganainy et al., 2013)</ref>, Google search results offer a useful basis for query expansion. Thus, we created our own GSE<ref type="foot" coords="2,114.43,193.00,3.99,6.91" target="#foot_0">1</ref> from the 123 most common linked domain names found in URL in the Tweets 2011 collection. Examples of these domains include * .nytimes.com and * .cnn.com. From the top K GSE (tuned to 20) returned GSE results,<ref type="foot" coords="2,116.55,260.75,3.99,6.91" target="#foot_1">2</ref> we extract the descriptive snippet (1-2 sentences) for each result, and we pick the top W GSE (tuned to 90) most frequently used words across all of these snippets to expand the original query term vector Q 0 as follows: (α 1 is tuned to 0.25):</p><formula xml:id="formula_0" coords="2,108.19,355.22,190.61,10.74">Q exp = (1 -α 1 ) • Q 0 + α 1 • Q GSE (1)</formula><p>• Query expansion with Pseudo-Relevance Feedback (PRF). PRF is a widely used query expansion technique that has been shown to improve (average) retrieval effectiveness in various tasks. In our work, we apply PRF after GSE, which in preliminary experiments was the ordering that generated the better results. We run PRF for R (tuned to 5) rounds so that the expanded queries could better converge.</p><p>For each round, the same expansion function is used:</p><formula xml:id="formula_1" coords="2,102.81,545.70,195.99,10.74">Q exp = (1 -α 2 ) • Q exp + α 2 • Q PRF (2)</formula><p>, where Q P RF is the most frequent W P RF (tuned to 50) words used in the top K P RF (tuned to 20) retrieved tweets (α 2 is tuned to 0.1).</p><p>• Tweet expansion with embedded URL. For each tweet retrieved from the Microblog Track corpus service, we follow any embedded Web Figure <ref type="figure" coords="2,346.03,241.49,4.24,9.46">1</ref>: Precision-Recall tradeoff for the corpus service using expanded queries, Topics 1-110.</p><p>links, parse the content text from the raw HTML, and if all of the initial query terms are present in the extracted text of the page use all of that text on the page to expand the original tweet. <ref type="foot" coords="2,377.27,341.38,3.99,6.91" target="#foot_2">3</ref> We include the requirement that the page used for expansion must contain ALL of the initial query terms because in initial experiments we found that in many cases links had been made to Web pages that contained nonrelevant (or principally non-text) content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Learning-based Re-ranking</head><p>Starting in 2013, the Microblog track adopted a corpus-as-service evaluation structure, which can be considered as a rough way of pre-selecting putatively relevant tweets. Figure <ref type="figure" coords="2,448.21,494.37,5.45,9.46">1</ref> shows a precisionrecall curve, averaged over topics 1-110, after query expansion using both GSE and PRF. To further improve retrieval effectiveness, we then re-rank these returned tweets, with the goal of placing those most likely to truly be relevant as near the top of the ranked list as possible. We do this using Learning-To-Rank (L2R), with topics 1-110 used for training and topics 111-170 used for development testing. We employ the RankLib toolkit,<ref type="foot" coords="2,470.78,614.73,3.99,6.91" target="#foot_3">4</ref> which implements 8 popular L2R algorithms Given a querytweet pair, we generate 9 query-dependent features, and 8 query-independent features. The 8 querydependent features include: • The Okapi BM25 score (with k1 tuned to 0.1, b tuned to 0.2, and the average tweet length set to be 28 words), with the IDF term approximated from the same 1 billion tweet collection;</p><p>• Cosine similarity scores based on TF alone or on TF•IDF.</p><p>With the exception of the corpus service retrieval score (which is computed only on unexpanded tweets), each of these query-dependent feature types are calculated separately for unexpanded and expanded tweets; this yields a total of 9 querydependent features. Tweet expansion can time out (we set up a maximum 3 seconds waiting time for raw HTML fetching and parsing), and it sometimes adds more noise than useful signal; including features based on unexpanded tweets helps to mitigate both risks.</p><p>Our goal in including query-independent features is to characterize some aspects of the "quality" of a tweet. We compute the following 8 features:</p><p>• A binary feature to indicate whether a tweet contains at least one external URL;</p><p>• A binary feature indicate whether a tweet contains at least one hashtag;</p><p>• A scaled number (at 10) indicating the tweet's retweet count (relative to other tweets);</p><p>• The length of the tweet, in words;</p><p>• The length of a tweet, in "informative" words (i.e., excluding common English stopwords and tweet-specific stopwords such as "rt" or "http");</p><p>• The number of stopwords in a tweet;</p><p>• The percentage of the words in a tweet that are informative words;</p><p>• The percentage of the words in a tweet that are stopwords.</p><p>Table <ref type="table" coords="3,352.60,172.95,5.45,9.46">1</ref> shows Mean Average Precision (MAP) averaged over the development test queries for 8 L2R algorithms. Coordinate Ascent (CA) <ref type="bibr" coords="3,502.44,200.05,37.56,9.46;3,313.20,213.60,77.29,9.46" target="#b5">(Metzler and Croft, 2007)</ref> gives the best MAP with tweet expansion features (a 7.75% statistically significant improvement, compared to not reranking), and the second-best MAP without the tweet expansion features. We therefore elected to use CA for our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tweet Timeline Generation</head><p>Working with the results from the ad-hoc search task, the TTG task tries to further reduce the amount of effort a user would need to expend to read at least one instance of the content of all topically relevant tweets. The track guidelines<ref type="foot" coords="3,441.68,374.47,3.99,6.91" target="#foot_4">5</ref> propose a clustering as one possible baseline, but prior experience with novelty detection suggests that clustering meaningfully can be difficult, and that simply returning some set of K top-ranked tweets can be a hard baseline to beat. We therefore used a simple top-K baseline during our system development process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Statistics of Training Data</head><p>In this first year of the TTG task there are only 10 training queries, each of which was manually created from past years' search tasks. Some statistics from the ground truth for the training topics are shown in Table <ref type="table" coords="3,384.25,550.03,4.09,9.46" target="#tab_2">2</ref>. Note that more than 75% of the clusters contain only 1 tweet. From these training topics, we choose to hold out topic 3 as a development test topic because the statistics for topic 3 seemed to us to be reasonably representative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Tweet Novelty Detection</head><p>Starting from our Top-K baseline, a straightforward improvement is to remove redundant tweets. By doing this, we convert the TTG task into a sequential binary decision task in which we ask whether each  tweet contains enough novel content to be retained, given the tweets we have previously decided to retain. We call this approach novelty detection (or deduplication). Set-based methods have been shown to be effective in similar novelty detection tasks <ref type="bibr" coords="4,280.63,437.66,13.63,9.46;4,72.00,451.21,69.84,9.46" target="#b0">(Allan et al., 2001;</ref><ref type="bibr" coords="4,145.61,451.21,126.05,9.46" target="#b7">Soboroff and Harman, 2005)</ref>. The key question, therefore, is how best to measure a tweet's novelty given a previous set of (putatively) novel tweets. In our experiments, we tried both setbased similarity and binary classification.</p><p>For set-based similarity, we compared 3 similarity measures: cosine similarity, Jaccard's coefficient, and shingling/hashing. Algorithm 1 shows our shingling/hashing algorithm, in which we employ Horner's method to hash shingles and set up a hash value space of cardinality 40,000,000. There are 3 parameters to tune: the k-shingles extracted from each tweet, the number m of k-shingles selected for comparison, and the threshold τ .</p><p>Table <ref type="table" coords="4,109.20,645.15,5.45,9.46" target="#tab_4">3</ref> shows effectiveness measures for the Top-K (tuned to 90), Jaccard's coefficient (tuned threshold = 0.8, K = 80), cosine similarity (tuned threshold = 0.6, K = 110), and shingling/hashing (tuned K = 90, k = 10, m = 20, τ = 0.5) on Topic 3 (the one held-out development topic). As can be seen, some Algorithm 1: Shingling/hashing De-duplication</p><formula xml:id="formula_2" coords="4,324.11,227.93,131.53,113.84">C ← {SEARCH RESULTS} R ← {} H ← {} for d ∈ C do S d ← SHINGLING(d, k) H d ← HASHING(S d ) Π d ← SHUFFLE(H d ){1 . . . m} if (Π d ∩ H)/m &lt; τ then H.ADD(H d ) R.APPEND(d) return R</formula><p>improvement on this one topic is observed, suggesting that set-based approaches to novelty detection seem promising.</p><p>We can then add more novelty features in combination with the 8 tweet quality features used in the search phase. Totally, there are 15 new features used, which include:</p><p>• 1 relevance score feature (∈ R &gt;0 ) from the CA L2R result;</p><p>• 8 set-based similarity features, where all previously selected tweets form the set, which is then used to calculate some similarity score (cosine, Okapi-BM25, unigram language modeling with Bayesian smoothing) with the (unexpanded or expanded) new tweet;</p><p>• 2 set-extreme similarity features, where only the maximum (Jaccard or cosine) similarity, maximized over all tweets in the set, is considered;</p><p>• 3 set-based distance features, which are either the number of new terms introduced by the new tweet (either un-normalized, or normalized by the length of the new tweet), or the elapsed minutes between the latest tweet in the set and the new tweet;<ref type="foot" coords="5,157.17,87.36,3.99,6.91" target="#foot_5">6</ref> </p><p>• 1 summary length control feature, which is currently effectively unused (it is simply set to the number of tweets in the current selected set).</p><p>Because we are interested in using a supervised binary classifier to learn how to make the novelty predication from the training topics, we manipulate the manual ground truth clustering of the training topics using the following steps to create novelty detection training data for each training topic:</p><p>Step 1: create an empty sample pool;</p><p>Step 2: collect up to 2000 tweets from the top search results (together with their CA L2R score), and put them into the sample pool;</p><p>Step 3: if any tweet in the manually created clusters does not exist in the sample pool, add it and assign a relevance score of 0;</p><p>Step 4: for each manually created ground truth cluster, mark the tweet with the highest relevance score as positive;</p><p>Step 5: mark all the rest of the tweets in the sample pool as negative.</p><p>Step 6: for each tweet in the sample pool, create feature vectors as listed above (∈ R 23 ) performing the calculation in order of decreasing CA L2R score, and incrementally adding only positive exemplars to the set.</p><p>There are plenty of binary classifiers we could choose from, but we have limited training data on which to base our choice. We visualized the feature space of the novelty detection training data, as shown in Figure <ref type="figure" coords="5,147.82,543.36,4.09,9.46" target="#fig_0">2</ref>,<ref type="foot" coords="5,156.00,541.31,3.99,6.91" target="#foot_6">7</ref> which is a R 2 projection from principal component analysis (PCA) with 55% of the variance preserved. This distribution suggests a non-linear classifier. We therefore chose to use a Support Vector Machine (SVM) with a radial basis kernel function.</p><p>The classifier's effectiveness on Topic 3, the one held-out development test topic, is shown in Table <ref type="table" coords="5,290.62,639.17,4.09,9.46" target="#tab_4">3</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation</head><p>For this year's Microblog track, there are 55 evaluation topics <ref type="bibr" coords="5,368.61,429.44,67.19,9.46">(topic 171-225)</ref>. This section summarizes our submitted runs and analyzes the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Submitted Runs</head><p>We submitted a total of 4 ad-hoc search runs and 4 TTG runs. Table <ref type="table" coords="5,388.72,496.65,5.45,9.46" target="#tab_5">4</ref> summarizes our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Result Analysis</head><p>For our ad-hoc search runs, we achieved a MAP improvement over the baseline (hltcoe0) of 0.1027 absolute (σ = 0.1734) using GSE alone, a further improvement of 0.0154 absolute (σ = 0.0426) using GSE followed by PRF, and yet a further improvement of 0.0377 absolute (σ = 0.0703) using CA L2R (with tweet expansion). Paired t-tests show that each of these incremental improvements is statistically significant at p &lt; 0.05. As a comparison, among all 73 automatic runs submitted, our best run (hltcoe3) achieved the maximum reported Average Precision (AP) for 19% of the topics and exceeded the median AP by for 87% of the topics. Corresponding figures for P@30 35% and 69% of the top- ics, respectively. However, we do note that GSE introduces large variance across topics, and for several topics the effectiveness measures with GSE alone are actually lower than the baseline. Figure <ref type="figure" coords="6,246.07,296.47,5.45,9.46" target="#fig_1">3</ref> illustrates the nature of this problem for topic 175 "commentary on naming storm nemo", for which GSE the greatest adverse effect. For this figure, we have manually clustered the informative query terms (after expansion) according to their semantic meaning (as we interpret that meaning), and then shown the fraction of the terms that appear in each cluster. For example, the original query contains four (nonstopword) terms, so 25% of the query is present in each cluster. The query drift resulting from expansion is clearly evident in this case, with the focus (GSE and PRF) changing from naming the storm to the storm itself (According to the manual evaluation description, relevant tweets should include "discussion/commentary/jokes about naming the northeaster storm Nemo".) With regard to the TTG runs, we see substantial and statistically significant improvements over the baseline (hltcoeTTG0) from all three set-based novelty detection methods that we tried both for weighted (where the importance of cluster is considered according to the number of tweets and informative tweets contained) and unweighted (where clusters are treated equally) evaluation measures. However, we note that our supervised binary classifier (hltcoeTTG2) did not do as well as either of the other two set-based methods, perhaps reflecting the limited amount of training data that we had available. Perhaps with cross-validation we might have done better than we did with a single held-out development test topic. Figure <ref type="figure" coords="6,420.00,403.87,5.45,9.46" target="#fig_2">4</ref> shows the learning curve for our SVM classifier on Topic 3. The fact that the error on the training set stops decreasing as we add more training instances suggests underfitting,<ref type="foot" coords="6,535.52,442.47,3.99,6.91" target="#foot_7">8</ref> which in turn suggests that additional work on model and feature engineering are called for.</p><p>From these results, we are now able to formulate some additional research questions, including:</p><p>• Currently, our novelty detection methods assume a relevance-ordered processing, but we might also try other orders (e.g., temporal).</p><p>• Because of the cascade design of the timeline generation, we should explore interaction effects with the design of our ad-hoc search. Ceiling analysis (using ground truth ad-hoc relevance judgments) shows that if we had a perfect ad-hoc search, our current best TTG system (hltcoeTTG3) could further improve weighted F 1 by 0.3777 absolute and unweighted F 1 by 0.3977 absolute. These fairly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clinical Decision Support</head><p>For this track we used the JHU HAIRCUT retrieval engine described by <ref type="bibr" coords="7,192.52,399.13,106.28,9.46;7,72.00,412.68,27.27,9.46" target="#b3">McNamee and Mayfield (2004)</ref>. One of HAIRCUT's distinctive traits is the use of character n-grams as indexing terms, which have proven to be effective for controlling the effects of morphological variation <ref type="bibr" coords="7,220.29,453.32,78.51,9.46;7,72.00,466.87,23.48,9.46" target="#b4">(McNamee et al., 2009)</ref>. While morphological variation is not nearly as substantial a problem in English as it is in some other languages, we thought it likely that the highly technical terminology prevalent in medical literature could benefit from n-gram indexing.</p><p>We had hoped to explore other techniques, including relevance feedback using "collection enrichment" from appropriate medical domain side corpora, however we did not have adequate time to conduct those experiments over the summer. Our submissions were produced in about 1.5 person-days of effort, with a significant portion of that time spent in preprocessing the PubMed Open Access Subset corpus.</p><p>We did not make any use of domain-specific resources such as ontologies, thesauri, or biomedical IE tools. We sought through our participation to determine the performance that a domain-agnostic, state-of-the-art retrieval engine might obtain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submissions</head><p>We submitted four runs to the track, that varied in several ways:</p><p>• the type of tokenization used -either plain words or overlapping character 5-grams;</p><p>• whether pseudo-relevance feedback was used, or not;</p><p>• if "description" or "summary" topics fields were used; and,</p><p>• whether the full collection was indexed.<ref type="foot" coords="7,509.09,356.96,3.99,6.91" target="#foot_8">9</ref> </p><p>The last variation, indexing of the entire collection, is due to an error that was only discovered just prior to submission. Due to the relatively small percentage of documents that were not indexed, we do not believe that the n-gram runs are too badly affected.</p><p>The parameters for each run are summarized in Table <ref type="table" coords="7,339.89,480.41,4.09,9.46" target="#tab_6">5</ref>.</p><p>When relevance feedback was used, queries were modified after examining the top 20 and bottom 75 (of 1000) documents, and selecting either 60 (words) or 200 (5-grams) expansion terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>NIST provided results for each of our official runs, and we report average results over the 30 topics in Table <ref type="table" coords="7,340.41,606.79,4.09,9.46" target="#tab_7">6</ref>. Metrics include inferred average precision (infAP), inferred normalized discounted cumulative gain (infNDCG), precision at the number of known relevant documents (R-prec), and precision at a fixed cutoff of 10 documents (P@10). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>We make several observations from these preliminary results. Summaries beat descriptions. Runs hltcoe5srf and hltcoe5drf differ only in whether description of summary topics were used. Performance in all metrics was notably higher using the Summary topics.</p><p>Relevance feedback boosts average performance. Relevance feedback led to substantial improvements, on both recall-sensitive and precisionfocused metrics. Runs hltcoe5s and hltcoe5srf differ only in the use of relevance feedback, which led to average relative gains of 44% in infAP and 29% in infNDCG.</p><p>Character 5-grams outperformed words. Run hlt-coe5srf beat hltcoewsf on all four metrics, however, while the gains were palpable, they were not dramatically different (i.e., relative gains varied from 3.5% to 12%, depending on the metric).</p><p>In comparison with all automatic runs submitted to the track, our submitted runs were substantially above the median (58% to 157%, depending on the metric). However, results were far below (about half of) the hypothetical results that would be produced by a per-topic oracle combination derived from post hoc examination of all automated submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In general we are pleased with this year's Microblog track, and in particular with the fact that we now will have a considerable amount of training data for the TTG task, which we see as a useful addition to the track. Our results in that track suggest several productive directions for future work, including: (1) term-weighting in the query expansion, (2) novelty detection features, and (3) novelty detection binary classification model.</p><p>In the first running of the Clinical Decision Support task, we attained high performing results that were considerably above the median of automatic runs, and we saw confirmation that established techniques, namely relevance feedback and use of character n-grams to address morphology are effective in this domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,313.20,241.49,75.96,9.83;5,389.16,239.19,4.23,6.99;5,396.88,241.49,143.12,9.46;5,313.20,255.04,82.94,9.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PCA R 2 Projection of the Novelty Detection Training Data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,72.00,657.88,226.80,9.46;6,72.00,671.43,46.40,9.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Query Drift from Query Expansion for Topic 175.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,72.00,258.51,46.20,9.46;7,134.59,258.51,164.22,9.46;7,72.00,272.06,64.55,9.46"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SVM Learning Curve on Training/Validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,332.42,226.80,23.01"><head>Table 2 :</head><label>2</label><figDesc>Clustering Statistics for TTG Training Topics, ground truth.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,313.20,351.50,226.80,23.01"><head>Table 3 :</head><label>3</label><figDesc>Performance of various Novelty Detection Methods on TTG.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,102.66,73.19,406.69,144.30"><head>Table 4 :</head><label>4</label><figDesc>2014 Microblog Track Results, Sumbitted Runs.</figDesc><table coords="6,102.66,73.19,406.69,122.10"><row><cell cols="2">Ad-hoc Search Runs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>Description</cell><cell>MAP</cell><cell>P@30</cell><cell>R-Prec</cell></row><row><cell>hltcoe0</cell><cell>baseline from corpus API</cell><cell>0.4149</cell><cell>0.6236</cell><cell>0.4434</cell></row><row><cell>hltcoe1</cell><cell>query expansion with GSE</cell><cell>0.5176</cell><cell>0.6733</cell><cell>0.5072</cell></row><row><cell>hltcoe2</cell><cell>query expansion with GSE+PRF</cell><cell>0.5330</cell><cell>0.6812</cell><cell>0.5211</cell></row><row><cell>hltcoe3</cell><cell>L2R from hltcoe2 with tweet expansion</cell><cell>0.5707</cell><cell>0.7121</cell><cell>0.5660</cell></row><row><cell cols="2">Tweet Timeline Generation Runs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>Description</cell><cell cols="3">Recall(Unweighted) Recall(Weighted) Precision</cell></row><row><cell cols="2">hltcoeTTG0 top 90 tweets from hltcoe3</cell><cell>0.4422</cell><cell>0.6356</cell><cell>0.2295</cell></row><row><cell cols="2">hltcoeTTG1 novelty detection with cosine similarity</cell><cell>0.4029</cell><cell>0.5915</cell><cell>0.3407</cell></row><row><cell cols="2">hltcoeTTG2 novelty detection with SVM</cell><cell>0.4622</cell><cell>0.6534</cell><cell>0.2909</cell></row><row><cell cols="3">hltcoeTTG3 novelty detection with shingling/hashing 0.4869</cell><cell>0.6658</cell><cell>0.2976</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,313.20,73.19,226.80,84.13"><head>Table 5 :</head><label>5</label><figDesc>Parameters for submitted Clinical Decision Support runs.</figDesc><table coords="7,329.75,73.19,193.70,48.37"><row><cell>Run</cell><cell>Topics</cell><cell>Terms</cell><cell>RF</cell><cell>Index</cell></row><row><cell>hltcoe5s</cell><cell cols="3">Summ 5-grams None</cell><cell>Partial</cell></row><row><cell>hltcoe5srf</cell><cell cols="2">Summ 5-grams</cell><cell>Yes</cell><cell>Partial</cell></row><row><cell>hltcoe5drf</cell><cell>Desc</cell><cell>5-grams</cell><cell>Yes</cell><cell>Partial</cell></row><row><cell cols="2">hltcoewsrf Summ</cell><cell>words</cell><cell>Yes</cell><cell>Complete</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,72.00,73.19,226.80,118.00"><head>Table 6 :</head><label>6</label><figDesc>Averaged results for submitted Clinical Decision Support runs, compared to median performance and oracle best results by automatic runs.</figDesc><table coords="8,86.48,73.19,197.84,68.70"><row><cell>Run</cell><cell cols="3">infAP infNDCG R-prec P@10</cell></row><row><cell>hltcoe5s</cell><cell>0.0562</cell><cell>0.2008</cell><cell>0.1739 0.3200</cell></row><row><cell>hltcoe5srf</cell><cell>0.0812</cell><cell>0.2587</cell><cell>0.2193 0.3700</cell></row><row><cell cols="2">hltcoe5drf 0.0677</cell><cell>0.2199</cell><cell>0.1834 0.3000</cell></row><row><cell cols="2">hltcoewsrf 0.0725</cell><cell>0.2412</cell><cell>0.2117 0.3533</cell></row><row><cell>median</cell><cell>0.0316</cell><cell>0.1514</cell><cell>0.1257 0.2333</cell></row><row><cell>oracle</cell><cell>0.1805</cell><cell>0.5197</cell><cell>0.3496 0.7100</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.14,683.36,102.03,7.77"><p>https://www.google.com/cse</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.14,694.24,210.67,7.77;2,72.00,704.20,226.80,7.77;2,72.00,714.16,22.08,7.77"><p>GSE results were customized to return only results that had been indexed by Google before the query time, specified in the query.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,329.34,693.17,181.18,7.93;2,528.55,693.33,11.45,7.77;2,313.20,703.29,135.71,7.77"><p>We use Goose to extract the text; see https://github.com/GravityLabs/goose</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,329.34,714.16,161.98,7.77"><p>http://sourceforge.net/p/lemur/wiki/RankLib/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,329.34,704.20,204.97,7.77;3,313.20,714.16,62.43,7.77"><p>https://github.com/lintool/twitter-tools/wiki/TREC-2014-Track-Guidelines</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,88.14,663.52,210.67,7.77;5,72.00,673.48,226.80,7.77;5,72.00,683.45,226.80,7.77;5,72.00,693.41,20.67,7.77"><p>The elapsed minutes could be negative, which would indicate seeing a new tweet that predates the last tweet in the set, since the tweets are processed in relevance rather than temporal order.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,84.65,702.34,2.99,5.18;5,88.14,704.20,210.67,7.77;5,72.00,714.16,131.63,7.77"><p>  7  In this figure, we show randomly selected negatives along with a balanced number of positives.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,329.34,703.91,206.51,8.06;6,535.85,702.15,3.65,5.24;6,313.20,714.16,169.53,7.77"><p>SVM parameter C and kernel function parameter Sigma 2 have already been tuned to optimal for this plot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="7,329.34,684.27,210.66,7.77;7,313.20,694.24,226.80,7.77;7,313.20,704.20,226.80,7.77;7,313.20,714.16,143.95,7.77"><p>For all runs docids 3016743 &amp; 3261719 were not indexed due to parsing difficulties. Additionally, and more significantly, the 5-gram runs ran on a slightly "broken" index which accidentally left out 3.5% of the documents.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,313.20,211.92,226.80,8.64;8,324.11,222.71,215.89,8.81;8,324.11,233.66,215.89,8.58;8,324.11,244.62,215.89,8.58;8,324.11,255.58,215.89,8.81;8,324.11,266.71,24.79,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,324.11,222.87,143.25,8.64">Temporal summaries of new topics</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vikas</forename><surname>Khandelwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,490.65,222.71,49.35,8.58;8,324.11,233.66,215.89,8.58;8,324.11,244.62,215.89,8.58;8,324.11,255.58,70.50,8.81">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,286.64,226.80,8.64;8,324.11,297.59,215.89,8.64;8,324.11,308.38,26.85,8.58" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,495.98,286.64,44.01,8.64;8,324.11,297.59,199.73,8.64">Web-based pseudo relevance feedback for microblog retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">El</forename><surname>Din</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

<biblStruct coords="8,313.20,328.48,226.80,8.64;8,324.11,339.44,215.89,8.64;8,324.11,350.23,26.85,8.58" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,381.44,339.44,138.60,8.64">Qcri at trec 2013 microblog track</title>
		<author>
			<persName coords=""><forename type="first">Tarek</forename><surname>El-Ganainy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

<biblStruct coords="8,313.20,370.32,226.80,8.64;8,324.11,381.28,215.89,8.64;8,324.11,392.07,118.83,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,490.36,370.32,49.64,8.64;8,324.11,381.28,212.37,8.64">Character ngram tokenization for european langauge text retrieval</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,324.11,392.07,85.07,8.58">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,412.16,226.80,8.64;8,324.11,423.12,215.89,8.64;8,324.11,433.91,215.89,8.81;8,324.11,444.87,215.89,8.58;8,324.11,455.83,210.23,8.81" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,353.95,423.12,186.05,8.64;8,324.11,434.08,60.61,8.64">Addressing morphological variation in alphabetic languages</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,406.10,433.91,133.90,8.58;8,324.11,444.87,215.89,8.58;8,324.11,455.83,124.26,8.58">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,475.93,226.80,8.64;8,324.11,486.72,215.89,8.81;8,324.11,497.67,108.47,8.81" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,514.00,475.93,26.00,8.64;8,324.11,486.88,189.76,8.64">Linear feature-based models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,526.59,486.72,13.41,8.58;8,324.11,497.67,17.53,8.58">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="274" />
			<date type="published" when="2007-06">2007. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,517.77,226.80,8.64;8,324.11,528.73,215.89,8.64;8,324.11,539.69,215.89,8.64;8,324.11,550.48,115.38,8.81" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,479.65,528.73,60.35,8.64;8,324.11,539.69,215.89,8.64;8,324.11,550.65,21.58,8.64">From tweets to polls: Linking text sentiment to public opinion time series</title>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramnath</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Balasubramanyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bryan R Routledge</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,352.87,550.48,28.36,8.58">ICWSM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,570.57,226.80,8.64;8,324.11,581.36,215.89,8.81;8,324.11,592.32,215.89,8.58;8,324.11,603.28,215.89,8.81;8,324.11,614.41,215.89,8.64;8,324.11,625.37,140.59,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,491.51,570.57,48.49,8.64;8,324.11,581.53,114.99,8.64">Novelty detection: The trec experience</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.12,581.36,76.88,8.58;8,324.11,592.32,215.89,8.58;8,324.11,603.28,215.89,8.81;8,324.11,614.41,11.83,8.64">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,645.29,226.80,8.64;8,324.11,656.08,211.49,8.81" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,474.02,645.29,65.98,8.64;8,324.11,656.25,74.03,8.64">Inverted files for text search engines</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,405.84,656.08,78.51,8.58">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006-07">2006. July</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
