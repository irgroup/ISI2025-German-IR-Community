<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.70,76.93,284.64,18.04">SIRA: TREC Session Track 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.30,97.66,76.36,10.80"><forename type="first">Brennan</forename><surname>Bushee</surname></persName>
							<email>bbushee@skidmore.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.12,97.66,58.09,10.80"><forename type="first">Drew</forename><surname>Pintus</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.05,97.66,63.47,10.80"><forename type="first">Patrick</forename><surname>Smith</surname></persName>
							<email>psmith4@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.99,97.66,100.52,10.80"><forename type="first">Sharon</forename><forename type="middle">Gower</forename><surname>Small</surname></persName>
							<email>ssmall@siena.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Siena College</orgName>
								<address>
									<addrLine>515 Loudon Road Loudonville</addrLine>
									<postCode>12211</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.70,76.93,284.64,18.04">SIRA: TREC Session Track 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">064C56E7B92446BB6D6C05DF5C7ABAB7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses Siena's Interactive Research Assistant's (SIRA) participation in the Text Retrieval Conference (TREC) Session Track of 2014. The overall goal of this track is to improve search results during query sessions based on a user's behavior. Query sessions include many aspects of a search, including query topics, initial retrieved webpages, clicked on links, visit times, etc. SIRA has used several methods to improve search results that will be discussed in this paper. Each method of query expansion utilized clicked-on and non-clicked-on links, pages with the longest visited time, and N-Percent (N%) of each page. Two of our three submissions improved over our baseline results and both of these were equal to the median submission for all participants in the track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Introduction</head><p>The Session Track is a program in the Text Retrieval Conference (TREC). TREC is a program co-sponsored by the National Institute of Standards and Technology (NIST) and the U.S. Department of Defense and it focuses on supporting research in information retrieval and extraction, and to increase availability of appropriate evaluation techniques. The Session Track <ref type="bibr" coords="1,72.02,373.57,14.08,10.80" target="#b0">[1]</ref> is in its fifth year and it focuses on using session information to improve system results during a series of queries. The session information that is available for use includes the top 10 pages returned for a query, their titles, a small preview of the page called a snippet, whether or not that page was clicked on and how much time the user spent on the clicked on pages. The idea is to lead the user to their answers by taking this information and giving them a new set of pages that are more tailored to their specific needs. The official scoring for the track uses Discounted Cumulative Gain for the top 10 pages, or nDCG@10. Normalized discounted cumulative gain (nDCG) is the DCG of the results divided by the optimal DCG. DCG measures the quality of the ranking of the returned documents, based on the assumption that more relevant documents should be placed higher in the returned list. For each session, the NIST judges rated each result by relevance: -2 (spam), 0 (not relevant), 1 (relevant, the content of the page provides some information of the topic), 2 (highly relevant, the content of the page provides substantial information of the topic), 3 (key, the page is dedicated to the topic) to 4 (navigational, the page represents a home page of an entity named in the query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Research</head><p>Since the Session Track began in 2009, many universities and groups have participated and reported their results and methods in detailed papers. Georgetown University <ref type="bibr" coords="1,459.19,610.50,14.11,10.80" target="#b1">[2]</ref> used slightly different approaches in three separate runs for their participation in the 2013 Session Track. Their first run involved a simple retrieval system, their second was based on query modification of the user, and lastly their third run incorporated results that were clicked on. One of the more successful groups, from the University of Pittsburgh <ref type="bibr" coords="1,335.83,665.70,12.87,10.80" target="#b2">[3]</ref>, also participated in the 2013 Session Track. University of Pittsburgh decided to move away from their 2012 method of using past queries, and instead used the relevance of the returned pages. The reason for the change comes from their study of their system from 2011 and 2012. According to these studies, they reported that incorporating past queries tended not to affect the search performance significantly. Their system also utilized results that occurred multiple times rather than simply discarding them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SIRA</head><p>The main focus of the SIRA system was to utilize "typical search behavior" in order to provide a better ranking of results to a user. This meant analyzing how a user normally utilizes a search engine and identifies what they see as most important/relevant. Such behaviors include skimming for interesting titles, reading only a certain small percentage of a page's content, and investing time on a page that they feel contains information that is useful to them, etc. By using these observations, we were able to take the session information and utilize it to return an improved re-ranking of results.</p><p>The remainder of this paper will discuss the modules of our SIRA system in detail as well as the results of our NIST evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SIRA: Document Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Corpus and interface</head><p>The ClueWeb12 1 corpus distributed by Carnegie Mellon University consists of about 733 million web pages. Participants are able to access ClueWeb12 through a web search interface that utilizes the Indri search engine <ref type="bibr" coords="2,223.49,325.33,12.85,10.80" target="#b3">[4]</ref>. Indri is able to take in many different parameters, from "andor" statements, to word displacement checks, to frequency of word appearance checks. We utilized Indri to effectively search the collection of web pages to retrieve an initial set of potentially relevant documents for a user's query. SIRA generates a list of parameters automatically after its first run by taking words from both the query and the information about the current session and makes a decision on what parameters should be wrapped around said words. Specifically, our system extracted nouns, verbs, adverbs, and adjectives and finds the highest frequency synonym. After we get a list of those words, we add them to the current query. We then produce a query text file and upload it to the ClueWeb12 Batch Query Service page to retrieve 100 relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Spam Filtering Module</head><p>The Spam Module incorporates a list of over 700 million ClueWeb12-IDs paired with a spam score, given by Waterloo Spam Rankings 2 for the ClueWeb12 Dataset. The scores range from 0 to 99 where 0 is most spam-like and 99 is least spam-like. Our original spam threshold was set to 30 (based on what groups had used in the past). After several experiments and manual judging, with spam threshold increments of five, we determined the optimal spam threshold to be 35.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SIRA System</head><p>SIRA's first process the TREC session file. It extracts all of the relevant information and organizes it in terms of a session's individual searches, their results, and how the user interacted with those results. The SIRA controller then takes that information, checks and runs each session through both the clicked and longest visit time modules or the framing module, depending on what was specified in the Control File. SIRA then produces a query expansion file formatted for clueWeb12 search engine batch search.</p><p>A manual retrieval of the batch results is required and after that's done, we run the program again. This time the controller uses our HTML to text program to extract each result retrieved from the batch result. Next our spam module compares the retrieved pages to a list of possible website IDs and removes any that match. Finally, Garbage Collect removes any duplicates and makes sure everything is in order before finally outputting a final list of top 10 results for each session. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">N% Module</head><p>The N% Module involves extracting the first N% of a document. This module was developed to model a user's typical web searching behavior. When a user enters a query and is presented with results, they normally scan or read the first few sentences of the document they clicked on to quickly judge its relevance. We developed a module that simply takes the given percent of each document and analyzes it from there. This method is much more efficient than searching through an entire document. We determined through our experiments that 10% is the optimal amount of a page to be analyzed by SIRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Clicked Module</head><p>We hypothesized that if a page was clicked on, then there was something attractive about the link that led the user to believe the full page would be relevant. A logical answer would be that the title contained words that the user found useful or relevant. The Clicked Page module takes information found in the current session and extracts the most clicked page (s)' titles. It then takes those titles and creates a batch query text file which is used to retrieve 100 documents.</p><p>The results of this module turned out to be better than anticipated. In the original test data, SIRA's clicked module produced an average nDCG@10 score of 0.15. Official results from our TREC submission had an average nDCG@10 score of about 0.17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Longest Visited Time Module</head><p>We also hypothesized that a page that was visited for a long time must have some information that the user found interesting or important. With that in mind, we built a module that automatically determined which results had been visited for the greatest length of time and extracted the snippets that accompanied it. To see what information was relevant to our search, we extracted the most frequent words including synonyms. For this process, words were extracted from the document's snippet. These words were then added to the batch query text file for query expansion. Using this method, we scored an average nDCG@10 of 0.169, which was just below the average of all teams participating, 0.1702. In the future, we would like to try and refine what parts of the snippet we decide to place in the batch query text file so that our query expansion is directed more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Framing</head><p>In order to impose some order on the large amount of unstructured data, SIRA stored all the "important" words in a structure called a frame. Frame semantics have been used before in special-purpose question answering systems, such as HITIQA, which was designed for intelligence analysts researching weapons of mass destruction <ref type="bibr" coords="4,387.29,350.65,12.85,10.80" target="#b4">[5]</ref>. In these domains, a small number of specific frames can be added to the basic general frame to describe all the events of interest to the user, but due to the wide range of possible search topics for our task, we decided to use just the general frame consisting of nouns, verbs, people, locations, and organizations recognized in the text. The system created a "goal frame" for each query and then created a "data frame" for each search result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Entity Recognizer</head><p>The Stanford Named Entity Recognizer (SER) <ref type="bibr" coords="4,304.13,463.35,14.08,10.80" target="#b5">[6]</ref> was one resource used in the framing of text. In the RL2 run we framed the current query and the description of the topic in each query. Using SER aided in this process. SER searches through a string of text and tags or labels sequences of words that are entities. These include people, locations, organizations, etc. This information was used as elements of our frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Frame Scoring</head><p>Documents were given relevance scores based on how well their data frame matched the goal frame. For example, in Session 11 from the 2012 Session Track, the user searched for information about the sinking of the Russian nuclear submarine Kursk. In the RL1 run, only the current query was framed. In our RL2 run, the system added previous queries and snippets from clicked links to the goal frame. Similarly, the N% module trimmed each document to 10 percent of its original size, and multiple snippets were taken from each result page. The one that generated the highest-scoring data frame was used as the data frame for that page. Here is an example of a frame created from a snippet: The frames are awarded one point for each field in which there is a word matching one in the corresponding field of the goal frame. For nouns and verbs, synonyms and hypernyms count as matches, and all verbs are converted to infinitive form before they are stored in the frame for ease of comparison. The framing module showed some promise in training, but did not perform as well as we had hoped on the real data. From the results, we were able to identify some areas in which it could improve:</p><p>-It is possible for a document to receive a high score if it contains words that are only tangentially related to one in the goal frame. It may be better to change our scoring system that takes into account distance between the matches in a semantic network.</p><p>-Another unforeseen problem was that sometimes common web words such as "search" and "contact" were added to the goal, creating false matches. Thus the system needs a way of knowing whether these words are part of the meaning of the text. Simply adding these common web search words to a stop word list would fix this.</p><p>-On average for framing, the RL2 results were worse than those of RL1. This suggests that a frame that is too large may be too general and generate false positives. The optimal size of a goal frame is open for experimentation.</p><p>-The system sometimes erroneously splits up phrases into different categories. For example, "Russian navy" in the above example may be better categorized as an organization; the system instead stored "Russian" as a location and "navy" as a noun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Garbage Collecting</head><p>Our garbage collecting module had the purpose of producing a list of ranked documents that did not share a common base URL. For example, if one of the documents has the URL "www.amazon.com/scooter", another document cannot have a URL starting with "www.amazon.com". The base URL of each document in question was compared to the rest of the URLs of documents gathered by the program. If any of the base URLs matched, the ranked list only returns the top rated document with the corresponding base URL. Although it seemed like this method wasn't useful at first, through closer inspection it removed many duplicated pages and gave our user more unique pages in their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Results</head><p>Our evaluation results can be seen in the graph below. The highest RL2 nDCG@10 score that we received was our Clicked module, which gave a 3% increase over our RL1 score. Our LVT also gave a slight increase with a 2.37% increase from RL1 to RL2. Our RL1 score was significantly higher than last year's median RL1 score, which was 0.1171, and our highest RL2 score was an improvement over last year's median of 0.15305 and equivalent to this years average. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>This paper reviews our work done for the TREC Session Track for 2014. The results show that it is possible to use past searches to effectively direct a future query with our Clicked and LVT modules. We are currently working on modifying our framing module as identified above and running new experiments utilizing the NIST judgment file to improve its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,232.97,478.31,145.87,8.96;3,102.00,170.09,407.98,304.60"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SIRA System Architecture</figDesc><graphic coords="3,102.00,170.09,407.98,304.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.02,624.82,205.20,7.21;4,72.02,635.02,113.40,7.21;4,108.02,645.22,140.43,7.21;4,108.02,655.42,75.60,7.21;4,108.02,665.62,81.00,7.21;4,108.02,675.82,75.60,7.21;4,108.02,686.02,118.80,7.21"><head></head><label></label><figDesc>Current query: "kursk UK subs in area" Current query framed: Nouns: [kursk, subs, area] Verbs: [none] Locations: [UK] People: [none] Organizations: [none]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,176.54,490.72,258.65,8.10;6,92.25,211.49,427.06,259.80"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: nDCG@10 results of SIRA modules and the TREC averages.</figDesc><graphic coords="6,92.25,211.49,427.06,259.80" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,87.33,627.74,451.99,8.96;6,72.02,639.26,387.13,8.96" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Cartette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashraf</forename><surname>Bah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<title level="m" coord="6,443.04,627.74,96.28,8.96;6,72.02,639.26,276.15,8.96">Overview of the TREC 2013 Session Track. The Twenty-Second Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>TREC 2013. Proceedings</note>
</biblStruct>

<biblStruct coords="6,87.14,662.18,452.79,8.96;6,72.02,673.70,299.62,8.96;6,72.02,685.21,133.61,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,214.13,662.18,325.81,8.96;6,72.02,673.70,105.08,8.96">Applying the Query Change Retrieval Model on Session Search-Georgetown at TREC 2013 Session Track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno>TREC 2013) Proceedings. 2013</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,183.98,673.70,187.67,8.96">The Twenty-Second Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.18,708.25,185.57,8.96;7,72.02,74.20,468.10,8.96;7,72.02,85.74,217.92,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,194.78,708.25,76.97,8.96;7,72.02,74.20,380.03,8.96">Pitt at TREC 2013: Different Effects of Click-through and Past Queries on Whole-session Search Performance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,460.27,74.20,79.85,8.96;7,72.02,85.74,106.45,8.96">The Twenty-Second Text REtrieval Conference</title>
		<meeting><address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct coords="7,86.30,108.66,441.51,8.96;7,72.02,120.18,452.93,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,422.03,108.66,105.78,8.96;7,72.02,120.18,168.26,8.96">Indri: A Language Modelbased Search Engine for Complex Queries</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,247.13,120.18,273.76,8.96">Proceedings of the International conference on Intelligence Analysis</title>
		<meeting>the International conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,86.30,143.22,427.78,8.96;7,72.02,154.74,46.29,8.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,235.61,143.22,278.47,8.96;7,72.02,154.74,41.66,8.96">HITIQA: A Data Driven Approach to Interactive Analytical Question Answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,86.18,177.66,445.82,8.96;7,72.02,189.18,466.31,8.96;7,72.02,200.70,433.37,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,366.75,177.66,165.26,8.96;7,72.02,189.18,205.06,8.96">Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,284.14,189.18,254.19,8.96;7,72.02,200.70,156.03,8.96">Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</title>
		<meeting>the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
