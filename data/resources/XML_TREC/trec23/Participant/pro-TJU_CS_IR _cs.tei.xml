<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.01,116.95,313.34,12.62;1,236.57,134.89,142.21,12.62">A New Approach to Contextual Suggestions Based on Word2Vec</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.91,172.56,71.68,8.74"><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tianjin Key Laboratory of Cognitive Computing and Application</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.14,172.56,60.88,8.74"><forename type="first">Zhenjun</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tianjin Key Laboratory of Cognitive Computing and Application</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.90,172.56,65.04,8.74"><forename type="first">Xiaozhao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tianjin Key Laboratory of Cognitive Computing and Application</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.81,172.56,31.41,8.74"><forename type="first">D</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tianjin Key Laboratory of Cognitive Computing and Application</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,426.86,172.56,33.78,8.74"><forename type="first">P</forename><surname>Zhang</surname></persName>
							<email>pzhang@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Tianjin Key Laboratory of Cognitive Computing and Application</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.01,116.95,313.34,12.62;1,236.57,134.89,142.21,12.62">A New Approach to Contextual Suggestions Based on Word2Vec</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C43D7FA726FB8C9680AE5C073C1A8D61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Contextual Suggestions</term>
					<term>Word2Vec</term>
					<term>user model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report our participation in the contextual suggestion track of TREC 2014 for which we submitted two runs using a novel approach to complete the competition. The goal of the track is to generate suggestions that users might fond of given the history of users' preference where he or she used to live in when they travel to a new city. We tested our new approach in the dataset of ClueWeb12-CatB which has been pre-indexed by Luence. Our system represents all attractions and user contexts in the continuous vector space learnt by neural network language models, and then we learn the user-dependent profile model to predict the user's ratings for the attraction's websites using Softmax. Finally, we rank all the venues by using the generated model according the users' personal preference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The contextual suggestion tracktrac <ref type="bibr" coords="1,296.24,453.88,10.52,8.74" target="#b1">[2]</ref> investigates search techniques for complex information needs that are highly dependent on context and user interests. In other words, the track focus on a situation like that: a user traveled to a new city and we only have got the personal history preference data where he or she used to live in, for example the user is fond of watching movies or visiting parks, how do we recommend some places in the new city that the user may interested in and give the places a short description for the user referring to. Similarly, the track give us three files: the 100 example venues include title, description and the corresponding URL for users to evaluate, the contexts that contain 50 cities to serve as new cities and the user profile which is consist of users' judgment to the 100 example venues based on their personal preference. We proposed a novel approach to provide users with suggestions that they may be keen on given the condition that the user stay at different cities.</p><p>We presents a new neural network architecture that using unified vector from down to up for contextual suggestions. Instead of exploiting man-made input features carefully optimized for user model, we represent all attractions and user contexts in the continuous vector space learnt by neural network language models (NNML <ref type="bibr" coords="1,206.68,657.11,11.65,8.74" target="#b0">[1]</ref>)as input layer. We use word2vec <ref type="bibr" coords="1,367.19,657.11,10.52,8.74" target="#b2">[3]</ref> to generate vectors for each venue that given by TREC in the example.csv. Right after this, we mix the users' preference score for the website and the description by different weights and use the mixed score we classify the users' preference degree into 5 categories, range from -1 to 4, i.e., from least interested to the most enjoyed. At last, we take all the vectors that represented the venues as input and take the 5 different like degrees as label to train models for each user using Softmax. Finally, we rank all the venues by using the generated model according the users' personal preference.</p><p>We take the word2vec technique as a tool to digitize all the venues. Given the description of the specific venue, we can use the word2vec to output a hundreds of dimensions of vector to represent it. Immediately following, we use the vectors generated by the wrod2vec and the users' preference degree label to train the preference model by employing the Softmax algorithm and then generate the final suggestion. The rest of the paper is organized as follows: we briefly introduce the word2vec technique in Section 2. In Section 3, we first describe our experiment setup procedure. We introduce the detailed process of user modeling in Section 4 and conclusions are provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Brief Introduction to Train Word Vectors</head><p>To avoid the inaccuracy caused by classifying the example into several categories given by TREC manually, we take the word2vec to represent all attractions and user contexts in the continuous vector space learnt by neural network language models. The base of NNML is using neural networks for the probability function. The model learns simultaneously a distributed representation for each word along with the probability function for word sequences, expressed in terms of these representations. Training such large models, we propose continuous bag of words as our framework, and soft-max as the active function. So we use the word2vec to train wikitravel corpus and got the word vector.</p><p>To avoid the curse of dimensionality by learning a distributed representation for words as our word vector, we define a test set that compare different dimensionality of vectors for our task using the same training data and using the same model architecture. In Table <ref type="table" coords="2,264.17,526.43,4.98,8.74" target="#tab_0">1</ref> below, it can be seen that adding more dimensions will increase improvement at the first stage, and then after some point, it provides diminishing improvement. As Tomas' paper,we observe that train the word vectors dimensions on relatively amount of data , So we chose dimensions as 200. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we first give an overview of test collections and the external knowledge base used in our experiments. Then we describe a novel metric to evaluate retrieval model's effectiveness and stability simultaneously. Finally, we demonstrate that expansion information from external sources is valuable for improving the overall performance of retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>Wikitravel dataset contains numerous famous cities all over around the world and each page corresponding to a specific city gives us a brief introduction to the city. The city homepages are structured according some general rules, with clearly separated sections for sightseeing, eating around, shopping and having fun. All the attractions listed on the city homepages are given by displaying its name, characteristic, open time and telephone number and so on. In this case, we extracted all the city homepages according the contexts given by TREC from the Wikitravel dataset and we clean all the extracted pages by removing the noisy data such as the History section or the Get in section. We only keep the See, Do, Buy, Eat, Drink and Sleep section to extract the places as candidate venues to be suggested for each city. We use H i (i = 1, 2, 50) to represent each extracted city homepage andV i is a set of all the venues in H i .</p><p>As is known, we should provide suggestions for every user-context pair with up to 50 venues which contains title, description and ClueWeb docID. The venues extracted from Wikitravel for each city apparently have no docIDs because the Wikitravel dataset is not part of CluWeb12-CatB. To better solve this problem, we reserve all the URLs of the venues extracted from the city homepage in Wikitravel. Hence, we have all the venues in V i with corresponding URLs. In the ClueWeb12 dataset, there is a file map almost 7 billion URLs to ClueWeb docIDs, so we can easily map all the venues' URLs in V i to docIDs and we use V i to represent all the venues with docIDs in H i . Apparently, V i is a pure subset of V i .</p><p>However, having considered not all the URLs could be found in the file, we figured out another way to ensure there are enough venues to be recommended. This track request all the participants to make suggestions to the travelers in a new city that has never been. That is to say the suggestions we made most are correlated with travel, so we download a travel attraction ontology, which contains many kinds of tour categories such as eating, sightseeing, playing and so on. We use each category C i in as input to word2vec to calculate the most relevant words. For example, we use food category as input, then we gained a ranked list of synonyms displaying by order like that: Hamburg, Bread, Restaurant etc. Each similar word in the ranked list we marked it asT i . We combined the venues that without URLs which can be clearly represented by ∆ i =V i -V i and T i as a query Q i (∆ i ,T i ) to find more venues in the pre-indexed ClueWeb12-CatB dataset. The detailed flow of the extract process could be described as below:</p><p>Algorithm 1 Extract Venues for all Ci such that Ci ∈ Ω do for all Ti such that Ti ∈ Ci do Query the ClueWeb12-CatB by Qi(∆i,Ti) Add the extracted venues into Hi end for end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Preprocessing</head><p>So far, we have generated all the venues which contains title, description and docID for each user-city pair to be suggested. The next step is how to digitize all these venues. We use d j i (m) to represent the mth description of the venues in the city θ j given user u i . The character v Ti represents the vector of term T i . The digitize process of the venues can be depicted as below: The symbol v j i (m) stands for the vector of the m th venue in the city θ j given user u i and U and Θ represents for the 562 users and 50 cities, respectively. We simply add up the vectors of term T i appear in the venue's description. For example, for the venue "Ploy Restaurant", there are terms like "Beef", "Pizza", and "Bread" in its description, we add up all these three terms' vector to form a new vector to represent the "Ploy Restaurant".</p><p>Up to now, we have digitized all the venues to be suggested, in the following section we will introduce how we take the v j i (m) as our user model input and rank the venues according users' personal preference.</p><p>So far, we have generated all the venues which contains title, description and docID for each user-city pair to be suggested. The next step is how to digitize all these venues. We use d j i (m) to represent the mth description of the venues in the city θ j given user u i . The character v Ti represents the vector of term T i . The digitize process of the venues can be depicted as below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Data Collection Preprocessing</head><p>for all u such that u ∈ U do for all θj city such that θj ∈ Θ do for all Ti such that Ti ∈ d j i (m) do Calculate how many Ti is destributed in d j i (m) Each venue's vector v j i (m) = vT i end for end for end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">User modeling</head><p>As mentioned above, we classify the user preference into 5 categories according their score corresponding to the venues' description and the URL in the example given by TREC. We use the supervised machine learning algorithm as the core to train user model. We assume the users' likeability to the venues obey Gaussian distribution and the 5 categories obey the Multiple Bernoulli distribution. Hence, taking the Softmax algorithm to do the multiple classify task will be much appropriate for meeting our needs.</p><p>We generated an N dimensions vector for each term T i and we simply add up linearly all the ( T i ) of term T i that contained in the example venue's description to form a new N dimensions vector ( example(i)). There are 50 example venues in total, so the matrix e xample on behalf of the 50N digitized example venues. The degree of user preference to each venue, i.e., the 5 integer preference categories range from -1 to 4, serve as supervised train label l i , and the 50 dimensions column vector L e xample = [l 1 , l 2 , l 5 0] T stands for the user label matrix. Similarly, the ( all -venues(i)) on behalf of the vector of the venue that to be suggested and ( all -venues) represent all of them.</p><p>The user modeling can be illustrated clearly by Figure <ref type="figure" coords="5,390.84,352.81,3.87,8.74" target="#fig_0">1</ref>: As we know, the algorithm Softmax is good at multi-classify. Given an independent variable as input, it outputs the different probabilities that decides which category the variable belongs to. Finally, the variable is classified to the category that has the highest probability. At the ranking procedure, all the venues are categorized to the 5 categories, we first rank all the venues according to their categories in descending order and then rank the venues through referring their probability belong to one specific category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>We submitted our run to TREC, the RunA use Softmax and RunB use KNN .our result is not as satisfied as we expected. As the TREC not publish all the ranked list and other participants' results, we cannot make it clear what is our rank on earth. However, comparing our performance with the other teams which focus their work in ClueWeb12 in 2013, our performance get the 6th rank position and is worth mentioning having considered the corpus we use is ClueWeb12-CatB which is a very small part of ClueWeb12. To the best of our knowledge, this is the first attempt to use word2vec approach to digitize the venues in order to calculate its attraction to a specific user given his or her history preference data.</p><p>Though the performance of our model is not as so good as others, we proposed a new method and open up a new path to make suggestions for users.</p><p>There are several factors account for the defect of our method. To begin with, the corpus we used is ClueWeb12-CatB which is a very small part of the whole ClueWeb12 and the directly consequence affected by this is that we couldn't find more precise or enough data to make suggestions according to the users' personal preference regardless of whatever the user model we use. Due to the objective factor, we have to leave testing our user model in the whole ClueWeb12 in the next year and we believe our performance will be improved given the whole dataset. In addition, we simply add up all the Ti of terms T i that appear in the venues' descriptions which may lack of considering the correlation among the term T i . And whether add them up linearly have some semantic meaning still remain explored. Lastly, the final factor contribute to influence our preference lays in the ranking procedure which acts as the decisive role. As is well known, the Softmax algorithm is good at classifying other than ranking. We make a bold attempt to use its output probability to rank the venues that belong to the same category. However, the facts proves what have done do not play well and we will modify it in the next year trec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,229.43,546.56,156.49,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. the flow chart of user modeling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,142.27,606.45,301.61,40.02"><head>Table 1 .</head><label>1</label><figDesc>Statistics of test collections and topics.</figDesc><table coords="2,142.27,627.25,301.61,19.22"><row><cell>Dimensionality</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell cols="2">Precision Accuracy [%] 9.7</cell><cell>13.6</cell><cell>20.4</cell><cell>33.8</cell><cell>19.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,209.23,242.10,193.65,51.38"><head>Table 2 .</head><label>2</label><figDesc>the performance of RunA and RunB.</figDesc><table coords="6,209.23,262.89,176.90,30.58"><row><cell>Runid</cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>RunA</cell><cell>0.0488</cell><cell>0.0889</cell><cell>0.1662</cell></row><row><cell>RunB</cell><cell>0.0247</cell><cell>0.0518</cell><cell>0.0581</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,586.20,342.24,7.86;6,146.91,597.16,286.21,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,356.45,586.20,124.14,7.86;6,146.91,597.16,22.41,7.86">A neural probabilistic language model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,177.88,597.16,172.74,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,607.28,342.24,7.86;6,146.91,618.24,333.68,7.86;6,146.91,629.20,67.96,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,146.91,618.24,224.52,7.86">Overview of the trec 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="6,138.35,639.32,342.24,7.86;6,146.91,650.28,263.44,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,342.66,639.32,137.93,7.86;6,146.91,650.28,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
