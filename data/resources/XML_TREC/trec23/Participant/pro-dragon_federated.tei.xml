<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.78,72.35,422.16,16.84">Drexel at TREC 2014 Federated Web Search Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,152.67,118.05,78.41,11.06"><forename type="first">Haozhen</forename><surname>Zhao</surname></persName>
							<email>haozhen.zhao@drexel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Informatics Drexel University Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.60,118.05,62.47,11.06"><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computing and Informatics Drexel University Philadelphia</orgName>
								<address>
									<postCode>19104</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.78,72.35,422.16,16.84">Drexel at TREC 2014 Federated Web Search Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B215D4177590369E64604B0A026B4D58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports our participation in the Federated Web Search Track in TREC 2014. We submitted 21 runs for all the three tasks: Vertical Selection (7), Resource Selection (7) and Results Merging (7). Our main purpose is to test several established resource selection methods on the new realistic FedWeb test collections. We evaluated 7 well known resource selection methods for the vertical selection and resource selection tasks. The effectiveness of these methods in the RS tasks does not carry to the VS tasks, which implies that more sophisticated algorithms and more diverse sources of evidence are needed for solving the VS task effectively. Our Results Merging experiments reveal the correlation between the performance of RM and the performance of its input RS results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Federated Web Search is the task of searching multiple search engines simultaneously and combining their results in a coherent way for presenting to the end user. The Federated Web Search Track 2014 (FedWeb 2014), with its precedent, FedWeb 2013 <ref type="bibr" coords="1,112.10,441.42,9.20,7.86" target="#b4">[4]</ref>, features realistic web test collections for the federated web search task. In addition to the Resource Selection (RS) and Results Merging (RM) tasks in FedWeb 2013, FedWeb 2014 introduced a new task, the Vertical Selection (VS) task. This is our first participation in the Federated Web Search track. In this year's tasks, our main purpose is to evaluate several established resource selection methods on the new Federated Web Search test collections. Though our focus is on the RS task, we also submitted runs for the VS and RM tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RESOURCE SELECTION IN FEDERATED SEARCH</head><p>In a federated search environment, it is generally desirable to query only a subset of all the available resources. Often, this is considered from efficiency point of view, as a selective search strategy generally means quicker search response and lower latency. Moreover, a recent study shows that search effectiveness would not be reduced even when searches are conducted selectively, in particular given the sources are partitioned or distributed properly <ref type="bibr" coords="1,440.93,262.37,11.52,7.86" target="#b5">[5]</ref>. The goal of RS is then, for a given query, to select only the most promising search engines from all those available.</p><p>Most existing methods for RS can be categorized into large document approaches, small document approaches, or classification based approaches <ref type="bibr" coords="1,428.85,314.67,9.20,7.86">[6]</ref>. In our experiments, we employ several small document approaches for Resource Selection task. Small document approaches rely on a centralized sample index (CSI) of the all the sampled documents from each sources. For a given query, search results on CSI are used to estimate the score of a particular resource. Different small document approaches vary in terms of how they use the search results. The following methods are used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ReDDE</head><p>ReDDE proposed by Si and Callan is arguably the most influential small document approach for resource selection <ref type="bibr" coords="1,542.92,441.98,14.62,7.86" target="#b11">[11]</ref>. For a given query, ReDDE estimates the quality of resources based on how relevant documents are distributed in the search results from the CSI. Generally, top k ranked documents are assumed to be relevant. Given sample S and its source resource R, ReDDE assumes each document in the sample represents |R|  |S| documents in the source, where |R|, |S| are the sizes of R and S respectively. It should be noted that in the original ReDDE, each document of the sampled index represents a fixed score for the source document. The score for a given resource is calculated by counting the number of documents from it in the top k search results, and then times the scaling factor |R| |S| :</p><formula xml:id="formula_0" coords="1,364.91,591.90,191.01,26.84">ReDDE(R|q) = |R| |S| • k i=1 I(di ∈ R).<label>(1)</label></formula><p>Later, ReDDE.top <ref type="bibr" coords="1,393.26,627.84,9.71,7.86" target="#b1">[1]</ref> is proposed by Arguello to replace the fixed score with the actual retrieval score of a document in the search result:</p><formula xml:id="formula_1" coords="1,332.78,664.70,223.13,26.84">ReDDE. top(R|q) = |R| |S| • k i=1 I(di ∈ R) RSV(di),<label>(2)</label></formula><p>where RSV(di) is the retrieval status value of di, e.g. P (di|q) in the case of using language model as the retrieval model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CRCS</head><p>The Central-Rank-based Collection Selection (CRCS) approach <ref type="bibr" coords="2,84.92,81.05,14.32,7.86" target="#b10">[10]</ref> proposed by Shokouhi uses the rank of a top k retrieved document to derive its contribution to the calculation of the relevance of a resource to the given query. It uses either a linear or a negative exponential function to convert the document rank to a score, which is then summed in a similar manner as ReDDE to determine the score of the resource. This results CRCSlinear and CRCSExp as two versions of the CRCS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SUSHI and CiSS</head><p>Contrary to ReDDE and CRCS which use only rank information of sampled documents, SUSHI <ref type="bibr" coords="2,207.11,196.56,14.32,7.86" target="#b12">[12]</ref> and CiSS <ref type="bibr" coords="2,262.85,196.56,9.72,7.86" target="#b9">[9]</ref> used the actual relevance scores of the sampled documents to derived the relevance of the sources. SUSHI fits the scores of documents from a particular resource to a smooth curve, and ranks resources via maximizing certain metric, e.g. P@10. SUSHI intentionally selects fewer resources than ReDDE and CRCS methods. To score a resource, CiSS gathers documents belong to that resource in the search result list, and generates a new rank of them based on their relative order. Then the document scores and their new ranks are transformed using exponential function and logarithmic function respectively. A linear function is used to fit documents in the space with log-transformed ranks being the x-axis and exponentially transformed document scores being the y-axis. The resource score is then an integral over this curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATASET AND RETRIEVAL SETUP</head><p>The FedWeb14 test collection, created by the University of Twente group, is used in this year Federated Web Search track <ref type="bibr" coords="2,77.50,401.68,9.71,7.86" target="#b4">[4,</ref><ref type="bibr" coords="2,90.48,401.68,6.47,7.86" target="#b8">8]</ref>. It consists of snippets and documents sampled from search result pages of 149 search engines. 4000 queries are used in building the sample set. As a part of the Vertical Selection task, search engines are categorized into 24 verticals, such as General, Video, Jobs, Academic, and so on. It is noted that each search engine belongs to only one vertical. Previous federated web search experiments generally run on dataset collection, customized by reusing existing IR test collections. The FedWeb13 and FedWeb14 test collections are crawled directly from different vertical search engines, making them more realistic. To our best knowledge, no work has been done to test established resource selection methods on them.</p><p>We created a centralized sample index (CSI) of all the sampled documents. Our index is built with the Indri Toolkit 1 , using the Krovetz stemmer and not removing any stop words.</p><p>For both the VS and RS tasks, the inputs are generated through the following procedure: retrieval top 1000 documents from CSI for each topic. For the retrieval, we used two kinds of retrieval models and two kinds of query modeling. Of the retrieval models, one is BM25 retrieval model with k = 1.2 and b = 0.75, the other is language model with Dirichlet smoothing and µ = 1350 which is about the average document length in the CSI. Of the query models, one uses the plain query terms (PlainQ), the other uses the Markov Random Field Model's sequential dependency query model (MRF-SD-Q) <ref type="bibr" coords="2,137.67,673.67,9.20,7.86" target="#b7">[7]</ref>. This results the following three set of retrieval results for the topics: plain query terms with language model (LM+PlainQ), plain query terms with Okapi 1 http://www.lemurproject.org/indri.php BM25 retrieval model (BM25+PlainQ), MRF sequential dependence query model with language model (LM+MRF-SD-Q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Resource Selection</head><p>The purpose of the RS task is to predict the quality of individual resources for given topics. It is required that all the resources should be ranked for a given search topic, with more relevant resources being ranked higher. Our RS procedure used the following seven RS methods: ReDDE, ReDDE.top, CRCSLinear, CRCSExp, CiSS, CiSSAprox, SUSHI. All of these small document RS approach have reference implementations in the LiDR library<ref type="foot" coords="2,453.37,209.09,3.65,5.24" target="#foot_0">2</ref> by Ilya Markov <ref type="bibr" coords="2,523.82,210.86,9.20,7.86">[6]</ref>. It is noted that many of these algorithms require the size of the resource to approximate the complete ranking with the sampled search results. In our case, size of most involved search engines are not available, therefore we took a bold assumption on the approximation issue by setting the proportion of resource size to sample size to a constant for all resources such that it would not affect the ranking of resources.</p><p>With the 3 retrieval setups detailed in Section 3 and 7 RS methods introduced in Section 2, there are 21 RS run settings in total. We first run all our settings on the FedWeb13 collection, and then choose the top 7 run settings for our FedWeb14 submissions.</p><p>Table <ref type="table" coords="2,351.10,346.85,4.61,7.86" target="#tab_0">1</ref> shows our submitted results: runID nDCG@20 nDCG@10 nP@1 nP@ nP@1 and nP@5 are the normalized graded precision measures introduce in <ref type="bibr" coords="2,391.61,579.55,9.20,7.86" target="#b4">[4]</ref>.</p><p>Based on our submitted results, SUSHI with language model and sequential dependency queries performs the best among all the submitted settings in terms of nDCG@20, nDCG@10 and nP@1. CRCSExp with language model and plain queries performs best in terms of nP@5.</p><p>A query by query comparison between the best performed runs, drexelRS7 and drexelRS1, shows that even though drexelRS7 outperforms drexelRS1 in nDCG@20, both of the two outperforms the other in half of the topics (Figure <ref type="figure" coords="2,540.32,673.69,3.58,7.86" target="#fig_0">1</ref>).</p><p>With the released RS qrels data, we analyzed all our 21 runs and report nDCG@20 and nDCG@10 for all the 21 runs in Table <ref type="table" coords="3,109.16,304.08,3.58,7.86" target="#tab_1">2</ref>. For the three retrieval settings, SUSHI performs best in two of them, and CiSSApprox performs best in the rest BM25 retrieval model setting. The performance of CRCS related methods is more robust across different setups than others, which is consistent with earlier findings in <ref type="bibr" coords="3,53.80,356.38,13.49,7.86" target="#b13">[13]</ref>. More in-depth study can be done to investigate the contributions of different factors, i.e. query model, retrieval model, and RS algorithm, to the differences in IR metrics.</p><formula xml:id="formula_2" coords="3,59.18,377.69,28.29,7.89">runID</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vertical Selection</head><p>In web search, verticals can be defined by topic, e.g. weather, sports, etc., or by media type, e.g. image, video, etc., or by genre of content, e.g. news, blogs, encyclopedia, etc. The user's query may have a strong indication of vertical intent, e.g. "arrow icon", which is clearly oriented to the image vertical, or is intrinsically ambiguous, e.g. "Barack Obama", which may be associated with verticals such as encyclopedia, news, general web and so on. In these scenarios, presenting search results from multiple relevant verticals is desirable and would improve users' satisfaction of the search service.</p><p>The task of vertical selection is to predict and rank the verticals for a given query. A vertical is relevant to a query can be interpreted in two senses. First, the vertical is overall aligned to the user's search intent. Second, the vertical has many relevant documents for the user's query. Zhou et. al. recently empirically showed that the two correlate well with each other. Therefore, the ground truth relevant vertical sets can be determined based on the vertical collection relevance <ref type="bibr" coords="3,381.93,225.01,15.28,7.86" target="#b14">[14]</ref>. The source of evidences for vertical selection may include query string, vertical-representative corpora, and query log associated with the vertical and so on <ref type="bibr" coords="3,324.14,256.39,10.99,7.86" target="#b2">[2]</ref>.</p><p>In this year's work, we approach the VS task in the same way as the RS task. Each vertical is treated as a single resource; all the returned results belong to the resources of a particular vertical are treated as being from the same source. Then the general resource selection procedures are applied on these verticals. Because in the vertical selection task, only a subset of verticals should be returned, we therefore applied a threshold in selecting only the top verticals. With the normalized scores of verticals for each query, we set a cutoff threshold only selecting verticals that by selecting which the discounted gain is beyond the threshold. In the submitted runs, this threshold value is set to 0.01. Table <ref type="table" coords="3,316.81,392.38,4.61,7.86" target="#tab_2">3</ref>  CRCSExp with language model and plain query achieved the highest precision and F1 scores. Overall, our approach are among the medianly performed submissions, perhaps due to to relatively low precision. With the release of the qrels for VS, we investigated whether increasing the cut-off threshold for VS will increase F1 score. Figure <ref type="figure" coords="3,508.42,669.34,4.61,7.86" target="#fig_2">2</ref> shows our results that sweep threshold value from 0.01 to 0.5. Some algorithms such as CiSS and CRCSLinear, witness an increase of F1 at some point, and many other algorithms do not. Our experiments indicated that naively treating verti-cal selection task as a traditional resource selection task is not very effective.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results Merging</head><p>The Results Merging (RM) task is to merge search result snippets from resources selected at the RS stage into a single rank ordered list. The track organizer provides topic search snippets from the 149 search engines for 75 topics. Therefore for each topic, there are 149 sets of snippets organized based on the resources, and for each resource there are 75 sets of snippets organized based on the topics. During the merging stage, only the top 20 resources can be selected as the sources of snippets to be merged. A baseline RS result is provided by the organizer and required to be the input of at least one submitted RM run.</p><p>There exist mainly two kinds of approaches of doing result merging: score based and rank based approaches. Previous researches show that rank-based approaches such as Reciprocal Rank Fusion (RRF) <ref type="bibr" coords="4,165.30,556.06,9.71,7.86" target="#b3">[3]</ref> generally outperforms score based approaches. In our case, there is no score information provided for the snippets, therefore rank-based approach becomes the natural choice.</p><p>Our solution to the result merging task is to leverage the reciprocal rank (RR) of a document as the basic retrieval status value (RSV) for a given snippet. For a given query q, the RR of a document d from the results of a resource Ri is given by:</p><formula xml:id="formula_3" coords="4,126.30,653.11,166.60,19.74">RR(d|q, Ri) = 1 k + r(d)<label>(3)</label></formula><p>where r(d) is d's rank in the result list, and k is generally set to 60. This score is further weighted based on the score or reciprocal rank of the selected resource. Document score weighted by selected resource score is: Score(d|q, Ri) = RS(Ri|q) × RR(d|q, Ri)</p><p>where RS(Ri|q) is the score of resource Ri from the RS stage. Document score weighted by selected resource reciprocal rank is:</p><formula xml:id="formula_5" coords="4,333.78,124.98,222.14,20.23">Score rank (d|q, Ri) = c RS rank (Ri|q) × RR(d|q, Ri)<label>(5)</label></formula><p>where RS rank (Ri|q) is the rank of resource Ri from the RS stage, and c is a constant. The above score is used to output the final merged document ranking list for a given query. It should be noted, we did not consider duplication in the submitted runs.</p><p>Other than the runs based on the baseline resource list from the organizer, we submitted 5 runs based on our resource selection results. The final results are shown in Table <ref type="table" coords="4,316.81,236.81,3.58,7.86" target="#tab_3">4</ref>; the runID prefix indicates its corresponding resource selection run, and the tailing W or R indicates whether it is based on resource score (W) or resource reciprocal rank (R).</p><p>From our results, we can see that the baseline resource list outperforms our RS results. With the qrels of the RS task, we find out the nDCG@20 and nDCG@10 for the baseline RS run is 0.428 and 0.372, respectively. For our best RS run drexelRS7, the nDCG@20 and nDCG@10 are 0.422 and 0.359, which is rather close to the baseline RS run. The nDCG@20 and nDCG@10 of their corresponding RM runs, FW14basemW and drexelRS7mW, are also very close. Therefore, there is a high possibility that performance of RM correlated with the performance of RS in our current methodology. More thorough analysis need to be done to confirm this conjecture.</p><p>Between the two weighting schemes, based on selected resource score or reciprocal rank, the latter generally performances better than the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>We described here the 21 runs we submitted to the Federated Web Search track in TREC 2014. We evaluated 7 well known resource selection methods for the vertical selection and resource selection tasks. The effectiveness of these methods in the RS tasks does not carry to the VS tasks, which implies that more sophisticated algorithms and more diverse sources of evidence are needed for solving the VS task effectively. Our Results Merging experiments reveal the correlation between the performance of RM and the performance of its input RS results.</p><p>More in-depth and comprehensive analysis and comparison of the all the runs, including submitted, not submitted and post-mortem, are planned on the realistic and valuable FedWeb13 and FedWeb14 test collections. runID nDCG@20 nDCG@100 nDCG@20 wdup nDCG@20 local nDCG@100 local nDCG-IA@20 FW14basemR 0. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,53.80,244.47,239.11,7.89;3,53.80,254.93,239.12,7.89;3,53.80,265.39,239.12,7.89;3,53.80,275.85,52.16,7.89"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: nDCG@20 differences between drexelRS7 and drexelRS1 among topics; positive bars indicate drexelRS7 works better for that topic and negative bars worse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,75.95,258.31,14.59,7.19;4,80.12,225.07,10.42,7.19;4,75.95,191.83,14.59,7.19;4,80.12,158.62,10.42,7.19;4,75.95,125.38,14.59,7.19;4,80.12,92.14,10.42,7.19;4,94.01,265.81,203.86,7.19;4,56.74,173.92,7.19,8.75;4,180.59,277.06,33.35,7.19;4,122.08,291.67,104.60,7.19;4,122.08,299.17,95.84,7.19;4,122.08,306.67,111.27,7.19;4,122.08,314.17,87.51,7.19;4,122.08,321.67,123.36,7.19;4,122.08,329.17,111.67,7.19;4,122.08,336.67,96.25,7.19"><head></head><label></label><figDesc>05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 F1 Threshold drexelVS1 (plain-lm-CRCSExp) drexelVS2 (plain-lm-ReDDE) drexelVS3 (plain-lm-CiSSApprox) drexelVS4 (plain-lm-CiSS) drexelVS5 (plain-bm25-CRCSLinear) drexelVS6 (mrfsd-lm-ReDDETop) drexelVS7 (mrfsd-lm-SUSHI)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,53.80,359.64,239.10,7.89;4,53.80,370.10,54.17,7.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Change of F1 as threshold is changed from 0.01 to 0.51</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,366.03,240.30,190.00"><head>Table 1 :</head><label>1</label><figDesc>Resource Selection Results</figDesc><table coords="2,550.44,366.03,6.67,7.89"><row><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,59.18,377.69,217.94,258.81"><head>Table 2 :</head><label>2</label><figDesc>Performance of all 21 RS runs</figDesc><table coords="3,59.18,377.69,217.94,238.03"><row><cell></cell><cell cols="2">nDCG@20 nDCG@10</cell></row><row><cell>mrfsd-lm-CRCSExp</cell><cell>0.3911</cell><cell>0.3450</cell></row><row><cell>mrfsd-lm-CRCSLinear</cell><cell>0.3618</cell><cell>0.2492</cell></row><row><cell>mrfsd-lm-CiSS</cell><cell>0.3487</cell><cell>0.2287</cell></row><row><cell>mrfsd-lm-CiSSApprox</cell><cell>0.3496</cell><cell>0.2289</cell></row><row><cell>mrfsd-lm-ReDDE</cell><cell>0.3464</cell><cell>0.2287</cell></row><row><cell>mrfsd-lm-ReDDETop</cell><cell>0.3821</cell><cell>0.2844</cell></row><row><cell>mrfsd-lm-SUSHI</cell><cell>0.4224</cell><cell>0.3591</cell></row><row><cell>plain-lm-CRCSExp</cell><cell>0.3889</cell><cell>0.3477</cell></row><row><cell>plain-lm-CRCSLinear</cell><cell>0.3498</cell><cell>0.2406</cell></row><row><cell>plain-lm-CiSS</cell><cell>0.3325</cell><cell>0.2289</cell></row><row><cell>plain-lm-CiSSApprox</cell><cell>0.3325</cell><cell>0.2288</cell></row><row><cell>plain-lm-ReDDE</cell><cell>0.3276</cell><cell>0.2268</cell></row><row><cell>plain-lm-ReDDETop</cell><cell>0.3452</cell><cell>0.2424</cell></row><row><cell>plain-lm-SUSHI</cell><cell>0.4047</cell><cell>0.3163</cell></row><row><cell>plain-bm25-CRCSExp</cell><cell>0.3796</cell><cell>0.3238</cell></row><row><cell cols="2">plain-bm25-CRCSLinear 0.3423</cell><cell>0.2414</cell></row><row><cell>plain-bm25-CiSS</cell><cell>0.3858</cell><cell>0.2927</cell></row><row><cell cols="2">plain-bm25-CiSSApprox 0.4095</cell><cell>0.3153</cell></row><row><cell>plain-bm25-ReDDE</cell><cell>0.3405</cell><cell>0.2307</cell></row><row><cell>plain-bm25-ReDDETop</cell><cell>0.3479</cell><cell>0.2349</cell></row><row><cell>plain-bm25-SUSHI</cell><cell>0.3336</cell><cell>0.2422</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.81,392.38,203.73,211.60"><head>Table 3 :</head><label>3</label><figDesc>shows the performance of our submitted runs. Vertical Selection Results</figDesc><table coords="3,316.81,414.51,203.73,189.47"><row><cell>runID</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>drexelVS1</cell><cell>0.240</cell><cell>0.506</cell><cell>0.284</cell></row><row><cell>drexelVS2</cell><cell>0.159</cell><cell>0.824</cell><cell>0.233</cell></row><row><cell>drexelVS3</cell><cell>0.134</cell><cell>0.960</cell><cell>0.212</cell></row><row><cell>drexelVS4</cell><cell>0.134</cell><cell>0.960</cell><cell>0.212</cell></row><row><cell>drexelVS5</cell><cell>0.163</cell><cell>0.824</cell><cell>0.244</cell></row><row><cell>drexelVS6</cell><cell>0.171</cell><cell>0.729</cell><cell>0.251</cell></row><row><cell>drexelVS7</cell><cell>0.189</cell><cell>0.732</cell><cell>0.271</cell></row><row><cell cols="3">drexelVS1 : LM+PlainQ+CRCSExp</cell><cell></cell></row><row><cell cols="2">drexelVS2 : LM+PlainQ+ReDDE</cell><cell></cell><cell></cell></row><row><cell cols="3">drexelVS3 : LM+PlainQ+CiSSAprox</cell><cell></cell></row><row><cell cols="2">drexelVS4 : LM+PlainQ+CiSS</cell><cell></cell><cell></cell></row><row><cell cols="3">drexelVS5 : BM25+PlainQ+CRCSLinear</cell><cell></cell></row><row><cell cols="3">drexelVS6 : LM+MRF-SD-Q+ReDDETop</cell><cell></cell></row><row><cell cols="3">drexelVS7 : LM+MRF-SD-Q+SUSHI</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,69.51,66.45,453.35,94.40"><head>Table 4 :</head><label>4</label><figDesc>Results Merging Task Results</figDesc><table coords="5,69.51,66.45,453.35,73.62"><row><cell></cell><cell>322</cell><cell>0.318</cell><cell>0.361</cell><cell>0.446</cell><cell>0.626</cell><cell>0.107</cell></row><row><cell>FW14basemW</cell><cell>0.260</cell><cell>0.298</cell><cell>0.312</cell><cell>0.367</cell><cell>0.592</cell><cell>0.086</cell></row><row><cell>drexelRS1mR</cell><cell>0.219</cell><cell>0.298</cell><cell>0.222</cell><cell>0.264</cell><cell>0.491</cell><cell>0.059</cell></row><row><cell>drexelRS4mW</cell><cell>0.144</cell><cell>0.244</cell><cell>0.148</cell><cell>0.177</cell><cell>0.420</cell><cell>0.036</cell></row><row><cell>drexelRS6mR</cell><cell>0.198</cell><cell>0.270</cell><cell>0.194</cell><cell>0.232</cell><cell>0.443</cell><cell>0.050</cell></row><row><cell>drexelRS6mW</cell><cell>0.196</cell><cell>0.270</cell><cell>0.193</cell><cell>0.231</cell><cell>0.444</cell><cell>0.049</cell></row><row><cell>drexelRS7mW</cell><cell>0.250</cell><cell>0.305</cell><cell>0.249</cell><cell>0.318</cell><cell>0.535</cell><cell>0.070</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,321.42,711.19,140.83,7.86"><p>https://github.com/markovi/LiDR</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,321.30,610.58,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.60,626.50,142.41,7.86;4,335.61,636.96,215.15,7.86;4,335.61,647.42,197.95,7.86;4,335.61,657.89,218.06,7.86;4,335.61,668.35,137.84,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,335.61,636.96,151.04,7.86">Classification-based resource selection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,504.89,636.96,45.86,7.86;4,335.61,647.42,197.95,7.86;4,335.61,657.89,142.36,7.86">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.60,679.80,197.97,7.86;4,335.61,690.26,176.08,7.86;4,335.61,700.73,206.08,7.86;4,335.61,711.19,179.60,7.86;5,72.59,182.62,219.48,7.86;5,72.59,193.08,117.12,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,335.61,690.26,160.90,7.86">Sources of evidence for vertical selection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Crespo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.61,700.73,206.08,7.86;4,335.61,711.19,179.60,7.86;5,72.59,182.62,132.26,7.86">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,204.54,205.46,7.86;5,72.59,215.00,202.04,7.86;5,72.59,225.46,207.57,7.86;5,72.59,235.92,206.90,7.86;5,72.59,246.38,211.78,7.86;5,72.59,256.85,219.59,7.86;5,72.59,267.31,24.30,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,72.59,215.00,202.04,7.86;5,72.59,225.46,131.76,7.86">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,223.49,225.46,56.66,7.86;5,72.59,235.92,206.90,7.86;5,72.59,246.38,211.78,7.86;5,72.59,256.85,39.56,7.86">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,278.76,187.23,7.86;5,72.59,289.22,209.97,7.86;5,72.59,299.68,158.99,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,127.52,289.22,155.04,7.86;5,72.59,299.68,65.82,7.86">Overview of the TREC 2013 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,156.81,299.68,46.48,7.86">TREC 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,311.14,183.24,7.86;5,72.59,321.60,218.86,7.86;5,72.59,332.06,200.38,7.86;5,72.59,342.52,155.61,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,183.17,311.14,72.66,7.86;5,72.59,321.60,203.43,7.86">Topic-based index partitions for efficient and effective selective search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,72.59,332.06,200.38,7.86;5,72.59,342.52,85.82,7.86">SIGIR 2010 Workshop on Large-Scale Distributed Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,353.98,202.05,7.86;5,72.59,364.44,202.22,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,117.61,353.98,157.03,7.86;5,72.59,364.44,34.46,7.86">Uncertainty in Distributed Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Lugano</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="5,72.59,375.90,211.55,7.86;5,72.59,386.36,204.98,7.86;5,72.59,396.82,220.06,7.86;5,72.59,407.28,211.78,7.86;5,72.59,417.74,219.59,7.86;5,72.59,428.20,24.30,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,190.76,375.90,93.38,7.86;5,72.59,386.36,114.26,7.86">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,205.64,386.36,71.93,7.86;5,72.59,396.82,220.06,7.86;5,72.59,407.28,211.78,7.86;5,72.59,417.74,39.56,7.86">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.59,439.66,187.23,7.86;5,72.59,450.12,191.45,7.86;5,72.59,460.58,213.20,7.86;5,72.59,471.04,219.60,7.86;5,72.59,481.50,210.95,7.86;5,72.59,491.96,199.98,7.86;5,335.61,182.62,24.30,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,127.52,450.12,136.52,7.86;5,72.59,460.58,197.73,7.86">Federated search in the wild: The combined power of over a hundred search engines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,72.59,471.04,219.60,7.86;5,72.59,481.50,210.95,7.86;5,72.59,491.96,10.74,7.86">Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1874" to="1878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.60,194.08,198.43,7.86;5,335.61,204.54,217.11,7.86;5,335.61,215.00,213.02,7.86;5,335.61,225.46,160.40,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,335.61,204.54,217.11,7.86;5,335.61,215.00,113.29,7.86">Modeling information sources as integrals for effective and efficient source selection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salampasis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Satratzemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,455.64,215.00,92.98,7.86;5,335.61,225.46,59.28,7.86">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.60,236.92,211.57,7.86;5,335.61,247.38,214.14,7.86;5,335.61,257.84,207.12,7.86;5,335.61,268.30,219.58,7.86;5,335.61,278.76,89.04,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,392.09,236.92,155.09,7.86;5,335.61,247.38,199.08,7.86">Central-rank-based collection selection in uncooperative distributed information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,335.61,257.84,207.12,7.86;5,335.61,268.30,73.32,7.86">Proceedings of the 29th European Conference on IR Research, ECIR&apos;07</title>
		<meeting>the 29th European Conference on IR Research, ECIR&apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.60,290.22,209.57,7.86;5,335.61,300.68,176.90,7.86;5,335.61,311.14,205.15,7.86;5,335.61,321.60,208.98,7.86;5,335.61,332.06,216.43,7.86;5,335.61,342.52,117.12,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,417.66,290.22,127.52,7.86;5,335.61,300.68,161.70,7.86">Relevant document distribution estimation method for resource selection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,335.61,311.14,205.15,7.86;5,335.61,321.60,208.98,7.86;5,335.61,332.06,129.21,7.86">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.60,353.98,209.16,7.86;5,335.61,364.44,220.32,7.86;5,335.61,374.90,205.73,7.86;5,335.61,385.36,219.65,7.86;5,335.61,397.07,200.77,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="5,456.44,353.98,88.32,7.86;5,335.61,364.44,107.40,7.86">SUSHI: scoring scaled samples for server selection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,460.62,364.44,95.30,7.86;5,335.61,374.90,205.73,7.86;5,335.61,385.36,216.07,7.86">Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.60,408.52,205.72,7.86;5,335.61,418.98,207.69,7.86;5,335.61,429.44,219.60,7.86;5,335.61,439.90,210.95,7.86;5,335.61,450.37,199.98,7.86;5,335.61,460.83,24.30,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="5,335.61,418.98,192.51,7.86">Evaluating reward and risk for vertical selection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,335.61,429.44,219.60,7.86;5,335.61,439.90,210.95,7.86;5,335.61,450.37,10.74,7.86">Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2631" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.60,472.28,217.15,7.86;5,335.61,482.74,211.18,7.86;5,335.61,493.20,155.23,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="5,399.48,482.74,147.31,7.86;5,335.61,493.20,62.84,7.86">Aligning vertical collection relevance with user intent</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,416.72,493.20,45.82,7.86">CIKM 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
