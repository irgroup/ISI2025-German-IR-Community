<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.93,115.90,313.49,12.90;1,155.06,133.83,305.24,12.90">Better Contextual Suggestions in ClueWeb12 Using Domain Knowledge Inferred from The Open Web</title>
				<funder ref="#_tUXFKhp">
					<orgName type="full">Netherlands Organization for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.61,171.88,51.18,8.64"><forename type="first">Thaer</forename><surname>Samar</surname></persName>
							<email>samar@cwi.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Centrum Wiskunde &amp; Informatica</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.24,171.88,76.65,8.64"><forename type="first">Alejandro</forename><surname>Bellogín</surname></persName>
							<email>alejandro.bellogin@uam.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Autónoma de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.21,171.88,68.07,8.64"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centrum Wiskunde &amp; Informatica</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.93,115.90,313.49,12.90;1,155.06,133.83,305.24,12.90">Better Contextual Suggestions in ClueWeb12 Using Domain Knowledge Inferred from The Open Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F66B943C84C8DDDBAF3E350AB264E96A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides an overview of our participation in the Contextual Suggestion Track. The TREC 2014 Contextual Suggestion Track allowed participants to submit personalized rankings using documents either from the Open Web or from an archived, static Web collection (ClueWeb12) collection. One of the main steps in recommending attractions for a particular user in a given context is the selection of the candidate documents. This task is more challenging when relying on ClueWeb12 collection rather than public tourist APIs for finding suggestions. In this paper, we present our approach for selecting candidate suggestions from the entire ClueWeb12 collection using the tourist domain knowledge available in the Open Web. We show that the generated recommendations to the provided user profiles and contexts improve significantly using this inferred domain knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>The Contextual Suggestion TREC Track investigates search techniques for complex information needs that are highly dependent on context and user interests. Input to the task are a set of profiles (users), a set of example suggestions (attractions), and a set of contexts (locations). Each attraction has a title, a description, and a URL. Each profile corresponds to a single user, and indicates the user's preference with respect to each attraction. Two ratings are used: one for the attraction's description and another one for its website. Finally, each context corresponds to a particular geographical location (a city and its corresponding state in the United States). With this information, the task is to provide a personalized ranked list of up to 50 suggestions for every (user, context) pair. Each suggestion should be appropriate to both the user's profile and the context. The description and title of the suggestion may be tailored to reflect the preferences of that user.</p><p>In our second year participating in the Contextual Suggestion track, our main goal has been to analyze the impact of applying the same retrieval model on two collections designed differently. We submitted two runs: one based on a collection which has high geographical precision (GeoFiltered) and another based on a collection created with high recall after projecting Open Web tourist domain knowledge on ClueWeb12 collection (TouristFiltered).</p><p>In the rest of this paper, we present how we generated these sub-collections, their statistics (Section 2), and, results obtained in each case (Section 3). Finally, we conclude the paper with future work lines and general conclusions (Section 4).</p><p>In this section, we describe our approach for generating a personalized rank list of suggestions for each (user, context) topic. First, we present our approach for selecting candidate documents from ClueWeb12 collection. Then, we describe how we generated user's profiles based on the descriptions of the attractions rated by her. For each user, we generate a positive and a negative profile. Then we represent the set of candidate documents and the profiles in the |V |-dimensional vector space (|V | is the size of the vocabulary), where each element in the vector is a pair of term id and the frequency of that term in the document <ref type="bibr" coords="2,238.67,228.70,10.58,8.64" target="#b2">[3]</ref>. Finally, we rank the suggestions based on their similarity with user's profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating the Set of Candidate Documents</head><p>In this section, we describe our approach for creating two sub-collections from ClueWeb12 collection. The first sub-collection named GeoFiltered was created by extracting all documents from ClueWeb12 that pass a geographical filter. The second sub-collection named TouristFiltered was created by exploiting the tourist domain knowledge inferred from the Open Web.</p><p>Geographically Filtered Sub-collection For this sub-collection, we extracted all documents from ClueWeb12 that pass a geographical filter geo filter. This filter selects documents that mention the contexts given by the organisers in the format (City, ST), ignoring those mentioning the city with different states or match multiple contexts. After this process, our subcollection contained 8, 883, 068 documents that passed the filter where each document mention only one context. Thereby, we want to make sure that the filtered documents mention the specific target context (hence, being geographically relevant documents). Despite this, we might still miss other relevant documents due to misspellings or because they mention more than one city at the same time. We name this subcollection GeoFiltered.</p><p>Tourist Filtered Sub-collection This sub-collection was created from ClueWeb12 collection using three different filters using the tourist domain knowledge available on the Open Web as described below:</p><p>1. Tourist sites: We manually selected a list of sites that are well-known of providing tourist information. More precisely, the list consists of (yelp, tripadvisor, wikitravel, zagat, xpedia, orbitz, and travel.yahoo). Then we extracted any document from ClueWeb12 whose host is one of the tourist sites. This part of the TouristFiltered sub-collection is called TouristListFiltered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tourist outlinks:</head><p>To complement the collection, we extracted the outlinks from all documents collected in step 1 and then we extracted those outlinks from the ClueWeb12 collection. We name this part of the TouristFiltered sub-collection Tourist-OutlinksFiltered.</p><p>3. Google and Foursquare APIs: This part was created in two steps. We first queried Foursquare API <ref type="bibr" coords="3,217.03,131.27,11.62,8.64" target="#b0">[1]</ref> to get venues for the given contexts, the API returns 50 urls per query. If the returned venue does not have a URL, then we use the Google Search API to get the URL. The query is the venue name and the context. As a second step, we extracted all the URLs found by Foursquare and Google API from ClueWeb12. We only found 234 out of 2316 documents by exact URL matching after normalizing the URL by removing the www, http://, or https://. The number of documents is very low and actually for some contexts no document was found. Therefore, to alleviate this coverage problem, we extracted the host information from each of the URLs returned by Foursquare and Google APIs, and then we used any document from ClueWeb12 matching these hosts. These documents form what we called the AttractionFiltered sub-collection.</p><p>Table <ref type="table" coords="3,174.10,285.62,4.98,8.64" target="#tab_0">1</ref> shows the number of documents extracted for each sub-collection, whereas Table <ref type="table" coords="3,159.48,297.57,4.98,8.64" target="#tab_1">2</ref> gives more details about the TouristFiltered collection parts. It shows the total number found by each TouristFiltered part filters. The unique column represents the total number of new documents found by the filter that are not already found by the previous filter starting with the TouristListFiltered filter, then TouristOutlinksFiltered, and finally the AttractionFiltered filter. We observe that most of documents in the Tourist-Filtered sub-collection came from the TouristListFiltered and the AttractionFiltered. There is a big overlap between TouristListFiltered and TouristOutlinksFiltered (around 52%). This is because TouristOutlinksFiltered consists of outlinks of TouristListFiltered documents, which contains both external links (link to a page from different host), and internal links (link to a page from the same host) which means that these documents are already in the TouristListFiltered part.  Figure <ref type="figure" coords="4,178.50,360.99,4.98,8.64" target="#fig_0">1</ref> shows the overlap between TouristFiltered parts and the GeoFiltered subcollection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating User Profiles</head><p>We generated a textual user's profile using the descriptions of the attractions rated by the user. The ratings of the descriptions where used to split the user's profiles into positive and negative profiles. The ratings are on a 5-point scale, each rating represents a user's level of interest in visiting the corresponding attraction, the levels ranging from "0" for strongly uninterested to "4" for strongly interested. In this context, we consider the "2.5" as threshold between negative and positive ratings. More precisely, the positive profile consists of all descriptions of the attractions that the user liked, whereas the negative profile is the concatenation of descriptions of the attractions that the user disliked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representation of Documents and User Profiles</head><p>To represent the candidate documents and user profiles in the Vector Space Model (VSM), we first filtered out the HTML tags from the content of the documents. Then we applied standard IR parsing techniques including stemming and stop-words removal from the documents and the user profiles. Once the documents and profiles have been parsed, we generate a dictionary containing a mapping between terms and their integer ids. Finally, we use this dictionary to transform the documents into vectors of weighted terms, where the weight of each dimension (term) is the standard term frequency tf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Personalizing Rankings</head><p>To generate the final ranking (given a pair of context and user information), we compute the similarity in the vector space representation between the document and both the positive and the negative user profiles. We used the cosine function to compute similarities between candidate documents and both the positive and negative profiles as follows:</p><formula xml:id="formula_0" coords="5,218.53,210.27,262.06,26.10">sim(u + , d) = cos(u + , d) = i u i + • d i u + 2 d 2<label>(1)</label></formula><formula xml:id="formula_1" coords="5,220.95,261.53,259.64,26.09">sim(u -, d) = cos(u -, d) = i u i -• d i u -2 d 2<label>(2)</label></formula><p>The final score is based on these two similarity scores using the following equation:</p><formula xml:id="formula_2" coords="5,225.04,335.62,255.55,10.80">score = a • sim(u + , d) + b • sim(u -, d)<label>(3)</label></formula><p>where a=2 and b=-1. The final score in Eq(3) was used to rank the suggestions per (user, context) pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Generating Descriptions and Titles</head><p>For each document suggested to a user in a context, we generate a description and title, which would be tailored to the particular user and context if possible. We decided to only provide personalized descriptions, since we consider the title as a global property of the document, inherent to its content and, thus, should not be different for each user.</p><p>In this situation, we generate the titles by extracting the title or heading tags from the HTML content of the document. On the other hand, we observe the task of generating descriptions similar to snippet generation where the query is the combination of context and user preferences <ref type="bibr" coords="5,221.25,515.65,10.58,8.64" target="#b1">[2]</ref>. Because of that, we aim at obtaining the most relevant sentences for the user within the document in a particular context. To do this, we first split the document into sentences by using the Java BreakIterator class <ref type="foot" coords="5,404.32,537.89,3.49,6.05" target="#foot_0">3</ref> which can detect sentence boundaries in a text. We then followed similar steps to those of the document ranking but at a sentence level, i.e., filter out those sentences not mentioning the context; and, we extracted the text of the description tag from the HTML content. Then we rank sentences according to their similarity with the user profile. Finally, we assumed that larger descriptions were preferred, and hence, we concatenated sentences -in decreasing order of similarity -until the maximum number of bytes (512) was reached, controlling to not combine two very similar sentences to decrease the redundancy.</p><p>In this section we present the analysis of the performance of our runs compared to all runs based on ClueWeb12 collection. We present a detailed comparison between our two runs to show the effect of applying the domain knowledge on extracting touristic related documents from ClueWeb12. Table <ref type="table" coords="6,309.37,185.85,4.98,8.64" target="#tab_2">3</ref> shows the performance of our runs (both using the same scoring function presented in Eq 3 but with a different set of candidate documents (either GeoFiltered or TouristFiltered) compared to the best and median scores of all runs based on the ClueWeb12 dataset. From the Table we see that the TouristFiltered run outperforms the GeoFiltered run in the three metrics. Additionally, to get a more fine grained comparison about the effect of the two subcollections on the performance, in Table <ref type="table" coords="6,295.09,423.38,4.98,8.64" target="#tab_3">4</ref> we present the percentage of topics for which the runs associated to the TouristFiltered and GeoFiltered subcollections give the best and the worst results (we consider only topics having a best score different to its worst score). We observe that, for instance, for the TBG metric, the TouristFiltered subcollection gives the best result for 28% of topics, whereas the GeoFiltered subcollection gives the best result for only 9% of topics; at the same time, the TouristFiltered subcollection gives the worst result for 23% of the topics, whereas the GeoFiltered subcollection gives the worst result for 49% of the topics. Hence, it is clear that these two subcollections produced very different results, where the TouristFiltered subcollection outperforms the GeoFiltered one. All the analyses presented so far confirm that the TouristFiltered run is significantly better than the GeoFiltered run in general. The three metrics P@5, MRR and TBG consider three dimensions of relevance, the geographical (geo) and profile relevance (both in terms of document (doc) and description (desc) judgments). In Table <ref type="table" coords="7,461.78,155.18,4.98,8.64">5</ref> we show how each run performed in the three relevance dimensions. The performance of the GeoFiltered run is on par with TouristFiltered when considering the document and the description relevance, this means that both are similar in terms of their appropriateness to the users. However, we observe a significant difference between TouristFiltered and GeoFiltered when considering the geographical aspect only. The TouristFiltered sub-collection is more geographically appropriate, implying that applying our domain knowledge on the sub-collection creation improves the performance with respect to the geographical dimension of relevance.</p><p>Table <ref type="table" coords="7,157.97,282.56,3.36,8.06">5</ref>. Contribution of description (desc), document (doc), and geographical (geo) relevance to P@5 and MRR metrics for the GeoFiltered and TouristFiltered runs. We denote with (all) when desc, doc, and geo relevance are considered. Bold highlights the best result for each run and metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>GeoFiltered TouristFiltered P@5 all 0.0468 0.1438 P@5 desc-doc 0.2281 0.2348 P@5 desc 0.3064 0.2910 P@5 doc 0. Finally, to provide a deeper insight into the question why the domain knowledgebased sub-collection improves so much over the other sub-collection on the different relevance dimensions, we present in Table <ref type="table" coords="7,309.73,524.68,4.98,8.64">6</ref> the contribution to the relevance dimensions of each of the sub-collections that take part to build the TouristFiltered subcollection (recall that it consists of three parts: TouristListFiltered, TouristOutlinks-Filtered, and AttractionFiltered). Note that the TouristFiltered sub-collection corresponds to the third column, where the three parts are combined.</p><p>To obtain these results, we modified the submission file of the run based on the TouristFiltered sub-collection. First, we compute the performance with keeping only suggestion in the run file that are from TouristListFiltered. Then we added those coming from the TouristOutlinksFiltered part, and finally those from AttractionFiltered. As we observe in the table, there is a major improvement after adding the AttractionFiltered part. As a final comparison, we also include the performance of the AttractionFiltered part alone; we observe that its suggestions are almost comparable to the whole sub-collection but not as good as those, in particular because of the description relevance dimension.</p><p>Table <ref type="table" coords="8,159.23,160.90,3.36,8.06">6</ref>. Effect of TouristFiltered sub-collection parts on P@5 and MRR metrics. We denote TouristListFiltered, TouristOutlinksFiltered, and AttractionFiltered parts as TLF, TOF, and AF respectively. In bold, the best result per column and metric; the best overall result is underlined. In our submission this year we focused on extracting a set of tourist documents from the ClueWeb12 collection. We discussed our approach of selecting candidate documents from the ClueWeb12 collection, using tourist domain knowledge inferred from the Open Web in order to get better suggestions from ClueWeb12 collection. We measured the impact of using the domain knowledge by applying the same retrieval model on two sub-collections created differently; one using the domain knowledge, while the other does not have this knowledge. Based on the analysis of the experimental results, we have observed that the sub-collection that uses the domain knowledge significantly improves the performance of the retrieval model. We applied domain knowledge for creating a subcollection which consists of parts, were each part was defined as a Boolean filter. For the future work we can extend our filter model towards weighted variants based on the content and the domain knowledge. Also in the future we want to study if there are properties of the sub-collection -or the documents that shape it (length, vocabulary distribution, etc.) -that allow to infer whether a subcollection will be useful for a particular task, even before running the retrieval method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,151.59,325.06,312.18,8.12"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Intersection between GeoFiltered collection and TouristFiltered collection parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,224.66,454.87,166.03,59.20"><head>Table 1 .</head><label>1</label><figDesc>Number of documents per collection.</figDesc><table coords="3,238.26,479.38,138.84,34.69"><row><cell cols="2">sub-collection Number of documents</cell></row><row><cell>GeoFiltered</cell><cell>8,883,068</cell></row><row><cell>TouristFiltered</cell><cell>324,374</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,160.90,572.94,293.56,86.12"><head>Table 2 .</head><label>2</label><figDesc>Number of documents for each part of the TouristFiltered sub-collection.</figDesc><table coords="3,170.89,597.44,266.01,61.61"><row><cell>Part</cell><cell cols="2">Number of documents Unique (not already in)</cell></row><row><cell>TouristListFiltered</cell><cell>175,260</cell><cell>175,260</cell></row><row><cell>TouristOutlinksFiltered</cell><cell>97,678</cell><cell>46,801</cell></row><row><cell>AttractionFiltered</cell><cell>102,604</cell><cell>102,313</cell></row><row><cell>Total</cell><cell>375,542</cell><cell>324,374</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,267.69,345.83,101.22"><head>Table 3 .</head><label>3</label><figDesc>Effect of the GeoFiltered and TouristFiltered collections on the performance of our retrieval model. The P@5, MRR and TBG of our runs and the median and best scores of the same metrics for all runs based on ClueWeb12.</figDesc><table coords="6,226.92,312.29,153.95,56.61"><row><cell></cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>GeoFiltered</cell><cell cols="3">0.0468 0.0767 0.1256</cell></row><row><cell cols="4">TouristFiltered 0.1438 0.2307 0.6013</cell></row><row><cell>Median</cell><cell cols="3">0.0542 0.0886 0.1382</cell></row><row><cell>Best</cell><cell cols="3">0.2328 0.4232 0.9615</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,562.98,345.83,86.12"><head>Table 4 .</head><label>4</label><figDesc>Percentage of best and worst topics per run for P@5, MRR and TBG metrics. Bold denotes best value per column.</figDesc><table coords="6,189.93,598.44,227.93,50.65"><row><cell></cell><cell>P@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell></cell><cell>best worst</cell><cell>best worst</cell><cell>best worst</cell></row><row><cell>GeoFiltered</cell><cell cols="3">9.03 41.14 8.70 41.14 9.03 49.16</cell></row><row><cell cols="4">TouristFiltered 28.43 20.07 25.42 20.07 28.43 23.41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="5,144.73,646.97,274.37,6.31;5,144.73,657.93,96.84,6.31"><p>http://docs.oracle.com/javase/6/docs/api/java/text/ BreakIterator.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research</rs> (<rs type="projectName">NWO</rs> project #<rs type="grantNumber">640.005.001</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_tUXFKhp">
					<idno type="grant-number">640.005.001</idno>
					<orgName type="project" subtype="full">NWO</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,151.36,143.22,70.55,8.64;9,253.44,144.16,227.15,7.01;9,151.36,156.11,80.20,7.01" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Foursquare</forename><surname>Api</surname></persName>
		</author>
		<ptr target="https://developer.foursquare.com/docs/venues/search" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,166.95,329.23,8.82;9,151.36,178.91,162.27,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,192.06,167.13,178.89,8.64">The automatic creation of literature abstracts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,379.36,166.95,101.24,8.59;9,151.36,178.91,67.76,8.59">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,151.36,191.04,329.22,8.64;9,151.36,202.82,212.23,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,299.28,191.04,177.17,8.64">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.36,202.82,112.34,8.59">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
