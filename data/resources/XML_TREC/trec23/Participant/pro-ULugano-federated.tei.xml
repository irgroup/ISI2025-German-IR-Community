<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,201.40,139.76,207.26,17.20;1,142.88,161.68,324.23,17.20;1,242.31,183.60,125.41,17.20">Opinions in Federated Search: University of Lugano at TREC 2014 Federated Web Search Track</title>
				<funder ref="#_J8rNb2t">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.54,225.36,108.15,3.02"><forename type="first">Anastasia</forename><surname>Giachanou</surname></persName>
							<email>anastasia.giachanou@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.58,225.36,61.78,3.02"><forename type="first">Ilya</forename><surname>Markov</surname></persName>
							<email>i.markov@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.74,225.36,75.00,3.02"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<email>fabio.crestani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,201.40,139.76,207.26,17.20;1,142.88,161.68,324.23,17.20;1,242.31,183.60,125.41,17.20">Opinions in Federated Search: University of Lugano at TREC 2014 Federated Web Search Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">159C7E4C61348AC052AF91058875B6FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>federated search</term>
					<term>resource selection</term>
					<term>vertical selection</term>
					<term>results merging</term>
					<term>sentiment diversification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical report presents the work carried out at the University of Lugano on TREC 2014 Federated Web Search track. The main motivation behind our approach is to provide better coverage of opinions that are present in federated resources. On the resource selection and vertical selection steps, we apply opinion mining to select opinionated resources/verticals given a user's query. We do this by combining relevancebased selection with lexicon-based opinion mining. On the results merging step, we diversify the final document ranking based on sentiment using the retrieval-interpolated diversification method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the University of Lugano in collaboration with the University of Amsterdam in the TREC 2014 Federated Web Search track (FedWeb14). 1 We participated in three tasks: resource selection, vertical selection and results merging. Our aims are, first, to examine the effectiveness of opinion mining approaches for the vertical and resource selection tasks and, second, to apply sentiment diversification to the results merging task and examine if this approach can lead to better retrieval performance.</p><p>Federated search, also known as Distributed Information Retrieval (DIR), o↵ers the means of simultaneously searching multiple information resources 2 using a single search interface and includes three phases: resource representation, resource selection and results merging <ref type="bibr" coords="1,301.76,640.42,10.51,9.52" target="#b3">[4,</ref><ref type="bibr" coords="1,315.59,640.42,11.62,9.52" target="#b9">10]</ref>.</p><p>The goal of the FedWeb track is "to evaluate approaches to federated search at very large scale in a realistic setting, by combining the search results of existing web search engines". The FedWeb14 collection is di↵erent from a typical document collection because it consists of search results retrieved from 149 di↵erent search engines, each of which is mapped to one vertical (e.g., news, sports, kids, etc).</p><p>The FedWeb14 track focuses on three tasks: vertical selection, resource selection and results merging. Vertical selection aims to identify the subset of categories that will give the most relevant results given a user's query. The aim of the resource selection task is to identify a set of the most relevant resources given the query, while in the results merging task the retrieval results from the selected resources should be merged into a single result list.</p><p>The experiments, described in this technical report, aim to explore an important issue: the e↵ect of considering opinions on di↵erent steps of federated search. For the resource selection task, we follow approaches that combine relevance and opinion <ref type="bibr" coords="2,215.15,283.11,9.96,9.52" target="#b5">[6]</ref>. To calculate the topical relevance of resources, we apply the widely used ReDDE resource selection method <ref type="bibr" coords="2,357.92,295.07,14.61,9.52" target="#b10">[11]</ref>. To calculate the opinionatedness of resources, we use the lexicon-based approach that counts the number of SentiWordNet terms appearing in documents of each resource <ref type="bibr" coords="2,463.59,318.98,9.96,9.52" target="#b1">[2]</ref>. For the last step, i.e., combining relevance and opinion, we use CombSUM <ref type="bibr" coords="2,461.19,330.93,9.96,9.52" target="#b8">[9]</ref>.</p><p>For the results merging task, we apply sentiment diversification to produce the final result which covers di↵erent sentiments, namely positive, negative and neutral. To this end, we first retrieve documents from the top-20 resources, selected at the resource selection phase. Second, we calculate document relevance scores based on their ranks and relevance scores of corresponding resources as in <ref type="bibr" coords="2,146.21,402.66,9.96,9.52" target="#b7">[8]</ref>. Third, we apply the retrieval-interpolated framework <ref type="bibr" coords="2,411.40,402.66,10.51,9.52" target="#b0">[1]</ref> to diversify results by their sentiments.</p><p>In this year's track, organisers introduced a new task: vertical selection. In this task, participants are asked to predict relevant verticals (such as news, sports, etc.) given a user's query. For this task, we simply used the ranking of resources, produced on the resource selection phase, and the mapping between these resources and corresponding verticals to produce our results.</p><p>The rest of the report is organised as follows. In Section 2 we detail our approach for resource selection, vertical selection and results merging tasks. In Section 3 we describe our experimental setup and report results. Section 4 concludes our report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Opinions in Federated Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Resource Selection</head><p>When a user submits a query to a federated search system, resource selection aims to identify the most relevant resources that will further process the query. For the resource selection task in FedWeb14, participants are given a set of queries, a set of search engines/resources and a set of sample documents for each resource. For each query, participants are asked to return a ranked list of search engines according to their relevance to the query. Our approach to the resource selection task focuses on identifying resources that are both relevant to a query and contain opinion.</p><p>In our experiments, we apply the widely used ReDDE resource selection technique <ref type="bibr" coords="3,178.13,115.74,15.49,9.52" target="#b10">[11]</ref> to produce the ranking of the resources. In particular, for every query q we first calculate retrieval scores s(d|q) for documents contained in the centralized sample index (CSI). To build CSI, we use documents sampled from 149 search engines using a set of 4000 queries (sample documents are provided by the organisers). We use the DFR BM25 retrieval function from Terrier<ref type="foot" coords="3,455.57,166.96,3.97,1.76" target="#foot_0">3</ref> because it showed slightly better results compared to other unsupervised retrieval approaches. Then the score of resource R is calculated as follows:</p><formula xml:id="formula_0" coords="3,256.66,196.57,215.96,31.03">s(R|q) = P d2R s(d|q) m (<label>1</label></formula><formula xml:id="formula_1" coords="3,472.63,211.24,4.24,9.52">)</formula><p>where m is the number of documents in CSI that were sampled from resource R.</p><p>In order to calculate the opinion score of resource R, we aggregate opinion scores of documents belonging to this resource. In particular, the opinion score of resource R is calculated as:</p><formula xml:id="formula_2" coords="3,264.08,274.98,212.79,38.11">o(R) = P d2R o(d) |R|<label>(2)</label></formula><p>where o(d) is the opinion score of document d and |R| is the number of documents sampled from resource R. The opinion score of a document is calculated as the expected opinion score of its terms:</p><formula xml:id="formula_3" coords="3,261.23,347.42,215.64,30.03">o(d) = X t2d o(t)p(t|d)<label>(3)</label></formula><p>where p(t|d) is the relative frequency of term t in document d and o(t) is the sentiment of the term obtained from a pre-built lexicon. The relative frequency of term t is calculated as:</p><formula xml:id="formula_4" coords="3,270.31,417.01,206.56,30.17">p(t|d) = tf (t, d) |d|<label>(4)</label></formula><p>where tf (t, d) denotes the number of occurrences of term t in document d and |d| denotes the total number of words in the document.</p><p>In order to produce the final ranking of resources, we need to combine their relevance and opinion scores. To do this, we used the CombSUM data fusion method <ref type="bibr" coords="3,169.42,492.94,9.96,9.52" target="#b8">[9]</ref>:</p><formula xml:id="formula_5" coords="3,224.31,504.90,252.55,10.27">s final (R|q) = s norm (R|q) + o norm (R)<label>(5)</label></formula><p>where s norm (R|q) and o norm (R) are MinMax-normalized relevance and opinion scores of resource R respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vertical Selection</head><p>In web search, a query is associated with a set of verticals each of which focuses on specific domains (e.g., news, travel, and sports) or media types (e.g., images, videos). For the vertical selection task of FedWeb14, participants are given a set of verticals and the mapping from resources to verticals. Each search engine is associated with one category, such as web, news, travel, video, etc. In order to identify the category of a query, we use the provided mapping and our results from the resource selection task. Given those, we assume that if a search engine is selected as relevant for a given user's query, then the category (vertical) of this engine can also be a category of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results Merging</head><p>Given a set of most relevant resources produced on the resource selection phase and their retrieval results, results merging aims to combine those results into a single list. The results merging task in FedWeb14 considers documents retrieved from the top-20 resources. Our approach to results merging aims to diversify the final result list to cover di↵erent sentiments, namely positive, negative and neutral. To this end, we consider both relevance and opinion scores of documents when creating the final merged list.</p><p>For each query and for each resource, the organisers provide a ranked list of documents. However, document relevance scores are not available. To approximate relevance scores s(d|q) for documents from resource R we transform corresponding document ranks r(d|q) as follows:</p><formula xml:id="formula_6" coords="4,258.14,269.38,214.49,16.36">s(d|q) = r(d|q) n s(R|q) (<label>6</label></formula><formula xml:id="formula_7" coords="4,472.63,269.38,4.24,9.52">)</formula><p>where n is the number of documents in the result list produced by R and s(R|q) is the resource selection score of R.</p><p>For the sentiment diversification step we follow the retrieval-interpolated diversification approach <ref type="bibr" coords="4,242.86,330.52,9.96,9.52" target="#b0">[1]</ref>. More specifically, we apply an adaption of the sentiment-contribution-by-strength model (SCS). According to SCS, we first need to calculate the sentiment of each document. We do this using a lexiconbased approach and the SentiWordNet lexicon <ref type="bibr" coords="4,341.79,366.38,9.96,9.52" target="#b1">[2]</ref>. In particular, we calculate the sentiment of a document as the expected sentiment of its terms:</p><formula xml:id="formula_8" coords="4,247.54,392.43,229.32,30.03">sent(d) = X t2d sent(t)p(t|d)<label>(7)</label></formula><p>where p(t|d) is the relative frequency of term t in document d (see Equation ( <ref type="formula" coords="4,464.42,433.63,4.15,9.52" target="#formula_4">4</ref>)) and sent(t) is the dictionary sentiment of t as given by SentiWordNet. The sentiment score sent(d) ranges from 1 to 1, where documents with sent(d) 2 [ 1, 0) are considered negative in terms of opinion, with sent(d) = 0 -neutral and with sent(d) 2 (0, 1] -positive. After calculating relevance and sentiment scores for all documents returned by selected resources, we merge these documents into a single list L by iteratively adding documents to the final list. Here, every next document d ⇤ should maximize the following function:</p><formula xml:id="formula_9" coords="4,225.13,549.26,251.74,13.14">d ⇤ = argmax d (s norm (d|q) + sent 0 (d))<label>(8)</label></formula><p>where s norm (d|q) is the MinMax-normalized document relevance score and sent 0 (d) is calculated as follows:</p><formula xml:id="formula_10" coords="4,203.54,597.72,138.28,38.27">sent 0 (d) = |sent(d)| Y d 0 2L d 0 of same sent.</formula><p>(</p><formula xml:id="formula_11" coords="4,346.25,605.05,60.25,18.52">|sent(d 0 )|)<label>1</label></formula><p>where | • | is the abs function and the product is performed over documents already added to the final list L, which are of the same sentiment as document d. Essentially, this equation promotes documents with a high sentiment score sent(d) and with sentiment, that has low probability in L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1 Tasks</head><p>The TREC 2014 Federated Web Search track proposed three tasks:</p><p>• Vertical selection: given a query and a set of verticals, the goal of this task is to select a subset of relevant verticals. In FedWeb 2014, participants are given 24 di↵erent verticals (e.g., news, blogs, videos etc).</p><p>• Resource selection: given a query, a set of search engines/resources and a set of sample documents for each resource, the goal of this task is to return a ranked list of search engines according to their relevance given the query.</p><p>• Results merging: given a query, the top-20 resources selected on the resource selection phase and their retrieval results, the goal is to merge these results into a single list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>FedWeb14 contains a collection of search results sampled from 149 search engines obtained between April and May 2014. We used Terrier to index this collection, thus, creating CSI. For the lexicon-based opinion mining methods we tried the following opinion lexicons: AmazonKLE, SentiWordNet and MPQA. Based on the experiments with the FedWeb13 dataset <ref type="foot" coords="5,326.68,386.95,3.97,1.76" target="#foot_1">4</ref> we decided to use SentiWordNet due to its superior performance. To calculate retrieval scores of documents in CSI, we considered the following scoring functions from Terrier: BM25, DLH13, Dirichlet Language Model and DFR BM25. The experiments on FedWeb13 showed that DFR BM25 produces the highest MAP, so we used this scoring functions in our runs. We submitted three runs for the resource and vertical selection tasks. One run does not consider opinion (ULuganoDFR) while the other two runs do (ULuganoColL2 and ULuganoDocL2). In ULuganoColL2 we rerank resources considering both their relevance and opinion, while in ULuganoDocL2 documents from CSI are reranked according to their opinion before resource selection is performed.</p><p>Four runs were submitted for the results merging task. Two runs included sentiment diversification while the other two not. In the ULugDFRNoOp and ULugDFROp runs the search engine ranking was obtained from the ULugan-oDFR resource selection run. The ULugFWBsNoOp and ULugFWBsOp runs exploited the baseline resource selection run provided by TREC.</p><p>The tasks were performed on a set of 50 queries provided by FedWeb14. The e↵ectiveness of vertical selection is evaluated by standard classification metrics: precision (P), recall (R) and F-measure (F1). The resource selection task is evaluated by the normalized discounted cumulative gain (nDCG), the variant introduced in <ref type="bibr" coords="5,193.70,634.60,10.51,9.52" target="#b2">[3]</ref> and the normalized precision (nP) introduced in <ref type="bibr" coords="5,417.73,634.60,9.95,9.52" target="#b4">[5]</ref>. The main metric for the results merging is nDCG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results for the vertical selection task are reported in Table <ref type="table" coords="6,400.50,122.17,3.87,9.52" target="#tab_0">1</ref>, for the resource selection task in Table <ref type="table" coords="6,239.80,134.13,4.98,9.52" target="#tab_1">2</ref> and for the results merging task in Table <ref type="table" coords="6,443.20,134.13,3.87,9.52" target="#tab_2">3</ref>. The di↵erence between nDCG@100 and nDCG@100 local is that the latter assumes that only the top-20 selected resources contain relevant documents.  <ref type="table" coords="6,174.90,515.90,4.98,9.52" target="#tab_0">1</ref> shows that all our approaches to vertical selection perform the same. This can be explained by the fact that we did not set any thresholds on the number of selected resources and/or verticals, so our vertical selection methods suggested a large number of verticals (on average, 17 verticals out of 24). This is the main reason for high recall and low precision of our vertical selection approaches.</p><p>Tables <ref type="table" coords="6,180.17,587.63,4.98,9.52" target="#tab_1">2</ref> and<ref type="table" coords="6,209.31,587.63,4.98,9.52" target="#tab_2">3</ref> show the results on resource selection and results merging respectively. The results show that there is no significant di↵erence between the methods that apply opinion mining or sentiment diversification in federated search and the baselines. This was not unexpected since the topics provided by FedWeb14 are not chosen in respect of their relevance to opinionated documents. On the other side, it could be the case some topics to ask for opinionated documents even if this is not required in this track. Having this in mind, FedWeb dataset seemed appropriate for our experiments as it provides the federated environment on which we could incorporate opinions in federated search.</p><p>Previously, sentiment diversification was mainly applied to controversial topics which required opinionated documents to appear in retrieval results <ref type="bibr" coords="7,444.67,115.74,9.96,9.52" target="#b6">[7]</ref>. For such topics presenting di↵erent viewpoints is important and, therefore, sentiment diversification usually performs well <ref type="bibr" coords="7,318.00,139.65,9.96,9.52" target="#b0">[1]</ref>.</p><p>To verify the above hypothesis, we applied sentiment diversification to results merging on the FedWeb13 dataset with available relevance judgements and topics' descriptions. Table <ref type="table" coords="7,249.27,175.51,4.98,9.52" target="#tab_3">4</ref> shows results for a subset of topics from FedWeb13. The descriptions of these topics are given in Table <ref type="table" coords="7,356.32,187.47,3.87,9.52" target="#tab_4">5</ref>. It can be seen that these topics require documents with opinion. From the results in Table <ref type="table" coords="7,419.36,199.42,3.87,9.52" target="#tab_3">4</ref>, we observe that our approach, which diversifies the final result list by sentiment, performs better than the baseline for these topics, proving that sentiment diversification should be used for controversial queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we described our participation in the TREC 2014 Federated Web Search track. For the resource selection and vertical selection tasks, we proposed to combine topical relevance with opinion and used a lexicon-based approach to calculate the opinionatedness of resources/verticals. For the results merging task, we used retrieval-interpolated diversification to provide a comprehensive overview of various opinions in the merged result list. The results of our participation in FedWeb14 did not manage to support the claim that applying opinion mining and sentiment diversification to federated search can lead to a better performance. This can be explained by the fact that topics in the FedWeb14 collection were not chosen for an opinion-related task and, therefore, did not require retrieving documents with opinion. On the other hand, FedWeb13 contains few topics that ask for opinions and, therefore, our methods could improve performance for those topics. We believe, this is a promising result which requires further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,209.05,188.81,187.51,63.34"><head>Table 1 :</head><label>1</label><figDesc>Results for vertical selection runs.</figDesc><table coords="6,218.94,201.76,172.16,50.39"><row><cell>Run</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>ULuganoDFR</cell><cell cols="3">0.117 0.983 0.197</cell></row><row><cell cols="4">ULuganoColL2 0.117 0.983 0.197</cell></row><row><cell cols="4">ULuganoDocL2 0.117 0.983 0.197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,207.34,299.48,190.92,63.34"><head>Table 2 :</head><label>2</label><figDesc>Results for resource selection runs.</figDesc><table coords="6,223.50,312.43,163.04,50.39"><row><cell>Run</cell><cell cols="2">nDCG@20 nP@5</cell></row><row><cell>ULuganoDFR</cell><cell>0.304</cell><cell>0.164</cell></row><row><cell>ULuganoColL2</cell><cell>0.297</cell><cell>0.158</cell></row><row><cell>ULuganoDocL2</cell><cell>0.301</cell><cell>0.160</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,148.11,410.15,296.87,115.27"><head>Table 3 :</head><label>3</label><figDesc>Results for results merging runs.</figDesc><table coords="6,148.11,425.04,296.87,100.38"><row><cell>Run</cell><cell cols="3">nDCG@20 nDCG@20 nDCG@100 local</cell></row><row><cell>ULugDFRNoOp</cell><cell>0.156</cell><cell>0.204</cell><cell>0.362</cell></row><row><cell>ULugDFROp</cell><cell>0.146</cell><cell>0.195</cell><cell>0.346</cell></row><row><cell>ULugFWBsNoOp</cell><cell>0.251</cell><cell>0.296</cell><cell>0.588</cell></row><row><cell>ULugFWBsOp</cell><cell>0.224</cell><cell>0.273</cell><cell>0.545</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,175.64,264.28,254.33,70.08"><head>Table 4 :</head><label>4</label><figDesc>nDCG@20 for a subset of topics from FedWeb13.</figDesc><table coords="7,181.44,279.17,247.16,55.19"><row><cell></cell><cell>Topics</cell></row><row><cell></cell><cell>7007 7084 7109 7415</cell></row><row><cell>Baseline(No Opinion)</cell><cell>0.461 0.847 0.659 0.253</cell></row><row><cell cols="2">Diversified By Sentiment 0.497 0.854 0.745 0.331</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,139.14,378.14,354.84,87.25"><head>Table 5 :</head><label>5</label><figDesc>Topic descriptions. for a thorough text review of Howl from Allen Ginsberg. 7084You want to read some reviews about the movie 'burn after reading'. 7109You are in New York, and are looking for a place to eat pho. 7415You want to know which are this year's most anticipates games.</figDesc><table coords="7,139.14,403.04,104.69,26.48"><row><cell cols="2">Topic Description</cell></row><row><cell>7007</cell><cell>You are looking</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,148.41,696.60,76.21,1.87"><p>http://terrier.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="5,148.41,671.15,160.89,1.87"><p>http://snipdex.org/datasets/fedweb2013</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was partially funded by grant <rs type="grantNumber">P2T1P2 152269</rs> and the <rs type="projectName">OpiTrack</rs> project of the <rs type="funder">Swiss National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_J8rNb2t">
					<idno type="grant-number">P2T1P2 152269</idno>
					<orgName type="project" subtype="full">OpiTrack</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,153.64,192.33,323.22,9.52;8,153.64,211.29,323.24,2.51;8,153.64,223.25,231.15,2.51;8,387.48,216.24,89.39,9.52" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,263.82,192.33,195.92,9.52">Sentiment diversification with di↵erent biases</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aktolga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,153.64,211.29,323.24,2.51;8,153.64,223.25,226.39,2.51">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR&apos;13)</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,236.16,323.23,9.52;8,153.64,248.12,323.23,9.52;8,153.64,267.09,323.22,2.51;8,153.64,279.04,120.94,2.51;8,277.91,272.03,100.78,9.52" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,355.81,236.16,121.07,9.52;8,153.64,248.12,303.22,9.52">SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,153.64,267.09,323.22,2.51;8,153.64,279.04,115.78,2.51">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2200" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,291.95,323.23,9.52;8,153.64,303.91,257.85,9.52;8,415.18,310.92,61.69,2.51;8,153.64,322.88,292.57,2.51;8,449.54,315.86,22.68,9.52" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,218.51,303.91,174.05,9.52">Learning to rank using gradient descent</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,415.18,310.92,61.69,2.51;8,153.64,322.88,287.54,2.51">Proceedings of the 22nd international conference on Machine learning (ICML&apos;05)</title>
		<meeting>the 22nd international conference on Machine learning (ICML&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,335.79,323.23,9.52;8,153.64,347.74,48.12,9.52;8,205.53,354.76,271.33,2.51;8,153.64,366.71,49.14,2.51;8,206.11,359.70,90.82,9.52" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,282.41,335.79,194.45,9.52;8,153.64,347.74,29.10,9.52">Distributed information retrieval and applications</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,205.53,354.76,271.33,2.51;8,153.64,366.71,44.23,2.51">Proceedings of European Conference on Information Retrieval (ECIR&apos;13)</title>
		<meeting>European Conference on Information Retrieval (ECIR&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="865" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,379.62,323.23,9.52;8,153.64,391.58,216.45,9.52;8,373.71,398.59,103.16,2.51;8,153.64,410.55,182.08,2.51;8,339.05,403.53,22.68,9.52" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,424.45,379.62,52.42,9.52;8,153.64,391.58,197.47,9.52">Overview of the TREC 2013 Federated Web Search Track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,373.71,398.59,103.16,2.51;8,153.64,410.55,177.55,2.51">Proceedings of the 22nd Text REtrieval Conference (TREC 2013)</title>
		<meeting>the 22nd Text REtrieval Conference (TREC 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,423.46,323.23,9.52;8,153.64,435.41,146.47,9.52;8,304.04,442.43,172.83,2.51;8,153.64,454.38,22.87,2.51;8,179.84,447.37,74.16,9.52" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,362.32,423.46,114.55,9.52;8,153.64,435.41,142.62,9.52">Aggregation methods for proximity-based opinion retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,304.04,442.43,172.83,2.51;8,153.64,454.38,18.30,2.51">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,467.29,323.22,9.52;8,153.64,479.25,47.73,9.52;8,205.08,486.26,271.79,2.51;8,153.64,498.22,226.28,2.51;8,383.25,491.20,80.86,9.52" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,287.12,467.29,189.74,9.52;8,153.64,479.25,28.89,9.52">Diversifying search results of controversial queries</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kacimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gamper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,205.08,486.26,271.79,2.51;8,153.64,498.22,221.11,2.51">Proceedings of the 20th ACM international Conference on Information and Knowledge Management (CIKM&apos;11)</title>
		<meeting>the 20th ACM international Conference on Information and Knowledge Management (CIKM&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,511.13,323.23,9.52;8,153.64,530.10,323.22,2.51;8,153.64,535.04,90.82,9.52" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,348.09,511.13,110.48,9.52">On CORI results merging</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,153.64,530.10,318.31,2.51">Proceedings of European Conference on Information Retrieval (ECIR&apos;13)</title>
		<meeting>European Conference on Information Retrieval (ECIR&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="752" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,554.97,271.52,9.52;8,427.70,561.98,49.16,2.51;8,153.64,573.93,166.80,2.51;8,323.77,566.92,90.82,9.52" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,267.33,554.97,141.32,9.52">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,427.70,561.98,49.16,2.51;8,153.64,573.93,161.49,2.51">The Second Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,586.85,193.55,9.52;8,355.58,593.86,121.30,2.51;8,153.64,605.82,97.12,2.51;8,254.09,598.80,74.16,9.52" xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,268.47,586.85,78.73,9.52;8,355.58,593.86,121.30,2.51;8,153.64,605.82,92.97,2.51">Federated Search. Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="102" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,153.64,618.73,323.23,9.52;8,153.64,630.68,95.41,9.52;8,253.17,637.70,223.70,2.51;8,153.64,649.65,323.23,2.51;8,153.64,661.61,52.40,2.51;8,209.37,654.59,90.82,9.52" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,240.93,618.73,235.94,9.52;8,153.64,630.68,75.54,9.52">Relevant document distribution estimation method for resource selection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,253.17,637.70,223.70,2.51;8,153.64,649.65,323.23,2.51;8,153.64,661.61,47.64,2.51">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR&apos;03)</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
