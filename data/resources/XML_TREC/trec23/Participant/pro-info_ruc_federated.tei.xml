<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,58.56,58.36,494.87,16.65">RUC at TREC 2014: Select Resources Using Topic Models</title>
				<funder ref="#_AxjNbYA">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.81,80.92,68.38,11.10"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
							<email>qiuyuew@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Renmin</orgName>
								<orgName type="institution">University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.29,80.92,71.94,11.10"><forename type="first">Shaochen</forename><surname>Shi</surname></persName>
							<email>shishaochen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Renmin</orgName>
								<orgName type="institution">University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.47,80.92,45.90,11.10"><forename type="first">Wei</forename><surname>Cao</surname></persName>
							<email>caowei@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Renmin</orgName>
								<orgName type="institution">University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,58.56,58.36,494.87,16.65">RUC at TREC 2014: Select Resources Using Topic Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05F468CB0FC1A54BDC5D7F6FDBE71921</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the work done in Renmin University of China for the Federated Web Search Track of TREC 2014. We participated in the resource selection task. We used the LDA topic modeling approach to select potentially relevant resources for a given query. The initial results are promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Federated Web Search Track [1] is intended to investigate federated search techniques in a realistic Web setting with a large number of online Web search services. This year the track contains three tasks: resource selection (select relevant resources for a given query), vertical selection (classify each query into a fixed set of 24 verticals) and results merging (merge results returned by several resources into a single ranked list). We participated in the resource selection task.</p><p>The input to the resource selection task is a list of 149 online search engines and a collection provided by the organizers consisting of sampled query results (pages and snippets) obtained by sampling 4000 queries on each of the 149 resources. Given a test query, the task is to return a list of resources ranked by their capabilities of returning relevant results for the query.</p><p>The potential relevance of a resource to a given query can be estimated based on many factors <ref type="bibr" coords="1,175.36,425.80,9.86,8.10" target="#b8">[9]</ref>, for example the authority or usefulness of the resource (which is query independent), its content relevance to the query based on text matching or on topical matching with the query.</p><p>Most previous approaches rank the resources according to their content relevance to the query mainly based on text matching, e.g. big-document approaches like CORI <ref type="bibr" coords="1,200.69,495.16,10.67,8.10" target="#b1">[2]</ref> and <ref type="bibr" coords="1,235.09,495.16,9.82,8.10" target="#b3">[4]</ref>, and smalldocument approaches like GAVG <ref type="bibr" coords="1,179.30,505.72,9.87,8.10" target="#b7">[8]</ref>, ReDDE <ref type="bibr" coords="1,226.25,505.72,10.67,8.10" target="#b4">[5]</ref> and CRCS <ref type="bibr" coords="1,281.09,505.72,9.79,8.10">[6]</ref>. In big-document approaches, each resource is represented as a single large document by concatenating all its sampled documents. Thus the resources can be ranked using any existing document retrieval models. In small-document approaches, the sampled documents for each resource are not concatenated but scored individually. Each resource is then ranked based on the matching scores of its sampled documents, either by aggregating the scores or by estimating the density of relevant documents based on the scores.</p><p>All these methods are based on text matching between sampled documents and query, which suffer from the problem of missing vocabulary in the relatively small samples for each resource. Some other approaches address this problem by describing each resource by the categories or topics that it covers and ranking the resources based on topical matching between the query and resource. By modelling resources in a low dimensional topic space, the model can generalize well to unseen documents and thus alleviate the problem of incomplete information caused by small samples.</p><p>Many existing category or topic based approaches make use of predefined category hierarchies <ref type="bibr" coords="1,444.79,199.91,10.67,8.10" target="#b6">[7]</ref>[11], e.g. Open Directory Project (ODP) or KDD-CUP 2005. For instance, <ref type="bibr" coords="1,508.30,210.47,15.24,8.10" target="#b10">[11]</ref> uses the online service of ODP to get the list of ODP categories for each resource and query, and ranks the resource by the similarity of its category vector to that of the query using cosine or Jaccard similarities. In <ref type="bibr" coords="1,373.61,252.74,9.77,8.10" target="#b6">[7]</ref>, resources are first classified into a predefined topical hierarchy by focused probing. Statistical summary for each resource is then smoothed with a set of topically related resources in the topical hierarchy to alleviate the problem of sparse samples. Some other topic based approaches use the extracted topics from the data, e.g. by clustering the documents <ref type="bibr" coords="1,495.10,311.54,10.67,8.10" target="#b2">[3]</ref> or by topic modelling approach <ref type="bibr" coords="1,400.51,322.10,14.09,8.10" target="#b9">[10]</ref>. In <ref type="bibr" coords="1,438.65,322.10,14.07,8.10" target="#b9">[10]</ref>, the authors introduce a hierarchical extension to LDA, which models the generative process of resources explicitly. However, the trained topic models are applied to smooth the estimated term distribution in text matching instead to match the topic distribution directly.</p><p>At TREC 2014, we used the topic modeling approach based on LDA to return the ranked list of resources. First, we train a LDAbased topic model over the collection of sampled documents for all the resources, and obtain the topic distribution for each resource. Given a query, we expand the query using Google Search API, and then infer the topic distribution of the expanded query based on the previously trained topic model. Finally, the resources are ranked according to the distance of their topic distributions to that of the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Resource Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topic Models</head><p>There are various probabilistic topic models used in many applications. Latent Dirichlet Allocation (LDA) is the most frequently used topic model. It is a probabilistic generative model for documents: each document is modelled as a mixture of topics where each topic is a probability distribution of words. The LDA model specifies a probabilistic procedure by which documents can be generated, thus it has the ability to generalize well to unseen documents. Given a set of observed documents, statistical techniques can be applied to infer the latent topic model that is most likely to have generated the observed data. This involves inferring the word distribution for each topic and topic distribution for each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Resource Representation</head><p>For the problem of resource selection, we are interested in knowing the latent topic structure for each resource. Each resource consists of a collection of documents with a small number of them being observed (i.e. sampled). To infer the topic distribution of resources, one can model the generation of resources explicitly in a multi-level topic model like the MCTM in <ref type="bibr" coords="2,64.55,77.39,14.07,8.10" target="#b9">[10]</ref>. Alternatively in this paper, we investigated two simpler strategies to infer the topic distribution for each resource: bigdocument and small-document strategies.</p><p>In the big-document strategy, all the sampled documents for each resource are concatenated to form a single large document. Then the topic model for generating the 149 large documents, each representing a resource, are inferred using a standard Gibbs sampling algorithm implemented in MALLET <ref type="bibr" coords="2,223.18,157.31,14.15,8.10" target="#b12">[13]</ref>.</p><p>At TREC FedWeb 2014, each resource is sampled by 4000 queries with top 10 pages returned for each query being collected, the size of the resulting large document for each resource could be of hundreds of megabytes or even several gigabytes, which renders the inference of topic models on these large documents very inefficient. Inspired by some previous study on predicting page relevance by its snippet <ref type="bibr" coords="2,162.51,237.26,14.70,8.10" target="#b13">[14]</ref>[15], we concatenate all the top 10 snippets instead of pages returned for each sample query to form the large document for a resource. There are 40,000 sampled snippets for each resource at TREC 2014, but the sizes of the snippets are far smaller than the corresponding Web pages. In addition, for some search engines, like the resource e122 (Picasa) in FedWeb 2014, all the sampled pages are non-text files, e.g. image or video files, so the big-documents for such engines by concatenating the text from all its sampled pages would be empty, which causes such resources would not be selected for any queries. When we concatenate the sampled snippets however, the titles and descriptions of the snippets would offer textual depictions about the sampled data so that the resource could still have the chance of being selected for relevant queries. The effectiveness of using snippets to represent resources is demonstrated in the experiment results in Section 3. We name this strategy as snippet-based bigdocument strategy.</p><p>In the small-document strategy, we treat all the sampled documents for all the resources as one collection and train the topic model over the collection to get the topic distribution for each sampled document. Then we represent the topic distribution of each resource using the mean topic distributions of all its sampled documents. We did not investigate the snippet-based small-document strategy because the snippets are short and LDA topic models in general suffer from the sparse word co-occurrence in short texts.</p><p>Before applying MALLET to train topic models, either on the set of large documents or small documents, we preprocess the data by parsing the pages (html, txt, doc, xls, ppt, pdf, xml files) into tokens, removing the stopwords listed in the Indri's standard stopword list, and stemming the tokens with the Krovetz stemmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Representation</head><p>Given a test query, we infer a topic distribution for it using the topic model trained on the document collection as described in Section 2.2. As the query is typically very short, it is hard to infer its topic distribution accurately. To overcome the problem, we first expand the given query using Google Search API. We submit the query to the Google Search API, and collect the top 10 snippets returned by the API. The top 50 most frequent terms occurring in the 10 snippets are selected to expand the query. We preprocess the expanded query using the same tokenization, stopword removal and stemming as that used for document preprocessing, and infer the topic distribution for the query with MALLET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Resource Ranking and Selection</head><p>With the topic distributions inferred for a resource and query, the relevance of the resource to the query can be measured by the extent that they share the same topics, i.e. the similarity between their topic distributions. The rationale behind this is that if a resource is more topically similar to the query, it is more likely to return relevant results for the query.</p><p>A standard function to measure the difference between two probability distributions is the Kullback Leibler (KL) divergence. We compute the KL divergence between the topic distributions of resource R and query Q as the following and rank the resources accordingly.</p><formula xml:id="formula_0" coords="2,365.94,231.12,192.16,21.51">   K i i i i KL R t P Q t P Q t P R Q D 1 ) | ( ) | ( ) | ( ) || (<label>(1)</label></formula><p>where K is the number of topics, which is an input parameter to the LDA topic model. In Section 3, we evaluate the performance with different K values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>Table <ref type="table" coords="2,342.64,319.70,4.50,8.10" target="#tab_0">1</ref> shows the results obtained by evaluating our resource selection approaches on the FedWeb 2013 collection. We evaluate the three strategies of generating resource representations as discussed in Section 2.2, with varying numbers of topics (K) in training the LDA topic model. The "engines" column shows the results of the runs generated using the big-document strategy; "search" column is about all the runs generated by the snippetbased big-document strategy; and "docs" column presents the results of the runs generated by the small-document strategy. The performance of runs is measured by the nDCG@20, which is the main evaluation metric used at the FedWeb research selection task. Our runs are the second best group of runs at FedWeb 2014, right after the runs submitted by East China Normal University. The good performance of their runs largely depends on a queryindependent prior ranking of the resources learned on the results from FedWeb 2013. Such query-independent factors are orthogonal to our approach, so combination of the two could probably further improve the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future work</head><p>In this paper, we describe our participation in the TREC 2014 Federated Web Search Track. We propose to use latent topic modelling approaches, e.g. LDA, to discover the topic structure for each resource and rank the resources with respect to a query based on their topic distribution similarities. To be able to infer topic distributions for very short queries, we expand the query using Google Search API. The initial results are promising. In contrast to the common findings in most text-matching methods, big-document strategy for resource representation is more effective the small-document strategy in our topic-matching approach. More encouraging finding is that snippet-only representation of resources can achieve very good performance, which makes this approach more feasible to be used in practice.</p><p>As for our future work, we would like to further investigate various other topic modelling approaches, e.g. the hierarchical LDA like the MCTM proposed in <ref type="bibr" coords="3,466.01,113.03,14.11,8.10" target="#b9">[10]</ref>, ESA and so on.</p><p>Combining text-matching, topic-matching, and prior ranking of resources offers another interesting opportunity for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,317.93,442.12,240.30,273.21"><head>Table 1 . Performance of variations of the approach on the FedWeb 2013 collection number of topics (K) engines nDCG@20 search docs</head><label>1</label><figDesc>With the increasing number of topics, i.e. larger K, training LDA topic models for the big-and small-document strategies becomes more and more inefficient. We failed in generating the results in some cases within a reasonable amount of time. The missing results are shown as "-" in Table1. We can observe that bigdocument strategy is the most effective one for resource selection, but also the most expensive one. Alternatively snippet-based bigdocument strategy is much cheaper with only a slight degradation on performance.Similar to the results on the FedWeb 2013 collection, snippetbased big-document strategy is more effective than the smalldocument strategy. The optimal number of topics in FedWeb 2014 is different from that in 2013 however. The performance in terms of nDCG@20 achieves the best when the number of topics is 50 in FedWeb 2014, while the optimal number of topics is 100 on the collection of FedWeb 2013. Thus the problem of how to choose the right number of topics arises, which we leave for future work.</figDesc><table coords="2,335.95,493.72,196.17,154.77"><row><cell>10</cell><cell>0.263</cell><cell>0.245</cell><cell>0.110</cell></row><row><cell>20</cell><cell>0.282</cell><cell>0.203</cell><cell>0.163</cell></row><row><cell>30</cell><cell>0.337</cell><cell>0.296</cell><cell>0.221</cell></row><row><cell>50</cell><cell>-</cell><cell>0.314</cell><cell>0.244</cell></row><row><cell>75</cell><cell>-</cell><cell>0.333</cell><cell>0.263</cell></row><row><cell>100</cell><cell>-</cell><cell>0.372</cell><cell>0.260</cell></row><row><cell>125</cell><cell>-</cell><cell>0.369</cell><cell>-</cell></row><row><cell>150</cell><cell>-</cell><cell>0.366</cell><cell>-</cell></row><row><cell>200</cell><cell>-</cell><cell>0.307</cell><cell>-</cell></row><row><cell>250</cell><cell>-</cell><cell>0.328</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,54.00,418.48,230.34,139.26"><head>Table 2 . Performance of variations of the approach on the FedWeb 2014 collection</head><label>2</label><figDesc></figDesc><table coords="3,54.00,445.12,230.00,112.62"><row><cell>Run ID</cell><cell>nDCG</cell><cell>nDCG</cell><cell>nP@1</cell><cell>nP@5</cell></row><row><cell></cell><cell>@20</cell><cell>@10</cell><cell></cell><cell></cell></row><row><cell>FW14Search100</cell><cell>0.505</cell><cell>0.425</cell><cell>0.278</cell><cell>0.384</cell></row><row><cell>FW14Search75</cell><cell>0.461</cell><cell>0.366</cell><cell>0.256</cell><cell>0.345</cell></row><row><cell>FW14Search50</cell><cell>0.517</cell><cell>0.426</cell><cell>0.271</cell><cell>0.404</cell></row><row><cell>FW14Docs100</cell><cell>0.444</cell><cell>0.337</cell><cell>0.165</cell><cell>0.239</cell></row><row><cell>FW14Docs75</cell><cell>0.422</cell><cell>0.306</cell><cell>0.106</cell><cell>0.198</cell></row><row><cell>FW14Docs50</cell><cell>0.419</cell><cell>0.292</cell><cell>0.174</cell><cell>0.203</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">ACKNOWLEDGMENTS</head><p>This work is supported by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61202331</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AxjNbYA">
					<idno type="grant-number">61202331</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,322.43,203.02,92.29,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,230.63,211.47,8.10;3,335.95,241.22,217.75,8.10;3,335.95,251.78,23.49,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,468.56,230.63,78.87,8.10;3,335.95,241.22,132.71,8.10">Searching Distributed Collections With Inference Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,484.88,241.22,42.03,8.10">SIGIR 1995</title>
		<imprint>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,266.30,214.48,8.10;3,335.95,276.86,136.37,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,419.69,266.30,130.75,8.10;3,335.95,276.86,74.52,8.10">Cluster-Based Language Models for Distributed Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,426.19,276.86,42.03,8.10">SIGIR 1999</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,291.38,221.82,8.10;3,335.95,301.94,214.95,8.10;3,335.95,312.50,103.89,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,476.11,291.38,81.67,8.10;3,335.95,301.94,201.02,8.10">A Language Modeling Framework for Resource Selection and Results Merging</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.95,312.50,42.17,8.10">CIKM 2002</title>
		<imprint>
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,327.02,188.75,8.10;3,335.95,337.58,214.16,8.10;3,335.95,348.14,55.29,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,407.11,327.02,117.58,8.10;3,335.95,337.58,152.35,8.10">Relevant Document Distribution Estimation Method for Resource Selection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,503.99,337.58,42.03,8.10">SIGIR 2003</title>
		<imprint>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,362.66,210.41,8.10;3,335.95,373.22,211.21,8.10;3,335.95,383.78,78.09,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,387.89,362.66,158.48,8.10;3,335.95,373.22,175.08,8.10">Central-Rank-Based Collection Selection in Uncooperative Distributed Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,526.66,373.22,20.50,8.10;3,335.95,383.78,16.37,8.10">ECIR 2007</title>
		<imprint>
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,398.32,221.79,8.10;3,335.95,408.88,224.30,8.10;3,335.95,419.44,102.69,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="3,449.67,398.32,108.07,8.10;3,335.95,408.88,105.56,8.10">Classification-Aware Hidden-Web Text Database Selection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,447.51,408.88,80.64,8.10">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008-04">April 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,433.96,206.45,8.10;3,335.95,444.52,161.04,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,422.09,433.96,120.31,8.10;3,335.95,444.52,32.25,8.10">Blog Site Search Using Resource Selection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,383.88,444.52,42.17,8.10">CIKM 2008</title>
		<imprint>
			<biblScope unit="page" from="1053" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,459.04,201.46,8.10;3,335.95,469.60,196.80,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,463.03,459.04,74.38,8.10;3,335.95,469.60,68.00,8.10">Classification-Based Resource Selection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,419.64,469.60,42.26,8.10">CIKM 2009</title>
		<imprint>
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,484.12,189.03,8.10;3,335.95,494.68,191.43,8.10;3,335.95,505.24,151.20,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="3,482.08,484.12,42.89,8.10;3,335.95,494.68,187.61,8.10">A Multiple-Collection Latent Topic Model for Federated Search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Baillie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carmen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,335.95,505.24,78.06,8.10">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="390" to="412" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,519.76,220.25,8.10;3,335.95,530.32,210.93,8.10;3,335.95,540.88,215.01,8.10;3,335.95,551.44,55.50,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="3,380.59,530.32,166.29,8.10;3,335.95,540.88,211.22,8.10">CWI and TU Delft at TREC 2013: Contextual Suggestion, Federated Web Search, KBA, and Web Tracks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bellogin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Samar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,345.79,551.44,22.91,8.10">TREC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,565.99,186.72,8.10;3,335.95,576.55,211.56,8.10;3,335.95,587.11,52.41,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="3,465.41,565.99,57.27,8.10;3,335.95,576.55,34.41,8.10">Latent Dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,376.09,576.55,139.86,8.10">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="993" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,601.63,193.26,8.10;3,335.95,612.19,187.20,8.10" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu" />
		<title level="m" coord="3,400.80,601.63,128.41,8.10;3,335.95,612.19,62.68,8.10">MALLET: A Machine Learning for Language Toolkit</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,626.71,216.48,8.10;3,335.95,637.27,221.82,8.10;3,335.95,647.83,43.17,8.10" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="3,335.95,637.27,208.20,8.10">What Snippets Say About Pages in Federated Web Search</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.95,662.35,213.79,8.10;3,335.95,672.91,195.00,8.10;3,335.95,683.47,139.20,8.10" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="3,384.31,672.91,146.64,8.10;3,335.95,683.47,79.90,8.10">Snippet-based Relevance Predictions for Federated Web Search</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
