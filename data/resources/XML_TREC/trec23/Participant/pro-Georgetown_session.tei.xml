<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.67,77.35,418.57,15.22;1,169.77,96.07,272.43,13.91">Modeling Rich Interactions in Session Search ⎯ Georgetown University at TREC 2014 Session Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,212.84,123.10,45.75,10.54"><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.18,123.10,61.66,10.54"><forename type="first">Xuchu</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,352.16,123.10,46.99,10.54"><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.georgetown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<addrLine>37 th and O Street NW</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.67,77.35,418.57,15.22;1,169.77,96.07,272.43,13.91">Modeling Rich Interactions in Session Search ⎯ Georgetown University at TREC 2014 Session Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BFBEC4515DF733CC2685ED05C9EF21EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year we participate in the TREC Session Track Task 1. We adopt the Query Change Model (QCM), weighted QCM, re-ranking, clustering, and error analysis in our approaches. The QCM retrieval model is employed to combine all queries in a session. QCM allows documents that are relevant to any query in a session to appear in the final retrieval list. Weighted QCM combines queries unevenly based on a prediction of query quality. It is based on the following intuition: if a query does not bring any document that leads to a SAT-Click from the user, it suggests that this query is poorly formed. Our re-ranking module is based on implicit feedback from the user; in this case the SAT-Clicked documents. The module boosts a document's ranking position if it has been SAT-Clicked in the session or in other sessions that share similar search topics. We apply K-means clustering algorithm to detect which sessions share similar search topics. Each unique term is one dimension of the vector and is weighted by its idf. We also apply session error analysis in RL3. From the query log, we first identify sessions with similar topics by clustering, then we use SAT-Clicks from most sessions to re-rank the documents for the sessions that the algorithm predicts as poorly issued sessions, i.e. more difficult session due to ill-form queries. Combining above approaches, we achieve a 20.9% nDCG@10 increment and a 13.0% P@10 increment from RL1 to RL2, and with utilization of the whole log data, we achieve a 4% nDCG@10 increment and a 0.5% P@10 increment from RL2 to RL3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Session search involves multiple search iterations triggered by query reformulations to accomplish a complex search task. In our groups' 2013 work <ref type="bibr" coords="1,284.06,456.53,11.64,9.48">[1]</ref>, we model this interactive process of session search as a MDP process. In our 2014 work <ref type="bibr" coords="1,230.87,469.01,24.33,9.48">[3][4]</ref>, we model it as a POMDP process. TREC 2014 Session track Task 1 intends to test whether we can utilize user interactions with a search engine in a session to improve search accuracy. The task data includes log data of 1021 sessions. The log data of each session records a sequence of queries q 1 ,q 2 , … ,q n-1 ,q n triggered by users, where q n is the current query in the session. The log also contains retrieved ranking lists for each past query, q 1 to q n-1 . Finally the log data collects userclicked documents/snippets and the dwell time that users spend on each clicked document. There are three subtasks, RL1, RL2 and RL3. RL1 ignores all information in the session log and only relies on the current query to retrieve results. RL2 uses only information from current session to retrieve. RL3 uses any information in the session log to retrieve.</p><p>We apply different technologies in each sub tasks. In RL1, we directly feed the last query of a session to Lemur Search Engine. The retrieval algorithm is set as Language Modeling with Dirichlet smoothing. The smoothing parameter mu is set as 5000. In RL2, we adopt QCM algorithm <ref type="bibr" coords="1,421.64,612.29,12.76,9.48">[1]</ref> where we combine all queries in a session to formulate effective structured queries. Each search term is assigned with a weight, which is calculated based on whether the term occurs in previous SAT-Clicked documents and whether the term is newly added or removed from previous query. Further more we decrease a previous query's weight if the query didn't bring any document, which leads to a SAT-Click from user. Finally we boost a document's ranking score if it has been SAT-Clicked in the session. In RL3, we also apply QCM and decrease query weight if no SAT-Click documents are retrieved by it. And we boost a document's ranking score if it is SAT-Clicked in sessions that belongs to the same or similar topic. We identify similar topics by clustering sessions based on query similarity using K-means clustering algorithm. Another tactic we applied in RL3 is to replacing bad session's retrieval results with good session's results whose search topic is similar. We evaluate session's performance based on user's click numbers.</p><p>We organize this paper as follow. We discuss each technical approach in detail from Section 2 to Section 7. In Section 8, we present our submissions and the evaluation results. In Section 9, we conclude our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ad-hoc Retrieval Model (Ad-hoc)</head><p>Our RL1 approach directly uses the current query of each session as search terms. The retrieval algorithm is Language Modeling with Dirichlet smoothing <ref type="bibr" coords="2,290.63,196.13,11.64,9.48">[2]</ref>. The document d's relevance score towards a search term t is calculated by formula:</p><formula xml:id="formula_0" coords="2,240.67,224.88,130.57,25.44">𝑃 𝑡 𝑞 = 𝑡𝑓 𝑡, 𝑑 + 𝜇𝑃(𝑡|𝐶) 𝑙𝑒𝑛𝑔𝑡ℎ 𝑑 + 𝜇</formula><p>where length(d) is document d's length. P(t|C) is the probability that term t appears in corpus C. µ is the Dirichlet smoothing parameter and is set to 5000 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Query Change Retrieval Model (QCM)</head><p>In session, users modify queries to better express their information needs. In Query Change Retrieval Model (QCM) [1], query changes are considered as relevance feedback to adjust query term weights. First, it defines Δq i =q i -q i-1 as the query change between two adjacent queries, q i-1 and q i . Then Δq i is divided into three parts: the added terms (+Δq i ) the removed terms (-Δq i ) and the theme terms (q theme ).</p><p>Table <ref type="table" coords="2,192.64,375.65,5.40,9.48">1</ref> A Query Change Example (TREC 2014 Session 52)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Session</head><p>Queries Query Change Q theme Session52 q 1 = hydropower efficiency +Δq 2 = environment hydropower q 2 = hydropower environment -Δq 2 = efficiency q 3 = hydropower damage +Δq 3 = damage -Δq 3 = environment Table <ref type="table" coords="2,101.61,466.37,5.40,9.48">1</ref> presents an example of query changes in TREC 2014 Session track. In query q i , query term weights are adjusted based on four types of strategies, W Theme , W Add,In , W Add,Out and W Remove <ref type="bibr" coords="2,502.50,478.85,11.64,9.48">[1]</ref>. The relevance score between query q i and a document d becomes:</p><formula xml:id="formula_1" coords="2,129.60,512.40,352.06,11.12">𝑆𝑐𝑜𝑟𝑒 𝑞 ! , 𝑑 = logP 𝑞 ! 𝑑 + 𝛼𝑊 !!!"! -𝛽𝑊 !"",!" + 𝜀𝑊 !"",!"# -𝛿𝑊 !"#$%"</formula><p>Parameters α, β, ε and δ are the linear weighting coefficients for each type of strategies. They are set as α=2.2, β=1.8, ε=0.07 and δ=0.4 in our submission. The QCM model combines all queries in a session using formula:</p><formula xml:id="formula_2" coords="2,209.17,578.88,193.58,37.28">𝑆𝑐𝑜𝑟𝑒 !"# 𝑞 !..! , 𝑑 = 𝛾 !!! ! !!! 𝑆𝑐𝑜𝑟𝑒(𝑞 ! , 𝑑)</formula><p>where γ is the discount factor for the prior queries in the session. In TREC Session track's setting, evaluation is based on the whole session. The prior queries are equally important as current query, hence we set γ as 1 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Weighted QCM</head><p>QCM allows documents that are relevant to any query in a session to appear in the final retrieval list. When set parameter γ = 1, we combine all queries in a session evenly. However we argue that queries shouldn't be evenly combined. Here we define two concepts, Strong SAT-Clicked document and Weak SAT-Clicked document. Strong SAT-Clicked document means a retrieved document that has been clicked by a user and he/she dwelled more than 30 seconds on this document. Weak SAT-Clicked document is also a clicked document but with dwell time more than 10 seconds and less than 30 seconds.</p><p>We assume that dwell time on a clicked document indicates how relevant that document is. If a query doesn't bring any document that leads to a SAT-Click from the user, it indicates that this query is poor formed. Hence these queries' weight should be decrease. Weighted QCM combine queries based on query quality. Poor formed queries' weight is decreased by a factor ω ∈ (0, 1). Its score function is:</p><formula xml:id="formula_3" coords="3,131.82,193.92,348.27,24.24">𝑆𝑐𝑜𝑟𝑒 !"#$ 𝑞 !..! , 𝑑 = 𝑆𝑐𝑜𝑟𝑒 !"# (𝑞 ! , 𝑑) ! ! ∈! !""# + 𝜔 𝑆𝑐𝑜𝑟𝑒 !"# (𝑞 ! , 𝑑) ! ! ∈! !"#</formula><p>Q good is the query set in which every query brings at least one SAT-Click from users. While Q bad is the query set in which every query brings zero SAT-Click from users. The current query is an exception. It brings zero SAT-Click because it has no retrieval results yet, however it belongs to Q good .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">User-Click Model</head><p>Since SAT-Click indicates a document's relevance, we boost a document's ranking score, if it is SAT-Clicked by users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Session Level User-Click Model</head><p>In this approach, we only use information in the current session. We boost a document's ranking score if it has been SAT-Clicked in the current session. The score function is:</p><formula xml:id="formula_4" coords="3,96.50,383.28,418.91,50.88">𝑆𝑐𝑜𝑟𝑒 !"!!#$%!!"#!$ 𝑞 !..! , 𝑑 = 𝑆𝑐𝑜𝑟𝑒 !"# 𝑞 !..! , 𝑑 + 𝑆𝑐𝑜𝑟𝑒 !"!!#$%!!""#$ (𝑞 !..! , 𝑑) 𝑆𝑐𝑜𝑟𝑒 !"!!#$%!!""#$ 𝑞 !..! , 𝑑 = 𝜓 𝑆𝑡𝑟𝑜𝑛𝑔𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! + 𝜃|𝑊𝑒𝑎𝑘𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! | ( 𝜓 𝑆𝑡𝑟𝑜𝑛𝑔𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! ! + 𝜃 𝑊𝑒𝑎𝑘𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! ! ) ! ! ∈!"!!#$%</formula><p>Where |StrongSATClicks d | is the number of times that document d is strongly SAT-Clicked in the current session. |WeakSATClicks d | is the number of times that d is weakly SAT-Clicked. The boosting score is normalized by the total number of SAT-Clicks in the session. We experimentally set ψ=2 and θ=1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Topic Level User-Click Model</head><p>This approach is similar to the Session Level User-Click Model. The difference is that instead of only using the information in the current session, we utilize information in all sessions that share similar search topics. We cluster sessions based on their search topics. The cluster algorithm is described in detail in Section 6. We boost a document's ranking score if it has been SAT-Clicked in sessions that share similar search topics with the current session. The score function is:</p><formula xml:id="formula_5" coords="3,96.62,582.72,418.68,50.64">𝑆𝑐𝑜𝑟𝑒 !"#$%&amp;'!!"#!$ 𝑞 !..! , 𝑑 = 𝑆𝑐𝑜𝑟𝑒 !"# 𝑞 !..! , 𝑑 + 𝑆𝑐𝑜𝑟𝑒 !"#$%&amp;'!!""#$ (𝑞 !..! , 𝑑) 𝑆𝑐𝑜𝑟𝑒 !"#$%&amp;'!!""#$ 𝑞 !..! , 𝑑 = 𝜓 𝑆𝑡𝑟𝑜𝑛𝑔𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! + 𝜃|𝑊𝑒𝑎𝑘𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! | ( 𝜓 𝑆𝑡𝑟𝑜𝑛𝑔𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! ! + 𝜃 𝑊𝑒𝑎𝑘𝑆𝐴𝑇𝐶𝑙𝑖𝑐𝑘𝑠 ! ! ) ! ! ∈!"#$%&amp;'</formula><p>Where Cluster is a set of sessions that share similar search topics with the current session. |StrongSATClicks d | is the number of times that document d is strongly SAT-Clicked in the Cluster. |WeakSATClicks d | is the number of times that d is weakly SAT-Clicked. The boosting score is normalized by the total number of SAT-Clicks in the Cluster. We also set ψ=2 and θ=1.</p><p>We cluster sessions based on search topics by comparing queries' similarity between different sessions.</p><p>• First, we combine all queries in one session and convert it into a term vector. Each unique search term is one dimension of the vector. • Then, we assign terms' idf value as weight to each term dimension.</p><p>• Finally, we cluster sessions based on the Euclidean distance of their query vectors.</p><p>We use K-means clustering algorithm and set K as 60, which is the number of distinctive topic ids in the log file. This number may not be obtainable in a real search environment. We can train it or choose a relatively large K in such situation. Other clustering algorithms without requirement of predetermination of cluster numbers could be other alternatives, however we didn't explore them in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Session Performance Prediction and Replacement</head><p>We a specific schema in sessions that share similar search topics, most of which contain SAT-Clicks, however a few do not. It indicates that for the few sessions, the bad retrieval results may be caused due to ill formed queries rather than difficult search tasks. For these sessions, we replace their retrieval results with good session's results whose search topic is similar. The user did not click any retrieved document in session s.</p><formula xml:id="formula_6" coords="4,89.28,350.93,312.97,67.93">F 3 𝑡 !"#$$ ≤5s. F 4 # of unique terms in the session s≥20. F 5 𝑡 !"#$$_!"#_!"#!$ &lt; 𝑡 !"#$$_!"#_!"#!$ (!) 2 F 6</formula><p>Session s does not contain the most frequent term in T(s). F 7 # of unique terms in session s≤6 F 8 # 𝑜𝑓 𝑆𝐴𝑇 𝑐𝑙𝑖𝑐𝑘𝑠 𝑖𝑛 𝑠𝑒𝑠𝑠𝑖𝑜𝑛 𝑠 &lt; # 𝑜𝑓 𝑆𝐴𝑇 𝑐𝑙𝑖𝑐𝑘𝑠 𝑖𝑛 𝑠𝑒𝑠𝑠𝑖𝑜𝑛 𝑠′</p><formula xml:id="formula_7" coords="4,325.77,438.96,27.51,7.04">!!∈!(!)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|𝑇(𝑠)|</head><p>In order to identify good sessions from bad sessions automatically, we extract eight features from session click data log. For convenience, we introduce some symbols firstly. For each session s, let us use 𝑡 !"#$$ to denote the user's total dwell time in the whole session and calculate the average dwell time</p><formula xml:id="formula_8" coords="4,72.00,508.56,325.72,41.36">𝑡 !"#$$_!"#_!"#!$ as 𝑡 !"#$$_!"#_!"#!$ = ! !"#$$ # !" !"#!$% !" !"!!#$% ! .</formula><p>Then all the average dwell times are sorted in a descending order,</p><formula xml:id="formula_9" coords="4,180.00,583.20,210.13,16.40">𝑡 !"#$$_!"#_!"#!" (!) , 𝑡 !"#$$_!"#_!"#!$ (!) , 𝑡 !"#$$_!"#_!"#!$ (!)</formula><p>, …</p><p>Moreover, we use T(s) to represent the topic cluster including s. Based on the above symbols, all the features can be listed in Table <ref type="table" coords="4,206.39,625.01,4.10,9.48" target="#tab_0">2</ref>.</p><p>Here, F 1 is set up to deal with a shortcoming of QCM. According to our experience, when applying QCM to session search, the nDCG scores are often small in case that the user try to compare several items in one session. For example, the user may want to compare different infant developmental milestones depending on culture through posing a query like "culture difference in milestones in 0-12 month olds". This is an example from the 111 th session in Session Track 2014. We treat one session as this kind when the queries include terms with patterns like "compare", "differ", "versus", "vs" and "v.s.".</p><p>All the eight features are Boolean, i.e. should be TRUE or FALSE. For each feature F i (i=1,2,…,8), we count the number of sessions satisfying F i =TRUE. For each session s, an estimation score score e (s) is calculated as follows:</p><formula xml:id="formula_10" coords="5,173.45,121.20,283.02,37.28">𝑠𝑐𝑜𝑟𝑒 ! 𝑠 = 1 # 𝑜𝑓 𝑠𝑒𝑠𝑠𝑖𝑜𝑛𝑠 𝑠𝑎𝑡𝑖𝑠𝑓𝑦𝑖𝑛𝑔 𝐹 ! = TRUE ! !!! * 𝐼(𝐹 ! )</formula><p>where I(F i ) is an indicator function. It returns 1 if session s satisfies feature F i , otherwise it returns 0. All the sessions are ranked according to their estimation scores. The top 1/3 sessions are regarded as bad sessions and the others are regarded as good sessions.  <ref type="table" coords="5,218.45,494.45,5.40,9.48">4</ref> nDCG@10 and P@10 for top 100 sessions GUS14RUN1 GUS14RUN2 GUS14RUN3 Max Med nDCG@10 P@10 nDCG@10 P@10 nDCG@10 P@10 nDCG@10 P@10 nDCG@10 P@10 RL1 0.  <ref type="table" coords="5,99.84,590.21,5.40,9.48" target="#tab_1">3</ref> lists our submissions in TREC 2014 Session Track and their configurations. We submit three runs in total: GUS14RUN1, GUS14RUN2 and GUS14RUN3. Each run contains three ranking lists, one for task RL1, one for task RL2 and one for task RL3.</p><p>It is worthwhile to point out that in GUS14RUN3 task RL3, we apply Topic Level User-Click differently.</p><p>Here we did not using clustering to determine sessions that share similar search topics, instead we directly apply topic id in the log file to determine session topic's similarity. By doing this we can evaluate the effectiveness of applying the clustering method in Session Search. Further when we apply Session Performance Prediction and Replacement, we also use topic id to determine session clusters. We don't use clustering to determine session clusters, because clustering is based on comparing query similarity. If the queries are similar, then the retrieval performance should be close too. Hence it is difficult to find a good session to replace bad sessions when sessions are clustered by query similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Results</head><p>Table <ref type="table" coords="6,99.89,128.93,5.40,9.48">4</ref> shows the evaluation results of our submissions. The result shows that by utilizing current session information, we achieve a 20.9% nDCG@10 increment and a 13.0% P@10 increment from RL1 to RL2, and with utilization of the whole log data, we achieve a 4% nDCG@10 increment and a 0.5% P@10 increment from RL2 to RL3. All submissions achieve a significant performance improvement from RL1 to RL2, however only GUS14RUN3 achieves a small improvement from RL2 to RL3. It may be caused by the features of Session track tasks. The search tasks are relatively complex. There are rich interactions in the session to help search engine to infer user intent. However there are few similar sessions can be used to recommend good documents for the current session. GUS14RUN2 RL3 and GUS14RUN3 RL3's performances are close, which suggests that clustering sessions by query similarity is as good as directly using topic ids. GUS14RUN3's RL3 gets highest P@10 scores in 20 sessions out of first 100 sessions. It proves that our approaches are highly effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>We apply a combination of several technologies to TREC 2014 Session track. We achieve a significant performance boost from RL1 to RL2, and a small improvement from RL2 to RL3. The evaluation results suggest that 1) considering previous queries and the current query is suitable for session search task; 2) user SAT-Clicks is useful to estimate query quality and document relevance; 3) clustering sessions by query similarity is effective; 4) in session search, a session itself contains rich interaction information which can be used to improve search accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Acknowledgement</head><p>The research is supported by NSF grant CNS-1223825 and DARPA Memex program. This material is based on research sponsored by DARPA under agreement number FA8750-14-2-0226. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,89.28,294.29,397.33,53.53"><head>Table 2 Features Extracted from Session Data Log</head><label>2</label><figDesc></figDesc><table coords="4,89.28,311.57,397.33,36.25"><row><cell>Feature</cell><cell>Definition</cell></row><row><cell>F 1</cell><cell>The user's intent of session s is to make comparison among two or more items.</cell></row><row><cell>F 2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.00,217.25,467.97,286.68"><head>Table 3 TREC 2014 Session Track Submissions</head><label>3</label><figDesc>We run our experience on dataset Clueweb12 CatA. It consists of 733,019,372 English web pages, collected between February 10, 2012 and May 10, 2012. Spam documents are out based on their Waterloo Spam scores.</figDesc><table coords="5,72.00,217.25,461.80,286.68"><row><cell>8 Experiments</cell><cell></cell><cell></cell></row><row><cell>8.1. Data preparation</cell><cell></cell><cell></cell></row><row><cell>8.2. Submission</cell><cell></cell><cell></cell></row><row><cell>GUS14RUN1</cell><cell>GUS14RUN2</cell><cell>GUS14RUN3</cell></row><row><cell cols="3">RL1 • Ad-hoc Retrieval Model • Ad-hoc Retrieval Model • Ad-hoc Retrieval Model</cell></row><row><cell>RL2 • Weighted QCM</cell><cell>• Weighted QCM (ω=0.8)</cell><cell>• Weighted QCM (ω=0.8)</cell></row><row><cell>(ω=0.65)</cell><cell>• Session Level User-</cell><cell>• Session Level User-Click</cell></row><row><cell>• Session Level User-</cell><cell>Click Model</cell><cell>Model</cell></row><row><cell>Click Model</cell><cell></cell><cell></cell></row><row><cell>RL3 • Weighted QCM</cell><cell>• Weighted QCM (ω=0.8)</cell><cell>• Weighted QCM (ω=0.8)</cell></row><row><cell>(ω=0.65)</cell><cell>• Topic Level User-Click</cell><cell>• Topic Level User-Click Model</cell></row><row><cell>• Topic Level User-Click</cell><cell>Model</cell><cell>using topic ids</cell></row><row><cell>Model</cell><cell></cell><cell>• Session Performance</cell></row><row><cell></cell><cell></cell><cell>Prediction and Replacement</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,92.10,480.53,447.90,9.48;6,72.00,493.01,467.99,9.48;6,72.00,505.73,321.81,9.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,333.01,480.53,188.96,9.48">Utilizing query change for session search</title>
		<author>
			<persName coords=""><forename type="first">Sicong</forename><surname>Dongyi Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,72.00,493.01,467.99,9.48;6,72.00,505.73,144.69,9.48">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;13)</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,91.24,522.29,448.73,9.48;6,72.00,535.01,264.21,9.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,201.51,522.29,338.46,9.48;6,72.00,535.01,35.42,9.48">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,114.14,535.01,96.99,9.48">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.64,551.81,447.25,9.48;6,72.00,564.29,466.87,9.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,269.51,551.81,270.39,9.48;6,72.00,564.29,27.98,9.48">Win-Win Search: Dual-Agent Stochastic Game in Session Search</title>
		<author>
			<persName coords=""><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,119.36,564.29,252.02,9.48">Proceedings of the 37th Annual ACM SIGIR Conference</title>
		<meeting>the 37th Annual ACM SIGIR Conference<address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Gold Coast</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,87.70,581.09,452.24,9.48;6,72.00,593.57,467.73,9.48" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,253.60,581.09,286.34,9.48;6,72.00,593.57,26.40,9.48">A POMDP Model for Content-Free Document Re-ranking (short paper)</title>
		<author>
			<persName coords=""><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,117.50,593.57,251.99,9.48">Proceedings of the 37th Annual ACM SIGIR Conference</title>
		<meeting>the 37th Annual ACM SIGIR Conference<address><addrLine>Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Gold Coast</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
