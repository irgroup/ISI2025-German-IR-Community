<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.34,137.98,401.31,15.12;1,184.40,159.90,243.20,15.12">The University of Illinois&apos; Graduate School of Library and Information Science at TREC 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-10-28">October 28, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.78,193.80,84.46,10.48"><forename type="first">Garrick</forename><surname>Sherman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,289.15,193.80,57.37,10.48"><forename type="first">Miles</forename><surname>Efron</surname></persName>
							<email>mefron@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.71,193.80,61.51,10.48"><forename type="first">Craig</forename><surname>Willis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Library and Information Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>Urbana-Champaign 501 E. Daniel St</addrLine>
									<postCode>61820</postCode>
									<settlement>Champaign</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.34,137.98,401.31,15.12;1,184.40,159.90,243.20,15.12">The University of Illinois&apos; Graduate School of Library and Information Science at TREC 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-10-28">October 28, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">E526B95799E154394D62AD85B272F701</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Illinois' Graduate School of Library and Information Science (uiucGSLIS) participated in TREC's Federated Web (FedWeb) and Knowledge Base Acceleration (KBA) tracks in 2014. Specifically, we submitted runs for the FedWeb resource selection and KBA cumulative citation recommendation (CCR) tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FedWeb Resource Selection</head><p>The Federated Web Search (FedWeb) resource selection task (RS) requires participants to rank candidate search engines, known as resources, according to the applicability of their contents to test topics. The use-case is that a user U issues an ad hoc query Q to a federated web search system S with access to a set of resources R = {R 1 , R 2 , ..., R n }. Given that S cannot efficiently submit Q to each resource R i , the system must rank the resources in order of their appropriateness to Q so that only the most applicable resources are searched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">The FedWeb 2014 Dataset</head><p>The FedWeb 2014 Dataset contains both result snippets and full documents sampled from 149 web search engines between April and May 2014. Snippets contain document title, description, and thumbnail image when available. Our system did not make use of snippets, instead focusing on the full text documents sampled. Documents are available in their original format; our system indexed content only from textual documents. Resources are classified into "verticals," which describe the topic, media type, or genre of resources. Though classification of resources into verticals was available, our system did not make use of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Topics</head><p>A set of 75 test topics was made available, of which 50 were ultimately used for system evaluation. The 50 evaluation topics were not made known at submission time, so runs were submitted with results for all 75 topics. Topics were in the form of ad hoc keyword queries, as are typical in web environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Base System</head><p>As with the KBA CCR task, we used the Indri search engine and API for core indexing and retrieval in both FedWeb runs. We did not apply a stop list or stem documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submitted Runs</head><p>We submitted two runs to the FedWeb RS task this year. The first, uiucGSLISf1, ranked each resource by its query clarity <ref type="bibr" coords="2,251.08,355.60,11.52,9.57" target="#b0">[1]</ref> score for the topic. The second, uiucGSLISf2, ranked resources by a CF-IDF (collection frequency-inverse document frequency) score. These techniques are described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Query Clarity</head><p>The uiucGSLISf1 run computed the well-known query clarity score for each query-resource pair and ranked resources by decreasing clarity for each topic.</p><p>The query clarity score is a measure of a query's ambiguity with respect to a document collection and is defined as the KL-divergence between the query and collection language models:</p><formula xml:id="formula_0" coords="2,224.49,505.54,161.83,29.64">S(Q, R) = wâˆˆV P (w|Q)log P (w|Q) P (w|R)</formula><p>where w is a word in the vocabulary V , P (w|Q) is the probability of the word in the query language model, and P (w|R) is the probability of the word in the overall resource language model. We treated each resource as a collection composed of the documents sampled for that resource. The relevance model was computed removing stopwords, with 20 feedback documents and 10 feedback terms.</p><p>The motivating intuition underlying this approach is as follows. If a query is applicable to a resource, the quality of the query language model computed from that resource should be high. High quality query language models are assumed to be those that minimize the query's ambiguity. If a resource is not appropriate for a query, it is unlikely to promote documents that cohere to a single topic and will therefore lead to a query language model that is very similar to the overall resource language model. If a resource is appropriate for a query, it will tend to promote relevant documents and lead to a query language model that diverges from the overall resource language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">CF-IDF</head><p>The uiucGSLISf2 run used a score computed from query terms' collection and inverse document frequencies (CF-IDF) to rank resources' appropriateness for each query. These quantities are computed as expected. The collection frequency of a word is defined as</p><formula xml:id="formula_1" coords="3,264.62,269.43,82.77,9.57">CF (w) = c(w, R)</formula><p>where c(w, R) is the count of the word w in the resource. The inverse document frequency is</p><formula xml:id="formula_2" coords="3,252.56,329.75,105.68,24.43">IDF (w) = log d(R) d(w, R)</formula><p>where d(R) is the number of documents in the resource and d(w, R) is the number of documents in the resource that contain w. Putting these together, the CF-IDF of a query can be computed as</p><formula xml:id="formula_3" coords="3,216.71,421.20,178.59,22.26">CF -IDF (Q) = qâˆˆQ CF (q) Ã— IDF (q).</formula><p>Intuitively, the CF component of this formulation is intended to capture to what degree the resource is "about" the query; that is, how often the resource includes content relating to the query. This mirrors the TF component of TF-IDF but spread across all documents in a resource. The IDF component is intended to penalize ambiguity of a query for a resource, in that the larger the proportion of documents in a resource matching a query, the less confident we can be that the documents match in a meaningful way. The decision to mix collection-level (CF) with document-level (IDF) was pragmatic: because resources are so large, most resources contain most query terms, reducing the discriminative power of a collection-level IDF factor. Use of document-level IDF allowed us to approximate a term's discriminative power on a per-resource basis.</p><p>The intuition underlying CF-IDF, then, approximates the intuition underlying the query clarity score. Both seek to reward resources containing a set of documents that are strongly on-topic in comparison with the resource as a whole. That is, both prefer resources that minimize the ambiguity of the query.</p><p>Run Name nDCG@20 nDCG@10 nP@1 nP@5 uiucGSLISf2 0.361 (+/-0.140) 0.274 (+/-0.172) 0.179 (+/-0.222) 0.213 (+/-0.164) uiucGSLISf1 0.348 (+/-0.117) 0.249 (+/-0.138) 0.101 (+/-0.207) 0.212 (+/-0.193) Table <ref type="table" coords="4,121.78,161.83,4.24,9.57">1</ref>: Results of Official GSLIS FedWeb RS Runs. Runs are shown in decreasing order of nDCG@20. Standard deviation across topics per run is in parentheses.</p><p>(a) nDCG@20 (b) nDCG@10 Figure <ref type="figure" coords="4,125.52,356.28,4.24,9.57">1</ref>: Per-topic nDCG@20 and nDCG@10 for both FedWeb RS runs. Runs are ordered by decreasing CF-IDF score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Analysis of Results</head><p>Table <ref type="table" coords="4,121.03,425.37,5.45,9.57">1</ref> summarizes the results of our runs for all official metrics. It shows that the CF-IDF run outperformed the query clarity run on all measures. Figure <ref type="figure" coords="4,143.93,452.47,5.45,9.57">1</ref> depicts nDCG@20 and nDCG@10 per topic for both runs. As these charts suggest, the Pearson correlation between the two runs is quite low: 0.3884 for nDCG@20 and 0.3407 for nDCG@10. These results demonstrate that, despite their shared motivating intuition to promote resources that minimize query ambiguity, the CF-IDF and query clarity approaches perform quite differently when applied to the same topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Conclusion and Future Directions</head><p>More investigation is needed to determine why the CF-IDF and query clarity runs differ so strongly in their performance on each topic. This will involve identification of differences between the two approaches (e.g., query expansion occurs in query clarity but not CF-IDF), common properties among topics that strongly favor one approach or the other, and commonalities among the resources that either approach might favor. Establishing the runs' respective strengths and weaknesses will help in the design of more effective techniques for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KBA CCR</head><p>The Knowledge Base Acceleration (KBA) cumulative citation filtering task (CCR), also referred to as "vital filtering", is a stream-oriented document filtering task. The use-case is that a user U is an editor of a node T for an entity in a knowledge base. Given an incoming time-ordered stream of documents D, the system must decide whether to recommend each document D i to U . The "vital" criteria means that the document would "motivate a change to an already up-to-date knowledge base article." In other words, our goal is to monitor D, signaling to U when we find a document that contains "edit-worthy" information regarding the entity T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">The 2014 KBA Stream Corpus</head><p>The KBA "Stream Corpus" is a collection of timestamped documents published on the web between October 2011 and January 2013<ref type="foot" coords="5,283.00,299.75,4.23,6.99" target="#foot_0">1</ref> from nine different sources including news, social medial, blogs, forums, linking services, and scholarly publications. This year we used the filtered subset<ref type="foot" coords="5,158.48,326.84,4.23,6.99" target="#foot_1">2</ref> which consists of approximately 20 million documents. The stream corpus includes additional metadata and derived data (e.g., part-of-speech tags) that were not used by our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Target Entities</head><p>The CCR task involves monitoring the stream corpus for documents that contain vital information about any of a set of 109 target entities. Each entity is associated with a URL representing a profile in an imaginary knowledge base. This year the entities also included an external profile URL that was not used by our system. This year, systems were evaluated on a subset of 71 target entities that contained sufficient training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Training Data</head><p>Track organizers provided a distinct training time range (TTR) for each entity. Teams were permitted to analyze documents from before the end of the TTR period for training purposes. Only those entities with sufficient training data were used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Annotation Data</head><p>The CCR task in 2014 recognized two levels of relevance with respect to an entity-document pair: useful and vital. A vital rating indicates a stronger usefulness of the document than a useful rating does. The official goal of the task was to maximize F1 based on these vital ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Base System</head><p>For core indexing and retrieval we used the Indri search engine and API <ref type="foot" coords="6,437.90,159.39,4.23,6.99" target="#foot_2">3</ref> . Very little preprocessing was used in our experiments: a standard stop list was used for all tasks. In general we did not stem documents, although stemming was used in one of our models (textttverb, described below).</p><p>All documents are filtered using a preliminary proximity query based on an ordered window of length 2. Documents that match the proximity query are then scored based on the criteria outlined below.</p><p>For assessing entity-document similarity, we used the negative KL-divergence between the language model Î¸ i for document D i and the language model Î¸ E for the entity E <ref type="bibr" coords="6,496.46,269.74,10.91,9.57" target="#b1">[2]</ref>:</p><formula xml:id="formula_4" coords="6,207.21,289.01,308.34,29.73">sim(D i , E) = - f âˆˆE P (f |Î¸ E ) log P (f |Î¸ E ) P (f |Î¸ i ) . (<label>1</label></formula><formula xml:id="formula_5" coords="6,515.55,296.39,4.65,9.57">)</formula><p>where f is a "feature" of the profile we defined to represent E. Usually, some feature f j is the simply a term that is highly associated with E. Additionally, sim(D i , E) is combined with the supplemental features listed in Section 3.4 below. This yields our final score for</p><formula xml:id="formula_6" coords="6,190.78,368.97,329.42,31.55">D i against E: score(D i , E) = sim(D i , E) + f 1 (D i ) + f 2 (D i ) . . .<label>(2)</label></formula><p>Conceptually, f i (D i ) plays the role of a prior over documents. However, use of KL divergence for measuring similarity does not lend itself to the proper introduction of a prior (unlike, say, query likelihood). The decision to emit true for D i is based on the magnitude of score(D i , E) with respect to a threshold Ï„ . We emit true iff score(D i , E) â‰¥ Ï„ .</p><p>In all experiments, Ï„ is set simply by scoring all training documents according to Eq. 2 and finding the cutoff that maximizes the F1 score with respect to the training annotations. When finding the optimal cutoff, F1 is calculated using only vital documents as positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Representation</head><p>This year, we used two different representations of each target entity:</p><p>â€¢ sf: The basic surface form of the entity name â€¢ rm3: Relevance model <ref type="bibr" coords="6,231.33,595.73,11.52,9.57" target="#b2">[3]</ref> interpolated with the surface form (Î» = 0.5) using true relevance feedback for vital documents from the training period for each entity. If a document was judged by multiple assessors, the highest score was used (any-up)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head><p>Our main research goal this year was to improve CCR effectiveness by combining scores based on purely lexical information with additional features. We focused on document attributes including length and source, temporal features, and a verb-based language model. The features and a brief description are listed in Table <ref type="table" coords="7,356.65,313.61,4.24,9.57" target="#tab_1">2</ref>. Formally, let R T be the set of documents labeled as vital in the training data. Let D i be the ith document. Let S(D i ) be the source of D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Document length</head><p>As demonstrated last year, vital documents tend to be well-behaved with respect to length. This year we explored the effect of document length conditioned on source. We used the training data to find the length of all vital documents for each source.</p><p>If we let L(D i ) be the length of document D i , we hypothesized that L(D) âˆˆ R T would follow a log-normal distribution N L,S . Using R T , we estimated the mean and standard deviation of N L,S for each source S by maximum likelihood, such that</p><formula xml:id="formula_7" coords="7,237.39,482.38,282.81,10.68">(D i ) = N L,S (log n(D i ), Î¼s , Ïƒs )<label>(3)</label></formula><p>where n(D i ) is the length of D i , and Î¼s , Ïƒs are the estimated parameters (mean and standard deviation) of the log-normal distribution for source S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Document source prior</head><p>We expect that some document sources will have a higher prior probability of producing vital documents than others. Given each source S and set of vital documents V in the training data, we calculated P (S i ) by maximum likelihood:</p><formula xml:id="formula_8" coords="7,268.29,619.80,74.23,24.43">P (S) = n(V, S) n(V )</formula><p>Where n(V, S) is the number of vital documents from source S and n(V ) is the total number of vital documents. For each document D, the prior probability of the document P (D) is estimated using this value. In runs using this model, log P (D) was added to the negative KL divergence score of each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Temporal features</head><p>We also explored two sources of temporal information: the previous number of documents (prevdoc) in a specified interval (e.g., 48 hours) and the amount of time between the current document and the previous document (burst).</p><p>The prevdocs feature models the probability that the current document is vital for the current entity given the number of documents seen about the current entity in specified interval P d for the current source S. Each source will likely have a different "tempo" by which documents enter the stream. We used the training data to calculate the probability of a vital judgment given the number of previous documents published in the last interval for the current document source. We hypothesize that this follows a Poisson distribution and estimated the mean over the training data:</p><formula xml:id="formula_9" coords="8,231.87,336.51,148.26,10.77">P (V |P d , S i ) âˆ¼ P oisson(Î» = 7)</formula><p>Similar to the previous number of documents, the burst feature uses the amount of time between the current document and the most recent previous document. The intuition here is that vital documents will be published in "bursts" and that the interval between documents about an entity will be shorter when events are happening to that entity. Given the time of the current document t i and the time of the previous document t i-1 and a specified interval (e.g., 24 hours), we model the distribution of differences as a normal distribution with Î¼ = 0 and Ïƒ = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Verb language model</head><p>Because the vital filtering task indirectly rests on changes that target entities undergo over time, we hypothesized that documents whose vocabulary includes a high proportion of words indicative of change might have a higher prior probability of relevance than documents that, for instance, simply enumerate many peoples' names or that include a target entity's name in an off-hand way. To operationalize this, we focused on verbs, particularly verbs that often convey changes in the state of affairs. We thus built a language model V where we estimate</p><formula xml:id="formula_10" coords="8,260.10,591.90,90.61,24.43">P (w|V ) = n(w, H) n(H)</formula><p>Where H are the verbs in the headlines of the Aquaint corpus. We extracted all article headlines from the Aquaint corpus (approximately 1 million headlines) and compared each token with the list of verbs included in the current WordNet distribution. The Porter stemmer was used for all work with verbs. This yielded 4786 verbs from the Aquaint headlines. The quantity n(w, H) is simply the number of headlines in which the word w occurs. We then applied the verb model to generate a document prior for each item in the streamcorpus. Given a document D, we define:</p><formula xml:id="formula_11" coords="9,211.83,179.38,183.19,9.57">P (D) = z â€¢ logit P (w|D) â€¢ P (w|V )</formula><p>where z is a normalizing constant. In runs using the verb model, log P (D) was added to the negative KL divergence score of each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Submitted Runs</head><p>Simple combinations of features (2 6 = 64) were tested on the KBA 2013 test collection using both the surface form (sf) and relevance model (rm3) entity models. The 6 best feature combinations for each entity model were used for the KBA 2014 submissions.</p><p>This year we submitted 14 runs. Results of these runs are shown in Table <ref type="table" coords="9,487.54,310.61,4.24,9.57" target="#tab_2">3</ref>. We submitted two broad classes of runs:</p><p>â€¢ sf: Runs where the entity model is simply the surface form entity name.</p><p>â€¢ rm3: Runs where the entity model is the RM3 over the high-vital training data.</p><p>The names of the runs shown in the table are intended to be self-explanatory given the abbreviations listed in Table <ref type="table" coords="9,231.35,405.25,4.24,9.57" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Analysis of Results</head><p>Table <ref type="table" coords="9,123.02,455.06,5.45,9.57" target="#tab_2">3</ref> summarizes the results of our runs. In general, no systematic differences were found.</p><p>Significance was determined using a paired t-test with Î± = 0.05 on the precision, recall, and scaled utility measures. Of the two baselines, unsurprisingly the basic surface form entity representation was significantly better than the true-RM3 model in terms of recall. The pdsrc rm3 model was significantly better than the baseline rm3 model in both precision and scaled utility. The srclen rm3 and verbsource rm3 models were significantly better than the baseline rm3 mode in both recall and scaled utility. However, none of these models were significantly different from the simple surface form baseline.</p><p>Figure <ref type="figure" coords="9,143.23,577.00,5.45,9.57">2</ref> presents a comparison of the length, sourceless, and verb source runs to the respective sf and rm3 baselines by F1. The baseline is represented by the solid black line with queries ordered on the x-axis by the descending value of the baseline measure (e.g., precision). Each run is plotted in red over the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Conclusion and Future Direction</head><p>Our investigation of document length and source priors as well as the temporal features and verb-based language model demonstrated little or no effect on vital filtering performance when compared to the simple surface-form baseline. Figure <ref type="figure" coords="10,419.07,419.62,5.45,9.57">2</ref> suggests significant improvements for some entities (and degradation for others). Future work will focus on the analysis of individual entities to improve our understanding of why these methods work or fail in these cases. After submitting our official runs, we noticed that the document sources had an unusual distribution in the 2014 stream corpus. Figure <ref type="figure" coords="10,347.95,487.36,5.45,9.57">3</ref> displays the temporal distribution of documents by source. As can be seen in from the figure, sources or source labels were apparently changed midway during the stream collection process. This means that the "social" and "News" sources occur only in the first half of the collection and the "WEBLOG" and "MAINSTREAM NEWS" sources appear in the second half. This likely has a significant effect on the use of a source prior, since little or no training data will be available for some entities for some sources. Normalizing the source labels would likely have a positive effect. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,91.80,592.50,428.39,9.57;12,91.80,606.05,64.70,9.57"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Comparison of length, source+length, and verb+source runs (red) to baseline (black) by F1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="13,95.75,209.98,420.50,301.00"><head></head><label></label><figDesc></figDesc><graphic coords="13,95.75,209.98,420.50,301.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,208.05,217.42,195.91,9.57"><head>Table 2 :</head><label>2</label><figDesc>Features evaluated in CCR task</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,91.80,97.47,428.39,249.07"><head>Table 3 :</head><label>3</label><figDesc>Results of Official GSLIS KBA CCR Runs. Vital Only. Runs are shown in decreasing order of F1.</figDesc><table coords="10,175.71,97.47,260.58,199.96"><row><cell>Run Name</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell>SU</cell></row><row><cell>sourcelen sf</cell><cell>0.3637</cell><cell cols="3">0.5746 0.4455 0.3526</cell></row><row><cell>verbsource sf</cell><cell>0.3679</cell><cell cols="3">0.5447 0.4392 0.3257</cell></row><row><cell>length sf</cell><cell>0.3833</cell><cell cols="3">0.4885 0.4296 0.3786</cell></row><row><cell>baseline sf</cell><cell>0.3607</cell><cell cols="3">0.5037 0.4204 0.3499</cell></row><row><cell>pdverb sf</cell><cell>0.3533</cell><cell cols="3">0.4466 0.3945 0.3518</cell></row><row><cell>pdsrc sf</cell><cell>0.3321</cell><cell cols="3">0.4727 0.3901 0.3382</cell></row><row><cell>verbsource rm3</cell><cell>0.3773</cell><cell cols="3">0.4029 0.3897 0.3505</cell></row><row><cell>sourcelen rm3</cell><cell>0.3526</cell><cell cols="3">0.4288 0.3870 0.3496</cell></row><row><cell>pdverb rm3</cell><cell>0.3623</cell><cell cols="3">0.3876 0.3745 0.3620</cell></row><row><cell>prevdocs sf</cell><cell>0.3261</cell><cell cols="3">0.4179 0.3663 0.3570</cell></row><row><cell>length rm3</cell><cell>0.3914</cell><cell cols="3">0.3411 0.3645 0.3872</cell></row><row><cell>prevdocs rm3</cell><cell>0.3384</cell><cell cols="3">0.3834 0.3595 0.3573</cell></row><row><cell>pdsrc rm3</cell><cell>0.2962</cell><cell cols="3">0.4183 0.3468 0.3063</cell></row><row><cell>baseline rm3</cell><cell>0.3998</cell><cell cols="3">0.2798 0.3292 0.4160</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,108.39,620.69,225.95,7.47"><p>http://trec-kba.org/kba-stream-corpus-2014.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.39,631.65,291.86,7.47"><p>http://s3.amazonaws.com/aws-publicdatasets/trec/kba/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.39,643.96,108.27,7.47"><p>http://lemurproject.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,108.77,122.90,411.43,9.57;11,108.77,136.45,411.43,9.57;11,108.77,150.00,411.43,9.57;11,108.77,163.55,57.27,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,355.66,122.90,141.81,9.57">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,108.77,136.45,411.43,9.57;11,108.77,150.00,231.16,9.57">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;02</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.77,186.06,411.42,9.57;11,108.77,199.61,411.43,9.57;11,108.77,213.16,411.43,9.57;11,108.77,226.71,99.46,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,225.07,186.06,295.13,9.57;11,108.77,199.61,138.41,9.57">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,272.80,199.61,247.41,9.57;11,108.77,213.16,407.05,9.57">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,108.77,249.23,411.42,9.57;11,108.77,262.78,411.42,9.57;11,108.77,276.33,270.50,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,255.77,249.23,156.71,9.57">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,435.39,249.23,84.81,9.57;11,108.77,262.78,411.42,9.57;11,108.77,276.33,163.03,9.57">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
