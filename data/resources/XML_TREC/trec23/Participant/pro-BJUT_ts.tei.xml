<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.42,98.99,329.17,12.90">BJUT at TREC 2014 Temporal Summarization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.14,138.44,47.47,10.75"><forename type="first">Yun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.51,138.44,35.55,10.75"><forename type="first">Fei</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.69,138.44,66.69,10.75"><forename type="first">Huayang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,363.10,138.44,55.32,10.75"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
							<email>yangzhen@bjut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.42,98.99,329.17,12.90">BJUT at TREC 2014 Temporal Summarization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1FE717069CC567F0A97464677CE5F31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the second participation of BJUT in the temporal summarization track. We performed the experiments on the TREC KBA 2014 stream corpus using the classic information retrieval models, such as B-M25, vector space model. Also, we introduce the details of our system, which consists of corpus pre-processing, information retrieval module and information process module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The TREC Temporal Summarization Track runs for the second time in this year, and different from 2013 <ref type="bibr" coords="1,232.41,358.33,12.76,8.64" target="#b0">[1]</ref>, this year's track focuses on only one task: Sequential Updates Summarization. All participants should answer a query based on topic a set of relevant and novel sentences ranked by time from a time-ordered stream of documents, through which users can efficiently monitor the information associated with an event (such as a natural disaster) over time <ref type="bibr" coords="1,240.57,424.09,11.93,8.64">[2]</ref>. Another primary difference lies in the data size, which reduced to 559G from 4. <ref type="bibr" coords="1,110.85,446.01,8.12,8.64">5T</ref>. The corpus, namely TREC-TS-2014F, is a filtered version of the full track. It is designed for using by participants of the TREC-TS track, aiming to provide a dataset with which groups can participate in the TREC-TS track without having to process the full corpus. The corpus consists of a set of time stamped documents from a variety of news and social media sources covering the time period from Oct.2011 to Apr. 2013. A document contains a set of sentences, each with a unique identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYSTEM</head><p>According to the task of Temporal Summarization Track, system should emit relevant, important and novel sentences to a specific event. We submitted three runs for this task and the implementation framework of our system is shown in FIG. <ref type="figure" coords="1,75.03,614.14,3.74,8.64" target="#fig_0">1</ref>.</p><p>As shown in FIG. <ref type="figure" coords="1,147.27,625.09,3.74,8.64" target="#fig_0">1</ref>, the framework of our temporal summarization system can be described as follows, which mainly includes corpus pre-processing, information retrieval module and information process module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Information pre-processing module</head><p>The corpus downloaded locally from streamcorpus -2014 -v0 3 0 -ts -f iltered[3] is encrypted file, which cannot be used directly. In this sense, firstly, decrypting the files uses the authorized key and converts the .GPG file format to .SC file format; Secondly, parsing the .SC files use stream corpus toolbox to .TXT files. The stream corpus toolbox is given by TREC and provides a common data interchange format for document processing pipelines that apply language-processing tools to large streams of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Information retrieval module</head><p>Firstly, building index for the .TXT files. Then, combining with query expansion for retrieval to get relevant sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Information process module</head><p>Text similarity and clustering can improve the accuracy and recall rate of the retrieval results. We used two methods to complete this part. After the topic clustering, the centers of the different clustering are chosen to build the summarizations. Then the summarizations are ranked by time factor and similarity factor. The frame with solid line in FIG. <ref type="figure" coords="1,468.15,438.72,4.98,8.64" target="#fig_0">1</ref> is the main methods we used for the temporal summarization track. The details of our work will be introduced in the next section. We emphasis on the description of the two key parts: information retrieval module and information process module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Retrieval Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Information Retrieval</head><p>In this part, we use Lemur[4] for information indexing and retrieval. Lemur is a toolkit designed to facilitate research in language modeling and information retrieval (IR). It supports the construction of basic text retrieval systems using language modeling methods such as BM25 <ref type="bibr" coords="1,329.46,582.37,10.58,8.64">[5]</ref>. Our experiment has two steps to build the index. First, create a parameter file tell the lemur toolkit how to index; Secondly, use IndriBuildIndex.exe application to build index. Accordingly, the realization of retrieval also has two steps. First, create a parameter file tell the lemur toolkit how to retrieve; Second, use IndriRunquery.exe application to retrieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Query expansion</head><p>Generally, when we retrieving information based on query words, there always exists one problem, namely word mismatch, which can be explained that people often use</p><p>In this part, we use Lemur [4] for information indexing and retrieval. Lemur is a toolkit designed to facilitate research in language modeling and information retrieval (IR). It supports the construction of basic text retrieval systems using language modeling methods such as BM25 [5] . Our experiment has two steps to build the index. First, create a parameter file tell the lemur toolkit how to index; Secondly, use IndriBuildIndex.exe application to build index.</p><p>Accordingly, the realization of retrieval also has two steps.</p><p>First, create a parameter file tell the lemur toolkit how to retrieve; Second, use IndriRunquery.exe application to retrieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query expansion</head><p>Generally, when we retrieving information based on query words, there always exists one problem, namely word mismatch, which can be explained that people often use different words to describe the same concepts between the queries and documents. To solve this problem, in this paper we used a method called query expansion. Query expansion can augment a query word, and through which, the query word can match more sentences, thus potentially increasing the number of relevant results. The query word is extended by using words with similar meaning to those in the query, and the chance of matching words in the relevant documents is therefore increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Information Process Module</head><p>After information retrieval module, we got a set of sentences related to a topic. Considering the large amount of these sentences, in order to simplify the computation, we first judge whether these sentences are in the range of each topic's begin time and end time. If a sentence is not in the time period, abandon it, and the rest sentences can be treated as candidate sentences, which should be had further processes such as text similarity calculation and text clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Text Similarity</head><p>We used two methods based on Vector Space Model (VSM) [6] , which is composed of eigen values extracted from documents and its weight. Vector Space Models is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. In VSM, sentences and queries are represented as vectors:</p><p>1, 2, , ( , ,..., )</p><formula xml:id="formula_0" coords="2,318.32,567.65,237.62,13.77">j j j tj d w w w <label>(1)</label></formula><p>Each dimension corresponds to a separate term. If a term occurs in the sentence, its value in the vector is non-zero.</p><p>There are several ways to compute these values (also called weight), and in this paper, we use tf-idf weighting. The VSM model is known as the term frequency-inverse document frequency model. The weight vector for document distance:  different words to describe the same concepts between the queries and documents. To solve this problem, in this paper we used a method called query expansion. Query expansion can augment a query word, and through which, the query word can match more sentences, thus potentially increasing the number of relevant results. The query word is extended by using words with similar meaning to those in the query, and the chance of matching words in the relevant documents is therefore increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Process Module</head><p>After information retrieval module, we got a set of sentences related to a topic. Considering the large amount of these sentences, in order to simplify the computation, we first judge whether these sentences are in the range of each topic's begin time and end time. If a sentence is not in the time period, abandon it, and the rest sentences can be treated as candidate sentences, which should be had further processes such as text similarity calculation and text clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Text Similarity</head><p>We used two methods based on Vector Space Model (VS-M) <ref type="bibr" coords="2,72.72,573.57,13.14,8.64" target="#b1">[6]</ref>, which is composed of eigen values extracted from documents and its weight. Vector Space Models is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. In VSM, sentences and queries are represented as vectors:</p><formula xml:id="formula_1" coords="2,124.16,656.14,168.34,9.65">d i = (w 1,j , w 2,j , • • • , w t,j )<label>(1)</label></formula><p>Each dimension corresponds to a separate term. If a term occurs in the sentence, its value in the vector is nonzero. There are several ways to compute these values (also called weight), and in this paper, we use tf-idf weighting.</p><p>The VSM model is known as the term frequency-inverse document frequency model. The weight vector for document distance:</p><formula xml:id="formula_2" coords="2,387.28,385.69,170.72,9.65">V d = (w 1,d , 2 2,d , • • • , w N,d )<label>(2)</label></formula><p>Where</p><formula xml:id="formula_3" coords="2,372.29,413.42,185.71,22.31">w t,d = tf t,d log( |D| |{d ∈ D|t ∈ d }| )<label>(3)</label></formula><p>And tf t,d is term frequency of term t in document d (a local parameter), log |D| |{d ∈D|t∈d }| is inverse document frequency (a global parameter). |D| is the total number of documents in the document set; |{d ∈ D|t ∈ d }| is the number of documents containing the term t. After getting the vectors, the VSM similarity between two documents can be calculated by using the cosine distance:</p><formula xml:id="formula_4" coords="2,388.83,528.76,169.17,23.22">sim(d i , d j ) = d i • d j ||d i || • ||d j || (4)</formula><p>Another method we used to calculate similarity is based on mutual information preserving mapping (MIPF), which is a manifold learning algorithm that computes low-dimensional, neighborhood-preserving based on mutual information of high-dimensional inputs. With sufficient data set, we expect each document text can be expressed as its neighbors' mutual information and its neighbors are lie on or close to a locally linear patch of the manifold. Then each text data can be reconstructed from its neighbors which are based on information content. Reconstruction errors are measured by the cost function In order to minimize the reconstruction errors, we can get the weight W . By using the weight, the low-dimensional vector Y can be measured by the embedding cost function:</p><formula xml:id="formula_5" coords="2,378.23,684.00,179.77,21.98">Σ i |I(X i ) - j W ij I(X i ; X j )| 2<label>(5)</label></formula><formula xml:id="formula_6" coords="3,119.45,380.22,173.05,21.98">i |I(Y i ) - j W ij I(Y i ; Y j )| 2<label>(6)</label></formula><p>Unlike traditional manifold methods for images, MIPF applies on text field, and its optimizations use the relationship between different texts. Meanwhile it remains the advantages of the manifold method. By exploiting the local symmetries of reconstructions, MIPF is able to learn the global structure of mutual-information-based manifolds, such as those generated by documents of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Text Clustering</head><p>The k-means <ref type="bibr" coords="3,120.78,519.35,11.62,8.64" target="#b2">[7]</ref> clustering is chosen after many experiments. The k-means clustering is a popular method for cluster analysis in data mining. The k-means clustering aims to partition n observations into k clusters, in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Sentence Selection</head><p>After text clustering, we can get the clusters based on topics between different events for information expansion. We choose the centers of the clusters and the top sentences as the summarization. Finally each event we totally choose about 100 sentences from the thousands sentences. The last step is to rank these central sentences. Time and similarity are the two factors that used to rank the summarizations. After this step, the final temporal summarization can be obtained.</p><p>EXPERIMENT RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>According the TREC authority, there are three metrics:</p><p>• Expected Gain. One way to evaluate an update system is to measure the expected gain for a system update. This is similar to traditional notions of precision in information retrieval evaluation.</p><p>• Comprehensiveness. Similar to tradition notions of recall in information retrieval evaluation.</p><p>• F measure. In order to summarize expected gain and comprehensiveness, we use an F measure based on both Expected Gain and Comprehensiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Table <ref type="table" coords="3,344.54,508.82,4.98,8.64" target="#tab_1">1</ref> shows the results of our system. In the first line of Table <ref type="table" coords="3,343.89,519.78,3.74,8.64" target="#tab_1">1</ref>, EG signifies the scores of the expected gain, C signifies the scores of the comprehensiveness, F signifies the scores of F measure. In the second row, Q0 and Q1 is the runs we submitted, AVG is the mean score for each topic over all runs submitted to the track. In the first column of Table <ref type="table" coords="3,344.36,574.57,3.74,8.64" target="#tab_1">1</ref>, the meaning of per-topic is obviously, mean signifies the average values of the scores over the 15 topics are given for each run. In the second column of Table1, All signifies the mean score over all topics and all runs submitted to the track. Through Table <ref type="table" coords="3,390.97,629.76,3.74,8.64" target="#tab_1">1</ref>, the performance of Q0 and Q1 with respect to the metrics Expected Gain and F measure are mostly better than AVG, which means that our methods are effectively. However, there are several topics whose Comprehensiveness value is smaller than the AVG, which means that our methods are not so well in recall. Through the contrast of the last three lines, we come to the conclusion that expect Comprehensiveness, our run's performance is better than the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we presented the implementation details of our runs for Temporal Summarization Track, and our runs performed well respect to Expected Gain and F score, but not so well respect to Comprehensiveness. The possible reason is that we excessive emphasis on the relevance between topic and sentence, and ignored the comprehensiveness of topic. Therefore, the future work's emphasis should be on how to improve the Comprehensiveness (or recall).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,180.98,306.04,250.05,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of the temporal summarization track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,101.91,64.27,408.18,233.06"><head>Table 1 :</head><label>1</label><figDesc>Experimental Result.</figDesc><table coords="3,101.91,80.48,408.18,216.86"><row><cell></cell><cell></cell><cell></cell><cell>EG</cell><cell></cell><cell></cell><cell>C</cell><cell></cell><cell></cell><cell>F</cell></row><row><cell></cell><cell></cell><cell>Q0</cell><cell>Q1</cell><cell>AVG</cell><cell>Q0</cell><cell>Q1</cell><cell>AVG</cell><cell>Q0</cell><cell>Q1</cell><cell>AVG</cell></row><row><cell></cell><cell>11</cell><cell cols="8">0.0504 0.0396 0.0358 0.1030 0.0962 0.3221 0.0677 0.0561 0.0552</cell></row><row><cell></cell><cell>12</cell><cell cols="8">0.0171 0.0176 0.0096 0.2367 0.2341 0.1986 0.0320 0.0327 0.0168</cell></row><row><cell></cell><cell>13</cell><cell cols="8">0.0538 0.0570 0.0172 0.6300 0.6295 0.4463 0.0992 0.1046 0.0314</cell></row><row><cell></cell><cell>14</cell><cell cols="8">0.0239 0.0271 0.0051 0.1833 0.1963 0.3317 0.0423 0.0477 0.0094</cell></row><row><cell></cell><cell>15</cell><cell cols="8">0.0701 0.0732 0.0439 0.7267 0.6977 0.7259 0.1278 0.0477 0.0094</cell></row><row><cell></cell><cell>16</cell><cell cols="8">0.0895 0.1253 0.0634 0.9812 1.0405 0.8336 0.1278 0.1326 0.0754</cell></row><row><cell></cell><cell>17</cell><cell cols="8">0.0414 0.0482 0.0240 0.6024 0.5279 0.6562 0.1641 0.2237 0.1146</cell></row><row><cell>Topic</cell><cell>18</cell><cell cols="8">0.0445 0.0358 0.0210 0.2479 0.1989 0.4575 0.0754 0.0607 0.0388</cell></row><row><cell></cell><cell>19</cell><cell cols="8">0.0796 0.0938 0.1043 0.4695 0.5284 0.4601 0.1361 0.1593 0.1237</cell></row><row><cell></cell><cell>20</cell><cell cols="8">0.0535 0.5552 0.0172 1.0214 1.0214 0.8989 0.1016 0.1047 0.0332</cell></row><row><cell></cell><cell>21</cell><cell cols="8">0.0955 0.0626 0.0341 0.4137 0.3841 0.4648 0.1551 0.1077 0.0552</cell></row><row><cell></cell><cell>22</cell><cell cols="8">0.1046 0.0973 0.0773 0.5108 0.5009 0.4625 0.1737 0.1629 0.1267</cell></row><row><cell></cell><cell>23</cell><cell cols="8">0.0805 0.0805 0.0612 0.1235 0.1235 0.1911 0.0975 0.0975 0.0840</cell></row><row><cell></cell><cell>24</cell><cell cols="8">0.0739 0.0739 0.0373 0.3847 0.3847 0.3589 0.1240 0.1240 0.0633</cell></row><row><cell></cell><cell>25</cell><cell cols="8">0.0992 0.0992 0.0329 0.4483 0.4483 0.4516 0.1624 0.1624 0.0570</cell></row><row><cell></cell><cell>ALL</cell><cell></cell><cell>0.0389</cell><cell></cell><cell></cell><cell>0.4840</cell><cell></cell><cell></cell><cell>0.0620</cell></row><row><cell>Mean</cell><cell>Q0</cell><cell></cell><cell>0.0652</cell><cell></cell><cell></cell><cell>0.4722</cell><cell></cell><cell></cell><cell>0.1091</cell></row><row><cell></cell><cell>Q1</cell><cell></cell><cell>0.0658</cell><cell></cell><cell></cell><cell>0.4675</cell><cell></cell><cell></cell><cell>0.1110</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,57.74,218.87,234.77,8.64;4,54.00,229.83,151.69,8.64" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m" coord="4,206.46,218.87,86.05,8.64;4,54.00,229.83,125.05,8.64">BJUT at TREC 2013 Temporal Summarization Track</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,57.74,306.54,234.77,8.64;4,54.00,317.50,238.50,8.64;4,54.00,328.46,58.11,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,192.97,306.54,99.54,8.64;4,54.00,317.50,75.64,8.64">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,137.04,317.50,116.95,8.64">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,57.74,339.42,234.77,8.64;4,54.00,350.38,238.50,8.64;4,54.00,361.34,187.49,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,244.03,339.42,48.48,8.64;4,54.00,350.38,234.44,8.64">BIRCH: an efficient data clustering method for very large databases</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,54.00,361.34,93.37,8.64">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
