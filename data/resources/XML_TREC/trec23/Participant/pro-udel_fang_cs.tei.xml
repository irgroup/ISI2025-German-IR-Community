<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.84,63.44,466.17,11.81">Exploration of Opinion-aware Approach to Contextual Suggestion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,249.36,100.28,50.22,9.13"><forename type="first">Peilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,322.32,100.28,40.26,9.13"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<email>hfang@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.84,63.44,466.17,11.81">Exploration of Opinion-aware Approach to Contextual Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">57819334483CE552AA97DBF895AF61D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our effort on TREC Contextual Suggestion Track. Using resources such as description or websites of example suggestions to build user profile has been proven to be effective on last year's TREC. This year we also leverage the power of using user profile. Real world opinions of the suggestions are used in our method to build both user profile and candidate suggestion profile. Two ranking method are investigated to rank the candidate suggestions: linear interpolation and learning to rank. For description generation, we apply the similar method as used in the last year. The structured description combines the category information of the suggestion, meta-description of the website, reviews of the suggestion and the similar example suggestions that the user liked. Official results of our submitted runs show the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 1014 Contextual Suggestion Track gives researchers the chance to test their methods on providing better personalized suggestions as well as concise and informative description of the suggestions to travellers who are travelling a new city and planning to have some fun in the city. This year is the third year of Contextual Suggestion Track in a row. There is basically one notable change comparing to Contextual Suggestion Track 2013 -the number of example suggestions in Contextual Suggestion Track 2014 is 100 (which was 50 in Contextual Suggestion Track 2013) and the example suggestion are from two cities (was from single city in Contextual Suggestion Track 2012 and Contextual Suggestion Track 2013). There are several groups (including us) that utilized different sources to model the user profile <ref type="bibr" coords="1,286.93,406.52,10.56,9.13" target="#b1">[2]</ref> in Contextual Suggestion Track 2013. Based on our last year's experience <ref type="bibr" coords="1,141.00,418.52,10.56,9.13" target="#b6">[7]</ref> and the increasing number of example suggestions, we still rely on building user profiles to help ranking candidate suggestions. When building user profile, we utilize the opinions from online resource to enrich the profiles as what we did in the last year. This year two different methods are tested to rank the candidate suggestions: linear interpolation and learning to rank. Linear interpolation is the method we used in last year but learning to rank is complete new. We would like to try learning to rank as we have bunch of features available. For description generation, we stick to our structured description generation method of last year because of its effectiveness <ref type="bibr" coords="1,204.24,490.28,9.99,9.13" target="#b6">[7]</ref>. Basically, we emphasize on learning to rank method to rank the candidate suggestions and other parts are pretty much similar to last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Framework</head><p>Our method consists of the following parts:</p><p>-Useful information gathering, -Profile modeling, -Candidate suggestion ranking, -Description generation.</p><p>We will describe them in details in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Useful Information Gathering</head><p>As similar to our submitted runs in Contextual Suggestion Track 2013 <ref type="bibr" coords="1,366.24,693.08,9.99,9.13" target="#b6">[7]</ref>, this year we also crawled the candidate suggestions from open web Yelp 1 . 16 high-level categories like restaurant, shopping are selected since they cover the most appropriate categories used in Contextual Suggestion Track. Approximately 1000 candidate suggestions are crawled for each category. Information including the name, average rating, address, business hour, all ratings and the associated text reviews of the candidate suggestion are crawled. Approximately 60,442 candidate suggestions are crawled for all contexts, resulting in average 1208 candidate suggestions per context. The websites of the candidate suggestions are also crawled (limit to 100 pages). The above mentioned information of example suggestions is also manually identified and then automatically crawled. The crawled data is stored mainly in JSON format and is used in both candidate suggestion ranking and description generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Profile Modeling</head><p>We use opinion to model user profile as well as candidate suggestion profile due to its richness. Specifically, we use positive opinions of the example suggestions that the user likes (positive example suggestions) to build her positive user profile, and use negative opinions of example suggestions that the user dislikes (negative example suggestions) to build negative user profile. The intuition is that users with similar ratings of a suggestions share something in common of why they like or dislike the suggestion.</p><p>Formally, the user profiles are estimated as <ref type="bibr" coords="2,268.20,241.64,9.99,9.13" target="#b6">[7]</ref>:</p><formula xml:id="formula_0" coords="2,201.72,262.16,347.91,20.73">U pos = esi∈ES(U) RU (esi)=P OS REP (O pos (es i ))<label>(1)</label></formula><formula xml:id="formula_1" coords="2,200.64,291.68,348.99,20.73">U neg = esi∈ES(U) RU (esi)=NEG REP (O neg (es i ))<label>(2)</label></formula><p>where es i is example suggestion i. O pos (es i ) represents all positive text reviews about es i , O neg (es i ) represents all negative text reviews, and REP (O(es i )) denotes how to represent text reviews O(es i ) in the profile. R U (es i ) is the rating of example suggestion es i given by user U . The original value of R U (es i ) could be numerical, and we map these values into either POS or NEG. We build the positive and negative profile for a candidate suggestion similar to what we did in building user profile as follows:</p><formula xml:id="formula_2" coords="2,247.80,388.28,116.28,24.38">CS pos = REP (O pos (CS)) CS neg = REP (O neg (CS)).</formula><p>For user ratings in Contextual Suggestion Track collection, we map the example suggestions with rating {3, 4} from the user as positive and opinions with rating {0, 1} from the user as negative. Example suggestions with rating {2} are simply ignored since they are hard to categorize into either positive or negative. For the opinions crawled from Yelp (either for example or candidate), opinions with rating {4, 5} are viewed as positive and opinions with rating {1, 2} are treated as negative. Opinions with rating {3} are simply ignored and are not used for building the profiles since they often contain mixed polarities and thus are hard to categorize. We tried several strategies of how to represent the profiles.</p><p>-Use full reviews (FR): use full text in the review.</p><p>-Use unique terms from full reviews (UFR): only the unique terms in the review.</p><p>-Use selective reviews (SR): most frequent terms in the review.</p><p>-Use unique terms from selective reviews (USR): only consider the unique terms in SR.</p><p>-Use nouns (NR): nouns in the review.</p><p>-Use review summaries (RS): summary of the review. RS are the terms extracted from the reviews using Opinosis Algorithm <ref type="bibr" coords="2,159.88,579.80,13.38,9.13" target="#b4">[5]</ref>.</p><p>Simple pre-processing are performed on the original text reviews: 1) Terms are lower cased; 2) Stop words are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Candidate Suggestion Ranking</head><p>Given a user U and a candidate suggestion CS, the similarity score can be computed as follows:</p><p>1. Build a positive and negative user profile, i.e., U pos and U neg , based on the information about example suggestions that the user has rated, i.e, ES(U ); 2. Build the positive and negative profile for the candidate suggestion CS, i.e., CS pos and CS neg ; 3. Predict the score of CS, i.e., S(U, CS), based on the similarities between U pos , U neg , CS pos and CS neg .</p><p>In order to compute S(U, CS), we investigate two categories of methods: linear interpolation and learning-to-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Interpolation</head><p>The main idea of linear interpolation is to linearly combine the similarity scores between user profile U pos , U neg and the candidate profile CS pos and CS neg . We use the following function to estimate the similarity between a user and a candidate suggestion:</p><formula xml:id="formula_3" coords="3,176.04,120.80,373.59,24.38">S(U, CS) = α × SIM (U pos , CS pos ) -β × SIM (U pos , CS neg ) -γ × SIM (U neg , CS pos ) + η × SIM (U neg , CS neg )<label>(3)</label></formula><p>where SIM (a, b) is text similarity measurement between any two profiles. α, β, γ, η ∈ [0, 1] are parameters that balance the impact of the different similarities to the final similarity score. We use an axiomatic similarity function F2EXP <ref type="bibr" coords="3,136.56,181.53,10.56,9.13" target="#b2">[3]</ref> when computing SIM (a, b). A preliminary training process was done on example suggestion using 5-fold cross validation. Different text review representations were tested. Finally, RS (review summary) was chosen to be used in ranking candidate suggestions because it achieves the best performance on training data. The optimal parameter set is α = 0.7, β = 0.0, γ = 0.4, η = 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Rank</head><p>The other ranking method we use is learning to rank method. The process is as follows: we first compute the similarity scores SIM (U pos , CS pos ), SIM (U neg , CS neg ), SIM (U pos , CS neg ), SIM (U neg , CS pos ) which is exactly the same as what we do in the linear interpolation method. Here we compute the similarities scores for all text review representations mentioned in 2.3. We then use these scores together with category similarity score and description similarity score which we used in Contextual Suggestion Track 2012 <ref type="bibr" coords="3,453.00,295.05,10.56,9.13" target="#b5">[6]</ref> as possible features and utilize the power of learning to rank to compute the score for each candidate suggestion.</p><p>Several learning to rank approaches with different feature combinations were tried in the training process. The approaches we tried including Multiple Additive Regression Trees (MART) <ref type="bibr" coords="3,400.92,330.80,9.99,9.13" target="#b3">[4]</ref>, LambdaMART <ref type="bibr" coords="3,488.28,330.80,9.99,9.13" target="#b0">[1]</ref>, AdaRank, RankNet, ListNet, Support Vector Machine (SVM) and etc. We also tried different feature combinations: using single opinion representation, e.g. using all four similarities scores generated by applying full review when computing the similarities scores; using combined similarities scores generated by different opinion representations, e.g. combine all similarities scores generated by FR (Full Review), SR (Short Review), NR (Noun Review); using combined similarities scores of opinion similarities scores and category and/or description similarities scores. Similar to the linear interpolation method, we apply 5-fold cross validation training on the example suggestions.</p><p>The result on example suggestions shows that LambdaMART together with combining all available similarities scores (category, description and all representations of reviews) as features achieves best performance. We then use all example data as the training data to rank candidate suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Description Generation</head><p>In last year's Contextual Suggestion Track we used a template to generate the descriptions of candidate suggestions <ref type="bibr" coords="3,86.88,500.00,9.99,9.13" target="#b6">[7]</ref>. Our description generation method ranked among the top in terms of all measurements, namely P@5, MRR and TBG <ref type="bibr" coords="3,132.36,512.00,9.99,9.13" target="#b1">[2]</ref>. This year we apply the same idea when generating the short description for candidate suggestion. We use all four parts: the opening sentence, "official" introduction, highlighted reviews and the concluding sentence of our structured description for both of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs and Experiment Results</head><p>We submitted two runs: UDInfoCS2014 1 and UDInfoCS2014 2. UDInfoCS2014 1 uses learning-to-rank method with LambdaMART as ranking method. All kinds of similarity scores with all kinds of text review representations are used as features. Example suggestions and user profiles are used to build the training data. UDInfoCS2014 2 uses linear interpolation method with RS text review representation. For both runs the description combines the opening sentence, meta-description and the picked sentences from the web site, highlighted reviews, and the concluding sentence.</p><p>Table <ref type="table" coords="3,105.48,663.20,4.98,9.13" target="#tab_0">1</ref> shows the overall mean performances of our runs in terms of all evaluation measures. We can see from the table that both of our runs outperform the TREC median score in terms of all measures. This confirms the effectiveness of using opinions to rank candidate suggestions and to generate the short descriptions. UDInfoCS2014 2, which uses linear interpolation as the candidate suggestion ranking method, achieves better performance. UDInfoCS2014 1, which uses learning to rank as the ranking method, also achieves promising performance. However, the learning to rank may be overfitted as the performance is not as good as its performance on training data. Moreover, using all kinds of similarity scores using different text review representations may contain noise data.</p><p>We also analyze other metrics mainly defined by ourselves:</p><p>1. Accuracy: number of suggestions of which the judgements of description and website are consistent divided by the total number of judged suggestions; 2. Precision: number of suggestions of which both description and website are relevant divided by the number of suggestions of which description are relevant; 3. Recall: number of suggestions of which the judgements of description and website are both relevant divided by the number of suggestions of which website are relevant;</p><p>Accuracy tests the overall performance. It also reflects the quality of the generated description. With high quality description, user either does not need to go to the website of the suggestion or get a better understanding of the suggestion. Precision is defined in this way since user may not go to the website of the suggestion if the description is not interesting. Recall mainly tests the quality of ranking candidate suggestion method. Table <ref type="table" coords="4,544.68,366.92,4.98,9.13" target="#tab_1">2</ref> shows the corresponding results of our submitted runs. We can see that UDInfoCS2014 2 has higher precision which could be the reason of its better performance over UDInfoCS2014 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In Contextual Suggestion Track 2014, we apply profile modeling in ranking the candidate suggestions and apply a structured description generation method. The similarity between user profile and candidate profile are used to feed two methods -linear interpolation and learning to rank. These two methods are used to rank the candidate suggestions. We apply the same idea when generating the description as what we did in Contextual Suggestion Track 2013. We submit two runs to Contextual Suggestion Track 2014. The performance of our two submitted runs are in general better than the TREC median performance which indicating the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,223.68,62.67,164.66,61.25"><head>Table 1 .</head><label>1</label><figDesc>Overall Mean Performances</figDesc><table coords="4,223.68,81.63,164.66,42.29"><row><cell>Runs</cell><cell>P@5 MRR TBG</cell></row><row><cell cols="2">UDInfoCS2014 1 0.4074 0.5482 1.6271</cell></row><row><cell cols="2">UDInfoCS2014 2 0.5572 0.7439 2.7043</cell></row><row><cell cols="2">TREC Median 0.3492 0.5350 1.3685</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,163.44,136.47,284.50,51.77"><head>Table 2 .</head><label>2</label><figDesc>More analysis of submitted runs</figDesc><table coords="4,163.44,157.23,284.50,31.01"><row><cell>runid</cell><cell>accuary</cell><cell>precision</cell><cell>recall</cell></row><row><cell cols="4">UDInfoCS2014 1 0.8375(1252/1495) 0.844(660/782) 0.8451(660/781)</cell></row><row><cell cols="4">UDInfoCS2014 2 0.8187(1224/1495) 0.9232(890/964) 0.8188(890/1087)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,72.36,723.63,86.50,8.21"><p>http://www.yelp.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,65.96,564.51,483.67,8.21;4,74.52,575.43,20.80,8.21" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,140.52,564.51,253.18,8.21">From RankNet to LambdaRank to LambdaMART: An overview</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="4,65.96,585.87,483.68,8.21;4,74.52,596.91,208.13,8.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,398.03,585.87,151.60,8.21;4,74.52,596.91,63.86,8.21">Overview of the trec 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,157.80,596.91,95.84,8.20">Proceedings of TREC&apos;13</title>
		<meeting>TREC&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.96,607.23,483.54,8.21;4,74.52,618.27,474.99,8.21;4,74.52,629.19,175.84,8.21" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,170.04,607.23,262.50,8.21">An exploration of axiomatic approaches to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,454.92,607.23,94.58,8.20;4,74.52,618.27,446.25,8.21">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.96,639.63,483.03,8.21;4,74.52,650.55,20.80,8.21" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,145.81,639.63,251.20,8.21">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,407.76,639.63,78.47,8.20">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.96,660.99,483.86,8.21;4,74.52,671.91,474.98,8.21;4,74.52,682.95,331.94,8.21" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,212.52,660.99,337.30,8.21;4,74.52,671.91,31.82,8.21">Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,131.52,671.91,387.92,8.21">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.96,693.27,483.56,8.21;4,74.52,704.31,242.56,8.21" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,165.62,693.27,267.63,8.21">An exploration of ranking-based strategy for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,452.88,693.27,96.64,8.20;4,74.52,704.31,85.71,8.20">Proceedings of 21st Text REtrieval Conference</title>
		<meeting>21st Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2012-11">2012. November 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,65.96,714.63,454.53,8.21" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,165.85,714.63,209.99,8.21">An opinion-aware approach to contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,395.64,714.63,95.84,8.20">Proceedings of TREC&apos;13</title>
		<meeting>TREC&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
