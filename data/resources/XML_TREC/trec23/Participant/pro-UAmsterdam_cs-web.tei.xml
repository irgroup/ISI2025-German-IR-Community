<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.10,72.35,351.52,16.84;1,210.46,92.27,188.80,16.84">Venue Recommendation and Web Search Based on Anchor Text</title>
				<funder ref="#_YyYryjQ #_ZdYnrP9">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_jJJSyGB">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_hqVsAQm">
					<orgName type="full">Netherlands Organization for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,200.24,137.97,117.78,11.06"><forename type="first">Seyyed</forename><forename type="middle">Hadi</forename><surname>Hashemi</surname></persName>
							<email>hashemi|kamps@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.27,137.97,66.21,11.06"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.10,72.35,351.52,16.84;1,210.46,92.27,188.80,16.84">Venue Recommendation and Web Search Based on Anchor Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D0C7713E4E1C1356E19FA1DDF2A05B4D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the University of Amsterdam's participation in TREC 2014. For the Contextual Suggestion Track, we experimented with the use of anchor text representations in the language modeling framework, and base our runs either on full ClueWeb12 or the subset of touristic aggregators (e.g., tripadvisor) provided by the organizers of the track. We also look at the effectiveness of priors (in particular, PageRank) and ways of formulating the query based on the context. Our main finding is that the anchor text representation is effective for retrieving candidate attractions, and performs better than a standard document text index. A linear combination of both anchor and document text leads to further improvement. For the Web Track, we continued our experiment with the fusion of anchor text relative to the text-based baseline run. Our main finding is, again, that the combination of anchor and document text leads to improvement, and we demonstrate how the fusion weight can be used as a handle to tune the amount of risk acceptable for the risk sensitive evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper, we present the University of Amsterdam participation in the Contextual Suggestion and Web tracks at TREC 2014 in two relatively self-contained sections. Section 2 discusses the Contextual Suggestion Track, where we define appropriate language models and take advantage of anchor texts of the ClueWeb12 web pages. Section 3 discusses the Web track, in which we indicate how we fuse the baseline with the model based on anchor texts of the Clue-Web12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CONTEXTUAL SUGGESTION TRACK</head><p>In this section, we present our participation in the TREC 2014 Contextual Suggestion Track. The main goal of this track is to investigate search techniques for complex information needs that are highly dependent on context and user interests. In each run, participants have to produce up to 50 ranked suggestions for each pair of profile and context based on a given set of profiles, a set of example suggestions and a set of contexts.</p><p>Each profile corresponds to a user who has judged the example suggestions' description and website of two seed cities (i.e., Chicago, IL and Santa Fe, NM). The user profiles contain a five-point scale rating for each pair of profile and example suggestion. Each example suggestion was rated by users and it includes a title, a description and a URL. The context is a city for which the suggestion rankings are going to be generated. This set has 50 randomly selected cities in the United States, and the name, state, latitude and longitude of each city are available in the contexts set.</p><p>There are two different kinds of submissions in the TREC Contextual Suggestion Track: 1) open web, or 2) Clue-Web12. Open web submissions usually crawl the aggregator websites such as Yelp, TripAdvisor or WikiTravel or use their API in order to gather potentially high quality and upto-date suggestion candidates for the given contexts <ref type="bibr" coords="1,532.87,304.21,9.72,7.86" target="#b3">[4,</ref><ref type="bibr" coords="1,546.21,304.21,6.47,7.86" target="#b4">5]</ref>. The two kinds of submissions are evaluated separately and it is clear that providing contextual suggestion ranking based on the ClueWeb12 is harder than giving contextual suggestion ranking based on the open web using aggregator APIs. This motivated us to focus on ClueWeb12, were in principle a reusable test collection is created.</p><p>The track presents several challenges. First, retrieving relevant suggestions to the given context from the huge number of webpages on the web or ClueWeb12 (i.e., 733,019,372 web pages) requires working with scalable methods able to cope with this volume of data. Second, the task is a genuine needle-in-a-haystack problem, where many of the pages matching a context (city name as query) present no touristic attraction in the city, requiring suitable features that can function as prior probability to separate potentially interesting suggestion candidates from the rest of the web pages Third, personalizing suggestion rankings based on the user profiles is hard giving that these are based on only few examples. Fourth, for each candidate web page a descriptive title and descriptions need to be extracted, good enough to motivate the users to select the result on a hitlist.</p><p>Previous work relied either on the open web or Clue-Web12, and used the textual content of the web pages as document representation. Inspired by the effectiveness of anchor text representations <ref type="bibr" coords="1,430.05,565.73,9.71,7.86" target="#b2">[3,</ref><ref type="bibr" coords="1,442.93,565.73,10.74,7.86" target="#b9">10]</ref>, we experiment with the incoming anchor text of suggestion candidates to estimate their relevancy to the contexts as well as profiles.</p><p>The rest of this section is organized as follows. In Section 2.1 we review some related work on Contextual Suggestion track at TREC 2012 and TREC 2013. In Section 2.2 we detail our models of Contextual Suggestion, and Section 2.3 is devoted to the experimental setup definition and reporting the experimental results. Finally, we present the conclusions and future work in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>In the TREC 2012 Contextual Suggestion Track, participants were allowed to use the open web to retrieve suggestion candidates. All of them used the webpages of the aggrega-tor websites such as Yelp, Google Places, Foursquare and Trip Advisor. A considerable fraction of the participants used category of suggestion candidates that is available in the Yelp website. In that track, the given context had geographical and temporal aspects. However, judging temporal aspect of the context was difficult for the NIST assessors, so it has not been used for the TREC 2013 and TREC 2014 <ref type="bibr" coords="2,53.80,130.86,9.20,7.86" target="#b3">[4]</ref>.</p><p>In the TREC 2013, the participants could use either the open web or the ClueWeb12 dataset, but there were only seven submitted runs out of 34 that were ClueWeb12 runs <ref type="bibr" coords="2,53.80,172.70,9.20,7.86" target="#b4">[5]</ref>. The common approach of the open web runs were retrieving a bag of relevant venues to the given context based on the aggregators' API such as Yelp API, and then re-rank the suggestion candidates based on the user profiles and/or the suggestion categories. In the following, we will discuss more about the previous works on the ClueWeb12 dataset.</p><p>The approach of CWI team had two main parts. They retrieved the most relevant documents for the contexts, and then personalized the sub collection based on the cosine similarity between the documents and the user profiles. The Georgetown University team provided two runs. In the first run, they extract venue names of each context from Wik-iTravel and used them as a query to retrieve the relevant pages to those venues in the ClueWeb12-B collection. Finally, they tried to find the home page of the retrieved venues based on the retrieval score or the similarity between venue name and anchor text of pages. In their second approach, the Georgetown University team created category-based language models and used them to retrieve relevant documents to the category mentioned in the user profiles. IRIT team assigned one or more categories from WordNet and Google Places to each user and used the Terrier to retrieve documents relevant to user categories <ref type="bibr" coords="2,188.78,402.84,9.20,7.86" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Approach</head><p>The Contextual Suggestion Track has two main challenges for ranking suggestions based on the user preferences. The first one is finding a way to answer the "what are the potentially good suggestions for the given context?" question. Moreover, the second main challenge is finding an effective approach to personalize the suggestion candidates based on users' profiles, so that the suggestion rankings could satisfy their needs. The open web submissions usually ignore the first challenge and just either aggregate suggestions of aggregators websites or use suggestions of a aggregator website like Yelp. When using ClueWeb12, this problem cannot be ignored.</p><p>Following the language modeling framework, we used the Bayes' Theorem in order to model contextual suggestion problem. As a result, we use this probabilistic model in order to rank the suggestion candidates. Regarding our two submissions, the only difference of them is the documents representative that is indexed for retrieving relevant suggestion candidates to the context and profile pairs. In our first submission (i.e., Model-Text), we extract textual content of the ClueWeb12 web pages and index them to be able to retrieve the relevant suggestion candidates. Our main contribution that is included in our second submission (i.e., Model-Anchor ) is based on incoming anchor text used to estimate suggestion candidates and contexts as well as suggestion candidates and profiles relevancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Model-Text</head><p>In this model, we wish to estimate P (s|p, c) effectively and rank the suggestion candidates according to this probability. The Bayes' Theorem is invoked in order to more accurately determine the suggestion candidate relevancy to the given pair of profile and context: Prior.</p><formula xml:id="formula_0" coords="2,388.34,129.62,75.69,13.67">p(s|c, p) = p(s)p(c,</formula><p>First, p(s) is the prior probability of each suggestion candidate, which could have a significant role on discriminating potentially high quality suggestion candidates from the other candidates. We had an observation upon the judgments of the ClueWeb12 submissions in Contextual Suggestion track at TREC 2013 that shows PageRank scores of the ClueWeb12 web pages judged as interested or strongly interested suggestions (i.e., interesting suggestions) are usually much higher than the ClueWeb12 web pages judged as uninterested, strongly uninterested or could not load ones (i.e., uninteresting suggestions). Specifically, the average PageRank scores of the ClueWeb12 pages judged interesting is more than 50 times higher than the ones judged uninteresting suggestion candidates at QRel of TREC 2013.</p><p>This observation motivates us to utilize PageRank of suggestion candidates in order to estimate the prior probability of them in our proposed model, so that we could discriminate high quality suggestion candidates from the ones that probably are not good venues for recommending to the users. Due to the availability of the PageRank of the ClueWeb12 dataset at the Lemur project webpage<ref type="foot" coords="2,473.61,678.00,3.65,5.24" target="#foot_0">1</ref> , we just normalize these PageRank scores and use it in our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context.</head><p>Second, p(c|s) is probability of the presence of a context c given a suggestion candidate s. In our first run at TREC 2014 Contextual Suggestion track, we extract the text content of the ClueWeb12 pages and index them using the Terrier IR platform <ref type="bibr" coords="3,125.55,316.35,13.49,7.86" target="#b10">[11]</ref>. Then, the relevance probability of a suggestion candidate s to a context c is estimated by a suggestion language model θs of a suggestion candidate s. Specifically, the following language model is used to estimate relevance of each context and suggestion candidate pair in the University of Amsterdam's submissions:</p><formula xml:id="formula_1" coords="3,123.30,385.62,100.11,19.91">p(c|θs) = t∈c p(t|θs) n(t,c) ,</formula><p>in which, p(t|θs) is the probability of term t given the suggestion language model θs, and n(t, c) is the number of times that term t occurs in context c. To avoid zero probabilities, the JM-smoothing <ref type="bibr" coords="3,131.23,445.95,14.32,7.86" target="#b11">[12]</ref> is used, so the probability p(t|θs) is estimated as follows:</p><formula xml:id="formula_2" coords="3,111.86,473.96,122.98,7.86">p(t|θs) = λp(t|s) + (1 -λ)p(t),</formula><p>where p(t|s) is the maximum likelihood estimation of the occurrence of a term t in a suggestion candidate s, and p(t) is the occurrence probability of the term t in the whole corpus.</p><p>In our experiments, we use the default smoothing parameter λ = 0.15 of Terrier.</p><p>Usually, it is not easy to retrieve web pages that are interesting for the tourists by only giving the context to the retrieval system. In order to retrieve relatively more interesting suggestion candidates for the tourists, the contexts are expanded by some general touristic terms, which are partly mentioned in Table <ref type="table" coords="3,163.22,596.12,3.58,7.86" target="#tab_1">1</ref>. This query expansion is beneficial to retrieve generally interesting venues like Museum web pages related to the context rather than retrieving some uninteresting web pages in this task such as news about that context.</p><p>Retrieving relevant suggestion candidates to a given context among the ClueWeb12 data set is not an easy problem. As a result, it is beneficial to find a proper way to filter useless pages. To this aim, we filter the ClueWeb12 web pages based on the ClueWeb12 IDs that were released by the CWI team; therefore, we only consider the ClueWeb12 pages that are in the domains of the "yelp", "tripadvisor", "wikitravel", "zagat", "xpedia", "orbitz", and "travel.yahoo" websites. This subset has the advantage of including many potentially good quality aggregators pages, and the disadvantage of missing a considerable amount of documents that are relevant to the contexts, but are not in the aggregators domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Profile.</head><p>Third, p(p|s) is probability of the presence of a profile p given a suggestion candidate s. We build positive and negative user profile based on the example suggestions' descriptions, which the user rates interesting and uninteresting, respectively. Finally, the user profile p and suggestion candidate s relevancy is estimated by the same suggestion language model θs that is used to estimate relevance of each context and suggestion candidate pair. The following equation indicates how the negative profile as well as the positive profile is considered for estimating this probability:</p><formula xml:id="formula_3" coords="3,351.82,255.02,169.10,7.86">p(p|θs) = wposp(ppos|θs) + wnegp(pneg|θs),</formula><p>in which, wpos and wneg are, respectively, weight of the positive user profile (i.e., ppos) relevancy and the negative user profile (i.e., pneg) relevancy in the final estimation of the profile relevancy to the suggestion candidate s. According to our experiments on our aggregator domain sub collection, wpos = 0.75 and wneg = -0.25 are the reasonable weights for suggestion candidates personalization.</p><p>Finally, as one of the challenges of the Contextual Suggestion track, we have to generate a title and a description for each of the suggestions. We extract the title of suggestions from the title tag of HTML content of the ClueWeb12 web pages. In order to render more informative sentences as a description, the description was extracted by first looking at sentences that mention the context. If none of the sentences mentions the context, then we extract the text content of the description tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Model-Anchor</head><p>Retrieving relevant suggestion candidates to a given context is one of the difficult challenges of the TREC Contextual Suggestion Track. The problem is that there are a lot of irrelevant suggestion candidates to the given context among the retrieved suggestion candidates, which include context's terms in their contents. In fact, in the case that the content of the ClueWeb12 web pages is indexed, due to the size of the text content of the ClueWeb12 pages in comparison to the short query (i.e., context), many of the irrelevant ClueWeb12 pages have the chance of counting as relevant suggestions.</p><p>We make two attempts to overcome this problem at TREC Contextual Suggestion track. In our first run, we filter the ClueWeb12 and only consider the ClueWeb12 web pages that are in the aggregators' domains. In our second run, instead of using the whole text content of the ClueWeb12 pages, we decide to consider a better representative of them. In fact, to overcome the mentioned problem, rather than indexing the text content of web pages, it is better to index a good summary of the ClueWeb12 pages, which could be the anchor text of them. Since anchor text is a short summary of webpages, it is helpful for the kind of search that users tend to submit a short query <ref type="bibr" coords="3,436.78,700.73,9.20,7.86" target="#b6">[7]</ref>. Similarly, it could be also beneficial to index anchor text of the ClueWeb12 pages for the Contextual Suggestion and retrieve relevant suggestion candidates to the short context queries.</p><p>This approach tends to filter those documents that include the given context in their content, but they might only give some unimportant information about the context so that they cannot be a relevant suggestion for the context. Therefore, in our second submission at TREC 2014 Contextual Suggestion track, we index the anchor text of the ClueWeb12 pages that was extracted in <ref type="bibr" coords="4,213.90,141.32,9.71,7.86" target="#b7">[8]</ref> and estimate the context and suggestion candidate as well as user profile and suggestion candidate relevance scores based on it. However, the retrieval model of the second submission of the University of Amsterdam is exactly same as the first submission. Specifically, PageRank of each suggestion candidate s is considered as its prior probability (i.e., p(s)), and the anchor text of the suggestion candidate s is used to build its suggestion language model θs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Model-LC</head><p>As it is mentioned before, Model-Anchor tends to have a better precision than Model-Text, and it is also expected that the Model-Text has a better recall than Model-Anchor in retrieving the relevant suggestions. As a result, we decide to use Linear Combination method to fuse the Model-Text with the Model-Anchor suggestion ranking. To this aim, the following equation is used to combine the two rankings:</p><formula xml:id="formula_4" coords="4,53.80,334.60,243.10,8.35">p(s|c, p, β) = βp Model-Text (s|c, p)+(1-β)p Model-Anchor (s|c, p),</formula><p>where p(s|c, p, β) is the relevance probability of a suggestion s to a context c and a profile p based on the weight β and 1 -β given to Model-Text and Model-Anchor rankings. The performance of this model and the optimal β parameter is discussed in Section 2.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Model-Anchor-Full</head><p>The aggregators sub collection has many good suggestion candidates, but it is a tiny collection (i.e., 175, 260 Clue-Web12 pages) and misses many of the suggestion candidates that are useful to be considered as a high quality suggestion candidate. For instance, the home page of suggestions are not included in this sub collection. As a result, we indexed the ClueWeb12-full anchor text, and run our proposed model based on this dataset. This model is exactly the same as Model-Anchor, but based on the ClueWeb12-full and without filtering the collection based on the aggregators' domains.</p><p>In this run, we personalize the ranking based on the positive, negative and neutral profile. We build the profiles based on the user judgments of example suggestions. Specifically, in the case that the user score either the description or the suggested website more than 2, that description has been added to the positive profile. We also create neutral user profile, and fill in it by example suggestions descriptions that both of theirs website and description were scored 2 in the user profile. Descriptions of the rest of the example suggestions are considered as a negative user profile.</p><p>In order to estimate a profile p and a suggestion candidate s relevancy (i.e., p(p|s)), we use the suggestion language model θs that is introduced in previous section. The following equation indicates how the positive, negative and neutral profiles are applied to the profile and suggestion candidate relevancy estimation:</p><formula xml:id="formula_5" coords="4,53.80,711.19,239.10,7.86">p(p|θs) = wposp(ppos|θs) + wneup(pneu|θs) + wnegp(pneg|θs),</formula><p>where wneu is the weight of the neutral user profile (i.e., pneu) relevancy in the final estimation of the profile relevancy to the suggestion candidate s. Due to the fact that the suggestion judgments is sparse, it is not easy to find the optimal weights for this model. However, we find wpos = 3, wneu = 1 and wneg = -1 as reasonable weights for Model-Anchor-Full personalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experiments</head><p>An extensive set of experiments are designed to address the following research questions:</p><p>• How efficient is using PageRank scores as prior probabilities of suggestion candidates?</p><p>• What is the effect of using query expansion in our proposed model rather than only using city name as a query?</p><p>• How do our two submissions (i.e., Model-Text as a baseline and Model-Anchor ) perform compared to each other? Is it beneficial to take into account the anchor text of suggestion candidates in order to estimate the relevancy of them to the given context and profile pair?</p><p>• How efficient is the linear combination of Model-Text and Model-Anchor ? What is the optimal parameter of the linear fusion?</p><p>• How efficient is the Model-Anchor in comparison to the Model-Text in each context?</p><p>• How efficient is the Model-Anchor in comparison to the Model-Text in each profile?</p><p>• Could anchor text be helpful to retrieve suggestion candidates related to context and profile pairs without the help of filtering based on the aggregators domains?</p><p>• What is the effect of using neutral, negative, and positive profiles in Model-Anchor-Full suggestion ranking personalization?</p><p>• How efficient are our proposed models based on the fraction of suggestions judged in the TREC 2014 judgments?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Experimental setup and metrics</head><p>In this section, we describe dataset and evaluation metrics. We build our models based on the ClueWeb12 dataset that was created to support research on Information Retrieval in 2012. This dataset consists of 733,019,372 English webpages and its size is 27.3 TB that shows how big this dataset is. The information about the PageRank of ClueWeb12 web pages is available at the Lemur project website. Therefore, we do not redo the effort for calculating the PageRank and use the PageRank score, which is provided there.</p><p>As it is mentioned before, our main idea is using the anchor text of the ClueWeb12 web pages to improve the precision of our proposed Contextual Suggestion model. It is worth mentioning that Djoerd Hiemstra extracted the anchor text of the ClueWeb12 and we reuse this data as an input to our model. The data contains anchor text of about 64 percent of the ClueWeb12-full dataset (i.e., 0.5 billion pages) <ref type="bibr" coords="4,345.51,700.73,9.20,7.86" target="#b7">[8]</ref>. Each record of this data consists of "ClueWeb12 ID", "URL" and "Anchor Text". The users whose profiles were given to the TREC Contextual Suggestion participants and also the NIST assessors judged the output suggestions of our proposed models. The evaluation results of our two submissions (i.e., Model-Text and Model-Anchor ) will be discussed in Section 2.3.2. We also evaluate the performance of the Model-LC based on the TREC 2014 official suggestion judgments. However, due to the fact that a small fraction of the submissions are based on the ClueWeb12 dataset, the provided suggestions judgments does not have enough information to judge the Model-Anchor-Full that is not submitted for the TREC Conference. In addition, the major part of the ClueWeb12 judged suggestions in TREC 2014 is a subset of the aggregators sub collection. As a result, it is difficult to evaluate suggestion candidates outside of this sub collection. To overcome this problem, we map the judged open web URL to the Clue-Web12 pages, and evaluate the Model-Anchor-Full by the expanded ClueWeb12 suggestion judgments.</p><p>In order to map the judged open web URLs to the Clue-Web12 pages, we consider exact matching of the URLs and also matching of the normalized URLs (i.e., the URLs that are normalized by removing the "http://", "https://", "www", and the last slash). Finally, we have mapped 1623 out of 15, 480 judged open web URLs to the ClueWeb12 pages, but it is still not enough to judge the Model-Anchor-Full.</p><p>The evaluation of the Contextual Suggestion track is based on the two common Information Retrieval metrics (i.e., P @N and M RR), and also one contextual suggestion specific made metric (i.e., T BG or Time-Biased Gain) <ref type="bibr" coords="5,218.82,532.53,9.20,7.86" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Experimental Results</head><p>To demonstrate how the idea of using anchor text as suggestions representative in estimating suggestion candidates relevancy to contexts or profiles can improve the contextual suggestion performance, we submitted Model-Text as a baseline in TREC 2014. We compare various aspects of Model-Anchor and Model-LC to the baseline (i.e., Model-Text) in the following paragraphs.</p><p>First of all, we want to test the effectiveness of using PageRank score as a prior probability of a suggestion candidate. According to Table <ref type="table" coords="5,406.75,491.51,3.58,7.86" target="#tab_2">2</ref>, using PageRank score as the prior probability of suggestion candidates has more effect on the Model-Text suggestion ranking than the Model-Anchor suggestion ranking. This observation shows that the retrieved suggestion candidates based on the web pages' anchor text usually have high PageRank scores. On the other hand, using PageRank scores of suggestion candidates significantly improves the Model-Text suggestion ranking. This experiment shows that using Pagerank scores as prior probabilities generally improves the suggestion ranking. Therefore, in the rest of the paper, we always use the PageRank score as a prior probability of suggestion candidates in our experiments.</p><p>In order to evaluate the effect of query expansion in retrieving relevant suggestion candidates to the given city, we run Model-Text and Model-Anchor without adding any term to the given city. As it is indicated in Table <ref type="table" coords="5,487.84,658.88,3.58,7.86" target="#tab_3">3</ref>, it has a considerable effect on the suggestion ranking of Model-Text as well as Model-Anchor. Specifically, expanding the city name with some general tourist attraction terms (e.g., restaurant) helps the proposed model to retrieve relevant suggestion candidates rather than retrieving relevant web pages to the given city, which is not guaranteed to be a proper suggestion. This experiment shows the value of using query expansion in retrieving relevant suggestion candidates. In the rest of the experiments, we always take query expansion into account in our suggestion ranking models. Table <ref type="table" coords="6,87.06,219.53,4.61,7.86" target="#tab_4">4</ref> reports the evaluation results of Model-Text, Model-Anchor and Model-LC. In this experiment, it is clear that Model-Anchor performs better than the baseline in terms of the average P @5, M RR and T BG. The improvement in the M RR proves that although using anchor text might worsen the recall of the relevant suggestion candidates, it definitely improves the precision and the first relevant suggestion in the ranking. Moreover, Model-Anchor improves the baseline in terms of the time-biased gain, so the suggestions provided by the Model-Anchor is more interesting for the users and they spend more time on exploring the suggestions in comparison to the Model-Text suggestions.</p><p>Moreover, we evaluate the performance of the linear fusion of Model-Text with Model-Anchor rankings and also answer the question "what is the optimal β value for the Model-LC ?" We test the performance of the Model-LC for different values of β ∈ [0, 1] with 0.1 intervals. Figure <ref type="figure" coords="6,212.15,386.90,4.61,7.86" target="#fig_0">1</ref> indicates that, for β = 0.1 and β = 0.2 (hence most weight on the Anchor text), the performance of Model-LC is better than Model-Anchor in terms of p@5 and T BG metrics. However, if you increase the β value, Model-Text will add noises to the ranking and worsen the overall performance. On the other hand, the Model-LC is not able to improve the M RR of the Model-Anchor. This observation also demonstrates the advantage of Model-Anchor over the Model-Text based on the M RR metric. The performance of Model-LC for β = 0.2 is also compared against the Model-Text and the Model-Anchor in Table <ref type="table" coords="6,79.12,501.97,3.58,7.86" target="#tab_4">4</ref>.</p><p>In order to have a more detail understanding on the evaluation results, we turn to a context-level as well as profilelevel analysis of the comparison illustrated in previous experiment. First, we plot the differences in average M RR as well as T BG between Model-Anchor and Model-Text per context in Figure <ref type="figure" coords="6,126.30,564.74,4.61,7.86">2</ref> and<ref type="figure" coords="6,151.85,564.74,3.58,7.86">3</ref>, respectively.</p><p>As it is obvious in the Figure <ref type="figure" coords="6,190.19,575.20,3.58,7.86">2</ref>, the Model-Anchor improves the performance of the baseline in most of the contexts, and worsen it in just 5 out of 50 contexts. Model-Anchor worsen the performance of the contextual suggestion in context number 138, which is corresponded to the "Clarksville" city in central Tennessee, more than the other 4 contexts. In order to find the reason, we look at suggestion rankings of the Model-Anchor as well as the Model-Text for the context and profile pair that judged the 1st suggestion of the Model-Text ranking and the 4th suggestion of the Model-Anchor ranking as relevant ones.</p><p>We find out that the city name of this context makes it difficult for Model-Anchor to improve the baseline in the tiny aggregators sub collection. As a matter of fact, there are more than 20 cities with the name of Clarksville in different states of the United States. Therefore, it is not easy to find the relevant suggestions of the Clarksville in Tennessee in the top-5 ranks based on the only anchor text of the pages in the tiny aggregators sub collection. For this context, the Model-Anchor retrieves the disambiguation page of the wikitravel for Clarksville cities. On the other hand, Model-Text provides the wikitravel page of the "Nashville" city in the state of Tennessee as the 1st suggestion in the ranking. Due to the fact that the Nashville is just 47.8 miles further than the Clarksville in the state of Tennessee, this page is judged as a relevant suggestion. However, the Clarksville is not mentioned in the anchor text of the Nashville wikitravel page, and it is reasonable that it is not included in the top-5 ranking of the Model-Anchor. As illustrated in Figure <ref type="figure" coords="6,512.43,439.21,3.58,7.86">3</ref>, a similar pattern is observed for the evaluation by the TBG metric. Figure <ref type="figure" coords="6,354.28,460.13,4.61,7.86" target="#fig_2">4</ref> is a profile-level analysis of the comparison of the Model-Anchor against the baseline. According to this figure, Model-Anchor improves the average M RR of the baseline for most of the given profiles, but only worsen 8 out of the 299 profiles. We analyze two of the profiles in which the Model-Anchor worsen the performance of the baseline more than the other 6 ones.</p><p>First, due to the fact that the profile 948 only judged 2 out of the 50 contexts, his/her average judgments is highly affected by the suggestions of the Clarksville TN context that we analyzed it before. Second, the reason of the difference between the average M RR of Model-Anchor and Model-Text for the profile 700 is his/her judgment in "Kalamazoo MI" context. It is so interesting to know that the Model-Anchor suggests the WikiTravel page of the Kalamazoo city that is judged as an irrelevant suggestion in the first rank. On the other hand, the first rank of the Model-Text suggestion is the WikiTravel page of the state of Michigan that is judged as a relevant suggestion. It seems that the profile looks for a broader suggestions rather than some specific suggestions for the context. According to the experiments, it is crystal clear that, in comparison to the baseline, the Model-Anchor tends to suggest more precise suggestions for the contexts and profiles pairs, so that it is able to improve the ranking of suggestion candidates. In this experiment, we analyze the performance of the Model-Anchor-Full based on the expanded ClueWeb12 judgments. Due to our observation that only 3 out of 74, 750 top-5 suggestions of Model-Anchor-Full is judged in the expanded TREC 2014 judgments, it is not possible to compare Model-Anchor-Full performance to other models in term of the precision@5. In addition, only 14 suggestions of Model-Anchor-Full are judged in our expanded ClueWeb12 judgments, which makes it too difficult to evaluate the performance based on any other metric.</p><p>However, according to the 14 judged suggestions, Table <ref type="table" coords="7,288.31,620.89,4.61,7.86" target="#tab_5">5</ref> indicates that the performance of the Model-Anchor-Full in providing relevant suggestion candidates is acceptable. As it is shown in Table <ref type="table" coords="7,126.85,652.27,3.58,7.86" target="#tab_5">5</ref>, only 2 out of the 14 suggestions scored less than 2. As a result, this observation demonstrates that the Model-Anchor-Full rarely suggest irrelevant suggestion candidates, which is so helpful for improving the performance of the contextual suggestion approaches in terms of precision and M RR.</p><p>As it is mentioned in the previous experiment, the TREC suggestion judgment is sparse, and it is not easy to evaluate the runs that are not submitted to the track. As a result, in the next experiments, we decide to evaluate the proposed models based on the fraction of suggestions, which are judged in the TREC 2014 judgments. In Table <ref type="table" coords="7,532.69,568.58,4.61,7.86" target="#tab_6">6</ref> and 7, it is indicated that how many percent of the top 5, 10, and 20 suggestions in the given rankings are judged. Moreover, Table <ref type="table" coords="7,365.47,599.96,4.61,7.86" target="#tab_6">6</ref> and<ref type="table" coords="7,391.97,599.96,4.61,7.86" target="#tab_7">7</ref> show how many percent of the judged suggestions are relevant. In these experiments, in the case that a suggestion is marginally or precisely geographically appropriate and judged strongly interesting or interesting, it is counted as a relevant suggestion. The rest of the suggestions are considered as irrelevant ones.</p><p>Due to the difficulty of the suggestion ranking on the Clue-Web12-Full, we decide to use the neutral profile in order to improve the performance of the Model-Anchor-Full in retrieving relevant suggestions. Table <ref type="table" coords="7,467.85,694.11,4.61,7.86" target="#tab_6">6</ref> shows the effectiveness of the idea of using neutral profile in suggestion rank-  ing personalization. In this experiment, we evaluate models based on the expanded suggestion judgment. According to this table, none of the suggestions retrieved by the model that only use positive profile is judged; as a result, it is not possible to compare this model to other variations of the Model-Anchor-Full. However, Table <ref type="table" coords="8,215.20,583.49,4.61,7.86" target="#tab_6">6</ref> indicates that the model whose personalization is based on the positive, negative and also neutral profiles is performing better than the model ignoring neutral profile in the suggestion ranking personalization. In the rest of the experiments, Model-Anchor-Full is the one using positive, negative and neutral profiles to personalize the suggestion ranking. Finally, Table <ref type="table" coords="8,121.10,656.72,4.61,7.86" target="#tab_7">7</ref> provides a fair metric to compare Model-Anchor-Full against our other proposed models. Table <ref type="table" coords="8,288.30,667.18,4.61,7.86" target="#tab_7">7</ref> indicates that, based on the evaluation that only considers judged suggestions, the Model-Anchor-Full is more effective than our other proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Conclusion and Future Work</head><p>In this paper, we studied Contextual Suggestion problem through proposing a model based on the Bayes' Theorem to retrieve relevant suggestion candidates to the given context and profile pair. We used anchor text of the ClueWeb12 web pages in order to precisely retrieve the relevant suggestions to the given context. We tested our proposed model on the aggregators sub collection of the ClueWeb12 as well as the ClueWeb12-full dataset. We also fuse the proposed model that is based on the anchor text with the model based on the text content of the web pages. The experimental results indicated that the idea of using anchor text is promising in retrieving relevant suggestion candidates, and the linear fusion of the proposed model based on the anchor text with the model based on the text content improves the performance of the contextual suggestion ranking in terms of p@5 and T BG. As a future work, we continue to work on defin-ing appropriate language models, investigating other priors and query expansions including (sub)domain classifiers, and improving the document and anchor text representations by including titles and other sources of annotation, such as can be found in Social Media. Our overall goal is to contribute to the building of a reusable test collection for contextual suggestion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WEB TRACK</head><p>In this section, we present our participation in TREC 2014 Web track. The goal of this track is to explore and evaluate retrieval approaches over large-scale subset of the Web (i.e., ClueWeb12 dataset that includes 733,019,372 English web pages) <ref type="bibr" coords="9,82.66,203.91,9.20,7.86" target="#b1">[2]</ref>. The Web track organizers provide topics, which were developed with the information extracted from the logs of commercial Web search engines. In order to test different aspects of the approaches, topics contain both broad and specific queries. In this track, the ClueWeb12 dataset is used as a data collection, but participants are also eligible to submit their runs based on the ClueWeb12-B13 dataset, which is smaller than ClueWeb12-full collection.</p><p>In this track, we want to answer these research questions:</p><p>1. What is the effect of using web pages' anchor texts to estimate their relevance to the given query?</p><p>2. What is the performance of the linear combination of our proposed approach with the baseline provided by the Web track organizers?</p><p>3. What are the optimal weights of the linear combination of the rankings the topics and the judgments of the TREC2013?</p><p>4. How does the Model-LC perform in the risk-sensitve task?</p><p>In order to participate in the Web track, we have used the ClueWeb12-full collection. Specifically, we tackle the Adhoc Retrieval task and propose an approach based on the anchor text of the ClueWeb12-full dataset. We have used the Bayes' Theorem to simplify the problem and take into account the prior probability of web pages. The anchor texts of web pages are indexed by the Terrier IR platform, and the relevance probability of them to the queries are estimated by web pages' language models. We also have used the Linear Combination method to fuse the baseline with the proposed approach. The optimal weights of the linear combination are learned by the logistic regression based on the TREC2013 judgments.</p><p>The rest of this section is organized as follows. In Section 3.1 we detail our proposed models for the Web Track Adhoc retrieval task, and Section 3.2 is devoted to a report of the experimental results. Finally, we present the conclusions and future work in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>We propose two approaches for the Web Track Ad-hoc retrieval task. The first approach is the model that is used anchor text as webpages' representatives, and the second one is the linear fusion of the baseline with the first proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Model-Anchor</head><p>The main challenge of the Web Track is finding most relevant results among the huge number of candidates (i.e., more than 0.7 billion ClueWeb12 pages). Test topics of the Web Track are relatively short queries that make it difficult to find the relevant results based on the whole text content of the web pages. Since anchor text is a short and informative summary of web pages, it is effective to use it in the kind of search that users tend to submit short queries <ref type="bibr" coords="9,519.67,143.81,9.20,7.86" target="#b6">[7]</ref>. As a result, we indexed anchor text of the ClueWeb12 webpages in order to precisely retrieve relevant webpages.</p><p>In order to retrieve relevant web pages to the given queries, we have used the following Bayes' Theorem:</p><formula xml:id="formula_6" coords="9,396.12,201.10,80.49,19.75">p(p|q) = p(p)p(q|p) p(q) ,</formula><p>where p(p|q) represents the relevance score of web page p for the given query q, p(p) is the prior probability of being relevant for a web page p, p(q|p) is the probability of the presence of a query q given a web page p, and p(q) is the prior probability of the presence of a query q. Since P (q) is a constant for a given query q, it can be ignored for the purpose of web page ranking. As a result, the equation is simplified as follows:</p><formula xml:id="formula_7" coords="9,397.31,318.61,78.10,7.86">p(p|q) = p(p)p(q|p).</formula><p>This model contains prior probability of web pages and probability of the presence of a query q for the given web page p. In our proposed model, the prior probability of the web pages are estimated by their Pagerank scores, which are available at the Lemur project web page. Moreover, the ClueWeb12 anchor text is indexed by the Terrier IR platform <ref type="bibr" coords="9,339.03,397.82,13.49,7.86" target="#b10">[11]</ref>, and the probability of the presence of a query q in a web page p is estimated by a web page language model θp of a web page p. As a matter of fact, the following web page language model is used to estimate this probability: q) , in which, p(t|θp) is the probability of term t given the web page language model θp, and n(t, q) is the number of times that term t occurs in query q. To avoid zero probabilities, the JM-smoothing <ref type="bibr" coords="9,394.17,504.93,14.31,7.86" target="#b11">[12]</ref> is used, so the probability p(t|θp) is estimated as follows:</p><formula xml:id="formula_8" coords="9,385.60,445.07,92.11,19.91">p(q|θp) = t∈q p(t|θp) n(t,</formula><formula xml:id="formula_9" coords="9,374.53,531.84,123.66,7.86">p(t|θp) = λp(t|p) + (1 -λ)p(t),</formula><p>where p(t|p) is the maximum likelihood estimation of the occurrence of a term t in a web page candidate p, and p(t) is the occurrence probability of the term t in the web page repository. In our experiments, we use the default smoothing parameter λ = 0.15 in the Terrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Model-LC</head><p>As we mentioned before, the Web track 2014 topics are relatively short queries, but not all of them. For instance, for the query "how has african american music influence history", it is better to also take advantage of the text content of the web pages. Therefore, we have used the linear combination method to fuse the Model-Anchor ranking with the baseline. Specifically, the following equation is applied to fuse the rankings: p(p|q, w1, w2) = w1p M odel-Anchor (p|q) + w2p baseline (p|q), where p(p|q, w1, w2) is the relevance probability of a web page p to a query q based on the weight w1 and w2 given to Model-Anchor and baseline rankings. The optimal weights of the linear combination are learned based on the TREC 2013 queries and judgments.</p><p>In order to learn a model to estimate the weights, in the preprocessing phase, we filter the documents, which were not retrieved by either the baseline or the Model-Anchor for each query of TREC 2013 topics. Then, we use relevance variable c ∈ {0, 1} to denote whether a web page p is relevant to a given query q or not. In fact, we want to learn the unknown parameters θ (i.e., w1 and w2 in the linear combination of the rankings) of the probability p θ (c = 1|p).</p><p>For each web we generate training set as T = {(p, q, p l )|p ∈ D qrel , q ∈ Q, p l ∈ {0, 1}}, where D qrel denotes the documents judged in the web track 2013 qrel, Q is the TREC 2013 queries and p l indicates the label of web page p based on the TREC 2013 judgments. In particular, p l = 1 if web page p is relevant to the query q.</p><p>The members of set T can be divided into positive and negative instances based on the relevance judgment of p l of web page p for a give query q. As a result, the likelihood L of the training data is as follows:</p><formula xml:id="formula_10" coords="10,84.19,303.14,178.32,27.50">L = |T | n=1 P θ (c = 1|pn, q) p l P θ (c = 0|pn, q) 1-p l .</formula><p>We model P θ (c = 1|pn, q) by logistic functions on a linear combination of features (i.e., Model-Anchor and baseline relevance scores). The estimated parameters can then be plugged into w1 and w2 weights of the linear fusion of Model-LC. According to the learned parameters on the TREC 2013 judgments, the optimal w1 and w2 weights are 171.472 and 33.426.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>A number of experiments are designed to address the following questions of the proposed research:</p><p>• How do our two proposed models (i.e., Model-Anchor and Model-LC ) perform compared to the Terrier baseline?</p><p>• What are the optimal parameters of the linear fusion?</p><p>• What is the advantage of fusing the Model-Anchor ranking with the basline?</p><p>• What is the optimal parameter of the Model-LC in the risk-sensitive task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Experimental setup and metrics</head><p>In this section, we describe dataset and evaluation metrics. We build our models based on the ClueWeb12 dataset that was created to support research on Information Retrieval in 2012. The anchor text of ClueWeb12 web pages that is extracted by Djoerd Hiemstra <ref type="bibr" coords="10,168.48,617.04,9.72,7.86" target="#b7">[8]</ref> is used as representatives of the web pages in estimating their relevancy. In addition, we have used the PageRank of ClueWeb12 web pages available at the Lemur project website.</p><p>The evaluation of the Web track Ad-hoc rertieval task is primarily based on the intent-aware expected reciprocal rank (ERR -IA) <ref type="bibr" coords="10,108.47,679.80,9.20,7.86" target="#b0">[1]</ref>. In addition to ERR -IA, some standard information retrieval measures such as M AP , precision@10 and N DCG@10 are reported to evaluate different aspects of the participants' approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Experimental Results</head><p>Indexing ClueWeb12 anchor text in Model-Anchor is beneficial to improve the precision of retrieving relevant web pages, but it is not as good as baseline in recall of the relevant webpages. Table <ref type="table" coords="10,406.69,282.29,4.61,7.86" target="#tab_8">8</ref> shows the performance measures of baseline, Model-Anchor and Model-LC. According to this table, the linear fusion of the baseline with the Model-Anchor ranking improves the baseline in terms of ERR-IA, nDCG, N RBP and nN RBP , but does not improve the baseline based on M AP -IA and P -IA.</p><p>The Model-Anchor improves the baseline of TREC 2013, and because of this, the weight of Model-Anchor is about 6 times more than the weight of the baseline in the linear fusion of them. However, as it is demonstrated in Table <ref type="table" coords="10,548.76,376.44,3.58,7.86" target="#tab_8">8</ref>, the baseline of TREC 2014 is performing better than the Model-Anchor, so the learned weights on the TREC 2013 topics could not be the optimal weights for the TREC 2014 queries. As Table <ref type="table" coords="10,389.59,418.28,4.61,7.86" target="#tab_8">8</ref> shows, although the learned weights are not optimal weights for the TREC 2014 queries, the Model-LC improves the baseline, which indicates the effectiveness of the linear combination of the baseline with the Model-Anchor. Figure <ref type="figure" coords="10,381.19,460.13,4.61,7.86" target="#fig_3">5</ref> plots the optimal parameters of Model-LC for TREC 2014. In this figure, for simplicity, we consider β = w 2 w 1 +w 2 ; as a result, w1 = 1 -β and w2 = β. According to Figure <ref type="figure" coords="10,357.12,491.51,3.58,7.86" target="#fig_3">5</ref>, w1 = 0.3 and w2 = 0.7 are the optimal weights of the Model-LC for TREC 2014.</p><p>Figure <ref type="figure" coords="10,355.92,512.43,4.61,7.86">6</ref> shows the effectiveness of the baseline, Model-Anchor and Model-LC for a long query "how has african american music influence history". This is a long and focused intent query, which could help us to gain a better understanding of how the linear fusion of Model-Anchor with the baseline could improve the performance of the Model-Anchor. As it is demonstrated in Figure <ref type="figure" coords="10,488.84,575.20,3.58,7.86">6</ref>, the Model-LC takes advantage of the baseline in recall of relevant web pages in order to improve the overall performance of the Model-Anchor.</p><p>There are also number of examples that indicates the performance of Model-LC is better than the baseline and the Model-Anchor. For instance, Figures <ref type="figure" coords="10,472.58,637.96,4.61,7.86">7</ref> and<ref type="figure" coords="10,499.80,637.96,4.61,7.86">8</ref> contain two examples of topics that in one of them, Model-Anchor performs better than the baseline and in the other one, baseline performs better than Model-Anchor. This experiment indicates that the linear combination of the baseline with the Model-Anchor could improve the performance of the baseline and Model-Anchor rankings independent of the information about the performance of them compared to each other.  In Figure <ref type="figure" coords="11,101.63,605.26,3.58,7.86">9</ref>, we analyze performance of the Model-LC having different β parameters in risk-sensitive task. As it is demonstrated in this figure, although we increase value of the risk factor, the Model-LC with β ∈ [0.7, 1.0] performs well and minimizes the retrieval losses with respect to the baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conclusion and Future Work</head><p>We participated in the Web Track's Ad-hoc retrieval task to evaluate effect of the anchor text in retrieving relevant web pages to the given query from a huge number of can- didates in the ClueWeb12-full dataset. We were also eager to investigate the effect of linear fusion of the ranking based on the anchor text with the baseline ranking. We learned the parameters of the linear combination of the rankings by the logistic regression binary classifier. The experimental results indicate that the fusion of the proposed model with the baseline improved the baseline in terms of the intent-aware expected reciprocal rank (ERR -IA), which is the primary metric in evaluating the submissions. We also evaluate performance of the linear fusion of the ranking based on the anchor text with the baseline ranking in risk-sensitive task, which shows the effectiveness of this model in minimizing the retrieval losses. As a future work, we plan to propose further effective data fusion method to combine these two different kinds of ranking, such as mixture language models <ref type="bibr" coords="12,260.60,68.10,9.20,7.86" target="#b8">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,158.37,208.59,292.98,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Impact of relative fusion weight (β) on M RR, P @5 and T BG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,115.68,259.41,378.35,7.86"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Difference between the average MRR of Model-Anchor and Model-Text per context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,117.97,260.42,373.77,7.86"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Difference between the average MRR of Model-Anchor and Model-Text per profile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,326.89,203.44,218.94,7.86"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of fusion weight (β) on ERR-IA@10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,53.80,347.69,239.11,7.86;11,53.80,358.15,65.02,7.86"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Query 276: "how has african american music influence history."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,371.90,347.69,130.05,7.86"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Query 275: "uss cole."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,59.18,55.00,228.35,182.49"><head>Table 1 :</head><label>1</label><figDesc>Expansion terms for the context as query Expanded query based on context (city name) plus . . .</figDesc><table coords="3,59.18,93.63,228.35,143.85"><row><cell>Food, cafes, desserts, sandwiches, coffee, ice cream,</cell></row><row><cell>gourmet, local flavor, tea, bbq, dim sum, sushi, vege-</cell></row><row><cell>tarian, chicken wings, breweries, chocolate, art &amp; enter-</cell></row><row><cell>tainment, ticket sales, theater, galleries, arcades, venues,</cell></row><row><cell>spas, massage, museums, jazz and blues, music venues,</cell></row><row><cell>mini golf, bowling, yoga, comedy clubs, Pilates, sports</cell></row><row><cell>teams, Nightlife, bars, wine bars, juice bars, event plan-</cell></row><row><cell>ning, dance clubs, gastro pub, lounges, jazz and blues,</cell></row><row><cell>pubs, outdoor, zoos, parks, amusement park, garden,</cell></row><row><cell>lake, travel service, Shopping, home decor, dept stores,</cell></row><row><cell>stationery, tobacco shop, gourmet, grocery, shopping cen-</cell></row><row><cell>ter, bookstores, farmers market, flea markets, hobby</cell></row><row><cell>shops, religious orgs, airport, hotel, tour, landmark, mon-</cell></row><row><cell>ument, public service, travel services.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,53.80,236.98,502.13,482.07"><head>Table 2 :</head><label>2</label><figDesc>Table 2 indicates the evaluation results of Model-Text and Model-Anchor with uniform as well as PageRank prior probability. As it is expected based on our observation in Section 2.2.1, taking in to account PageRank scores of suggestion candidates as prior probabilities of them is helpful to Effectiveness of the uniform and PageRank-based prior probability (official qrels)</figDesc><table coords="5,325.66,271.11,221.42,58.72"><row><cell>Method</cell><cell>p@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>Model-Text-Uniform</cell><cell>0.0279</cell><cell>0.0707</cell><cell>0.1198</cell></row><row><cell>Model-Text-PageRank</cell><cell cols="3">0.0602 0.0994 0.1969</cell></row><row><cell>Model-Anchor-Uniform</cell><cell>0.0863</cell><cell>0.1858</cell><cell>0.3239</cell></row><row><cell cols="4">Model-Anchor-PageRank 0.0903 0.1979 0.3411</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,316.81,348.98,243.81,139.93"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness of city name versus expanded query (official qrels)</figDesc><table coords="5,316.81,383.11,243.81,105.81"><row><cell>Method</cell><cell>p@5</cell><cell>MRR</cell><cell>TBG</cell></row><row><cell>Model-Text-QUnExpanded</cell><cell>0.0328</cell><cell>0.0410</cell><cell>0.0441</cell></row><row><cell>Model-Text-QExpanded</cell><cell cols="3">0.0602 0.0994 0.1969</cell></row><row><cell cols="2">Model-Anchor-QUnExpanded 0.0502</cell><cell>0.1373</cell><cell>0.1919</cell></row><row><cell>Model-Anchor-QExpanded</cell><cell cols="3">0.0903 0.1979 0.3411</cell></row><row><cell cols="4">discriminate potentially relevant suggestion candidates from</cell></row><row><cell>potentially irrelevant ones.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,53.80,55.00,239.11,77.88"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of combined text and anchor (linear combination, β = 0.2, official qrels)</figDesc><table coords="6,59.18,89.12,228.95,43.75"><row><cell>Method</cell><cell>p@5</cell><cell cols="3">(%) MRR (%) TBG (%)</cell></row><row><cell>Model-Text</cell><cell cols="2">0.0602 -</cell><cell>0.0994 -</cell><cell>0.1969 -</cell></row><row><cell cols="5">Model-Anchor 0.0903 50% 0.1979 99% 0.3411 73%</cell></row><row><cell>Model-LC</cell><cell cols="4">0.0943 57% 0.1643 65% 0.3780 92%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,316.81,55.00,239.11,203.41"><head>Table 5 :</head><label>5</label><figDesc>Relevant Descriptions and Documents of Model-Anchor-Full based on 14 profile, context, suggestion triples (expanded qrels)</figDesc><table coords="6,322.19,99.58,233.64,158.82"><row><cell>Prof. Cont.</cell><cell>ClueWeb12 ID</cell><cell cols="2">Top-5 Des. Doc.</cell></row><row><cell cols="3">983 133 ClueWeb12-0007wb-23-21753 Yes</cell><cell>3</cell><cell>3</cell></row><row><cell cols="3">801 110 ClueWeb12-1601wb-75-03301 Yes</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">938 110 ClueWeb12-1601wb-75-03301 Yes</cell><cell>1</cell><cell>1</cell></row><row><cell cols="3">834 110 ClueWeb12-0000wt-00-11531 No</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">748 112 ClueWeb12-1713wb-89-10896 No</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">855 112 ClueWeb12-1713wb-89-10896 No</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">898 112 ClueWeb12-1713wb-89-10896 No</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">976 112 ClueWeb12-1713wb-89-10896 No</cell><cell>3</cell><cell>3</cell></row><row><cell cols="3">765 133 ClueWeb12-0007wb-23-21753 No</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">852 133 ClueWeb12-0107wb-00-20064 No</cell><cell>1</cell><cell>0</cell></row><row><cell cols="3">918 133 ClueWeb12-0007wb-23-21753 No</cell><cell>3</cell><cell>3</cell></row><row><cell cols="3">945 133 ClueWeb12-0007wb-23-21753 No</cell><cell>1</cell><cell>2</cell></row><row><cell cols="3">980 133 ClueWeb12-0007wb-23-21753 No</cell><cell>2</cell><cell>2</cell></row><row><cell cols="3">726 146 ClueWeb12-0207wb-47-26667 No</cell><cell>3</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,53.80,281.84,502.12,98.80"><head>Table 6 :</head><label>6</label><figDesc>Fractions of judged pages, and fraction of relevant within the judged pages, for Model-Anchor-Full based on different personalization approaches.</figDesc><table coords="8,113.44,315.97,382.85,64.68"><row><cell>Method</cell><cell>Personalization TBG</cell><cell>Judged (%)</cell><cell>Relevant Judged (%)</cell></row><row><cell></cell><cell>Pos. Neg. Neu.</cell><cell cols="2">Top-5 Top-10 Top-20 Top-5 Top-10 Top-20</cell></row><row><cell>Model-Anchor-Full</cell><cell>0.0004</cell><cell>00.73 00.36 00.19</cell><cell>27.27 27.27 27.27</cell></row><row><cell>Model-Anchor-Full</cell><cell cols="2">0.0016 00.20 00.10 00.08</cell><cell>33.33 33.33 40.00</cell></row><row><cell>Model-Anchor-Full</cell><cell>0.0000</cell><cell>00.26 00.20 00.14</cell><cell>25.00 16.66 25.00</cell></row><row><cell>Model-Anchor-Full</cell><cell>0.0000</cell><cell>00.00 00.00 00.00</cell><cell>00.00 00.00 00.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,53.80,396.48,502.12,109.26"><head>Table 7 :</head><label>7</label><figDesc>Impact of qrels expansion (mapping judged URLs to ClueWeb12 IDs) on fractions of judged pages, and fraction of relevant within the judged pages</figDesc><table coords="8,59.64,430.60,490.45,75.14"><row><cell>Method</cell><cell>Qrels</cell><cell>TBG</cell><cell>Judged (%)</cell><cell>Relevant Judged (%)</cell><cell></cell><cell cols="2"># Judged</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Top-5 Top-10 Top-20 Top-5 Top-10 Top-20 Top-5 Top-10 Top-20</cell></row><row><cell>Model-Text</cell><cell>(Official Qrels)</cell><cell>0.1969</cell><cell>100.00 54.12 30.43</cell><cell>08.17 08.98 10.42</cell><cell cols="3">1481 1581 1737</cell></row><row><cell>Model-Anchor</cell><cell>(Official Qrels)</cell><cell>0.3411</cell><cell>100.00 65.98 37.58</cell><cell>11.81 11.10 10.54</cell><cell cols="3">1481 1936 2162</cell></row><row><cell>Model-LC</cell><cell>(Official Qrels)</cell><cell cols="2">0.3780 100.00 76.10 43.80</cell><cell>14.79 13.90 12.78</cell><cell cols="3">1481 2233 2520</cell></row><row><cell cols="2">Model-Anchor-Full (Official Qrels)</cell><cell>0.0016</cell><cell>00.06 00.03 00.05</cell><cell>100.00 100.00 66.66</cell><cell>1</cell><cell>1</cell><cell>3</cell></row><row><cell cols="3">Model-Anchor-Full (Expanded Qrels) 0.0016</cell><cell>00.20 00.10 00.08</cell><cell>33.33 33.33 40.00</cell><cell>3</cell><cell>3</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,65.64,55.00,478.45,268.24"><head>Table 8 :</head><label>8</label><figDesc>Effectiveness of baseline (Terrier) and anchor-text in isolation and combination .5183 0.5313 0.5313 0.5697 0.6109 0.4871 0.4931 0.2043 0.4331 0.4142 0.4131 Model-Anchor 0.4883 0.5075 0.5152 0.5241 0.5643 0.5909 0.4725 0.4893 0.0694 0.3495 0.2955 0.2356 Model-LC 0.5263 0.5418 0.5524 0.5623 0.5919 0.6270 0.5139 0.5316 0.0862 0.3737 0.3104 0.2775</figDesc><table coords="11,65.64,78.66,471.92,33.29"><row><cell>Method</cell><cell cols="2">ERR-IA</cell><cell></cell><cell></cell><cell>nDCG</cell><cell>NRBP nNRBP MAP-IA</cell><cell></cell><cell>P-IA</cell></row><row><cell></cell><cell>@5</cell><cell>@10</cell><cell>@20</cell><cell>@5</cell><cell>@10</cell><cell>@20</cell><cell>@5</cell><cell>@10</cell><cell>@20</cell></row><row><cell>baseline</cell><cell>0.5012 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,321.42,711.19,231.24,7.86"><p>http://www.lemurproject.org/ClueWeb12/PageRank.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research is funded in part by the <rs type="funder">European Community</rs>'s <rs type="programName">FP7</rs> (project meSch, grant # <rs type="grantNumber">600851</rs>) and the <rs type="funder">Netherlands Organization for Scientific Research</rs> (<rs type="projectName">WebART</rs> project, NWO CATCH # <rs type="grantNumber">640.005.001</rs>; <rs type="projectName">ExPoSe</rs> project, NWO CI # <rs type="grantNumber">314.99.108</rs>; <rs type="projectName">DiLiPaD</rs> project, NWO Digging into Data # <rs type="grantNumber">600.006.014</rs>). The authors thank <rs type="person">Thaer Samar</rs> for helping in extracting title and description of suggestions, preparing text content of aggregators sub collection, and mapping open web qrels to ClueWeb12 ids.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jJJSyGB">
					<idno type="grant-number">600851</idno>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
				<org type="funded-project" xml:id="_hqVsAQm">
					<idno type="grant-number">640.005.001</idno>
					<orgName type="project" subtype="full">WebART</orgName>
				</org>
				<org type="funded-project" xml:id="_YyYryjQ">
					<idno type="grant-number">314.99.108</idno>
					<orgName type="project" subtype="full">ExPoSe</orgName>
				</org>
				<org type="funded-project" xml:id="_ZdYnrP9">
					<idno type="grant-number">600.006.014</idno>
					<orgName type="project" subtype="full">DiLiPaD</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,72.59,223.02,220.32,7.86;12,72.59,233.48,190.61,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,190.55,223.02,102.36,7.86;12,72.59,233.48,79.52,7.86">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,171.60,233.48,35.19,7.86">CIKM&apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,243.94,220.31,7.86;12,72.59,254.40,220.32,7.86;12,72.59,264.86,220.32,7.86;12,72.59,275.32,45.38,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,177.59,254.40,115.32,7.86;12,72.59,264.86,110.32,7.86">Overview of the TREC 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,205.42,264.86,87.49,7.86;12,72.59,275.32,17.08,7.86">Proceedings of TREC 2013</title>
		<meeting>TREC 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,285.78,220.31,7.86;12,72.59,296.24,220.32,7.86;12,72.59,306.70,188.53,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,258.24,285.78,34.66,7.86;12,72.59,296.24,166.37,7.86">Effective site finding using link anchor information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.74,296.24,33.17,7.86;12,72.59,306.70,70.53,7.86">Proceedings of SIGIR &apos;01</title>
		<meeting>SIGIR &apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,317.16,220.31,7.86;12,72.59,327.62,220.32,7.86;12,72.59,338.08,220.31,7.86;12,72.59,348.55,20.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,145.48,327.62,147.42,7.86;12,72.59,338.08,84.41,7.86">Overview of the TREC 2012 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,179.84,338.08,108.80,7.86">Proceedings of TREC 2012</title>
		<meeting>TREC 2012</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,359.01,220.31,7.86;12,72.59,369.47,220.31,7.86;12,72.59,379.93,220.32,7.86;12,72.59,390.39,74.78,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,197.43,369.47,95.47,7.86;12,72.59,379.93,135.56,7.86">Overview of the TREC 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,234.62,379.93,58.30,7.86;12,72.59,390.39,46.48,7.86">Proceedings of TREC 2013</title>
		<meeting>TREC 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,400.85,220.32,7.86;12,72.59,411.31,220.31,7.86;12,72.59,421.77,96.10,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,120.35,411.31,132.76,7.86">Evaluating contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dean-Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.27,411.31,17.64,7.86;12,72.59,421.77,66.87,7.86">Proceedings of EVIA</title>
		<meeting>EVIA</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,432.23,220.31,7.86;12,72.59,442.69,220.32,7.86;12,72.59,453.15,40.38,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,198.79,432.23,94.11,7.86;12,72.59,442.69,54.95,7.86">Analysis of anchor text for web search</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Eiron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Mccurley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,145.27,442.69,98.57,7.86">Proceedings of SIGIR &apos;03</title>
		<meeting>SIGIR &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="459" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,463.61,220.31,7.86;12,72.59,474.07,220.32,7.86;12,72.59,484.54,220.32,7.86;12,72.59,495.00,89.04,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,194.41,463.61,98.49,7.86;12,72.59,474.07,220.32,7.86;12,72.59,484.54,39.29,7.86">Mapreduce for information retrieval evaluation: &quot;let&apos;s quickly test this on 12 tb of data</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,137.62,484.54,95.40,7.86">Proceedings of CLEF&apos;10</title>
		<meeting>CLEF&apos;10</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,505.46,220.32,7.86;12,72.59,515.92,168.62,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,115.58,505.46,113.73,7.86">Web-centric language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,247.05,505.46,45.86,7.86;12,72.59,515.92,50.62,7.86">Proceedings of CIKM &apos;05</title>
		<meeting>CIKM &apos;05</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="307" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,526.38,220.31,7.86;12,72.59,536.84,220.32,7.86;12,72.59,547.30,129.08,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,186.15,526.38,106.75,7.86;12,72.59,536.84,119.31,7.86">The importance of anchor text for ad hoc search revisited</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,208.62,536.84,84.29,7.86;12,72.59,547.30,11.08,7.86">Proceedings of SIGIR &apos;10</title>
		<meeting>SIGIR &apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,557.76,220.32,7.86;12,72.59,568.22,220.32,7.86;12,72.59,578.68,194.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,115.31,568.22,157.65,7.86">Terrier Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johnson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,72.59,578.68,103.39,7.86">Proceedings of ECIR 2005</title>
		<meeting>ECIR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,72.59,589.14,220.31,7.86;12,72.59,599.60,220.31,7.86;12,72.59,610.07,100.23,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,169.64,589.14,123.27,7.86;12,72.59,599.60,216.75,7.86">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,97.31,610.07,19.93,7.86">TOIS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
