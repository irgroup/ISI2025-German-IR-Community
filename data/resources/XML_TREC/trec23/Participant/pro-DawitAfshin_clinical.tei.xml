<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.28,86.27,349.50,11.52">Query Expansion Using SNOMED-CT and Weighing Schemes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.20,114.77,97.04,15.32"><forename type="first">Dawit</forename><surname>Girmay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.76,114.77,103.78,15.32"><forename type="first">Afshin</forename><surname>Deroie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.28,86.27,349.50,11.52">Query Expansion Using SNOMED-CT and Weighing Schemes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BF69965D407D53D7634BB395EBE134C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Query Expansion</term>
					<term>Ontology</term>
					<term>relevance score Weighing Methods</term>
					<term>SNOMED-CT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite all the advancements that have been made in the field of Information Retrieval, there are still so many challenges. These challenges are magnified when the information that is being retrieved is in a specialized domain such as healthcare. In order to tackle these challenges and encourage research in these domains, TREC (Text RETrival Conference) has instituted a Clinical Track in 2014. This paper is the result of participation in 2014 TREC Clinical Track. It entails the approach and the results that were obtained by utilizing Ontology to expand the original topics. Ontology was used in order to improve the quality of the terms present in the queries or topics, so that the queries are better structured, and they can better target documents of interest. The value that each term brings to the result was measured by way of weighing method algorithms in the retrieval system, BM25 and InL2c1. For this research, we have used SNOMED-CT along with UMLS Methathesaurus as our ontology in medical domain to expand the queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and Motivation</head><p>The Clinical Track is instituted to encourage text retrievals in the area of diagnostics, treatment, and testing. The hope is that individuals in the healthcare field could leverage a retrieval model and make faster and sound decisions about the three areas of interest. As such, TREC has provided text documents from which participants need to use 30 topics and extract relevant documents. The documents provided are from Pub Med Central's database.</p><p>In the remainder of this paper, we discuss related works, give overview of the method of retrieval that we propose and instituted, discuss result of submission, and finally end with conclusion and future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are several methods to achieve retrieval of clinical documents of interest to users. However, most related researches available make use of query expansion, and therefore, that method was of interest to our team as well. Incidentally, we start the discussion regarding related work with publication that had to do with query expansion. Jun Miao et al. <ref type="bibr" coords="2,164.40,172.49,12.78,10.73" target="#b0">[1]</ref>, from York University describe both query expansion and enhancement of information retrieval model. The authors start with objective of retrieving patients from a collection that had/have a certain medical condition. BM25 was used to get baseline result. Then the performance of this model was compared with other models that are called York UMC2 which was semantically enriched methodology, York UMQ3 which combined Rocchio's feedback method with BM25's weighing method and York UMP4 which basically did the same thing as UMQ3, but added the idea of proximity of terms. The end result showed that the expansion of the queries that was augmented ended up outperforming the baseline. The authors finally recommend using the full capacity of the different Ontology that they used such as MeSH.</p><p>Martinez et al. <ref type="bibr" coords="2,162.60,341.09,14.07,10.73" target="#b1">[2]</ref> from University of Melbourne, Australia and University of the Basque country discuss their finding on query expansion using external sources headlined by Unified Medical Language System (UMLS), Wikipedia, and Dbpedia. The query was enriched with medical terms from these systems. At the same time, ICD descriptions which are publicly available were put in documents and run through Terrier to index them. Then they used 35 test queries from TREC 2011 to retrieve ICD codes. This was done to take advantage of the ICD codes present in the documents. Once the ranking was available from Terrier, they picked ICD codes deemed appropriate and retrieved documents. This method outperformed the method that uses external sources. In the end, authors conclude by suggesting that expanding query could also have negative impact on performance.</p><p>Koopman et al. <ref type="bibr" coords="2,166.56,525.65,14.07,10.73" target="#b2">[3]</ref> from Australian e-Health Research Centre also talk about query expansion by using Semantics. This was a very interesting approach in that they map both the query and the document using UMLS. Then, they map concepts from both the query and document to SNOMED-CT ontology. Once that was in place, they were able to match the queries to documents. By doing this, they were able to improve performance because with the SNOMED-CT ontology in place they were just not looking for key words, but concepts. This was a semantic approach. However, based on what they presented in the paper, it was not easy to tell how much advantage they take on the SNOMED-CT Ontology. It was simply stated that they are using it to map UMLS codes to SNOMED-CT without really delving too much into the relationships of concepts in the SNOMED-CT ontology.</p><p>Qi et al. <ref type="bibr" coords="3,129.49,74.81,12.85,10.73" target="#b3">[4]</ref> from NEC Labs America experiment with, expansion with UMLS concept. However, they suggested that the result was less than expected, and they went on with the submission only with the other methods by excluding UMLS expansion.</p><p>Henriksson et al. <ref type="bibr" coords="3,172.09,132.41,12.85,10.73" target="#b4">[5]</ref> in from Stockholm University and University of California, San Diego present an approach where they find new synonyms for preferred terms in SNOMED-CT using distributional similarity Methods. They use MIMIC-II database which is a large medical corpora in order to come up with the additional synonyms. Their objective was not information retrieval, but to identify synonyms of preferred terms. They were able to successfully do that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach for Retrieval</head><p>For the 2014 TREC clinical track, our research focuses on query expansion. The following are the details. The topics that have been provided are short clinical cases pertaining to three medical areas which are diagnosis, testing, and treatment. Then, document relevant to the case would need to be selected from PubMed Central (PMC) database file provided by TREC.</p><p>Keeping this in mind, the expansion intended in this research would use Metamap, UMLS Metathesaurus, and SNOMED-CT to find relevant documents pertaining to the query/topic.</p><p>The following are the proposed steps to achieve the retrieval set up.</p><p>1) The first step would be to map or enrich the textual query with Concept Unique Identifier(CUI) using Metamap tool for each of the thirty queries provided by TREC</p><p>2) From the Metamap results, terms associated with "Finding", "Disease or Syndrome", "Sign Symptom" would be the ideal candidates for expansion</p><p>3) Then UMLS concepts generated from Metamap would be mapped to SNOMED-CT concepts using mapping file/database created using file provided by UMLS Metathesaurus 4) In the next step, SNOMED-CT concepts that are deemed appropriate, synonyms, based on relationships in the SNOMED-CT ontology, are retained in the query. These would be concepts that are plain English at this point.</p><p>5) Information retrieval system, Terrier from University of Glasgow School of Computing Science <ref type="bibr" coords="3,225.84,667.01,12.87,10.73" target="#b5">[6]</ref>, would be used to do indexing and information retrieval using InL2C1 and BM25 weighing model</p><p>The following Diagram is the proposed architecture showing the interaction between the different systems involved in the retrieval process.</p><p>Diagram 1 -This is a depiction of all the nodes involved for the retrieval process</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval Process</head><p>In this section, a high level description of the retrieval process would be given. The objective of our retrieval process is to get clinical terms that would help us retrieve accurate and relevant documents. As such, we take a single query provided by TREC and get all possible terms that could be used to expand it. The query is then interjected with the new terms one at a time and the relevance score observed for movement upward or downward. Given the newly interjected term improves performance of the query, (upward trend), the new term is retained as part of the query. If not, we move to the next possible term in the queue and discard the term that did not help with performance.</p><p>The extent to which the newly interjected term contributes positively or negatively to performance is measured by looking at the relevance weight score of the first document assigned by the weighing scheme after the interjection of each expansion term.</p><p>The following section is a graphical representation of how the expansion process works given query one from TREC 2014 and its sub queries. The sub queries are generated when a possible expansion term has been identified that could be interjected into the original query. The assumption, in this case, is that there are nine potential expansion terms as represented by the sub queries. The weight is relevance scores assigned to the first documents retrieved by any of the weighing schemes we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagram 2 -Query Expansion values for a single query</head><p>The x-axis represents the queries and y-axis, the retrieval score. The baseline is query #1.0 with score of 20.1. At the start, any term that would lower the score below that is discarded as we are looking to increase the score. Therefore, expansion terms that were in queries #1.1, #1.2, #1.5, #1.9 would be part of the final expanded query while the rest would not be. Note that query 1.6 would be discarded as its result would lower the score from query 1.5. To that end, it could be said that the cumulative score of all expansion terms at a point and time is what we focused on. Therefore, it's a moving baseline that starts from 20.1 and goes to 24 for query 1.9. Hence, given the above example, query 1.9 would be the final query as it would already be inclusive of all the prior expansion terms that improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval Types and Weighing Methods</head><p>As briefly mentioned, Terrier Information Retrieval tool version 3.6 was used to retrieve the documents. Both the terrier default weighing method of InL2c1 and BM25 weighing methods were used. There were four types of retrievals that were done for our submission in 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InL2c1 and BM25</head><p>The summary queries that were provided by TREC were directly used to retrieve documents. For these two runs, there was no expansion of query involved. The summary part of the query was used to retrieve the documents, so these could be used as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InL2c1EXP and BM25EXP</head><p>For these runs, the queries were expanded using the SNOMED-CT ontology. The results from the previous two runs of InL2c1 and BM25 were used as baseline for the expanded queries. Terms that were already in the summary queries that were also returned from SNOMED-CT were discarded as they would just be redundancies. Here is how the expansion process is initiated for the first query.</p><p>Query #1 58-year-old woman with hypertension and obesity presents with exercise-related episodic chest pain radiating to the back The first three terms that were identified in SNOMED-CT were:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperpiesia</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypertensive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systemic arterial hypertension</head><p>Each one of the terms were then included with the original query as following.</p><p>Query#1.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;Summary&gt;58-year-old woman with hypertension and obesity presents with exerciserelated episodic chest pain radiating to the back&lt;/Summary&gt; &lt;Expanded&gt;&lt;/Expanded&gt;</head><p>Query #1.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;Summary&gt;58-year-old woman with hypertension and obesity presents with exerciserelated episodic chest pain radiating to the back&lt;/Summary&gt; &lt;Expanded&gt; Hyperpiesia&lt;/Expanded&gt;</head><p>Query #1.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;Summary&gt;58-year-old woman with hypertension and obesity presents with exerciserelated episodic chest pain radiating to the back&lt;/Summary&gt; &lt;Expanded&gt; Hyperpiesia, hypertensive&lt;/Expanded&gt;</head><p>Query #1.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;Summary&gt;58-year-old woman with hypertension and obesity presents with exerciserelated episodic chest pain radiating to the back&lt;/Summary&gt; &lt;Expanded&gt; Hyperpiesia, Systemic arterial hypertension&lt;/Expanded&gt;</head><p>Note that Query #1.0 is always the same as the summary which is the original query provided by TREC. The next two sub queries shown above are the original query along with the potential expansion terms generated from SNOMED-CT.</p><p>Using the above example, we can assume that the addition of term "Hyperpiesia" onto the query gave a relevance weight score that was higher than its corresponding baseline for the first returned document. Hence, it was retained as part of the query that would make sub query 1.3. While on the other hand, query #1.2 was discarded by the time we got to query #1.3 as the term "hypertensive" was not present.</p><p>For our TREC submission, for topic one alone, there were 22 possible terms that were identified for the first query alone, so this process was repeated 22 times for all the terms that were extracted from SNOMED-CT. For each of the queries, the impact of the interjection of each of the terms was assessed by looking at the first relevance weight score. At the end of this recursive process, we were able to identify candidates from SNOMED-CT that would improve relevance. They were all then included as part of the query that was inclusive of all expansion terms. This query was used to do the retrieval for both InL2c1EXP and BM25EXP runs. This process was repeated for all the thirty queries for all the possible SNOMED-CT terms that were yielded by the mapping database. The result of this process was submitted for TREC 2014 Clinical Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results of Retrieval</head><p>Below table shows the result of the four submissions done as evaluated by TREC. All of our runs were manual. The following results pertain to InFap, infNDCG, R-Prec, p@10 assessments.  Based, solely on InfAP, the performance of BM25Exp was the best of all the four. This is the query that had the original summary topics along with the expansion terms. The InL2c1 queries both the expanded and unexpanded versions performed lower than or same as the BM25 queries. This was a bit of a surprise to us, but not unexpected as we had anticipated InL2c1EXP would perform the best followed by BM25EXP. For InL2c1EXP, the interjection of expansion terms seems to have worsened InFap.</p><p>Based on infNDCG, both the expanded queries BM25EXP and InL2c1EXP outperformed the unexpanded queries. This was in line with what we expected, and the expansion seems to have contributed positively in this respect. BM25EXP was also the superior performer when it comes to R-prec while both InL2c1 methods performed poorly. Finally InL2c1EXP had the best performance when it comes to P@10. This is very significant because our expansion mainly focused on the document that was ranked the highest having an improved score as a result of additional terms we introduced. It has also been our observation that the improvement of the score of the first file meant improvement in the weight score of the few files ranked right after the first one. Therefore, it seems like the terms introduced improved the mean precision of the first ten documents. Moreover, it is our belief that the first few files are what most users would be interested in when looking at returned results, so this was a significant finding.</p><p>Below is a table showing the breakdown for all the 11 manual runs for best and median for all the 2014 clinical track submissions.   Judging by this, all of the runs that we did performed better than the median in terms of all metrics infAP, infNDCG, R-Prec, p@10 for both manual as well as automatic runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>The clinical track of TREC, aims to help healthcare professionals retrieve documents that are related to diagnostics, treatment, and testing. To this end, we have constructed a simple but effective model that lets users do exactly that by using SNOMED-CT, UMLS, Metamap and lets users add query expansion terms. Once, terms were added, impact of terms on performance was monitored by looking at trend of weighing schemes on the first ranked document returned. For the purpose of our participation in 2014 TREC, we have submitted four runs that were able to perform reasonably well.</p><p>As a recommendation to expand this research, we propose looking into the different types of topics diagnostics, treatment, and testing and seeing if the retrieval would work better for any particular genre. Secondly, although we noticed an improved score for the highest ranked document meant an improved score for the documents returned after the first one, we only looked at that there is an improvement on the highest ranked document's score as the main criteria. Perhaps, there could be a dynamic way where we can look at improvement of more documents rather than just the first one. This, in turn, could lead to overall improvement in terms of metrics used to measure retrieval precision. Last but not least, we believe using other well known or custom weighing methods could yield different results for the metrics. Therefore based, on the results we attained in our participation for 2014, we believe these ideas mentioned in the preceding lines deserve further look.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,616.91,288.77,34.31"><head>Table 1 -Relevance scores for added expansion terms</head><label>1</label><figDesc></figDesc><table coords="4,94.68,616.91,284.09,22.22"><row><cell>Query</cell><cell>1 1.1</cell><cell>1.2</cell><cell>1.3 1.4 1.5</cell><cell>1.6 1.7 1.8 1.9</cell></row><row><cell cols="5">Weight 20.1 22 22.5 19.8 18 23 22.6 16 17 24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,604.60,128.55,8.70"><head>Table 2 Trec Evaluation Results</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,90.00,363.88,431.99,104.01"><head>Table 3 TREC evaluation best and median results (Manual Runs)</head><label>3</label><figDesc>Below is a table showing the breakdown for all the 92 automatic runs for best and median for all the 2014 clinical track submissions.</figDesc><table coords="8,126.00,430.54,241.07,37.35"><row><cell>Run</cell><cell>infAP</cell><cell>infNDCG</cell><cell>R-prec</cell><cell>P@10</cell></row><row><cell>Best</cell><cell>0.1805</cell><cell>0.5197</cell><cell cols="2">0.3496 0.7100</cell></row><row><cell>Median</cell><cell>0.0316</cell><cell>0.1514</cell><cell cols="2">0.1257 0.2333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.00,471.40,275.88,8.70"><head>Table 4 TREC evaluation best and median results (Automatic Runs)</head><label>4</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.04,274.61,379.89,10.73;9,90.00,290.57,139.31,10.73" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,294.24,274.61,195.69,10.73;9,90.00,290.57,68.12,10.73">York University at TREC 2012: Medical Records Track</title>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</author>
		<idno>TREC 2012</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.16,316.37,409.79,10.73;9,90.00,332.21,139.31,10.73" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,343.94,316.37,173.01,10.73;9,90.00,332.21,68.12,10.73">NICTA and UBC at the TREC 2012 Medical Track</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno>TREC 2012</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.16,358.13,405.24,10.73;9,90.00,373.97,417.43,10.73;9,90.00,389.81,23.99,10.73" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,434.15,358.13,78.25,10.73;9,90.00,373.97,373.32,10.73">AEHRC &amp; QUT at TREC 2011 Medical Track: a concept-based information retrieval approach</title>
		<author>
			<persName coords=""><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurianne</forename><surname>Sitbon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Lawley</surname></persName>
		</author>
		<idno>TREC 2011</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.16,415.73,365.75,10.73;9,90.00,431.57,413.63,10.73" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,312.89,415.73,160.02,10.73;9,90.00,431.57,342.41,10.73">Retrieving Medical Records with &quot;sennamed&quot;:NEC Labs America at TREC 2012 Medical Records Track</title>
		<author>
			<persName coords=""><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-FranÂ¸cois</forename><surname>Laquerre</surname></persName>
		</author>
		<idno>TREC 2012</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.16,457.49,236.51,10.73;9,384.24,457.49,124.55,10.73;9,90.00,473.33,417.09,10.73;9,90.00,489.17,387.70,10.73;9,90.00,505.13,68.51,10.73" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,455.45,457.49,53.34,10.73;9,90.00,473.33,417.09,10.73;9,90.00,489.17,181.87,10.73">Identifying Synonymy between SNOMED Clinical Terms of Varying Length Using Distributional Analysis of Electronic Health Records</title>
		<author>
			<persName coords=""><forename type="first">Aron</forename><surname>Henriksson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Duneld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.36,489.17,192.34,10.73;9,90.00,505.13,36.33,10.73">AMIA Annual Symposium Proceedings Archive</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.16,530.93,379.43,10.73;9,90.00,546.77,395.13,10.73;9,90.00,562.73,405.11,10.73;9,90.00,578.57,336.97,10.73" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,174.24,546.77,310.89,10.73;9,90.00,562.73,39.99,10.73">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,150.84,562.73,344.27,10.73;9,90.00,578.57,43.89,10.73">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08-10">2006. 10th August, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
