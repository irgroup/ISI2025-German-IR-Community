<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.65,91.09,451.80,12.90">Modelling Psychological Needs for User-dependent Contextual Suggestion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,249.61,119.70,25.79,10.29"><forename type="first">Di</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,283.81,119.70,63.67,10.29"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.65,91.09,451.80,12.90">Modelling Psychological Needs for User-dependent Contextual Suggestion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DB71EAAEB7E7C2767EF01882ABA50102</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>information Extraction</term>
					<term>intelligent information systems</term>
					<term>support vector machines</term>
					<term>text mining</term>
					<term>machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our approach for the Contextual Suggestion track of 2014 Text REtrieval Conference (TREC). The task aims to provide recommendations on points of interests (POI) for various kinds of users under different contexts. This becomes challenging due to the limited amount of training data provided by TREC and the demanding constraints for a suggestion to be judged as relevant. Our approach does not deviate from existing Machine Learning based methods in principle, but sticks closely to the defined relevance judgement criteria, by focusing primarily on modelling users' preferences on POI categories, and investigating upon their psychological expectations on the textual descriptions of the POIs. The latter is considered as our novelty in this work. Support Vector Regression was used for suggestion ranking, an ad-hoc web information extractor was used to collect POI descriptions, and a description evaluation mechanism was engaged to select proper POI descriptions subject to the nature of the POIs. Our results suggest that our methods are effective in obtaining satisfying user-specific POI rankings and generating descriptions that meet users' psychological expectations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advancement of mobile and wireless communication technologies, the popularization of location based recommender systems (LBRSs) has taken place in the past couple of years. Yelp, Google Places, Foursquare, etc. are typical examples of LBRSs. These systems by far have been successful, and they do not require very sophisticated ranking mechanisms because simple features such as the number of good ratings on a particular point of interest (POI) are pretty strong indications of a good suggestion to most users, as the majority of users share the common patterns of interests. These facts have, however, also lead to the less efforts made on generating user-dependent LBRSs.</p><p>A user dependent LBRSs should not provide suggestions to a user without considering its prior interests on such kind of POIs, even though the suggestion may be well endorsed in a global sense. The preferences regarding entertainments may vary from different user groups. Different cultural background, social economic status, personalities, gender, ages, etc. all play an important role in shaping one's preference towards different POIs. For example, a female user is likely to prefer shopping as opposed to a male user.</p><p>The 2014 Text REtrieval Conference (TREC) Contextual Suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests, as suggested by the guidelines. In this task, a set of geographical regions are provided as contexts, along with a set of user profiles. Each user profile consists of rankings of sets of provided POIs in several contexts. The goal is to learn from the provided context-specific information for each user, and provide a ranked list of POIs for each user under a new context. In addition, the URL and a textual description of each recommended POI are needed along with the title of the POI.</p><p>The majority of the systems presented in the past collect POI information from well-known recommender services such as Yelp, Google Places, Trip Advisor, Foursqure, etc. Some systems exploit the open web to a greater extent by performing Information Extraction (IE) directly from hub pages that already contain a list of promising POIs that are publicly desired. In the work done by Luo &amp; Yang <ref type="bibr" coords="1,423.99,357.96,9.52,7.77" target="#b1">[1]</ref>, a Wiki Travel homepage for a target city is first located, and then POI names are extracted using a variety of heuristics. The collected POI names are then reinvestigated for more information such as URL, category, description and geographical location.</p><p>Many existing approaches adopt an Information Retrieval (IR) based framework for retrieving and ranking POI. For example, George et al. <ref type="bibr" coords="1,393.97,430.83,10.45,7.77" target="#b2">[2]</ref> crawled and indexed web pages of certain levels of interests, and in their work, user profiles are analyzed to generate queries that reflect users' preferences, and then structured queries are generated to address important fields such as title and anchor text.</p><p>In addition to traditional Rocchio-like query generation approaches, a more principled approach it to explicitly extract features from the POIs, user profile and context information. This allows the traditional Vector Space models to be used, and also motivates machine learning based methods such as ranking by Support Vector Machines (SVMRank). Yang &amp; Fang <ref type="bibr" coords="1,509.87,534.94,10.45,7.77" target="#b3">[3]</ref> used both positively and negatively rated POIs in the profiles to score new POIs based on their distances to the positive and negative POIs. Later, Jiang &amp; He <ref type="bibr" coords="1,408.36,566.18,10.45,7.77" target="#b4">[4]</ref> used linear regression to further include other features to generate a final ranking.</p><p>Unfortunately, only a limited amount of context-specific user profiles are provided by TREC, making it difficult to train more sophisticated ranking models. In fact, this makes practical sense because recommender systems in real life often face the same issue of insufficient training data to generate user-specific models. Still, it is noticeable that the user profiles provided this year intentionally span two different contexts: Chicago, Illinois and Santa Fe, New Mexico, which is an indication that the recommender system is expected to make context-dependent suggestions.</p><p>We have observed two major trends through the past works: (1) feature extractions and the use of machine learning algorithms are replacing traditional IR based methods; (2) more attentions are being given to finding POIs that are of particular interests to different user groups, rather than relying on a univer-sal background model. We will follow these trends and propose our machine learning based approach, which focuses on making reasonable and practical assumptions to model user-specific preference patterns and learning from users' general psychological expectations towards POI descriptions conditioned on their categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Formulation</head><p>The task in general is to provide a user with recommendations about the entertainments available under a particular context subject to the person's preferences. We are provided with a number of user profiles, and each user profile consists of ratings made by that user on different POIs under different contexts. In detail, each user profile has 50 rated POIs in Chicago, Illinois and 50 in Santa Fe, New Mexico. Our goal is to provide for each user 50 such suggestions for additional 50 contexts. Each suggestion consists of the title, description and URL of the POI. We formulate the task into the following subtasks:</p><p>Web crawling and information extraction is to collect POI information including title, description, URL and other features from the web. This is discussed in section 3.2 and 3.6.</p><p>Category-based user preference modelling is to learn the preference of each user towards each POI category from their profiles, as discussed in section 3.4. We use the mean and variance of the user's ratings on each particular POI category to model the user's preference.</p><p>User and context-specific POI ranking is to put together useful features introduced in section 3.3 for each usercategory-context triple. SVM based ranking mechanism is used for scoring and ranking. This will be discussed in detail in section 3.5</p><p>Category based description selection as described in section 3.7 is to find the best description text among those collected during the description crawling process described in section 3.6. Text mining techniques are attempted to collect features that reflects the nature of each chunk of description text. For each category, a Regression Tree trained on user profiles will be used to score the alternative descriptions and select the most promising one. We consider this to be a novelty in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Descriptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Important Assumptions</head><p>We assume (1) the preferences of the users remain the same as what they used to be at the time when their profiles were generated. This assumption should hold as introduced in the TREC 2014 Contextual Suggestion Guidelines that during the evaluation the same users who created the training user profiles will be summoned to rate suggested the POIs in different contexts. We assume that (2) most of the time users directly infer the category from the title, and (3) sometimes from both the title and a brief glance of the description if the title appears ambiguous, and (4) if they have to look into the description, it triggers a two-stage process where in the first stage the category is inferred, then in the second stage reflections and judgements are made. Further, we assume (5) a user will not actively pursue or positively rate a POI unless its category is of interests.</p><p>The above assumptions make practical sense because in real life most POI titles contain keywords that allow users to directly infer their categories. Should the title be ambiguous, a quick glance at some keywords in the description will help them identify the category. For instance, if a person does not recognize the POI title "Carnegie Mellon", a glance at the first sentence of the description will help him target keywords such as "University", thus he immediately refers the category as school without looking further. As for whether the description text is interesting by itself, he may look further down the description to spot information such as histories and specialties of the university. According to Cognitive Psychology <ref type="bibr" coords="2,447.99,171.82,9.52,7.77" target="#b5">[5]</ref>, peoples' judgements are biased by their preference on the categories, such as prior knowledge, past experience, personal attitudes and many other subjective views, leading them to rate a POI by comparing it with their mental prototypes or relating it to past experiences. For example, mercenary people are unlikely to be interested in luxury stores where price tags contradicts to their money saving principles; males are less likely to be interested in clothing stores since wandering among aisles of clothes is simply boring to them; and people studying sciences are less interested in art or history museums due to knowledge limitations. Notice that these are broad generalizations that are true for the majority of people, and my not hold for particular exceptions. But such generalizations are important as they provide prior knowledge that is useful with the absence of other evidences, which is typical in our case and many other real-life applications.</p><p>Additionally, under the context of recommender systems where dozens of alternatives are available, if the category of a POI is deemed not interesting, users are not likely to bother wasting time looking in detail, but to resort to other alternatives of interested categories.</p><p>These are fundamental modelling assumptions, as our POI ranking model does not consider features from the description text or the website, because we regard being interested in its category the prior condition for a user to be interested in a POI. Apart from that, our description selection model assumes that (6) if user is interested in a POI based on its category inferred from its title or a glance at its description, his expectations on POI descriptions are guided by the category. We will elaborate more in section 3.7.</p><p>In terms of geographical relevance, as introduced in the guidelines, any POI with 5 hours driving distance from the provided coordinate is considered appropriate. Therefore we will not make discriminations on POIs that are within the acceptable distance to the provided coordinates of each context, based on their geographical proximities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">POI Collection</head><p>We used Google Places API primarily to collect POI titles and other information. We decided not to rely on other POI search engines such as Yelp or Trip Advisor because we focused on POI category analysis, which requires a consistent labelling mechanism. Yelp provides two-level categorical information for most POIs, while Google Places provides significantly fewer number of categories. We chose Google Places because our training data is very limited and fewer numbers of categories suggests less training data needed to train the model. This step is crucial to the recall performance of the system, and beyond this stage no extra POIs will be considered. In particular, Nearby Search and Text Search APIs are used to obtain the titles and references of the POIs.</p><p>To perform Nearby Search, one needs to define the POI categories to be searched over. We have defined a list of POI categories used by Google Places API, including amusement park, aquarium, art gallery, bakery, bar, book store, bowling alley, cafe, casino, church, city hall, department store, food, library, mosque, movie theater, museum, neighborhood, night club, park, place of worship, restaurant, RV park, shopping mall, stadium, synagogue, university and zoo. The search radius is set to 320,000 meters which is calculated by assuming an average driving speed of 40 mph in 5 hours.</p><p>To perform Text Search, we enumerated based on common sense a list of general queries from the composition {"popular", "interesting", "amazing"} × {"attractions", "entertainments"}, which motivates the search engine to return well known POIs that are within the search radius. Moreover, we have analyzed the Wiki Travel Pages for all the contexts and automatically extracted name entities in bold face that are likely to be POI titles. These titles are then searched over through the Google Places Text Search API to obtain more POIs in each context.</p><p>The results obtained from both Nearby Search and Text Search are merged to produce a final pool of POI references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">POI Feature Extraction</head><p>The collected POI references are stable IDs used to obtain detailed information of the POIs via the Place Details API. The API provides a certain amount of information regarding the POI, including title, address, categories, user ratings, etc. In order to support user preference modelling and ranking, features need to be developed based on those information. In particular, we have considered the following features for each POI.</p><p>• the number of user ratings warped by a power function</p><p>• the first moment (mean) of user ratings</p><p>• the second moment (variance) of user ratings</p><p>• the number of uploaded photos</p><p>• the number of associated categories</p><p>• the core category based on inverted document frequency (idf) heuristics These features will be used during the POI ranking along with features extracted for a particular user and context.</p><p>We chose to warp the number of user ratings by a power function because in the places that are less visited by tourists, such Santa Fe, New Mexico, restaurants often receives higher numbers of ratings than the actual tourist destinations. This is because most restaurants constantly receives comments from local residents, and since Santa Fe is not a popular tourist destination comparing to many others, its true tourist destinations may receive fewer comments comparing to local restaurants. To correct this problem and let the restaurants to be less dominating due to their higher number of received user comments, a power function is used to warp this number. We have set the warping factor to be 0.3 for contexts that are similar to Santa Fe.</p><p>Idf based heuristics are used to select the core category for those POIs that have multiple categories on record by Google. For example, a restaurant may be co-labelled as cafe and establishment. This is because a cafe is a restaurant in most cases and almost all POIs that profit from customers are establishments. To infer the most representative functionality of a POI, using idf is a reasonable solution because the more frequent a category is labelled, the more general this category may be, and vice versa. Here, the category "cafe" appears far less frequently than the other two labels on record. Therefore the core category of this POI is regarded as "cafe".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">User Preference Modelling</head><p>The concept of user preference is open-ended. Commonly it refers to a user's levels of interests towards different POI categories. But there is more than one data-driven way of estimating the preferences.</p><p>One approach will be consider the relative frequency of the number of good or poor ratings given by a user on a particular category with respect to the user's total number of good or poor ratings. But this method suffers from serious bias as the sampled data is seriously unbalanced regarding POI categories. Therefore the estimation obtained with this method will be significantly biased or even inconclusive for some categories.</p><p>Another approach will be considering the mean rating of the POIs of a particular category made by the same user. This approach becomes problematic if a user rates the POIs in an unstable way. For example, a user who is not interested in restaurants may rate all instances as neutral, while another user who specializes in eating may give high rating to some restaurants, but also give poor ratings to other restaurants. Therefore the mean rating may not be informative in these scenarios. To resolve such issues, higher order moments need to be used.</p><p>We chose to use the mean, the second and third momentums of ratings made by the target user on POIs of a particular category as features of his preference pattern towards that category. If a rated POI has multiple categories, its rating will be used not only when computing user preference statistics on its core category, but also for all other associated categories.</p><p>When it comes to pattern mining, a common approach is to perform clustering on the data points and learn models for each cluster. The intuition is that by merging patterns are similar to each other as one cluster, fewer models are needed to model the patterns, and thus more training data will be available for parameter estimations with lower variances. Motivated by this, we have also tried clustering users based on their preferences encoded by the above features. Unfortunately, we have observed no obvious formation of distinguishable clusters, as illustrated in figure <ref type="figure" coords="3,345.72,458.12,3.36,7.77" target="#fig_0">1</ref>. That is why we chose not to perform clustering on user profiles, although intuitively it sounds reasonable. Therefore, we will take statistics from each user profile and model the preference of the user independently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">User and Context Specific POI Ranking</head><p>So far, we have features for each POI and features that reflect a user's preference pattern on particular POI categories, in order to perform context and user dependent POI ranking, the only thing left is to design features to distinguish different contexts.</p><p>In the user profiles, unfortunately, only two different contexts are provided, therefore we could not conduct extensive studies or design sophisticated models on how the nature of the context affects user's preferences over various POIs. Intuitively, in metropolis such as New York, Los Angeles and Chicago, a person tends to visit man-made landscapes such as landmarks, museums, amusement parks and famous restaurants, while in smaller cities with relatively lower population, a person tends to visit some natural landscape such as national parks, natural reserves and resorts. Based on this weak assumption, we adopt a binary feature to simply distinguish small cities from metropolises.</p><p>Now that we have features for each POI, user and context, but to train a regression model, we need to determine from the user profiles the training label of each POI-user-context triple. That label should be a rating that best reflects the user's preference on the POI category. However, what we are provided with are user's ratings on the description and the website for each POI, which are indirect reflections of their preference on the categories.</p><p>Interestingly, we have observed that most of the time a user's rating on the description is no worse than his rating on the website, but sometimes the rating on the website may be higher. In order to determine a rating that accurately reflects one's general preference towards POI categories, we will pick the maximum value of the two ratings as a best guess. Based on the assumptions that when a user gives lower website rating than the description rating, it implies the user has a clear idea of the category of the POI, only the website does not look impressive to him; however, when the situation is reversed, it implies the user failed to infer the category from the description, possibly due to low quality gibberish texts, but managed to infer the category from the website and restore his interests.</p><p>With all the features and training labels mentioned above, we have constructed a ν-Support Vector Regression model <ref type="bibr" coords="4,273.92,489.35,10.45,7.77" target="#b6">[6]</ref> to model each user's interest in a particular POI under a particular context, knowing his profile. The underlying implementation is based on libSVM <ref type="bibr" coords="4,156.30,520.58,9.52,7.77">[7]</ref>, with minor modifications. The model will be evaluated by applying it directly to rank the POIs and computing the resulting precision at the top 20 ranked POIs. Therefore, our ranking system is technically precision-oriented.</p><p>The training has been conducted over the 299 user profiles regarding 100 POIs located in 2 different contexts. Degrees of interests are addressed by duplicating the instances where the user is "strongly interested" or "strongly disinterested". To avoid over-fitting as our data is not large enough, we have used linear kernel with leave-1-out cross validation, and selected a relatively large regularization coefficient for a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Description Crawling</head><p>Extracting description text from the web is ad-hoc by nature, because the homepages of the POIs are highly diversified in terms of structure and organization, and the occasionally we also need descriptions available from third-party sources such as Wikipedia, and customer reviews from Yelp. Therefore, one may find multiple chunks of descriptive text for the same POI, but address different aspects of it. Unfortunately, the qualities of the texts are not consistent, such that there is no unique source that consistently provides the best descriptive text of all POIs.</p><p>An ad-hoc and highly engineered homepage spider has been developed to extract textual information from the homepage of POIs. The program will identify chunks of text on a web page and also navigate to tabs that are labelled such as "about us", "history", "mission", etc. which are likely to contain descriptive text addressing the nature of the POI. On average, the spider finds about 3 chunks of text from a homepage. The number of chunks the spider crawls is affected by the design of the homepage, and typically the number of tabs available on the page. Apart from the homepage of a POI, Wikipedia also contains introductory information regarding some popular POIs. Google, Bing and Yahoo Local Search occasionally provides such introduction crawled from Wikipedia and customer reviews from Yelp and Google+ along with the normal search results on a particular POI. Such introductions and reviews are also collected by the web crawler. The average length of the crawled descriptions is close to 500 characters long. Now that for each POI we have a pool of descriptions that are of different styles and qualities. The motivation of collecting more than one chunks of descriptive text for a POI is to allow us to find more appropriate ones among others to ensure the quality of the description text, which is an important part of the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Category Based Description Selection</head><p>Recall that we have made the assumption that users' expectation of a description is affected by the categories that the POI belongs to. For example, given a museum or cultural district, one may expect narrative description texts that address the history of the place; and given a famous restaurant or shopping center, one may expect persuasive text that address how wonderful and cheap the foods or products are. On the other hand, the reverse may also be true. A user is more likely to be interested in a restaurant or shopping center if he is informed about how delicious the foods are or how cheap the products sale; and one may not be convinced if the description about a museum or cultural district lacks literal seriousness as they should have.</p><p>Furthermore, we have also formed groups of categories. This is because although different categories may lead to different user expectations on the description text, many of them do not differ much from each other. For example, the desired description texts for zoos and aquariums may not differ significantly, and similarly for museums and galleries. Therefore, one may consider amusement park, restaurant, shopping center, night club as one group, while museum, historical district, place of worship, library as another group, and park, zoo, aquarium, natural reserve as yet another group. We have manually distributed all existing POI categories into 3 groups. One group is about culture, history and art; one group is about money spending events; and the last one is about nature and wild life. The benefit of grouping categories is that it brings more training data for each group, because we have only a limited number of labelled category-description pairs for model training, and their corresponding POIs may not span all categories to be studied.</p><p>The labelled training data were generate by randomly selecting 150 POIs from all the top 50 POIs returned by the POI ranking system for all the contexts, and labelling their descriptions texts manually on a scale of -1 to 1, with -1 as unsatisfying, 0 as mediocre, and 1 as satisfying.</p><p>Text Mining techniques have been adopted in selecting features from the description texts that reflects their characteristics. In particular we have extracted some heuristic features, such as the number of exclamatory marks and question marks, the longest possible timespan indicated by four-digit numbers range from 1500 to 2014, the number of dollar signs, and the length of the description text. Moreover, we have also used the Stanford Parser <ref type="bibr" coords="5,114.37,119.76,10.45,7.77" target="#b8">[8]</ref> to inspect the grammatical correctness of the sentences in the text, and calculate the proportion of grammatically correct sentences. In addition, we have also used the Stanford Log-linear Part-Of Speech (POS) Tagger <ref type="bibr" coords="5,222.54,150.99,10.45,7.77" target="#b9">[9]</ref> to extract features, such as the proportion of nouns (NN+NNS), proper nouns (NNPS/NNP), cardinal numbers (CD), adjectives (JJ), comparative adjectives (JJR) and superlative adjectives (JJS), etc.. On top of that, we have also gathered all adjectives and adverbs and manually crafted a set of general commendatory terms such as "excellent", "awesome", "delicious", "exciting", and a set of general derogatory terms such as "awful", "bad", "horrible", "boring". Based on that we have also calculated the proportion of commendatory terms and derogatory terms in each description, during the process we also consider negations that may appear before commendatory and derogatory terms. If a commendatory term is preceded by a negation, we will count it as a half of a derogatory term.</p><p>Three Regression Trees were trained independently using Scikit-learn <ref type="bibr" coords="5,102.96,307.16,14.94,7.77" target="#b10">[10]</ref> with the manually labelled description texts mentioned above, one for each category group. A tree takes a description whose POI's category belongs to the tree's designated category group, and outputs a predicted level of satisfactoriness. For a particular POI, all of its collected description texts were run through the tree and ranked by their scores, and the top ranked description was selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Results and Analysis</head><p>Table <ref type="table" coords="5,83.47,408.90,4.48,7.77">1</ref> shows the evaluation results of our submission "dixlticmu" and the average of the medians of all submissions across 299 POI-user pairs. We have also performed statistical significance analysis using Paired t-Test. The results suggest that our user and context specific POI ranking system generally performs better than the average. In particular, our system is significantly better in terms of precision (prec@5) with strong statistical evidences, and Time-Based Gain (TBG) with some evidence, and is slightly better in terms of Mean Reciprocal Recall (MRR). This indicates that our system is able to return more POIs of interest, with proper description text and URL. This is expected as we trained our user-specific POI ranking SVM based on prec@5, which also explains why our MRR performance is not as good as precision, since reciprocal recall was not investigated or optimized in our system.</p><p>Unfortunately, at this point we do not have a direct feedback on how our POI description selection system works comparing to others. Yet based on our assumptions, we can still infer that the description selection system has managed to provide reasonably well description texts for the returned POIs. This is because if the top ranked POIs deviates from a user's preference, we assume the user will not bother giving the description text a high rate since the rating reflects his level of interests. Therefore if the performance of the user and context specific POI ranking system is horrible, we do not have evidence on the quality of the description texts. However, since our POI ranking model has shown to be working well, we therefore have some evidence to claim that the description selection system is working reasonably well. Of course, it is also possible that the POI ranking model works even better than it appeared to be, due to degradation caused by a poorer description selection system.</p><p>Motivated by the released evaluation results for each con-prec@5 MRR TBG avg. medians 0.3491 0.535 1.3685 dixlticmu 0.3906 0.5431 1.4828 Advantage +12% +1.5% +8.4% P-value 0.0011 0.3986 0.1254 Table <ref type="table" coords="5,364.95,139.67,3.49,7.77">1</ref>: Evaluation results released by TREC.</p><p>text, we have conducted a study to see if our recommender system performs better or worse for certain contexts than others in terms of prec@5. In particular, we have found that for our system performed more than 0.1 absolute better than the averaged median for Boise, Walla Walla, College Station, Bloomington, Portland, Redding, San Diego, Virginia Beach, Yuma, Clarksville, Buffalo, Sacramento, Anchorage, Honolulu and Lawton. And for Erie, Lancaster, Kalamazoo, Homosassa Springs, Toledo, Albuquerque and Kansas City, our system performed more than 0.1 absolute worse than median. The above locations have been labelled in figure <ref type="figure" coords="5,448.37,274.38,3.36,7.77">2</ref>.</p><p>Figure <ref type="figure" coords="5,338.16,400.64,3.49,7.77">2</ref>: The distribution of contexts (cities) for which our recommender system performs more than 0.1 better (green stars) or worse (red cubes) than averaged median in terms of prec@5.</p><p>The map was provided by Map data c 2014 Google.</p><p>Unfortunately, we cannot conclude from these observations that our system is better or worse on contexts in terms of the city scale, economic status, population density, climate, geology or even physical distance to the border. It is noticeable that most contexts that our system performed poorer are close to the Great Lakes region, but this information alone does not make much sense to us. Therefore, from this analysis we can claim with enough evidence that our system performs equally well for contexts of different scale, economic status, population density and geographical attributes, which may be considered as another merit of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we have summarized our recommender system for the TREC 2014 Contextual Suggestion task. Our system consists of a web information crawling and extraction module that prepare the resources for making suggestions, a SVM based POI ranking module, and a Regression Tree based POI description selection module. We have also made some practical assumptions that have made our models simpler and more efficient. Generally speaking, our approach focused on using Machine Learning approaches to model user's needs and expectations from a psychological perspective.</p><p>The evaluation results suggest that our precision oriented system is competent in terms of prec@5 and TBG. They also suggest that the user preference models were successful in capturing particular interests of most users, which also implies that the underlying features are useful. Although the results are not directly indicative of the performance of our category based description selection method, but at least we can tell that it is reasonably good, otherwise the results could have been much worse than the median. More importantly, the results give us some evidence to claim that our human Psychology motivated approach works well in general.</p><p>In future, we will focus more on improving MRR performance of our system, and also on engineering better features for user preference modelling and description selection. We will also focus on incorporating more POI information from multiple sources, and adopt a more fine-grained POI categorization schema for more accurate preference modelling. We are also interested in developing a recounting system that extracts information from the web that are align with users' preferences, and automatically generates description text accordingly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,312.72,701.10,226.77,7.77;3,312.72,711.51,163.71,7.77;3,312.72,510.08,226.78,176.49"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Principled Component (PC) Analysis on Category Preferences for the 299 provided user profiles</figDesc><graphic coords="3,312.72,510.08,226.78,176.49" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>We would like to thank <rs type="person">Guanzhong Xu</rs>, Master's student at <rs type="affiliation">Ohio State University</rs> for providing feedbacks during the early stage of this project. We would also like to especially thank <rs type="person">Chi Liu, PhD</rs> student at <rs type="institution">Carnegie Mellon University</rs> and <rs type="person">Liyang Yan</rs>, Master's student at <rs type="institution">New York University</rs>, for their assistance in annotating some of the crawled POI descriptions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,139.63,345.44,67.20,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,361.99,208.51,6.91;6,75.86,370.98,208.51,6.91;6,75.86,379.97,133.95,6.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,143.51,361.99,140.87,6.91;6,75.86,370.98,202.54,6.91">Boosting venue page rankings for contextual retrieval-georgetown at trec 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,153.70,379.97,32.82,6.91">Tech. Rep</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Georgetown University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,392.95,208.51,6.91;6,75.86,401.94,208.51,6.91;6,75.86,410.93,208.51,6.91;6,75.86,419.92,56.11,6.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,79.76,401.94,142.96,6.91">Duth at trec 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Drosatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stamatelatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">S</forename><surname>Efraimidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,230.64,401.94,53.73,6.91;6,75.86,410.93,205.24,6.91">Democritus University of Thrace and Athena Research and Innovation Center</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,432.90,208.51,6.91;6,75.86,441.89,208.51,6.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,147.50,432.90,136.87,6.91;6,75.86,441.89,67.32,6.91">An exploraton of ranking-based strategy for contextual suggestion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of Delaware, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,454.87,208.51,6.91;6,75.86,463.86,159.42,6.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,153.75,454.87,130.62,6.91;6,75.86,463.86,14.93,6.91">Pitt at trec 2013 contextual suggestion track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,476.70,208.51,7.05;6,75.86,485.69,117.12,7.05" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,128.17,476.70,156.20,6.86;6,75.86,485.69,28.38,6.86">Cognitive psychology: An overview for cognitive scientists</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,498.80,208.51,6.91;6,75.86,507.66,208.51,7.05;6,75.86,516.79,89.66,6.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,80.46,507.79,99.05,6.91">New support vector algorithms</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,189.28,507.66,63.38,6.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1245" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,529.76,208.51,6.91;6,75.86,538.62,208.51,7.05;6,75.86,547.61,128.17,7.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,168.46,529.76,115.91,6.91;6,75.86,538.75,28.06,6.91">Libsvm: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,114.08,538.62,170.29,6.86;6,75.86,547.61,43.15,6.86">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,560.72,208.51,6.91;6,75.86,569.58,208.51,7.05;6,75.86,578.57,92.35,7.05" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,244.09,560.72,40.28,6.91;6,75.86,569.71,100.39,6.91">Parsing with compositional vector grammars</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,205.64,569.58,78.73,6.86;6,75.86,578.57,33.33,6.86">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,591.68,208.51,6.91;6,75.86,600.67,208.51,6.91;6,75.86,609.53,208.51,6.86;6,75.86,618.52,208.51,6.86;6,75.86,627.51,208.51,7.05;6,75.86,636.64,101.18,6.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,257.62,591.68,26.75,6.91;6,75.86,600.67,193.59,6.91">Featurerich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,75.86,609.53,208.51,6.86;6,75.86,618.52,208.51,6.86;6,75.86,627.51,70.24,6.86">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.86,649.61,208.51,6.91;6,75.86,658.47,208.51,7.05;6,75.86,667.46,208.51,7.05;6,75.86,676.45,179.29,7.05" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,78.93,667.59,131.99,6.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,220.46,667.46,63.91,6.86;6,75.86,676.45,78.12,6.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
