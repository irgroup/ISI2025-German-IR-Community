<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.03,84.43,433.66,14.93;1,198.48,106.35,212.76,14.93">Learning to Combine Collection-centric and Document-centric Models for Resource Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,267.17,138.67,75.38,10.37"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>krisztian.balog@uis.no</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Stavanger</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.03,84.43,433.66,14.93;1,198.48,106.35,212.76,14.93">Learning to Combine Collection-centric and Document-centric Models for Resource Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">185DF3A45B5E00BF32B1F1901F3EDCD9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the Federated Web Search track at TREC 2014. Our main focus is on the resource selection task, where we employ a learning-to-rank approach to combine various (instantiations of) resource ranking models. Further, we show that vertical selection can be run on the output from resource selection, and that it directly benefits from the improvements of thereof.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We describe our participation in the Federated Web Search track at TREC 2014. Specifically, we took part in the resource selection and vertical selection tasks. For resource selection, our focus was on finding a way to effectively combine two principal strategies, Collection-centric (CC) and Document-centric (DC), we developed in prior work <ref type="bibr" coords="1,275.20,423.00,13.28,8.64;1,53.80,434.96,39.68,8.64" target="#b0">(Balog, 2014)</ref>. We employ a learning-to-rank approach, where various instantiations of the CC and DC models, using different representations and relevance cutoff values, are used as features. We present our approach and results in Section 2. We base our vertical selection runs on the outcomes of resource selection step. Specifically, we use the estimated collection relevance scores as binary judgments, thereby essentially delegating the "selection" problem to the resource ranking component. The method and the results are described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Resource selection</head><p>In prior work, we presented two approaches to the resource selection task based on generative language modeling techniques <ref type="bibr" coords="1,83.34,627.14,54.58,8.64" target="#b0">(Balog, 2014)</ref>. According to the Collection-centric (CC) model, each collection is represented as a term distribution, which is estimated from all sampled documents. The second model, Document-centric (DC), first scores individual sampled documents, then considers the top-K ranked ones to determine collection relevance. Despite its relative simplicity, the DC model delivers solid performance; at TREC 2013 it came very close to the top performing runs on all metrics <ref type="bibr" coords="1,362.11,206.91,95.74,8.64" target="#b2">(Demeester et al., 2014)</ref>. We also experimented with the combination of the CC and DC strategies in our participation last year, using a linear mixture model, but it did not improve over the DC model. This year our aim is to find a way to effectively combine the CC and DC models. To this end, we employ learning-to-rank techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach</head><p>We use the scores estimated by the CC and DC models as features. Specifically, we consider a number of different configurations, based on the type of document representation (title, snippet, page) and the cutoff value (K, only for the DC model). In the following subsections, we briefly present the CC and DC models; for a more detailed description we refer to <ref type="bibr" coords="1,349.14,384.96,51.58,8.64" target="#b0">Balog (2014)</ref>. Additionally, we take collection size to be a feature as well (previously, it was incorporated as a prior collection probability). Table <ref type="table" coords="1,457.11,408.87,4.98,8.64" target="#tab_0">1</ref> lists our features (36 in total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Collection-centric Model</head><p>Drawing on <ref type="bibr" coords="1,368.44,465.37,79.88,8.64" target="#b1">Callan et al. (1995)</ref> and <ref type="bibr" coords="1,473.14,465.37,60.51,8.64" target="#b4">Si et al. (2002)</ref>, this approach treats each collection as a single, large document. Under the language modeling framework, the probability of the collection generating the query is expressed as follows:</p><formula xml:id="formula_0" coords="1,324.74,518.75,231.18,36.61">P(q|c) = ∏ t∈q (1 -λ) ∑ d∈c P(t|d)P(d|c) + λP(t) n(t,q) , (1)</formula><p>where n(t, q) is the number of times term t is present in the query q, P(t|d) and P(t) are maximum-likelihood estimates of the probability of observing term t given the document and background language models, respectively, and λ is a smoothing parameter. The background language model is estimated form all sampled documents. Here, all documents are assumed to be equally important within a given collection, therefore, P(d|c) is set to 1/|c|, where |c| is the number of (sampled) documents in collection c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Document-centric Model</head><p>Instead of creating a direct term-based representation of collections, we model and query individual (sampled) docu-  <ref type="bibr" coords="2,87.84,269.99,83.43,8.64" target="#b3">(Si and Callan, 2003)</ref>. Formally:</p><formula xml:id="formula_1" coords="2,62.55,290.43,230.35,22.39">P(q|c) = ∑ d∈c P(d|c) ∏ t∈q (1 -λ)P(t|d) + λP(t) n(t,q) ,<label>(2)</label></formula><p>where, as before, P(t|d) and P(t) and the document and background term probabilities, λ is the smoothing parameter, and P(d|c) is the importance of the document given the collection. Additionally, we apply a rank-based cut-off and consider only the top K most relevant documents in the sample index for the computation of Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Combining Models</head><p>We employ a listwise learning-to-rank approach, Lamb-daMART <ref type="bibr" coords="2,94.09,442.11,66.14,8.64" target="#b5">(Wu et al., 2010)</ref>. For training the machine learning model we use data from prior editions of the TREC Fed-Web track. Our results in §2.2 indicate that the choice of the training material has a major impact on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Runs and results</head><p>We submitted the following runs:</p><p>NTNUiSrs1 Document-centric model using the entire document text (r = document) and a cutoff value of K = 500. This particular setting was chosen based on a (non-extensive) set of experiments performed on the FedWeb'13 collection. NTNUiSrs2 Learning-to-rank approach trained on the FedWeb'13 data set. NTNUiSrs3 Learning-to-rank approach trained on the FedWeb'12 and '13 data sets.</p><p>Table <ref type="table" coords="2,77.64,663.00,4.98,8.64" target="#tab_1">2</ref> presents the results. We find that the learning-to-rank approach trained on FedWeb'13 outperforms the DC model by over 13% in terms of the official metric, nDCG@20 (NTNUiSrs2 vs. NTNUiSrs1). Interestingly, when training was done on both FedWeb'12 and '13 performance dropped 3 Vertical selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>Our choice of method for the vertical selection task is closely tied to our resource selection approach. We assume that resource selection produces a relevance score s(q, c) for each collection such that</p><formula xml:id="formula_2" coords="2,354.82,349.11,201.09,20.91">s(q, c) = &gt; 0 c is relevant ≤ 0 c is nonrelevant<label>(3)</label></formula><p>Then, we simply select all collections that have a positive relevance score:</p><formula xml:id="formula_3" coords="2,388.48,417.95,167.44,8.96">V (q) = {c|s(q, c) &gt; 0},<label>(4)</label></formula><p>where V (q) denotes the set of selected verticals for query q.</p><p>In a way, we delegate the "selection" problem to the resource ranking component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Runs and results</head><p>We submitted the following runs:</p><p>NTNUiSvs2 Based on resource selection run NTNUiSrs2. NTNUiSvs3 Based on resource selection run NTNUiSrs3.</p><p>Table <ref type="table" coords="2,342.34,567.36,4.98,8.64" target="#tab_2">3</ref> displays precision (P), recall (R), and F1-measure (F1) for our submitted runs. Based on these results, we make the not surprising observation that better resource selection indeed leads to better vertical selection. The scores, however, are quite low in absolute terms, which suggests that the scores produced by the resource selection approach may not satisfy the criteria that we have specified regarding the signs of collection scores (cf. Eq. 3). We hypothesize that using a simple score-based thresholding (i.e., changing the value 0 to a parameter in Eq. 3) might alleviate this issue. It might also be the case that the underlying resource selection step needs to be casted as a classification task as opposed to a ranking problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We described our participation in the TREC 2014 Federated Web Search track. For resource selection we have experimented with a discriminative learning approach for combining numerous instantiations of resource selection models. We have shown that it can outperform a competitive baseline model, but is sensitive to the choice of the underlying training material. We have used the estimated collection relevance scores, as binary judgments, to make a selection of verticals. We have found that improvements in resource selection indeed translate to better vertical selection performance. At the same time, making a binary judgement about the relevance of a collection remains to be challenging, given that resource selection is approached as a ranking problem, and not as a classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.80,64.07,242.14,214.56"><head>Table 1 :</head><label>1</label><figDesc>List of features used for resource selection.</figDesc><table coords="2,53.80,77.74,242.14,200.89"><row><cell>Feature</cell><cell>Description</cell></row><row><cell>CC r (q, c)</cell><cell>P(q|c) estimated using the CC model (Eq. 1)</cell></row><row><cell></cell><cell>representations:</cell></row><row><cell></cell><cell>r = {title, snippet}</cell></row><row><cell cols="2">DC r,K (q, c) P(q|c) estimated using the DC model (Eq. 2)</cell></row><row><cell></cell><cell>representations:</cell></row><row><cell></cell><cell>r = {title, snippet, document}</cell></row><row><cell></cell><cell>cutoff values:</cell></row><row><cell></cell><cell>K = {10, 20, 50, 75, 100, 150, 200,</cell></row><row><cell></cell><cell>250, 300, 500, 1000}</cell></row><row><cell cols="2">snippets(c) Number of snippets in the sample of c</cell></row><row><cell cols="2">ments, then aggregate their relevance estimates. This ap-</cell></row><row><cell cols="2">proach closely resembles the ReDDE collection selection al-</cell></row><row><cell>gorithm</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.81,64.07,239.10,159.73"><head>Table 2 :</head><label>2</label><figDesc>Results for our official resource selection runs. Best scores for each metric are in boldface.</figDesc><table coords="2,316.81,86.90,239.10,136.90"><row><cell>Run</cell><cell cols="3">nDCG@20 nDCG@10 P@1 P@5</cell></row><row><cell>NTNUiSrs1</cell><cell>0.306</cell><cell>0.225</cell><cell>0.148 0.195</cell></row><row><cell>NTNUiSrs2</cell><cell>0.348</cell><cell>0.281</cell><cell>0.206 0.257</cell></row><row><cell>NTNUiSrs3</cell><cell>0.248</cell><cell>0.205</cell><cell>0.202 0.189</cell></row><row><cell cols="4">substantially (NTNUiSrs3 vs. NTNUiSrs1). Discriminative</cell></row><row><cell cols="4">learning is indeed a promising direction for this task, but fur-</cell></row><row><cell cols="4">ther research is needed to understand how the training mate-</cell></row><row><cell cols="4">rial should be composed. It is also left to future work to ex-</cell></row><row><cell cols="4">periment with different learning-to-rank algorithms, specifi-</cell></row><row><cell cols="3">cally pointwise and pairwise approaches.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,53.80,64.07,239.10,55.78"><head>Table 3 :</head><label>3</label><figDesc>Results for our official resource selection runs. Best scores for each metric are in boldface.</figDesc><table coords="3,98.93,86.90,148.84,32.95"><row><cell>Run</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="4">NTNUiSvs2 0.157 0.406 0.205</cell></row><row><cell cols="4">NTNUiSvs3 0.145 0.281 0.177</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,53.80,376.66,239.10,8.64;3,63.76,388.44,229.14,8.82;3,63.76,400.39,194.27,8.59" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,129.32,376.66,163.58,8.64;3,63.76,388.62,99.44,8.64">Collection and document language models for resource selection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,183.36,388.44,109.54,8.59;3,63.76,400.39,135.88,8.59">Proceedings of the Twenty-Second Text REtrieval Conference</title>
		<meeting>the Twenty-Second Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2013">2014. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,53.80,420.50,239.10,8.64;3,63.76,432.27,229.14,8.82;3,63.76,444.23,143.32,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,253.06,420.50,39.84,8.64;3,63.76,432.45,188.68,8.64">Searching distributed collections with inference networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,275.09,432.27,17.81,8.59;3,63.76,444.23,83.74,8.59">Proceedings of SIGIR&apos;95</title>
		<meeting>SIGIR&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,53.80,464.33,239.10,8.64;3,63.76,476.29,229.14,8.64;3,63.76,488.07,229.14,8.82;3,63.76,500.02,144.48,8.59" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,113.10,476.29,179.81,8.64;3,63.76,488.24,47.58,8.64">Overview of the TREC 2013 federated web search track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,133.57,488.07,159.33,8.59;3,63.76,500.02,86.09,8.59">Proceedings of the Twenty-Second Text REtrieval Conference</title>
		<meeting>the Twenty-Second Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2013">2014. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,53.80,520.12,239.10,8.64;3,63.76,531.90,229.14,8.82;3,63.76,543.86,115.93,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,167.88,520.12,125.02,8.64;3,63.76,532.08,161.22,8.64">Relevant document distribution estimation method for resource selection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,243.55,531.90,49.35,8.59;3,63.76,543.86,46.39,8.59">Proceedings of SIGIR&apos;03</title>
		<meeting>SIGIR&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,53.80,563.96,239.10,8.64;3,63.76,575.92,229.14,8.64;3,63.76,587.69,229.14,8.82;3,63.76,599.83,17.43,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,266.38,563.96,26.52,8.64;3,63.76,575.92,229.14,8.64;3,63.76,587.87,52.70,8.64">A language modeling framework for resource selection and results merging</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,138.89,587.69,99.40,8.59">Proceedings of CIKM&apos;02</title>
		<meeting>CIKM&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,53.80,619.75,239.10,8.64;3,63.76,631.53,229.14,8.82;3,63.76,643.48,47.59,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,63.76,631.71,208.54,8.64">Adapting boosting for information retrieval measures</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,279.49,631.53,13.41,8.59;3,63.76,643.48,17.53,8.59">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
