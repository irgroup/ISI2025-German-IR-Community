<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.43,114.94,263.14,15.12">University of Delaware at TREC 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.65,148.84,55.70,10.48"><forename type="first">Ashraf</forename><surname>Bah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.23,148.84,113.70,10.48"><forename type="first">Karankumar</forename><surname>Sabhnani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.46,148.84,77.91,10.48"><forename type="first">Mustafa</forename><surname>Zengin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,429.43,148.84,76.57,10.48"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.43,114.94,263.14,15.12">University of Delaware at TREC 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CAA42E08047FD8045FA04E9A524B5F89</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the work of the Information Retrieval Lab at the University of Delaware (team name "udel") on TREC 2014 tracks. We participated in five different tracks: Contextual Suggestion, Federated Web, Microblog, Session, and Web.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Information Retrieval Lab at the University of Delaware participated in five different tracks at TREC 2014. This paper describes our work in those tracks. Sections are organized as follows (with primary author(s)/contact person):</p><p>Section 2 Contextual Suggestion track, by Karankumar Sabhnani. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contextual Suggestion Track</head><p>Our system tackles the contextual suggestion task in multiple phases. The first phase-the learning phase-is where we analyze user profiles and generate bags of keywords depicting the user interests. The second phase-retrieval-is where given a context, we query the Google Places API with each bag and retrieve lists containing places in that context which fit in the specified genre. We then aggregate all sets of results for a single user's profile into one set of 50 ranked results, which is the third and the final phase.</p><p>This approach, which we also used for last year's track <ref type="bibr" coords="1,353.74,568.19,10.91,9.57" target="#b0">[1]</ref>, worked very well, achieving highest performance for most of last year's user-context pairs and performance above the median for most. However there were a few pairs for which it generated poor suggestions. We did some analysis to investigate the reason behind this behavior and came up with some techniques that could boost our performance. This report summarizes, 1) the new components in our approach, 2) submitted runs, and 3) performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Keyword ranking and description generation</head><p>Our analysis indicates that the learning and retrieval phases work well. To improve the performance, we should optimize the order in which the top 50 places are picked. This tells us that the ordering of keywords within the bag of each profile can improve the results significantly.</p><p>Another thing that we didn't address in last year's runs was the generation of description for every suggestion. Our current approach incorporates both of the above-mentioned ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Keyword ranking</head><p>One way of ordering keywords within the bag is by assigning weights to each keyword while learning. The weight could be as simple as the number of times the user has liked that category. Ordering the keywords in descending order, i.e. the keyword with the highest weight first, will make sure that places from categories that a user prefers more will occur at higher ranks.</p><p>This approach didn't actually work as the data was biased towards particular categories. Out of the 50 training examples from 2013, there are around 10 museums, 5-7 landmarks, 4 theaters and a couple of breweries; because of this, the top 5 keywords for all the profiles turned out to be museums, landmark, theater, breweries/spas/tours/markets. This year's data has more examples and two different seed cities, so it is a little less biased, but still won't help much as only top 5 places are evaluated and the top 5 keywords for most of the profiles turn out to be the same.</p><p>As it was not feasible to rank all the keywords, we prioritized some of the common keywords such as restaurants, museum/landmark, desserts, etc. and moved them up in the list if they already exist in the bag. This will make sure that we don't present the users with bad categories at top ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Description generation</head><p>Descriptions play an important role as the user generally reads and rates them first before visiting the place website and are equally important as the website ratings. We generate descriptions for each place suggestion using categories, aspects, and user reviews (all returned by Google Places) of that place. Aspects include serivce, decor, food, etc.; we try and include aspects for which the place is recognized. Along with its best aspects, we also include one review and its overall Google rating, if available. All this information is retrieved using Google Places API and then clubbed to create sentences in two different ways. Here are two examples of actual descriptions that were generated and included in our runs: Sweet Beginnings Bakery, a bakery in Buffalo,NY is highly appreciated for its quality, appeal, and service with service being the best aspect. Here's one customer review: Everytime I buy something there it is the best quality I have ever seen and I have been to a lot of bakeries. The best in the Buffalo area by far.;. Bunk Morrison is one of the best sandwich places in Portland,OR. It is know for its food, service, and decor. In fact, its decor makes it work visiting. The overall google rating of this place is 4.3. Here's one customer review: Yes... word around Portland is true: Bunk Sandwiches are amazing!!! I have dreams at night, and cravings in the day, about the chicken salad sandwich. Also, it is entirely true that one breakfast sandwich can be THAT much better than the others... grand central bread, top qualit This process is fully automated and one of two ways to generate and club sentences is randomly selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Submitted runs</head><p>Our main idea is to maintain diversity among all the genres that suit the user's taste. We have submitted two runs based on this approach. For both the runs, we start by analyzing user profiles and generating bags of keywords depicting the user interests. Then, given a context, we query Google Places API with each bag, retrieving lists containing places in that context which fit in the specified genre. The API results are used for generating descriptions for each suggestion and top 50 suggestions for every profile-context pair are then used for creating our final run. The only difference in both the runs is that we prioritized keywords in each bag for the first run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation</head><p>Our first run (run FDWD) performs much better on average compared to all our runs submitted so far for this track at TREC. It has an average of 0.4201 for P@5, MRR of 0.5649 and TBG of 1.7589; it is as good as the best performance for some profile-context pairs and performs better than the median for the rest. These new components improved the performances by approximately 15%.</p><p>The second run (run DwD) has an average of 0.3097 for P@5, MRR of 0.3760 and TBG of 0.9532. It does not perform well and this clearly indicates that the ordering of keywords plays an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Federated Web Track</head><p>The Federated Web track this year included three tasks: vertical selection, resource selection, and results merging. We submitted runs for vertical selection (Section 3.2) and resource selection (Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Resource Selection</head><p>We submitted two runs (udelftrssn, udelftrsbs) to the resource selection task. For both of the runs we employed the same method on snippets and sampled pages, in order to rank resources according to their relevance to a given query. We created two central indexes; one for snippets and one for sampled pages. For each query, we ranked the documents according to their query-likelihood score and selected the top 100 documents for further processing. Then the resources were ranked according to the number of documents they contributed to the selected documents. Table <ref type="table" coords="3,503.32,477.73,5.45,9.57" target="#tab_0">1</ref> shows the performances of the same algorithm on snippets (udelftrssn) and on sampled pages (udelftrsbs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RunID</head><p>nDCG@10 nDCG@20 nP@1 nP@ Based on these results, ranking based on the full sampled pages is unequivocably better than just using snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vertical Selection</head><p>We submitted two runs to vertical selection task and in both runs we used the udelftrsbs resource selection run. Our baseline run, udelftvql ranked verticals according to the number of resources ranked for udelftrsbs.</p><p>For the udelftvqlR run, we applied certain rules to modify and re-rank udelftvql. We assigned two scores to verticals. The first score, s 1 , is the score that a vertical gets from the rules. The second score, s 2 , is the negative rank of the vertical in udelftvql. The rules are as follows:</p><p>• If a query starts with an interrogative word (e.g. who, what, where, why, when), Q&amp;A and General verticals should be added to the verticals list. Their s 1 scores were set to 100 and 80 respectively.</p><p>• If a query does not contain any interrogative word, Q&amp;A vertical should be removed from the verticals list.</p><p>• Vertical words ("video" for video vertical, "shopping" for shopping vertical, and so forth) should be searched for in a query. Vertical names that contain query terms should be added to the verticals list. Their s 1 scores should be set to 90.</p><p>After applying the rules, we sort the verticals according to s 1 and broke the ties with s 2 . If no rules apply, then we simply default to the original ranking. We then pick the top 5 verticals for each query. Here we see that using our rules improves precision by a substantial amount (over 40%), though hurts recall (by 20%). The result is a strong improvement in F1 of 27%.</p><p>Rules applied to 40 of the 50 queries (10 did not match any of the conditions). We analyzed how selecting only the top 5 verticals from udelftrsbs and using our rules affects the performance on these 40 queries. It can be seen that picking top 5 verticals for each query (the udelftvqlTOP5@40Q run) improves precision by 40%, F1 by 21%, and hurts recall by 30%. Picking top 5 vertical and applying rules (udelftvqlR@40Q) improves the performance further. Improvements in precision and F1 reach to 49% and 30% respectively. Both improvements are significant by a paired two-sided t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Microblog Track</head><p>Tweet Timeline Generation (TTG) is a new task for Microblog track at TREC 2014. The TTG task attracts us as it supplements the standard challenges of ad hoc retrieval with issues from topic detection and tracking (TDT), and multi-document summarization. Our system efficiently detects and eliminates redundant tweets and also maintains a good balance between the cluster precision and cluster recall. For every topic, we start by retrieving relevant tweets from the official search API. Then, a subset of these tweets is used for generating semantic clusters using the Quality Threshold (QT) algorithm. Once we have the clusters, we pick exactly one tweet from every cluster that represents it the best and use those to present the user with a list of chronologically ordered tweets that summarize their respective topic/event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extracting relevant tweets and selecting subsets for clustering</head><p>We use the official search API to retrieve top 2000 relevant tweets for every topic. Only a subset of these tweets is used for clustering. Scores and ranks provided by the search API are used to select tweets up to a certain score threshold subject to a fixed number of minimum (75) and maximum (300) tweets. Our goal with this threshold and cap on number of tweets considered for clustering is to help in ensuring a good precision, whereas having a constraint on the minimum number of tweets will hopefully prevent the recall from dropping. This process helps in maintaining a decent balance between the cluster precision and cluster recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Creating semantic clusters</head><p>The quality threshold algorithm with complete linkage distance is used for creating clusters <ref type="bibr" coords="5,525.46,324.16,10.91,9.57" target="#b1">[2]</ref>. Every subset is first sorted on the basis of the tweet time. The first cluster is built with the first tweet in the subset. As long as other tweets are close enough to be within the specified diameter, they are added to the cluster. Once all tweets have been read, the tweets that have been added to the cluster are set aside and the algorithm is repeated recursively on the rest of the tweet collection. The distance between a tweet and the cluster is computed using the complete linkage distance, i.e. the distance from the tweet and the furthest tweet in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Selecting cluster representatives and generating summary</head><p>The next step is to select exactly one tweet from each cluster that represents it the best and then use them to present the user with a list of chronologically ordered tweets that summarize their respective topic/event. One way is to pick the earliest single tweet in each cluster. Summaries generated with these tweets will truly represent the chronological order of events. Another way is to pick the single highest-scoring tweet from each cluster and then sort them on the basis of their tweet time. This will ensure that the summaries are generated with the most likely relevant tweets. For simplicity, in this year's TTG task, all tweets from a cluster are considered equivalent and thus this step won't really affect the performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Submitted runs</head><p>We have submitted 4 runs for the TTG task and 1 run for the ad hoc task. For the ad hoc task, we have filtered retweets and non-english tweets.</p><p>Our first two runs for the TTG task use our ad hoc run and the last two use the baseline run as the feed for clustering. The tweet collection for every topic is a subset of the tweets retrieved for the ad hoc or the Baseline run respectively. We set the minimum and maximum size of the tweet collections to be 75 and 300 respectively. The score threshold that we have used for all the TTG runs is determined by taking the score of the tweet at rank 15, then subtracting 3.5 from that (these values were picked manually with no training). The cluster representatives for the first and third run are the earliest tweets from every cluster, and the cluster representatives for the second and the fourth run are the highest-scoring ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation</head><p>Our ad hoc run (udelRunAH) had an average precision of 0.1841, precision at 30 of 0.5103, and R-precision of 0.2485. These are not particularly good (or bad).</p><p>The first four runs in Table <ref type="table" coords="6,223.52,384.88,5.45,9.57" target="#tab_5">4</ref> are the TTG runs that we submitted to TREC. Despite the fact that they use different inputs (our ad hoc run for TTG1.3 and TTG2.3 vs the official baseline for TTG3.3 and TTG4.3) and different approaches to drawing from clusters, they all have very similar (around average) performance. Their feeds are actually almost identical in nature and to keep the evaluations simple, the effect of cluster representatives is ignored.</p><p>One reason for low recall and precision values could be the feed itself. Both the baseline ad hoc run and our ad hoc run have below average precision and recall values, which indicates that the feed itself has many non-relevant tweets, which in turn results in unwanted clusters and low precision. Given a good feed, our system may yield clusters with much better precision and recall.</p><p>Another good aspect of our approach is that all our runs very well balance the ratio of cluster precision to cluster recall. To confirm this fact, we did some additional experiments with parameter values.</p><p>On increasing the number of maximum tweets considered for clustering to 600 (TTG1.6), recall increased but precision dropped. However, increasing the score bound by 1.5 (subtracting 2.0 rather than 3.5 from the 15th-ranked tweet's score) (TTG6.6) compensates for this loss of precision. Another parameter that can affect the recall is the similarity radius of the cluster. Reducing it to 0.2 (TTG9.6) boosts the recall to 0.4454.</p><p>Overall, the best performance seems to be achieved with a cluster radius of 0.2 and a score bound of 2 less than the 15th-ranked tweet, with the constraint that at most 600 tweets be taken for clustering. This indicates that these parameters indeed help in maintaining the ratio of recall to precision and given a good feed, our approach can create clusters with better precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Session and Web Tracks</head><p>Our runs submitted to the Session and Web tracks used the same indexes and similar methods. The indexes are the same one we used for last year's submissions <ref type="bibr" coords="7,386.28,113.41,10.91,9.57" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Method 1: Field &amp; Phrase Features</head><p>For this method, we use the indri query language to compute language model scores for title, inlink, and url fields for independent query terms as well as an unordered phrase containing all query terms, and combine them with a basic full-document language modeling query. The weights assigned to each feature are hand-tuned.</p><p>An example indri query is as follows:</p><p>#combine( #weight(1 #combine(identifying spider bites) 1 #weight(1 #combine(identifying.title spider.title bites.title) 1 #uw(identifying spider bites).title) 50 #weight(1 #combine(identifying.inlink spider.inlink bites.inlink) 1 #uw(identifying spider bites).inlink) 0.5 #weight(1 #combine(identifying.url spider.url bites.url) 1 #uw(identifying spider bites).url)))</p><p>These queries were submitted to two different ClueWeb12 indexes to generate two different runs.</p><p>The first, udel itu, used an index that represents documents by only their title, anchor text from inlinks, and url. The second, udel itub, used an index that contains full document text in addition to those three fields. We submitted udel itu runs to both the Session and Web tracks. We submitted a udel itub run to the Web track but not the Session track due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Method 2: CombCAT Fusion</head><p>CombCAT is a variation of CombMNZ wherein we not only take the overall similarity values into account, but more importantly we also account for the frequency of a document. In CombCAT, for each query, we first group documents into different categories such that documents that appeared in n different rankings are put in the same category category n . Then we proceed with our re-ranking by first ranking documents in decreasing order of n, and within category n in decreasing order of sum score. Thus CombCAT explicitly rewards documents that appear in the largest number of rankings.</p><p>For one set of runs, we use Bing to obtain "related queries" for each query. Then we use Indri to run each of these related queries as well as the original query. We then aggregate the resulting rankings using CombCAT. This produces our Session track Udel14Run1 RL1 run as well as our Web track UdelCombCAT runs.</p><p>The Udel14Run1 RL3 run takes advantage of the additional session data by collecting all of the last queries for all sessions on the same topic, collecting "related queries" for these, submitting these to our indri index, then aggregating results using CombCAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Table <ref type="table" coords="7,101.33,687.24,5.45,9.57" target="#tab_6">5</ref> shows Session track results for nDCG@10, while Table <ref type="table" coords="7,368.27,687.24,5.45,9.57" target="#tab_7">6</ref> shows Web track results for ad hoc and diversity measures. The results show that, using the official measure nDCG@10, CombCAT with Bing related queries outperforms simple field and phrase features. Additionally, incorporating information from other user sessions successfully improves the performance. However, as Table <ref type="table" coords="8,179.24,274.25,5.45,9.57">7</ref> shows, the field &amp; phrase features are less "risky" than CombCAT. When poorly-performing queries are discounted at a weight of α = 5, the two field-and-phrase runs significantly outperform the CombCAT run. Interestingly, ignoring the full page and just using title, inlink text, and url (the udel itu run) is slightly less risky than using the full page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The Information Retrieval Lab at the University of Delaware participated in five tracks: Contextual Suggestion, Federated Web, Microblog, Web, and Sessions. In all five we performed well, with many of our runs above the average median, and in many individual topics/scenarios achieving the best performance. rm (indri) terrier udel itu udel itub α=0 0.02584 -0.00992 0.00344 0.01516 α=5 -0.11092 -0.27713 -0.37582 -0.32152 Table <ref type="table" coords="9,102.88,410.90,4.24,9.57">7</ref>: ERR@20 Risk-aware measures for udelCombCAT2 relative to the other runs (listed in columns)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,72.00,384.63,254.87,9.60;1,72.00,406.53,264.41,9.60;1,72.00,428.43,334.66,9.60"><head>Section 3 FederatedSection 4</head><label>34</label><figDesc>Web track, by Mustafa Zengin. Microblog track, by Karankumar Sabhnani. Section 5 Web &amp; Session tracks, by Ashraf Bah and Ben Carterette.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,181.18,515.85,249.64,60.06"><head>Table 1 :</head><label>1</label><figDesc>Resource Selection Runs</figDesc><table coords="3,423.96,515.85,6.86,9.57"><row><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,130.22,284.81,319.57,60.64"><head>Table 2</head><label>2</label><figDesc>below shows the performance of udelftrsbs and udelftvqlR.</figDesc><table coords="4,225.72,308.38,160.56,37.06"><row><cell>RunID</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="4">udelftvql 0.167 0.852 0.257</cell></row><row><cell cols="4">udelftvqlR 0.236 0.680 0.328</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,228.76,358.87,154.49,9.57"><head>Table 2 :</head><label>2</label><figDesc>Vertical Selection Runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,72.00,448.02,468.00,87.74"><head>Table 3</head><label>3</label><figDesc>below shows the performance of udelftrsbs, udelftvqlR and a run that only consists of top 5 verticals from udelftrsbs on 40 queries.</figDesc><table coords="4,201.17,485.14,209.65,50.61"><row><cell>RunID</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>udelftvql@40Q</cell><cell cols="3">0.156 0.857 0.245</cell></row><row><cell cols="4">udelftvqlTOP5@40Q 0.218 0.602 0.297</cell></row><row><cell>udelftvqlR@40Q</cell><cell cols="3">0.233 0.642 0.317</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,197.38,549.19,217.25,9.57"><head>Table 3 :</head><label>3</label><figDesc>Vertical Selection Runs on 40 Query</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,72.00,74.43,468.01,196.56"><head>Table 4 :</head><label>4</label><figDesc>Unweighted Recall (UR), Weighted Recall (WR), and Precision (P) values for different values of Cluster Radius (Rad), Maximum and Minimum number of tweets considered for clustering (Max &amp; Min), and the Relevancy threshold (Rt). The first four lines are our official submissions; the last 5 are additional experiments.</figDesc><table coords="6,153.13,74.43,305.73,132.31"><row><cell cols="2">Runtag Rad Max Min Rt</cell><cell>UR</cell><cell>WR</cell><cell>P</cell></row><row><cell>TTG1.3 0.24 300</cell><cell cols="2">75 3.5 0.1873</cell><cell>0.3645</cell><cell>0.2793</cell></row><row><cell>TTG2.3 0.24 300</cell><cell cols="2">75 3.5 0.1845</cell><cell>0.3548</cell><cell>0.2752</cell></row><row><cell>TTG3.3 0.24 300</cell><cell cols="2">75 3.5 0.1832</cell><cell>0.3575</cell><cell>0.2782</cell></row><row><cell>TTG4.3 0.24 300</cell><cell cols="2">75 3.5 0.1900</cell><cell>0.3697</cell><cell>0.2779</cell></row><row><cell>TTG1.6 0.24 600</cell><cell cols="2">75 3.5 0.2223</cell><cell>0.3971</cell><cell>0.2319</cell></row><row><cell>TTG6.6 0.24 600</cell><cell cols="2">75 2.0 0.2065</cell><cell>0.3833</cell><cell>0.2606</cell></row><row><cell>TTG9.6 0.20 600</cell><cell cols="4">75 2.0 0.2588 0.4454 0.2550</cell></row><row><cell>TTG9.3 0.20 300</cell><cell cols="2">75 2.0 0.2349</cell><cell>0.4174</cell><cell>0.2780</cell></row><row><cell>TTG10.3 0.22 300</cell><cell cols="2">75 2.0 0.2172</cell><cell cols="2">0.3951 0.2865</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,110.51,74.43,390.98,126.28"><head>Table 5 :</head><label>5</label><figDesc>Session results (nDCG@10)</figDesc><table coords="8,110.51,74.43,390.98,126.28"><row><cell></cell><cell>Runs</cell><cell>RL1</cell><cell cols="2">%change</cell><cell>RL3</cell><cell>%change</cell></row><row><cell></cell><cell>udel itu</cell><cell>0.1515</cell><cell>0%</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">udel14Run1 0.2262</cell><cell>0%</cell><cell></cell><cell>0.2432</cell><cell>7.5%</cell></row><row><cell>Run</cell><cell cols="6">nDCG@20 ERR@20 α-nDCG@20 ERR-IA@20 strec@20</cell></row><row><cell>udelCombCAT2</cell><cell>0.26135</cell><cell cols="2">0.17880</cell><cell cols="2">0.655802</cell><cell>0.583244</cell><cell>0.834286</cell></row><row><cell>udel itu</cell><cell>0.19938</cell><cell cols="2">0.17536</cell><cell cols="2">0.565817</cell><cell>0.472367</cell><cell>0.831190</cell></row><row><cell>udel itub</cell><cell>0.18607</cell><cell cols="2">0.16364</cell><cell cols="2">0.534793</cell><cell>0.443358</cell><cell>0.812524</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,163.61,214.14,284.77,9.57"><head>Table 6 :</head><label>6</label><figDesc>Graded relevance measures and diversity measures</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,88.97,472.99,451.03,9.57;8,88.97,486.54,211.75,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,415.87,472.99,124.13,9.57;8,88.97,486.54,54.72,9.57">University of Delaware at TREC 2013</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sabhnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zengin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,167.14,486.54,98.30,9.57">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.97,509.05,451.03,9.57;8,88.97,522.60,279.68,9.57" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,238.38,509.05,301.62,9.57;8,88.97,522.60,234.10,9.57">Efficiency analysis of quality threshold clustering algorithms. Production Systems and Information Engineering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bednarik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kovács</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.97,545.12,451.03,9.57;8,88.97,558.67,368.99,9.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,347.76,545.12,192.24,9.57;8,88.97,558.67,155.28,9.57">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,253.75,558.67,98.79,9.57">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.97,581.18,451.03,9.57;8,88.97,594.73,217.06,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,399.95,581.18,140.06,9.57;8,88.97,594.73,60.45,9.57">Overview of the TREC 2012 Session track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,172.44,594.73,98.30,9.57">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.97,617.25,451.03,9.57;8,88.97,630.80,347.92,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,381.48,617.25,158.53,9.57;8,88.97,630.80,234.32,9.57">Federated search in the wild: the combined power of over a hundred search engines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,346.47,630.80,55.30,9.57">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.97,653.31,451.03,9.57;8,88.97,666.86,190.19,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,359.16,653.31,180.84,9.57;8,88.97,666.86,125.00,9.57">Indri: a language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,223.32,666.86,21.89,9.57">ICIA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
