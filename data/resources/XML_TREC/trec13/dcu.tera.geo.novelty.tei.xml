<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,63.84,95.28,473.29,13.13;1,143.16,111.84,314.70,13.13">EXPERIMENTS IN TERABYTE SEARCHING, GENOMIC RETRIEVAL AND NOVELTY DETECTION FOR TREC-2004</title>
				<funder ref="#_kW5xFMV">
					<orgName type="full">Enterprise Ireland</orgName>
				</funder>
				<funder ref="#_CaNv7Gn">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
				<funder>
					<orgName type="full">Informatics Directorate of Enterprise Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,55.86,155.07,54.07,8.50"><forename type="first">Stephen</forename><surname>Blott</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,118.76,155.07,55.08,8.50"><forename type="first">Oisin</forename><surname>Boydell</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Smart Media Institute</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<addrLine>Dublin 4</addrLine>
									<settlement>Belfield</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.78,155.07,64.24,8.50"><forename type="first">Fabrice</forename><surname>Camous</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.06,155.07,56.77,8.50"><forename type="first">Paul</forename><surname>Ferguson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.74,155.07,74.56,8.50"><forename type="first">Georgina</forename><surname>Gaughan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.20,155.07,54.57,8.50"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,467.68,155.07,72.00,8.50"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,55.86,167.73,52.57,8.50"><forename type="first">Noel</forename><surname>Murphy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,116.49,167.73,60.80,8.50"><forename type="first">Noel</forename><surname>O'connor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,185.33,167.73,65.64,8.50"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<email>alan.smeaton@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.24,167.73,50.03,8.50"><forename type="first">Barry</forename><surname>Smyth</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Smart Media Institute</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<addrLine>Dublin 4</addrLine>
									<settlement>Belfield</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.25,167.73,53.16,8.50"><forename type="first">Peter</forename><surname>Wilkins</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,63.84,95.28,473.29,13.13;1,143.16,111.84,314.70,13.13">EXPERIMENTS IN TERABYTE SEARCHING, GENOMIC RETRIEVAL AND NOVELTY DETECTION FOR TREC-2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1F0FDBFDA0DFE9E772C840C97D45868</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC2004, Dublin City University took part in three tracks, Terabyte (in collaboration with University College Dublin), Genomic and Novelty. In this paper we will discuss each track separately and present separate conclusions from this work. In addition, we present a general description of a text retrieval engine that we have developed in the last year to support our experiments into large scale, distributed information retrieval, which underlies all of the track experiments described in this document.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>In TREC2004, Dublin City University took part in three tracks, Terabyte, Genomic and Novelty. In this paper section 2 presents a general description of a text retrieval engine that supports all experiments and sections 3-5 discuss each track separately and present separate conclusions for each track. Our experiments in Terabyte searching primarily focused on the integration of document structure and anchor text evidence to support enhanced retrieval performance. For the Genomics track we experimented with combining MEDLINE abstract searching with MEDLINE MeSH (Medical Subject Headings controlled thesaurus) field matching and for Novelty we evaluated a number of techniques for identifying novel information by exploiting various term characteristics though adaptation of traditional IR approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GENERAL SEARCH ENGINE DESCRIPTION</head><p>This section will detail in general terms the workings of the underlying text retrieval engine (referred to as Físréal) that we developed within the past year in the Center for Digital Video Processing (CDVP). Since Físréal has been used in all of our TREC-2004 experiments we briefly discuss its operation and architecture. Section 2.1 will discuss the indexing process and section 2.2 will cover the retrieval engine architecture and associated retrieval concepts.</p><p>Físréal was primarily developed to provide fast search capabilities over very large amounts of data (i.e. of Terabyte size or greater), whilst at the same time providing a framework that would allow for experimental retrieval approaches to be quickly deployed and observed. A second requirement of Físréal was that it should be flexible enough to allow nonenvisioned search tasks or the searching heterogeneous data types to be achieved.</p><p>As with any type of search system, the first necessary component is a representation of the search candidate data in a form to allow for retrieval, in this case an index and indexer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing</head><p>Físréal, as noted previously, was utilised by experiments for all three tracks in which we participated. Each of the data sets for these tracks was comprised of text, but due to differences between the source data, indexing required different preprocessing techniques for data from each track. However whilst the method of indexing varied from case to case, the end result in each instance was an index that was structured similarly to all other indexes created. The structure of the index we employed relies closely on the underlying filesystem and is to be documented elsewhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval</head><p>The following section will detail at a high level the architecture that was employed by our search engine, and introduce the concept of 'pipelines' which allow us to rapidly configure different retrieval methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Retrieval Engine Architecture</head><p>The retrieval engine was designed primarily to act as a distributed search engine made up of a series of 'leaf' search engines (or nodes) which would be invoked by an 'aggregate' search engine. An aggregate search engine is the same as any other instance of the search engine (leaf node) except that it handles all incoming search requests. Any given installation of a retrieval engine could integrate as many leaf nodes as required, or could operate as a single leaf node in a standalone fashion, see Figure <ref type="figure" coords="2,132.65,281.23,3.79,8.77" target="#fig_0">1</ref>: The setup as defined above is the setup used for the Terabyte track (with four leaf nodes<ref type="foot" coords="2,417.66,396.31,2.92,5.09" target="#foot_0">1</ref> ). For each of the other tracks however only one leaf node was required. Each leaf server is in itself a totally independent instance of a search engine. Figure <ref type="figure" coords="2,83.31,422.97,4.87,8.50" target="#fig_1">2</ref> provides a high-level architecture overview of each search engine instance: As can be seen above, the search engines utilize a two-tier cache, a query cache followed by a term cache. The first tier or query cache examines incoming queries and if a match is found in the query cache then the result can be directly returned without further processing. The second or term cache is employed for any terms not picked up by the query cache and serves to further reduce the number of disk accesses. It is important to note that the caches were turned off for each track. The diagram also introduces the 'Pipelines' file. For any given search engine the retrieval methods that it can employ are defined by its associated pipeline file. Each incoming query to a search engine defines which pipeline is to process its query. We now briefly discuss pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Pipelines</head><p>A pipeline is comprised of a series of pipeline components, self contained blocks of code that implement a common interface. Each pipeline component has only two methods it needs to implement, namely StartUp and Process. StartUp is invoked at search engine start time and any loading of files or setup actions occur within this method. The Process method is the only entry point for a pipeline component within an executing search engine. The argument and return value for this method is a Search Data Unit or SDU. An SDU is an augmented container object that amongst other attributes defines the following; the initial query, the pipeline to call, the result formatting type and an empty container for results. A pipeline component will take in a SDU, modify or add to it, then pass it onto the next pipeline component. The following diagram provides a definition of a basic BM25 pipeline with pipeline components illustrated. In Step 1, a SDU has been generated by the server for the standard BM25 pipeline. The first step of this pipeline is query stopword removal. In step 2, the SDU is moved onto the next component which will stem the stopped query within the SDU before (in Step 3) the relevant index entries are placed into the SDU. Ranking of these entries occurs utilising BM25 (in Step 4). This is the end of the pipeline, the SDU exits having had its query modified and result space filled. The above example pipeline would be represented in the Pipelines.xml file by the following entry: New pipelines can thus be formed by creating a new entry in a Pipelines.xml configuration file and aligning the various components into the desired order. This made the reconfiguration of Físreál for different TREC tasks, quite straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TERABYTE TRACK ACTIVITIES</head><p>The TREC2004 Terabyte task used the new .GOV2 collection and required the processing of fifty topics (in standard TREC format). All of our runs were automatic, with the title of the topic being used as the query for each topic, with no manual intervention.</p><p>Our experiments can be divided into two distinct groups, and will now be presented accordingly. Three of our five runs were based on utilising either document text, URL text or anchor text. The last two runs, though based on our baseline run, integrated collaborative search techniques in order to generate improved ranked lists of results. We begin by discussing our three runs that are based on utilising the document text, URL text and the anchor text for retrieval and refer to these as our Content Experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content-Experiments</head><p>Our content experiments consisted of three runs, one a baseline run and two additional runs that utilise additional sources of evidence (URL text and anchor text) from the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Baseline Run</head><p>For our baseline content-only run we used Okapi BM25 <ref type="bibr" coords="4,304.38,191.49,97.88,8.50" target="#b6">(Robertson et al., 2004)</ref> to rank the results, with k1=1.2, k3=1000 and b=0.75. Físréal supports sorted index creation, thus allowing us to sort document instances for terms in descending order of document normalized TF values. In this way we could read from disk only a subset of the top ranked entries for each term in order to improve retrieval latency. Given that the query time was an important factor of the experiments, we choose to read the top ten thousand documents for each term at retrieval time. This figure of ten thousand was chosen after comparisons between the result sets where all the documents were read versus a subset of the top documents, as well as taking the query timings into consideration to try and find a good balance between precision and speed.</p><p>For this run we returned the top 10,000 document Ids and refer to this run as DcuTB04Base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weighted BM25</head><p>The purpose of our second run was to examine what benefit we could gain by utilizing HTML markup elements to weight certain terms more than others. In Table <ref type="table" coords="4,244.39,357.75,4.87,8.50" target="#tab_1">1</ref> we outline what we considered to be the tags that would contain the most representative words of each document. Then for these tags we defined the extra weighting that we felt any word contained in these tag deserved. Normal body text was weighted at 1.0. Again Okapi BM25 is used with similar parameters to rank the top ten thousand documents for each term and this run was labeled DcuTB04Wbm25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BM25, URL &amp; Anchor Text Run</head><p>It is believed that integrating anchor text into the retrieval process can be useful in improving retrieval performance for Web IR <ref type="bibr" coords="4,88.39,536.19,118.27,8.50" target="#b2">(Craswell and Hawking, 2003)</ref>. We generated anchor text surrogate documents by extracting the anchor text (with a window of 56 bytes either side) from all documents that link to a given document. We created an index from these documents and used Okapi BM25 ranking again here, but with parameter values k1=50, k3=1000, and b=0. These parameters are best guesses and will be the subject of further experimentation.</p><p>One additional source of evidence we incorporated into the ranking process for this run was the URL text of each document, where the document content was based on extracting terms from the URL string utilizing URL syntax to identify partitioning characters. This provided an additional index which was queried using Okapi BM25 with more conventional parameter weighting of k1 = 1.2, k3 = 1000 and b = 0.75.</p><p>The combination parameters for combining scores from each source of evidence used was as follows (text=0.6, anchor=0.3, URL=0.15). We were interested to see if these parameters (our best guess) could improve retrieval performance over a baseline of our Weighted BM25 run. This run is labeled DcuTB04Combo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Collaborative Search Experiments</head><p>Our other two runs for the TREC Terabyte search track were based on the idea of collaborative search, which we briefly describe in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Background</head><p>The I-SPY Web search engine <ref type="bibr" coords="5,195.38,127.89,81.87,8.50" target="#b3">(Freyne et al., 2004)</ref> developed at University College Dublin proposes an approach to search known as collaborative search. The basic intuition is that the world of web search can be both repetitive and regular. Like-minded users tend to use similar queries and select similar results in response to these queries. Collaborative search takes advantage of this by reusing the past search behavior of similar users within well-defined communities of like-minded individuals. For example, an I-SPY search box located on a motoring Web site will attract motoring related queries and result selections so that ambiguous queries such as "jaguar" will result in the selection of car-related sites rather than those relating to cats or operating systems of the same name.</p><p>To achieve this, I-SPY operates as a meta-search engine (See Figure <ref type="figure" coords="5,356.31,216.39,3.53,8.50">4</ref>), dispatching queries to multiple traditional (underlying) search engines and merging their result-lists. However, I-SPY also tracks the results that are selected for given queries and stores this information in a so-called hit-matrix. The importance of the hit-matrix is that it allows I-SPY to estimate the relevance of a page p i for a query q j in terms of the relative proportion of times that p i has been selected for those queries that are similar to q j . Crucially, this relevance information can be used directly as a means to promote and rank those results that have been previously selected for q j or related queries. When I-SPY receives a new target query q T , in addition to retrieving a set of results from its underlying search engines, it also locates a set of results from its hit-matrix, by locating all queries that are above a set similarity threshold with the target query as in Formula 1.</p><p>(1) These hit-matrix results are ranked according to their weighted relevance metric, and then combined with those from the underlying search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: The I-SPY architecture and hit-matrix</head><p>A separate hit-matrix is maintained for each well-defined community of users, and in this way I-SPY adapts its ranking of results to reflect the past preferences of a given community by promoting those results that have been preferred in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Applying I-SPY to the Terabyte Track</head><p>It is worth noting that I-SPY's approach is not only limited to the use of user selection information as a source of relevance. In fact, we can view the hit-matrix as a general mechanism for implementing a wide range of relevance feedback strategies that are based on any number of factors. To this end we apply I-SPY to two different Terabyte search engines, both of which are instantiations of the Físréal search engine developed at CDVP, DCU. The first is the search engine that is based on the standard Terabyte track document index as discussed in Section 1. The second is a similar search engine, but one that is based on the anchor-text index (as described in section 3.3). Both search engines were configured to use BM25 as the ranking algorithm.</p><p>To apply I-SPY to these document collections we must first train its hit-matrix to reflect our relevance model of choice, which involves the following steps:</p><formula xml:id="formula_0" coords="5,122.28,318.15,352.44,202.30">p 1 p i p n q 1 q j q m 1 5 3 11 H ij 7 8 9 I-SPY ENGINE S1 S2 Sn Adapter Adapter Adapter U HIT MATRICES Admin Reporter q R R' 1 R' 2 R' n R 1 R 2 R n q 1 q 1 q 1 q q q H p 1 p i p n q 1 q j q m 1 5 3 11 H ij 3 11 H ij 7 8 9 I-SPY ENGINE S1 S2 Sn Adapter Adapter Adapter U HIT MATRICES Admin Reporter q R R' 1 R' 2 R' n R 1 R 2 R n q 1 q 1 q 1 q q q H 2 n p 1 p i p n q 1 q j q m 1 5 3 11 H ij 7 8 9 I-SPY ENGINE S1 S2 Sn Adapter Adapter Adapter U HIT MATRICES Admin Reporter q R R' 1 R' 2 R' n R 1 R 2 R n q 1 q 1 q 1 q q q H p 1 p i p n q 1 q j q m 1 5 3 11 H ij 3 11 H ij 7 8 9 I-SPY ENGINE S1 S2 Sn Adapter Adapter Adapter U HIT MATRICES Admin Reporter q R R' 1 R' 2 R' n R 1 R 2 R n q 1 q 1 q 1 q q q H 2 n ) , ( ) , ( Exists ) , ( ) , ( Relevance ) ,..., , , ( Rel ... 1 ... 1 1 j T n i j i j T n j j i n T i q q ilarity Sim q p q q ilarity Sim q p q q q p W ∑ ∑ = = • • =</formula><p>1) A set of training queries must be generated. In this work we derive these queries by selecting sets of terms from the narrative and description of each topic. 2) These queries are submitted to I-SPY and the returned results are processed for entry into the hit-matrix according to a suitable selection model. In Section 3.1.3 we describe the use of a simple selection model based on the inverse rank of the top 100 returned results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">The I-SPY Terabyte Track Runs</head><p>We produced two separate runs of I-SPY which differ in terms of the underlying search engines that I-SPY calls upon and in the manner in which hit-matrix training occurs. For the first I-SPY run (DcuTB04Ucd1) we configured I-SPY to operate with both the standard document index and the anchor-text index as underlying search engines. We generated 500 training queries per topic with each query being between 2 and 8 terms in length. Each query was generated by combining words from the topic's narrative and description sections after stop-word removal. During training, each query was submitted to I-SPY and the top 100 results returned were used to update the hit-matrix using a linear inverse ranking function. The query-result hit-matrix entry for the top result is incremented by 100, with the increment for each subsequent result reduced by a constant amount.</p><p>The second I-SPY run ( <ref type="formula" coords="6,164.64,291.99,55.73,8.50">DcuTB04Ucd2</ref>) is similar to the first except that it relied on the anchor-text index alone as its underlying search engine, only 250 queries per topic were used in training, and the hit-matrix was updated with only the top 20 results per query. When training was completed the system was ready to accept queries. When a new query is submitted to a trained version of I-SPY, it needs to combine the results obtained from its underlying search engines (described earlier in section 3) with those results obtained from the hit-matrix. For both DcuTB04Ucd1 and DcuTB04Ucd2, we used the search engine (Físréal) that is based on the standard Terabyte track document index as the single underlying search engine when submitting the test queries. Each query was used to probe the hit-matrix, retrieving the entries associated with all similar queries (above a set similarity threshold with the original query) and ranking these results according to the weighted relevance metric outlined above. In DcuTB04Ucd1 we set the similarity threshold at ≥50% and in DcuTB04Ucd2 it was set at &gt;0%; the former reused queries that shared 50% or more of their terms with the test query while the latter reused queries that shared at least 1 term with the test query.</p><p>In both of our TREC Terabyte runs we actively promoted the hit-matrix results, which also appear in the top 10,000 results returned from the underlying search engines, ahead of the results from the underlying search engines, and ranked them by their weighted relevance, as shown in Figure <ref type="figure" coords="6,272.92,602.79,3.65,8.50" target="#fig_3">5</ref>. This promotion is intended to improve the search precision. The promoted results are from related searches using more descriptive terms from the topic's narrative and description sections, and also from searches on the anchor text index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GENOMIC TRACK ACTIVITIES</head><p>This section describes the methods used to respond to the challenge of the TrecGen 2004 ad hoc retrieval task. It follows up last year participation to the primary task (ad hoc) of TrecGen 2003 <ref type="bibr" coords="6,385.07,705.87,74.15,8.50" target="#b0">(Blott et al., 2003)</ref>. For this previous participation we had used pseudo-relevance feedback, also known as "retrieval feedback", in order to improve a baseline provided by Jacques Savoy of the University of Neuchatel, Switzerland. For TrecGen 2004, however, we used Físréal to provide our own underlying search facility. Físréal allowed us to build different indexes for document representation (section 4.3.1) and use appropriate algorithms to retrieve and rank documents. The TrecGen 2004 ad hoc task involved dealing with new types of information needs and a much bigger document collection compared to the previous year, a subset of MEDLINE consisting of 10 years of completed citations inclusive from 1994 to 2003 and the total number of documents was 4,591,008. The 50 topics were obtained through interviews with biologists and thus reflect authentic needs. Each topic included a title (abbreviated statement of the information need), an information need (full statement of the information need) and a context (background information to put the information need in context). Five sample topics were provided with partial relevance judgements relative to these topics. This section is organized in the following way: in section 4.1, we present the main specificity of this year approach and briefly introduce the MEDLINE document record format (http://www.ncbi.nlm.nih.gov/entrez/query/static/help/pmhelp.html#MEDLINEDisplayFormat) and the Medical Subject Heading controlled vocabulary (http://www.nlm.nih.gov/mesh/meshhome.html). In section 4.2 we discuss our retrieval strategy which includes our indexing method, query construction and ranking strategies before presenting our results in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>The use of MeSH descriptors MEDLINE records have several fields including title, abstract, author and MeSH fields (http://www.ncbi.nlm.nih.gov/entrez/query/static/help/pmhelp.html#MEDLINEDisplayFormat). Some fields are not mandatory and they may not appear in all MEDLINE records. However, very few documents do not have any MeSH fields in the TrecGen 2004 collection. MeSH is the National Library of Medicine's controlled vocabulary thesaurus (http://www.nlm.nih.gov/). MeSH descriptors are manually added to the MEDLINE records by experts after the reviewing the full text of the article. We believe that the use of descriptors in the query should bring some consistency to the retrieval results as similar articles will have similar or identical descriptors, whereas the vocabulary used in their titles and abstracts might differ due to synonymy and polysemy of the natural and scientific languages. There are 22,568 unique descriptors available in the current version of MeSH, and the average number of descriptors per document in the TrecGen 2004 collection is 12. The MeSH thesaurus is organized hierarchically and includes 15 trees where MeSH descriptors or "Main Headings" (MH) may appear several times (1.8 times on average). In our TrecGen 2004 experiment, we chose to represent the documents as "bags of descriptors". We did not use the trees of the hierarchy to compute a tree similarity measures between documents and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Our approach</head><p>As we mentioned above, the terms used in the title and abstract fields of MEDLINE records vary from one author to an author and they will represent a subset of the terms used in the full article. However, we have seen that MeSH terms are added to the MEDLINE record by a human expert after reading the full article. Also MeSH terms are part of a controlled vocabulary and will therefore be more consistent from one expert reviewer to another. A study by <ref type="bibr" coords="7,452.16,559.65,87.64,8.50" target="#b4">Funk and Reid (1983)</ref> showed that the indexing of MEDLINE records was highly consistent and represented "the state of the art in manually indexed data bases". Accordingly we decided to have a dual representation for each record: a "bag of title/abstract terms" and a "bag of descriptors" which allowed us to build two queries for each topic; one was a bag of terms extracted automatically from the different fields of the topic and the other one was a bag of descriptors that was created manually by a human expert after reading the topic fields content. The second query can be seen as an expansion of the first. The process of document representation and query building is described in detail in sections 4.3.1 and 4.3.2. We want to show that integrating two representations of MEDLINE records, a thesaurus-based one and a "free" text one, can improve the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Previous and related work</head><p>Using the MeSH vocabulary for query expansion has been studied before. Srinivasan (1996) experimented with query expansion and retrieval feedback using an inter-field statistical thesaurus with a collection of 2,334 documents with 75 user queries. The thesaurus was built by looking at co-occurrences in documents of terms from the title/abstract fields and terms from the MH field. The co-occurrences with the best scores were used to expand the user queries and build MeSH queries that were used to search the MH fields of the documents. Using Cornell University's SMART retrieval system, Srinivasan combined different SMART retrieval strategies with different thesaurus-building strategies. The improvements obtained over the different baselines by the thesaurus expansion strategies were in the range of 8-10.6%. This approach contrasts with the experiments described here, in that we did not here build an inter-field thesaurus, but rather, asked an expert to analyse the 50 TrecGen 2004 queries and search the MeSH vocabulary with the MeSH browser available on the Web. We intend to build our own statistical inter-field thesaurus in the future and compare its expansion queries to the ones build by our expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retrieval Strategy</head><p>We will discuss our retrieval strategy in terms of indexing, query construction and ranking strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Indexing Methodology</head><p>We built two distinct indices for the TrecGen 2004 document collection: a title/abstract index and a MeSH index. For the title/abstract index, terms were extracted from the title (TI) and abstract (AB) fields of the MEDLINE records. No stemming was performed.</p><p>For the MeSH index we extracted all the descriptor from the Main Heading (MH) fields of the MEDLINE records. A star is used in MEDLINE records (e.g. "*Carbohydrate Sequence") to differentiate a "central concept" from another descriptor. A central concept is a descriptor that describes the central topic of the article whereas a non-central descriptor would describe peripheral issues also mentioned in the article. The descriptors are often accompanied by a "qualifier" or "subheading" that allows the domain of application for the chosen descriptor to be specified. In the MH field of the MEDLINE record, the subheading is added directly after the Main Heading it qualifies, with a forward slash preceding it. For example, "Immunoglobulin G/chemistry" is a Main Heading (MH)/subheading (SH) pair. The notion of "central concept" can also apply to a combination of MH and SH. In that case, a star is added at the beginning of the qualifier (e.g. "Immunoglobulin G/*metabolism"). Although we intend to use these distinctions in future work, in TrecGen 2004 we limited our experiments to represent documents as "bags of descriptors" and did not include the subheadings. So we removed all subheadings and made no distinctions between the "starred" Main Headings and other Main Headings. The end result of this was that we had two indices where documents were represented as bags of terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Query construction</head><p>For the title/abstract query construction (referred to as "ta" queries below), we chose two strategies. The first consisted in only taking terms from the title and the information need fields of the topic (we refer to this as the "tn" strategy). The second was to extract terms from each field of the topic, but with different query weight depending on the topic field they were extracted from. The weighting used was 4 for title terms, 2 for information need terms, and 1 for context terms. This strategy was named the "w" strategy.</p><p>In order to face the problem of the morphological variations of biological terms present in medical article abstracts, we expanded the queries obtained with the two strategies described above with the help of a script provided given to us by De <ref type="bibr" coords="8,55.44,635.49,100.18,8.50" target="#b1">Bruijn and Martin (2003)</ref> from the Institute of Information Technology at the National Research Council of Canada. This script creates morphological variations on the use of numerical symbols (Roman, Greek, contemporary), and by attaches or detaches numbers to or from the term they are related to. This expansion strategy was used for the TrecGen 2003 primary task and showed some success at improving the recall. The result is two non-expanded "ta" query strategies, "tn1" and "w1", and two expanded query strategies, "tn2" and "w2".</p><p>The construction of the MeSH queries was done manually. An expert from within the University read each one of the 55 topics (50 main topics plus the 5 sample topics) and chose the appropriate Main Headings with the assistance of the MeSH browser available on the Web. We also asked our expert to distinguish the most important Main Headings amongst those she had chosen for the topic. These Main Headings were very important in the sense that all documents deemed relevant to the topic should contain some or all of them. These we refer to as "Magic" Main Headings. This leads to a further retrieval strategy "ma" that only yields results that do in fact contain all the magic headings.</p><p>Similarly to the "ta" queries, we chose to have an expansion strategy for the MeSH queries. The MeSH vocabulary is organized into 15 trees. Each tree has a root that represents the most general concept of all the terms contained in the tree. Example of such concepts are "Anatomy" (tree A), "Chemicals and Drugs" (tree D) or "Geographic Locations" (tree Z). Further down the tree are represented concepts that are increasingly specific. Main Headings can appear several times in the hierarchy (1.8 times on average) and therefore can have several parents, siblings and children. However, multiple occurrences of the same Main Heading tend to be found in the same tree (85% of the time). Our expansion strategy involved looking up all the possible occurrences of a MeSH query term in the hierarchy and adding the parent(s) and the children of each to the query. We called the non-expanded MeSH query strategy and the expanded one respectively "m1" and "m2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Ranking strategies</head><p>Following the different types of queries we obtained three types of ranking: a "ta" ranking after querying the "ta" index with either strategy "tn" of "w" (expanded or not), a MeSH ranking after querying the MeSH index with strategy "m" (expanded or not), and a "magic" ranking after querying the MeSH index with the "magic" Main Headings only.</p><p>When the three types of ranking were used together, we used the "magic" ranking as a filter to the "m" ranking, i.e. documents retrieved in the "m" ranking but not retrieved in the "magic" ranking were removed from the "m" ranking. The resulting "m" ranking was then merged with the "ta" ranking as follows. Scores in each ranking were normalized with the best score in each ranking. A document found in only one ranking kept its original score in the merged ranking. A document found in both rankings would be integrated into the merged ranking with a score equal to the sum of its scores from both original rankings. The documents were then re-ranked according to their new score, and only the top 1000 were kept. To represent this new ranking, we used, for example, tn1_ma_m1, which describe the combination in which the nonexpanded version of "tn" and "m" is used, a "magic" filtering of the "m1" ranking is completed, and the resulting "m1" ranking and "tn1" ranking are merged in the way we described above. The possible combinations including cases where the "ma" ranking was not used as a filter but as a proper MeSH ranking that was merged with a "ta" ranking (tn1, tn2, w1, w2) and cases where no filtering was used, and also where only one ranking was used on its own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Genomic Results</head><p>The evaluation of these rankings or combinations of rankings uses the mean average precision over the 5 sample topics provided for the TrecGen 2004 ad hoc task. We used the trec_eval program along with the partial relevance judgements provided.  shows the Mean Average Precision and Recall values for each ranking strategies described in section 4.2.3. We evaluated rankings from using each strategy on their own, the "ta" strategies (tn1, tn2, w1, w2) and the "m" strategies (m1, m2). Even if the performance was low for each one of these unique strategies, we notice a significant difference between the "m1" strategy and all the others. With 2.01% average precision and 64.5% recall, "m1" is the best of the unique strategies. The ranking obtained from the expanded query versions has lower performance than the nonexpanded ones, especially "m2". Our morphological text expansion that we applied in "tn2" and "w2" decreases the Mean Average Precision (MAP) without improving the recall. Our policy of adding the parents and children of a MeSH Main Heading in "w2" also has a clear negative effect on both MAP and recall.</p><p>The rankings obtained from the combinations of the "text" or "ta" strategies with the MeSH or "m" strategies give better results on average than the unique strategies. Any combinations with "m1" would outperform any combinations with "m2" and given a combination with "m1" or "m2", the expanded "ta" strategies, "tn2" and "w2", yield lower MAP, as the early results seemed to suggest. However, in the case of "m1_tn2" and "m1_w2", we observe improvements in recall.</p><p>Next, consider text and MeSH strategy combinations where the "magic" filter was used. As explained above, this filtering process takes away from the MeSH ranking the documents which were not retrieved in the top 1000 documents ranked as a result of the "ma" query. This "ma" query contains descriptors that were judged as highly important by our expert. The results suggest that filtering always improves the MAP, and either improves the recall or leaves it unchanged. This suggests in turn that the "magic" descriptors provided by the expert were helping in the selection of relevant document and were not harming the recall level.</p><p>The rest of the table gives the results for the "ma" ranking on its own as well as the merging of the "ma" ranking with all the rankings resulting from "ta" or title/abstract queries. Although "ma" ranking was originally intended to be used as a filter to improve precision, it gave us the best result of all rankings and merging of rankings in both MAP and recall. As a consequence we proceeded to merge it with the "ta" strategies (tn1, tn2, w1, w2). The results show that those combinations yield a better MAP than any combination of strategies and unique strategies (except the "ma" ranking). However the recall is inferior to the one obtained when the "ta" strategies are merged with m1 (see m1_tn1, m1_tn2, m1_w1, m1_w2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Genomic conclusions and future work</head><p>In order to select the two runs for submission, we decided to take a broad look at the results. Table <ref type="table" coords="10,482.64,458.55,4.87,8.50" target="#tab_1">1</ref> allows us to compare the different elements of the ranking combinations more than it allows us to choose a particular combination, given the partial nature of the relevance judgments. We thus conclude that the expanded text queries ("tn2" and "w2") did not seem to work and that the terms extracted from the title and information need fields ("tn") of the topics were better query terms than the ones extracted from all the topic fields with weights ("w"). We inferred that the "tn1" ranking strategy should be part of the run submitted. The expanded version of the MeSH queries damaged the performance and the magic queries gave the best performance in both Mean Average Precision and recall. We were confident that a combination of "tn1" and "ma" would give satisfactory results and we submitted the "ma_tn1" combination as our first choice. As a second choice we submitted the "ma" ranking on its own because it's high performance with the 5 sample topics although we had little confidence that it would perform well over the 50 topics.</p><p>The "bag of descriptors" representation of documents does not allow us to calculate a suitable similarity measure between documents that have few terms in common but which terms are related in the MeSH trees. We plan to make full use the MeSH hierarchy and experiment with several tree similarity measures in order to build a better document representation and more appropriate similarity measures. We also want to exploit the distinction made in the MEDLINE manual indexing process between "central concepts" and "peripheral concepts" and integrate the use of "qualifiers" during the document representation building process (as discussed in section 4.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NOVELTY TRACK ACTIVITIES</head><p>The TREC 2004 Novelty Track was structured into three distinct tasks. Participants in the first task were required to retrieve a subset of the twenty-five relevant documents for each of the fifty topics. Once this task was performed, participants were required to extract from within those retrieved documents all the sentences that were relevant to each of the fifty topics. Finally, given this list of retrieved relevant sentences, which were ranked in a predetermined order, participants were required to select a subset of sentences that provided novel or new information on the topic.</p><p>A sentence S n is considered novel if and only if its similarity with all previously seen and highlighted novel sentences S 1 …..S n-1 , is below a certain threshold.</p><p>As this is the first year that DCU has participated in the Novelty Track, we first investigated various methods employed in the previous two years the Novelty Track has run. We present here the algorithms we developed to complete Task 1 and Task 2. We built three models; the first focused on retrieving the twenty-five documents that were relevant to each topic; the second focused on retrieving relevant sentences from this list of retrieved documents to satisfy each individual topic; the third focused on the detection of novel sentences from this relevant list.</p><p>In Task 1 we were given a collection of documents and are asked to retrieve all new and relevant information for each of the fifty topics. We used Físréal, an information retrieval system developed by the CDVP for the Terabyte Track (described in section 2), as a basis for our experiments, which was configured to use BM25 weighting. In Task 2 we were given the relevant sentences and are asked to find the novel sentences. This allows us to accurately assess our novelty models, as we are not depending on a system's ability to retrieve sentences. In this task we regarded sentences as documents, and developed two formulas, which exploit traditional document similarity methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task1 : Redundancy</head><p>Traditionally in Information Retrieval, implementation and experimentation of retrieval algorithms have dealt with large collections, containing many documents. However, the Novelty Track explores various methods to find relevant sentences within a document. It has been shown over the last two years that sentence retrieval is more difficult to accomplish than traditional document retrieval as there are less words in each document, and hence less accuracy and more ambiguity <ref type="bibr" coords="11,520.38,395.31,19.53,8.50;11,55.44,407.97,47.70,8.50" target="#b5">(Min et al., 2003)</ref>. Our approach to improving performance was to extend the document and the query data using Dekang's (http://www.cs.ualberta.ca/~linkdek/downloads.htm) dictionary of both similarity and proximity term dependency. This dictionary was successfully employed to improve system performance by THUIR at TREC2003. We submitted five runs which we discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Stage One</head><p>This involved the detection of the relevant twenty-five documents in the collection supplied by NIST for each of the fifty individual topics. We automatically generated a query for each topic, which concatenated together the title, the description and the narrative (negative information from the narrative was automatically removed) from the TREC query. The top twenty-five highest ranked documents returned from Físréal were chosen as our relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Stage Two</head><p>This involved the detection of relevant sentences for each of the fifty topics, where our input was the set of documents retrieved in stage one. We experimented with different combinations of query and document expansion, utilising Dekang's dependency dictionary. In Run 1 (cdvp4CnS101), we queried the system using a long query against document expanded using the similarity dictionary. In Run 2 (cdvp4QePnD2), we examined the effects of expanding the query alone using the proximity dictionary. Run 3 (cdvp4CnQry2), used traditional searching with no performance enhancement on either the query or document. In Run 4 (cdvp4QePDPC2), we experimented with a combination of query and document expansion using the proximity dictionary and in the final Run 5 (cdvp4QeSnD1), we investigated the benefits of using just query expansion which utilised term similarity dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Stage Three</head><p>This stage takes as input the ranked list of relevant sentences produced in the previous step, and returns only those sentences that provide new or novel information for each topic. This novelty model cannot be accurately assessed as its overall performance is dependent on the results of both stages one and two, in which the initial detection of relevant data is carried out. With so many contributing factors, it is difficult to decipher whether the novelty algorithms are performing well. The first method used in Task 1 to detect sentences that were novel was based on the UniqueHistory formula (Section 5.2) using a threshold of 3. The second method used was the NewSentenceValue formula (Section 5.2) using a threshold of 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task 2 : Novelty</head><p>In this task, participants were given an ordered list of relevant sentences for each of the fifty topics and were required to detect a subset of this ranked list, which will provide novel or new information to the users. We evaluated the performance of our system using variations of three different formulas ImportanceValue (Section 5.2.1), NewSentenceValue (Section 5.2.2) or UniqueHistory (5.2.3). We submitted a total of five runs (see Table <ref type="table" coords="12,362.37,243.57,3.51,8.50" target="#tab_5">3</ref>). Given a sentence S n in the ordered list of relevant sentences, we evaluated the number of unique words U s that occur in the set of words S s in the current sentence S n , against an accumulating list of all unique words U H encountered to this point (for a particular topic). We then evaluated U s using one of the three formulas, and assigned to each sentence its novelty score. If this score was above a predefined threshold, the set of unique terms U s was added to the history set U H and the sentence was added to the list of novel sentences to be returned to the user. The Baseline run (cdvp4NSnoH4) used the UniqueHistory formula however we did not keep an accumulated history set of all the previous sentences. The results were very poor for this run, which in retrospect, was not surprising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The Importance Value</head><p>This function (see Formula 2) exploits the properties of a term from both within a sentence and the collection of sentences from which it originates. It models the assumption that a term with a high term frequency (tf) and a high inverse document frequency (idf) would most likely be a valuable term in providing new and valuable information about a topic. A sentence assigned a novel score, higher than a predefined threshold, is considered a novel sentence.</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2</head><p>The NewSentenceValue</p><p>The NewSentenceValue formula (see Formula 3) exploits the characteristics of a term within a collection of sentences. We analyse the importance-value of the unique terms U s and assess their overall benefit, in contributing novel information to the user. A sentence with a novel score above a predefined threshold is considered a novel sentence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>The UniqueHistory</p><p>The novel score of a sentence was determined by the evaluating the number of unique words U s that occurred in the sentence word set against an accumulating list of all unique words U H encountered to this point (for a particular topic). If the number of unique words exceeds a particular threshold then the sentence was considered novel. This is a crude way to determine novelty but as the results show it is a method which gives comparable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Novelty Results</head><p>It can be observed (Table <ref type="table" coords="13,171.40,218.31,4.06,8.50" target="#tab_5">3</ref>) that all relevance runs performed equally well and all were above the mean results for the track. The highest F-scores (marginally) were achieved by applying query expansion, using the term proximity only (run cdvp4QePnD2). When we compare the novel results of Task 1 to the overall results of Task 2, it can be seen that the performance of Task 1, in which used a combination of two methods, is substantially lower than Task 2. These results could be caused by the dependency of Task 1's novelty system on relevant results.  <ref type="table" coords="13,92.34,411.87,4.87,8.50" target="#tab_6">4</ref> displays the results of task2. It can be clearly seen that the performance of run (cdvp4NTerFr1) implementing the ImportanceValue formula at threshold 1.5 was better than that of other runs and all runs, except the Baseline, were above the mean. Analysing the performance of runs by topic type, we find that in detecting relevance (task1, Table <ref type="table" coords="13,457.02,559.65,4.02,8.50" target="#tab_7">5</ref>) our F-scores show that there is a variance between the systems performance on event and opinion topics. This variance can also been seen in detecting the novel sentences between opinion and event topics of task1. In Task2 (see Table <ref type="table" coords="14,146.76,102.36,4.05,8.77" target="#tab_8">6</ref>) where the relevant judgments have been provided we notice that the performance of our system in detecting opinions and events is very comparable and once again, all our results except baseline are above the mean.. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,259.38,373.22,146.65,7.88;2,196.50,298.50,245.04,72.12"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General Architecture of Físréal</figDesc><graphic coords="2,196.50,298.50,245.04,72.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,248.88,560.90,167.66,7.88;2,152.76,446.40,261.54,111.90"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High-level architecture of a leaf node</figDesc><graphic coords="2,152.76,446.40,261.54,111.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,252.66,310.22,160.09,7.88;3,125.46,237.84,316.32,57.12"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A Basic BM25 Pipeline Component</figDesc><graphic coords="3,125.46,237.84,316.32,57.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,261.90,451.94,141.57,7.88;6,196.50,342.30,202.80,96.30"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Generating the final result list</figDesc><graphic coords="6,196.50,342.30,202.80,96.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,431.28,699.21,11.35,8.50;12,62.82,362.59,49.44,6.77;12,77.46,375.04,206.36,6.98;12,331.26,375.04,133.04,6.98;12,77.46,387.64,127.58,6.98;12,77.46,400.30,142.76,6.98;12,331.26,400.30,102.59,6.98;12,77.46,412.96,236.93,6.98;12,74.52,425.56,66.93,8.32;12,331.26,425.56,106.44,8.53;12,76.44,438.22,91.89,6.98;12,192.54,444.02,1.87,4.33;12,185.58,439.57,5.95,7.42;12,171.30,436.92,13.79,10.09;12,74.52,450.88,82.00,6.98;12,181.50,456.23,3.62,4.51;12,174.12,451.63,6.21,7.74;12,159.42,448.86,14.16,10.52;12,331.26,450.88,91.64,6.98;12,448.68,455.78,2.18,5.04;12,443.04,450.64,4.80,8.64;12,426.00,447.55,15.36,11.75;12,294.42,702.62,12.44,21.38;12,251.70,718.22,12.44,21.38;12,252.00,687.08,12.44,21.38;12,299.52,721.81,3.72,8.32;12,256.74,737.35,3.72,8.32;12,256.26,706.21,3.72,8.32;12,290.16,705.77,2.91,14.26;12,299.40,701.02,1.89,6.11;12,295.62,724.00,3.02,6.11;12,320.94,715.78,5.29,6.11;12,255.48,716.62,4.91,6.11;12,254.22,739.54,1.89,6.11;12,278.22,731.32,4.91,6.11;12,256.50,685.42,3.40,6.11;12,253.80,708.40,1.89,6.11;12,278.52,700.18,5.29,6.11;12,326.16,718.79,2.15,4.37;12,284.16,734.39,1.35,4.37;12,283.98,703.19,1.35,4.37;12,308.10,709.52,12.30,10.48;12,265.38,725.12,12.30,10.48;12,265.68,693.99,12.30,10.48;12,302.88,724.18,3.40,5.92;12,260.10,739.72,3.40,5.92;12,259.62,708.58,3.40,5.92;12,323.10,585.24,7.79,10.51;12,287.70,576.12,12.33,10.51;12,252.30,576.12,6.49,10.51;12,243.12,567.57,3.41,6.13;12,240.42,590.61,1.89,6.13;12,278.52,567.57,3.41,6.13;12,275.88,590.61,1.89,6.13;12,300.60,582.33,5.30,6.13;12,259.32,582.33,5.30,6.13;12,306.42,586.60,1.35,4.38;12,264.84,585.40,1.35,4.38;12,324.24,569.06,5.84,10.18;12,246.24,590.79,3.41,5.94;12,281.64,590.79,3.41,5.94;12,317.16,572.36,2.92,14.30;12,311.34,572.54,4.47,14.30;12,311.34,583.64,4.47,14.30;12,311.34,565.04,4.47,14.30;12,232.86,572.54,4.47,14.30;12,232.86,583.64,4.47,14.30;12,232.86,565.04,4.47,14.30;12,269.76,572.36,2.92,14.30;12,238.62,569.19,12.47,21.44;12,274.01,569.19,12.47,21.44;12,242.88,588.41,3.73,8.34;12,278.27,588.41,3.73,8.34"><head></head><label></label><figDesc>Current sentence = Sn Set of words for the current sentence= Ss |Ss| the size of the current sentence set= N All previously seen sentences= S1…..Sn-1 Set of Unique Words in current sentence= Us |Us| the size the of unique set = n Accumulating set(History Set) of unique words over a particular topic = UH</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,55.44,407.24,478.74,33.89"><head>Table 1 ,</head><label>1</label><figDesc>Tag weights employed for DcuTB04Wbm25</figDesc><table coords="4,55.44,419.22,478.74,21.91"><row><cell>Tag</cell><cell cols="2">TITLE B</cell><cell>H1</cell><cell>H2</cell><cell>H3</cell><cell>H4</cell><cell>H5</cell><cell>H6</cell><cell>I</cell><cell>EM</cell><cell>U</cell><cell>A</cell><cell>ALT</cell></row><row><cell cols="2">Weight 6</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,55.44,306.81,484.49,33.76"><head></head><label></label><figDesc>/www.ncbi.nlm.nih.gov/entrez/query/static/help/pmhelp.html#Stopwords) were removed.</figDesc><table coords="8,55.44,306.81,484.49,33.76"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>All</cell></row><row><cell>characters</cell><cell>were</cell><cell>made</cell><cell>lowercase</cell><cell>and</cell><cell>stopwords</cell><cell>taken</cell><cell>from</cell><cell>the</cell><cell>MEDLINE</cell><cell>stopword</cell><cell>list</cell></row><row><cell>(http:/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,55.44,543.98,491.05,180.43"><head>Table 2 :</head><label>2</label><figDesc>Mean Average Precision and Recall for each Ranking Strategy</figDesc><table coords="9,55.44,555.32,491.05,169.09"><row><cell>Ranking</cell><cell cols="3">Mean Average Precison Recall (over the 5 queries) Ranking</cell><cell cols="2">Mean Average Precison Recall (over the 5 queries)</cell></row><row><cell>Strategy</cell><cell></cell><cell></cell><cell>Strategy</cell><cell></cell><cell></cell></row><row><cell>m1</cell><cell>0.0201</cell><cell>64.5%</cell><cell>m1_ma_tn1</cell><cell>0.0205</cell><cell>60.2%</cell></row><row><cell>m2</cell><cell>0.0011</cell><cell>41.9%</cell><cell>m1_ma_tn2</cell><cell>0.0189</cell><cell>66.7%</cell></row><row><cell>tn1</cell><cell>0.0044</cell><cell>48.4%</cell><cell>m2_ma_tn1</cell><cell>0.0100</cell><cell>48.4%</cell></row><row><cell>tn2</cell><cell>0.0035</cell><cell>37.6%</cell><cell>m2_ma_tn2</cell><cell>0.0086</cell><cell>43%</cell></row><row><cell>w1</cell><cell>0.0035</cell><cell>49.5%</cell><cell>m1_ma_w1</cell><cell>0.0206</cell><cell>59.1%</cell></row><row><cell>w2</cell><cell>0.0026</cell><cell>38.7%</cell><cell>m1_ma_w2</cell><cell>0.0199</cell><cell>64.5%</cell></row><row><cell>m1_tn1</cell><cell>0.0201</cell><cell>59.1%</cell><cell>m2_ma_w1</cell><cell>0.0092</cell><cell>49.5%</cell></row><row><cell>m1_tn2</cell><cell>0.0182</cell><cell>66.7%</cell><cell>m2_ma_w2</cell><cell>0.0075</cell><cell>45.2%</cell></row><row><cell>m2_tn1</cell><cell>0.0065</cell><cell>47.3%</cell><cell>Ma</cell><cell>0.0356</cell><cell>72%</cell></row><row><cell>m2_tn2</cell><cell>0.0055</cell><cell>39.8%</cell><cell>ma_tn1</cell><cell>0.0215</cell><cell>48.4%</cell></row><row><cell>m1_w1</cell><cell>0.0202</cell><cell>59.1%</cell><cell>ma_tn2</cell><cell>0.0221</cell><cell>49.5%</cell></row><row><cell>m1_w2</cell><cell>0.0190</cell><cell>62.4%</cell><cell>ma_w1</cell><cell>0.0222</cell><cell>41.9%</cell></row><row><cell>m2_w1</cell><cell>0.0057</cell><cell>45.2%</cell><cell>ma_w2</cell><cell>0.0226</cell><cell>41.9%</cell></row><row><cell>m2_w2</cell><cell>0.0045</cell><cell>37.6%</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,70.02,102.63,63.08,8.50"><head>Table 2 (</head><label>2</label><figDesc>above)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,67.08,294.56,313.02,125.80"><head>Table 3 :</head><label>3</label><figDesc>Task1 -Results , Relevance and Novelty</figDesc><table coords="13,67.08,411.87,22.15,8.50"><row><cell>Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,201.84,459.38,191.66,7.88"><head>Table 4 :</head><label>4</label><figDesc>Task2 Results , Precision, Recall and F-score</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,54.84,318.92,478.18,392.89"><head>Table 5 ,</head><label>5</label><figDesc>Task1 Performance of the system over Event and Opinion Topics</figDesc><table coords="13,54.84,318.92,478.18,392.89"><row><cell>Task1 Relevance</cell><cell>Run</cell><cell></cell><cell cols="4">Precision Recall F-Score</cell><cell cols="2">Task1 Novelty Precision</cell><cell>Recall</cell><cell>F-Score</cell></row><row><cell>Average(mean)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.388</cell><cell></cell><cell></cell><cell>0.203</cell></row><row><cell>CnQueryDocExpSim</cell><cell cols="2">cdvp4CnS101</cell><cell>0.30</cell><cell>0.73</cell><cell>0.402</cell><cell></cell><cell></cell><cell>0.13</cell><cell>0.71</cell><cell>0.201</cell></row><row><cell cols="3">QeProxnoDocExpUniqclr cdvp4QePnD2</cell><cell>0.31</cell><cell>0.71</cell><cell>0.406</cell><cell></cell><cell></cell><cell>0.13</cell><cell>0.70</cell><cell>0.205</cell></row><row><cell>CnQueryUniqHistory</cell><cell cols="2">cdvp4CnQry2</cell><cell>0.32</cell><cell>0.67</cell><cell>0.405</cell><cell></cell><cell></cell><cell>0.15</cell><cell>0.55</cell><cell>0.221</cell></row><row><cell cols="4">QeProxDocExpProxUnqH cdvp4QePDPC2 0.29</cell><cell>0.78</cell><cell>0.397</cell><cell></cell><cell></cell><cell>0.14</cell><cell>0.63</cell><cell>0.212</cell></row><row><cell cols="3">QeSnoDocExpNewSenVal cdvp4QeSnD1</cell><cell>0.31</cell><cell>0.72</cell><cell>0.404</cell><cell></cell><cell></cell><cell>0.14</cell><cell>0.64</cell><cell>0.216</cell></row><row><cell></cell><cell cols="2">Task2 Novelty</cell><cell></cell><cell>Run</cell><cell></cell><cell cols="2">Precision</cell><cell>Recall F-Score</cell></row><row><cell></cell><cell cols="2">Average(mean)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.597</cell></row><row><cell></cell><cell cols="3">ImportanceValue &gt;1.5</cell><cell cols="3">cdvp4NTerFr1 0.49</cell><cell></cell><cell>0.90</cell><cell>0.622</cell></row><row><cell></cell><cell cols="3">ImportanceValue &gt;3.5</cell><cell cols="3">cdvp4NTerFr3 0.51</cell><cell></cell><cell>0.83</cell><cell>0.616</cell></row><row><cell></cell><cell cols="5">NewSentenceValue&gt;0.08 cdvp4NSen4</cell><cell>0.47</cell><cell></cell><cell>0.91</cell><cell>0.609</cell></row><row><cell></cell><cell cols="2">UniqueHistory &gt;3</cell><cell></cell><cell cols="2">cdvp4UnHis3</cell><cell>0.50</cell><cell></cell><cell>0.84</cell><cell>0.615</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell cols="3">cdvp4NSnoH4 0.38</cell><cell></cell><cell>0.49</cell><cell>0.383</cell></row><row><cell cols="2">Task1 Relevance</cell><cell>Run</cell><cell></cell><cell cols="3">Events Opinion</cell><cell></cell><cell>Task1 Novelty Event</cell><cell>Opinion</cell></row><row><cell>Average(mean)</cell><cell></cell><cell></cell><cell></cell><cell>0.472</cell><cell cols="2">0.304</cell><cell></cell><cell>0.251</cell><cell>0.148</cell></row><row><cell cols="2">CnQueryDocExpSim</cell><cell cols="2">cdvp4CnS101</cell><cell>0.473</cell><cell cols="2">0.330</cell><cell></cell><cell>0.248</cell><cell>0.153</cell></row><row><cell cols="4">QeProxnoDocExpUniqclr cdvp4QePnD2</cell><cell>0.477</cell><cell cols="2">0.335</cell><cell></cell><cell>0.252</cell><cell>0.158</cell></row><row><cell cols="2">CnQueryUniqHistory</cell><cell cols="2">cdvp4CnQry2</cell><cell>0.472</cell><cell cols="2">0.339</cell><cell></cell><cell>0.267</cell><cell>0.176</cell></row><row><cell cols="5">QeProxDocExpProxUnqH cdvp4QePDPC2 0.471</cell><cell cols="2">0.322</cell><cell></cell><cell>0.263</cell><cell>0.161</cell></row><row><cell cols="4">QeSnoDocExpNewSenVal cdvp4QeSnD1</cell><cell>0.475</cell><cell cols="2">0.333</cell><cell></cell><cell>0.266</cell><cell>0.167</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="14,55.44,139.40,371.61,139.11"><head>Table 6 ,</head><label>6</label><figDesc>Task2 Performance of the system over Event and Opinion Topics 6.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,60.54,742.76,431.33,7.63"><p>The documents were distributed to the four leaf nodes without any examination of their contents for optimisation purposes.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The support of the <rs type="funder">Informatics Directorate of Enterprise Ireland</rs> is gratefully acknowledged. Part of this material is based on work supported by <rs type="funder">Science Foundation Ireland</rs> under Grant Nos. <rs type="grantNumber">03/IN.3/I361</rs>. The work described in section 4 is part of project GenIRL that is funded by <rs type="funder">Enterprise Ireland</rs> under the <rs type="grantName">Basic Research Grants Scheme</rs>, project number <rs type="grantNumber">SC-2003-0047-Y</rs>. We would like to thank our biology domain expert <rs type="person">Fionnuala McDyer</rs> for helping us by manually annotating each topic with terms from the MeSH controlled thesaurus.</p></div>
<div><head>7.</head></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CaNv7Gn">
					<idno type="grant-number">03/IN.3/I361</idno>
				</org>
				<org type="funding" xml:id="_kW5xFMV">
					<idno type="grant-number">SC-2003-0047-Y</idno>
					<orgName type="grant-name">Basic Research Grants Scheme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Srinivasan P <ref type="bibr" coords="14,101.68,632.36,21.84,7.63">(1996)</ref>. Query Expansion and MEDLINE. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,55.44,409.22,474.95,7.63;14,72.96,419.36,289.32,7.63" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,300.52,409.22,222.98,7.63">On the use of MeSH headings to improve retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sodring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,82.49,419.36,148.34,7.63">Proceedings of the 12th TREC Conference</title>
		<meeting>the 12th TREC Conference<address><addrLine>Gaithersburg, Md</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-11">2003. November 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,446.96,448.74,7.88;14,72.96,457.34,90.21,7.63" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,176.78,446.96,278.06,7.88">Finding Gene Function using LitMiner. The Twelfth Text REtrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">De</forename><surname>Bruijn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,460.88,446.96,21.42,7.88">TREC</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<pubPlace>Gaithersburg, MD. NIST</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,482.60,484.42,7.63;14,66.96,493.28,53.27,7.63" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,187.94,482.60,146.07,7.63">Overview of the TREC 2003 Web Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,340.84,482.60,195.15,7.63">Proceedings of the Twelfth Text REtrieval Conference</title>
		<meeting>the Twelfth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="78" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,514.70,484.44,7.63;14,66.96,525.38,430.69,7.63" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,274.39,514.70,265.49,7.63;14,66.96,525.38,22.31,7.63">Further Experiments on Collaborative Ranking in Community-Based Web Search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Freyne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coyle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Balfe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Briggs</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,107.15,525.38,282.38,7.63">Artificial Intelligence Review: An International Science and Engineering Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="252" />
			<date type="published" when="2004-06">2004. June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,546.50,451.11,7.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,170.97,546.74,121.67,7.63">Indexing consistency in MEDLINE</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Reid C A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,300.06,546.50,150.94,7.88">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="183" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,568.16,484.45,7.63;14,66.96,578.84,238.12,7.63" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,237.32,568.16,302.58,7.63;14,66.96,578.84,36.87,7.63">THUIR at TREC 2003: Novelty, Robust, Web and Hard, State Key Lab of Intelligent tech &amp; Sys</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">100084</biblScope>
			<pubPlace>Beijing; China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CST Dept, Tsinghua University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,55.44,600.26,484.53,7.63;14,66.96,610.94,106.11,7.63" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="14,325.35,600.26,214.62,7.63;14,66.96,610.94,38.65,7.63">Okapi at TREC-3. Proceedings of the Third Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1994">2004. 1994</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
