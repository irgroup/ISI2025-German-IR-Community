<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,204.00,82.36,187.32,14.42">THUIR at TREC 2004: QA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.04,107.97,28.09,8.10"><forename type="first">Wei</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.32,107.97,46.23,8.10"><forename type="first">Qunxiu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.09,107.97,48.23,8.10"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,204.00,82.36,187.32,14.42">THUIR at TREC 2004: QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7EA57A5869E616A4AE813D781C33088D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe ideas and related experiments of Tsinghua University IR group in TREC 2004 QA track. In this track, our system consists three components: Question analysis, Information retrieval, and Answer extraction. Question analysis component extracts Query Term and answer type. Information retrieval component retrieves candidate documents from index set based on paragraph level and re-ranks them to find more relevant documents. And then Answer extraction component matches empirical phrases according to answer type to extract final answer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>TREC introduced the first question answering(QA) track in <ref type="bibr" coords="1,371.35,325.95,61.92,9.45">TREC-8(1999)</ref>.The goal of the track is to foster research on systems that retrieve answers rather than documents in response to a question, with particular emphasis on systems that can function in unrestricted domains. The tasks in the track have evolved over the years to focus research on particular aspects of the problem deemed important to improving the state-of-the-art.</p><p>Compared to TREC 2003, TREC 2004 QA track has done some changes: The TREC 2004 QA track consists of a single task that is a combination of factoid, list, and definition-like questions. There are 65 targets of a definition. For each target, there are a series of factoid and list questions that relate to that target. The final question in each series is an explicit "other" question that should be interpreted as "tell me other interesting things about this target I didn't know enough to ask directly". This final question is roughly equivalent to the TREC 2003 QA track's definition question.</p><p>Section 2 describes the overview of THUIR QA system. The system consists of three components: Question analysis, Information retrieval, Answer extraction, as detailed in section 3, 4 and 5. Section 6 presents the final result and evaluation. Finally, section 7 describes the discussion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of THUIR Question Answering System</head><p>Similar to other systems, our system consists of three components: Question analysis, Information retrieval, and Answer extraction. The architecture is illustrated by  The Question Analysis component processes user questions and extracts useful information: Query Term and Answer Type. Ranked candidate documents are retrieved by Information Retrieval component from indexed documents set. Each candidate document is assigned with a rank score according to the similarity between candidate document and the Query Term. Then the top-ranked documents are processed by Answer Extraction component, and the final answer can be extracted with the Answer Type and some empirical feature matching strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Question Analysis</head><p>In contrast with several years ago, questions in TREC 2004 QA track are not independent, but context-sensitive, such as there are 65 targets, and each target consist a topic and several questions correlated to the topic. In some questions, some topic words are replaced by pronoun with the context of the question and the topic. So, it is necessary to consider of the relationship between the pronouns of questions and the topic words when questions are analyzed. In our system, pronouns reference to the topic are replaced by topic words and extracted as a Query Term.</p><p>Answer Type of questions is classified according to the interrogative word and the sentence structure, listed in Table <ref type="table" coords="2,193.75,318.15,3.95,9.45" target="#tab_1">3</ref>   , PLACE, METHOD and REASON type are easy to be classified by interrogative word, so these types are not subdivided. Imperative sentences are generally treated as MATTER type. Because the questions just as "who is person-name" type are considered to ask the position of the "person-name", PERSON type need to be subdivided into Name and Position sub-type. Questions like "how + adj/adv" type are mostly answered by number, and then with the difference of adj/adv NUMBER type is subdivided into several sub-type such as duration, age, size, money, and so on. Among the questions beginning with "what", questions as "what is/was NP" type are assigned to Definition sub-type, questions in which "what" is object are assigned to Object sub-type, and questions as "what + NP" type is assigned to What+NP sub-type.</p><p>NP chunk is extracted and NE (Named Entity) is recognized by GATE from questions, and every elements of question are experientially assigned a weight such as NE and number is 10, noun is 5, notional verb is 3, adjective and adverb is 1, and auxiliary verb, interrogative word and punctuation is 0. In addition, the topic word of the target, which questions belong to, is treated as Query Term and assigned weight 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Information Retrieval</head><p>The document set uses documents on the AQUAINT disk set just as the past two QA tracks. Each document is divided several paragraphs, and the index is built to each paragraph. Furthermore, another index is built to "Headline" of each document for answer extraction of questions of "Other" type. Index built and information retrieval uses the Lemur of CMU, retrieval model is simple tfidf model, and top-ranked 1000 documents are candidate documents.</p><p>And then candidate documents are re-ranked by the formula given below:</p><p>;   When all query terms in documents centralize, the d Score is close to 1. We think that the answer may be extracted from the sentence in which more query terms centralize. So we use the linear sum of these three score as re-rank score. In our experiments, linear coefficients are all 1/3. After re-ranked, the more relevant documents can be ranked ahead.</p><formula xml:id="formula_1" coords="3,127.14,382.66,351.51,183.82">α β γ = ⋅ + ⋅ + ⋅ = ⎧ ⎪ = ⎨ ⎪ ⎩ = - - 2 1 2 (4.4)<label>( 1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Answer Extraction</head><p>First of all, candidate documents are processed by a named entity finder (GATE), which recognizes NE nouns such as person name, location, organization, date, time, money, etc. Different strategy of answer extraction is adapted to different answer type.</p><p>1) For answer type as PERSON-name, TIME, and PLACE, answer can be extracted from corresponding named entity nouns. 2) For NUMBER type, sub-type decides the answer extraction method. For "age" sub-type, phrase "number + years + old" is matched in documents; For "duration" sub-type, phrase "number + time noun" is expected as an answer; Other sub-type is just similarly processed to match some empirical phrase.</p><p>3) MATTER type is processed as NUMBER type. VP is extracted from questions, phrase " s NP ＋VP " is matched in documents, and then the object of matched sentence is expected as answer. For "what＋NP" sub-type, phrase "NP + VP" is matched, where VP is verb phrase of questions. The modifier of NP and NP is extracted as an answer.</p><p>If there are several matched phrases, each phrase is assigned a weight score according to the distance between the phrase position and central position of all Query term in documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>In TREC2004 QA track, we submitted one run, and here is evaluation result:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Future Work</head><p>This is the first time that Tsinghua University IR group (THUIR) participates in TREC QA track. The Evaluation shows that our system and method need be improved. We still have a lot of things to do in the future. Question analysis component needs not only syntax analysis but semantic one of questions to a certain extent. Re-rank strategy of Information retrieval component will be mended so that the more relevant documents can be ranked more ahead. Answer extraction component exists very great shortcoming because of empirical phrase extraction strategy. For some questions, the top-ranked documents contain answers, however our answer extraction strategy can not find them. So, we need to research more in answer extraction and build new extraction strategy in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,421.13,630.15,45.77,9.45"><head></head><label></label><figDesc>Figure 2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="1,201.24,755.55,192.83,8.10;1,187.14,642.54,220.92,106.98"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Architecture of the THUIR QA System</figDesc><graphic coords="1,187.14,642.54,220.92,106.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,248.44,555.69,5.42,10.80;3,213.90,521.64,3.47,6.24;3,211.14,545.22,1.93,6.24;3,288.36,562.26,10.04,6.24;3,349.32,546.33,49.45,10.80;3,222.18,555.69,6.00,10.80;3,255.60,555.69,33.36,10.80;3,213.60,542.99,3.80,8.49;3,136.56,498.54,5.93,14.68;3,136.56,510.06,5.93,14.68;3,136.56,518.28,5.93,14.68;3,136.56,527.28,5.93,14.68;3,136.56,538.74,5.93,14.68;3,136.56,550.26,5.93,14.68;3,230.28,551.82,6.57,14.68;3,136.56,555.96,5.93,14.68;3,209.22,523.24,12.81,22.02;3,107.94,589.89,30.65,9.45;3,172.80,595.44,18.38,6.24;3,145.80,588.86,27.29,10.80;3,199.08,589.89,264.12,9.45;3,497.28,595.48,5.42,6.27;3,470.28,588.89,37.66,10.85;3,119.28,626.64,13.54,6.24;3,91.98,620.03,27.31,10.85;3,135.00,621.09,21.01,9.45;3,190.20,626.64,3.47,6.24;3,163.20,620.03,35.63,10.85;3,233.10,626.68,5.42,6.27;3,206.04,620.09,299.29,10.85;3,90.00,652.29,58.68,9.45"><head></head><label></label><figDesc>of documents re-ranked, and it consists three parts: rate of the number of Query Term in documents and total one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,200.28,657.88,3.48,6.27;3,155.28,651.29,209.41,10.85;3,416.34,657.84,9.98,6.24;3,371.34,651.23,44.74,10.85;3,434.16,652.29,71.15,9.45;3,90.00,676.95,415.45,9.45;3,90.00,699.63,202.10,9.45;3,326.52,705.18,13.54,6.24;3,299.22,698.57,27.31,10.85;3,342.24,699.63,88.89,9.45;3,465.66,705.18,13.54,6.24;3,438.36,698.57,27.31,10.85;3,486.66,699.63,18.65,9.45;3,90.00,730.83,43.47,9.45;3,168.00,736.38,13.54,6.24;3,140.70,729.77,27.31,10.85;3,189.00,730.83,54.44,9.45;3,304.02,736.38,9.98,6.24;3,251.04,729.77,254.28,10.85;4,90.00,82.89,15.16,9.45;4,165.78,88.48,3.48,6.27;4,112.80,81.89,249.10,10.85;4,413.52,88.44,9.98,6.24;4,368.58,81.83,136.69,10.85;4,90.00,114.63,176.18,9.45;4,300.30,120.18,3.47,6.24;4,273.30,113.57,231.99,10.85;4,90.00,145.83,317.20,9.45;4,441.36,151.38,3.47,6.24;4,414.42,144.77,90.95,10.85;4,90.00,177.03,193.71,9.45;4,313.08,182.58,1.93,6.24;4,291.00,175.97,214.27,10.85;4,90.00,208.23,48.12,9.45;4,167.82,213.78,14.92,6.24;4,145.50,207.17,21.98,10.85;4,190.62,208.23,168.58,9.45;4,399.36,213.78,9.98,6.24;4,366.60,207.17,33.39,10.85;4,416.70,208.23,88.62,9.45;4,90.00,232.35,44.03,9.45;4,107.94,254.49,95.55,9.45;4,237.66,260.08,5.42,6.27;4,210.66,253.49,37.66,10.85;4,282.84,260.04,13.54,6.24;4,255.54,253.43,27.31,10.85;4,298.56,254.49,21.28,9.45;4,354.06,260.04,3.47,6.24;4,327.12,253.43,178.22,10.85;4,90.00,285.69,283.45,9.45;4,407.58,291.28,5.42,6.27;4,380.58,284.69,124.73,10.85;4,90.00,316.89,250.12,9.45;4,374.58,322.44,13.54,6.24;4,347.28,315.83,27.31,10.85;4,395.58,316.89,64.79,9.45;4,494.58,322.48,5.42,6.27;4,467.58,315.89,37.67,10.85"><head>qFrom</head><label></label><figDesc>QTCount is total number of Query Term, and doc QTCount is the number of the Query Term existing in documents. If existing, the Query Term counts once. The rate of proper nouns extracted from questions is presented by NNP Score . If no proper nouns, of doc NNPCount , the number of proper nouns in documents, and q NNPCount , total number of proper nouns, and just as doc QTCount , if a proper noun exist in documents, it only counts once. d Score is the density of Query Term distributing in documents. If the number of Query Term in documents is no more than 1, d Score is 0. n is the total number of Query Term in documents, i POS is the position of the ith Query Term in documents, mean POS is the mean of all Query Terms, and doc Length is the length of the document. three score is just between 0 and 1. The more the Query Terms appear in documents, the closer qt Score is to 1, and then we consider these documents are more relevant to questions. NNP Score is just same as qt Score .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,347.88,611.58,3.47,6.24;4,334.38,604.97,170.94,10.85;4,111.00,637.23,223.96,9.45;4,350.82,642.78,3.47,6.24;4,337.26,635.38,55.66,11.63;4,408.78,642.78,3.47,6.24;4,395.28,635.38,110.06,11.63;4,111.00,668.43,292.89,9.45;4,425.04,673.98,2.70,6.24;4,411.48,667.37,93.83,10.85"><head>d</head><label></label><figDesc>NP is noun phrase to be defined in question of "definition" sub-type, and then phrase " d NP ＋be", " d NP ， known as", etc is matched in documents. For "object" sub-type, subject noun phrase s NP and verb phrase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,230.52,705.15,134.21,8.10"><head>Table 3 .1: Answer Type of question</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,221.22,211.53,152.88,8.10"><head>Table 6 .1: The TREC Evaluation Result</head><label>6</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,94.38,444.80,410.88,9.71;5,111.00,460.40,220.78,9.71" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,199.60,444.80,285.95,9.71">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Voorhees</forename><surname>Ellen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,111.00,460.40,52.55,9.71">TREC 2003</title>
		<meeting><address><addrLine>Gaithersburg, Maryland. NIST</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,94.38,476.00,410.99,9.71;5,111.00,491.60,394.25,9.71;5,111.00,507.20,89.84,9.71" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,320.14,476.00,185.23,9.71;5,111.00,491.60,155.57,9.71">The Use of Predictive Annotation for Question Answering in TREC8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,292.79,491.60,212.46,9.71;5,111.00,507.20,55.14,9.71">proceedings of the Eighth Text Retrieval Conference</title>
		<meeting>the Eighth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,94.38,522.80,410.91,9.71;5,111.00,538.40,351.76,9.71" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,328.11,522.80,177.18,9.71;5,111.00,538.40,44.88,9.71">FDU at TREC-9: CLIR, Filtering and QA Tasks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,176.41,538.40,251.65,9.71">proceedings of the ninth Text Retrieval Conference</title>
		<meeting>the ninth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,94.38,554.00,410.98,9.71;5,111.00,569.60,394.27,9.71;5,111.00,585.20,394.26,9.71;5,111.00,600.80,300.99,9.71" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,382.16,554.00,123.20,9.71;5,111.00,569.60,389.21,9.71">GATE: A Framework and Graphical Development environment for Robust NLP Tools and Applications</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,111.00,585.20,394.26,9.71;5,111.00,600.80,170.08,9.71">Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL&apos;02</title>
		<meeting>the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL&apos;02<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
