<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.43,112.60,312.42,18.35;1,156.59,137.51,282.11,18.35">Conceptual Language Models for Context-Aware Text Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,206.48,177.92,62.13,8.74"><forename type="first">Henning</forename><surname>Rode</surname></persName>
							<email>h.rode@cs.utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.04,177.92,72.76,8.74"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<email>d.hiemstra@cs.utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.43,112.60,312.42,18.35;1,156.59,137.51,282.11,18.35">Conceptual Language Models for Context-Aware Text Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5485552729A070D0041B5E291865A2E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Contextual Information Retrieval</term>
					<term>Context Modeling</term>
					<term>Language Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While participating in the HARD track our first question was, what an IR-application should look like that takes into account preference meta-data from the user, without the need of explicit (manual) meta-data tagging of the collection. Especially, we touch the question how contextual information can be described in an abstract model appropriate for the IR-task, which further allows improving and fine-tuning of the context representations by learning from the user. As a first result, we roughly sketch a system architecture and context representation based on statistical language models that fits well to the task of the HARD track. Furthermore, we discuss issues of ranking and score normalizations on this background.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Observing that humans are thinking about, searching for and working with information highly depending on their current (working) context, leads directly to the hypothesis that information systems could increase their performance by learning how to deal with such contextual information. Among other ongoing research projects the HARD track is trying to tackle these issues. It especially addresses the question, how already available information about the user's context can be employed effectively to gain highly precise search results.</p><p>A user's information need is only vaguely described by the typical short query, the user states him-/herself to the system. There are at least two reasons for this lack of input precision. First of all, users who search for a certain piece of information have only a limited knowledge about it themselves. The difficulty to describe it is thus an immanent problem of any information need and hardly to overcome. A second reason for insufficient query input, however, touches the area of context information and might in principle be easier to address. Although a humans' search context provides a lot of information about his/her specific information need, a searcher is often not able and not used to explicitly mention it to a system. Comparing the situation to question another human, the counterpart would be able to derive contextual information him-/herself.</p><p>In order to outline this paper, we start with an overview on context modeling in the area of information retrieval. The section thereafter will introduce our own approach of context modeling in more detail and sketch a context-aware retrieval system. We proceed further by taking a closer look at incorporating context information in ranking algorithms. Finally, the last section describes our HARD track experiments and presents some empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Context Modeling for Information Retrieval</head><p>Aiming at a context-aware text retrieval system, we first have to investigate how context can be modeled appropriately that an IR-system can take advantage of this information. One of the first upcoming matters will probably be described by the following question: Should we try to build a model for each individual user or should it classify the user with respect to user-independent predefined context-categories? Both kind of systems are outlined in Figure <ref type="figure" coords="2,142.77,297.31,3.88,8.74" target="#fig_0">1</ref>. We will choose here the second option, but first discuss the advantages and disadvantages of both by pointing to some related research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">User-Dependent Models</head><p>A first and typical example for this approach is shown by Diaz and Allan in <ref type="bibr" coords="2,200.08,394.65,9.97,8.74" target="#b4">[6]</ref>. The authors suggested to build a user preference language model from documents taken out of the browsing-history. Since the model reflects the browsing behavior of each individual user, it describes his/her preferences in a very specific way. However, humans work and search for information often in multitasking environments (see <ref type="bibr" coords="2,270.52,479.06,14.76,8.74" target="#b10">[11]</ref>). Thus, their information need changes frequently, or even overlaps between different tasks. A static profile of each user is not appropriate to take into account rapid contextual changes. For this reason, Diaz and Allan <ref type="bibr" coords="2,146.29,538.84,10.52,8.74" target="#b4">[6]</ref> also tested the more dynamic version of session models derived from the most recent visited documents only. With the same intention but following a more "exotic" approach, Bauer and Leake <ref type="bibr" coords="2,121.02,586.66,10.52,8.74" target="#b1">[3]</ref> introduced a genetic sieve algorithm, that filters out temporally frequent words occurring in a stream of documents, whereas it stays unaffected by longterm front-runners like stop words. The system is thus aware of sudden contextual changes, but cannot come up directly with sound models describing the new situation.</p><p>Summarizing the observations, individual user models enable the most user specific systems, but either lack a balanced and complete description or remain unaware of alternating contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">User-Independent Models</head><p>Although context itself is by definition userdependent, it is possible to approximately describe a specific situation by selecting best-matching predefined concepts, that are themselves independent of any specific user. A concept in this respect might range from a subject description (e.g. Music) to geographical and temporal information (e.g. Netherlands, 16th century).</p><p>Examples for this approach can be found among others in last years' HARD track. Along with the query, a set of meta-data items characterized the context of each specific information need. Since the meta-data was structured in categories restricted to certain set of pre-defined items, it was in theory possible to build stable and sound models to classify documents according to each of these concepts.</p><p>Following this approach of context modeling, it needs to be explained where the additional context meta-data comes from. Whereas Belkin et al. <ref type="bibr" coords="2,317.81,575.43,10.52,8.74" target="#b2">[4]</ref> preferred to think of it as derived by automatic context-detection from the users' behavior, He and Demner-Fushman <ref type="bibr" coords="2,432.50,599.34,10.52,8.74" target="#b6">[7]</ref> described the collecting of contextual information in a framework of explicit negotiation between the search-system and user. Further experiments in this area are presented in <ref type="bibr" coords="2,360.37,647.16,14.61,8.74" target="#b9">[10]</ref>. The authors tried to employ a conceptual hierarchy of subjects, as established by the "open directory project" <ref type="bibr" coords="2,429.43,671.07,10.52,8.74" target="#b0">[1]</ref> or "Yahoo" [2], as contextual models. In a first experiment, queries were compared to these concepts and the best-matching subjects were displayed to the user for explicit selection. In order to avoid this negotiation process, long-term user profiles were introduced for automatic derivation of matching subjects, which cluster the former interests of the user in suitable groups. However, these user-dependent models suffer from the same limitations as mentioned above.</p><p>Although the question is not answered satisfyingly, how automatic context detection can be performed, user-independent context modeling comes up with a good deal of advantages:</p><p>• Whereas user modeling suffers often from sparse data, conceptual models are trained by all users of the systems and therefore will become more balanced and complete.</p><p>• Conceptual models do not counteract search on topics entirely new to the user.</p><p>• Assuming a perfect context detection unit, the search system can react more flexible with respect to a changing context of a user.</p><p>• New users can search efficiently without the need to train their user preference models in advance.</p><p>• It is theoretically possible to switch back anytime from automatic context detection to a negotiation mode, which enables the user to control the system effectively.</p><p>Taking a closer look on conceptual context modeling, the first task will be to identify appropriate categories of the users situation with respect to the information retrieval task. Whereas we can call almost everything surrounding the user as context, we only need those data that allows to further specify the information need of the user. For instance, the HARD track comes with the categories familiarity, genre, subject, geography and related documents. We can easily extend this set by further categories like language or time/date of the desired information.</p><p>One might notice, that the chosen categories originate more from a document than from a user centered view. Since we want to fine-tune the retrieval process, it is handy to have categories that directly support the document search, however, starting from the users context, this already requires a first translation. For instance the situation of a biology scientist sitting at his work, might be translated to the following context categorization: familiarity with search-topic: high, search genre: scientific articles, general subject: biology.</p><p>The translation of the users situation into the desired context categorization is, of course, itself an error-prone process. Thus, the before-mentioned possibility allowing the user to explicitly edit the automatically performed categorization of his/her context might be an important issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conceptual Language Models</head><p>The retrieval process itself is enhanced by multiple text-categorizations based on the selected context models best-matching the users' situation. Thus, the maintained models for each context concept will be used by the system as classifiers, e.g. a model for scientific articles should be applicable to filter out scientific articles from an arbitrary set of documents.</p><p>Looking back at last years' HARD track experiments (e.g. at <ref type="bibr" coords="3,376.43,322.76,10.52,8.74" target="#b2">[4,</ref><ref type="bibr" coords="3,389.03,322.76,7.20,8.74" target="#b7">8]</ref>), every context category is handled with different techniques ranging from a set of simple heuristic rules as used for classifying the genre to applying external ill-founded though efficient algorithms like the "Fog-Index Measure" to rate the readability. The techniques might enable an IR-system to utilize the specific given metadata, but the approaches lack a uniform framework that enables extending the system to work with other meta-data categories as well.</p><p>Instead of introducing another set of new techniques, our basic hypothesis is that statistical language models are a sufficient mean to be applied as a universal representation for all context categories as long as they are used to support text retrieval. Obviously, language models can be utilized effectively as subject classifiers, but we think, it is also possible to use them to judge about the genre or readability of a document. In the latter case, we can for instance assume that easily readable articles will probably consist of common rather than of special terms. For geography models, on the other hand, we would expect a higher probability to see certain city names and persons, whereas genre models might contain often occurring specific verbs.</p><p>In order to make our text retrieval system context-aware, it is thus sufficient to enhance it by a set of language model classifiers for each context category. For the purpose of the HARD track, we assume a context detection unit is able to perform the translation process from a concrete user Figure <ref type="figure" coords="4,105.59,221.07,3.88,8.74">2</ref>: Context Modeling with Conceptual Language Models situation to a characteristic selection of conceptual models. The remaining task to perform all document classifications and to combine them for a final ranking according to the entire search topic will be addressed in the next section. Figure <ref type="figure" coords="4,248.05,312.88,4.98,8.74">2</ref> sketches roughly the described system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Application</head><p>Apparently, an IR-system working with conceptual models will profit from being a self-learning application. It might be possible to start the system with basic models for each category, but in order to have an easily extendable application, which enables to build new categories and models, it is beneficial to have a system that is able to train its models itself by the feedback of the user.</p><p>Anytime a user indicates (explictly or observed by his browsing behavior) that a certain document matches her/his information need, we can assume that it also matches the selected conceptual models. Therefore, the content of such a document can be used to train the context models. In the setting of the HARD track we will use the LDC annotation of the training topics to improve our models in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ranking Algorithms</head><p>Having language models at hand that describe the users context, we are able to classify the documents according to each single context category, but we need to come up with one final ranking including every single source of relevance evidence. There are basically three options to perform this task:</p><p>• Query Expansion or any kind of uniting of terms, taken from all context models, to build one large final query.</p><p>• Reranking or filtering of the results according to each classifier.</p><p>• Using combined ranking algorithms to aggregate the scores of single classifications.</p><p>Using query expansion techniques would lead to the difficult task to select a certain number of representative terms from each model. Since the query and "meta-query" models differ highly in length, we cannot simply unite all terms to one combined query. Filtering or reranking, if it is used in a more "aggressive" way, can be regarded as black-andwhite decisions for or against a document. However, thinking of several meta-query categories it is likely that a document is judged relevant by a user even if it does not match one of the associated classifiers. Therefore, we opt here for a combined ranking solution, which is comparable to "softer" reranking strategies and allows to consider each context-classification step adequately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Combined Ranking of Query and Meta-Query</head><p>For discussing the ranking of documents according to the query and meta-query we first introduce some common notation. Let the random variables Q, D denote the choice of a query, respectively document, and r/r mark the event, that D is regarded as relevant/not relevant. Further, M represents in our case the meta-query, consisting of several single models for each involved category M i :</p><formula xml:id="formula_0" coords="4,362.43,488.27,98.62,7.86">M = {M1, M2, . . . , Mn}.</formula><p>Using the odds of relevance as a basis, we can deduce it to probabilities we are able to estimate. Q and M are assumed to be independent given D and r. The prior document relevance P (r|D)/P (r|D) is dropped from the formula in the last step. We assume that there is not a-priori reason that a user would like one document over another, effectively making the prior document relevance constant in this case.</p><p>The simple derivation now allows to handle query and meta-query separately but in a similar manner. In terms of the user's information need we can regard Q and M as alternative incomplete and noisy query representations. Combining the resulting document rankings from both queries gathers different pieces of evidence about relevance and thus helps to improve retrieval effectiveness <ref type="bibr" coords="5,279.38,157.00,9.96,8.74" target="#b3">[5]</ref>.</p><p>The remaining probabilities can be estimated following the language modeling approach. D, Q and M are interpreted here as a sequence of terms, and the probability to "generate" a sequence X out of Y is estimated by the sum of the log likelihood probabilities of the terms occurring in X interpolated by a smoothing factor. We denote a maximum likelihood probability of a term t within a sequence of words X by P (t|X). According to Kraaij <ref type="bibr" coords="5,106.19,266.94,10.51,8.74" target="#b8">[9]</ref> the probabilities of the form P (X|t, r) where a term t is non-relevant for the generation of X can be estimated by the collection likelihood of the term. Combing both enables to determine the required relevance odds, also called the logarithmic likelihood ratio of a query Q given a document D: Here, C represents the entire collection and λ the smoothing factor to interpolate document and collection likelihood.</p><p>Since we want to relate the scores of the query and meta-query to each other, we have to ensure that their probability estimates deliver "compatible" values (see <ref type="bibr" coords="5,146.04,494.41,10.29,8.74" target="#b3">[5]</ref>). Especially query length normalization plays a crucial role in this case. Notice, that Q and M differ widely with respect to their length. Thus, a simple LLR-ranking would produce by far higher values when it is applied to the meta-query. Using NLLR instead, the query length normalized variant of the above measurement, helps to avoid score incompatibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ranking according to the Meta-Query</head><p>As mentioned above, we would like to rank documents according to query and meta-query in the same way. However, since M consists of several single language models M 1 , . . . , M n we need to take a closer look to this matter as well.</p><p>If M is substituted by M 1 , . . . , M n , the resulting formula can be factorized, given the independence of M 1 , . . . , M n : We introduced the factor 1 n in the last row as a second normalization step due to the number of involved meta-data models. Especially if the number n of models is growing, not only the overall score of the documents would increase, but also the entire meta-query would outweigh the original query in the final rank.</p><p>A last remark concerns the choice of the smoothing factor λ. In contrast to typical short queries, the role of smoothing is less important here, since we can assume that the model is a good representation of relevant documents and therefore contains most of their words itself. We thus argue to use a smaller value for λ here than in case of the query ranking to stress the selectivity of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>As apparent from the description of our system, our HARD track research focuses on the usage of the meta-data coming along with the search topics. We have neither tried to take advantage of clarification forms for relevance feedback nor have we applied our algorithms to perform retrieval on passage level. Furthermore, we abstained completely from techniques like stemming and stopword removal, since they might degrade the effectivity of classifiers, like genre language models.</p><p>Unfortunately, the submission deadline for the runs has met a quite early phase of our research. The submitted runs, though they performed quite well, do not reflect all considerations presented in this paper. In order to provide a consistent presentation, we decided to show only "post-track" runs in this section, which directly follow the given description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Collecting Data for the Models</head><p>For this years' experiments, we have used only a part of the meta-data that came along with the queries, namely the subject, geography and related text sections. Having appropriate models at hand is a crucial requirement for any kind of experiments The subject data was chosen, because it was considered to work best with respect to the purpose to classify texts. It is probably easier to identify sport articles by their typical vocabulary then to distinguish between genres. Geography data, on the contrary, can be regarded as a less typical domain for applying language model classifiers. And finally related text documents were used to demonstrate their straightforward integration in the proposed context modeling framework.</p><p>In order to construct language models for subject classification, we used three different sources of data: manual annotation, APE keywords, and the training data.</p><p>Firstly, we manually annotated 500 documents for each chosen subject among the queries, e.g. sports, health and technology. The 500 documents have been preselected by a simple query containing the subject term and additional terms found in a thesaurus. The aim of this step was to detect 150-200 relevant documents as a basic model representing its subject. For construction of a language model all terms occurring in those documents were simply united to build one large "vocabulary" and probability distribution.</p><p>Although the number of documents might look appropriate for building a basic text classifier, the way we gathered the documents cannot ensure the models to be unbiased. In order to further improve the models, we used the keyword annotation coming along with the documents. During the manual classification process we observed that the keyword section of documents from the Associated Press Newswire (APE) provide very useful hints and in many cases HARD subjects can easily be assigned to APE keywords. It seemed admissible from research perspective to employ this information as long as we restrict it to a small part of the corpus, in this case APE news only. However, since HARD subjects cannot be mapped one-to-one to APE keywords, our subject models differed considerably afterwards in length and quality. For the geography models, the link between query metadata and document keywords was easier to estab-lish. Therefore, the geography models highly profit by this technique.</p><p>In a last step, we automatically enhanced the models by data obtained from the annotated training topics as mentioned above. If any document was judged as relevant to a specific training query, this also means that the document matches all the meta-data constraints of that query. Thus, all relevant documents belonging to a query asking e.g. for sport articles, apparently are sport articles themselves, and can therefore be used to enrich the sport articles model.</p><p>Baseline Runs Every HARD track topic was specified by a title, description and topic-narrative section, which could be used for the baseline runs.</p><p>The easiest solution to compose a query of this data was thus to unite all occurring terms in the three sections to one query model. The queries of our first baseline run utwenteB111 were composed in this way. The second baseline run utwenteB21 reflected the more realistic situation of shorter user queries, combing only the title and description section. Since the number and expressiveness of terms in the title section differs from the topicdescription, we weighted title terms higher here by adding them two times to the query.</p><p>For both runs we computed a ranking according to NLLR(Q|D) as a basis for later comparisons with the meta-data runs. The two baseline runs gives us the possibility to examine the influence of the initial query length to improvements by context meta-data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-data</head><p>Runs Corresponding to the equally named baseline runs, utwenteM111 and utwenteM21 were calculated as shown in the last section and take into account subject, geography, and related texts as M 1 . . . M 3 . Table 1 gives an overview on the achieved average R-Precision of all runs. It shows first of all that our approach for handling contextual data is able to improve retrieval results, for soft as well as for hard relevance. We expected higher relative improvements when using context information together with short user queries, however, our  We performed further experiments to find out if the given context categories are equally useful for improving the system performance. Figure <ref type="figure" coords="7,287.68,346.61,4.98,8.74" target="#fig_5">3</ref> presents the resulting precision-recall graph if the queries are associated with only one specific metadata category. It considers short queries and hard relevance only. In order to get comparable results for all categories, we needed to restrict the evaluation to a small subset of 11 topics that came with geographical and subject requirements we could support with appropriate models. For instance, we dropped topics asking for the subject society, since the associated classifier was considered rather weak compared to others. Such a restriction is admissible, since we were interested in the retrieval improvements in the case appropriate models are available, however, the remaining topic set was unfortunately a relative small base for drawing strong conclusions.</p><p>The graph suggests that the utilization of geography and subject preferences allow small improvements whereas related texts considerably increase the retrieval performance. In fact, using related text information alone shows even better results than its combination with other meta-data. As a conclusion, it might be interesting to test in further experiments, if a more elaborated approach of combining the rankings according to each single meta-data category is able to correct such effects. The displayed graph shows further that the usage of contextual data especially enhances the precision at small levels of recall, which meets perfectly the "high accuracy" task of the track.</p><p>At last, the HARD track provided a very suitable environment for our research and we are looking forward to continue our experiments on other context categories and with different ranking functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,176.79,253.53,241.70,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Context Modeling: User vs. Category Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,347.69,555.54,53.98,7.86;4,347.69,567.50,53.98,7.86;4,407.98,561.42,7.16,7.86;4,421.46,555.54,81.43,7.86;4,421.46,567.50,81.43,7.86;4,329.36,586.33,7.16,7.86;4,347.69,580.45,130.22,7.86;4,347.69,592.41,130.22,7.86;4,329.36,611.24,7.16,7.86;4,347.69,605.36,40.53,7.86;4,347.69,617.32,40.53,7.86;4,391.45,611.24,4.61,7.86;4,399.31,605.36,43.13,7.86;4,399.31,617.32,43.13,7.86"><head>P</head><label></label><figDesc>(r|Q, M, D) P (r|Q, M, D) = P (Q, M, D|r) * P (r) P (Q, M, D|r) * P (r) = P (Q|D, r) * P (M |D, r) * P (r|D) P (Q|D, r) * P (M |D, r) * P (r|D) ∝ P (Q|D, r) P (Q|D, r) * P (M |D, r) P (M |D, r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,103.69,349.07,11.77,7.86;5,125.66,343.19,40.53,7.86;5,125.66,355.15,40.53,7.86;5,86.56,376.96,7.16,7.86;5,104.08,390.71,13.61,5.24;5,119.62,376.96,50.02,7.86;5,179.84,371.08,100.56,7.86;5,216.40,383.04,27.44,7.86;5,86.56,401.54,62.83,7.86"><head></head><label></label><figDesc>log P (Q|D, r) P (Q|D, r) = t∈Q |t in Q| * log (1 -λ)P (t|D) + λP (t|C) P (t|C) = LLR(Q|D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,341.74,100.33,11.77,7.86;5,363.71,94.45,80.79,7.86;5,363.71,106.41,80.79,7.86;5,324.61,128.22,28.91,7.86;5,363.71,122.34,46.31,7.86;5,363.71,134.30,46.31,7.86;5,413.26,128.22,24.06,7.86;5,440.57,122.34,47.26,7.86;5,440.57,134.30,47.26,7.86;5,324.61,153.50,7.16,7.86;5,343.41,147.70,4.61,7.86;5,342.94,159.58,5.56,7.86;5,351.23,153.50,157.61,7.86"><head></head><label></label><figDesc>log P (M1, . . . , Mn|D, r) P (M1, . . . , Mn|D, r) = log P (M1|D, r) P (M1|D, r) * . . . * P (Mn|D, r) P (Mn|D, r) ∝ 1 n (NLLR(M1|D) + . . . + NLLR(Mn|D)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,74.41,238.04,218.25,8.74;7,74.41,250.00,103.53,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing Precision/Recall for each single Meta-data Category</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,74.41,72.75,393.72,93.53"><head>Table 1 :</head><label>1</label><figDesc>R-Precision for Baseline and Meta-data Runs and the need to construct them ourselves has led to this limitation.</figDesc><table coords="6,127.15,72.75,340.98,29.93"><row><cell></cell><cell cols="4">utwenteB21 utwenteM21 utwenteB111 utwenteM111</cell></row><row><cell>R-Precision (Hard)</cell><cell>0.267</cell><cell>0.289</cell><cell>0.294</cell><cell>0.349</cell></row><row><cell cols="2">R-Precision (Hard+Soft) 0.268</cell><cell>0.308</cell><cell>0.308</cell><cell>0.366</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,323.10,195.76,197.77,8.74;7,323.10,207.72,192.93,9.02" xml:id="b0">
	<monogr>
		<ptr target="http://www.dmoz.org" />
		<title level="m" coord="7,323.10,195.76,197.77,8.74;7,323.10,207.72,81.66,8.74">Open Directory Project. Netscape Communication Cooperation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.10,260.79,197.77,8.74;7,323.10,272.75,197.78,8.74;7,323.10,284.70,197.77,8.74;7,323.10,296.66,197.77,8.74;7,323.10,308.61,197.76,8.74;7,323.10,320.57,170.23,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,446.90,260.79,73.97,8.74;7,323.10,272.75,197.78,8.74;7,323.10,284.70,28.04,8.74">Real Time User Context Modeling for Information Retrieval Agents</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Leake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,382.84,284.70,138.03,8.74;7,323.10,296.66,197.77,8.74;7,323.10,308.61,142.38,8.74">Proceedings of the 2001 ACM CIKM International Conference on Information and Knowledge Management</title>
		<meeting>the 2001 ACM CIKM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="568" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.10,341.13,197.77,8.74;7,323.10,353.08,197.77,8.74;7,323.10,365.04,197.77,8.74;7,323.10,376.99,197.77,8.74;7,323.10,388.95,160.72,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,426.51,341.13,94.36,8.74;7,323.10,353.08,197.77,8.74;7,323.10,365.04,18.16,8.74">Rutgers&apos; HARD and Web Interactive Track Experiences at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,370.89,365.04,149.98,8.74;7,323.10,376.99,93.37,8.74">The Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="418" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.10,409.51,197.77,8.74;7,323.10,421.46,197.77,8.74;7,323.10,433.42,197.77,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,375.48,409.51,145.39,8.74;7,323.10,421.46,74.00,8.74">Combining Approaches to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,505.31,421.46,15.55,8.74;7,323.10,433.42,138.79,8.74">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Croft</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.10,477.89,87.89,8.74;7,426.34,477.89,94.53,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,426.34,477.89,94.53,8.74">Browsing-based User</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.10,489.84,197.78,8.74;7,323.10,501.80,197.76,8.74;7,323.10,513.75,72.67,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,323.10,489.84,193.58,8.74">Language Models for Information Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>CIIR University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="7,323.10,534.31,197.77,8.74;7,323.10,546.27,197.77,8.74;7,323.10,558.22,197.77,8.74;7,323.10,570.18,197.77,8.74;7,323.10,582.13,197.77,8.74;7,323.10,594.09,97.44,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,469.59,534.31,51.28,8.74;7,323.10,546.27,197.77,8.74;7,323.10,558.22,154.43,8.74">HARD Experiment at Maryland: from Need Negotiation to Automated HARD Process</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,504.06,558.22,16.81,8.74;7,323.10,570.18,197.77,8.74;7,323.10,582.13,22.68,8.74">The Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="707" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.09,614.65,197.78,8.74;7,323.10,626.60,197.77,8.74;7,323.10,638.56,197.77,8.74;7,323.10,650.52,160.72,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,402.18,614.65,118.69,8.74;7,323.10,626.60,31.50,8.74">UMass at Trec2003: HARD and QA</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,378.00,626.60,142.87,8.74;7,323.10,638.56,93.37,8.74">The Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="715" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,323.09,671.07,197.77,8.74;7,323.10,683.03,197.76,8.74;7,323.10,694.98,162.89,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,376.76,671.07,144.10,8.74;7,323.10,683.03,106.84,8.74">Variations on language modeling for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="8,94.89,73.32,197.77,8.74;8,94.89,85.27,197.77,8.74;8,94.89,97.23,197.77,8.74;8,94.89,109.18,197.78,8.74;8,94.89,121.14,197.77,8.74;8,94.89,133.09,90.22,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,254.97,73.32,37.69,8.74;8,94.89,85.27,197.77,8.74;8,94.89,97.23,148.16,8.74">Inferring User&apos;s Information Context: Integrating User Profiles and Concept Hierarchies</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sieg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mobasher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,273.55,97.23,19.11,8.74;8,94.89,109.18,197.78,8.74;8,94.89,121.14,193.75,8.74">Proceedings of the 2004 Meeting of the International Federation of Classification Societies</title>
		<meeting>the 2004 Meeting of the International Federation of Classification Societies<address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,323.10,73.32,197.76,8.74;8,323.10,85.27,197.78,8.74;8,323.10,97.23,197.77,8.74;8,323.10,109.18,197.77,8.74;8,323.10,121.14,44.01,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,413.61,73.32,107.25,8.74;8,323.10,85.27,179.65,8.74">Information Retrieval as Multitasking: An Exploratory Framework</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Spink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,323.10,97.23,139.24,8.74">ACM SIGIR 2004 Workshop on</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
