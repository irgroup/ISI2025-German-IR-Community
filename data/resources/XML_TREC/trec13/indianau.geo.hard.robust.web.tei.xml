<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.52,75.53,396.95,12.58">WIDIT in TREC-2004 Genomics, HARD, Robust, and Web tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,111.54,104.22,46.83,9.02"><forename type="first">Kiduk</forename><surname>Yang</surname></persName>
							<email>kiyang@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.68,104.22,32.30,9.02"><forename type="first">Ning</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.36,104.22,47.66,9.02"><forename type="first">Adam</forename><surname>Wead</surname></persName>
							<email>awead@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.68,104.22,60.69,9.02"><forename type="first">Gavin</forename><forename type="middle">La</forename><surname>Rowe</surname></persName>
							<email>glarowe@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.01,104.22,44.55,9.02"><forename type="first">Yu-Hsiu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.90,104.22,74.14,9.02"><forename type="first">Christopher</forename><surname>Friend</surname></persName>
							<email>cmfriend@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,460.61,104.22,39.75,9.02"><forename type="first">Yoon</forename><surname>Lee</surname></persName>
							<email>yoonlee@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<postCode>47405</postCode>
									<settlement>Bloomington</settlement>
									<region>Indiana</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.52,75.53,396.95,12.58">WIDIT in TREC-2004 Genomics, HARD, Robust, and Web tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA175833BE69E281DD089BA3C4EF527E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To facilitate understanding of information as well as its discovery, we need to combine the capabilities of the human and the machine as well as multiple methods and sources of evidence. Web Information Discovery Tool (WIDIT) Laboratory at the Indiana University School of Library and Information Science houses several projects that aim to apply this idea of multi-level fusion in the areas of information retrieval and knowledge organization. The TREC research group of WIDIT, who engages in examination of information retrieval strategies that can accommodate a variety of data environments and search tasks, participated in the Genomics, HARD, Robust, and Web tracks in <ref type="bibr" coords="1,250.01,274.14,50.92,9.88">TREC-2004</ref>. The basic approach of WIDIT was to leverage multiple sources of evidence, combine multiple methods, and integrate the strengths of man and the machine. Our main strategies for the tracks were: the use of gene name thesaurus in the Genomics track; query expansion and relevance feedback in the HARD track; query expansion with keywords from Web search in the Robust track, and the interactive system tuning process called "Dynamic Tuning" in the Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Web track</head><p>In the Web track, we participated in the mixed query task as well as the query classification subtask. Our main strategies were fusion retrieval, where we combined different sources of evidence (e.g. body text, anchor text, header text), and post-retrieval reranking, where query typespecific methods were applied to adjust the document scores. The key component of WIDIT for the Web track was an interactive system tuning process called "Dynamic Tuning", which optimizes the fusion formula that combines the contributions of multiple sources of evidence (e.g. hyperlinks, URL, document structure). Dynamic tuning is a novel approach to system tuning that harnesses both the human intelligence and the computational power of the machine.</p><p>In addition to the dynamic tuning for fusion, we explored a query classification strategy that combines statistical and linguistic classification methods to identify the query type so that the system can adapt its retrieval methods according to the query type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Classification</head><p>The goal of query classification task was to identify the categories of 225 mixed queries that consisted of 75 topic distillation (TD), 75 homepage finding (HP), and 75 named page finding (NP) queries. The main challenge of the query classification task stemmed from the short length of the queries, which contained only three words on the average (Table <ref type="table" coords="1,419.79,609.18,3.97,9.88" target="#tab_0">1</ref>). We suspected that machine learning approaches may not be very effective in classifying texts with only a few words. Furthermore, the quality and the quantity of the training data available from previous years also seemed suboptimal for machine learning. There were 100 TD training queries compared to 300 HP and 295 NP queries, which were also short in length (Table <ref type="table" coords="1,451.32,659.76,4.58,9.88" target="#tab_1">2</ref>) and appeared to be often ambiguous upon manual examination. Consequently, we decided to combine the statistical approach (i.e. automatic classifier) of machine learning with a linguistic classifier based on word cues. To supplement the training data for automatic classifiers, which had three times as many HP and NP than TD queries, we created a lexicon of US government topics by manually selecting keywords from the crawl of the Yahoo!'s U.S. Government category. The linguistic classifier used a set of heuristics based on the linguistic patterns specific to each query type identified from the analysis of the training data. For example, we noticed that queries that end in all uppercase letters tended to be HP, queries containing 4-digit year were more likely to be NP, and TD queries were shorter in general than HP or NP queries. We also identified some word cues for NP (e.g. about, annual, report, etc.) and HP (e.g. home, welcome, office, bureau, etc.) query types. After constructing the linguistic classifier, we combined the automatic classifier and the heuristic classifier with a simple ad-hoc heuristic that arrived at the query classification in the following manner: if single word, assign TD. else if strong word cue, assign linguistic classification. else assign statistical classification.</p><p>We tested Naïve Bayes and SVM classifiers with the Yahoo-enriched training data, which showed little difference in performance. The classifier comparisons (i.e. statistical vs. linguistic vs. combination) showed the best performance by the combination classifier, which was the classifier used in our official run. Only three TREC groups who participated in the query classification task, and there was little difference in performance across systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixed Query Task</head><p>Our main task in the mixed query task was to optimize the system for mixed topic. Having engaged in the query classification, our approach to the mixed query task was based on optimizing retrieval strategy for each of the query types. To leverage the multiple sources of evidence, we created separate document indexes for body text, anchor text of incoming links, and header text that consists of meta field text and emphasized portion of body text. The retrieval results using each index were combined using weighted sum with various weights to determine the optimum fusion formula for the baseline run without regards to query types.</p><p>In addition to fusion by result merging, we employed a post-retrieval rank-boosting strategy to rerank the merged results for each query type. Our general approach to query type-specific reranking was as follows: boost the rank of potential homepages if the query is topic-distillation or homepage finding type; boost the rank of pages with keyword matches if the query is hompage or named page finding type. More specifically, our rank boosting heuristic kept top 5 ranks static, while boosting the ranks of potential homepages (identified by URL type determination) as well as pages with keyword matches in document titles and URLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">WIDIT Web IR System</head><p>WIDIT Web IR system consists of five main modules: indexing, retrieval, fusion (i.e. result merging), reranking, and query classification modules. The indexing module processes various sources of evidence to generate multiple indexes. The retrieval module produces multiple result sets from using different query formulations against multiple indexes. The fusion module, which is optimized via the static tuning process, combines result sets using weighted sum formula. The reranking module uses query-specific reranking formulas optimized via dynamic tuning process to rerank the merged results, and the query classification module uses a combination of statistical and linguistic classification methods to determine query types. The overview of WIDIT Web IR system architecture is displayed in Figure <ref type="figure" coords="3,275.77,329.22,4.12,9.88" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Reranking Factors</head><p>Previous TREC participants found various sources of evidence such as anchor text <ref type="bibr" coords="4,476.45,93.60,45.56,9.88;4,90.00,106.26,131.08,9.88" target="#b1">(Craswell, Hawking &amp; Robertson, 2001;</ref><ref type="bibr" coords="4,224.15,106.26,125.04,9.88" target="#b3">Hawking &amp; Craswell, 2002;</ref><ref type="bibr" coords="4,352.27,106.26,125.76,9.88" target="#b0">Craswell &amp; Hawking, 2003)</ref> and URL characteristics <ref type="bibr" coords="4,156.22,118.86,87.51,9.88">(Kraajj et al., 2002;</ref><ref type="bibr" coords="4,247.03,118.86,73.37,9.88" target="#b5">Tomlinson, 2003</ref><ref type="bibr" coords="4,320.40,118.86,93.27,9.88">, Zhang et al., 2003)</ref> to be useful in the Web track tasks. Based on those findings as well as the analysis of our Web track results in 2003, we decided to focus on four categories of the reranking factors. The first category was the fieldspecific match, where we scored each document by counting the occurrences of query words (keyword, acronym, phrase) in URL, title, header, and anchor texts. The second category of reranking factors we used was the exact match, where we looked for exact match of query text in title, header, and anchor texts (exact), or in the body text (exact2) of documents. The third category was link-based, where we counted documents' inlinks (indegree) and outlinks (outdegree). The last category was the document type, which was derived based on its URL <ref type="bibr" coords="4,90.00,232.62,82.96,9.88" target="#b5">(Tomlinson, 2003;</ref><ref type="bibr" coords="4,176.75,232.62,84.17,9.88">Kraajj et al., 2002)</ref>, or derived using a linguistic heuristic similar to the one used in query classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Dynamic Tuning</head><p>Our findings from TREC-2003 <ref type="bibr" coords="4,230.40,573.72,117.01,9.88" target="#b7">(Yang &amp; Albertson, 2003)</ref> indicated that fusion by result merging could be supplemented with post-retrieval reranking based on metadata (e.g. link count, URL characteristics) to enhance retrieval performance in the topic distillation task. In 2003, however, we were not successful in devising effective reranking strategies for the homepage and named page finding tasks, nor were we able to adequately address the question of how to deal with mixed query searches.</p><p>Thus, the focus of our TREC-2004 Web track efforts was to extend the fusion approach by introducing the "dynamic tuning" process with which to optimize the fusion formula that combines the contributions of multiple sources of evidence (e.g. hyperlinks, URL, document structure). The dynamic tuning process is implemented as a Web application (Figure <ref type="figure" coords="4,479.30,687.54,4.07,9.88" target="#fig_1">2</ref>); where interactive system parameter tuning by the user produces in real time the display of system performance changes as well as the new search results annotated with metadata of fusion parameter values (e.g. link counts, URL type, etc.). The key idea of dynamic tuning, which is to combine the human intelligence, especially pattern recognition ability, with the computational power of the machine, is implemented in this Web application that allows human to examine not only the immediate effect of his/her system tuning but also the possible explanation of the tuning effect in the form of data patterns. By engaging in iterative dynamic tuning process, where we successively fine-tuned the fusion parameters based on the cognitive analysis of immediate system feedback, we were able to significantly increase our system performance.</p><p>The effective reranking factors observed from the iterations of dynamic reranking were: indegree, outdegree, exact match, and URL/Pagetype with the minimum number of outdegree of 1 for HP queries; indegree, outdegree, and URLtype for NP queries (1/3 impact of HP factors); acronym, outdegree, and URLtype with the minimum number of outdegree of 10 for TD queries. In addition to harnessing both the human intelligence and machine processing power to facilitate the process of system tuning with many parameters, dynamic tuning turned out to be a good tool for failure analysis. We examined severe search failure instances by WIDIT using the dynamic tuning interface and observed the following:</p><p>• Acronym Effect -WIDIT expanded acronyms and ranked documents about the acronym higher than the specific topic. -e.g. CDC documents ranked higher than Rabies documents for topic 89 ("CDC Rabies homepage") • Duplicate Documents -WIDIT eliminated documents with the same URLs and ranked mirrored documents higher. -e.g. Relevant documents with the same URL (G00-74-1477693 and G00-05-3317821 for topic 215) were not indexed by WIDIT. -e.g. G32-10-1245341 is a mirror document of G00-48-1227124 (relevant for topic 188) but not counted as relevant by TREC official judgments. • Link Noise Effect -Non-relevant documents with irrelevant links are ranked high by WIDIT e.g. The relevant document for topic 197 ("Vietnam War") is Johnson Administration's "Foreign Relations" volumes with 4 links to Vietnam volumes, but WIDIT retrieved pages about Vietnam with many irrelevant (e.g. navigational) links at top ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Topic Drift</head><p>-Topically related documents with high frequency of query terms were ranked high by WIDIT. -e.g. Documents about drunk driving victims, MADD, etc. were ranked higher than the impaired driving program of NHTSA page for topic 192 ("Drunk driving").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Web Track Results</head><p>Table <ref type="table" coords="5,120.40,634.25,5.49,9.88" target="#tab_2">3</ref> shows the results of our mixed query task runs. Since we were not able to fully implement the dynamic tuning module in time, our official submission consisted of fusion runs and reranking runs using a static reranking formula based on past findings regarding specific query types. The post-submission runs, which employed dynamic tuning, achieved better performance in general, especially when using the official query types. The best fusion run combined the best baseline result, which used anchor text index, and the top two fusion runs, which merged the results of body, anchor, and header index results. In order to assess the effect of query classification error, we generated random assignment of query types (DR_r) and worst possible assignment of query types (DR_b). Table <ref type="table" coords="6,462.40,332.58,4.58,9.88" target="#tab_3">4</ref>.1 compares the classification error of WIDIT query classification algorithm with random and worst classification. Because TD task is biased towards homepages, HP-TD error is the least severe type of error. Since HP and NP tasks are both known-item search task, HP-NP error is less severe than NP-TD, which is the least similar. In table 4.2, which shows the results of dynamic reranking using each query classification, we can see that random or poor query classification will adversely affect the retrieval performance. Table <ref type="table" coords="6,344.42,408.42,4.58,9.88" target="#tab_3">4</ref>.2 also shows the random query type results to be comparable with TREC median performance for TD and HP queries. DR_o: Dynamic reranking run using the official query type DR_g: Dynamic reranking run using the guessed query type DR_r: Dynamic reranking run using the random query type DR_b: Dynamic reranking run using the bad query type</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HARD track</head><p>Conventional retrieval systems, which ignore the fact that users are different, often fail to satisfy the various aspects of user's information need beyond the topical relevance. The HARD (High Accuracy Retrieval from Documents) track, introduced in 2003, investigates approaches that can enhance the retrieval performance by tailoring the search to the user.</p><p>The HARD track has three phases. First, the HARD participant produces baseline retrieval results using the initial topic descriptions without any user-specific metadata. After the baseline run, the participant creates the Clarification Forms (CF), which is given to the user to collect relevance data for each query. In the third and final phase, the participant can use the metadata about queries (e.g. familiarity, genre, subjects, and geography) that are provided by the user, the relevance data collected from the clarification forms, or a combination of both to generate the final submission runs.</p><p>We participated in all three phrases (baseline run, clarification form, final run). The focal points of our strategy were query expansion and relevance feedback by clarification form. For the baseline retrieval, we examined the effectiveness of query formulation strategies with emphasis on automatic query expansion. Initial query formulations involved combinations of topic fields (title, description, narrative) and stemming (simple plural stemmer, combination stemmer), to which were added combinations of expansion components such as synonyms from the WordNet, noun phrases identified by Brill Tagger, expanded acronyms and word definitions from Web search. The analysis of the results based on the training data suggested that automatic query expansion with synonyms and word definition terms can introduce noise that hurt retrieval performance, whereas acronyms, nouns and noun phrases found in the topic titles tend to be terms with more discriminating power. We also investigated query expansion via pseudo-relevance feedback, but it showed adverse effect on retrieval performance.</p><p>In an attempt to decrease the noise introduced in automatic query expansion, we involved the user to filter the expanded query via clarification forms (Figures 3.1 and 3.2), where the user selected relevant query expansion terms and best sentences from top 25 baseline results for each query. The best sentence of a document was extracted using strategies from the past Q&amp;A track. In the final run, the baseline query was to be modified with information from CF feedback as well as metadata information provided in the metadata version of HARD topics. Post-retrieval reranking and metadata labeling were the main tasks in this stage. In order to "label" (or extract) metadata on documents, different lexicon bases were generated for each metadata fields (e.g. location, subject) and documents were scored for each metadata using a combination of statistical and linguistic classification methods. Unfortunately, we were not able to implement the reranking module in time, which was to be based on explicit relevant judgment we got from CF as well as implicit clues, such as emphasis on specific fields, noun phrases, domain-specific lexicon use, and linguistic clues. Although our official submission included only the baseline runs and CF-enhance runs, we include the description of our original metadata strategy below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metadata Strategy</head><p>The four metadata types associated with HARD topics were geography, genre, familiarity, and subject. For geography, we created US and non-US location lexicon from mining the Web resources (e.g. Yahoo!) and counting the occurrences of the lexicon terms in the first line of news or keywords field. For genre, we considered the document with high proportion of quoted string as opinion/editorial. As for familiarity, we created a rare word lexicon from an online dictionary and scored documents by the proportion of rare words. We also created subject lexicon for each subject value by querying Yahoo! category and WordNet Hyponyms (… is a kind of subject) and counting the lexicon term occurrences in the keyword fields of the documents. The metadata scores thus computed can then be used in the post-retrieval reranking process such as dynamic reranking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HARD Track Results</head><p>Table <ref type="table" coords="9,118.93,93.60,5.49,9.88" target="#tab_4">5</ref> shows our best baseline and CF result. The best baseline run, which used Okapi term weight, query expansion with acronyms and nouns, and the combo stemmer <ref type="bibr" coords="9,432.35,106.26,89.74,9.88;9,90.00,118.92,69.78,9.88" target="#b8">(Yang, Maglaughlin &amp; Newby, 2001</ref>) that combines simple plural removal and inflectional stemming, performed well above the median level. The CF run, using the relevance feedback from the clarification form, improved the retrieval performance of the baseline only slightly, which suggests the effectiveness of the automatic query expansion in the baseline run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robust track</head><p>The Robust track explored methods for improving the consistency of retrieval technology by focusing on poorly performing topics. The 2004 Robust track was a classic ad-hoc retrieval task using 250 topics. Query expansion with keywords from Web search was the main WIDIT approach to the Robust track, which extended the methodology of the best Robust track system in previous TREC <ref type="bibr" coords="9,161.47,401.76,96.77,9.88" target="#b2">(Grunfeld et al., 2003)</ref>. We submitted four runs for the Robust track using combinations of topic field text, different term weighting formula, and query expansion methods. Title, description and narrative text were combined to create wdoqla1 and wdo25qla1, but wdoqla1 used the original Okapi term weight formula, whereas wdo25qla1 used the modified Okapi BM25 formula. wdoqdn1, based on description field, and wdoqsn1, based on title field, expanded the query with nouns extracted by the Brill tagger. Both runs were weighted with the original Okapi term weights.</p><p>All runs used a simple affix removal stemming algorithm that included various topic-specific exception word lists. Stemmed words were then compared against a dictionary for accuracy. Other retrieval runs were attempted using query expansion with web search engines such as Yahoo, Google, Altavista and Search, as well as lexically-based query expansion methods with WordNet.com; however, these methods introduced a high level of noise and did not deliver good retrieval results and thus excluded from the official submission. Table <ref type="table" coords="9,406.26,553.56,4.12,9.88" target="#tab_5">6</ref>, which shows the robust track results by topic type, indicates above median level of performance by WIDIT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Genomics track</head><p>The Genomics track investigated how exploiting domain-specific information improves retrieval effectiveness. The 2004 genomics track contained an ad-hoc retrieval task and three variants of a categorization task using a 10-year subset <ref type="bibr" coords="10,303.06,118.92,25.85,9.88">(1994)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(1995)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(1996)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(1997)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(1998)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(1999)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(2000)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(2001)</ref><ref type="bibr" coords="10,328.91,118.92,5.17,9.88">(2002)</ref><ref type="bibr" coords="10,334.08,118.92,25.85,9.88">(2003)</ref> of MEDLINE data (4.5 million MEDLINE records, 9 GB) and 50 topics derived from information needs of biomedical researchers. One of the main WIDIT approach to the Genomics track was to build a gene name thesaurus by a combination of statistical (e.g. Latent Semantic Indexing) and linguistic (e.g. Gene Ontology harvest) clustering methods. We could not scale up the LSI module in time to handle the Genomics data, so we only used the gene synonyms created from the Gene Ontology harvest and nouns and phrases identified by the NLP module to expand the queries. For the Categorization task, we only attempted the triage task using a Naïve Bayes classifier. The WIDIT results for both ad-hoc and triage tasks were below the median level of performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,360.66,198.63,9.02;3,90.00,375.48,431.94,315.06"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. WIDIT Web IR System Architecture</figDesc><graphic coords="3,90.00,375.48,431.94,315.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.00,276.66,154.19,9.02;4,90.00,291.48,441.36,234.78"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Dynamic Tuning Interface</figDesc><graphic coords="4,90.00,291.48,441.36,234.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,90.00,112.26,126.40,9.02;8,90.36,127.44,365.76,232.20"><head>Figure</head><label></label><figDesc>Figure 3.1 Clarification Form I</figDesc><graphic coords="8,90.36,127.44,365.76,232.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.00,74.34,357.76,100.70"><head>Table 1 .</head><label>1</label><figDesc>2004 Web track queries</figDesc><table coords="2,90.00,93.24,357.76,81.80"><row><cell>Query Length (# of words)</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>Avg.</cell></row><row><cell>TD queries</cell><cell>12</cell><cell>41</cell><cell>16</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2.2</cell></row><row><cell>NP queries</cell><cell>2</cell><cell>11</cell><cell>28</cell><cell>20</cell><cell>9</cell><cell>4</cell><cell>1</cell><cell>3.5</cell></row><row><cell>HP queries</cell><cell>3</cell><cell>9</cell><cell>38</cell><cell>15</cell><cell>8</cell><cell>2</cell><cell>0</cell><cell>3.3</cell></row><row><cell>All queries</cell><cell>17</cell><cell>61</cell><cell>82</cell><cell>41</cell><cell>17</cell><cell>6</cell><cell>1</cell><cell>3.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,90.00,199.74,301.23,101.66"><head>Table 2 .</head><label>2</label><figDesc>Training queries from 2001-2003 Web tracks</figDesc><table coords="2,90.00,218.16,301.23,83.24"><row><cell></cell><cell></cell><cell>Query Counts</cell><cell></cell><cell>Avg. Length</cell></row><row><cell></cell><cell>2001</cell><cell>2002</cell><cell>2003</cell><cell>(# of words)</cell></row><row><cell>TD queries</cell><cell></cell><cell>50</cell><cell>50</cell><cell>2.8</cell></row><row><cell>NP queries</cell><cell></cell><cell>150</cell><cell>150</cell><cell>4.0</cell></row><row><cell>HP queries</cell><cell>145</cell><cell></cell><cell>150</cell><cell>3.6</cell></row><row><cell>All queries</cell><cell>145</cell><cell>200</cell><cell>350</cell><cell>3.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,112.26,381.85,189.34"><head>Table 3 .</head><label>3</label><figDesc>Mixed Query Task Results (MAP = mean average precision, MRR = mean reciprocal rank)</figDesc><table coords="6,90.00,130.68,242.19,170.92"><row><cell cols="2">MAP (TD)</cell><cell>MRR (NP)</cell><cell>MRR (HP)</cell></row><row><cell>F3</cell><cell>0.0974</cell><cell>0.6134</cell><cell>0.4256</cell></row><row><cell>SR_g</cell><cell>0.0949</cell><cell>0.6018</cell><cell>0.4487</cell></row><row><cell>DR_g</cell><cell>0.1274</cell><cell>0.5418</cell><cell>0.6371</cell></row><row><cell>SR_o</cell><cell>0.0986</cell><cell>0.6258</cell><cell>0.4341</cell></row><row><cell>DR_o</cell><cell>0.1349</cell><cell>0.6545</cell><cell>0.6265</cell></row><row><cell>TREC Median</cell><cell>0.1010</cell><cell>0.5888</cell><cell>0.5838</cell></row><row><cell>F3: Best fusion run</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SR_g: Static reranking run using the guessed query type</cell></row><row><cell cols="4">DR_g: Dynamic reranking run using the guessed query type</cell></row><row><cell cols="4">SR_o: Static reranking run using the official query type</cell></row><row><cell cols="4">DR_o: Dynamic reranking run using the official query type</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,90.00,452.34,243.29,217.16"><head>Table 4 .1</head><label>4</label><figDesc>Query classification error by error type</figDesc><table coords="6,90.00,470.82,243.29,198.68"><row><cell></cell><cell cols="3">Error Type</cell></row><row><cell></cell><cell>HP-TD</cell><cell cols="2">HP-NP</cell><cell>NP-TD</cell></row><row><cell>DR_g</cell><cell>26</cell><cell></cell><cell>49</cell><cell>17</cell></row><row><cell>DR_r</cell><cell>54</cell><cell></cell><cell>48</cell><cell>44</cell></row><row><cell>DR_b</cell><cell></cell><cell></cell><cell>75</cell><cell>150</cell></row><row><cell cols="5">Table 4.2 Query classification error by error type</cell></row><row><cell></cell><cell cols="2">MAP (TD)</cell><cell cols="2">MRR (NP)</cell><cell>MRR (HP)</cell></row><row><cell>DR_o</cell><cell>0.1349</cell><cell></cell><cell cols="2">0.6545</cell><cell>0.6265</cell></row><row><cell>DR_g</cell><cell>0.1274</cell><cell></cell><cell cols="2">0.5418</cell><cell>0.6371</cell></row><row><cell>DR_r</cell><cell>0.1235</cell><cell></cell><cell cols="2">0.4450</cell><cell>0.5285</cell></row><row><cell>DR_b</cell><cell>0.0922</cell><cell></cell><cell cols="2">0.2995</cell><cell>0.3105</cell></row><row><cell>TREC Median</cell><cell>0.1010</cell><cell></cell><cell cols="2">0.5888</cell><cell>0.5838</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,90.00,188.16,315.64,113.18"><head>Table 5 .</head><label>5</label><figDesc>HARD results (MAP = mean average precision, MRP = mean R-precision)</figDesc><table coords="9,90.00,206.58,228.22,94.76"><row><cell></cell><cell>MAP</cell><cell>MRP</cell></row><row><cell>TREC Best</cell><cell>0.3554</cell><cell>0.3717</cell></row><row><cell>WIDIT CF run</cell><cell>0.3287</cell><cell>0.3454</cell></row><row><cell>WIDIT best baseline run</cell><cell>0.3128</cell><cell>0.3366</cell></row><row><cell>TREC Median</cell><cell>0.2634</cell><cell>0.2906</cell></row><row><cell>TREC Worst</cell><cell>0.0288</cell><cell>0.0673</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,90.00,592.32,385.74,104.72"><head>Table 6 .</head><label>6</label><figDesc>Mean Average Precision of Robust runs by topic type</figDesc><table coords="9,90.00,610.80,385.74,86.24"><row><cell></cell><cell>Old Topics</cell><cell>New Topics</cell><cell>Difficult</cell><cell>All Topics</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Topics</cell><cell></cell></row><row><cell>TREC Best</cell><cell>0.3429</cell><cell>0.4227</cell><cell>0.1949</cell><cell>0.3586</cell></row><row><cell>WIDIT Best (wdoqla1)</cell><cell>0.2819</cell><cell>0.3300</cell><cell>0.1363</cell><cell>0.2914</cell></row><row><cell>TREC Median</cell><cell>0.2667</cell><cell>0.2979</cell><cell>0.1260</cell><cell>0.2755</cell></row><row><cell>TREC Worst</cell><cell>0.0692</cell><cell>0.0529</cell><cell>0.0207</cell><cell>0.0756</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,90.00,283.32,427.62,9.88;10,90.00,295.98,233.49,9.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,253.40,283.32,174.85,9.88">Overview of the TREC-2002 Web track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.12,283.32,82.50,9.88;10,90.00,295.98,197.64,9.88">Proceedings of the 11th Text Retrieval Conference (TREC 2002)</title>
		<meeting>the 11th Text Retrieval Conference (TREC 2002)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="86" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,321.30,403.35,9.88;10,90.00,333.90,420.67,9.88;10,90.00,346.56,140.61,9.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,321.34,321.30,172.02,9.88;10,90.00,333.90,50.10,9.88">Effective site finding using link anchor information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,147.42,333.90,363.25,9.88;10,90.00,346.56,94.02,9.88">Proceedings of the 24th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,371.88,420.12,9.88;10,90.00,384.54,242.83,9.88;10,332.88,382.27,5.43,6.32;10,341.10,384.54,177.16,9.88;10,90.00,397.20,39.46,9.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,346.67,371.88,163.45,9.88;10,90.00,384.54,135.60,9.88">TREC 2003 Robust, HARD, and QA track experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.64,384.54,96.19,9.88;10,332.88,382.27,5.43,6.32;10,341.10,384.54,115.49,9.88">Proceedings of the 12 th Text Retrieval Conference</title>
		<meeting>the 12 th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="510" to="521" />
		</imprint>
	</monogr>
	<note>TREC2003</note>
</biblStruct>

<biblStruct coords="10,90.00,422.46,430.48,9.88;10,90.00,435.12,230.72,9.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,256.29,422.46,174.85,9.88">Overview of the TREC-2001 Web track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,437.94,422.46,82.54,9.88;10,90.00,435.12,137.90,9.88">Proceedings of the 10th Text Retrieval Conference</title>
		<meeting>the 10th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,460.44,408.93,9.88;10,90.00,473.04,376.33,9.88;10,90.00,485.70,201.69,9.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,320.28,460.44,178.65,9.88;10,90.00,473.04,75.30,9.88">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,175.20,473.04,291.13,9.88;10,90.00,485.70,166.14,9.88">Proceedings of the 25th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,511.02,417.60,9.88;10,90.00,523.68,157.36,9.88;10,247.32,521.41,5.43,6.32;10,255.54,523.68,219.27,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,189.63,511.02,317.97,9.88;10,90.00,523.68,50.71,9.88">Robust, Web and Genomic retrieval with Hummingbird SearchServer at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.08,523.68,96.28,9.88;10,247.32,521.41,5.43,6.32;10,255.54,523.68,171.80,9.88">Proceedings of the 12 th Text Retrieval Conference (TREC2003)</title>
		<meeting>the 12 th Text Retrieval Conference (TREC2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="254" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,548.94,429.79,9.88;10,90.00,561.60,424.05,9.88" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<title level="m" coord="10,375.97,548.94,143.82,9.88;10,90.00,561.60,376.91,9.88">THUIR at TREC 2003: Novelty, Robust, Web and HARD. The 12th Text Retrieval Conference (TREC 2003) Notebook</title>
		<imprint>
			<date type="published" when="2003">2003a</date>
			<biblScope unit="page" from="137" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,586.92,400.45,9.88;10,490.50,584.65,5.43,6.32;10,498.66,586.92,18.88,9.88;10,90.00,599.58,197.67,9.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,241.29,586.92,143.36,9.88">WIDIT in TREC2003 Web track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Albertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,394.26,586.92,96.19,9.88;10,490.50,584.65,5.43,6.32;10,498.66,586.92,18.88,9.88;10,90.00,599.58,150.14,9.88">Proceedings of the 12 th Text Retrieval Conference (TREC2003)</title>
		<meeting>the 12 th Text Retrieval Conference (TREC2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="328" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,624.84,409.68,9.88;10,90.00,637.50,181.76,9.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,315.81,624.84,121.10,9.88">Passage feedback with IRIS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Maglaughlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Newby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,447.06,624.84,52.62,9.88;10,90.00,637.50,117.64,9.88">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="521" to="541" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
