<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.96,142.18,375.51,15.49">Melbourne University 2004: Terabyte and Web Tracks</title>
				<funder>
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">ARC Center for Perceptive and Intelligent Machines in Complex Environments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,231.84,175.65,40.15,10.76"><forename type="first">Vo</forename><surname>Ngoc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.99,175.65,19.30,10.76;1,321.17,175.65,70.63,10.76"><forename type="first">Anh</forename><forename type="middle">Alistair</forename><surname>Moffat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.96,142.18,375.51,15.49">Melbourne University 2004: Terabyte and Web Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCF15D493F63410B0FF3526CD3BBB911</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Melbourne carried out experiments in the Terabyte and Web tracks of TREC 2004. We applied a further variant of our impact-based retrieval approach by integrating evidence from text content, anchor text, URL depth, and link structure into the process of ranking documents, working toward a retrieval system that handles equally well all of the four query types employed in these two tracks. That is, we sought to avoid special techniques, and did not apply any explicit or implicit query classifiers. The system was designed to be scalable and efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Melbourne carried out experiments in the Terabyte and Web tracks of TREC 2004. In the Terabyte Track we used title-only queries, and considered the task to be one of content search on web data. In the Web Track, the 225 mixed queries of different types (topic distillation, homepage finding, and named page finding) were processed in our retrieval system without any modifications.</p><p>The goals of our investigation in 2004 included:</p><p>• to create a system that handles equally well queries of any type, without classification being required;</p><p>• to develop further our retrieval approach based on qualitative document-centric impacts <ref type="bibr" coords="1,126.48,541.41,41.56,9.82" target="#b0">[Anh and</ref><ref type="bibr" coords="1,171.76,541.41,112.47,9.82">Moffat, 2003, Anh, 2004]</ref>, by integrating evidence from the content layout and incoming anchor text;</p><p>• to investigate the possibility of combining evidence from URL depth and link structure as document parameters into the retrieval scores established by the impact-based retrieval scheme; and</p><p>• to demonstrate a scalable system that operates with large data collections at high throughput rates for both query processing and indexing.</p><p>For the Web task, only effectiveness results are presented in this report. On the other hand, the large size of the GOV2 collection makes it an attractive basis for a discussion of efficiency concerns, as well as effectiveness outcomes.</p><p>Our retrieval process is briefly described as follows. During index construction, term statistics from documents and their incoming anchors are employed to assign an integral document-centric impact score to each valid pairing of terms and documents. The triples (term, document, impact) are stored in an impact-sorted inverted index for querying. In addition, document URL depths and authority scores are computed, and similarly quantized to small integers. These weights are also included as components in the index. During query evaluation, the impacts, the URL weights, and the authority weights are combined in an efficient way to produce a ranking of documents with respect to a query.</p><p>The rest of this paper is organized as follows. Section 2 briefly reviews the impact-based ranking scheme for text documents. Next, Section 3 describes the experimental options for combining web-specific evidence from anchor text, URL content, and link structures, with the impact ranking. Section 4 lists and then discusses the results returned from NIST for the Web Track. Section 5 introduces the measures taken to ensure a scalable and efficient implementation, and is followed by Section 6, which analyzes the efficiency and effectiveness of our runs for the Terabyte Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Impact-Based Document Ranking</head><p>This section describes the formulation of document-centric impacts and the general scheme for using them in ranking.</p><p>For a text collection containing N documents and n distinct terms, a n-dimensional vector space is formed, with each dimension associated with one of the terms. Every document d of the collection is represented by a document impact vector,</p><formula xml:id="formula_0" coords="2,247.92,328.76,127.66,11.97">Ω d = {ω d,1 , ω d,2 , . . . , ω d,n } ,</formula><p>and every query q by a query impact vector,</p><formula xml:id="formula_1" coords="2,242.40,374.24,138.70,11.85">Ω q = {ω q,1 , ω q,2 , . . . , ω q,n } ,</formula><p>where each of the document or query impacts is an integer between 1 and k inclusive, with k a predefined parameter. In this paper, k = 12 is used. Document impacts and query impacts might be defined differently, which is why different notion has been used for them. If a term t ∈ q does not have a corresponding dimension in the n-space, it is ignored.</p><p>A two-phase process is used to assign the document impact values ω d,t for the terms t that appear in d. In the first sorting phase, documents are parsed and frequency statistics gathered. Quantities accumulated include the term frequency f d,t (the number of times term t appears in document d), and the document frequency f t (the number of documents that contain t). At the end of the sorting phase, the list of distinct terms of each document d is rearranged into decreasing order of the term frequencies f d,t , with ties broken by the inverse document frequency 1/f t <ref type="bibr" coords="2,501.96,515.97,22.28,9.82;2,99.24,529.05,82.18,9.82" target="#b2">[Anh and Moffat, 2002b]</ref>.</p><p>In the second mapping phase of assigning impacts, the ordered list of terms for each document d is partitioned into k consecutive non-overlapping segments numbered from k down to 1 so that the number of elements x i in segment i is</p><formula xml:id="formula_2" coords="2,99.24,588.24,260.75,50.09">x i = (B -1) • B k-i , where B = (n d + 1) 1/k ,</formula><p>and n d is the number of elements in the list being partitioned -in other words, the number of distinct terms in document d. Finally, each term t is assigned as its impact the index i of the partition it falls in. The result of this step is that a small number of terms are deemed to be of high impact in the document, and a much greater number of terms are deemed to be of low impact.</p><p>To complete the process of index construction, the impacts associated with each term are gathered, and sorted into decreasing order of impact score. That is, the inverted list for each distinct term is ordered by decreasing impact, and because there are only k possible different impact scores for each term, can be thought of as a sequence of k (or fewer) blocks, each of which contains document numbers d in which that term has the same impact. The result is an impact-sorted index that allows very fast query processing.</p><p>The query term impacts for the terms that appear in q are defined during query processing time, and are also formed in two phases, where the first phase is similar to that of the document impacts. Due to the short average query length, the second phase is simply the mapping of the ordered list of terms into the list of integer from k to 1, where k is either k or the number of distinct terms in q, whichever is larger. This method of assigning query term impacts is not the best in terms of retrieval effectiveness <ref type="bibr" coords="3,196.44,169.17,51.03,9.82" target="#b0">[Anh, 2004]</ref>, but is used here because of its simplicity.</p><p>Once the impact vectors for a document d and a query q have been formed, they are employed to define i d,q , the impact score of d with respect to q:</p><formula xml:id="formula_3" coords="3,256.80,219.20,110.03,26.49">i d,q = Ω d × Ω q = t ω d,t • ω q,t .</formula><p>In the standard implementation without considering web features, the impact score i d,q is employed as the similarity score S(d, q), that is:</p><formula xml:id="formula_4" coords="3,279.72,292.88,64.19,11.97">S(d, q) = i d,q ,</formula><p>and serves as the sort key for the ranking process. The similarity score is defined differently when the features of web documents are taken into account, as discussed shortly.</p><p>In practical implementations an incomplete version of the i d,q computation is often performed, with only a partial evaluation of the inner product computed, using additional heuristics that are generically referred to as dynamic pruning techniques. <ref type="bibr" coords="3,343.15,370.29,107.19,9.82">Anh and Moffat [2002a]</ref> describe several pruning methods that can be used with impact-based similarity scores, and the same methods are applied in all of the experiments reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adding Web-Based Evidence</head><p>We now consider the types of evidence that can be used in the ranking of web documents, and the ways they can be combined with the impact-based ranking scheme described in Section 2. Four kinds of information can be used in calculating the similarity score between a document d and a query q:</p><p>• The appearance of terms in the content part of document d, query q, and in the whole collection. For this type of evidence, we use the statistics f d,t , the term frequency of t in d; f q,t , the term frequency of t in q; and f t , the document frequency of t in the collection.</p><p>• The appearance of terms in the incoming anchor text of d. The only parameter used is a d,t , the within-anchor frequency of t in the incoming anchor text of d. This evidence requires a non-trivial amount of preparatory cross-document computation which is not included in any of the timings given here.</p><p>• The depth of the URL of d, on the assumption that documents with near the root of a web site are more important than documents deep in subdirectories.</p><p>• The link structure of the documents in the collection. The information extracted from the collection is the set of links d i → d j showing that document d i contains at least one link to document d j . As with the incoming anchor text, we suppose that the link information is already available without needing to be computed.</p><p>In combining these four types of evidence, the main principle is to keep query processing costs low, by performing as much as possible of the computation during indexing. It is clear that the within-anchor frequencies can be combined with the within-document frequencies when defining document-centric impacts by integrating them during the sorting phase. On the other hand, the URL depth and link information are invariant for all the terms in a document, so it makes more sense to include these factors separately. We decided to preprocess the URL depth and link information and store them as document-dependent factors. Each of these factors are quantized to integers in range from 1 to 256 to facilitate compact storage.</p><p>Balancing the term and anchor frequencies in defining impacts We considered two options for combining the within-anchor frequencies a d,t with the within-document frequencies f d,t :</p><p>• Content: Impacts are defined exactly as described in Section 2.</p><p>• Content+Anchor: In the standard Content implementation, the sorting phase that forms the document impacts uses f d,t values as the primary sort key. In this variant, we combine f d,t with a d,t by supposing that they are equally important, and as the primary sort key employ</p><formula xml:id="formula_5" coords="4,259.20,284.12,132.46,11.97">F d,t = (1 + f d,t ) • (1 + a d,t ) .</formula><p>Because we are only interested in the relative ordering of terms within the document, taking the product in this way is equivalent to taking the sum of the logarithms of the (1 + f d,t ) and (1 + a d,t ) values. Each of the two frequencies contributes separately to the overall score F d,t for term t, with a diminishing return for large frequency values. The set of scores {F d,t | t ∈ d} for document d is then ordered into decreasing order, with ties broken by the value of 1/f t , and an assignment of integer impacts is carried out, as described in Section 2.</p><p>Quantizing URL depth The URL depth is derived for each document and the resulting list of distinct URL depths for the collection is sorted into increasing order. The first 255 elements of this list -typically depths 1, 2, and so on -are assigned the integer weights from 256 down to 2. Any remaining depths -which must be at least 256 -are assigned weight 1. At the end of this process, each document d is assigned a URL weight u d corresponding to its URL depth.</p><p>Preprocessing the link structure We preprocess the link structure to extract an authority score for each document using the standard pagerank algorithm of <ref type="bibr" coords="4,360.89,494.13,90.11,9.82" target="#b5">Brin and Page [1998]</ref>. At the completion of this step, each document is assigned an authority score s d which is a real number between 0 and 1. Hub scores are not calculated. The population of authority scores s d is then transformed to a list of integral authority weights a d ranging from 1 to 256, using a logarithmic transformation and quantization defined as</p><formula xml:id="formula_6" coords="4,233.76,568.76,155.97,25.79">a d = 256 • L • log B (s d /L) U -L + + 1 ,</formula><p>where L and U are the minimum and maximum authority scores over the whole collection, is a small positive constant, and B is computed as</p><formula xml:id="formula_7" coords="4,265.20,641.52,85.89,28.87">B = U L L/(U -L)</formula><p>. <ref type="bibr" coords="4,99.24,682.41,84.53,9.82" target="#b6">Witten et al. [1999]</ref> describe other examples of this type of transformation.</p><p>Integration during query processing For this year's experiments we took the view that the document-centric impacts alone (that is, not including the URL and authority weights) could be used to define the list of top r answers, where r is 1,000 for the web task, and either 10,000 or 20 for the Terabyte Track. The URL weights and authority weights are then employed to re-rank the elements of that answer list.</p><p>Query evaluation proceeds in two stages. In the first stage, the standard impact-based ranking scheme described in Section 2 is applied to the chosen impacts (Content or Content+Anchor) to determine a list of the top r documents. At the end of this stage, each document d is assigned an impact score i d,q , with the list of r documents provided in decreasing order of these scores.</p><p>In the second stage of query evaluation, the impact scores i d,t , URL weights u d , and authority weights a d for the selected subset of documents are integrated in a number of different ways (listed shortly) to produce the final similarity scores for the query. The list of top r documents is then re-sorted in decreasing order of the new combined scores, and presented as the final answers.</p><p>The different methods to combine impact scores i d,q with URL weight u d and authority weight a d are:</p><p>• Method A: Using impact scores only. This is the baseline method, where the URL and authority weights are ignored. That is, the final similarity scores S(d, q) are defined as the corresponding impact scores i d,q . However, it should be noted that there are two variants of impacts, as discussed at the beginning of this section.</p><p>• Method B: Adding impact scores and scaled URL weights. In this option, the authority weights are ignored. The final similarity score for a document d is defined as</p><formula xml:id="formula_8" coords="5,231.48,344.84,187.77,11.97">S(d, q) = i d + u d • (max{i d }/ max{u d }) ,</formula><p>where max i d and max u d are the maximum value of i d and u d , respectively, computed locally within the subset of r documents selected by the first phase.</p><p>• Method C: Adding impact scores and scaled authority weights. Here, the URL weights are ignored. The final ranking score for d is computed as the weighted sum of its impact score (with a weight of 2) and authority score (with a weight of 1):</p><formula xml:id="formula_9" coords="5,274.68,454.64,101.51,11.97">S(d, q) = 2 • i d,q + a d .</formula><p>• Method D: Using authority weights to break ties. There is no re-computation of similarity scores S(d, q). Rather, the r top documents are inspected for ties, and the authority scores a d used to break them.</p><p>• Method E: Combining all available evidence. The final similarity score for d is defined as</p><formula xml:id="formula_10" coords="5,149.64,555.68,351.59,11.97">S(d, q) = 2 • (max{i d } • max{u d } -(max{i d } -i d ) • (max{u d } -u d )) + a d .</formula><p>That is, the impact score and URL weight are uniformly combined, and then that combined score is assigned a weight of 2 and added to the authority weight.</p><p>• Method F: Sorting based on all evidence. The list of r documents is sorted in decreasing order by a key that is defined as in the case of Method B :</p><formula xml:id="formula_11" coords="5,256.08,652.28,138.70,11.97">i d + u d • (max{i d }/ max{u d })</formula><p>with ties broken by a d . Experimental settings Five runs were submitted for the Web Track. Each used the collection .GOV and 225 mixed queries, as supplied by the track organizers. The parameters of the runs are listed in Table <ref type="table" coords="6,163.30,260.25,4.07,9.82" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance in the Web Track</head><p>Effectiveness The performance of our five runs is summarized in Figure <ref type="figure" coords="6,419.96,289.29,4.07,9.82" target="#fig_1">1</ref>. The figure shows that combining URL and authority weights with the impact scores improves effectiveness for the topic distillation and homepage finding, but not for the named page finding task. For the named page finding task, the baseline approach is the best choice (of our runs) in terms of average precision. Run MU04web1, which combines evidence from all of content, anchor text, URL depth, and link structure using Method F, performs best. It gives the best results for the distillation and homepage finding task, and reasonable effectiveness for the named page finding task. This combination should be considered to be the best achievable with our approach at this time.</p><p>Compared to other groups, the MU04web1 run was around the median of the best runs from each group for the homepage finding and distillation tasks, but was below the median for the named page and the mixed query tasks. In that sense, we have not achieved our goal of excellent performance, and it may well be that the difference between what we have attained and the higher performance of other systems is a consequence of our integration of several different types of evidence into a single impact score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Scalable and Parallel Processing</head><p>The main challenge for the Terabyte Track this year was the large size of the input document collection. To manage this volume of data, the retrieval mechanism must be scalable, effective, and fast in terms of both indexing and querying. This section reports the techniques we used to handle this large collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalable indexing</head><p>We indexed the collection by dividing it in to manageable sub-collections, indexing them separately, and then merging the partial indexes. If required, the merging process can be performed in a hierarchy of levels, depending on the number of sub-collections and the number of files the system is allowed to open. In our experiments, we used a 10-way merge, and limited the sub-collection size to 2 GB or 100,000 documents. The merging process can run in an interleaved fashion if restricting the number of temporary index files is important.</p><p>Construction of an impact-sorted index consists of three components: primary indexing (the total time spent indexing sub-collections); merging (the total time of the merge sequence); and sorting (the time required to impact-sort the index).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalable querying</head><p>When the collection is large, it may not be feasible to hold the vocabulary in main memory. It might also be necessary to allow for inverted lists that are longer than the available memory. To avoid the first problem, we employed the standard technique of blocking the Av.Prec. Suc@1 Suc@5 Suc@10 ( lexicon <ref type="bibr" coords="7,133.96,481.05,85.86,9.82" target="#b6">[Witten et al., 1999]</ref>. To address the latter problem, we process inverted lists sequentially using fixed-length buffers.</p><p>Another potential problem with querying large collections that we have not yet rectified in our implementation is the use of accumulators. Currently, a 4-byte accumulator is used for each document of the collection, regardless of whether it is assigned a value. With 1 GB of memory for accumulators, that means an upper bound of 250 million documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index compression</head><p>Indexes should be small while allowing fast processing. To that end we employed the word-aligned compression scheme described by <ref type="bibr" coords="7,376.24,589.05,101.14,9.82" target="#b4">Anh and Moffat [2005]</ref>. No term positional information is stored in our indexes.</p><p>Another possibility is to use static pruning, and simply reduce the number of pointers stored, perhaps through the use of a stoplist, for example. <ref type="bibr" coords="7,336.37,628.41,110.31,9.82">Anh and Moffat [2002a]</ref> describe a static pruning scheme for use with impact-sorted indexes, but no static pruning is employed here, and the listed index sizes can be compared with other systems.</p><p>Parallel processing A Beowulf-style cluster was used in our experiments, consisting of a server and eight additional nodes. The server is a Dual 2.8 GHz Intel Xeon with 2 GB of memory, running Debian GNU/Linux. The server supports a 73 GB SCSI disk for system files, and twelve 146 GB SCSI disks in a RAID-5 configuration for data. Each of the nodes is a 2.8 GHz Intel Pentium-4 with 1 GB of memory and 250 GB of local SATA disk.</p><p>Our objective was to achieve fast processing of individual queries. To this end, the data collection was evenly distributed across the cluster nodes in a partitioned local index arrangement. Each of the eight nodes constructed an index for its sub-collection, and executed queries against that index. When querying, the server played the role of a receptionist, and received queries, broadcast them to the nodes, and received back the results lists. The final result listing was then prepared by the receptionist by merging the node result sets, using the local similarity scores supplied by the nodes. There was no exchange of global information between the nodes and the receptionist, and all scoring and ranking was on the basis of local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance in the Terabyte Track</head><p>This section reports the settings and outcomes of our experiments in the 2004 Terabyte Track.</p><p>Hardware configuration Two alternative hardware configurations were explored:</p><p>1. Cluster: The cluster of eight nodes plus the server, as described in Section 5 is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Single:</head><p>The server only is employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency metrics</head><p>The efficiency metrics reported consist of:</p><p>• IndexTime: The elapsed time for index construction. Only three time components were included: primary indexing time, merging time, and sorting time. The time required to transfer the original collection to the nodes, and for preprocessing the anchor text, URL weights, and authority weights, was not included. In particular, the time spent preprocessing anchor text would have been large if the extra information supplied by NIST (URL-id and link files) had not been available.</p><p>• IndexSize: The total size of the index, summed across all nodes. In calculating the index size, all index components (inverted file, lexicon, URL weight file, authority weight file) were taken into account. The document representation and the mapping from document number to the document location were not counted.</p><p>• AverageQueryTime: Queries were processed sequentially, with a single query active in the system at any given time. Each query is considered as being "finished" once the result list of document identifications is finalized, without any time being spent to actually retrieve the answer documents. The elapsed time was measured from the moment when the first query arrives, until the last query has been processed, not including the initialization costs associated with loading a range of memory-resident files. The total time is then divided by the number of queries in the stream to obtain AverageQueryTime. Run settings Five runs were submitted to the Terabyte Track. Of the five, four runs used the full power of the cluster, and one used the server only. In terms of similarity computation, two runs used the Content impacts, and three others the Content+Anchor impacts. Three variants of evidence integration were tested: using impacts only (Method A); using authority weights to break ties on impact scores (Method D); and combining all evidence (Method E). The details of the combinations tested are presented in Table <ref type="table" coords="9,303.04,462.09,4.07,9.82" target="#tab_1">2</ref>.</p><p>Efficiency Table <ref type="table" coords="9,183.09,491.13,5.39,9.82" target="#tab_2">3</ref> shows our achievements in terms of efficiency. Using a single machine, we can index the GOV2 collection at a rate of approximately 33 GB per hour, and process queries at the rate of approximately three per second. When all nine machines are engaged, indexing throughput increases by a factor of 7.5, and querying throughput by a factor of 4.5. The latter is smaller than the former (and also smaller than the ideal factor of 8) because we do not support intra-query parallelism. For example, when the receptionist is engaged merging the result lists from the nodes, the nodes are always idle. The use of threading to allow multiple queries to be active at any given moment would increase the throughput of the Single system by a non-trivial amount and of the Cluster system by a greater factor. In all scenarios the cost of storing the compressed index is very small.</p><p>Effectiveness Effectiveness scores for our Terabyte runs are shown in Table <ref type="table" coords="9,449.89,638.49,4.07,9.82" target="#tab_3">4</ref>. The runs that employ the incoming anchor text failed completely, the result of a programming error, and the last three rows of the table demonstrate only that buggy programs remain the bane of these experiments. Hence, our discussion focuses on the results for the content-only and content-and-linkstructure runs.</p><p>Relative to the title-only runs lodged by the TREC participants, the performance of MU04tb1 and MU04tb4 is good, especially in terms of P@5, P@10, P@20 (where one or the other of the two runs holds the first position over all title-only submitted runs) and Recip.Rank (where the two runs obtain the second and third positions over all title-only submitted runs).</p><p>On the other hand, Table <ref type="table" coords="9,231.26,756.81,4.07,9.82" target="#tab_3">4</ref>, also shows that although the use of structure link information to break the ties in defining impact order did give some positive results, the gain is marginal.</p><p>Considering the large effort spent for collecting data and calculating pagerank scores, the modest gain represents a very small return on effort spent. It is also possible that using the pagerank information in a different way may result in a better gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Directions</head><p>We intend to continue our development of integral impact-based scoring mechanisms. Their speed during querying, and compact index requirements, make them an attractive proposition for web retrieval, and</p><p>have not yet exhausted their potential. For example, we plan to incorporate hub weighting, elements of the document markup and structure, and terms extracted from the URL string. In addition, we plan to examine the possibility of incorporating all evidence at indexing time, to further reduce the query-time costs. Extension to collections beyond the current terabyte is also of considerable interest.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,99.24,412.89,424.79,9.82;7,99.24,425.06,424.76,8.97;7,99.24,436.70,359.00,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of the runs. Retrieval effectiveness for: (a) the topic distillation queries; (b) the named page finding queries; (c) the homepage finding queries; and (d) all 225 mixed queries. Note that the metrics used for topic distillation differ from the metrics used in the other three quadrants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,99.24,736.17,424.14,22.90"><head>Table 1 :</head><label>1</label><figDesc>Parameters used in the web runs.</figDesc><table coords="6,200.40,102.09,222.33,76.07"><row><cell>Run tag</cell><cell cols="2">Type of impacts Integration method</cell></row><row><cell>MU04web5</cell><cell>Content</cell><cell>Method A</cell></row><row><cell>MU04web2</cell><cell>Content</cell><cell>Method B</cell></row><row><cell>MU04web3</cell><cell>Content</cell><cell>Method C</cell></row><row><cell>MU04web4</cell><cell>Content</cell><cell>Method E</cell></row><row><cell>MU04web1</cell><cell>Content</cell><cell>Method F</cell></row></table><note coords="5,99.24,736.17,424.14,9.82;5,99.24,749.25,125.97,9.82"><p>This section reports our runs for the Web Track. The listed effectiveness scores are as advised by the NIST email notifications.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,165.96,102.09,285.11,98.62"><head>Table 2 :</head><label>2</label><figDesc>Parameter settings for the Terabyte Track.</figDesc><table coords="8,165.96,102.09,285.11,76.07"><row><cell>Run tag</cell><cell>Type of impacts</cell><cell cols="2">Intergration method Hardware</cell></row><row><cell>MU04tb1</cell><cell>Content</cell><cell>Method A</cell><cell>Cluster</cell></row><row><cell cols="2">MU04tb2 Content+Anchor</cell><cell>Method A</cell><cell>Cluster</cell></row><row><cell>MU04tb4</cell><cell>Content</cell><cell>Method D</cell><cell>Single</cell></row><row><cell cols="2">MU04tb5 Content+Anchor</cell><cell>Method D</cell><cell>Cluster</cell></row><row><cell cols="2">MU04tb3 Content+Anchor</cell><cell>Method F</cell><cell>Cluster</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,108.24,102.09,406.45,236.63"><head>Table 3 :</head><label>3</label><figDesc>Efficiency outcomes for the Terabyte Track.</figDesc><table coords="9,108.24,102.09,406.45,236.63"><row><cell cols="6">IndexSize Type of impacts Hardware in GB % of collection (minutes) (seconds per query) IndexTime AverageQueryTime</cell></row><row><cell cols="2">Content+Anchor Cluster</cell><cell>7.18</cell><cell>1.68</cell><cell>153</cell><cell>0.08</cell></row><row><cell>Content</cell><cell>Cluster</cell><cell>7.06</cell><cell>1.65</cell><cell>104</cell><cell>0.08</cell></row><row><cell>Content</cell><cell>Single</cell><cell>5.60</cell><cell>1.31</cell><cell>785</cell><cell>0.36</cell></row><row><cell>Run tag</cell><cell cols="2">MAP R-prec</cell><cell>P@5</cell><cell cols="2">P@10 P@20 Recip.Rank</cell></row><row><cell cols="2">Content-only:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MU04tb1 0.2657 0.3318</cell><cell>0.6000</cell><cell>0.5857 0.5378</cell><cell>0.7556</cell></row><row><cell cols="3">Content and link structure:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MU04tb4 0.2679 0.3336</cell><cell>0.6122</cell><cell>0.5878 0.5449</cell><cell>0.7599</cell></row><row><cell cols="4">Content, anchor text, and link structure:</cell><cell></cell><cell></cell></row><row><cell cols="3">MU04tb2 0.0634 0.1210</cell><cell>0.3633</cell><cell>0.3408 0.2898</cell><cell>0.5253</cell></row><row><cell cols="3">MU04tb3 0.0434 0.0924</cell><cell>0.3388</cell><cell>0.2735 0.2429</cell><cell>0.4776</cell></row><row><cell cols="3">MU04tb5 0.0639 0.1218</cell><cell>0.3755</cell><cell>0.3408 0.2939</cell><cell>0.5616</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,198.48,351.45,223.27,9.82"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness outcomes for the Terabyte Track.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was supported by the <rs type="funder">Australian Research Council</rs>, the <rs type="funder">ARC Center for Perceptive and Intelligent Machines in Complex Environments</rs>, and the <rs type="institution">NICTA Victoria Laboratory</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,99.24,395.13,424.32,9.82;10,110.16,408.21,137.47,9.82" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,139.01,395.13,149.77,9.82">Impact-Based Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, The University of Melbourne</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,99.24,430.41,424.00,9.82;10,110.16,443.49,413.16,9.82;10,110.16,456.69,413.28,9.82;10,110.16,469.77,168.52,9.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,197.33,430.41,252.15,9.82">Impact transformation: effective and efficient web retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,290.87,443.49,232.45,9.82;10,110.16,456.69,268.10,9.82">Proc. 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Myaeng</surname></persName>
		</editor>
		<meeting>25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002-08">August 2002</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,99.24,491.97,424.23,9.82;10,110.16,505.05,413.14,9.82;10,110.16,518.25,77.13,9.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,201.65,491.97,195.19,9.82">Vector space ranking: Can we keep it simple</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,135.05,505.05,245.09,9.82">Proc. 2002 Australian Document Computing Symposium</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Thom</surname></persName>
		</editor>
		<meeting>2002 Australian Document Computing Symposium<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12">December 2002b</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,99.24,540.33,424.21,9.82;10,110.16,553.41,413.20,9.82;10,110.16,566.61,413.15,9.82;10,110.16,579.57,414.17,10.07;10,110.16,592.77,25.75,10.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,211.25,540.33,289.91,9.82">Robust and web retrieval with document-centric integral impacts</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec12/t12_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,284.19,553.41,234.52,9.82">The Twelth Text REtrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>Special Publication</publisher>
			<date type="published" when="2003-11">November 2003</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,99.24,614.97,424.05,9.82;10,110.16,628.17,298.80,9.82" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,199.73,614.97,323.55,9.82;10,110.16,628.17,80.88,9.82">Inverted index compression using word-aligned binary codes. Source code available from www</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<ptr target="cs.mu.oz.au/~alistair/carry/" />
		<imprint>
			<date type="published" when="2005-01">January 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,99.24,650.25,424.19,9.82;10,110.16,663.33,413.27,9.82;10,110.16,676.41,370.94,10.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,193.74,650.25,267.34,9.82">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<ptr target="http://www7.scu.edu.au/programme/fullpapers/1921/com1921.htm" />
	</analytic>
	<monogr>
		<title level="m" coord="10,483.20,650.25,40.23,9.82;10,110.16,663.33,186.74,9.82">Proc. 7th International World Wide Web Conference</title>
		<meeting>7th International World Wide Web Conference<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-04">April 1998</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,99.24,698.61,424.08,9.82;10,110.16,711.81,305.02,9.82" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,252.53,698.61,270.79,9.82;10,110.16,711.81,47.99,9.82">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
