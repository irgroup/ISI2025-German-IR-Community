<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,118.92,115.70,379.99,15.49;1,274.68,137.66,68.81,15.49">University of Lethbridge&apos;s Participation in TREC-2004 QA Track</title>
				<funder>
					<orgName type="full">Alberta Heritage Foundation for Science and Engineering Research</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.40,171.49,57.43,10.76"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
							<email>chali@cs.uleth.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Lethbridge</orgName>
								<address>
									<addrLine>4401 University Drive Lethbridge</addrLine>
									<postCode>T1K 3M4</postCode>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.75,171.49,76.70,10.76"><forename type="first">Stephen</forename><surname>Dubien</surname></persName>
							<email>dubiensÂ¡@cs.uleth.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Lethbridge</orgName>
								<address>
									<addrLine>4401 University Drive Lethbridge</addrLine>
									<postCode>T1K 3M4</postCode>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,118.92,115.70,379.99,15.49;1,274.68,137.66,68.81,15.49">University of Lethbridge&apos;s Participation in TREC-2004 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A626AA0F992B9339394C0022E371F9A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text Retrieval Conference (TREC), organised by National Institute for Standards and Technology (NIST), is a set of tracks that represent different areas of text retrieval. These tracks provide a way to measure systems progress in certain fields in text retrieval such as crosslanguage retrieval, retrieval filtering and genomics. We participated in the question answering track.</p><p>The questions in the TREC-2004 QA track are clustered by target, which is the topic of the question. The QA track for 2004 has three types of questions: factoid questions that require only one correct response, list questions that require a non redundant list of correct responses and other questions that require a non redundant list of facts about the target that has not already been discovered by a previous answer. These questions will be answered using the AQUAINT collection, a collection of over a million newswire documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The global architecture of our system is shown in Figure <ref type="figure" coords="1,370.05,503.89,4.07,9.82" target="#fig_0">1</ref>. Each of the modules in the system will be described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Topic Handler</head><p>This module is the driver for the system that handles the question file that is given for this track. This module extracts from the question file the target and the set of questions that corresponds to that topic. The target is then passed to the passage retrieval module.</p><p>The text of the question and question type (e.g. list, factoid or other) are then given to the question information extractor. For redundancy checking for the other questions, after each question is answered, the answers collected by this module and a list of all the answers for a topic is passed to the question information extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>The AQUAINT collection is quite big and processing each document at runtime would be time consuming. An information retrieval system can be used to index the documents and a system can query the documents. This will allow a system to process less documents. NIST also provides their top 1000 ranked documents for a question although, they do not guarantee that the answer will be contained in those 1000 documents. We used Managing Gigabytes <ref type="bibr" coords="2,157.22,606.73,139.01,9.82" target="#b2">(Witten, Muffat, and Bell, 1999)</ref> to index the AQUAINT collection by paragraph. Indexing by paragraph instead of by document, increases the frequency that the retrieved data is related to the target.</p><p>This module creates a query from the target. We are also using boolean queries rather than ranked queries. The query will retrieve all documents that contain the target because boolean queries retrieve all the documents that exactly match the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Tagger</head><p>A program does not understand the meaning of words. Taggers and parsers are used to convey to the system, knowledge of what the words are and how they relate to each other. The system needs to be able to handle the information that is extracted from a document for it to be useful.</p><p>We tag the documents returned by the passage retrieval module with the Oak System tagger <ref type="bibr" coords="3,140.82,164.89,63.82,9.82" target="#b1">(Sekine, 2002)</ref> with named entities and chunked parts of speech. Adding a syntactic parser to this module would improve the knowledge gained about the passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Question Information Extractor</head><p>The information extracted from the question will give important clues on how to discover the answer. If no information is extracted from the question the correct answer will be impossible to find. Similar to the document tagger, the more information extracted from the question and used, the more precise the system will be at understanding the words and finding the answer.</p><p>This module and following modules have separate sections for the different question types because they handle each of them differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Factoid and List</head><p>This module determines the answer type, question type and the important words from the questions that give clues about where an answer could be found. Answer type of a question is useful because it gives the system knowledge of which named entity to look for. The question type is useful because it gives clues for which patterns the answer is likely to be found in.</p><p>The words of the question can give clues to the words that will be found around the answer. Matching words in the question to the documents retrieved is not the most intelligent way of approaching answering questions. A more efficient approach would be to put the question into a dependency form <ref type="bibr" coords="3,272.79,469.45,125.62,9.82" target="#b0">(Pasca and Harabagiu, 2001)</ref> and use it to match with the dependency form of a prospective answer. Finding word dependencies will require a syntactic parse of the question and retrieved passages. Proper nouns from the question are going to be represented in any paragraph containing a possible answer. Other words in the question might be represented in the question by a synonym which will not be found by simple pattern matching. Proper nouns will be used in answer extraction and other words will be used later in answer ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Other</head><p>Other questions are stated as the string "Other" with no extra information to extract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Extractor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Factoid and List</head><p>This module works with the tagged passages and the proper nouns from the question. A passage is sent to the answer ranking module if the passage contains the proper nouns from the question, and a named entity that matches the answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Other</head><p>Answers for other questions are extracted using patterns for finding facts about a target. An example of one of the patterns the system uses, if the target is a person, is "TOPIC,fact," if fact is not a name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer Ranker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Factoid</head><p>The answer extractor may return multiple possible answers for a factoid question. This module gives a rank to each possible answer and returns the one with the highest rank. A possible answer is given weighted ranks for: how many times it appear in the possible answer list, if the passage it is found in include words from the question, and if it is found in a pattern associated with the type of question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">List</head><p>For list questions, every possible answer might not be a correct answer. This module will give points to each answer in a way similar to factoid questions. The answers that get high enough scores will be included in the list of answers given to the redundancy checker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Other</head><p>We do not currently have a method for ranking facts for the other questions and all facts are passed to the redundancy checker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Redundancy Checker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.1">Factoid</head><p>Factoid questions require only one answer so no redundancy checking is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.2">List and Other</head><p>For list and other questions, a system will lose ranks for an answer list that contains redundancies. When this module returns an answer, it checks to see if the answer has already been in the list of answers given. This check is performs by checking if the answer contains a non-trivial word from answers already given. Examples of trivial words are: stop words, pronouns and prepositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>This is the first year that University of Lethbridge has participated in TREC question answering track and we did not rank as competitively as we wanted. Table <ref type="table" coords="4,421.89,641.53,5.39,9.82" target="#tab_0">1</ref> outlines the range of scores for each type of question in the QA track for all participants and the results for our system (UofL1). If the questions that have no known answer in the documents were removed, our system would have received 0.043 for the factoid questions. The lower result is because our system will return NIL if the answer type of the question is undetermined. NIST changes the type of questions they ask from year to year to keep the question answering track challenging and to move the field in the direction of being able to answer any question. For instance, questions that start with "Where" or "Who" made up less then one percent of questions in the question answering track in 2003. This is significantly different then TREC-2004 where "Where" and "Who" questions made up 25 percent of the questions.</p><p>A system that ranks consistently high in the QA Track has taken into account many different domains of questions. Our system ranked low because we did not consider enough question domains. After receiving our TREC results, we were able to change our system to recognise a greater variety of question to answer type patterns. This change raised our attempts at answering questions from TREC-2004 from 24% to 70%. Without performing any other changes to our system the increase in attempts raised our overall accuracy from .104 to .217 on the questions from TREC-2004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The TREC-2004 question answering track has been a learning experience for us. Preparation for next year will include many improvements to our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,193.68,532.81,230.07,9.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model of Our Question Answering System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,175.80,75.25,265.76,73.66"><head>Table 1 :</head><label>1</label><figDesc>EvaluationsResults for QA Track fromTREC-2004    </figDesc><table coords="5,196.92,75.25,223.81,50.86"><row><cell cols="3">Question Type Median Worst Best UofL1</cell></row><row><cell>Factoid</cell><cell>0.170</cell><cell>0.009 0.770 0.104</cell></row><row><cell>List</cell><cell>0.094</cell><cell>0.000 0.622 0.024</cell></row><row><cell>Other</cell><cell>0.184</cell><cell>0.000 0.460 0.023</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> research grant and the <rs type="funder">Alberta Heritage Foundation for Science and Engineering Research</rs> under the Research Excellence Envelope funding.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,111.00,567.01,395.13,9.82;5,127.92,580.57,378.20,9.82;5,127.92,594.13,333.40,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,288.63,567.01,174.68,9.82">Answer mining from on-line documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,486.73,567.01,19.40,9.82;5,127.92,580.57,344.59,9.82">Proceedings of 39th Annual Meeting and 10th Conference of the European Chapter</title>
		<meeting>39th Annual Meeting and 10th Conference of the European Chapter<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.00,616.57,85.04,9.82;5,214.75,616.57,291.29,9.82;5,127.92,630.13,109.76,9.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<ptr target="http://nlp.cs.nyu.edu/oak" />
		<title level="m" coord="5,214.75,616.57,286.82,9.82">Proteus Project -OAK System (English Sentence Analyzer)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,111.00,652.69,394.83,9.82;5,127.92,666.25,194.03,9.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,290.39,652.69,215.44,9.82;5,127.92,666.25,99.73,9.82">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Muffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
