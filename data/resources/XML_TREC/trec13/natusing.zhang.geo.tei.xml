<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,55.92,76.44,500.08,16.65;1,235.68,98.04,140.57,16.65">Experience of Using SVM for the Triage Task in TREC2004 Genomics Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,136.32,121.30,58.74,11.10"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
							<email>dell.z@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<address>
									<postCode>S16-05-08</postCode>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Science Drive</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University</orgName>
								<address>
									<postCode>117543</postCode>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive</addrLine>
									<postCode>E4-04-10</postCode>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<address>
									<postCode>117576</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<address>
									<postCode>SOC1-05-26</postCode>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Science Drive</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">National University</orgName>
								<address>
									<postCode>117543</postCode>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive</addrLine>
									<postCode>E4-04-10</postCode>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<address>
									<postCode>117576</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,55.92,76.44,500.08,16.65;1,235.68,98.04,140.57,16.65">Experience of Using SVM for the Triage Task in TREC2004 Genomics Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE0C76BDD9B5DCB23FC4828E988CD2DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Categorization</term>
					<term>Machine Learning</term>
					<term>Support Vector Machine</term>
					<term>Feature Selection</term>
					<term>Distribution Change</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports our knowledge-ignorant machine learning approach to the triage task in TREC2004 genomics track, which is actually a text categorization problem. We applied Support Vector Machine (SVM) and found that information-gain based feature selection is helpful. Although we achieved decent performance in leave-one-out cross-validation experiments, the evaluation result on the test data turned out to be surprisingly poor. Further experiments revealed that there is a chasm between the training and test data distributions. It seems that more aggressive feature selection can partially alleviate the trouble caused by distribution change.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this year's TREC conference, we have only tried to attack the triage task of the genomics track 1 . The goal of this task is to correctly identify which mouse-related papers have been deemed to have experimental evidence warranting annotation with GO 2 codes by MGI 3 . It is exactly a text categorization <ref type="bibr" coords="1,170.47,572.07,16.80,9.07" target="#b11">[11]</ref> problem. Since we do not have any biological or medical background, we just took a knowledge-ignorant machine learning <ref type="bibr" coords="1,211.68,596.08,11.76,9.07" target="#b8">[8]</ref> approach. Support Vector Machine (SVM) <ref type="bibr" coords="1,204.72,614.08,10.92,9.07" target="#b1">[1,</ref><ref type="bibr" coords="1,222.72,614.08,13.44,9.07" target="#b10">10]</ref> is generally regarded as one of the most powerful machine learning methods. It has shown very promising performances in a number of recent text categorization studies <ref type="bibr" coords="1,236.64,650.08,10.92,9.07" target="#b2">[2,</ref><ref type="bibr" coords="1,251.04,650.08,7.56,9.07" target="#b6">6,</ref><ref type="bibr" coords="1,262.08,650.08,7.98,9.07" target="#b12">12</ref>]. An 1 http://medir.ohsu.edu/~genomics/2004protocol.html 2 http://www.geneontology.org/ 3 http://www.informatics.jax.org/ efficient SVM implementation, SVMlight<ref type="foot" coords="1,494.16,275.49,3.24,5.83" target="#foot_0">4</ref>  <ref type="bibr" coords="1,500.64,277.59,10.71,9.07">[5]</ref>, was used throughout our experiments.</p><p>In fact, we achieved decent performance in leave-one-out cross-validation experiments. However, to our surprise, the evaluation result on the test data is quite poor. We present our approach in section 2, and investigate the reason of failure in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The given document collection consists of mouse-related articles from three journals (JBC, JCB and PNAS) over two years <ref type="bibr" coords="1,368.16,437.92,74.50,9.07">(2002 and 2003)</ref> Each example corresponds to a journal article which can be uniquely identified by its PMID. The SGML format full-text information of each example was provided. Furthermore, we fetched the XML format bibliographic information of each example from the PubMed<ref type="foot" coords="1,507.60,561.81,3.24,5.83" target="#foot_1">5</ref> server. We obeyed the "separate" strictness of data usage: no information from any test example is allowed to affect the processing of any other test example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>To apply SVM, the data need to be represented as vectors. Inspired by the traditional bag-of-words <ref type="bibr" coords="1,485.04,656.07,11.76,9.07" target="#b6">[6]</ref> representation of text documents, we converted each example into a bagof-features through the feature extraction and selection process explained later. Then we constructed a vector for each example based on its bag-of-features: the entries/dimensions of the vectors correspond to all distinct features, and the value of each entry is the weight of its corresponding feature. Here we used the SMART <ref type="bibr" coords="2,282.24,146.80,11.76,9.07" target="#b9">[9]</ref> word-vector-weighting 6 scheme ltc. Finally all vectors are normalized to have unit length.</p><p>For each example, the features are extracted from the following fields of its semi-structured bibliographic and full-text information: MESH, JOURNAL, CHEMICAL, GRANT, AUTHOR, AFFILIATION, TITLE, ABSTRACT, ST (section title) and CAPTION (table/figure caption). Especially the CAPTION fields have been reported to be quite useful for a similar task <ref type="bibr" coords="2,175.32,260.80,15.46,9.07" target="#b14">[14]</ref>.</p><p>MESH 7 (Medical Subject Headings) is a controlled vocabulary produced by the National Library of Medicine and used for indexing, cataloging, and searching for biomedical and health-related information and documents. All MESH descriptor are organized in a tree structure. Each MESH descriptor can be mapped to a specific node in the MESH tree. For each MESH descriptor of the given example, we found its corresponding node in the MESH tree. Then we would generate a feature for every node in the path from the root to that node. If the MESH descriptor is modified by a qualifier (subheading), we would generate a new feature identified by that descriptor plus that qualifier. If the MESH descriptor/qualifier is considered describing the major topic of the document, we would generate another new feature indicating that it is a major MESH term. For example, the MESH term &lt;MeshHeading&gt; &lt;DescriptorName MajorTopicYN="N"&gt; Transcription Factors &lt;/DescriptorName&gt; &lt;QualifierName MajorTopicYN="Y"&gt; physiology &lt;/QualifierName&gt; &lt;/MeshHeading&gt; would be converted into a set of features:</p><p>MESH_D12, MESH_D12_776, MESH_D12_776_930, MESH_D12_776_930_Q000502, MESH_D12_776_930_Q000502_MAJOR, where "D12.776.930" indicates the position of the MESH descriptor "Transcription Factors" in the MESH tree, and "Q000502" is the ID of the qualifier "physiology". 6 http://people.csail.mit.edu/people/jrennie/ecoc-svm/smart.html 7 http://www.nlm.nih.gov/mesh/meshhome.html For the JOURNAL, CHEMICAL, GRANT, AUTHOR and AFFILIATION fields, every specific entity would be treated as a feature. For example, such kind of features of the example with PMID 11677243 would include: JOURNAL_0021_9258, CHEMICAL_Plasmids, GRANT_ID_HL_4518, and AUTHOR_WM_Canfield, etc.</p><p>For the TITLE, ABSTRACT, ST and CAPTION fields, the contained texts would be extracted and canonicalized by the UMLS SPECIALIST lexical tool LuiNorm 8 , then we would generate two features for every specific term in the texts: one is the term itself, the other is the term tagged by its occurring field. For example, such kind of features of the example with PMID 11677243 would include: clone, TITLE_clone, mouse, ABSTRACT_mouse, rna, CAPTION_rna, etc.</p><p>The feature selection criterion we used is the informationgain, since it has been shown to work well for various text categorization tasks <ref type="bibr" coords="2,404.44,305.20,10.92,9.07" target="#b3">[3,</ref><ref type="bibr" coords="2,420.00,305.20,11.97,9.07" target="#b13">13]</ref>. The decline of informationgain across features is very sharp, as shown in Figure <ref type="figure" coords="2,536.64,317.20,3.78,9.07" target="#fig_1">1</ref>.</p><p>It is generally believed that SVM requires no or very little feature selection for text categorization <ref type="bibr" coords="2,482.83,545.20,10.71,9.07" target="#b6">[6]</ref>. However, we have found that aggressive feature selection is very helpful to SVM for this specific problem. We think the reason is that a large number of features generated by the above feature extraction method are irrelevant or redundant. This finding is consistent with a recent study <ref type="bibr" coords="2,481.44,605.20,10.71,9.07" target="#b3">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Runs</head><p>We used linear kernel and accepted the default values for all parameters of SVMlight except C and J. The parameter C determines the trade-off between training error and margin, while the parameter J specifies the cost-factor by Our tactic for parameter tuning is pretty much like that of <ref type="bibr" coords="3,54.00,128.80,10.71,9.07" target="#b7">[7]</ref>. We trained SVM classifiers with different parameter settings and estimated their performance by leave-one-out cross-validation (LOOCV). SVMlight can compute LOOCV performances very efficiently using a clever algorithm that prunes away cross-validation folds that do not need to be explicitly executed <ref type="bibr" coords="3,207.84,188.80,10.71,9.07" target="#b4">[4]</ref>. In addition, we found that a faster approximate version of pruning (the options "-x 1" and "-o 1") gave almost identical estimates as the exactly correct version of pruning (options "-x 1" and "-o 2"). A minor complexity was that SVMlight only outputs LOOCV estimates of error-rate, precision and recall, but the official performance measure is the utility score defined as</p><formula xml:id="formula_0" coords="3,72.96,284.66,115.56,23.04">raw r nr norm max r U u TP u FP U U u A P + = = i i i ,</formula><p>where u r = 20 is the relative utility of a relevant document, and u nr = -1 is the relative utility of a non-relevant document. The solution is to calculate the utility score based on the precision and the recall:</p><formula xml:id="formula_1" coords="3,72.96,360.39,150.83,48.22">1 ( 1 ) nr norm r u U recall recall u precision = + - i i 1 1 ( 1 ) 20 recall recall precision = - - i i</formula><p>.</p><p>Our experiments showed that heuristically setting C = 1 / 20 and J = 20 * (#neg / #pos) generated optimal LOOCV performances, where #pos and #neg represents the number of positive and negative training examples respectively. For the SVM with this parameter setting, the relationship between the LOOCV utility score and the number of selected features is shown in Figure <ref type="figure" coords="3,226.05,486.16,3.78,9.07" target="#fig_3">2</ref>. The optimal LOOCV performance was achieved using about 20,000 features, which is only 8% of all features.</p><p>We submitted four runs with slight different feature selection levels: nusbird2004a, nusbird2004b, nusbird2004d, and nusbird2004e. The best performing run among these four submissions is nusbird2004a that used 20,192 features. Its LOOCV utility score on the training data is 0.8892. However, its utility score on the test data is only 0.2302, even worse than the baseline run that classifies all examples as positive. Such a large discrepancy is surprising.</p><p>In addition to these four regular runs, we also submitted another run nusbird2004c, using extra positive training examples from the ground-truth MGI database. On the MGI website, there is a database report file named go_refs.mgi 9 , which is supposed to contain all "references used in GO annotations to mouse Markers". There were <ref type="bibr" coords="3,317.76,98.80,9.12,9.07" target="#b3">3,</ref><ref type="bibr" coords="3,326.88,98.80,13.68,9.07">817</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATA DISTRIBUTION CHANGE</head><p>Why did the classifiers with high LOOCV performance on the training data worked so poorly on the test data? We suspect that this strange phenomenon is due to the data distribution change. Most machine learning algorithms, including SVM, make the assumption that all training and test examples are independently and identically distributed. The data distribution change could make the basis of SVM invalid.</p><p>To scrutinize this problem, we model the training and test data as two multinomial probability distributions and employ KL-divergence (also known as relative entropy) to measure their dissimilarity. Note that this can only be done with the availability of the test instances although it does not require the test labels. At this stage, we only used the official training data, and stuck to the SVM parameters C = 1 / 20 and J = 20 * (#neg / #pos).</p><p>One possible reason for the data distribution change is that we have chosen many inappropriate features. Therefore we tried to apply more aggressive feature selection to improve the performance of SVM. With the feature set used by nusbird2004a, the KL-divergence between the training and test data distributions was 0.0236. We first unconditionally removed all features that occurred less than 3 times in the training data. The KL-divergence dropped to 0.0136. Meanwhile, the utility score on the test data dramatically increased from 0.2302 to 0.4935, although the LOOCV performance became worse. Then we performed more aggressive feature selection based on information-gain. The KL-divergence decreased while fewer features were selected, as shown in Figure <ref type="figure" coords="4,185.04,258.88,3.78,9.07" target="#fig_4">3</ref>. If 1,000 features were selected, the KL-divergence would be 0.0121 and the utility score on the test data would be 0.5779.</p><p>Another factor that could be partially responsible for the data distribution change is the labeling error (noise). We compared the training data with the ground-truth positive data, go_refs.mgi (dated on Aug 8, 2004). There were 233 examples in their intersection, while 52 of them were labeled negative, i.e., mis-labeled. Therefore the positive noise rate (the fraction of mis-labeled positive examples among all positive examples) was roughly at the scale of 22.32% (52/233). Such a high level positive noise rate was not negligible, especially when using a utility score dominated by recall to measure performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This triage task is simply a binary text categorization problem, yet it has some interesting properties: the documents are semi-structured, the evaluation measure is a utility score that puts very high weights on true positive examples, the number of positive examples is small and the training and test data distributions have a noticeable difference. Our preliminary finding is that feature selection plays an important role to help SVM achieve good classification performance for this task. Its effect is twofold: removing irrelevant/redundant features and coping with distribution change.</p><p>Our best submitted run got the utility score 0.4440, with the help from information-gain based feature selection and extra positive training examples. Through more aggressive feature selection, we can achieve the utility score as high as 0.5779. How far can we expect a knowledge-ignorant approach to go for this task? </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,442.67,437.92,115.10,9.07;1,317.76,449.92,240.01,9.07;1,317.76,461.92,240.15,9.07;1,317.76,473.92,239.92,9.07;1,317.76,485.92,240.14,9.07;1,317.76,497.92,158.04,9.07"><head></head><label></label><figDesc>. The first year's (2002) documents comprise the training data, while the second year's (2003) documents make up the test data. The training data include 375 positive examples and 5462 negative examples. The test data include 420 positive examples and 5623 negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,324.96,514.17,213.01,8.21"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of features by information-gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,343.92,98.80,213.71,9.07;3,317.76,110.80,240.00,9.07;3,317.76,122.80,240.17,9.07;3,317.76,134.80,239.88,9.07;3,317.76,146.80,240.00,9.07;3,317.76,158.80,240.45,9.07;3,317.76,170.80,239.77,9.07;3,317.76,182.80,239.76,9.07;3,317.76,194.80,239.99,9.07;3,317.76,206.80,240.00,9.07;3,317.76,218.80,240.00,9.07;3,317.76,230.80,239.93,9.07;3,317.76,242.80,240.16,9.07;3,317.76,254.80,121.32,9.07"><head></head><label></label><figDesc>examples in the go_refs.mgi file (dated on Aug 28, 2004) after removing those contained in the training and test data. We took those examples as extra positive examples, and used them to augment the training data. The run nusbird2004c set the parameter C = 1, J = 20 * (#neg / #pos), and used 9,162 features. Its LOOCV utility score on the training data is 0.8968, and its utility score on the test data is 0.4440. This is our best official evaluation result. Note that the extra positive training examples did not include any test example. A few extra positive training examples were published after 2002. We also did experiments with those "future" examples removed from the augmented training data and found that the performance was not affected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,324.96,451.29,219.50,8.21;3,324.96,462.09,126.83,8.21"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LOOCV performance on the training data at different feature selection levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,61.20,481.53,219.52,8.21;4,61.20,492.33,208.92,8.21"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: KL-divergence between the training and test data distributions at different feature selection levels.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="1,323.04,692.73,105.26,8.21"><p>http://svmlight.joachims.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="1,323.04,707.61,169.33,8.21"><p>http://www.ncbi.nlm.nih.gov/entrez/query.fcgi</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,322.26,237.40,91.94,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,250.95,214.56,9.07;4,335.76,262.95,209.30,9.07;4,335.76,274.96,118.68,9.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,485.76,250.95,64.56,9.07;4,335.76,262.95,111.87,9.07">An Introduction to Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,291.03,215.01,9.07;4,335.76,303.04,211.09,9.07;4,335.76,315.04,202.84,9.07;4,335.76,327.04,208.31,9.07;4,335.76,339.04,200.53,9.07;4,335.76,351.04,61.56,9.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,335.76,303.04,211.09,9.07;4,335.76,315.04,93.44,9.07">Inductive Learning Algorithms and Representations for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,446.88,315.04,91.72,9.07;4,335.76,327.04,208.31,9.07;4,335.76,339.04,132.31,9.07">Proceedings of the 7th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 7th ACM International Conference on Information and Knowledge Management (CIKM)<address><addrLine>Bethesda, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,367.12,163.84,9.07;4,335.76,379.12,219.19,9.07;4,335.76,391.12,181.52,9.07;4,335.76,403.12,203.71,9.07;4,335.76,415.12,193.44,9.07;4,335.76,427.12,196.44,9.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,481.20,367.12,18.40,9.07;4,335.76,379.12,219.19,9.07;4,335.76,391.12,181.52,9.07;4,335.76,403.12,90.35,9.07">Text Categorization with Many Redundant Features: Using Aggressive Feature Selection to Make SVMs Competitive with C4.5</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,444.00,403.12,95.47,9.07;4,335.76,415.12,193.44,9.07;4,335.76,427.12,28.08,9.07">Proceedings of the 21st International Conference on Machine Learning (ICML)</title>
		<meeting>the 21st International Conference on Machine Learning (ICML)<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,443.20,216.17,9.07;4,335.76,455.20,132.12,9.07" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,390.00,443.20,161.93,9.07;4,335.76,455.20,66.75,9.07">Learning to Classify Text using Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,471.28,197.76,9.07;4,335.76,483.28,219.31,9.07;4,335.76,495.28,195.04,9.07;4,335.76,507.28,214.19,9.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,390.00,471.28,143.52,9.07;4,335.76,483.28,34.31,9.07">Making large-Scale SVM Learning Practical</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,373.20,495.28,157.60,9.07;4,335.76,507.28,64.61,9.07">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT-Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.75,523.36,219.84,9.07;4,335.76,535.36,216.56,9.07;4,335.76,547.36,201.40,9.07;4,335.76,559.36,201.27,9.07;4,335.76,571.36,61.56,9.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,389.99,523.36,165.60,9.07;4,335.76,535.36,201.82,9.07">Text Categorization with Support Vector Machines: Learning with Many Relevant Features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.76,547.36,201.40,9.07;4,335.76,559.36,108.31,9.07">Proceedings of the 10th European Conference on Machine Learning (ECML)</title>
		<meeting>the 10th European Conference on Machine Learning (ECML)<address><addrLine>Chemnitz, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,587.44,221.45,9.07;4,335.76,599.44,205.17,9.07;4,335.76,611.44,206.38,9.07;4,335.76,623.44,205.08,9.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,387.36,587.44,169.85,9.07;4,335.76,599.44,190.18,9.07">Applying Support Vector Machines to the TREC-2001 Batch Filtering and Routing Tasks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.76,611.44,206.38,9.07;4,335.76,623.44,28.70,9.07">Proceedings of the 10th Text Retrieval Conference (TREC)</title>
		<meeting>the 10th Text Retrieval Conference (TREC)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="286" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,639.52,209.40,9.07;4,335.76,651.52,48.60,9.07" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m" coord="4,386.88,639.52,73.25,9.07">Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,667.60,205.44,9.07;4,335.76,679.60,137.40,9.07" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="4,379.44,667.60,118.15,9.07">The SMART Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,335.76,695.68,218.52,9.07;4,335.76,707.68,141.72,9.07" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="4,461.04,695.68,88.99,9.07">Learning with Kernels</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,74.79,211.91,9.07;5,72.00,86.79,211.44,9.07;5,72.00,98.80,12.60,9.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="5,129.59,74.79,154.31,9.07;5,72.00,86.79,58.12,9.07">Machine Learning in Automated Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,137.04,86.79,101.24,9.07">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.00,114.87,195.04,9.07;5,72.00,126.88,212.39,9.07;5,72.00,138.88,197.76,9.07;5,72.00,150.88,191.64,9.07;5,72.00,162.88,111.48,9.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="5,161.04,114.87,106.00,9.07;5,72.00,126.88,95.28,9.07">A Re-examination of Text Categorization Methods</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,185.52,126.88,98.87,9.07;5,72.00,138.88,197.76,9.07;5,72.00,150.88,187.40,9.07">Proceedings of the 22nd ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 22nd ACM International Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.75,74.80,217.92,9.07;5,335.76,86.80,177.67,9.07;5,335.76,98.80,215.06,9.07;5,335.76,110.80,219.87,9.07;5,335.76,122.80,17.64,9.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="5,453.59,74.80,100.08,9.07;5,335.76,86.80,162.83,9.07">A Comparative Study on Feature Selection in Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,335.76,98.80,215.06,9.07;5,335.76,110.80,105.84,9.07">Proceedings of the 14th International Conference on Machine Learning (ICML)</title>
		<meeting>the 14th International Conference on Machine Learning (ICML)<address><addrLine>Nashville, TN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.76,138.88,181.12,9.07;5,335.76,150.88,184.09,9.07;5,335.76,162.88,214.48,9.07;5,335.76,174.88,188.04,9.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="5,335.76,150.88,184.09,9.07;5,335.76,162.88,214.48,9.07;5,335.76,174.88,14.49,9.07">Evaluation of Text Data Mining for Database Curation: Lessons Learned from the KDD Challenge Cup</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,357.84,174.88,58.58,9.07">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
