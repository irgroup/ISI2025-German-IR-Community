<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.60,101.79,298.68,15.58">Overview of the TREC 2004 Novelty Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,274.68,134.26,62.71,10.87"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.60,101.79,298.68,15.58">Overview of the TREC 2004 Novelty Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F70B23B574BC2D77E81C3E2C7045EA55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2004 marks the third and final year for the novelty track. The task is as follows: Given a TREC topic and an ordered list of documents, systems must find the relevant and novel sentences that should be returned to the user from this set. This task integrates aspects of passage retrieval and information filtering. As in 2003, there were two categories of topics -events and opinions -and four subtasks which provided systems with varying amounts of relevance or novelty information as training data. This year, the task was made harder by the inclusion of some number of irrelevant documents in document sets. Fourteen groups participated in the track this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The novelty track was introduced in TREC 2002 <ref type="bibr" coords="1,287.55,425.01,9.89,9.96" target="#b0">[1]</ref>. The basic task is as follows: given a topic and an ordered set of documents segmented into sentences, return sentences that are both relevant to the topic and novel given what has already been seen. This task models an application where a user is skimming a set of documents, and the system highlights new, on-topic information.</p><p>There are two problems that participants must solve in the novelty track. The first is identifying relevant sentences, which is essentially a passage retrieval task. Sentence retrieval differs from document retrieval because there is much less text to work with, and identifying a relevant sentence may involve examining the sentence in the context of those surrounding it. We have specified the unit of retrieval as the sentence in order to standardize the task across a variety of passage retrieval approaches, as well as to simplify the evaluation.</p><p>The second problem is that of identifying those relevant sentences that contain new information. The operational definition of "new" is information that has not appeared previously in this topic's set of documents. In other words, we allow the system to assume that the user is most concerned about finding new information in this particular set of documents and is tolerant of reading information he already knows because of his background knowledge. Since each sentence adds to the user's knowledge, and later sentences are to be retrieved only if they contain new information, novelty retrieval resembles a filtering task.</p><p>To allow participants to focus on the filtering and passage retrieval aspects separately, the novelty track has four different tasks. The base task was to identify all relevant and novel sentences in the documents. The other tasks provided varying amounts of relevant and novel sentences as training data.</p><p>The track has changed slightly from year to year. The first run in 2002 used old topics and relevance judgments, with sentences judged by new assessors <ref type="bibr" coords="1,345.80,417.69,9.89,9.96" target="#b0">[1]</ref>. TREC 2003 included separate tasks, made the document ordering chronological rather than relevance-based, and introduced new topics and the different topic types <ref type="bibr" coords="1,423.87,453.57,9.98,9.96" target="#b1">[2]</ref>. This year, the major change is the inclusion (or perhaps re-introduction) of irrelevant documents into the document sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Input Data</head><p>The documents for the novelty track are taken from the AQUAINT collection. This collection is unique in that it contains three news sources from overlapping time periods: New York Times News Service <ref type="bibr" coords="1,519.78,568.89,20.02,9.96;1,311.04,580.89,20.12,9.96">(Jun 1998</ref><ref type="bibr" coords="1,335.24,580.89,51.53,9.96">-Sep 2000)</ref>, AP (also <ref type="bibr" coords="1,439.18,580.89,40.63,9.96">Jun 1998</ref><ref type="bibr" coords="1,484.02,580.89,51.53,9.96">-Sep 2000)</ref>, and Xinhua News Service <ref type="bibr" coords="1,424.95,592.89,42.78,9.96">(Jan 1996</ref><ref type="bibr" coords="1,470.73,592.89,49.01,9.96">-Sep 2000)</ref>. As a result, this collection exhibits greater redundancy than other TREC collections, and thus less novel information, increasing the realism of the task.</p><p>The NIST assessors created fifty new topics for the 2004 track. As was done last year, the topics were of two types. Twenty-five topics concerned events, such as India and Pakistan's nuclear tests in 1998, and twenty-five topics focused on opinions about controversial subjects such as the safety of irradiated food and the so-called "abortion pill" RU-486. The topic type was indicated in the topic description by a &lt;toptype&gt; tag. The assessors, in creating their topics, searched the AQUAINT collection using WebPRISE, NIST's IR system, and collected 25 documents they deemed to be relevant to the topic. They also labeled some documents as irrelevant, and all documents judged irrelevant and ranked above the 25 relevant documents were included in the document sets. Note that this means that the irrelevant documents are close matches to the relevant ones, and not random irrelevant documents.</p><p>Once selected, the documents were ordered chronologically. (Chronological ordering is achieved trivially in the AQUAINT collection by sorting document IDs.) The documents were then split into sentences, each sentence receiving an identifier, and all sentences were concatenated together to produce the document set for a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>There are four tasks in the novelty track: Task 1. Given the set of documents for the topic, identify all relevant and novel sentences.</p><p>Task 2. Given the relevant sentences in all documents, identify all novel sentences.</p><p>Task 3. Given the relevant and novel sentences in the first 5 documents only, find the relevant and novel sentences in the remaining documents. Note that since some documents are irrelevant, there may not be any relevant or novel sentences in the first 5 documents for some topics.</p><p>Task 4. Given the relevant sentences from all documents, and the novel sentences from the first 5 documents, find the novel sentences in the remaining documents.</p><p>These four tasks allowed the participants to test their approaches to novelty detection given different levels of training: none, partial, or complete relevance information, and none or partial novelty information.</p><p>Participants were provided with the topics, the set of sentence-segmented documents, and the chronological order for those documents. For tasks 2-4, training data in the form of relevant and novel "sentence qrels" were also given. The data were released and results were submitted in stages to limit "leakage" of training data between tasks. Depending on the task, the system was to output the identifiers of sentences which the system determined to contain relevant and/or novel relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Creation of truth data</head><p>Judgments were created by having NIST assessors manually perform the first task. From the concatenated document set, the assessor selected the relevant sentences, then selected those relevant sentences that were novel. Each topic was independently judged by two different assessors, the topic author and a "secondary" assessor, so that the effects of different human opinions could be assessed.</p><p>The assessors only judged sentences in the relevant documents. Since, by the definition of relevance in TREC, a document containing any relevant information would itself be relevant, the assessors would not miss any relevant information by not judging the sentences in the irrelevant documents. This does give the second assessor some advantage against systems attempting task 1, since the assessor was not confronted with irrelevant documents in the sentence judging phase.</p><p>Since the novelty task requires systems to automatically select the same sentences that were selected manually by the assessors, it is important to analyze the characteristics of the manually-created truth data in order to better understand the system results. The first novelty track topics (in 2002) were created using topics from old TRECs and relevant documents from manual TREC runs, and the sentences judgments were made by new assessors. Those topics had very few relevant sentences and consequently nearly every relevant sentence was novel. Last year's topics, which were each newly developed and judged by a single assessor, resulted in topics with much more reasonable levels of relevant and new information. This year the inclusion of irrelevant documents means that fewer sentences are relevant. Somewhat surprisingly, perhaps, the fraction of relevant sentences which are novel is lower than last year as well.</p><p>Table <ref type="table" coords="2,349.66,616.89,5.03,9.96" target="#tab_0">1</ref> shows the number of relevant and novel sentences selected for each topic by each of the two assessors who worked on that topic. The column marked "assr-1" precedes the results for the primary assessor, whereas "assr-2" precedes those of the secondary assessor. The column marked "rel" is the number of sentences selected as relevant; the next column, "% total", is the percentage of the total set of sentences for that topic that were selected as relevant. The column marked "new" gives the number of sentences selected as novel; the next column, "% rel", is the percentage of relevant sentences that were marked novel. The column "sents" gives the total number of sentences for that topic, and "type" indicates whether the topic is about an event (E) or about opinions on a subject (O).</p><p>Because this year's document sets include irrelevant documents, the fraction of relevant sentences is less than half that of last year: a mean of 19.2%, compared with 41.1% in TREC 2003. However, the amount of novel information as a fraction of relevant is also lower: a 42% this year vs. 64.6% in TREC 2003. This was somewhat surprising as the collection and topic types are the same, and the topics have the same number of relevant documents. Beyond simple intertopic variation, these topics just have more redundant information.</p><p>Opinion topics tended to have fewer relevant sentences than event topics. 25.9% of sentences in event topics were relevant, compared to only 12.6% in opinion topics. Even though the topics are about opinions, the documents are still news stories and thus include current events and background information in addition to the relevant opinion material. The fraction of relevant sentences which were novel was the same for both types, 42%.</p><p>In examining assessor effects, this year we were able to achieve much better balance in the second round of assessing, with each assessor judging five topics written by someone else. Overall, the assessors tended to find about the same amount of relevant information whether they were judging their own topics or someone else's (19.2% for their own topics vs. 21.7% in the second round, not significant by a t-test), but identified more novel sentences (42% vs. 52.6%, significant at p = 0.0009). We have not made a detailed analysis of how the assessors differed in particular judgments or in their judging patterns.</p><p>In summary, the topics for this year seem comparable in quality to the TREC 2003 topics, with minimal assessor effects. The inclusion of irrelevant documents makes the task this year harder for systems, and thus the two topic sets should not be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scoring</head><p>The sentences selected manually by the NIST assessor who created the topic were considered the truth data. The judgments by the secondary assessor were taken as a human baseline performance in the first task. Relevant and novel sentence retrieval have each been evaluated separately.</p><p>Because relevant and novel sentences are returned as an unranked set in the novelty track, we cannot use traditional measures of ranked retrieval effectiveness such as mean average precision. One alternative is to use set-based recall and precision. Let M be the number of matched sentences, i.e., the number of sentences selected by both the assessor and the system, A be the number of sentences selected by the assessor, and S be the number of sentences selected by the system. Then sentence set recall is M/A and precision is M/S.</p><p>As the TREC filtering tracks have demonstrated, set-based recall and precision do not average well, especially when the assessor set sizes vary widely across topics. Consider the following example as an illustration of the problems. One topic has hundreds of relevant sentences and the system retrieves 1 relevant sentence. The second topic has 1 relevant sentence and the system retrieves hundreds of sentences. The average for both recall and precision over these two topics is approximately .5 (the scores on the first topic are 1.0 for precision and essentially 0.0 for recall, and the scores for the second topic are the reverse), even though the system did precisely the wrong thing. While most real submissions won't exhibit this extreme behavior, the fact remains that set recall and set precision averaged over a set of topics is not a robust diagnostic indicator of system performance. There is also the problem of how to define precision when the system returns no sentences (S = 0). Leaving that topic out of the evaluation for that run would mean that different systems would be evaluated over different numbers of topics, while defining precision in the degenerate case to be either 1 or 0 is extreme. (The average scores given in Appendix A defined precision to be 0 when S = 0 since that seems the least evil choice.)</p><p>To avoid these problems, the primary measure for novelty track runs is the F measure. The F measure (from van Rijsbergen's E measure) is a function of set recall and precision, together with a parameter β which determines the relative importance of recall and precision. A β value of 1, indicating equal weight, is used in the novelty track. F β=1 is given as:</p><formula xml:id="formula_0" coords="4,392.64,652.53,64.40,23.52">F = 2 × P × R P + R</formula><p>Alternatively, this can be formulated as For any choice of β, F lies in the range [0, 1], and the average of the F measure is meaningful even when the judgment sets sizes vary widely. For example, the F measure in the scenario above is essentially 0, an intuitively appropriate score for such behavior. Using the F measure also deals with the problem of what to do when the system returns no sentences since recall is 0 and the F measure is legitimately 0 regardless of what precision is defined to be.</p><p>Note, however, that two runs with equal F scores do not indicate equal precision and recall. Figure <ref type="figure" coords="5,295.86,490.89,5.03,9.96" target="#fig_0">1</ref> illustrates the shape of the F measure in precisionrecall space. An F score of 0.5, for example, can describe a range of precision and recall scores. Figure <ref type="figure" coords="5,295.83,526.77,5.03,9.96" target="#fig_0">1</ref> also includes the per-topic scores for a particular run are also plotted. It is easy to see that topics 98, 83, 82, and 67 exhibit a wide range of performance, but all have an F score of close to 0.6. Thus, two runs with equal F scores may be performing quite differently, and a difference in F scores can be due to changes in precision, recall, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>Table <ref type="table" coords="5,100.18,664.65,5.03,9.96" target="#tab_1">2</ref> lists the 14 groups that participated in the TREC 2004 novelty track. Nearly every group attempted the first two tasks, but tasks three and four were less popular than last year, with only 8 groups participating in each (compared to 10 last year). The rest of this section contains short summaries submitted by most of the groups about their approaches to the novelty task. For more details, please refer to the group's complete paper in the proceedings.</p><p>Most groups took a similar high-level approach to the problem, and the range of approaches is not dramatically different from last year. Relevant sentences were selected by measuring similarity to the topic, and novel sentences by dissimilarity to past sentences. As can be seen from the following descriptions, there is a tremendous variation in how "the topic" and "past sentences" are modeled, how similarity is computed when sentences are involved, and what constitutes the thresholds for relevance and novelty. Many groups tried variations on term expansion to improve sentence similarity, some with more success than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Chinese Academy of Sciences -ICT</head><p>In TREC 2004, ICT divided novelty track into four sequential stages. It includes: customized language parsing on original dataset, document retrieval, sentence relevance and novelty detection. In the first preprocessing stage, we applied sentence segmenter, tokenization, part-of-speech tagging, morphological analysis, stop word remover and query analyzer on topics and documents. As for query analysis, we categorized words in topics into description words and content words. Title, description and narrative parts are all merged into query with different weights. In the stage of document and sentence retrieval, we introduced vector space model (VSM) and its variants, probability model (OKAPI) and statistical language model. Based on VSM, we tried various query expansion strategies: pseudo-feedback, term expansion with synset or synonym in WordNet and expansion with highly local co-occurrence terms. With regard to the novelty stage, we defined three types of new degree: word overlapping and its extension, similarity comparison and information gain. In the last three tasks, we used the known results to adjust threshold, estimate the number of results, and turn to classifier, such as inductive and transductive SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CL Research</head><p>The CL Research novelty assessment is based on a full-scale parsing and processing of documents and This significantly increased recall over TREC 2004, without a significant degradation of precision. CL Research's novelty component was unchanged, but precision on Task 2 was considerably lower. This lower precision was observed in other tasks as well, and perhaps reflects the significantly lower scores among all participants. CL Research has set up an evaluation framework to examine the reasons for these lower scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Columbia University</head><p>Our system for the novelty track at TREC 2004, Sum-Seg, for Summary Segmentation, is based on our observations of data we collected for the development of our system to prepare update summaries, or bulletins. We see that new information often appears in text spans of two or more sentences, and at other times, a piece of new information is embedded within a sentence mostly containing previously seen mate-rial. In order to capture both types of cases, we avoided direct sentence similarity measures, and took evidence of unseen words as evidence of novelty. We employed a hill climbing algorithm to learn thresholds for how many new words would trigger a novel classification. We also sought to learn different weights for different types of nouns, for example, persons, or locations or common nouns. In addition, we included a mechanism to allow sentences that had few strong content words to "continue" the classification of the previous sentence. Finally, we used two statistics, derived from analysis of the full AQUAINT corpus, to eliminate low-content words. We submitted a total of five runs: two used learned parameters to aim at high precision output, and one aimed at higher recall. Another run was a straightforward vector-space model used as a baseline, and the last was a combination of the high recall run with the vector-space model.</p><p>Training was done on the 2003 TREC novelty data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dublin City University</head><p>This is the first year that DCU has participated in the novelty track. We built three models; the first focused on retrieving the twenty-five documents that were relevant to each topic; the second focused on retrieving relevant sentences from this list of retrieved documents to satisfy each individual topic; the third focused on the detection of novel sentences from this relevant list. In Task1 we used an information retrieval system developed by the CDVP for the terabyte track as a basis for our experiments. This system used the BM25 ranking algorithm. We used various query and document expansion techniques to enhance the performance for sentence level retrieval.</p><p>In Task 2 we developed two formulas, the Impor-tanceValue and The NewSentenceValue, which exploit term characteristics using traditional document similarity methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Institut de Recherche en Informatique de Toulouse (IRIT)</head><p>In TREC 2004, IRIT modified important features of the strategy that was developed for TREC 2003. These features include both some parameter values, topic expansion and taking into account the order of sentences. According to our method, a sentence is considered relevant if it matches the topic with a certain level of coverage. This coverage depends on the category of the terms used in the texts. Four types of terms have been defined -highly relevant, scarcely relevant, non-relevant (like stop words), highly nonrelevant terms (negative terms). Term categorization is based on topic analysis: highly non-relevant terms are extracted from the narrative parts that describe what will be a non-relevant document. The three other types of terms are extracted from the rest of the query and are distinguished according to the score they obtain. The score is based both on the term occurrence and on the topic part they belong to (Title, descriptive, narrative). Additionally we increase the score of a sentence when the previous sentence is relevant. When topic expansion is applied, terms from relevant sentences (task 3) or from the first retrieved sentences (task 1) are added to the initial terms. With regard to the novelty part, a sentence is considered as novel if its similarity with each of the previously processed and selected as novel sentences does not exceed a certain threshold. In addition, this sentence should not be too similar to a virtual sentence made of the n best-matching sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">University of Iowa</head><p>Our system for novelty this year comprises three distinct variations. The first is a refinement of that used for last year involving named entity occurrences and functions as a comparative baseline. The second variation extends the baseline system in an exploration of the connection between word sense and novelty through two alternatives. The first alternative attempts to address the semantics of novelty by expanding all noun phrases (and contained nouns) to their corresponding WordNet synset IDs, and subsequently using synset IDs for novelty comparisons. The second alternative performs word sense disambiguation using an ensemble scheme to establish whether the additional computational overhead is warranted by an increase in performance over simple sense expansion.</p><p>The third variation involves more 'traditional' similarity schemes in the positive sense for relevance and the negative sense for novelty. SMART is first used to identify the top 25 documents and then judges relevance at the sentence level to generate a preliminary pool of candidates and then incrementally extends a matched terminology vector. The matched term vector is then used to rematch candidate sentences. Only similarities below a threshold -and hence possessing sufficient dissimilarity are declared novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">University of Massachusetts</head><p>For relevant sentences retrieval, our system treated sentences as documents and took the words in the title field of the topics as queries. TFIDF techniques with selective feedback were used for retrieving relevant sentences. Selective pseudo feedback means pseudo feedback was performed on some queries but not on other queries based on an automatic analysis on query words across different topics. Basically, a query with more focused query words that rarely appear in relevant documents related to other queries was likely to have a better performance without pseudo feedback. Selective relevance feedback was performed when relevance judgment of top five documents was available as for Task 3. Whether to performance relevance feedback on a query was determined by the comparison between the performance with and without relevance feedback in the top five documents for this query.</p><p>For identifying novel sentences, our system started with the sentences returned from the relevant sentences retrieval. The cosine similarity between a sentence and each previous sentence was calculated. The maximum similarity was used to eliminate redundant sentences. Sentences with a maximum similarity greater than a preset threshold were treated as redundant sentences. The value of the same threshold for all topics was tuned with the TREC 2003 track data when no judgment was available. The value of the threshold for each topic was trained with the training data when given the judgment of the top five documents. In addition to the maximum similarity, new words and named entities were also considered in identifying novel sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">University of Michigan</head><p>We view a cluster of documents as a graph, where each node is a sentence. We define an edge between two nodes where the cosine similarity between the corresponding two sentences is above a predefined threshold. After this graph is constructed, we find the eigenvector centrality score for each sentence by using a power method, which also corresponds to the stationary distribution of the stochastic graph.</p><p>To find the relevant sentences, we compute eigenvector centrality for each sentence together with some other heuristic features such as the similarity between the sentence and the title and/or description of the topic. To find the new sentences, we form the cosine similarity graph that consists of only the relevant sentences. Since the order of the sentences is important, unlike the case in finding the relevant sentences, we form a directed graph where every sentence can only point to the sentences that come after and are similar to it. The more incoming edges a sentence has, the more repeated information it contains. Therefore, the sentences with low centrality scores are considered as new. The system is trained on 2003 data using maximum entropy or decision lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Université Paris-Sud -LRI</head><p>The text-mining system we are building deals with the specific problem of identifying the instances of relevant concepts found in the texts. This has several consequences. We develop a chain of linguistic treatment such that the n-th module improves the semantic tagging of the (n -1)-th. This chain has to be friendly toward at least two kinds of experts: a linguistic expert, especially for the modules dealing mostly with linguistic problems (such as correcting wrong grammatical tagging), and a field expert for the modules dealing mostly with the meaning of group of words. Our definition of friendliness includes also developing learning procedures adapted to various steps of the linguistic treatment, mainly for grammatical tagging, terminology, and concept learning. In our view, concept learning requires a special learning procedure that we call Extensional Induction. Our interaction with the expert differs from classical supervised learning, in that the expert is not simply a resource who is only able to provide examples, and unable to provide the formalized knowledge underlying these examples. This is why we are devel-oping specific programming languages which enable the field expert to intervene directly in some of the linguistic tasks. Our approach is thus not particularly well adapted to the TREC competition, but our results show that the whole system is functional and that it provides usable information.</p><p>In this TREC competition we worked at two levels of our complete chain. In one level, we stopped the linguistic treatment at the level of terminology (i.e., detecting the collocations relevant to the text). Relevance was then defined as the appearance of the same terms in the task definition (exactly as given by the TREC competition team) and in the texts. Our relatively poor results show that we should have been using relevance definitions extended by human-provided comments. Novelty was defined by a TF*IDF measurement which seems to work quite correctly, but that could be improved by using the expert-defined concepts as we shall now see. The second level stopped the linguistic treatment after the definition of the concepts. Relevance was then defined as the presence of a relevant concept and novelty as the presence of a new concept. For each of the 5 runs, this approach proved to be less efficient than the simpler first one. We noticed however that the use of concepts enabled us to obtain excellent results on specific topics (and extremely bad ones as well) in different runs. We explain these very irregular results by our own lack of ability to define properly the relevant concepts for all the 50 topics since we got our best results on topics that either we understood well (e.g., Pinochet, topic N51) or that were found interesting (e.g., Lt-Col Collins, topic N85).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">University of Southern California -ISI</head><p>Our system's two modules recognize relevant event and opinion sentences respectively. We focused mainly on recognizing relevant opinion sentences using various opinion-bearing word lists. This year, each topic contained 25 relevant documents, possibly mixed with additional irrelevant documents. Thus, before proceeding to the next phase we had to separate relevant documents from irrelevant documents. We treat this problem as a standard Information Retrieval (IR) procedure. We used a probabilistic Bayesian inference network model to identify the relevant documents. For opinion topics, we used unigrams as subjectivity clues and built four different systems to generate opinion-bearing word lists. After building these unigram lists, we checked each sentence in the relevant documents for the presence of opinion-bearing words. For event topics, we treat event identification as a traditional document IR task. For the IR part we treat each sentence independently of other sentences and index them accordingly. We thus reduce the problem of event identification to that of sentence retrieval. We choose the description &lt;desc&gt; field for formulating the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">Tsinghua University</head><p>• Text feature selection and reduction, including using Named Entities, POS-tagging information, and PCA transformation which has been shown to be more effective;</p><p>• Improve sentence classification to find relevant information using SVM;</p><p>• Efficient sentence redundancy computing, including selected pool approach, tightness restriction factor, and PCA-based cosine similarity measurement;</p><p>• Effective result filtering, combining sentence and document similarities.</p><p>Several approaches are investigated for the step two of novelty (redundancy reduction): Combining the pool method and sentence to sentence overlap, we have a selected pool method, where unlike in the pool method, not all previously seen sentences are included into the pool, only those thought to be related are included. Tightness restriction to overcome one disadvantage of overlap methods is studied. We observed not all sentences with an overlap of 1 (complete term overlap) are really redundant, so we came up with the idea of tightness restriction which tries to recover highly overlapping but in fact novel sentences. In this method, the ratio of the range of common terms in the previous sentence over the range in the later sentence is used as a statistic. Cosine similarity between sentences after PCA is also investigated, and is proved to be most effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Figures <ref type="figure" coords="9,107.73,628.89,7.79,9.96" target="#fig_1">2,</ref><ref type="figure" coords="9,119.24,628.89,3.90,9.96">4</ref>, 5, and 6 show the average F scores for tasks 1, 2, 3, and 4 respectively. For task 1, the system scores are shown alongside the "score" of the secondary assessor, who essentially performed this task (with the caveat that they did not judge sentences in the irrelevant documents). Within the margin of error of human disagreement, the assessor lines can be thought of as representing the best possible performance, and are fairly close to the scores for the second assessor last year.</p><p>Last year, the top systems were performing at the level of the second assessor, but this year there is a large gap between the second assessor and the systems. Moreover, nearly all systems had low average precision and high average recall. These two observations seem to imply that systems are much too lenient with what they accept as relevant or novel. Some runs with the lowest F scores actually achieved the highest precision of any run in task 1.</p><p>We cannot simply say that the difference in performance is due to the inclusion of irrelevant documents. In task 2, where systems are given all relevant sentences and therefore no interference from irrelevant documents, performance is much lower than in the same task last year. It may be that the systems have overly tuned to the 2003 data.</p><p>The systems all scored within a very small range, mostly between 0.36 -0.4 for relevant sentences and 0.18 -0.21 for novel. Precision is very uniform, but recall varies a lot. Last year, the best runs were also very close to one another; this year, the bottom systems have caught up, but the top systems have not improved very much.</p><p>Event topics proved to be easier than opinion topics. Figure <ref type="figure" coords="9,362.84,412.17,5.03,9.96">3</ref> illustrates this for task 1, where every run did better on event topics than on opinions. The gap between opinions and events in task 1 is also larger than last year. The same gap exists in task 3, but in tasks 2 and 4, where all relevant sentences are provided, performance on opinion topics is much improved, and some runs do better on opinion topics than events. Thus, we can conclude that identifying sentences containing an opinion remains a hard problem.</p><p>Scores for task 2 (Figure <ref type="figure" coords="9,433.08,532.41,4.43,9.96">4</ref>) and task 4 (Figure <ref type="figure" coords="9,530.91,532.41,4.43,9.96">6</ref>) are shown against a baseline of returning all relevant sentences as novel. Most systems are doing better than this simplistic approach, both by F score and precision, indicating that the algorithms are successfully being somewhat selective.</p><p>It is also surprising how little the systems seem to benefit from training data. Overall scores did not improve between tasks 1 and 3, and from task 2 to task 4, novel sentence retrieval actually decreased significantly (see Figure <ref type="figure" coords="9,392.68,652.77,3.88,9.96" target="#fig_3">7</ref>). To be fair, this analysis needs to be balanced across groups, as tasks 3 and 4 had fewer runs and fewer groups participating, and some groups use radically different approaches in the pres-ence of training data. But whereas last year additional training data helped relevant sentence retrieval markedly, this year there is no improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This is the third and final year for the novelty track. We have examined a particular kind of novelty detection, that is, finding novel information within documents that the user is reading. This is by no means the only kind of novelty detection. Another important kind is detecting new events, which has been studied in the TDT evaluations. There, the user is monitoring a news stream and wants to know when something new, such as a plane crash, is first reported. Yet a third is the problem of returning new stories about a known topic, studied in the TREC filtering track and also in TDT topic tracking and story link detection.</p><p>We have seen here that filtering and learning approaches can be applied to detecting novel relevant information within documents, but that it remains a hard problem. Because the unit of interest is a sentence, there is not a lot of data in each unit on which to base the decision. Allowing arbitrary passages would make for a much more complicated evaluation.</p><p>The exploration into event and opinion topics has been an interesting and fruitful one. The opinions topics are quite different in this regard than other TREC topics. By mixing the two topic types within each task, we have seen that identifying opinions is hard, even with training data, while detecting new opinions (given relevance) seems analogous to detecting new information about an event.</p><p>One interesting footnote to the novelty track has been the use of the data outside the track. We know of two scenarios, namely summarization evaluation in DUC and an opinion detection pilot in AQUAINT, which have made use of topics from the novelty track. It's rewarding to see that this data is proving useful beyond the original scope of the track.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,72.00,259.05,228.78,9.96;5,72.00,270.93,228.90,9.96;5,72.00,282.93,228.89,9.96;5,72.00,294.93,220.46,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The F measure, plotted according to its precision and recall components. The lines show contours at intervals of 0.1 points of F. The black numbers are per-topic scores for one novelty track run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,72.00,620.85,467.88,9.96;11,72.00,632.73,293.57,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F, precision, and recall scores for Task 1, along with the "average score" of the secondary assessor. Runs are ordered by average F score for relevant sentence retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,109.92,618.09,392.03,9.96"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure 4: Scores for Task 2, against a baseline of returning all relevant sentences as novel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,118.44,618.09,374.90,9.96"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of F scores between Tasks 1 and 3, and between Tasks 2 and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,78.00,81.45,431.96,625.80"><head>Table 1 :</head><label>1</label><figDesc>Analysis of relevant and novel sentences by topic</figDesc><table coords="3,78.00,98.73,431.96,608.52"><row><cell cols="4">Topic type sents assr-1</cell><cell cols="4">rel % total new % rel assr-2</cell><cell cols="3">rel % total new % rel</cell></row><row><cell>N51</cell><cell>E</cell><cell>669</cell><cell>C</cell><cell>107</cell><cell>15.99</cell><cell>26 24.30</cell><cell>B</cell><cell>112</cell><cell>16.74</cell><cell>33.93</cell></row><row><cell>N53</cell><cell>E</cell><cell>667</cell><cell>E</cell><cell>106</cell><cell>15.89</cell><cell>31 29.25</cell><cell>C</cell><cell>136</cell><cell>20.39</cell><cell>63.24</cell></row><row><cell>N54</cell><cell>E</cell><cell>1229</cell><cell>E</cell><cell>198</cell><cell>16.11</cell><cell>71 35.86</cell><cell>B</cell><cell>384</cell><cell cols="2">31.24 58.33</cell></row><row><cell>N55</cell><cell>E</cell><cell>536</cell><cell>C</cell><cell>56</cell><cell>10.45</cell><cell>21 37.50</cell><cell>E</cell><cell>96</cell><cell>17.91</cell><cell>47.92</cell></row><row><cell>N56</cell><cell>E</cell><cell>1904</cell><cell>E</cell><cell>196</cell><cell cols="2">10.29 103 52.55</cell><cell>A</cell><cell>133</cell><cell>6.99</cell><cell>35.34</cell></row><row><cell>N57</cell><cell>E</cell><cell>378</cell><cell>B</cell><cell>21</cell><cell>5.56</cell><cell>10 47.62</cell><cell>D</cell><cell>170</cell><cell cols="2">44.97 68.24</cell></row><row><cell>N59</cell><cell>E</cell><cell>855</cell><cell>D</cell><cell>214</cell><cell>25.03</cell><cell>86 40.19</cell><cell>C</cell><cell>152</cell><cell>17.78</cell><cell>40.79</cell></row><row><cell>N64</cell><cell>E</cell><cell>679</cell><cell>C</cell><cell>214</cell><cell cols="2">31.52 140 65.42</cell><cell>A</cell><cell>228</cell><cell>33.58</cell><cell>28.07</cell></row><row><cell>N68</cell><cell>E</cell><cell>1331</cell><cell>B</cell><cell>200</cell><cell>15.03</cell><cell>45 22.50</cell><cell>E</cell><cell>210</cell><cell>15.78</cell><cell>39.05</cell></row><row><cell>N69</cell><cell>E</cell><cell>367</cell><cell>D</cell><cell>169</cell><cell>46.05</cell><cell>55 32.54</cell><cell>B</cell><cell>122</cell><cell>33.24</cell><cell>48.36</cell></row><row><cell>N72</cell><cell>E</cell><cell>1007</cell><cell>B</cell><cell>147</cell><cell>14.60</cell><cell>43 29.25</cell><cell>D</cell><cell>144</cell><cell>14.30</cell><cell>33.33</cell></row><row><cell>N73</cell><cell>E</cell><cell>380</cell><cell>D</cell><cell>268</cell><cell cols="2">70.53 139 51.87</cell><cell>A</cell><cell>164</cell><cell>43.16</cell><cell>56.71</cell></row><row><cell>N74</cell><cell>E</cell><cell>502</cell><cell>D</cell><cell>240</cell><cell cols="2">47.81 107 44.58</cell><cell>C</cell><cell>129</cell><cell>25.70</cell><cell>61.24</cell></row><row><cell>N79</cell><cell>E</cell><cell>1580</cell><cell>C</cell><cell>199</cell><cell>12.59</cell><cell>69 34.67</cell><cell>D</cell><cell>188</cell><cell cols="2">11.90 61.70</cell></row><row><cell>N80</cell><cell>E</cell><cell>447</cell><cell>E</cell><cell>74</cell><cell>16.55</cell><cell>48 64.86</cell><cell>B</cell><cell>104</cell><cell>23.27</cell><cell>49.04</cell></row><row><cell>N81</cell><cell>E</cell><cell>684</cell><cell>A</cell><cell>173</cell><cell>25.29</cell><cell>31 17.92</cell><cell>C</cell><cell>236</cell><cell cols="2">34.50 70.76</cell></row><row><cell>N82</cell><cell>E</cell><cell>1152</cell><cell>C</cell><cell>355</cell><cell cols="2">30.82 165 46.48</cell><cell>B</cell><cell>100</cell><cell>8.68</cell><cell>44.00</cell></row><row><cell>N83</cell><cell>E</cell><cell>816</cell><cell>A</cell><cell>250</cell><cell>30.64</cell><cell>62 24.80</cell><cell>E</cell><cell>227</cell><cell cols="2">27.82 53.74</cell></row><row><cell>N85</cell><cell>E</cell><cell>1419</cell><cell>B</cell><cell>181</cell><cell>12.76</cell><cell>95 52.49</cell><cell>E</cell><cell>116</cell><cell>8.17</cell><cell>50.86</cell></row><row><cell>N87</cell><cell>E</cell><cell>1026</cell><cell>D</cell><cell>476</cell><cell cols="2">46.39 163 34.24</cell><cell>C</cell><cell>369</cell><cell cols="2">35.96 62.60</cell></row><row><cell>N88</cell><cell>E</cell><cell>708</cell><cell>C</cell><cell>312</cell><cell cols="2">44.07 171 54.81</cell><cell>E</cell><cell>307</cell><cell cols="2">43.36 42.67</cell></row><row><cell>N90</cell><cell>E</cell><cell>1971</cell><cell>B</cell><cell>529</cell><cell cols="2">26.84 168 31.76</cell><cell>D</cell><cell>762</cell><cell cols="2">38.66 40.68</cell></row><row><cell>N92</cell><cell>E</cell><cell>879</cell><cell>B</cell><cell>188</cell><cell cols="2">21.39 172 91.49</cell><cell>A</cell><cell>199</cell><cell>22.64</cell><cell>41.71</cell></row><row><cell>N95</cell><cell>E</cell><cell>627</cell><cell>E</cell><cell>78</cell><cell>12.44</cell><cell>36 46.15</cell><cell>D</cell><cell>168</cell><cell cols="2">26.79 64.29</cell></row><row><cell>N98</cell><cell>E</cell><cell>408</cell><cell>C</cell><cell>171</cell><cell>41.91</cell><cell>65 38.01</cell><cell>A</cell><cell>267</cell><cell>65.44</cell><cell>25.09</cell></row><row><cell>N52</cell><cell>O</cell><cell>1018</cell><cell>B</cell><cell>103</cell><cell>10.12</cell><cell>55 53.40</cell><cell>C</cell><cell>298</cell><cell cols="2">29.27 67.79</cell></row><row><cell>N58</cell><cell>O</cell><cell>1346</cell><cell>A</cell><cell>146</cell><cell>10.85</cell><cell>42 28.77</cell><cell>C</cell><cell>252</cell><cell cols="2">18.72 64.68</cell></row><row><cell>N60</cell><cell>O</cell><cell>948</cell><cell>B</cell><cell>172</cell><cell>18.14</cell><cell>64 37.21</cell><cell>A</cell><cell>257</cell><cell>27.11</cell><cell>30.74</cell></row><row><cell>N61</cell><cell>O</cell><cell>1150</cell><cell>A</cell><cell>70</cell><cell>6.09</cell><cell>21 30.00</cell><cell>B</cell><cell>78</cell><cell>6.78</cell><cell>51.28</cell></row><row><cell>N62</cell><cell>O</cell><cell>3132</cell><cell>E</cell><cell>89</cell><cell>2.84</cell><cell>45 50.56</cell><cell>D</cell><cell>97</cell><cell>3.10</cell><cell>81.44</cell></row><row><cell>N63</cell><cell>O</cell><cell>518</cell><cell>B</cell><cell>49</cell><cell>9.46</cell><cell>21 42.86</cell><cell>E</cell><cell>84</cell><cell>16.22</cell><cell>65.48</cell></row><row><cell>N65</cell><cell>O</cell><cell>705</cell><cell>B</cell><cell>95</cell><cell>13.48</cell><cell>61 64.21</cell><cell>C</cell><cell>113</cell><cell>16.03</cell><cell>79.65</cell></row><row><cell>N66</cell><cell>O</cell><cell>795</cell><cell>A</cell><cell>195</cell><cell>24.53</cell><cell>25 12.82</cell><cell>E</cell><cell>286</cell><cell cols="2">35.97 47.90</cell></row><row><cell>N67</cell><cell>O</cell><cell>423</cell><cell>E</cell><cell>113</cell><cell>26.71</cell><cell>72 63.72</cell><cell>C</cell><cell>109</cell><cell>25.77</cell><cell>75.23</cell></row><row><cell>N70</cell><cell>O</cell><cell>1030</cell><cell>D</cell><cell>94</cell><cell>9.13</cell><cell>31 32.98</cell><cell>E</cell><cell>237</cell><cell cols="2">23.01 43.88</cell></row><row><cell>N71</cell><cell>O</cell><cell>908</cell><cell>B</cell><cell>62</cell><cell>6.83</cell><cell>28 45.16</cell><cell>A</cell><cell>127</cell><cell>13.99</cell><cell>22.05</cell></row><row><cell>N75</cell><cell>O</cell><cell>2922</cell><cell>B</cell><cell>169</cell><cell cols="2">5.78 100 59.17</cell><cell>C</cell><cell>284</cell><cell cols="2">9.72 86.27</cell></row><row><cell>N76</cell><cell>O</cell><cell>1697</cell><cell>A</cell><cell>217</cell><cell>12.79</cell><cell>51 23.50</cell><cell>D</cell><cell>118</cell><cell>6.95</cell><cell>33.05</cell></row><row><cell>N77</cell><cell>O</cell><cell>1144</cell><cell>D</cell><cell>74</cell><cell>6.47</cell><cell>23 31.08</cell><cell>B</cell><cell>102</cell><cell>8.92</cell><cell>35.29</cell></row><row><cell>N78</cell><cell>O</cell><cell>1308</cell><cell>A</cell><cell>145</cell><cell>11.09</cell><cell>59 40.69</cell><cell>B</cell><cell>59</cell><cell>4.51</cell><cell>42.37</cell></row><row><cell>N84</cell><cell>O</cell><cell>1363</cell><cell>D</cell><cell>101</cell><cell>7.41</cell><cell>31 30.69</cell><cell>E</cell><cell>153</cell><cell>11.23</cell><cell>52.29</cell></row><row><cell>N86</cell><cell>O</cell><cell>493</cell><cell>D</cell><cell>67</cell><cell>13.59</cell><cell>33 49.25</cell><cell>A</cell><cell>96</cell><cell>19.47</cell><cell>47.92</cell></row><row><cell>N89</cell><cell>O</cell><cell>1271</cell><cell>B</cell><cell>204</cell><cell cols="2">16.05 130 63.73</cell><cell>A</cell><cell>181</cell><cell>14.24</cell><cell>33.70</cell></row><row><cell>N91</cell><cell>O</cell><cell>1473</cell><cell>B</cell><cell>112</cell><cell>7.60</cell><cell>51 45.54</cell><cell>D</cell><cell>123</cell><cell>8.35</cell><cell>80.49</cell></row><row><cell>N93</cell><cell>O</cell><cell>1017</cell><cell>B</cell><cell>181</cell><cell>17.80</cell><cell>56 30.94</cell><cell>E</cell><cell>255</cell><cell cols="2">25.07 50.59</cell></row><row><cell>N94</cell><cell>O</cell><cell>1099</cell><cell>E</cell><cell>102</cell><cell>9.28</cell><cell>59 57.84</cell><cell>A</cell><cell>91</cell><cell>8.28</cell><cell>50.55</cell></row><row><cell>N96</cell><cell>O</cell><cell>1328</cell><cell>A</cell><cell>131</cell><cell>9.86</cell><cell>60 45.80</cell><cell>D</cell><cell>61</cell><cell>4.59</cell><cell>73.77</cell></row><row><cell>N97</cell><cell>O</cell><cell>1416</cell><cell>A</cell><cell>123</cell><cell>8.69</cell><cell>31 25.20</cell><cell>B</cell><cell>122</cell><cell>8.62</cell><cell>72.95</cell></row><row><cell>N99</cell><cell>O</cell><cell>1192</cell><cell>C</cell><cell>259</cell><cell cols="2">21.73 131 50.58</cell><cell>D</cell><cell>495</cell><cell cols="2">41.53 68.89</cell></row><row><cell>N100</cell><cell>O</cell><cell>530</cell><cell>E</cell><cell>148</cell><cell>27.92</cell><cell>52 35.14</cell><cell>B</cell><cell>152</cell><cell>28.68</cell><cell>51.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,81.45,455.33,383.52"><head>Table 2 :</head><label>2</label><figDesc>Organizations participating in the TREC 2004 novelty track</figDesc><table coords="6,317.88,98.37,209.45,21.96"><row><cell>Runs submitted</cell></row><row><cell>Run prefix Task 1 Task 2 Task 3 Task 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,72.00,173.21,467.71,435.04"><head></head><label></label><figDesc>Figure 3: Average F scores per run for opinion and event topic types. Runs are grouped by tag for easier identification.</figDesc><table coords="12,81.75,173.21,438.88,380.10"><row><cell></cell><cell></cell><cell></cell><cell>Task 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Task 1, average F scores by topic type</cell></row><row><cell></cell><cell></cell><cell></cell><cell>new</cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4 0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F score F score 0.5 0.1 0.2 0.4 0.6</cell><cell></cell><cell cols="2">relevant oo ooooo oo oooooo ooooo oooo oooooo oo ooo oo o oo oooo o o o o oo o o o + + + + + + + ++ ++ + + + + +++ + + + + + + + ++ + + + + + + + + + ++</cell><cell>o</cell><cell>o</cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3 0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell cols="2">Events Opinions</cell><cell>o +</cell></row><row><cell></cell><cell>CIIRT1R1</cell><cell cols="3">CIIRT1R2 cdvp4NTerFr1 CIIRT1R3 MeijiHIL2WRS CIIRT1R5 CIIRT2R2 CIIRT1R6 MeijiHIL2WR HIL10 novcolrcl ICTOKAPIOVLP cdvp4NTerFr3 ICTVSMCOSAP cdvp4UnHis3 ICTVSMFDBKH LRIaze22 ICTVSMFDBKL LRIaze12 ICTVSMLCE CIIRT2R1 IRITT1 MeijiHIL2RS IRITT2 cdvp4NSen4 IRITT3 UIowa04Nov21 IRITT4 THUIRnv0424 IRITT5 MeijiHIL2WCS ISIALL04 THUIRnv0422 ISIRUN204 THUIRnv0421 ISIRUN304 THUIRnv0423 ISIRUN404 ISIRUN504 IritTask2 LRIaze1 UIowa04Nov22 LRIaze2 LRIaze52 LRIaze3 NTU21 LRIaze4 novcosine LRIaze5 ICT2VSMOLP MeijiHIL1cfs NTU23 MeijiHIL1odp NTU22 NTU11 LRIaze32 NTU12 novcombo NTU13 ccsmmr4t2 NTU14 LRIaze42 NTU15 ccsqrt2 THUIRnv0411 umich0422 THUIRnv0412 THUIRnv0425 THUIRnv0413 ccsmmr3t2 THUIRnv0414 ccsmmr5t2 THUIRnv0415 MeijiHIL2CS UIowa04Nov11 Irit2T2 UIowa04Nov12 UIowa04Nov23 UIowa04Nov13 UIowa04Nov14 novcolp1 UIowa04Nov15 ICT2VSMLCE ccs1f0t1 umich0425 ccs1ftop0t1 UIowa04Nov24 ccs3fmmrt1 umich0423 ccs3fqrt1 umich0421 ccs3ftop0t1 ICT2OKALCEAP cdvp4CnQry2 ICT2OKAPIAP cdvp4CnS101 ICT2VSMIG95 cdvp4QePDPC2 novcolp2 cdvp4QePnD2 ccsmmr2t2 cdvp4QeSnD1 NTU24 clr04n1h2 umich0424 clr04n1h3 UIowa04Nov25 umich0411 NTU25 umich0412 clr04n2 umich0413 cdvp4NSnoH4 umich0414</cell><cell>umich0415</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,87.47,628.89,213.50,9.96;10,87.48,640.77,213.55,9.96;10,87.48,652.77,213.39,9.96;10,87.48,664.65,213.27,9.96;10,87.48,676.65,213.49,9.96;10,87.48,688.65,118.81,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,159.29,628.89,141.68,9.96;10,87.48,640.77,41.61,9.96">Overview of the TREC 2002 novelty track</title>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,152.72,652.77,148.16,9.96;10,87.48,664.65,213.27,9.96;10,87.48,676.65,104.67,9.96">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002), NIST Special Publication 500-251</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002), NIST Special Publication 500-251<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">November 2002</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,326.51,74.49,213.29,9.96;10,326.52,86.49,213.27,9.96;10,326.52,98.37,213.14,9.96;10,326.52,110.37,213.26,9.96;10,326.52,122.37,213.17,9.96;10,326.52,134.25,69.88,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,472.32,74.49,67.48,9.96;10,326.52,86.49,108.85,9.96">Overview of the TREC 2003 novelty track</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,428.08,98.37,111.59,9.96;10,326.52,110.37,180.34,9.96">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-11">November 2003</date>
			<biblScope unit="volume">500</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
