<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.35,163.37,405.35,21.70;1,197.25,188.28,217.55,21.70">York University at TREC 2004: HARD and Genomics Tracks</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.06,232.06,69.72,11.46"><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
							<email>jhuang@yorku.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.16,232.06,74.42,11.46"><forename type="first">Yan</forename><forename type="middle">Rui</forename><surname>Huang</surname></persName>
							<email>yhuang@cs.yorku.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.95,232.06,48.82,11.46"><forename type="first">Miao</forename><surname>Wen</surname></persName>
							<email>mwen@cs.yorku.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.33,232.06,57.90,11.46"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.35,163.37,405.35,21.70;1,197.25,188.28,217.55,21.70">York University at TREC 2004: HARD and Genomics Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F6C9F829879CB6D474484010B2AD971</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>York University participated in HARD and Genomics tracks this year. For both tracks, we used Okapi BSS (basic search system) as the basis. Our experiments mainly focused on exploiting various methods for combining document and passage scores, new term weighting formulae and feedback methods for query expansion. For HARD track, we built two levels of indexes, and search against both indexes for each topic. Then we combine these two searches into one. For Genomics track, we used a new strategy to automatically expand search terms by using relevance feedback and combined retrieval results from different fields into the final result. We achieved good results on the HARD task and average results on the Genomics task. For the HARD passage level evaluation, the automatic run 'yorku04ha1' obtained the best result (0.358) in terms of Bpref measure at 12K characters. The evaluation results show that Algorithm 1 is more effective than Algorithm 2 for the passage level retrieval,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first time York participated in TREC. We participated in both High Accuracy Retrieval from Documents (HARD) track and Genomics track. For both tracks, we used Okapi BSS (basic search system) as the basis. Our experiments mainly focused on exploiting various feedback methods for query expansion and term weighting formulae.</p><p>For the HARD task, both document level index and passage level index are built for retrieval purpose. We use Okapi BM25 for passage retrieval, but for document level retrieval, we applied a modified version of BM25, named BM50, by adding a correction factor which is based on the length of document. This correction factor is added at the end of the usual BM25 function, and serves as an adjusting factor that gives relative low weight to those documents with considerably short or long lengths.</p><p>Our experiments at the genomics track mainly focused on the following methodology: <ref type="bibr" coords="2,116.16,223.58,14.96,12.55" target="#b0">(1)</ref> We generated initial query terms automatically. <ref type="bibr" coords="2,382.69,223.58,14.96,12.55" target="#b1">(2)</ref> We used a new strategy to automatically expand search terms by using relevance feedback. <ref type="bibr" coords="2,437.21,238.03,14.96,12.55" target="#b2">(3)</ref> We built five indexes on the fields of "PMID", "Title", "Abstract", "Mesh heading" and "NameOf-Substance" for retrieval; (4) We designed a new retrieval strategy to combine retrieval results from different fields as the final result.</p><p>The rest of the paper is organized as follows. We first give a brief description of the Okapi information retrieval system in Section 2. Then, HARD track and Genomics track are presented in Section 3 and 4. Following that, experimental results for both HARD and Genomics tracks are provided in Section 5. Finally, the conclusion and future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Based on Probabilistic Model</head><p>We used Okapi BSS (Basic Search System) as our main search system. Okapi is an information retrieval system based on the probability model of Robertson and Sparck Jones <ref type="bibr" coords="2,122.78,448.71,11.70,12.55" target="#b5">[6]</ref>. The retrieval documents are ranked in the order of their probabilities of relevance to the query. Search term is assigned weight based on its within-document term frequency and query term frequency. There is a Go-See-List (GSL) file containing synonym for indexing. The weighting function used is BM25 <ref type="bibr" coords="2,423.93,492.04,11.70,12.55" target="#b1">[2]</ref>:</p><formula xml:id="formula_0" coords="2,98.93,528.41,418.08,29.83">w = (k 1 + 1) * tf K + tf * log N -n + 0.5 n + 0.5 * (k 3 + 1 ) * qtf k 3 + qtf ⊕ k 2 * nq * (avdl -dl ) (avdl + dl ) (<label>1</label></formula><formula xml:id="formula_1" coords="2,517.01,536.49,4.99,12.55">)</formula><p>where w is the weight for each query term, N is the number of indexed documents in the collection, n is the number of documents containing a specific term, tf is withindocument term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, the k i s are tuning constants (which depend on the database and possibly on the nature of the queries and are empirically determined), K equals to</p><formula xml:id="formula_2" coords="2,393.40,641.17,128.60,13.55">k 1 * ((1 -b) + b * dl/avdl),</formula><p>and ⊕ indicates that its following component is added only once per document, rather than for each term. In our experiments, the values of k 1 , k 2 , k 3 and b in the BM25 function are set to be 1.2, 0, 8 and 0.75 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HARD Track</head><p>The main task of HARD is to achieve high accuracy retrieval from documents. For this track, we explored some techniques that could enhance the search results. We mainly focused on the usage of the query information provided and interactive user feedback. We also investigated the effectiveness of different indexing and term weighting methods. Two levels of search, document-level and passage-level, are performed for each topic. Then we combined these two search results into one. This year, we submitted one baseline run, two clarification forms and four final runs. For the four final runs, two of them are automatic runs and the other two are manual runs. Our experiments were conducted on a double-processor server which has 2 Intel Xeon 2.40GHz CPU and 2G memory. The version of Linux kernel we used is version 2.4.26.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Description</head><p>Figure <ref type="figure" coords="3,127.55,350.03,5.85,12.55">1</ref> shows the system architecure of our search system. Search terms are first generated by parsing the original topics, metadata and the interactive user feedback from the clarification forms. These terms are used to generate initial search results. After initial results are generated, we use the blind feedback to do the query expansion. Then we choose the top 80 terms as the finalized search terms. These search terms are used to search the database again and newly generated results will be used as the final results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topics, Clarification Forms and Metadata</head><p>There are 20 training topics and 50 evaluation topics. For the baseline run, each topic contains a short title, a sentence long query description and a paragraph long narrative. After the baseline was submitted, more detailed information called metadata was added for each topic. The metadata includes the following fields: Genre, Geography, Granularity, Familiarity, Subject, and Related Text. However, only the granularity, geography and related text information were used in our HARD experiment. The granularity information was used to decide if we should have document level retrieval or passage level retrieval. The geography information was used to filter out some documents<ref type="foot" coords="4,197.63,259.24,4.24,8.37" target="#foot_0">1</ref> and the related text information was used to automatically expand the query terms.</p><p>Two sets of clarification forms (CFs) were submitted. The first one concentrated on short paragraph feedback and the second one concentrated on keyword feedback within the context. The CF results were then used in query expansion. The main goal of using CFs is to get interactive feedback from the users. Both positive and negative feedback can be obtained from CFs. One of the automatic run was generated by using only the positive feedback and the other one was generated by using both positive and negative feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Term Selection and Expansion</head><p>The first task we need to perform before searching is to find the right terms to search. For the baseline run, there are only three fields provided for each topic: title, description and narrative. The title is a very short phrase, the description and narrative are provided in a natural language format. All the search terms should be extracted from these three fields. We use a list of stop words to filter out the words which will not be helpful for the topic searching. For each search term, we count the total number of occurrence within the topic and the number of occurrence in each field. The following structure is used for each term. The above structure is extendable. For the baseline run, we only count the first three fields. After we get the feedback of the clarification form and related text, we extracted more terms from these two resources and expand the search terms. Then one counter for each new field is added for every search term, and total number of occurrences is updated correspondingly. These counters are indicators which show the importance of the term and will be used later to address the weight of the terms. More search terms can be added by using the blind feedback technique. So the final terms for searching include the terms extracted from original topics, the terms extracted from the metadata, the terms extracted from clarification forms and the terms extracted from the blind feedback. Currently, all the search terms are single words. No phrase has been extracted from the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BM50 Function and Document Length Adjustments</head><p>The component shown below is called correction factor which was designed to take into account the length of a document. The value of the correction factor decreases with dl, from a maximum as dl → 0, through zero, at which dl = avdl, and to a minimum as dl → ∞</p><formula xml:id="formula_3" coords="5,253.99,319.26,268.01,28.84">k 2 * nq * (advl -dl) (advl + dl) (2)</formula><p>This design of the correction factor assumes that, the shorter the document is, the more value the correction factor should have, i.e., the terms in a short document becomes more significant for that document and thus the ranking of a short document is improved. However, under certain circumstance, the correction factor is misleading. For example, when the document length is 0 or extremely small, it doesn't contain sufficient information, and most likely this document is a noise. In this case, it should not be considered as the relevant document. Thus, it should not gain more weight from the correction factor than other documents <ref type="bibr" coords="5,342.36,455.41,11.70,12.55" target="#b2">[3]</ref>.</p><p>In order to find a better solution, we conducted some analysis of the 2004 Hard Corpus. The statistical information is shown in Table <ref type="table" coords="5,368.95,484.31,5.85,12.55">1</ref> and<ref type="table" coords="5,401.47,484.31,4.55,12.55" target="#tab_2">2</ref> From the two tables above, we can find that the average documents length is shorter than the average relevant documents length. To illustrate the results graphically, we re-plot the data in Figure <ref type="figure" coords="5,276.65,722.50,5.85,12.55" target="#fig_0">2</ref> and<ref type="figure" coords="5,313.25,722.50,4.55,12.55" target="#fig_1">3</ref>. Figure <ref type="figure" coords="5,364.71,722.50,5.85,12.55" target="#fig_0">2</ref> shows the distribution of the whole documents, while Figure <ref type="figure" coords="6,254.93,122.47,5.85,12.55" target="#fig_1">3</ref> shows the distribution of the relevant documents. What we can do here is to find a function which best fits the curve of the relevant documents for the training topics so that a negative factor will be given for a very short document. For a document whose length is equal to the average length of the relevant document, a maximum correction factor will be assigned.  Using the statistic software TableCurve 2D from Systat<ref type="foot" coords="6,393.56,582.39,4.24,8.37" target="#foot_1">2</ref> , we found the following function which best fits the curve of the relevant dataset for the training topics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discretized Document Length</head><note type="other">Frequency</note><formula xml:id="formula_4" coords="6,229.23,623.46,292.74,15.77">y = 4 * a * e -x-b c * (1 -e -x-b c ) (3)</formula><p>This function is added to the end of the BM25 function. Thus, the refined BM25 function, BM50, is as follows: Where</p><formula xml:id="formula_5" coords="6,257.23,690.95,264.77,13.55">w = w 1 + w 1 * y/k l (4)</formula><formula xml:id="formula_6" coords="7,193.21,513.11,328.78,29.84">w 1 = (k 1 + 1) tf K + tf log N -n + 0.5 n + 0.5 (k 3 + 1) qtf k 3 + qtf (5)</formula><p>The y/k l component indicates the percentage of the final weights which is contributed by the correction factor. Here, we set a=13, b=1.3 and c=44 to match the distribution curve we get from the relevance dataset and set k l =50 to make the correction factor contributes no more than 26% of the final weight.</p><p>The experiments show that the new correction factor changes the weight of the documents. The weight of documents which have very short and very long lengths will be decreased compared to the weight get from the original BM25 function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Search</head><p>After search terms are extracted, we use them to search against the HARD corpus by using Okapi. We implemented a JNI (Java Native Interface) for our search engine to interactive with Okapi BSS<ref type="foot" coords="8,247.74,121.51,4.24,8.37" target="#foot_2">3</ref> . For the baseline run, we only use terms extracted from the original topic to search against the HARD corpus.</p><p>For the final runs, we first expand our query terms by using clarification forms and metadata. The new list of terms are searched against the HARD corpus, and the statistical information of these terms is collected. Each term's weight is calculated based on the Okapi weighting function and the importance of the term (e.g., the query term frequency). Then we choose the top 80 terms as our finalized search terms according to their weights.</p><p>After we finalize the top 80 search terms, an U.S filter is used first to filter out the documents if there is a geographical restriction on the topic. The U.S filter contains a list of states and cities of the United States, and all those documents which do not contain any name from the list are considered to be non-relevant and will be filtered out if there is a geography restriction. For the manual run, we also use a key filter. The format of the key filter is as the following: each line contains several keywords which can be a single term or phrase. The keywords in a single line are "OR" related. This key filter can be used to filter out some unrelated documents. Since the filter can be highly restrictive, it should be used with caution. In the manual experiment, we only applied it to the first round of search.</p><p>Other than the interactive user feedback that we obtain from the CFs, we also use blind feedback to expand our query terms. From the initial search result, we select the top 50 documents, calculate the RSV values of the terms within these documents, and add the top ranked terms into the existing search terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Combining Document and Passage Scores</head><p>The following two algorithms were applied in the final submissions. Both of them are based on the same topics and CF results.</p><p>Algorithm 1: For each topic, we do both document level search and passage level search. Then we combine these two searches into one. Our basic assumption for this combination is: if an article is hit by both searches, it should be assigned more weight than others that are hit by only one search. After initial results are generated, we use blind feedback to do the query expansion. Then we generate the final results by using the same algorithm. BM25 was used for passage level search and BM50 was used for document level search.</p><p>Algorithm 2: For each topic, we do only document level search or passage level search according to the value of "retrieval-element" (document or passage). The search terms are automatically extracted from topics and CF results. The terms extracted from CF results were classified into two sets: positive and negative terms. We assign positive weights to positive terms and negative weights to negative terms. BM25 was used for both document level search and passage level search.</p><p>For Algorithm 1, we use different merge functions to update the weights for document and paragraph by combining the results from both indexes. If the granularity is "document", the following merge function is used:</p><formula xml:id="formula_7" coords="9,193.07,175.71,328.91,31.33">W dnew = (W d + k x=1 W d.x |P | ) * log 10 (10 * |P |) (6)</formula><p>where W dnew is the new weight of the document, W d is the weight obtained from the document level index, W d.x is the weight obtained from the paragraph level index, x ranges from 1 to k, where k equals to the total number of paragraphs retrieved from this document in the top 1000 paragraphs from the paragraph level index. |P | is the total number of paragraphs retrieved from this document. If the granularity is "passage" and the paragraphs found in a document are not adjacent, the following merge function is used to assign a new weight to each of these paragraphs:</p><formula xml:id="formula_8" coords="9,201.90,343.27,320.08,14.58">W pnew = (W p + h 1 * W d ) * log 10 (10 * |P |) (7)</formula><p>where W pnew is the new weight of the paragraph, W p is the weight of the paragraph obtained from the paragraph level index, W d is the weight of the document containing the paragraph, which is obtained from the document level index, |P | is the total number of paragraphs retrieved from this document, and h 1 is a coefficient, which is set to be 3 in our experiments.</p><p>If there are n adjacent paragraphs found in a document, we merge these paragraphs into one and use the following function to assign a weight to the newly merged paragraph:</p><formula xml:id="formula_9" coords="9,167.67,488.76,354.33,37.42">W pnew = (W p 1 + h 1 * W d ) * log 10 (10 * |P |) + 1 2 n k=1 W p k (8)</formula><p>where W pnew is the weight of the newly merged paragraph, W p 1 is the weight of the first of these adjacent paragraphs obtained from the paragraph level index, W d is the weight of the document obtained from the document level index, |P | is the total number of paragraphs retrieved from this document. W p k is the weight of the kth of these n adjacent paragraphs, and h 1 is a coefficient, which is set to be 3 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Genomics Track</head><p>In recent years, there has been a large amount of experiments and researches conducted in the area of genomics and its related disciplines. As a result, a vast amount of scientific literature has been published. One purpose of Genomics track is to study the retrieval task in the domain of genomics. Given the MEDLINE database, how can we adapt the general purpose information retrieval system for the genomic domain? There are two tasks for this year's Genomics track. The primary task is an ad-hoc retrieval from MEDLINE corpus. The second task is categorization. This year, we only participated in the ad-hoc task. Our efforts concentrated on applying different techniques to Okapi system for improving the retrieval performance.</p><p>The goal of the York team to participate in TREC Genomics track is to evaluate the Okapi information retrieval system in the genomic domain. In the past TRECs, BM25 has been shown to be a very good probabilistic weighting scheme in text retrieval. However, the Okapi system has never been evaluated in the genomic domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topics</head><p>The 2004 topics are completely different from the 2003 topics. There are 50 topics derived from interviews elicitingal information needs of real biologists. Each topic has the following format:</p><p>1. ID -1 to 50 2. Title -abbreviated statement of information need 3. Information need -full statement information need 4. Context -background information to place information need in context This year's topics are more varies and specific than last year's topics, which only require to find all MEDLINE references that focus on the basic biology of the gene or its protein products from the designated organism. This year two new fields are added for each topic: information need and context. Both of these fields are stated in a natural language form and give more specific instruction for each topic. Only five training topics are given this year, thus increasing the difficulty of the search compared to last year.</p><p>For each topic, we first need to extract the search terms from the topic. Then, we searched against the MEDLINE corpus using Okapi information retrieval system for each term. After the first round search, more terms are added by using blind feedback. A second round search is performed and its result will be used as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Term Selection</head><p>For query term generation, a simple and automatic method is used to select at most 110 query terms from genomic topics. The initial query term selection is not only based on their weights but also based on their frequencies in the topics. The blind feedback is used to expand the initial query terms after the first round search is done. Those new terms are extracted from the top 10 first round search documents and are added into the original query terms. The expanded query terms are then used for the second round search. For each search term extracted from topics, we count its occurrences. An example is shown as follows:</p><formula xml:id="formula_10" coords="11,107.56,194.69,91.09,12.55">transgenic 3 1 1 1</formula><p>The first column is the term extracted from the topic. The second column contains the total number of occurrences of the terms in the topic. The third one indicates the number of occurrences of the term in field TITLE. The fourth and fifth indicated the number of occurrences of the term in the field NEED and CONTEXT correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Search</head><p>For retrieval, the retrieval documents are ranked based on the Best Match function BM25 together with the frequency of the search term. The search results from different fields are assigned different weights. Four different retrieval results are generated for each topic by retrieving documents based on the indexes constructed according to Title, Abstract, Mesh terms and NameOfSubstance. Those four results are then combined into the final result. Since the lengths of document in the collection are more or less the same, we do not use global correction factor in our genomics experiments. An organism filter is used in the experiments to filter some the irrelevant documents.</p><p>After the first round search, we choose the top 10 documents resulting from the first round search and assume them as relevant documents. Then we rank these new terms extracted from the top 10 documents by using their RSV values and add them into the original search terms. By doing in this way, we can keep the terms from the original topics and also add some new terms extracted from the top 10 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">HARD Experiments</head><p>The original HARD corpus is around 1.6G. It contains one year (2003) of newswire data from eight sources: AFE (Agence France Presse -English), APE (Associated Press Newswire), CNE (Central News Agency Taiwan), LAT (Los Angeles Times), NYT (New York Times), SLN (Salon.com), UME (Ummah Press -English), and XIE (Xinhua News Agency -English). In order to use the original data on our search system, we first need to convert this raw data into a format which can be recognized by the Okapi system, called exchange format. Based on this exchanged database, a runtime database and indexes are built by using Okapi. Both document-level index and passage-level index are built for the document-level retrieval and passage-level retrieval. A GSL file, which contains the most common synonyms, is used during the process of building index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Evaluation Results at TREC 2004</head><p>We submitted five runs for the HARD track at TREC 2004, one baseline run, two automatic runs and two manually runs. The document-level evaluation results are presented in Table <ref type="table" coords="12,192.15,219.14,5.85,12.55" target="#tab_5">3</ref> and the passage-level evaluation results are given in Table <ref type="table" coords="12,507.14,219.14,4.55,12.55" target="#tab_6">4</ref>.  The runs 'york04ha1' and 'york04hm1' were generated by Algorithm 1 and the runs 'york04ha2' and 'york04hm2' were generated by Algorithm 2. That is: the runs 'york04ha1' and 'york04hm1' were generated by <ref type="bibr" coords="12,366.11,621.74,14.96,12.55" target="#b0">(1)</ref> doing both document level search and passage level search and (2) using both BM25 and BM50 for term weighting. However, the runs 'york04ha2' and 'york04hm2' didn't use these two methods. Instead, it assigned negative weight to negative terms returned from the clarification form. The difference between automatic run 'york04a1' and manual run 'york04m1' is that for the run 'york04m1', a manually generated key filter is used at the very beginning of the search. It is easy to find from Table <ref type="table" coords="12,368.04,708.41,5.85,12.55" target="#tab_5">3</ref> and 4 that all the final runs made significant improvements over the baseline run. For passage level retrieval, Algorithm 1 is obviously better than Algorithm 2. For the passage level evaluation, the automatic run 'yorku04ha1' achieves the best result (0.358) in terms of Bpref measure at 12K characters <ref type="bibr" coords="13,184.67,165.80,11.70,12.55" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">U.S. Filter and Related Text</head><p>More experiments are conducted in order to evaluate the effectiveness of using the U.S. filter and the Related Text metadata. Table <ref type="table" coords="13,382.26,233.59,5.85,12.55" target="#tab_8">5</ref> gives us a comparison of three runs. The first run 'york04ha1' is our official submission in TREC 2004. The other two runs from 'experiment 1' and 'experiment 2' are based on the first run. For 'experiment 1', the environment setting is the same as the one set for 'york04ha1' except that the U.S filter is turned off. This experiment is used to test the effectiveness of the U.S filter. As we can see, turning off the U.S. filter can improve the retrieval performance comparing to turning on the U.S.filter. The possible explanation to this phenomenon is that the documents without the geographical information are eliminated even though they are relevant. This may be caused by the high restriction of the U.S. filter. For 'experiment 2', the environment setting is the same as the one set for 'york04ha1' except that no related text information is used. Without using the related text information, the retrieval performance decreases on both document-level and passage-level. It is obvious that the related text information can make a positive contribution to the retrieval performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">BM50 vs BM25</head><p>To study the effectiveness of our proposed BM50 term weighting function on document-level retrieval, we conducted a series of experiments by tuning the k l constant in the term weighting function.</p><p>The experimental results are shown in Table <ref type="table" coords="13,345.26,630.31,4.55,12.55" target="#tab_9">6</ref>. All the experiments in Table <ref type="table" coords="13,516.10,630.31,5.85,12.55" target="#tab_9">6</ref> are based on the official submission except that we do not use the blind feedback for retrieval. In the experiments, we found that the retrieval performance decreases for document-level retrieval by using blind feedback <ref type="foot" coords="13,333.37,672.69,4.24,8.37" target="#foot_3">4</ref> . So the blind feedback was not used in our experiments for evaluating the BM50 term weighting function. From Table <ref type="table" coords="13,512.84,688.10,4.55,12.55" target="#tab_9">6</ref>, we can observe that the document-level performance in terms of hard-rel R-precision achieves the best when the correction factor contributes 13% of the total weight. Clearly, these experiments demonstrate that BM50 can make a positive contribution to retrieval performance on the document level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Genomics Experiments</head><p>MEDLINE is the bibliographical database of biomedical articles maintained by the National Library of Medicine (NLM). The subset of MEDLINE is used for the TREC 2004 Genomics Track. It consists of 10 years of completed citations from the database inclusive from 1994 to 2003. Records were extracted using the Date Completed (DCOM) field for all references in the range of 19940101 -20031231. This provided a total of 4,591,008 records.</p><p>Each MEDLINE record contains a number of fields, but we are only interested in the following fives fields: (a) PMID, which is the unique identifier of the PubMed ( NLM's database that incorporates MEDLINE). (b) Title, which contains the entire title of the journal article. (c) Abstract, which is taken directly from the published article. (d) Mesh, NLM's controlled vocabulary, which is used to characterize the content of the articles represented by MEDLINE citations. (e) NameOfSubstance, which is the controlled vocabulary as well.</p><p>We use Okapi BSS as our search baseline. The first thing we need to do is to convert the database to the format which can be read by the Okapi system. We have developed a small tool, which convert xml file to exchange format, the format which can be used by the Okapi system. It reads through the xml file and extracts the information we need.</p><p>We submit two runs for the Genomics track. One is automatic run and the other one is manually run. The mean precision of the automatic run is 0.1794 and the mean precision of the manually run is 0.2011. Detailed information for these two runs are shown as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>York Team participated in both Genomics and HARD tracks this year. For the Genomics track, we adopted the Okapi system in the genomic domain without using any biomedical knowledge. In the Genomics experiments, we did not incorporate domain expertise and did not use external biomedical resources. For the HARD track, we tested our new ideas on indexing and evaluated the effectiveness of different weighting formulae. The HARD experimental results showed that using two-level indexes improves the performance greatly for both passage level and document level retrieval. For the passage level evaluation, the automatic run 'yorku04ha1' achieves the best result (0.358) in terms of Bpref measure at 12K characters. BM50 can also make a positive contribution for document level retrieval.</p><p>For the HARD track, only the Geography and Related-text metadata are used. However, the retrieval performance decreases with U.S. filter turned on (the Geography field). The Related-text field is useful in our experiments for improving performance on both document and passage level retrieval. There is a potential to use other fields as well, e.g. the Genre and Subject fields. We can find some statistic information from the database and use it to boost the retrieval performance.</p><p>For both Genomics and HARD tracks, we only use a very simple topic extraction method to extract the search terms from the given topics. A simple count of the terms within topics is served as a query term frequency. No phrase is constructed from the topics. For the future work, we will design new algorithms to find the most relevant search terms and phrases from the given topics. We expect a better topic extraction algorithm will improve the performance.</p><p>Other methods can also be applied to improve the performance, e.g. designing new term weighting formulae and using machine learning methods. For the BM50 weighting function, we can try to test different values of the parameters to improve the retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,168.72,554.95,274.54,12.55"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution curve for the whole documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,163.19,468.94,285.60,12.55"><head>FrequencyFigure 3 :</head><label>3</label><figDesc>Figure 3: Distribution curve for the relevant documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,167.78,484.31,276.42,180.54"><head>Table 2 :</head><label>2</label><figDesc>: 2004 relevant dataset from the training topic</figDesc><table coords="5,190.04,520.99,231.90,119.86"><row><cell cols="3">Min Length Max Length Average Length</cell></row><row><cell>2</cell><cell>63870</cell><cell>2188.0344</cell></row><row><cell></cell><cell cols="2">Table 1: 2004 Hard Corpus</cell></row><row><cell cols="3">Min Length Max Length Average Length</cell></row><row><cell>131</cell><cell>26461</cell><cell>3320.695</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,110.88,352.31,388.28,187.80"><head>Table 3 :</head><label>3</label><figDesc>Document level evaluation results</figDesc><table coords="12,110.88,403.92,388.28,136.19"><row><cell>Run</cell><cell cols="4">Description Bpref@12K Char PassagePrec@10 PassageRPrec</cell></row><row><cell>york04hb1</cell><cell>baseline</cell><cell>0.088</cell><cell>0.1532</cell><cell>0.0672</cell></row><row><cell>york04ha1</cell><cell>automatic</cell><cell>0.358</cell><cell>0.4420</cell><cell>0.2856</cell></row><row><cell>york04ha2</cell><cell>automatic</cell><cell>0.186</cell><cell>0.2729</cell><cell>0.2481</cell></row><row><cell>york04hm1</cell><cell>manually</cell><cell>0.352</cell><cell>0.3956</cell><cell>0.3511</cell></row><row><cell>york04hm2</cell><cell>manually</cell><cell>0.183</cell><cell>0.2778</cell><cell>0.2926</cell></row><row><cell>Average</cell><cell>Over all</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best</cell><cell>136 runs</cell><cell>0.358</cell><cell>0.4420</cell><cell>0.3511</cell></row><row><cell>Average</cell><cell>Over all</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Median</cell><cell>136 runs</cell><cell>0.200</cell><cell>0.1732</cell><cell>0.1091</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,200.93,551.50,206.22,12.55"><head>Table 4 :</head><label>4</label><figDesc>Passage level evaluation results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,166.07,506.73,279.84,12.55"><head>Table 5 :</head><label>5</label><figDesc>Experiments for U.S. Filter and Related Text</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,104.34,192.53,401.34,95.87"><head>Table 6 :</head><label>6</label><figDesc>Experiments for BM50</figDesc><table coords="14,104.34,192.53,401.34,72.23"><row><cell>Run</cell><cell>Description</cell><cell cols="3">Hard-rel RPrec Bpref@12K Char PassageRPrec</cell></row><row><cell cols="2">ExperimentA No BM50 &amp; No BF</cell><cell>0.3132</cell><cell>0.3181</cell><cell>0.2864</cell></row><row><cell>ExperimentB</cell><cell>BM50 (6.5%)</cell><cell>0.3255</cell><cell>0.3144</cell><cell>0.2756</cell></row><row><cell>ExperimentC</cell><cell>BM50 (13%)</cell><cell>0.3256</cell><cell>0.311</cell><cell>0.2769</cell></row><row><cell>ExperimentD</cell><cell>BM50 (20%)</cell><cell>0.3185</cell><cell>0.3093</cell><cell>0.2800</cell></row><row><cell>ExperimentE</cell><cell>BM50 (25%)</cell><cell>0.3195</cell><cell>0.3088</cell><cell>0.2899</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="15,125.73,121.02,360.55,80.68"><head>Table 7 :</head><label>7</label><figDesc>2004 Genomic Track results</figDesc><table coords="15,125.73,121.02,360.55,56.68"><row><cell cols="2">Description Mean</cell><cell cols="3">&gt; Median Median &lt; Median</cell></row><row><cell></cell><cell>Precision</cell><cell></cell><cell></cell><cell></cell></row><row><cell>york04g1 automatic</cell><cell>0.1794</cell><cell>19</cell><cell>3</cell><cell>28</cell></row><row><cell>york04g2 manually</cell><cell>0.2011</cell><cell>22</cell><cell>15</cell><cell>13</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,107.93,692.05,414.07,10.46;4,90.00,704.00,432.01,10.46;4,90.01,715.95,384.81,10.46"><p>The Geography parameter restricts the region discussed in the returned articles. For example, articles concerned with the state of affairs in other countries will not be welcome returns for topics in which the US value has been selected. It is used as the U.S. filter in our experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,107.93,714.75,101.96,10.46"><p>http://www.systat.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,107.93,708.04,319.65,10.46"><p>Okapi BSS is written in C and our main search engine is written in Java.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="13,107.93,711.23,414.08,10.46;13,90.00,723.20,61.23,10.46"><p>This is because we use documents instead of passages in the blind feedback process for documentlevel retrieval.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgement</head><p>This research is supported by research grants from the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC) of Canada and Atkinson Faculty of York University</rs>. We also would like to thank <rs type="person">Kai Zheng</rs> and <rs type="person">Xin Gao</rs> for their help in this research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,108.21,148.74,413.76,12.55;16,108.21,163.19,193.60,12.55" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,155.43,148.74,366.54,12.55;16,108.21,163.19,20.29,12.55">HARD Track Overview in TREC 2004. The 13th Text Retrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,137.47,163.19,128.62,12.55">NIST Special Publication</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,108.21,187.60,413.74,12.55;16,108.21,202.05,413.77,12.55;16,108.21,216.49,277.46,12.55" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,160.61,202.05,88.30,12.55">Okapi at TREC-5</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,384.19,202.05,137.80,12.55;16,108.21,216.49,103.79,12.55">Proceedings of the 5th Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the 5th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,108.21,240.90,413.73,12.55;16,108.21,255.34,413.75,12.55;16,108.21,269.80,413.78,12.55;16,108.21,284.24,76.24,12.55" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,259.76,240.90,262.18,12.55;16,108.21,255.34,177.37,12.55">A Probabilistic Approach to Chinese Information Retrieval: Theory and Experiments</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,294.17,255.34,227.79,12.55;16,108.21,269.80,270.90,12.55">Proceedings of the BCS-IRSG 2000: the 22nd Annual Colloquium on Information Retrieval Research</title>
		<meeting>the BCS-IRSG 2000: the 22nd Annual Colloquium on Information Retrieval Research<address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="178" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,108.21,308.65,413.73,12.55;16,108.21,323.09,413.77,12.55;16,108.21,337.54,413.71,12.55;16,108.21,351.98,322.28,12.55" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,466.81,308.65,55.14,12.55;16,108.21,323.09,329.19,12.55">Using Self-Supervised Word Segmentation in Chinese Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,450.12,323.09,71.86,12.55;16,108.21,337.54,413.71,12.55;16,108.21,351.98,165.04,12.55">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">August 11-15, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,108.21,376.40,413.73,12.55;16,108.21,390.84,413.78,12.55;16,108.21,405.29,187.19,12.55" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,492.95,376.40,29.00,12.55;16,108.21,390.84,338.80,12.55">Using Machine Learning for Text Segmentation in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,460.86,390.84,61.13,12.55;16,108.21,405.29,43.82,12.55">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="333" to="362" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,108.21,429.69,413.78,12.55;16,108.21,444.14,388.70,12.55" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,272.77,429.69,185.80,12.55">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Sparck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,468.64,429.69,53.35,12.55;16,108.21,444.14,234.48,12.55">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,108.21,468.54,413.73,12.55;16,108.21,483.00,413.77,12.55;16,108.21,497.44,26.66,12.55" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,338.03,468.54,183.90,12.55;16,108.21,483.00,265.01,12.55">Microsoft Cambridge at TREC-12: HARD Track. The 12th Text Retrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,384.71,483.00,132.11,12.55">NIST Special Publication</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
