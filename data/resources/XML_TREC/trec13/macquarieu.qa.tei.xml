<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,208.92,94.70,179.73,12.91">AnswerFinder at TREC 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.48,138.21,62.14,10.76"><forename type="first">Diego</forename><surname>Mollá</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology Division of Information and Communication Sciences Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,304.80,138.21,79.23,10.76"><forename type="first">Mary</forename><surname>Gardiner</surname></persName>
							<email>gardiner@ics.mq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology Division of Information and Communication Sciences Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,208.92,94.70,179.73,12.91">AnswerFinder at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F13421281A8B1123989B682A59B7038</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AnswerFinder combines lexical, syntactic, and semantic information in various stages of the question answering process. The candidate sentences are preselected on the basis of (i) the presence of named entity types compatible with the expected answer type, and (ii) a score combination of the overlap of words, grammatical relations, and flat logical forms. The candidate answers, in turn, are extracted from (i) the set of compatible named entities and (ii) the output of a logical-form pattern matching algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This document describes the AnswerFinder system as it stood at the time of the TREC 2004 question answering track. 1 Section 2 describes the architecture of the system as a whole, Section 3 details the function of each module within the An-swerFinder system, Section 4 gives the system performance on the TREC 2004 question set and Section 5 gives the conclusions and future work.</p><p>The AnswerFinder question answering system is designed to answer TREC-style factoid questions with an exact answer as below: Q: How far is it from Mars to Earth?</p><p>A: 416 million miles 1 This work is supported by the Australian Research Council, ARC Discovery grant n. DP0450750.</p><p>In 2004, TREC questions consisted of factoid, list, and "other" questions and they were XMLformatted in groups of questions about a single topic as shown in Figure <ref type="figure" coords="1,417.00,314.01,4.06,9.82" target="#fig_0">1</ref>.</p><p>All the questions in the example of Figure <ref type="figure" coords="1,520.08,328.41,5.45,9.82" target="#fig_0">1</ref> refer to Fred Durst, so question with ID number 2.2 is a factoid question asking What record company is Fred Durst with?. Question with ID number 2.3 is a list question in which all correct factoids should be listed in response, and question with ID number 2.5 is an "other" question in which relevant nuggets of information not given in response to earlier questions should be returned.</p><p>We spent most of our effort to handle the factoid questions and very little effort was done for the list and "other" questions. In this paper we will therefore focus on AnswerFinder's handling of factoid questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The question answering procedure used by An-swerFinder follows the pipeline structure that is typical of rule-based question answering systems. The process is outlined in Figure <ref type="figure" coords="1,458.16,595.65,5.45,9.82">2</ref> and is as follows:</p><p>1.  5. Exact answers -fragments like 416 million miles -are extracted from the candidate answer sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>. The exact answer list is sorted, re-scored and filtered for duplicate exact answers.</p><p>7. A number of exact answers from the top of the list are selected, depending on the question being factoid, list, or "other".</p><p>AnswerFinder uses the following knowledge sources to analyse the question and to select from among possible answers: ¯Named entity data generated by the GATE system <ref type="bibr" coords="2,362.16,599.25,106.19,9.82" target="#b1">(Gaizauskas et al., 1996)</ref>. The named entity types are listed in Table <ref type="table" coords="2,464.28,612.81,4.06,9.82" target="#tab_1">1</ref>. The named entity detector was run off-line and all the named entities of the AQUAINT corpus was stored in a set of files prior to the processing of any questions.</p><p>¯The list of documents provided by NIST, containing for each target entity the 1000 top scoring documents for that entity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Normalisation</head><p>Given that the questions may contain anaphoric references to information external to the questions, AnswerFinder performs simple anaphora resolution on question strings.</p><p>Questions in the TREC 2004 competition coreferred with previous questions or with their target in a number of ways.</p><p>Questions might co-refer with their target pronominally:</p><formula xml:id="formula_0" coords="3,93.84,373.17,172.85,29.35">Target ¾ : Fred Durst Q ¾ ¾ : What record company is he with?</formula><p>Questions might co-refer with their target using a definite noun phrase: Questions might co-refer with another question:</p><formula xml:id="formula_1" coords="3,93.84,561.81,174.51,73.06">Target ¾ : Fred Durst Q ¾ ½ : What is the name of Durst's group? Q ¾ ¿ : What are titles of the group's re- leases?</formula><p>Finally, questions may relate to their target associatively, that is, there may not be a direct coreference:</p><p>Target : Heaven's Gate Q ¿ : When did the mass suicide occur?</p><p>AnswerFinder normalises questions in the first case only, where the question co-refers with the target pronominally. It performs a simple replacing of the pronouns of the question with the target's identity. Since AnswerFinder needs syntactically correct questions, the target is transformed to the plural form or the possessive form where necessary. The transformation uses very simple morphological rules to transform the questions as shown in Table <ref type="table" coords="3,376.08,211.05,4.06,9.82" target="#tab_2">2</ref>.</p><p>In addition, "other" type questions containing the single word Other are transformed into What is TARGET? so that question 2.5 in Figure <ref type="figure" coords="3,483.00,252.33,5.45,9.82" target="#fig_0">1</ref> is transformed into What is Fred Durst?. This is a very crude attempt at doing something useful with the "other" type questions. Clearly more targeted processing of these questions is advisable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Classification</head><p>Particular questions signal particular named entity types expected as responses. Thus, the example below expects a person's name in response to the question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Who founded the Black Panthers organization?</head><p>AnswerFinder uses a set of 29 regular expressions to determine the expected named entity type. These regular expressions are the same used in our contribution to TREC 2003 and they target the occurrence of Wh-question words. In addition, specific keywords in the questions indicate expected answer types as shown in Table <ref type="table" coords="3,447.36,537.93,4.06,9.82" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Candidate Sentence Extraction</head><p>Given the set of AQUAINT documents preselected by the NIST information extraction system, An-swerFinder selects 100 sentences from these documents as candidate answer sentences.</p><p>Candidate sentences are selected in the following way:</p><p>1. The 1000 preselected documents provided by NIST for each target are split into sentences.</p><p>The sentence splitter follows a simple approach based on the use of a fixed list of sentence delimiters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What record company is he with?</head><p>What record company is Fred Durst with? How many of its members committed suicide?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How many of Heaven's Gate's members committed suicide? In what countries are they found?</head><p>In what countries are agoutis found? 1 point for each non-stopword overlapping with the question string, and 10 points for the presence of a named entity of the expected answer type.</p><p>3. The 100 top scoring sentences are returned as candidate answer sentences.</p><p>As an example of the scoring mechanism, consider this question/sentence pair:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: How far is it from Mars to Earth?</head><p>A: According to evidence from the SNC meteorite, which fell from Mars to Earth in ancient times, the water concentration in Martian mantle is estimated to be 40 ppm, far less than the terrestrial equivalents.</p><p>The question and sentence have 2 shared nonstopwords: Mars and Earth. Further, this sentence has a named entity of the required type (Number): 40 ppm, making the total score for this sentence 12 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence Re-scoring</head><p>The 100 candidate sentences are re-scored based upon the combination of lexical, syntactic, and semantic features:</p><p>Lexical: The combined word overlap and named entity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic:</head><p>The grammatical relation overlap score.</p><p>Semantic: Two ways of computing overlaps with flat logical form patterns.</p><p>The use of lexical information has been described in Section 3.3. Below we will briefly explain the use of syntactic and semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Grammatical Relation Overlap Score</head><p>The grammatical relations devised by <ref type="bibr" coords="4,307.32,547.77,87.27,9.82" target="#b0">Carroll et al. (1998)</ref> encode the syntactic information of questions and candidate answer sentences. We decided to use grammatical relations and not, say, parse trees or dependency structures for two reasons:</p><p>1. Unlike parse trees, and like dependency structures, grammatical relations are easily incorporated into an overlap-based similarity measure.</p><p>2. Parse trees and dependency structures are dependent on the grammar formalism. In contrast, the grammatical relations have been devised as a means to normalise the output of parsers for their comparative evaluation. As a result, the grammatical relations are independent of the grammar formalism or the actual parser used. Our choice of parser was the Connexor Dependency Functional Grammar and parser <ref type="bibr" coords="5,142.92,156.81,142.43,9.82" target="#b9">(Tapanainen and Järvinen, 1997)</ref>.</p><p>Since it is dependency-based, the transformation to grammatical relations is relatively straightforward.</p><p>An example of the grammatical relations for question and candidate sentence follows: The similarity-based score is the number of relations shared between question and sentence. In example 3.4.1, the overlap between the grammatical relations of question and candidate sentence is 2: (subj be it ) and (ncmod earth from to).</p><formula xml:id="formula_2" coords="5,93.84,250.41,34.88,9.82">Q: How</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Flat Logical Form Patterns</head><p>Semantic information is represented by means of flat logical forms <ref type="bibr" coords="5,166.80,567.57,60.11,9.82" target="#b6">(Mollá, 2001)</ref>. These logical forms use reification to flatten out nested expressions in a way similar to other QA systems <ref type="bibr" coords="5,72.00,608.13,106.95,9.82" target="#b2">(Harabagiu et al., 2001;</ref><ref type="bibr" coords="5,183.72,608.13,46.71,9.82" target="#b3">Lin, 2001;</ref><ref type="bibr" coords="5,235.20,608.13,55.05,9.82;5,72.00,621.69,84.36,9.82">Mollá et al., 2000, for example)</ref>. The logical forms are produced by means of a process of bottom-up traversal of the dependency structures returned by Connexor <ref type="bibr" coords="5,99.36,662.37,129.95,9.82" target="#b4">(Mollá and Hutchinson, 2002)</ref>.</p><p>Our contribution to TREC 2003 <ref type="bibr" coords="5,228.72,675.93,56.31,9.82" target="#b7">(Mollá, 2003</ref>) manipulated flat logical forms in the same way as grammatical relations are manipulated above, so that the score of example 3.4.2 would be computed as follows: In our 2003 system, the flat logical form score of example 3.4.2 would have been 2, as the number of overlaps between the logical form of question and answer is 2. Note that the process to compute the overlap of logical forms must map the variables from the question to variables from the candidate answer sentence. AnswerFinder uses Prolog unification for this process.</p><formula xml:id="formula_3" coords="5,329.04,89.13,37.23,9.82">Q: What</formula><p>With the goal to take in consideration the differences between a question and the various forms to answer it, AnswerFinder in TREC 2004 uses patterns to capture the expected form of the answer sentence and locate the exact answer. Thus, if a question is of the form what is X of Y?, then a likely answer can be found in sentences like Y has a X of ANSWER. In contrast with other approaches, AnswerFinder uses flat logical form patterns. For example, the pattern for what is X of Y? is:</p><formula xml:id="formula_4" coords="5,329.04,523.89,141.81,63.94">Question Pattern: object(ObjX,VobjX,[VeX]), object(what, ,[VeWHAT]), object(ObjY,VobjY,[VeWHAT]), prop(of, ,[VexistWHAT,VeX])</formula><p>And the pattern of Y has a X of ANSWER is: <ref type="bibr" coords="5,329.04,643.29,109.55,9.82">prop(of, ,[VeY,VeANSW]</ref>), object <ref type="bibr" coords="5,358.06,656.85,82.23,9.82">(ObjX,VobjX,[VeX]</ref>), evt <ref type="bibr" coords="5,342.15,670.41,101.25,9.82">(have, ,[VeX,VeWHAT]</ref>), object <ref type="bibr" coords="5,357.88,683.97,81.71,9.82">(ObjY,VobjY,[VeY]</ref>) Borrowing Prolog notation, the above patterns use uppercase forms or ' ' to express the arguments that can unify with logical form arguments.</p><formula xml:id="formula_5" coords="5,329.04,616.17,159.69,23.38">Answer Pattern: dep(ANSWER,ANSW,[VeANSW]),</formula><p>As the logical form of What is the population of Iceland? matches the above question pattern, then its logical form is transformed into: <ref type="bibr" coords="6,111.40,166.17,91.86,9.82">(of, ,[VeY,VeANSW]</ref>), object <ref type="bibr" coords="6,119.33,179.73,67.98,9.82">(iceland,o6,[x6]</ref>), evt <ref type="bibr" coords="6,106.95,193.29,62.97,9.82">(have, ,[x6,x1]</ref>), object <ref type="bibr" coords="6,121.15,206.85,54.63,9.82">(population,</ref><ref type="bibr" coords="6,175.78,206.85,13.66,9.82">o4,</ref><ref type="bibr" coords="6,189.44,206.85,22.76,9.82">[VeY]</ref>)</p><formula xml:id="formula_6" coords="6,93.84,139.05,165.17,36.94">Q: What is the population of Iceland? dep(ANSWER,ANSW,[VeANSW]), prop</formula><p>Now the transformed logical form shares all five terms with the logical form of Iceland has a population of 270000, hence the score of this answer sentence is 5. In addition, AnswerFinder knows that the answer is 270000, since this is the value that fills the slot ANSWER.</p><p>The use of flat logical forms makes it possible to handle certain types of paraphrases <ref type="bibr" coords="6,227.88,324.57,62.37,9.82;6,72.00,338.13,25.23,9.82" target="#b8">(Rinaldi et al., 2003)</ref> and therefore we believe that patterns based on flat logical forms such as the ones described above are more appropriate than patterns based on syntactic information or surface strings for the task of identifying answer sentences. However, the resulting patterns are difficult to read by humans and the process of developing the patterns becomes very time-consuming. Due to time constraints we developed 10 generic templates only. Each template consists of a pattern to match the question, and one or more replacement patterns. The above example is one of the question/replacement patterns.</p><p>There were two flat logical form pattern overlap algorithms used by AnswerFinder in TREC 2004: an algorithm which can extract one value for the ANSWER variable, and an algorithm which can extract all possible values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Exact Answer Extraction, Filtering and Scoring</head><p>Having selected and re-ranked the 100 candidate sentences, AnswerFinder then selects an exact answer string from within them. Ideally, the logical form patterns will have identified the exact answer. In practise, given the reduced number of patterns developed, many good answer sentence or even questions would not have a logical form that matches any of the patterns. As a result we integrate the expected answer type provided by the question analyser and the results of the named entity recogniser. In particular:</p><p>1. For each candidate sentence, extract all named entities that match the question classification.</p><p>2. For each candidate sentence, extract AN-SWER values from any matching flat logical form pattern.</p><p>Exact answers are scored in this way:</p><p>1.</p><p>If the exact answer is an identified named entity of the expected answer type, its score is the score of the candidate sentence it is found in.</p><p>2.</p><p>If the exact answer is an ANSWER value from a flat logical form answer pattern, its score is the score of the candidate sentence it is found in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>If the exact answer is both a named entity and an ANSWER value from a flat logical form answer pattern, its score is twice the score of the candidate sentence it is found in.</p><p>If an answer is a duplicate of a higher scoring answer, the two answers are merged and the score becomes the sum of the scores of the duplicate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Exact Answer Selection</head><p>AnswerFinder selects the answers in the following ways:</p><p>Factoid questions requiring exactly one answer, or "NIL" indicating no answer:</p><p>¯if there are no answers with a score more than 0, return "NIL"; otherwise ¯return one of the top scoring answers List questions and "other" questions requiring a number of answers: return all top scoring exact answers. If there is no exact answer with a score of more than 0, return the top scoring candidate sentence.</p><p>We submitted three runs based on combinations of candidate answer sentence re-ranking scores and the use or not of all the possible answers returned by the logical form patterns: answfind1 used the overlap of logical forms (Ð Ó) and the logical form patterns returned the first possible answer (single).</p><p>answfind2 used 3 times the grammatical relation overlap score added to the flat logical form pattern overlap score (¿ ÖÓ•Ð Ó). This combination was determined empirically. The logical form patterns returned the first possible exact answer (single).</p><p>answfind3 used the same score combination as "answfind2" (¿ ÖÓ • Ð Ó), but this time the logical form patterns returned all the possible exact answers (multi).</p><p>The results of the three runs (Table <ref type="table" coords="7,237.84,392.97,4.52,9.82" target="#tab_7">4</ref>) were surprisingly similar. The F measure of the list question is remarkably close to the median considering our simple treatment, and the F measure of the "other" questions is understandably low given that we treated them as list questions of the form what is TARGET?. A subsequent analysis of the code revealed a bug that made the program to effectively ignore the use of the logical form patterns. This explains the nearly similar results in all runs. After fixing the bug the accuracy of our system changed as shown in Table <ref type="table" coords="7,142.68,662.37,4.06,9.82" target="#tab_8">5</ref>. The evaluation was performed using Ken Litkowsky's patterns. Since this is an automatic evaluation, the results had lower accuracy than the ones presented in  The results of our internal evaluation indicate that the first answer returned by a logical form pattern is not usually the correct one. As a result, incorrect answers were given a score boost. The main reason for logical form patterns not giving the correct answer is that some of them were so general that they identified a large set of possible answers within a sentence. As an indication, Table <ref type="table" coords="7,323.28,301.89,5.45,9.82" target="#tab_10">6</ref> shows the coverage of the patterns used.  Clearly, this pattern will match any object of the candidate answer sentence. In the "answfind1" and "answfind3" runs, more often than not the answer chosen was the wrong one. However, in the "answfind3" run, since all matching answers were returned, those that were of the correct expected answer type would have an additional score boost.</p><p>Our results also indicate that even a small set of patterns such as the ones used in our system can help to obtain better results. By extending the set of patterns and reducing the use of general patterns the overall performance is expected to increase.</p><p>AnswerFinder combines lexical, syntactic, and semantic information in a simple pipeline-based system. Lexical information is based on the list of non-stop words. Syntactic information is based on the use of grammatical relations. Semantic information is based on the use of flat logical forms. The main differences from the system participating in TREC 2003 are the use of named entities and the development of a process to handle logical form patterns and exact answer extraction.</p><p>Further work includes the fine-tuning of various parameters used by AnswerFinder such as the ideal combination of the lexical, syntactic, and semantic information. Time and resource permitting we will use machine learning methods to optimise the weights of each element in the final scoring formula. Another parameter that may be optimised by means of machine learning methods is the scoring threshold for NIL answers and for returning an answer to a list question.</p><p>An obvious direction of further research is the treatment of "other" questions. The transformation from other to what is TARGET? performed by AnswerFinder assumes that these questions are answered as definitions but this is not necessarily the case. We will explore the use of logical form patterns inspired in methods to answer definition questions.</p><p>Additional further research includes the refining of the candidate sentence scoring and the exact answer scoring, and the development of a more comprehensive and detailed set of logical form patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,657.69,218.19,9.82;2,72.00,671.25,116.79,9.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a group of questions using the TREC 2004 format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,93.84,452.13,89.69,11.23;3,93.84,470.25,174.29,11.23;3,93.84,483.81,118.01,9.82;3,93.84,501.93,174.53,11.23;3,93.84,515.49,28.53,9.82"><head>Target</head><label></label><figDesc>¾½ : Club Med Q ¾½ ½ : How many Club Med vacation spots are there worldwide? Q ¾½ ¾ : List the spots in the United States.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.36,87.57,217.74,91.54"><head>Table 1 :</head><label>1</label><figDesc>Named entity types recognised by GATE</figDesc><table coords="3,152.76,87.57,56.33,64.06"><row><cell>Date</cell></row><row><cell>Location</cell></row><row><cell>Money</cell></row><row><cell>Organization</cell></row><row><cell>Person</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,145.92,151.53,305.55,142.78"><head>Table 2 :</head><label>2</label><figDesc>Examples of pronoun resolution performed by AnswerFinder</figDesc><table coords="4,163.08,175.77,271.25,118.54"><row><cell>Keyword</cell><cell>Answer Type</cell></row><row><cell>city, cities</cell><cell>Location</cell></row><row><cell>percentage</cell><cell>Number</cell></row><row><cell>first name, middle name</cell><cell>Person</cell></row><row><cell>range</cell><cell>Number</cell></row><row><cell>rate</cell><cell>Number</cell></row><row><cell>river, rivers</cell><cell>Location</cell></row><row><cell cols="2">what is, what are, what do Person, Organisation or Location</cell></row><row><cell>how far, how long</cell><cell>Number</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,80.64,312.33,369.64,43.06"><head>Table 3 :</head><label>3</label><figDesc>Examples of question keywords and associated answer types 2. Each sentence is assigned a numeric score:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,93.84,250.41,174.53,189.70"><head></head><label></label><figDesc>far is it from Mars to Earth?</figDesc><table coords="5,93.84,263.97,174.53,176.14"><row><cell>(subj be it )</cell></row><row><cell>(xcomp from be mars)</cell></row><row><cell>(ncmod be far)</cell></row><row><cell>(ncmod far how)</cell></row><row><cell>(ncmod earth from to)</cell></row><row><cell>A: It is 416 million miles from Mars to</cell></row><row><cell>Earth.</cell></row><row><cell>(ncmod earth from to)</cell></row><row><cell>(subj be it )</cell></row><row><cell>(ncmod from be mars)</cell></row><row><cell>(xcomp be mile)</cell></row><row><cell>(ncmod million 416)</cell></row><row><cell>(ncmod mile million)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,72.00,86.86,419.42,653.01"><head>Table 4</head><label>4</label><figDesc>, but the relative increase or decrease of accuracy was significant.</figDesc><table coords="7,341.28,86.86,150.14,48.27"><row><cell>Run</cell><cell cols="2">Before After</cell><cell>Accuracy</cell></row><row><cell>Name</cell><cell>Fixes</cell><cell cols="2">Fixes Increase</cell></row><row><cell cols="2">answfind1 0.086</cell><cell cols="2">0.071 -17%</cell></row><row><cell cols="2">answfind2 0.086</cell><cell cols="2">0.066 -23%</cell></row><row><cell cols="2">answfind3 0.086</cell><cell cols="2">0.096 11%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,307.32,147.57,217.97,23.38"><head>Table 5 :</head><label>5</label><figDesc>Automatic analysis of factoid accuracy with Ken Litkowsky's patterns</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="7,307.32,416.13,218.20,152.38"><head>Table 6</head><label>6</label><figDesc></figDesc><table coords="7,307.32,416.13,218.20,152.38"><row><cell>: Number of questions triggering each</cell></row><row><cell>question pattern; note that a question may trigger</cell></row><row><cell>several patterns</cell></row><row><cell>The most frequent pattern by far is</cell></row><row><cell>"what generic", which was defined as:</cell></row><row><cell>Question Pattern:</cell></row><row><cell>object(what, ,[XWho])</cell></row><row><cell>Answer Pattern:</cell></row><row><cell>object( ,ANSWER,[XWho])</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,595.82,218.49,8.97;8,82.92,606.86,207.55,8.97;8,82.92,617.78,98.73,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,112.44,606.86,178.03,8.97;8,82.92,617.78,19.68,8.97">Parser evaluation: a survey and a new proposal</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Sanfilippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,120.84,617.78,55.53,8.97">Proc. LREC98</title>
		<meeting>LREC98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,635.90,218.34,8.97;8,82.92,646.82,207.57,8.97;8,82.92,657.86,207.43,8.97;8,82.92,668.78,207.42,8.97;8,82.92,679.70,207.32,8.97;8,82.92,690.74,207.57,8.97;8,82.92,701.66,29.73,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,82.92,657.86,207.43,8.97;8,82.92,668.78,181.57,8.97">GATE: an environment to support research and development in natural language engineering</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Humphreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,82.92,679.70,207.32,8.97;8,82.92,690.74,162.11,8.97">Proceedings of the 8th IEEE International Conference on Tools with Artificial Intelligence</title>
		<meeting>the 8th IEEE International Conference on Tools with Artificial Intelligence<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,719.78,218.23,8.97;8,82.92,730.70,207.34,8.97;8,318.24,89.78,207.57,8.97;8,318.24,100.70,207.30,8.97;8,318.24,111.62,207.33,8.97;8,318.24,122.66,207.33,8.97;8,318.24,133.58,207.43,8.97;8,318.24,144.50,58.77,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,318.24,100.70,207.30,8.97;8,318.24,111.62,137.82,8.97">Answering complex, list and context questions with LCC&apos;s question-answering server</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pas ¸ca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Gîrju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finley</forename><surname>Lȃcȃtus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rȃzvan</forename><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,504.12,122.66,21.45,8.97;8,318.24,133.58,207.43,8.97;8,318.24,144.50,28.88,8.97">Proc. TREC 2001, number 500-250 in NIST Special Publication</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC 2001, number 500-250 in NIST Special Publication</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.32,164.42,218.41,8.97;8,318.24,175.46,207.33,8.97;8,318.24,186.38,20.13,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,399.48,164.42,126.25,8.97;8,318.24,175.46,137.96,8.97">Indexing and retrieving natural language using ternary expressions</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="8,307.32,206.30,218.59,8.97;8,318.24,217.22,207.57,8.97;8,318.24,228.26,173.01,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,472.56,206.30,53.36,8.97;8,318.24,217.22,203.71,8.97">Dependencybased semantic interpretation for answer extraction</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,328.92,228.26,157.65,8.97">Proc. 2002 Australasian NLP Workshop</title>
		<meeting>2002 Australasian NLP Workshop</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.32,248.18,218.34,8.97;8,318.24,259.10,207.44,8.97;8,318.24,270.02,207.45,8.97;8,318.24,281.06,62.37,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,418.20,259.10,107.48,8.97;8,318.24,270.02,44.49,8.97">Extrans, an answer extraction system</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rolf</forename><surname>Schwitter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rachel</forename><surname>Fournier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,372.72,270.02,148.41,8.97">Traitement Automatique des Langues</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="495" to="522" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.32,300.98,218.17,8.97;8,318.24,311.90,207.43,8.97;8,318.24,322.82,207.37,8.97;8,318.24,333.86,176.97,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,399.48,300.98,126.01,8.97;8,318.24,311.90,85.48,8.97">Ontologically promiscuous flat logical forms for NLP</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.32,322.82,62.29,8.97;8,318.24,333.86,29.29,8.97">Proceedings of IWCS-4</title>
		<editor>
			<persName><forename type="first">Harry</forename><surname>Bunt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Elias</forename><surname>Ielka Van Der Sluis</surname></persName>
		</editor>
		<editor>
			<persName><surname>Thijsse</surname></persName>
		</editor>
		<meeting>IWCS-4<address><addrLine>Tilburg University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="249" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.32,353.78,218.22,8.97;8,318.24,364.70,73.29,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,399.12,353.78,106.20,8.97">Answerfinder in trec 2003</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,318.24,364.70,48.29,8.97">Proc. TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.32,384.62,218.49,8.97;8,318.24,395.54,207.43,8.97;8,318.24,406.58,207.30,8.97;8,318.24,417.50,207.31,8.97;8,318.24,428.42,48.45,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,492.84,395.54,32.84,8.97;8,318.24,406.58,189.81,8.97">Exploiting paraphrases in a question answering system</title>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Dowdall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaarel</forename><surname>Kaljurand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,318.24,417.50,207.31,8.97;8,318.24,428.42,16.78,8.97">Proc. Workshop in Paraphrasing at ACL2003, Sapporo</title>
		<meeting>Workshop in Paraphrasing at ACL2003, Sapporo<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,307.32,448.34,218.35,8.97;8,318.24,459.38,207.45,8.97;8,318.24,470.30,22.05,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,496.08,448.34,29.60,8.97;8,318.24,459.38,117.40,8.97">A nonprojective dependency parser</title>
		<author>
			<persName coords=""><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Järvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,459.96,459.38,65.73,8.97;8,318.24,470.30,16.54,8.97">Proc. ANLP-97. ACL</title>
		<meeting>ANLP-97. ACL</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
