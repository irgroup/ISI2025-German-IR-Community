<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,118.62,74.96,374.77,10.80">UNT at TREC 2004: Question Answering Combining Multiple Evidences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,192.66,116.24,72.31,10.80"><forename type="first">Jiangping</forename><surname>Chen</surname></persName>
							<email>jpchen@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postBox>P.O. Box 311068</postBox>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.63,116.24,28.33,10.80"><forename type="first">He</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postBox>P.O. Box 311068</postBox>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.63,116.24,36.52,10.80"><forename type="first">Yan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postBox>P.O. Box 311068</postBox>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.92,116.24,62.42,10.80"><forename type="first">Shikun</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postBox>P.O. Box 311068</postBox>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,118.62,74.96,374.77,10.80">UNT at TREC 2004: Question Answering Combining Multiple Evidences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF5951C980483A9836A9A23EF2DF6ABB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) aims at identifying answers to users' natural language questions. A QA system can release the users from digesting large amount of text in order to locate particular facts or numbers. The research has drawn great attention from several disciplines such as information retrieval, information extraction, natural language processing, and artificial intelligence. TREC QA track has provided comparable QA system evaluation on a set of test questions since 1999. The degree of difficulty of the test questions has increased substantially in recent two years, which push the research toward applying more sophisticated strategies and better understanding of English texts.</p><p>Question answering is very challenging due to the ambiguity of the questions, complexity of linguistic phenomena involved in the documents, and the difficulty to understand natural languages. More challenging is to locate short snippets or answers from a document collection with texts written in different languages, which is within our research interests focusing on cross-lingual or multilingual information access and retrieval. We have decided to participate in TREC 2004 Question Answering Track as our first step toward exploring advanced multilingual information retrieval. Our goal of this year is to develop a prototype automatic question answering system that can be continually expanded and improved. Our prototype QA system, named EagleQA, made use of available NLP (Natural Language Processing) tools and knowledge resources for question understanding and answer finding. This paper describes the overall structure of the system, NLP tools and lexical resources employed, our QA methodology for TREC 2004, QA test results &amp; analysis, and our plan for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Current EagleQA system is comprised of 6 major subsystems: Question Processing, Document Annotation, Sentence Retrieval, Web QA, Answer Finding, and Answer Formulation. Following will briefly discuss each of the subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing</head><p>Question Processing subsystem accepts users' questions and performs several processing including linguistic analysis, keyword identification and expansion, and answer type identification. Linguistic analysis performs part-of-speech tagging and phrase bracketing on original questions. Keyword identification and expansion first extracts important words or phrases from the annotated question. A word or a phrase is regarded as important if it's not included in the stopword list of the system. The stopword list was generated manually by identifying words that occur frequently in previous TREC questions. Next, for each extracted noun and verb, its synonyms and derivation forms were identified based on WordNet 2.0 (www.princeton.edu). Those terms are added to the keyword list. Answer type identification is another important procedure in Query Processing. We developed a simple ontology for QA purpose. 16 top level categories were identified from previous TREC questions. Sample categories include ANIMAL, CODE, CURRENCY, LOCATION, NUMBER, ORGANIZATION, and PERSON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Annotation</head><p>This year we didn't carry out our own IR experiments to find relevant documents for test questions. Instead, we used the ranked document list provided by NIST. We plan to use Lemur (http://www-2.cs.cmu.edu/~lemur/) as the search engine for QA in the future.</p><p>Our Document Annotation subsystem combines document annotation results from two NLP systems: LingPipe (http://www.alias-i.com/lingpipe/) and Minipar <ref type="bibr" coords="2,436.69,364.64,52.52,10.80" target="#b3">(Lin, 1994)</ref>. LingPipe is used first to detect sentence boundaries, the identified sentences are sent to Minipar for part-of-speech tagging and named entity categorization. LingPipe can also perform named entity categorization and co-reference annotation. At last, we integrate the results of annotations from the two systems using an XML format. Figure <ref type="figure" coords="2,464.59,419.84,6.00,10.80" target="#fig_0">1</ref> shows an example of the combined annotated text. The categorization results from LingPipe are identified after 'ling_type =' in the xml brackets, while those from Minipar are labeled after 'mini_cat='. &lt;sent id="19"&gt; &lt;TOK id="1" pos="U"&gt;Last&lt;/TOK&gt; &lt;TOK id="2" pos="N"&gt;week&lt;/TOK&gt; &lt;TOK id="3" pos="U"&gt;,&lt;/TOK&gt; &lt;TOK id="4" pos="DET"&gt;the&lt;/TOK&gt; &lt;TOK id="5" pos="N"&gt;literature&lt;/TOK&gt; &lt;TOK id="6" pos="N"&gt;prize&lt;/TOK&gt; &lt;TOK id="7" pos="V" base="go" subj="prize:TOK_6"&gt;went&lt;/TOK&gt; &lt;TOK id="8" pos="PREP"&gt;to&lt;/TOK&gt; &lt;TOK id="9" pos="N" mini_cat="LANG"&gt;Portuguese&lt;/TOK&gt; &lt;TOK id="10" pos="N"&gt;novelist&lt;/TOK&gt; &lt;NP id ="1" mini_cat="PERSON" ling_type="PERSON"&gt; &lt;TOK id="11" pos="U"&gt;Jose&lt;/TOK&gt; &lt;TOK id="12" pos="N" mini_cat="PERSON"&gt;Saramago&lt;/TOK&gt; &lt;/NP&gt; &lt;TOK id="13" pos="U"&gt;.&lt;/TOK&gt; &lt;/sent&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Retrieval</head><p>The Sentence Retrieval subsystem identifies a certain number of non-duplicate sentences (500 sentences maximum for this year) from the annotated documents as sentence candidates which may contain an answer to each test question. The keyword lists and answer type information obtained in Question Processing are utilized to find matched sentences for each factoid and list question. For questions labeled other, the sentence retrieval subsystem returns the sentences that match the target as answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Web QA</head><p>The Internet is a huge and unique knowledge base. Our Web QA subsystem attempts to make use of this knowledge resource by submitting the original test questions to Google. The short summaries returned by Google are annotated and analyzed. A list of answer candidates that match the answer type of each question is then identified. Their frequency information is also kept as a factor for ranking the candidate by the Answer Finding subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Finding</head><p>Answer Finding subsystem applies multiple evidences to find answers for factoid and list test questions. Factors that are taken into account when ranking an answer candidate include: 1) answer type; 2) weight of the sentence; 3) distance to keywords in the same sentence; and 4) whether it is a candidate returned by Web QA 2.6 Answer formulation Finally, the system combines the answers for different types of questions such as factoid, list, and other questions. The duplicate answers are filtered out from the list of answers to other questions. An answer file is formulated at the end of this stage for submission.</p><p>Figure <ref type="figure" coords="3,124.35,461.18,6.00,10.80" target="#fig_1">2</ref> outlined the current architecture of our EagleQA system for TREC 2004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NLP tools and Knowledge Resources</head><p>As mentioned in the introduction, we chose to make use of freely available NLP tools and knowledge resource and to integrate them into our QA system. Following describes the NLP tools and knowledge resource employed by the different subsystems of EagleQA to process test questions or documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LingPipe (http://www.alias-i.com/lingpipe/)</head><p>LingPipe, an open source NLP software, is developed by Alias-I, Incorporated (http://www.alias-i.com/). LingPipe is regarded as "a suite of Java tools designed to perform linguistic analysis on natural language data." LingPipe provides linguistic analysis functions such as sentence boundary detection, named entity detection for person, organization, and location, and within-document co-reference resolution. Evaluation of LingPipe system performance for various tasks can be found at http://www.alias-i.com/lingpipe/benchmarks.html.  <ref type="bibr" coords="4,419.31,410.84,49.67,10.80" target="#b3">Lin (1994)</ref>, provides NLP functions such as part-of-speech tagging, phrase bracketing, and named entity categorization. An executive version can be downloaded from Dr. Lin's website. Minipar was used by our system in combination with LingPipe and WordNet to annotate questions and documents.</p><p>3.3 WordNet (http://www.cogsci.princeton.edu/~wn/) and Related Tools WordNet <ref type="bibr" coords="4,137.69,507.44,68.05,10.80" target="#b4">(Miller, 1990)</ref> is the well-known English ontology freely available on the Web and covers the vast majority of nouns, verbs, adjectives, and adverbs from the English language. It has been widely used in many NLP applications and other QA systems <ref type="bibr" coords="4,90.00,548.84,115.09,10.80" target="#b1">(Harabagiu, et al., 2003)</ref>.</p><p>In addition to the WordNet database itself, we made use of a Perl interface to WordNet: WordNet::QueryData developed by Jason Rennie (http://people.csail.mit.edu/u/j/jrennie/public_html/WordNet/). It allows the user direct access to the full WordNet semantic lexicon. For example, you can query synonyms, hyponyms, the gloss of a particular sense, and derived forms of a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Google (www.google.com)</head><p>As many other QA systems, we used Google.com to search for answers on the Internet. The Web QA subsystem submits the original questions to Google for short summaries from which we identify a list of possible answer candidates. The test results showed that Answer file generation by removing the duplicates our approach was too naïve and the performance was not satisfactory. Further analysis will be conducted to improve the strategies of identifying and ranking answer candidates from the search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QA Methodology for 2004</head><p>We basically employed similar strategy to factoid questions and list questions except that a threshold was applied to list questions in order to determine the number of answers that should be returned. The threshold was predetermined based on experiments using last year's test questions. Other questions are quite different from factoid questions. Therefore we used a different approach to answer them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">factoid questions and list questions</head><p>As depicted by 2, factoid questions and list questions were handled following the procedures of question processing, document annotation, sentence retrieval, Web QA, answer finding, and answer formulation. Following we will describe more specifically approaches for some of the subsystems, such as question processing, sentence retrieval, and answer finding.</p><p>For question processing, EagleQA first performed a shallow co-reference resolution to replace pronouns in questions with the actual targets. For example, in question 4.5 "Which was the first movie that he was in?" "he" was replaced by the target "James Dean". Then we used Minipar to tag each question and WordNet to expand nouns and verbs in the question, as described in 2.1. Final result for each question included the answer type and a list of keywords with expansions if any. For example, Question: Which was the first movie that James Dean was in?</p><p>Answer type: movie Keywords: movie(synonym: movie, film, picture, moving picture, moving-picture show, motion picture, motion-picture show, picture show, pic, flick) / James Dean / After annotating the retrieved documents using LingPipe and Minipar, we employed a simple strategy to find sentences that may contain the answers. Two factors were considered for ranking the sentences: number of question keywords occurred in the sentence, and whether the sentence contains the same answer type as the question. Higher weights were assigned to sentences that contain more keywords and named entities that are annotated the same as the answer type of the question.</p><p>At the answer finding stage, we first attempted to identify possible answer candidates. Named entities annotated by LingPipe and Minipar were identified as answer candidates if their categories are the same as those of the question. However, the document annotation using LingPipe and Minipar can only identify named entities in certain categories, such as person, country, location, city, money, number, and organization. Many questions ask about names in other categories, or cannot be answered using entitybased strategy. We therefore developed following additional strategies for answer candidate identification:</p><p>• WordNet hypernyms. For those questions asking about certain categories such as animal, disease, plant and color, we evaluated each noun or noun phrase in the sentence using knowledge about hypernym relationship in WordNet. For example, question "What is their gang color?" the answer type is color. For each noun in a candidate sentence, we used WordNet to check whether 'color' is one of its hypernyms. If conformed, the word or phrase was regarded as an answer candidate.</p><p>• Name lists. Some frequently asked questions, such as "Who was the 23 rd president of the United States?" can be easily answered if a list of US presidents has been stored in the system. We therefore manually developed about thirty short lists to store names about US presidents, NBA teams, candy brands, state nicknames, and newspaper agencies. If a noun or noun phrase matched one of the names in an appropriate list, it was regarded as an answer candidate. The criterion to choose the appropriate list depended on the degree of match between keywords of the question and the title of the list.</p><p>• Additional named entity patterns. In order to compensate for the incomplete document annotation, we developed a pattern set to identify names or titles of a book, a move, a poem, and a song. Also included are patterns to identify terms or phrases regarding speed, temperature, and age.</p><p>• Answer patterns. Some questions are difficult to apply entity-based strategy. For instance, question "How did James Dean die?" is ambiguous with regard to the answer type. We therefore developed simple patterns for answering such questions. These patterns were derived from the answers to the similar test questions of 2003.</p><p>Once the answer candidates were identified, the next step was to rank them in order to determine the final answer. The ranking strategy depended on following three evidences:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The weighting score of the answer sentence obtained in Sentence Retrieval subsystem. This score is determined by two factors: number of question keywords and answer type presence if applicable, as discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The weighting score of the answer candidates. Candidates obtained through different strategies outlined above might be assigned a different weighting score. For example, answer candidates obtained from document annotation had a higher score than those from additional named entity patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Web QA. If an answer candidate was also returned by the Web QA subsystem, it received a higher ranking.</p><p>A formula considering the above factors was used to calculate the final score for each candidate. The parameters in the formula were trained using a small number of test questions of last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other questions</head><p>The other questions were previous definition questions. Other questions require QA systems to provide information about the targets in addition to answers to the factoid and list questions for the same targets. The usual ways to answer definition questions include developing heuristic linguistic patterns to find matched sentence segments <ref type="bibr" coords="6,449.70,668.18,64.38,10.80;6,90.00,681.98,89.55,10.80" target="#b2">(Hildebrandt, Katz, &amp; Lin, 2004)</ref>. Due to the time constraints, we were unable to develop training material and heuristic patterns for other questions. We instead employed a simple strategy: just return the whole sentences which include more than 50% of the words in the targets. In ranking candidate sentences, higher scores were given to sentences which include the whole targets as phrases. This simple strategy sacrifices precision based on current performance measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Analysis</head><p>We submitted three runs, namely UNTQA04M1, UNTQA04M2, and UNTQA04M3. Their scores are listed in Table <ref type="table" coords="7,241.36,185.24,4.50,10.80" target="#tab_1">1</ref>. The three runs applied the same program for other questions. The difference between UNTQA04M1 and UNTQA04M2 was that UNTQA04M2 took into account Web QA or Google results for weighting the answer candidates, but UNTQA04M1 didn't. The difference between UNTQA04M1 and UNTQA04M3 was the different post processing for other questions. UNTQA04M1 returned non-duplicate answers for each other question. Non-duplicate means if an answer sentence had returned an answer for factoid or list questions for the same target, it was removed from the answer list for the other question. UNTQA04M3 didn't perform this process of removing duplicate answers. The results showed that the process produced no effect on system performance. Run UNTQA04M2 did slightly better than the other two runs, mainly because of the use of Google results. Event though the three runs were all above the median, there is big room for further improvement.</p><p>We are still in the process of analyzing our QA results. The analysis aims at discovering the weakest point of the system so that we can take it as the start point for future development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Research</head><p>Our QA system is still at a very early development stage. Some of the subsystems had not been fully tested before the TREC experiments due to time constraints. We will continue our effort to develop and evaluate the system.</p><p>We plan to use Lemur (http://www-2.cs.cmu.edu/~lemur/) as our search engine for document retrieval for QA. Based on our testing using 2003 questions, Lemur could retrieve more relevant documents than NIST search engine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,172.80,699.08,266.39,10.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Text Annotation Using LingPipe and Minipar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,228.96,355.94,154.02,10.80"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. EagleQA Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,96.66,107.68,404.21,207.32"><head>TREC questions Questions Processing Document Annotation Retrieved documents Answer type and keywords Answer file Sentence Retrieval Web QA Answer finding Answer Formulation</head><label></label><figDesc></figDesc><table coords="4,96.66,107.68,404.21,207.32"><row><cell>TREC questions</cell><cell>Answer type identification, Keywords identification &amp; expansion identification, Keywords expansion identification &amp; Answer type Questions Processing</cell><cell cols="2">Answer finding using Web QA finding using Answer Answer type and keywords</cell><cell>Answer candidates candidates Answer</cell><cell>Answer identification &amp; ranking for factoid and list questions based questions based factoid and list ranking for identification &amp; Answer finding Answer</cell><cell>Answers to factoid &amp; list questions Answer file generation by Formulation Answer Answers to factoid &amp; list questions</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Google Google</cell><cell></cell><cell>on multi-on multi-</cell><cell>removing the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>evidence evidence</cell><cell>duplicates</cell></row><row><cell>Retrieved documents</cell><cell>POS tagging, Co-reference resolution, NE categorization Document Annotation categorization resolution, NE Co-reference POS tagging,</cell><cell>Annotated docs docs Annotated</cell><cell>Answer Sentence Weighting &amp; Sentence &amp; Weighting Sentence Answer Retrieval</cell><cell cols="2">Answers to other questions Sentence candidates Sentence Answers to other questions candidates</cell><cell>Answer file</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Extraction Extraction</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,99.00,337.04,384.45,102.12"><head>Table 1 .</head><label>1</label><figDesc>Official Results</figDesc><table coords="7,104.40,357.32,379.05,81.84"><row><cell>Run</cell><cell>Factoid</cell><cell>List</cell><cell>Other</cell><cell>Final Score</cell></row><row><cell></cell><cell>(Accuracy )</cell><cell>(Average F )</cell><cell>( Average F )</cell><cell></cell></row><row><cell cols="2">UNTQA04M1 0.187</cell><cell>0.128</cell><cell>0.305</cell><cell>0.202</cell></row><row><cell cols="2">UNTQA04M2 0.196</cell><cell>0.123</cell><cell>0.305</cell><cell>0.205</cell></row><row><cell cols="2">UNTQA04M3 0.187</cell><cell>0.127</cell><cell>0.307</cell><cell>0.202</cell></row><row><cell>median</cell><cell>0.170</cell><cell>0.094</cell><cell>0.184</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,90.00,74.84,431.62,10.80;8,90.00,88.64,408.34,10.80;8,90.00,102.44,409.51,10.80;8,90.00,116.24,345.96,10.80" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,90.00,74.84,431.62,10.80;8,90.00,88.64,408.34,10.80;8,90.00,102.44,338.85,10.80">For question processing and answer finding, we are attempting template-based techniques in combination with machine learning to improve the system performance. Preparing training material will be our first step of exploring these new strategies</title>
		<imprint/>
	</monogr>
	<note>We expect to improve the QA performance of our EagleQA system in the near future</note>
</biblStruct>

<biblStruct coords="8,102.00,143.96,55.99,10.80;8,90.00,171.44,426.32,10.80;8,108.00,185.24,392.89,10.80;8,108.00,199.04,331.56,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,108.00,185.24,371.96,10.80">Answer Mining by combining extraction techniques with abductive reasoning</title>
		<author>
			<persName coords=""><forename type="first">References</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.00,199.04,261.31,10.80">Proceedings of The Twelfth Text Retrieval Conference</title>
		<meeting>The Twelfth Text Retrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,212.84,430.95,10.80;8,108.00,226.64,362.99,10.80;8,108.00,240.50,398.11,10.80;8,108.00,254.24,406.80,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,303.00,212.84,217.95,10.80;8,108.00,226.64,89.81,10.80">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,205.69,226.64,265.31,10.80;8,108.00,240.50,398.11,10.80;8,108.00,254.24,231.23,10.80">Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004)</title>
		<meeting>the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05">2004. May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,268.04,418.49,10.80;8,108.00,281.84,266.69,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,165.36,268.04,325.52,10.80">PRINCIPAR---An Efficient, broad-coverage, principle-based parser</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,108.00,281.84,132.04,10.80">Proceedings of COLING-94</title>
		<meeting>COLING-94<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="42" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,295.64,389.24,10.80;8,108.00,309.44,163.39,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,178.77,295.64,176.68,10.80">WordNet: an on-line lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,363.30,295.64,115.94,10.80;8,108.00,309.44,63.69,10.80">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>Special issue</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
