<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,172.80,47.09,266.30,18.66">The University of Michigan in Novelty 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,277.68,88.38,56.59,12.96"><forename type="first">G</forename><surname>Ünes ¸erkan</surname></persName>
							<email>gerkan@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,172.80,47.09,266.30,18.66">The University of Michigan in Novelty 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">585073DFAACC53F89407AE3459607372</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year we participated in the Novelty track. To find the relevant sentences, we combine sentence salience features that are inherited from text summarization domain with other heuristic features based on topic statements. We propose a novel method to extract the new sentences based on the graph-based ranking of the similarity relation between the sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview</head><p>The University of Michigan participated in all four tasks of the TREC 2004 Novelty track.</p><p>To find the relevant sentences in Tasks 1 and 3, we experimented with more than ten features. The system was trained with all possible subsets of these features on the Novelty 2003 data using different learning algorithms to be explained below. All of the features were integrated into the MEAD 1 text summarization system <ref type="bibr" coords="1,120.69,298.01,173.62,12.00" target="#b3">(Radev, Blair-Goldensohn, &amp; Zhang, 2001)</ref>. The following is a brief description of the features we used in the actual submissions, which gave us the best results on the training data:</p><p>• Centroid: The centroid score that is a measure of how close is the sentence to the centroid pseudosentence of the entire cluster. This is a measure of sentence salience which is proven to be successful in multi-document summarization domain <ref type="bibr" coords="1,285.16,355.85,145.09,12.00" target="#b4">(Radev, Jing, &amp; Budzikowska, 2000)</ref>.</p><p>• LexRank: The LexRank score <ref type="bibr" coords="1,241.06,375.77,88.66,12.00" target="#b0">(Erkan &amp; Radev, 2004</ref>) is a measure of sentence salience based on the eigenvector centrality of the graph-based representation of the sentences in a cluster. We will give a brief explanation of how to compute LexRank in Section 2 and Section 3.</p><p>• Length: The number of words in the sentence.</p><p>• QueryTitleCosine: The cosine similarity between the "title" field of the topic statement and the sentence weighted by the word idf's. Formally, the cosine between two sentences x and y is defined by idf-modified-cosine(x, y) = w∈x,y tf w,x tf w,y (idf w ) 2 xi∈x (tf xi,x idf xi ) 2 × yi∈y (tf yi,y idf yi ) 2 where tf w,s is the number of occurrences of the word w in the sentence s.</p><p>• QueryDescriptionCosine: The idf-modified-cosine similarity between "description" field of the topic and the sentence.</p><p>• QueryTitleWordOverlap: The similarity between the "title" field of the topic statement and the sentence based on simple word overlap.</p><p>• QueryDescriptionWordOverlap: The similarity between the "description" field of the topic statement and the sentence based on simple word overlap.</p><p>• FQueryDescriptionWordOverlap: Same as QueryDescriptionWordOverlap, but after expanding the "description" field with the relevant sentences in the first 5 documents of the cluster. This feature is used only in Task 3.</p><p>First three features above do not use the topic statements at all while the others use only one field of the topic statement. We never used the "narrative" field of the topic statement since it gave us no improvement in our experiments on last year's data.</p><p>To find the new sentences, we used two approaches. In Task 1, we implemented a very simple idea based on the cosine (dis)similarity between a sentence and the sentences that precede that sentence. In other tasks, we developed a novel method that makes use of the LexRank feature, which is explained in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task 1</head><p>To find the relevant sentences in Task 1, we used several sets of features for each run. We used Maxent 2.1.0 maximum entropy tool<ref type="foot" coords="2,180.96,208.02,3.48,8.40" target="#foot_0">2</ref> for training our system on the 2003 data. Since all of the features are real-valued, we discretized the features using the entropy-based discretization algorithm proposed in <ref type="bibr" coords="2,427.15,220.73,90.34,12.00">(Fayyad &amp; Irani, 1993)</ref>.</p><p>The LexRank feature we used in our fourth submission is a measure of sentence salience originally proposed for multi-document summarization <ref type="bibr" coords="2,270.49,244.73,90.81,12.01" target="#b0">(Erkan &amp; Radev, 2004)</ref>. To compute LexRank, we construct an undirected graph where each node is a sentence. We define an edge between any two sentences if the cosine similarity between the sentences is above a pre-defined threshold. Edge weights are normalized so that the sum of the outgoing edges of a node is always equal to 1. Since the matrix representation of the graph is stochastic, we can consider this graph as a discrete Markov chain. The LexRank score of a sentence is the corresponding value in the stationary distribution of this Markov chain. Intuitively, the LexRank score of a sentence will be high if it is similar to many other sentences and the sentences that it is similar to have also high LexRank scores. We set the cosine similarity threshold to 0.1 while constructing the similarity graph to compute LexRank scores in Task 1.</p><p>We implemented a very simple idea for finding the new sentences. For each run, we considered the set of relevant sentences we found as the correct set. Then for each "relevant" sentence, we computed the cosine similarity between the sentence and all of the sentences that precede this sentence. We marked a sentence as "new" if the maximum cosine value found for that sentence exceeded a predefined threshold.</p><p>Table <ref type="table" coords="2,129.09,400.13,5.03,12.01" target="#tab_0">1</ref> shows the features and the novelty cosine threshold used in each run. We also include the average scores for all of our submissions in the table. The motivation behind using our topic-independent features Centroid and LexRank was to eliminate the sentences in the irrelevant documents that were introduced in this year's document sets. Centroid and LexRank try to extract the most salient information in a document cluster so that irrelevant sentences in a noisy cluster are usually eliminated provided that the noise in the cluster is small. However, we found out that the number of irrelevant documents in some clusters are so large (even exceeding the number of relevant documents in some cases) that they change and dominate the general topic of the cluster. Since we did not make any other effort to eliminate the irrelevant documents in Task 1, our results were severely affected by this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Relevance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task 2</head><p>To find the new sentences, we used LexRank to model the dissimilarity of a sentence to the former sentences. We formed the cosine similarity graph of the sentences like we did in the summarization or the relevant sentences domain with the difference that a sentence was allowed to "vote" for only the sentences that appeared later in the cluster. Figure <ref type="figure" coords="3,223.65,96.77,5.03,12.01" target="#fig_1">1</ref> shows a sample similarity graph constructed for detecting new sentences. Sentence 1 in the figure is similar to sentences 2 and 4, sentence 2 is similar to 3, etc.  To assess the new information in each sentence, we ran LexRank on this directed graph. Ideally, the sentences with low LexRank scores should contain new information since they have no or few incoming arcs, i.e. they are dissimilar to almost all of the former sentences. Note that the first sentence cannot have any incoming arcs, thus it will always have a low LexRank value.</p><p>We used a simple decision list algorithm to predict the novelty of each sentence based solely on its LexRank value. The system was trained on Novelty 2003 data. While constructing the similarity graphs, we used a different cosine threshold in each submission to define the similarity between two sentences. Unlike in the summarization domain, higher thresholds are better in this case since we try to model the "information subsumption". Low cosine similarities (values between 0 and 0.5) say that two sentences share some common information, but are not enough to conclude that one sentence subsumes the information in the other. Table <ref type="table" coords="3,516.42,352.73,5.03,12.01" target="#tab_1">2</ref> shows the cosine threshold used and the average scores we got in each submission. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Task 3</head><p>In Task 3, we tried to eliminate the irrelevant documents first, and then perform relevant sentence extraction.</p><p>To eliminate the irrelevant documents, we implemented two different algorithms, both of which made use of the relevant sentences in the first 5 documents that were provided for Task 3. In the first method, we computed the word overlap between each document and the relevant sentences in the first 5 documents, then took 25 documents that had the highest overlap score. In the second method, we computed LexRank values for the sentences in each document by appending the relevant sentences to the document. We took 25 documents that had the highest average LexRank score. These document filtering methods performed badly so that we got lower scores compared to Task 1.</p><p>To find the new sentences, we used the algorithm in Task 2. Since our starting point was the set of relevant sentences we extracted, the scores for new sentences were affected by the low accuracy for the relevant sentences. Table <ref type="table" coords="4,191.87,65.57,5.03,12.01" target="#tab_2">3</ref> shows the features we used for extracting relevant sentences, the cosine threshold used in building the directed graph for detecting new sentences, and the scores for each submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Relevance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Task 4</head><p>For Task 4, we applied the same methods we used in Task 2. The only difference was that we used the relevant sentences in the first 5 documents provided for Task 4 as our training data, instead of using the 2003 data as we did in Task 2. However, this did not give us any improvement over Task 2 as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Although we performed significantly better than last year, the results are not still satisfactory. This year, we proposed a novel method, LexRank, for detecting new sentences and got our most promising results in Task 2.</p><p>Considering the simplistic approaches we have followed, this gives us a motivation for future improvements by integrating more advanced methods into our graph-based model. Our long-term benefit from studying novelty detection is to understand how the information in a set documents is organized across sentences and use this knowledge in other natural language processing problems, especially in text summarization and question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,162.96,221.21,285.87,12.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sample cosine similarity graph for detecting new sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,96.96,519.29,419.57,107.40"><head>Table 1 :</head><label>1</label><figDesc>Average precision, recall and F scores for Task 1.</figDesc><table coords="2,96.96,519.29,419.57,85.69"><row><cell>No.</cell><cell>Features / Novelty Threshold</cell><cell cols="3">Relevant sentences Prec. Recall F</cell><cell cols="3">New sentences Prec. Recall</cell><cell>F</cell></row><row><cell>1</cell><cell>Centroid,Length,QueryTitleCosine / 0.7</cell><cell>0.30</cell><cell>0.83</cell><cell cols="2">0.408 0.14</cell><cell>0.78</cell><cell>0.219</cell></row><row><cell>2</cell><cell>Centroid,Length,QueryTitleCosine / 0.9</cell><cell>0.30</cell><cell>0.83</cell><cell cols="2">0.408 0.14</cell><cell>0.80</cell><cell>0.216</cell></row><row><cell>3</cell><cell cols="2">Centroid,QueryDescriptionWordOverlap / 0.7 0.29</cell><cell>0.86</cell><cell cols="2">0.399 0.13</cell><cell>0.80</cell><cell>0.210</cell></row><row><cell>4</cell><cell cols="2">LexRank,QueryDescriptionWordOverlap / 0.7 0.27</cell><cell>0.84</cell><cell cols="2">0.374 0.12</cell><cell>0.79</cell><cell>0.200</cell></row><row><cell>5</cell><cell>Length,QueryTitleWordOverlap / 0.7</cell><cell>0.27</cell><cell>0.89</cell><cell cols="2">0.386 0.13</cell><cell>0.85</cell><cell>0.208</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,190.20,387.53,231.44,107.52"><head>Table 2 :</head><label>2</label><figDesc>Average precision, recall and F scores for Task 2.</figDesc><table coords="3,220.56,387.53,172.05,85.81"><row><cell cols="5">Cosine No. Threshold Prec. Recall New sentences</cell><cell>F</cell></row><row><cell>1</cell><cell>0.6</cell><cell>0.45</cell><cell>0.85</cell><cell cols="2">0.551</cell></row><row><cell>2</cell><cell>0.7</cell><cell>0.45</cell><cell>0.93</cell><cell cols="2">0.594</cell></row><row><cell>3</cell><cell>0.9</cell><cell>0.43</cell><cell>0.90</cell><cell cols="2">0.553</cell></row><row><cell>4</cell><cell>0.5</cell><cell>0.42</cell><cell>0.70</cell><cell cols="2">0.492</cell></row><row><cell>5</cell><cell>0.8</cell><cell>0.43</cell><cell>0.88</cell><cell cols="2">0.554</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,97.92,100.49,416.92,107.41"><head>Table 3 :</head><label>3</label><figDesc>Average precision, recall and F scores for Task 3.</figDesc><table coords="4,97.92,100.49,416.92,85.69"><row><cell>No.</cell><cell>Features / Novelty Threshold</cell><cell cols="3">Relevant sentences Prec. Recall F</cell><cell cols="3">New sentences Prec. Recall</cell><cell>F</cell></row><row><cell>1</cell><cell cols="2">Length,FQueryDescriptionWordOverlap / 0.7 0.36</cell><cell>0.50</cell><cell cols="2">0.373 0.14</cell><cell>0.42</cell><cell>0.182</cell></row><row><cell>2</cell><cell>Length,QueryDescriptionCosine / 0.6</cell><cell>0.32</cell><cell>0.60</cell><cell cols="2">0.358 0.14</cell><cell>0.51</cell><cell>0.185</cell></row><row><cell>3</cell><cell cols="2">Length,FQueryDescriptionWordOverlap/ 0.5 0.36</cell><cell>0.50</cell><cell cols="2">0.373 0.14</cell><cell>0.41</cell><cell>0.182</cell></row><row><cell>4</cell><cell cols="2">Length,FQueryDescriptionWordOverlap/ 0.6 0.36</cell><cell>0.47</cell><cell cols="2">0.369 0.14</cell><cell>0.38</cell><cell>0.181</cell></row><row><cell>5</cell><cell>Length,QueryDescriptionCosine / 0.6</cell><cell>0.32</cell><cell>0.59</cell><cell cols="2">0.367 0.14</cell><cell>0.52</cell><cell>0.193</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,219.60,282.17,261.24,96.25"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table coords="4,219.60,305.09,173.01,73.32"><row><cell cols="5">Run No. Threshold Prec. Recall Cosine New sentences</cell><cell>F</cell></row><row><cell>1</cell><cell>0.5</cell><cell>0.39</cell><cell>0.89</cell><cell cols="2">0.513</cell></row><row><cell>2</cell><cell>0.6</cell><cell>0.38</cell><cell>0.92</cell><cell cols="2">0.519</cell></row><row><cell>3</cell><cell>0.8</cell><cell>0.39</cell><cell>0.92</cell><cell cols="2">0.521</cell></row><row><cell>4</cell><cell>0.4</cell><cell>0.39</cell><cell>0.93</cell><cell cols="2">0.525</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,190.20,388.13,231.44,12.00"><head>Table 4 :</head><label>4</label><figDesc>Average precision, recall and F scores for Task 4.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,104.99,646.48,139.14,8.64"><p>http://maxent.sourceforge.net</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,90.00,572.57,431.59,12.00;4,114.96,584.57,406.66,12.00;4,114.96,596.57,108.47,12.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,236.32,572.57,247.40,12.00">Lexpagerank: Prestige in multi-document text summarization</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,204.73,584.57,117.98,11.90">Proceedings of EMNLP 2004</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</editor>
		<meeting>EMNLP 2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,90.00,616.49,43.73,12.00" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,136.14,616.49,385.35,12.00;4,114.96,628.37,121.53,12.00" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,242.31,616.49,279.17,12.00;4,114.96,628.37,63.67,12.00">Multi-interval discretization of continuous-valued attributes for classification learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename></persName>
		</author>
		<idno>IJCAI-93</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.00,41.69,431.69,12.01;5,114.96,53.69,350.59,12.01" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,314.21,41.69,207.48,12.01;5,114.96,53.69,82.45,12.01">Experiments in single and multi-document summarization using MEAD</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,218.05,53.69,172.35,11.90">First Document Understanding Conference</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.00,73.61,431.68,12.00;5,114.96,85.61,406.81,12.00;5,114.96,97.49,83.31,12.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,302.58,73.61,219.10,12.00;5,114.96,85.61,241.66,12.00">Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budzikowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,374.67,85.61,147.10,11.90;5,114.96,97.49,31.04,11.90">ANLP/NAACL Workshop on Summarization</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
