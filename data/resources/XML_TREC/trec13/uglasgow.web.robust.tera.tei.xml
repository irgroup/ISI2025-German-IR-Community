<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.21,160.70,259.67,20.74;1,89.26,182.61,421.38,20.74">University of Glasgow at TREC2004: Experiments in Web, Robust and Terabyte tracks with Terrier</title>
				<funder ref="#_gWGvVuV">
					<orgName type="full">UK Engineering and Physical Sciences Research Council</orgName>
					<orgName type="abbreviated">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.61,216.37,89.81,14.41"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.44,216.37,33.49,14.41"><forename type="first">Ben</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.79,216.37,52.72,14.41"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.21,160.70,259.67,20.74;1,89.26,182.61,421.38,20.74">University of Glasgow at TREC2004: Experiments in Web, Robust and Terabyte tracks with Terrier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FEB3EDC56177837F67DED57847111229</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With our participation in TREC2004, we test Terrier, a modular and scalable Information Retrieval framework, in three tracks. For the mixed query task of the Web track, we employ a decision mechanism for selecting appropriate retrieval approaches on a per-query basis. For the robust track, in order to cope with the poorlyperforming queries, we use two pre-retrieval performance predictors and a weighting function recommender mechanism. We also test a new training approach for the automatic tuning of the term frequency normalisation parameters. In the Terabyte track, we employ a distributed version of Terrier and test the effectiveness of techniques, such as using the anchor text, query expansion and selecting an optimal weighting model for each query. Overall, in all three tracks we participated, Terrier and the tested Divergence From Randomness models were shown to be stable and effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With our participation in TREC2004, we test our Information Retrieval (IR) framework, Terrier, in a variety of different settings. Terrier is a modular and scalable framework, for the rapid development of large-scale IR applications. It provides indexing and retrieval functionalities, as well as a number of parameter-free weighting models, based on the Divergence From Randomness (DFR) framework <ref type="bibr" coords="1,355.13,496.17,10.57,12.00" target="#b1">[2]</ref>. Terrier stands for TErabyte RetrIEveR, and further information can be found at http://ir.dcs.gla.ac.uk/terrier.</p><p>We have submitted official runs to three tracks of TREC2004. For the Web track, we test the selective application of different retrieval approaches on a per-query basis. In the Robust track, we employ two novel pre-retrieval performance predictors in a weighting function recommender mechanism, in order to use the optimal weighting functions/models for the poorly-performing queries. We also refine the automatic tuning of the term frequency normalisation parameters, by creating samples of queries, instead of using relevance information. In both the Web and Robust tracks, we use a centralised version of Terrier. For the Terabyte track, we use Terrier in a distributed setting, in order to handle the test collection .GOV2, and evaluate retrieval techniques, which have been effective in the context of previous ad-hoc and Web retrieval TREC tasks. In all three tracks we have participated, Terrier performed extremely well and the tested DFR models were shown to be effective in different settings.</p><p>The remainder of the paper is organised as follows. Section 2 contains a description of the Terrier framework. In Section 3, we describe our approach for the mixed query task of the Web track. Section 4 presents our experiments for the Robust track. In Section 5, we describe our participation in the Terabyte track, and we close with some concluding remarks in Section 6.</p><p>Among its various features, Terrier offers a number of DFR-based models for document weighting, as well as classical models, such as BM25 <ref type="bibr" coords="2,207.23,159.78,16.59,12.00" target="#b18">[19]</ref> and tf-idf, and recent models, such as Ponte-Croft's language model <ref type="bibr" coords="2,506.04,159.78,15.25,12.00" target="#b16">[17]</ref>. Terrier also provides DFR-based and classical term weighting models for query expansion. The relevance score of a document d for a particular query Q is given by: score</p><formula xml:id="formula_0" coords="2,250.25,203.85,275.28,22.54">(d, Q) = t∈Q qtf n • w(t, d)<label>(1)</label></formula><p>where w(t, d) is the weight of the document d for a query term t and qtf n is the normalised frequency of term t in the query. It is given by qtf /qtf max , where qtf is the original frequency of term t in the query, and qtf max is the maximum qtf of all the composing terms of the query. In Table <ref type="table" coords="2,350.74,257.03,3.73,12.01">1</ref>, we provide the formulas for the different models w(t, d) we have used in our experiments for TREC2004. Table <ref type="table" coords="2,131.23,448.19,3.87,12.01">1</ref>: Terrier DFR-based document weighting models used in the experiments for TREC2004.</p><formula xml:id="formula_1" coords="2,84.41,291.06,427.14,111.70">Model Formula BB2 w(t, d) = F +1 N t •(tf n+1) -log 2 (N -1) -log 2 (e) + f (N + F -1, N + F -tf n -2) -f (F, F -tf n) BL2 w(t, d) = 1 tf n+1 -log 2 (N -1) -log 2 (e) + f (N + F -1, N + F -tf n -2) -f (F, F -tf n) PB2 w(t, d) = F +1 N t •(tf n+1) tf n • log 2 tf n λ + (λ + 1 12•tf n -tf n) • log 2 e + 0.5 • log 2 (2π • tf n) PL2 w(t, d) = 1 tf n+1 tf n • log 2 tf n λ + (λ + 1 12•tf n -tf n) • log 2 e + 0.5 • log 2 (2π • tf n) I(n)B2 w(t, d) = F +1 N t •(tf n+1) tf n • log 2 N +1 N t +0.5 I(n)L2 w(t, d) = 1 tf n+1 tf n • log 2 N +1 N t +0.5 I(F)B2 w(t, d) = F +1 N t •(tf n+1) tf n • log 2 N +1 F +0.5 I(F)L2 w(t, d) = 1 tf n+1 tf n • log 2 N +1<label>F</label></formula><p>The notation in Table <ref type="table" coords="2,177.44,470.40,4.97,12.01">1</ref> is explained below:</p><p>• tf is the within-document frequency of term t in document d.</p><p>• F is the term frequency of term t in the whole collection.</p><p>• N is the number of documents in the collection.</p><p>• N t is the document frequency of term t.</p><p>• n e is given by N</p><formula xml:id="formula_2" coords="2,169.57,537.91,77.34,14.07">• 1 -(1 -Nt N ) F .</formula><p>• λ is given by F N and F N . • The relation f is given by the Stirling formula:</p><formula xml:id="formula_3" coords="2,206.14,580.01,319.39,23.53">f (n, m) = (m + 0.5) • log 2 n m + (n -m) • log 2 n<label>(2)</label></formula><p>• tf n is the normalised term frequency. It is given by the normalisation 2:</p><formula xml:id="formula_4" coords="2,249.95,625.29,271.72,23.53">tf n = tf • log 2 1 + c • avg l l (<label>3</label></formula><formula xml:id="formula_5" coords="2,521.66,630.29,3.87,12.01">)</formula><p>where c is a parameter, l is the document length, which corresponds to the number of tokens in a document, and avg l is the average document length in the collection.</p><p>• tf n e is the normalised term frequency, which is given by the modified version of the normalisation 2:</p><formula xml:id="formula_6" coords="3,247.91,144.86,277.63,23.53">tf n e = tf • log e 1 + c • avg l l<label>(4)</label></formula><p>The only free parameter of the DFR framework is the term frequency normalisation parameter c from Equations (3) and ( <ref type="formula" coords="3,130.56,187.44,3.52,12.01" target="#formula_6">4</ref>). The tuning of such a parameter is a crucial issue in IR, because it has an important impact on the retrieval performance <ref type="bibr" coords="3,162.65,199.40,10.78,12.00" target="#b6">[7,</ref><ref type="bibr" coords="3,175.87,199.40,7.19,12.00" target="#b1">2]</ref>. A classical tuning method is the pivoted normalisation <ref type="bibr" coords="3,408.36,199.40,15.26,12.00" target="#b20">[21]</ref>, which fits the document length distribution to the length distribution of relevant documents. However, since the document length distribution is collection-dependent, the pivoted normalisation suffers from the collection-dependency problem. Indeed, the optimal parameter settings of diverse document collections are different <ref type="bibr" coords="3,377.68,235.26,10.57,12.00" target="#b6">[7]</ref>.</p><p>In our experiments with Terrier, the parameter c is automatically tuned, according to a method proposed by He and Ounis <ref type="bibr" coords="3,133.74,259.17,15.26,12.00" target="#b11">[12]</ref>. This method assumes a constant optimal normalisation effect with respect to the document length distribution of the collection, and it assigns the parameter value such that it gives this constant. Thus, it is a collection-independent approach. The proposed method in <ref type="bibr" coords="3,309.70,283.08,16.59,12.00" target="#b11">[12]</ref> uses real queries and the corresponding relevance information for training. Moreover, this tuning method can also be applied to Okapi's BM25 <ref type="bibr" coords="3,445.72,295.04,15.26,12.00" target="#b13">[14]</ref>.</p><p>Terrier provides various DFR-based models for query expansion, based on extracting the most informative terms from a set of top-ranked documents. In Table <ref type="table" coords="3,288.89,318.94,3.73,12.01">2</ref>, we present the term weighting models w(t) used in our experiments for TREC2004.</p><formula xml:id="formula_7" coords="3,204.69,353.02,190.99,64.88">Model Formula KL w(t) = Px • log 2 Px Pc Bo1 w(t) = tfx • log 2 1+Pn Pn + log 2 (1 + Pn) Bo2 w(t) = tfx • log 2 1+P f P f + log 2 (1 + P f ) CS w(t) = lx • D + 0.5 • log 2 (π • lx • 1-tfx tokenc )</formula><p>Table <ref type="table" coords="3,141.75,426.62,3.87,12.01">2</ref>: Terrier DFR-based term weighting models used in the experiments for TREC2004.</p><p>The notation in Table <ref type="table" coords="3,177.44,449.55,3.73,12.01">2</ref>, is explained below:</p><p>• l x is the sum of the length of the exp doc top-ranked documents, and exp doc is a parameter of the query expansion methodology. • tf x is the frequency of the query term in the top-ranked documents.</p><p>• token c is the total number of tokens in the whole collection.</p><p>• P n is given by F N , where F is the term frequency of the query term in the whole collection and N is the number of documents in the whole collection.</p><p>• P f is given by tfx•lx tokenc . • D is given by:</p><formula xml:id="formula_8" coords="3,246.56,567.92,278.97,23.96">P x • log 2 P x P c + P x • log 2 1 -P x 1 -P c<label>(5)</label></formula><p>where P x = tf x /l x and P c = F tokenc . The normalised query term frequency qtf n of an expanded query term is given by:</p><formula xml:id="formula_9" coords="3,264.74,633.25,260.79,23.96">qtf n = w(t) w max (t)<label>(6)</label></formula><p>where w(t) is the weight of term t and w max (t) is the maximum w(t) of the expanded query terms.</p><p>Our experiments for the Web track of TREC2004 continue the evaluation of a decision mechanism for the dynamic application of appropriate retrieval approaches on a per-query basis. We use Terrier, a modular Information Retrieval framework and its associated DFR-based weighting models, as described in Section 2.</p><p>We have submitted runs for the mixed query task of the Web track. In this task, there are 225 topics, which can be either topic distillation, named page finding, or homepage finding topics. The queries are created from the title of each topic. However, the system is not aware of the actual type of each query, during retrieval. This task is more similar to the operational setting of a Web search engine, which receives user queries without explicit evidence of the query type. Our aim is to use a decision mechanism for selecting an appropriate retrieval approach for each query, based on evidence from the hyperlink structure and the anchor text of the set of retrieved documents. More specifically, the decision mechanism is focused on identifying when to favour the entry points or homepages of relevant web sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decision Mechanism</head><p>The decision mechanism we use employs two characteristics of the set of retrieved documents, in order to select an appropriate retrieval approach for each query.</p><p>The first characteristic is the usefulness of the hyperlink structure, which estimates whether there are nonrandom patterns of hyperlinks within the set of retrieved documents <ref type="bibr" coords="4,356.35,349.69,15.26,12.00" target="#b15">[16]</ref>. If we detect such patterns, then we assume that there are clusters of documents about the query topic. Therefore, it is preferred to favour the entry points, or the central nodes of these clusters.</p><p>We define the usefulness of the hyperlink structure as the symmetric Jensen-Shannon divergence between two different score distributions. The first one is the content analysis score distribution S = {s i }, where s i is the content analysis score of the document d i from the set of retrieved documents D. In order to reduce the computational overhead, we consider only the set D k of the top k ranked documents, according to the distribution {s i }. We define the second distribution U = {u i }, so as to favour the relevant documents that point to other relevant documents in D:</p><formula xml:id="formula_10" coords="4,220.63,457.47,158.83,22.42">u i = s i + di→dj s j , d i ∈ D k , d j ∈ D</formula><p>where d i → d j denotes that there is a hyperlink from document d i to document d j . We normalise both distributions S and U , so that di∈D k s i = di∈D k u i = 1 and obtain the distributions S n = {sn i } and U n = {un i }, respectively. The usefulness of the hyperlink structure is defined as the symmetric Jensen-Shannon divergence L(S n , U n ) between S n and U n , as follows:</p><formula xml:id="formula_11" coords="4,161.56,534.73,363.97,28.23">L(S n , U n ) = di∈D k un i log 2 un i uni 2 + sni 2 + di∈D k sn i log 2 sn i uni 2 + sni 2<label>(7)</label></formula><p>The second characteristic of the set of retrieved documents is a novel estimate of the number of potential homepages with all the query terms in the anchor text of their incoming hyperlinks. We assume that if the user submits a query, where all the terms appear in the anchor text of hyperlinks pointing to a homepage of a web site, then it is more useful to favour the homepage as the entry point for the site.</p><p>The set of potential homepages H corresponds to the documents with root, subroot, or path URL types, as defined by Westerveld et al. <ref type="bibr" coords="4,188.17,627.68,15.25,12.00" target="#b22">[23]</ref>. If we denote the anchor text terms of a document d i by a i , and the set of query terms by q, then the number ph anchor of potential homepages with all the query terms in anchor text is defined as follows:</p><formula xml:id="formula_12" coords="4,214.45,663.54,311.07,12.12">ph anchor = |{d i |d i ∈ (D ∩ H) ∧ q ⊆ a i }|<label>(8)</label></formula><p>ph anchor ≤ t ph ph anchor &gt; t ph L(Sn, Un) ≤ tL case I (do not favour entry points) case III (low confidence) L(Sn, Un) &gt; tL case II (low confidence) case IV (favour entry points)</p><p>Table <ref type="table" coords="5,136.25,167.82,3.87,12.01">3</ref>: The decision mechanism that selects an appropriate retrieval approach for each query.</p><p>Our decision mechanism employs L(S n , U n ) and ph anchor , as shown in Table <ref type="table" coords="5,411.22,199.86,3.73,12.01">3</ref>. More specifically, if both L(S n , U n ) and ph anchor are lower or equal to the thresholds t L and t ph respectively (case I), then we assume that the query is specific and we do not favour the entry points or homepages of web sites. On the other hand, if both L(S n , U n ) and ph anchor are higher than the thresholds t L and t ph respectively (case IV), then we assume that it is more useful to favour the entry points or homepages of web sites, from the set of retrieved documents. For the two other cases, we cannot say with confidence whether we should favour the entry points from the set of retrieved documents. In these cases, we will decide about which approach to use, as described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of experiments and results</head><p>We have submitted five official runs for the mixed query task. For all submitted runs, we have indexed the .GOV test collection by removing standard stop-words and applying Porter's stemming algorithm. For the content analysis, we have used the weighting model PL2, as described in Section 2 and Table <ref type="table" coords="5,431.34,342.70,3.73,12.01">1</ref>. The term frequency normalisation parameter was automatically set equal to c=1.28, using the approach described in Section 2.</p><p>We have used two different retrieval approaches. For the first one (CA), we extend the documents by adding the anchor text of their incoming hyperlinks, and perform content analysis with PL2. For the second approach (CAU150), we re-rank the top 150 documents retrieved with CA, using the score:</p><formula xml:id="formula_13" coords="5,220.97,411.79,304.56,23.96">score i = s i × 1 log 2 (urlpath len i + 1)<label>(9)</label></formula><p>where s i is the score assigned to document d i by the approach CA, and urlpath len i is the length in characters of the URL path of d i .</p><p>For both retrieval approaches CA and CAU150, the content analysis scores of documents are increased by a given percentage if the query terms appear either in the anchor text, or in the title of the documents. The percentage of the increase was set experimentally, using training data from the TREC2003 topic distillation and named page finding topics <ref type="bibr" coords="5,131.49,503.95,10.57,12.00" target="#b7">[8]</ref>. More specifically, if we apply CA and a query term t appears in the anchor text or in the title of a document, then we increase the term's weight in the document's score by 8% or 7%, respectively. If we apply CAU150 and a query term t appears in the anchor text of a document, then we increase the weight of t in the document's score by 20%.</p><p>The evaluation results of our official submitted runs for all topics, as well as for each type of topics, are shown in Table <ref type="table" coords="5,111.58,563.72,3.73,12.01" target="#tab_0">4</ref>. The evaluation measures are the mean average precision (MAP), success at 1 retrieved document (Suc@1), success at 5 retrieved documents (Suc@5) and success at 10 retrieved documents (Suc@10). For the named page finding and homepage finding topics, average precision is equivalent to the reciprocal rank of the first relevant retrieved document, provided that there is one relevant document for the topic. The bold entries in Table <ref type="table" coords="5,520.11,599.58,4.97,12.01" target="#tab_0">4</ref> correspond to the run which resulted in the highest value of the respective evaluation measure.</p><p>The first two runs, uogWebCA and uogWebCAU150, correspond to our baselines, where we apply CA or CAU150 for all queries, respectively. With respect to MAP from Table <ref type="table" coords="5,364.87,635.45,3.73,12.01" target="#tab_0">4</ref>, CA is more effective for named page finding queries, while CAU150 is more effective for topic distillation queries. Their performance is similar for homepage finding queries, while CA is more effective than CAU150 over all queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run MAP</head><p>Suc@1 Suc@5 Suc@10 MAP Suc@1 Suc@5 Suc@ Table <ref type="table" coords="6,202.60,335.18,3.87,12.01">5</ref>: The decision mechanism used in run uogWebSelAn.</p><p>For the next three runs, we use the decision mechanism, where the thresholds are set after training with the TREC2003 topic distillation and known item topics. More specifically, in the third run, uogWebSelAn, we use only ph anchor , as shown in Table <ref type="table" coords="6,212.06,391.13,3.73,12.01">5</ref>, and apply CAU150 when there are more than t ph = 1 potential homepages with all the query terms in the anchor text, otherwise we apply CA. From Table <ref type="table" coords="6,386.70,403.08,3.73,12.01" target="#tab_0">4</ref>, we can see that this run results in the highest MAP, and success at 1 and 5 retrieved documents, over all queries. Moreover, it performs similarly to the baselines for the topic distillation and named page finding queries, while it outperforms both CA and CAU150 for the homepage finding queries.</p><p>The fourth run, uogWebSelL, is based on a decision mechanism that employs the usefulness of the hyperlink structure L(S n , U n ), computed from the top k = 150 retrieved documents (Table <ref type="table" coords="6,401.98,462.86,3.59,12.01" target="#tab_1">6</ref>). If L(S n , U n ) is higher than the threshold t L = 0.26, then we apply CAU150, otherwise we apply CA. Considering MAP from Table <ref type="table" coords="6,487.96,474.81,3.73,12.01" target="#tab_0">4</ref>, we can see that this approach works well for the topic distillation and the homepage finding topics, but it is not equally effective for the named page finding topics. If we consider all queries, the run uogWebSelL performs similarly to the baseline uogWebCAU150.</p><p>For the fifth run, uogWebSelAnL, we select an appropriate retrieval approach based on both ph anchor and L(S n , U n ), as shown in Table <ref type="table" coords="6,198.17,534.59,3.73,12.01" target="#tab_2">7</ref>. More specifically, we apply CAU150 if L(S n , U n ) &gt; 0.26 and ph anchor &gt; 1, otherwise we apply CA. This run performs as well as the best one, uogWebSelAn, with respect to MAP from Table <ref type="table" coords="6,99.21,558.51,3.73,12.01" target="#tab_0">4</ref>. In addition, it is the most effective for the homepage finding topics and equally effective as applying CA uniformly for named page finding topics.</p><p>Overall, we can see from the results in Table <ref type="table" coords="6,272.36,582.42,4.97,12.01" target="#tab_0">4</ref> that the selective application of different retrieval approaches is more effective than the uniform application of one retrieval approach for all queries. The decision mechanism L(Sn, Un) ≤ 0.26 L(Sn, Un) &gt; 0.26 apply CA apply CAU150  that employs ph anchor is the most effective over all queries. In addition, the decision mechanism that employs both ph anchor and L(S n , U n ) performs similarly well. Moreover, it is the most effective approach for both named page and homepage finding queries. In both cases, the textual information from the anchor text is an important source of evidence for selecting an appropriate retrieval approach on a per-query basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Robust Track</head><p>In our participation in the Robust Track, we aim to test a series of techniques, including two novel pre-retrieval query performance predictors, a refined weighting function recommender (WFR) mechanism and an enhanced term frequency normalisation parameter tuning method. In the remainder of this section, we introduce these techniques in Sections 4.1, 4.2 and 4.3, respectively. We also provide the experimental setting in Section 4.4 and describe our runs in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-retrieval Query Performance Predictors</head><p>For the query performance prediction, we follow a pre-retrieval approach, where the prediction does not involve the use of relevance scores. We applied two newly proposed predictors, namely the average inverse collection term frequency (AvICTF) and the standard deviation of idf (σ idf ). Unlike the state-of-the-art predictors, such as clarity score <ref type="bibr" coords="7,125.94,422.51,11.61,12.00" target="#b8">[9]</ref> and query difficulty <ref type="bibr" coords="7,220.18,422.51,10.57,12.00" target="#b2">[3]</ref>, the computation of the proposed pre-retrieval predictors does not involve the use of relevance scores. As a consequence, the cost of computing these predictors is marginal. The two applied predictors are the following:</p><p>• Average inverse collection term frequency (AvICTF). Intuitively, the performance of a query can be reflected by the average quality of its composing terms. To represent the quality of a query term, instead of idf , we apply Kwok's inverse collection term frequency (ICTF). In <ref type="bibr" coords="7,368.29,492.25,15.26,12.00" target="#b14">[15]</ref>, Kwok suggested that ICTF can be a good replacement for idf which indicates the quality of a query term t. In our work, we use the average of the ICTF values of the composing query terms to infer the overall quality/performance of a query:</p><formula xml:id="formula_14" coords="7,204.35,542.23,321.18,26.55">AvICT F = log 2 t∈Q ICT F ql = log 2 t∈Q tokenc F ql<label>(10)</label></formula><p>In the above formula, F is the number of occurrences of a query term in the whole collection and token c is the number of tokens in the whole collection. ql is the number of tokens in a given query Q.</p><p>• Standard deviation of idf (σ idf ). This predictor is defined as the standard deviation of the idf of the composing query terms, where idf is given by the INQUERY's idf formula <ref type="bibr" coords="7,403.31,620.73,10.78,12.00" target="#b0">[1]</ref>:</p><formula xml:id="formula_15" coords="7,260.84,646.87,264.69,23.62">idf = log 2 (N + 0.5)/N t log 2 (N + 1)<label>(11)</label></formula><p>where N t is the number of documents in which the query term t appears and N is the number of documents in the whole collection.</p><p>The assumption behind this predictor is that the composing terms of a poorly-performing query tend to have similar idf values. This indicates that idf fails to differentiate the informative query terms from the non-informative ones, resulting in poor performance.</p><p>According to our work in <ref type="bibr" coords="8,200.38,193.20,15.26,12.00" target="#b12">[13]</ref>, σ idf has significant linear and Spearman's correlations with average precision on the collection used in this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weighting Function Recommender Mechanism</head><p>The weighting function recommender (WFR) mechanism refines our last year's model selection mechanism <ref type="bibr" coords="8,506.03,252.30,15.25,12.00" target="#b10">[11]</ref>.</p><p>The idea of WFR is to cope with the poorly-performing queries by recommending the optimal weighting functions, including document weighting and term weighting (query expansion) functions, from a set of candidate weighting functions on a per-query basis. The mechanism follows the steps listed below:</p><p>1. Using a specific clustering algorithm, cluster a set of training queries into k clusters. The clustering process is based on the above two proposed query performance predictors, i.e. AvICTF and σ idf .</p><p>2. Associate the optimal document weighting and term weighting functions to each cluster of training queries by relevance assessment (in this track, we use all the 11 document weighting functions and the 4 term weighting functions, listed in Tables <ref type="table" coords="8,246.57,365.48,4.97,12.01">1</ref> and<ref type="table" coords="8,270.90,365.48,3.73,12.01">2</ref>, as the candidate weighting functions).</p><p>3. For a given new query, allocate the closest cluster to the query, and apply the associated optimal weighting functions of the allocated cluster.</p><p>For the query clustering, we adopt the CURE algorithm <ref type="bibr" coords="8,314.20,418.90,15.25,12.00" target="#b9">[10]</ref>. In the CURE algorithm, initially, each vector is an independent cluster. The similarity between two clusters is measured by the cosine similarity of the two closest vectors (having the highest cosine similarity), where the two vectors come from each cluster respectively. If we have n vectors to be processed, we start with n clusters. Then, we merge the closest pair of clusters (according to the cosine similarity measure) as a single cluster. The merging process is repeated until it results in k clusters.</p><p>Here the number k of clusters is the halting criterion of the algorithm. For the scaling of the elements in a vector, we simply divide each element by the maximum value of the dimension to which the element belongs:</p><formula xml:id="formula_16" coords="8,274.66,512.13,250.86,23.96">E s = E E max<label>(12)</label></formula><p>where E s is the scaled value for a given element E. E max is the maximum value of all the elements in the dimension that E belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Term Frequency Normalisation Parameter Tuning</head><p>As mentioned in Section 2, the term frequency normalisation parameter tuning method proposed in <ref type="bibr" coords="8,469.05,598.18,16.59,12.00" target="#b11">[12]</ref> uses a set of real queries as training queries. In our participation in this year's TREC, these training queries were obtained using a novel query simulation method that follows the steps listed below:</p><p>1. Randomly choose a seed-term from the vocabulary.</p><p>2. Rank the documents containing the seed-term using a specific document weighting function.</p><p>3. Extract the exp term -1 most informative terms from the exp doc top-ranked documents using a specific term weighting/query expansion function. exp term is the required number of composing terms of the generated query. exp doc is a parameter of the applied query expansion methodology, as described in Section 2.</p><p>4. To avoid selecting a junk term as the seed-term, we consider the most informative one of the extracted terms in step 3 as the new seed-term. Note that the original seed-term is discarded at this stage.</p><p>5. Repeat steps 2 and 3 to extract the exp term -1 most informative terms from the exp doc top-ranked documents, which are ranked according to the new seed-term.</p><p>6. The sampled query consists of the new seed-term and the exp term -1 terms extracted in Step 5.</p><p>Adopting the above query simulation method, our tuning method does not involve the use of real queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Setting</head><p>In this track, there are 249 test topics in total. More specifically, there are 200 old topics used in last year's Robust Track and 49 new topics. Also, from the 200 old topics, 50 poorly-performing topics are chosen as the hard topics.</p><p>In our submitted runs, we experimented with three types of queries with respect to the use of different topic fields. The three types of queries are:</p><p>• Short queries: Only the title field is used.</p><p>• Normal queries: Only the description field is used.</p><p>• Long queries: All the three fields (title, description and narrative) are used.</p><p>All the applied document weighting and term weighting (query expansion) functions were chosen from the DFR models introduced in Section 2.</p><p>For the weighting function recommender (WFR) mechanism, all the 11 DFR document weighting functions and the 4 DFR term weighting functions, listed in Tables <ref type="table" coords="9,303.22,457.69,4.97,12.01">1</ref> and<ref type="table" coords="9,327.55,457.69,3.73,12.01">2</ref>, are used as the candidate weighting functions.</p><p>For the query simulation of our term frequency normalisation parameter tuning method described in Section 4.3, we applied PL2 and Bo1 weighting functions. We simulated 200 queries to sample the document length distribution of the collection. Using our automatic tuning method, the obtained parameter settings are c = 5.90 for short queries, c = 1.61 for normal queries and c = 1.73 for long queries.</p><p>In all our experiments, automatic stop-word removal and Porter's stemming algorithm were applied. Query expansion was applied in all our experiments. Using a given term weighting model, we extract the 40 most informative terms from the 10 top-ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Description of Experiments</head><p>In the Robust Track, we submitted 10 runs. For each type of queries, the baseline corresponds to the DFR document weighting and term weighting functions from Tables <ref type="table" coords="9,330.97,600.53,4.97,12.01">1</ref> and<ref type="table" coords="9,355.95,600.53,3.73,12.01">2</ref>, respectively, that resulted in the highest mean average precision for the 200 old queries. Among the submitted runs (see Table <ref type="table" coords="9,431.97,612.48,4.97,12.01" target="#tab_3">8</ref> for run ids and more details):</p><p>• We submitted three runs for short queries. AvICTF is applied in all these runs for query performance prediction. uogRobSBase is the baseline for short queries runs. The applied document weighting and term weighting functions are PL2 and Bo1, respectively. Compared to this baseline, uogRobSWR5 and uogRobSWR10 aim to test the weighting function recommender (WFR) mechanism. The threshold setting of WFR, i.e. the number of clusters, is set to 5 for uogRobSWR5 and 10 for uogRobSWR10.</p><p>• Our experiments for normal queries are similar. uogRobDBase is the baseline, and WFR is applied in uogRobDWR5 and uogRobDWR10 with the use of different threshold settings (i.e. 5 and 10 respectively). However, I(n)L2 and CS are chosen as the baseline weighting models. AvICTF and σ idf are applied in uogRobDWR10 and the other two, respectively.</p><p>• For long queries, besides of WFR, our term frequency normalisation parameter tuning method is also tested.</p><p>According to our study in <ref type="bibr" coords="10,203.03,261.06,15.26,12.00" target="#b11">[12]</ref>, this method outperforms the default setting for normal and long queries, and provides comparable performance with the default setting for short queries. We compare the tuning method to the use of a default setting that is applied in uogRobLBase. Note that the tuning method is applied in all the runs except this baseline. uogRobLBase uses PL2 and Bo1, respectively. The use of the tuning method differs uogRobLT from uogRobLBase. The other two runs, uogRobLWR5 and uogRobLWR10, are again proposed to evaluate WFR.</p><p>Furthermore, in order to better evaluate our predictors, besides of the Kendall's tau, we measure the Spearman's correlation of the predictors with average precision. Moreover, since we apply query expansion in all our submitted runs, we also measure the above two correlation measures without query expansion, in order to check how query expansion affects the effectiveness of our predictors. The applied setting of parameter c for run uogRobLBase, i.e. c = 1, is the default setting. WFR stands for the weighting function recommender mechanism.</p><p>Tables 9, 10 and 11 summarise the experiment results for short, normal and long queries, respectively. Also, Tables <ref type="table" coords="10,103.70,644.80,9.94,12.01">12</ref> and<ref type="table" coords="10,134.26,644.80,9.94,12.01" target="#tab_7">13</ref> provide the obtained Kendall's tau and Spearman's correlation of our predictors with average precision, with and without query expansion, respectively. From the results, we have the following observations: Table <ref type="table" coords="11,212.70,561.73,8.47,12.01">10</ref>: Results of the official runs for normal queries.</p><p>• In general, WFR achieves higher mean average precision (MAP) than the baselines for the old queries, including the hard queries, but not for the new queries. We suggest that this is due to the used scaling formula (see Equation ( <ref type="formula" coords="11,160.28,617.68,7.46,12.01" target="#formula_16">12</ref>)), which simply divides a given element by the maximum value in the dimension that the element belongs to. This formula implies that 0 is meaningful and does not take the actual distribution of the values (i.e. AvICTF and σ idf values). Therefore, we use the following alternate formula for the data scaling: Table <ref type="table" coords="12,98.76,569.31,8.47,12.01">12</ref>: The Kendall's tau (τ ) and the Spearman's correlation (ρ) of the applied predictors with average precision for the official runs in the Robust track. Query expansion is applied in all these runs.</p><formula xml:id="formula_17" coords="12,269.69,621.66,251.69,23.96">E s = E -E min E max -E min (<label>13</label></formula><formula xml:id="formula_18" coords="12,521.38,626.67,4.14,12.01">)</formula><p>where E s is the scaled value for a given element E. E max and E min are the maximum and minimum of all the elements in the dimension that E belongs to, respectively.  We refine our WFR mechanism by using the above formula for the data scaling and run additional experiments. Table <ref type="table" coords="13,160.03,351.79,9.94,12.01" target="#tab_9">14</ref> compares the obtained results with results obtained by the original WFR. The applied threshold setting for both the original and refined WFR mechanisms is k = 5. Since the original WFR with k = 5 results in consistently higher mean average precision than the original WFR with k = 10, we only apply the refined WFR for k = 5. As we can see from the table, the refined WFR clearly outperforms the original WFR and achieves comparable performance with the baselines. Indeed, by applying Equation ( <ref type="formula" coords="13,509.86,399.62,7.63,12.01" target="#formula_17">13</ref>), we improved the WFR mechanism. We have also tried using the Ward algorithm <ref type="bibr" coords="13,422.55,411.57,16.58,12.00" target="#b21">[22]</ref> and the Support Vector Regression <ref type="bibr" coords="13,160.24,423.52,11.61,12.00" target="#b5">[6]</ref> instead of the CURE algorithm. Results show that these two algorithms do not improve the WFR mechanism in terms of average precision.</p><p>• For the new queries, it is interesting to see that WFR leads to higher pre@10, but lower MAP than the baselines, when we use normal and long queries.</p><p>• Our term frequency normalisation parameter tuning method outperforms the baseline in the experiments for long queries. Compared with the baseline, i.e. uogRobLBase, uogRobLT achieves 5.30% of improvement for the new queries, and 2.36% of improvement for all the 249 queries (see Table <ref type="table" coords="13,424.83,511.20,7.88,12.01" target="#tab_5">11</ref>).</p><p>• According to the results in Tables <ref type="table" coords="13,236.94,531.13,9.94,12.01">12</ref> and<ref type="table" coords="13,266.54,531.13,8.29,12.01" target="#tab_7">13</ref>, query expansion has no significant effect on the performance prediction, except when Spearman's correlation measure is used for long queries. Our predictors work better for title and normal queries than for long queries. Moreover, in general, Spearman's correlation measures are significantly higher than Kendall's tau measures. We suggest that it is due to the fact that the Kendall's tau compares only the ranking of a list of given values, while the Spearman's correlation takes the actual values into consideration.</p><p>To summarise, for the performance prediction, the computation of our pre-retrieval predictors does not involve the use of relevance scores, and the correlations of the predictors with average precision is higher for the short and normal queries than for long queries. Moreover, we have improved the WFR mechanism by changing the data scaling formula, as shown in the additional experiments. Finally, our automatic term frequency normalisation tuning method outperforms the empirical setting and optimises the retrieval performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Terabyte Track</head><p>In the Terabyte track, we use Terrier in a distributed setting, inspired by our simulation study in <ref type="bibr" coords="14,462.33,268.51,10.57,12.00" target="#b3">[4]</ref>. We test the effectiveness of techniques such as the use of anchor text, query expansion, and the automatic parameter tuning of term frequency normalisation, for an ad-hoc retrieval task and the .GOV2 test collection. Moreover, we use a selection mechanism, which allocates the optimal document ranking and query expansion models on a per-query basis. In the remainder of this section, we describe the indexing process and our retrieval experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Indexing</head><p>In order to index the .GOV2 test collection, we employ a local inverted file approach <ref type="bibr" coords="14,413.87,363.53,15.25,12.00" target="#b17">[18]</ref>. We split the collection in a number of disjoint sets of documents and index them separately. While indexing, we remove standard stopwords and apply the first step of Porter's stemming algorithm. For each disjoint set of documents, we create the following data structures:</p><p>• a direct file that contains all the terms of each document. The direct file is used for the query expansion models, given in Table <ref type="table" coords="14,192.27,431.27,3.73,12.01">2</ref>.</p><p>• an inverted file that contains all the document identifiers, in which a term appears.</p><p>• a lexicon that contains the vocabulary of the indexed documents.</p><p>• a document index that contains information about the indexed documents.</p><p>The direct and inverted files are compressed using gamma encoding for the differences of term and document identifiers respectively, and unary encoding for the within-document and within-collection frequencies. The sizes of the data structures on disk are shown in Table <ref type="table" coords="14,270.81,516.95,8.29,12.01" target="#tab_11">15</ref>. Although we index the full text of all documents, the use of compression results in great savings of disk space. More specifically, when we index the content of documents only, the total size of the data structures on disk is 17.48GB, which corresponds to 4.10% of the collection size. In the same index, the total size of the inverted files is 7.77GB, or 1.82% of the collection size. In order to apply query expansion efficiently, we also build a global lexicon for the whole collection, the size of which is 0.60GB.</p><p>Using the same indexing approach, we index the collection a second time, after adding to the documents the anchor text of the incoming hyperlinks. We have added the anchor text from 361,379,741 hyperlinks, without using the information about duplicate documents, or redirects between documents. From Table <ref type="table" coords="14,464.82,600.63,8.29,12.01" target="#tab_11">15</ref>, we can see that the total size of the data structures on disk is 18.29GB, or 4.29% of the collection size, while the total size of the inverted files only is 8.47GB (1.99% of the collection size).</p><p>For indexing the collection, we used one AMD Athlon 1600 processor, running at 1.4GHz and one Intel Xeon processor, running at 2.8GHz. The total cumulative CPU time required for indexing the content of documents, and the content and anchor text of documents, was 12,037 minutes and 30,104 minutes, respectively. The indexing  time corresponds to the time required to build both the direct and the inverted files, even though the inverted file is sufficient for performing retrieval and the direct file is only used for applying query expansion. Note that the indexing time can be improved by using more processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Description of Experiments</head><p>For our experiments in the ad-hoc retrieval task of the Terabyte track, we have used a distributed version of Terrier.</p><p>In this system, a central broker receives the queries and submits them to several independent query servers. The query servers assign scores to documents and send the partial lists of results back to the broker. The broker collects all the partial lists of results and merges them in order to create a final ranked list of retrieved documents. The scores of documents are computed using global statistics, collected by the broker from the query servers. Therefore, the results of our distributed retrieval system are equivalent to the results we would obtain if we used Terrier in a centralised setting.</p><p>We have tested both short and long queries. The short queries were created from the title field of the topics (T), while the long queries were created from all fields of the topics (TDN).</p><p>In Table <ref type="table" coords="15,125.93,569.22,8.29,12.01" target="#tab_12">16</ref>, we present an overview and the evaluation results of our official submitted runs. We report the values of mean average precision (MAP), precision at 10 retrieved documents (pre@10) and bpref. For all five runs, the only parameter of the system, related to the term frequency normalisation, was automatically set to c = 15.34 for short queries and c = 2.16 for long queries, using the approach described in Section 2, with the sampling of queries described in Section 4.3.</p><p>Our first run, uogTBBaseS is a content-only baseline, where we employ short queries and assign scores to documents using the weighting model PL2 from the DFR framework, as described in Section 2 and Table <ref type="table" coords="15,500.33,640.95,3.73,12.01">1</ref>. For the second run, uogTBBaseL, we use the weighting model PL2 with long queries. In the third run, uogTBQEL, we employ query expansion. More specifically, we expand the original query by adding the 20 most informative terms from the 5 top-ranked documents, using the term weighting model Bo1 from Table <ref type="table" coords="16,416.38,137.51,3.73,12.01">2</ref>. Both runs uogTBBaseL and uogTBQEL, where we employ long queries, significantly improve the retrieval effectiveness over the baseline uogTBBaseS, where we employ short queries. The run with the highest MAP and bpref is uogTBQEL. In addition, the run uogTBBaseL, where we use long queries, achieves the highest pre@10.</p><p>In the fourth run, uogTBAnchS, we extend documents by adding the anchor text of their incoming hyperlinks, and use short queries for retrieval with PL2. Although anchor text has been shown to be very effective for topic distillation and known-item finding tasks (see Section 3), it does not affect the retrieval effectiveness for this ad-hoc task. For the last run, uogTBPoolQEL, we used a simple pooling technique to select the appropriate weighting models on a per-query basis. We consider 8 document weighting models from Table <ref type="table" coords="16,469.81,233.16,4.97,12.01">1</ref> (i.e. all the weighting models apart from BB2, PB2 and I(F)B2), and the 4 term weighting models from Table <ref type="table" coords="16,481.92,245.11,3.73,12.01">2</ref>, in order to create the pool. Thus, we have 8 × 4 = 32 pairs of document weighting and term weighting models. For a given query, we create a pool, which contains documents retrieved among the top 15 ranks by at least 28 pairs of models. Then, we apply the weighting models that retrieve most of the documents in the pool. Compared to the baseline uogTBBaseL, the run uogTBPoolQEL does not improve retrieval effectiveness, and further investigation is needed in order to refine it.</p><p>Regarding the hardware setting, for all official submitted runs, we used 4 machines, with 8 processors and 6GB of memory in total. The configuration of the machines is the following:</p><p>• one machine with 2GB of memory and 4 Intel Xeon processors at 2.8GHz.</p><p>• one machine with 2GB of memory and 2 AMD Athlon processors at 1.4GHz.</p><p>• two machines with 1GB of memory and one Intel Pentium 4 at 2.4GHz.</p><p>All the data structures were saved on a 1.6TB RAID disk, mounted on the first machine. The time to retrieve the top 20 documents for each of the five runs is shown in Table <ref type="table" coords="16,325.99,408.50,8.29,12.01" target="#tab_12">16</ref>. It should be stressed that a better throughput could be achieved by using more query servers, as suggested in <ref type="bibr" coords="16,329.14,420.45,10.57,12.00" target="#b3">[4]</ref>.</p><p>Overall, we have found that the retrieval approaches that generally work effectively for ad-hoc retrieval from smaller collections, are still very effective for ad-hoc retrieval with the .GOV2 collection. All our official submitted runs are significantly more effective than the median. In addition, applying query expansion with long queries achieves the highest mean average precision, while our baseline that uses long queries performs similarly well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Additional Experiments</head><p>In addition to the official submitted runs, we have performed several unofficial experiments, in order to evaluate the effectiveness of our tuning method for the term frequency normalisation parameter. We also investigate different settings for applying query expansion.</p><p>In Table <ref type="table" coords="16,125.40,551.34,8.29,12.01" target="#tab_13">17</ref>, we present the results for different values of the parameter c for the DFR weighting model PL2. We can see that the tuning method performs very well for long queries. The estimated value c = 2.16 is very close to the manually best-obtained setting c = 2. For short queries, the tuning method estimates the value c = 15.34, while the manually best-obtained setting is c = 5. This results in a difference of 4% in mean average precision.</p><p>We have also conducted experiments using Okapi's BM25 <ref type="bibr" coords="16,327.51,599.16,16.58,12.00" target="#b18">[19]</ref> with different settings for its term frequency normalisation parameter b, including the setting obtained using our tuning approach described in Section 2. The other parameters, k 1 and k 3 , are set by default to 1.2 and 1000, respectively <ref type="bibr" coords="16,382.59,623.07,15.25,12.00" target="#b18">[19]</ref>. As we can see from Table <ref type="table" coords="16,512.77,623.07,8.29,12.01" target="#tab_3">18</ref>, the performance obtained using our tuning method is very close to the performance of the manually best-obtained setting. Table <ref type="table" coords="17,99.17,284.72,8.47,12.01" target="#tab_3">18</ref>: Results for short queries, using BM25 and different parameter settings. The value in bold is the highest mean average precision (MAP) and the underlined value is the MAP obtained using our tuning method.</p><p>Instead of Equation ( <ref type="formula" coords="17,175.24,328.72,3.52,12.01" target="#formula_9">6</ref>), we can employ alternate mechanisms to determine the qtf n for an expanded query term. The first one is inspired by Rocchio's relevance feedback <ref type="bibr" coords="17,329.94,340.67,16.59,12.00" target="#b19">[20]</ref> and introduces the Rocchio's β to adjust the qtf n in Equation ( <ref type="formula" coords="17,150.17,352.63,3.87,12.01" target="#formula_9">6</ref>) <ref type="bibr" coords="17,160.39,352.63,10.78,12.00" target="#b1">[2]</ref>:</p><formula xml:id="formula_19" coords="17,258.07,374.79,267.46,23.96">qtf n = β • w(t) w max (t)<label>(14)</label></formula><p>where w(t) is the weight of term t, w max (t) is the maximum w(t) of the expanded query terms and β is a parameter. All our official runs with query expansion can be seen as having β = 1.0. We can also employ a second parameter-free mechanism of Terrier, where the qtf n for the expanded query terms is set automatically according to the distribution of the expanded query terms in the top ranked documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short queries (T)</head><p>exp doc expterm β = 0.1 β = 0.2 β = 0. In Table <ref type="table" coords="17,126.78,614.67,8.29,12.01" target="#tab_14">19</ref>, we present results using different settings for the number of expanded query terms (exp term ) from the exp doc top returned documents, with the query expansion parameter β (see Equation ( <ref type="formula" coords="17,454.76,626.64,7.46,12.01" target="#formula_19">14</ref>)), and with the parameter-free mechanism of Terrier. The applied setting for parameter c is the same as that in our official runs, i.e. c = 15.34 for short queries and c = 2.16 for long queries. As shown by the results, compared with the results obtained without query expansion (see Table <ref type="table" coords="17,257.02,662.50,7.88,12.01" target="#tab_13">17</ref>), the query expansion does improve retrieval performance, if an appropriate setting is applied.</p><p>From our additional experiments, we have seen that the automatic setting of the term frequency normalisation parameter c, without employing relevance assessments, has been particularly effective for long queries. In addition, query expansion is beneficial for both short and long queries, provided that an appropriate setting is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have participated in the Web, the Robust and the Terabyte tracks of TREC2004, using our retrieval system, Terrier, in both a centralised and a distributed setting.</p><p>In our experiments for the Web track, we have used a decision mechanism that identifies the queries for which to favour the entry points of relevant web sites and applies an appropriate retrieval approach. From our results, we can see that using the decision mechanism results in important improvements over the uniform application of one retrieval approach for all queries.</p><p>For the Robust track, we have proposed two novel pre-retrieval performance predictors. We employ these predictors in a weighting function recommender mechanism that selects the optimal weighting function for the poorly-performing queries in an effective way. Furthermore, we have employed a refined approach for automatically setting the value of the term frequency normalisation parameters, without the need of real user queries in the tuning process.</p><p>With our participation in the Terabyte track, we have evaluated the scalability of a distributed version of Terrier in handling very large test collections, such as the .GOV2. We have seen that even with very limited resources, we can use Terrier to index and experiment with .GOV2. Our results show that the retrieval methods that generally work well for ad-hoc retrieval from smaller collections, are still very effective for the larger collection .GOV2. In both our official and additional experiments, we show that using long queries and query expansion is very effective. Moreover, the automatic term frequency normalisation tuning performs very well for retrieval with long queries, without the need of relevance assessments.</p><p>Overall, we have seen that Terrier is a scalable and modular framework, which provides parameter-free baselines and it can be used effectively in a variety of different retrieval settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,236.83,397.05,15.15,5.98;2,84.41,402.17,76.84,10.80;2,176.68,402.30,14.84,5.98;2,165.06,409.29,38.06,6.24;2,208.57,403.73,32.73,8.96;2,241.46,408.21,3.65,5.98;2,252.32,402.30,15.66,5.98;2,248.34,409.29,23.63,5.43;2,84.41,414.46,76.85,10.80;2,173.72,414.59,3.65,5.98;2,165.06,421.58,20.95,5.43;2,191.45,416.02,32.74,8.96;2,224.34,420.51,3.65,5.98;2,235.22,414.59,15.66,5.98;2,231.23,421.58,23.62,5.43;2,84.41,426.42,76.84,10.80;2,178.61,426.55,14.84,5.98;2,165.06,433.54,41.95,6.24;2,212.44,427.98,36.76,8.96;2,249.35,432.47,3.65,5.98;2,260.22,426.55,15.66,5.98;2,256.23,433.54,23.62,5.43"><head>+0. 5 I 5 I 5 I</head><label>555</label><figDesc>(ne)B2 w(t, d) = F +1 N t •(tf n+1) tf n • log 2 N +1 ne+0.(ne)L2 w(t, d) = 1 tf n+1 tf n • log 2 N +1 ne+0.(ne)C2 w(t, d) = F +1 N t •(tf ne+1) tf ne • log 2 N +1 ne+0.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,113.89,124.59,372.40,200.88"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of the official submitted runs to the mixed query task of the Web track.</figDesc><table coords="6,476.07,124.59,10.21,10.80"><row><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,181.29,657.32,237.54,12.00"><head>Table 6 :</head><label>6</label><figDesc>The decision mechanism used in run uogWebSelL.</figDesc><table coords="7,201.57,126.15,197.08,31.95"><row><cell></cell><cell>ph anchor ≤ 1</cell><cell>ph anchor &gt; 1</cell></row><row><cell>L(Sn, Un) ≤ 0.26</cell><cell>apply CA</cell><cell>apply CA</cell></row><row><cell>L(Sn, Un) &gt; 0.26</cell><cell>apply CA</cell><cell>apply CAU150</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,175.20,167.82,249.68,12.00"><head>Table 7 :</head><label>7</label><figDesc>The decision mechanism used in run uogWebSelAnL.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,74.83,401.71,450.28,195.31"><head>Table 8 :</head><label>8</label><figDesc>The submitted runs to the Robust track. Query expansion is applied for all the runs. docW function and termW function stand for the applied document weighting function and term weighting function, respectively.</figDesc><table coords="10,159.23,401.71,281.73,161.64"><row><cell>Run id</cell><cell cols="2">docW function termW function</cell><cell>c</cell><cell>Predictor</cell></row><row><cell></cell><cell></cell><cell>Short Queries</cell><cell></cell><cell></cell></row><row><cell>uogRobSBase</cell><cell>PL2</cell><cell>Bo1</cell><cell>c = 5.90</cell><cell>AvICTF</cell></row><row><cell>uogRobSWR5</cell><cell>WFR</cell><cell>WFR</cell><cell>c = 5.90</cell><cell>AvICTF</cell></row><row><cell>uogRobSWR10</cell><cell>WFR</cell><cell>WFR</cell><cell>c = 5.90</cell><cell>AvICTF</cell></row><row><cell></cell><cell></cell><cell>Normal Queries</cell><cell></cell><cell></cell></row><row><cell>uogRobDBase</cell><cell>I(n)L2</cell><cell>CS</cell><cell>c = 1.61</cell><cell>σ idf</cell></row><row><cell>uogRobDWR5</cell><cell>WFR</cell><cell>WFR</cell><cell>c = 1.61</cell><cell>σ idf</cell></row><row><cell>uogRobDWR10</cell><cell>WFR</cell><cell>WFR</cell><cell>c = 1.61</cell><cell>AvICTF</cell></row><row><cell></cell><cell></cell><cell>Long Queries</cell><cell></cell><cell></cell></row><row><cell>uogRobLBase</cell><cell>PL2</cell><cell>Bo1</cell><cell>c = 1</cell><cell>AvICTF</cell></row><row><cell>uogRobLT</cell><cell>PL2</cell><cell>Bo1</cell><cell>c = 1.73</cell><cell>σ idf</cell></row><row><cell>uogRobLWR5</cell><cell>WFR</cell><cell>WFR</cell><cell>c = 1.73</cell><cell>σ idf</cell></row><row><cell>uogRobLWR10</cell><cell>WFR</cell><cell>WFR</cell><cell>c = 1.73</cell><cell>AvICTF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,194.99,124.59,210.19,427.42"><head>Table 9 :</head><label>9</label><figDesc>Results of the official runs for short queries.</figDesc><table coords="11,195.45,124.59,209.34,427.42"><row><cell>Run id</cell><cell cols="4">pre@10 MAP MAP(X) #norel</cell></row><row><cell></cell><cell cols="2">Old queries</cell><cell></cell><cell></cell></row><row><cell>uogRobSBase</cell><cell>.4400</cell><cell>.2826</cell><cell>.0087</cell><cell>32</cell></row><row><cell>uogRobSWR5</cell><cell>.4455</cell><cell>.2911</cell><cell>.0072</cell><cell>35</cell></row><row><cell>uogRobSWR10</cell><cell>.4605</cell><cell>.2961</cell><cell>.0097</cell><cell>32</cell></row><row><cell></cell><cell cols="2">New queries</cell><cell></cell><cell></cell></row><row><cell>uogRobSBase</cell><cell>.4816</cell><cell>.3482</cell><cell>.0265</cell><cell>7</cell></row><row><cell>uogRobSWR5</cell><cell>.4571</cell><cell>.3272</cell><cell>.0176</cell><cell>8</cell></row><row><cell>uogRobSWR10</cell><cell>.4531</cell><cell>.3216</cell><cell>.0215</cell><cell>6</cell></row><row><cell></cell><cell cols="2">Hard queries</cell><cell></cell><cell></cell></row><row><cell>uogRobSBase</cell><cell>.2640</cell><cell>.1237</cell><cell>.0030</cell><cell>14</cell></row><row><cell>uogRobSWR5</cell><cell>.2780</cell><cell>.1305</cell><cell>.0013</cell><cell>15</cell></row><row><cell>uogRobSWR10</cell><cell>.3160</cell><cell>.1360</cell><cell>.0025</cell><cell>13</cell></row><row><cell></cell><cell cols="2">All queries</cell><cell></cell><cell></cell></row><row><cell>uogRobSBase</cell><cell>.4482</cell><cell>.2955</cell><cell>.0098</cell><cell>39</cell></row><row><cell>uogRobSWR5</cell><cell>.4478</cell><cell>.2982</cell><cell>.0075</cell><cell>43</cell></row><row><cell>uogRobSWR10</cell><cell>.4590</cell><cell>.3011</cell><cell>.0106</cell><cell>38</cell></row><row><cell>Run id</cell><cell cols="4">pre@10 MAP MAP(X) #norel</cell></row><row><cell></cell><cell cols="2">Old queries</cell><cell></cell><cell></cell></row><row><cell>uogRobDBase</cell><cell>.4305</cell><cell>.2732</cell><cell>.0062</cell><cell>38</cell></row><row><cell>uogRobDWR5</cell><cell>.4460</cell><cell>.2822</cell><cell>.0070</cell><cell>31</cell></row><row><cell>uogRobDWR10</cell><cell>.4535</cell><cell>.2861</cell><cell>.0072</cell><cell>32</cell></row><row><cell></cell><cell cols="2">New queries</cell><cell></cell><cell></cell></row><row><cell>uogRobDBase</cell><cell>.5510</cell><cell>.3888</cell><cell>.0259</cell><cell>6</cell></row><row><cell>uogRobDWR5</cell><cell>.5408</cell><cell>.3834</cell><cell>.0234</cell><cell>6</cell></row><row><cell>uogRobDWR10</cell><cell>.5286</cell><cell>.3736</cell><cell>.0227</cell><cell>6</cell></row><row><cell></cell><cell cols="2">Hard queries</cell><cell></cell><cell></cell></row><row><cell>uogRobDBase</cell><cell>.3000</cell><cell>.1230</cell><cell>.0033</cell><cell>15</cell></row><row><cell>uogRobDWR5</cell><cell>.3040</cell><cell>.1328</cell><cell>.0032</cell><cell>10</cell></row><row><cell>uogRobDWR10</cell><cell>.2960</cell><cell>.1308</cell><cell>.0019</cell><cell>14</cell></row><row><cell></cell><cell cols="2">All queries</cell><cell></cell><cell></cell></row><row><cell>uogRobDBase</cell><cell>.4542</cell><cell>.2959</cell><cell>.0070</cell><cell>44</cell></row><row><cell>uogRobDWR5</cell><cell>.4647</cell><cell>.3021</cell><cell>.0079</cell><cell>37</cell></row><row><cell>uogRobDWR10</cell><cell>.4683</cell><cell>.3033</cell><cell>.0083</cell><cell>38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,193.61,124.59,212.95,435.00"><head>Table 11 :</head><label>11</label><figDesc>Results of the official runs for long queries.</figDesc><table coords="12,196.16,124.59,207.95,435.00"><row><cell>Run id</cell><cell cols="5">pre@10 MAP MAP(X) #norel</cell></row><row><cell></cell><cell></cell><cell cols="2">Old queries</cell><cell></cell></row><row><cell>uogRobLBase</cell><cell cols="2">.4715</cell><cell>.2927</cell><cell>.0130</cell><cell>31</cell></row><row><cell>uogRobLT</cell><cell cols="2">.4705</cell><cell>.2970</cell><cell>.0136</cell><cell>31</cell></row><row><cell>uogRobLWR5</cell><cell cols="2">.4800</cell><cell>.3028</cell><cell>.0134</cell><cell>26</cell></row><row><cell>uogRobLWR10</cell><cell cols="2">.4815</cell><cell>.3084</cell><cell>.0133</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell cols="2">New queries</cell><cell></cell></row><row><cell>uogRobLBase</cell><cell cols="2">.4939</cell><cell>.3586</cell><cell>.0325</cell><cell>3</cell></row><row><cell>uogRobLT</cell><cell cols="2">.5000</cell><cell>.3776</cell><cell>.0390</cell><cell>2</cell></row><row><cell>uogRobLWR5</cell><cell cols="2">.5122</cell><cell>.3703</cell><cell>.0388</cell><cell>2</cell></row><row><cell>uogRobLWR10</cell><cell cols="2">.5143</cell><cell>.3679</cell><cell>.0295</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell cols="2">Hard queries</cell><cell></cell></row><row><cell>uogRobLBase</cell><cell cols="2">.3100</cell><cell>.1609</cell><cell>.0150</cell><cell>34</cell></row><row><cell>uogRobLT</cell><cell cols="2">.3240</cell><cell>.1552</cell><cell>.0161</cell><cell>33</cell></row><row><cell>uogRobLWR5</cell><cell cols="2">.3180</cell><cell>.1608</cell><cell>.0158</cell><cell>28</cell></row><row><cell>uogRobLWR10</cell><cell cols="2">.3120</cell><cell>.1571</cell><cell>.0148</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell cols="2">All queries</cell><cell></cell></row><row><cell>uogRobLBase</cell><cell cols="2">.4759</cell><cell>.3056</cell><cell>.0150</cell><cell>34</cell></row><row><cell>uogRobLT</cell><cell cols="2">.4763</cell><cell>.3128</cell><cell>.0161</cell><cell>33</cell></row><row><cell>uogRobLWR5</cell><cell cols="2">.4863</cell><cell>.3161</cell><cell>.0158</cell><cell>28</cell></row><row><cell>uogRobLWR10</cell><cell cols="2">.4880</cell><cell>.3201</cell><cell>.0148</cell><cell>28</cell></row><row><cell>Run id</cell><cell></cell><cell cols="2">Predictor</cell><cell>τ</cell><cell>ρ</cell></row><row><cell></cell><cell></cell><cell cols="2">Short queries</cell><cell></cell></row><row><cell cols="2">uogRobSBase</cell><cell cols="2">AvICTF</cell><cell cols="2">0.259 0.358</cell></row><row><cell cols="2">uogRobSWR5</cell><cell cols="2">AvICTF</cell><cell cols="2">0.257 0.365</cell></row><row><cell cols="2">uogRobSWR10</cell><cell cols="2">AvICTF</cell><cell cols="2">0.270 0.356</cell></row><row><cell></cell><cell cols="3">Normal queries</cell><cell></cell></row><row><cell cols="2">uogRobDBase</cell><cell></cell><cell>σ idf</cell><cell cols="2">0.258 0.317</cell></row><row><cell cols="2">uogRobDWR5</cell><cell></cell><cell>σ idf</cell><cell cols="2">0.259 0.300</cell></row><row><cell cols="2">uogRobDWR10</cell><cell cols="2">AvICTF</cell><cell cols="2">0.240 0.298</cell></row><row><cell></cell><cell></cell><cell cols="2">Long queries</cell><cell></cell></row><row><cell cols="2">uogRobLBase</cell><cell cols="2">AvICTF</cell><cell cols="2">0.163 0.213</cell></row><row><cell cols="2">uogRobLT</cell><cell></cell><cell>σ idf</cell><cell cols="2">0.166 0.194</cell></row><row><cell cols="2">uogRobWR5</cell><cell></cell><cell>σ idf</cell><cell cols="2">0.172 0.200</cell></row><row><cell cols="2">uogRobWR10</cell><cell cols="2">AvICTF</cell><cell cols="2">0.176 0.219</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,74.83,295.94,450.24,23.96"><head>Table 13 :</head><label>13</label><figDesc>The Kendall's tau (τ ) and the Spearman's correlation (ρ) of the applied predictors with average precision for the official runs in the Robust Track but without the use of query expansion.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,74.84,180.37,450.64,47.87"><head>Table 14 :</head><label>14</label><figDesc>The mean average precision for the 49 new queries obtained by the baselines (MAP b ), by the original WFR mechanism (MAP WFR5 ), and by the refined WFR mechanism MAP r5 . The applied threshold setting is k = 5 for both original and refined WFR mechanisms. The MAP b and MAP WFR5 values are taken from Tables 9, 10 and 11.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="15,74.84,202.29,450.25,134.50"><head>Table 15 :</head><label>15</label><figDesc>The total size on disk of the data structures (inverted files, direct files, lexicons and document indexes), with or without anchor text.</figDesc><table coords="15,86.30,235.33,421.74,101.47"><row><cell>Run</cell><cell>Description</cell><cell>Query Type</cell><cell>MAP</cell><cell>pre@10</cell><cell>bpref</cell><cell>Time to retrieve 20 documents</cell></row><row><cell>uogTBBaseS uogTBBaseL uogTBQEL uogTBAnchS</cell><cell>PL2 content retrieval PL2 content retrieval Query expansion PL2 content and anchor text retrieval</cell><cell>T TDN TDN T</cell><cell>0.2709 0.3054 0.3075 0.2690</cell><cell>0.5306 0.6327 0.6163 0.5245</cell><cell>0.3026 0.3356 0.3359 0.3025</cell><cell>4 sec 28 sec 46 sec 3 sec</cell></row><row><cell>uogTBPoolQEL</cell><cell>Weighting model selection</cell><cell>TDN</cell><cell>0.2311</cell><cell>0.5592</cell><cell>0.2589</cell><cell>46 sec</cell></row><row><cell>median</cell><cell></cell><cell></cell><cell>0.1427</cell><cell>0.4102</cell><cell>0.2015</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="15,74.84,346.51,450.26,23.96"><head>Table 16 :</head><label>16</label><figDesc>Description and evaluation results of our official submitted runs to the Terabyte track. For running the experiments, we used 4 machines with 8 processors and 6GB of RAM in total.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="17,74.83,124.59,450.29,150.95"><head>Table 17 :</head><label>17</label><figDesc>Results for short and long queries, using PL2 and different values of the parameter c. The value in bold is the highest mean average precision (MAP) and the underlined value is the MAP obtained using our tuning method.</figDesc><table coords="17,161.96,124.59,276.14,150.95"><row><cell></cell><cell></cell><cell></cell><cell cols="3">Short queries (T)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>c</cell><cell>0.1</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>15.34</cell><cell>16</cell></row><row><cell cols="9">MAP .0978 .1968 .2374 .2659 .2822 .2772 .2709 .2703</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Long queries (TDN)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>c</cell><cell>0.1</cell><cell>0.5</cell><cell>0.8</cell><cell>1</cell><cell>2</cell><cell>2.16</cell><cell>3</cell><cell>4</cell></row><row><cell cols="9">MAP .1390 .2530 .2812 .2903 .3058 .3054 .3017 .2951</cell></row><row><cell>b</cell><cell>0.2</cell><cell>0.34</cell><cell>0.40</cell><cell>0.50</cell><cell>0.65</cell><cell>0.75</cell><cell>0.80</cell><cell>0.90</cell></row><row><cell cols="9">MAP .2660 .2771 .2785 .2764 .2626 .2478 .2362 .2098</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="17,74.83,475.88,450.29,126.90"><head>Table 19 :</head><label>19</label><figDesc>Results using PL2 with Bo1 query expansion model for different settings of Rocchio's β and the parameter-free query expansion of Terrier.</figDesc><table coords="17,339.91,475.88,110.05,10.80"><row><cell>5 β = 1.0 parameter-free</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is funded by a <rs type="funder">UK Engineering and Physical Sciences Research Council (EPSRC)</rs> project grant, number <rs type="grantNumber">GR/R90543/01</rs>. The project funds the development of the Terrier Information Retrieval framework (url: http://ir.dcs.gla.ac.uk/terrier).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gWGvVuV">
					<idno type="grant-number">GR/R90543/01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,91.42,592.11,433.70,12.00;18,91.43,604.06,433.68,12.00;18,91.43,616.02,22.38,12.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,305.18,592.11,144.01,12.00">Recent experiments with INQUERY</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,472.20,592.11,52.91,11.90;18,91.43,604.06,290.73,11.90">NIST Special Publication 500-236: The Fourth Text REtrieval Conference (TREC-4)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="49" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,91.42,635.94,433.68,12.00;18,91.43,647.90,260.19,12.00" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="18,133.13,635.94,339.62,11.90">Probabilistic Models for Information Retrieval based on Divergence from Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="19,91.42,125.56,433.67,12.00;19,91.43,137.51,433.68,12.00;19,91.43,149.47,272.94,12.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="19,260.44,125.56,264.65,12.00;19,91.43,137.51,40.03,12.01">Query Difficulty, Robustness, and Selective Application of Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,151.14,137.51,373.97,11.90;19,91.43,149.47,109.00,11.90">Lecture Notes in Computing Science, Proceedings of the 26th European Conference on Information Retrieval (ECIR&apos;04)</title>
		<meeting><address><addrLine>Sunderland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,91.42,169.39,433.69,12.00;19,91.43,181.35,356.17,12.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,263.40,169.39,261.71,12.00;19,91.43,181.35,100.45,12.00">A case study of distributed information retrieval architectures to index one terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cacheda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,198.92,181.35,155.83,11.90">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>article in press</note>
</biblStruct>

<biblStruct coords="19,91.42,201.28,433.69,12.00;19,91.43,213.23,239.57,12.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,203.20,201.28,160.34,12.00">Query-based sampling of text databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,373.94,201.28,151.17,12.00;19,91.43,213.23,31.50,11.90">In ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="130" />
			<date type="published" when="2001-04">April, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,91.42,233.16,433.67,12.00;19,91.43,245.11,138.75,12.01" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="19,184.39,233.16,245.96,12.00">Training nu-support vector regression: theory and algorithms</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,440.03,233.16,80.51,11.90">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1959" to="1977" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,91.42,265.03,433.66,12.00;19,91.43,276.99,433.67,11.90;19,91.43,288.95,179.40,12.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="19,337.49,265.03,134.19,12.00">Document normalization revisited</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Mccabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,489.07,265.03,36.00,11.90;19,91.43,276.99,433.67,11.90;19,91.43,288.95,34.93,11.90">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="381" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,91.42,308.87,433.73,12.00;19,91.43,320.82,412.05,12.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="19,317.43,308.87,164.56,12.00">Overview of the TREC 2003 Web Track</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,504.70,308.87,20.45,11.90;19,91.43,320.82,327.87,11.90">NIST Special Publication 500-255: The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="78" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,91.42,340.75,433.69,12.00;19,91.43,352.71,433.65,12.00;19,91.43,364.66,137.12,12.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,290.79,340.75,117.84,12.00">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,429.35,340.75,95.75,11.90;19,91.43,352.71,404.53,11.90">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,384.59,428.69,12.00;19,91.43,396.54,289.05,12.00" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,236.46,384.59,234.21,12.00">CURE: an efficient clustering algorithm for large databases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,489.08,384.59,36.00,11.90;19,91.43,396.54,151.31,11.90">Proceedings of the ACM SIGMOD Conference</title>
		<meeting>the ACM SIGMOD Conference<address><addrLine>Seatltle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,416.47,428.71,12.00;19,91.43,428.42,433.65,12.00;19,91.43,440.38,223.24,12.00" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,177.22,416.47,347.89,12.00;19,91.43,428.42,114.39,12.00">University of Glasgow at the robust track -a query-based model selection approach for the poorly-performing topics</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,225.34,428.42,299.75,11.90;19,91.43,440.38,51.55,11.90">NIST Special Publication 500-255: The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="636" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,460.31,428.73,12.00;19,91.43,472.26,433.66,12.00;19,91.43,484.21,48.36,12.00" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,185.82,460.31,254.97,12.00">A study of parameter tuning for term frequency normalization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,464.39,460.31,60.74,11.90;19,91.43,472.26,348.87,11.90">Proceedings of the 12th International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 12th International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,504.14,428.70,12.01;19,91.43,516.10,433.68,12.00;19,91.43,528.05,22.38,12.00" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,179.32,504.14,230.98,12.01">Inferring query performance using pre-retrieval predictors</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,429.81,504.14,95.29,11.90;19,91.43,516.10,302.06,11.90">Proceedings of the 11th Symposium on String Processing and Information Retrieval (SPIRE 2004)</title>
		<meeting>the 11th Symposium on String Processing and Information Retrieval (SPIRE 2004)</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,547.98,428.71,12.00;19,91.43,559.93,433.64,12.00;19,91.43,571.89,145.39,12.00" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,179.56,547.98,233.96,12.00">Term Frequency Normalisation for BM25 and DFR Model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,154.85,559.93,328.38,11.90">Proceedings of the 27th European Conference on Information Retrieval (ECIR&apos;05)</title>
		<title level="s" coord="19,434.06,547.98,91.04,11.90;19,91.43,559.93,56.69,11.90">Lecture Notes in Computing Science</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR&apos;05)<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-03">March, 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,591.81,428.72,12.00;19,91.43,603.77,433.65,12.00;19,91.43,615.72,112.93,12.00" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,155.75,591.81,249.05,12.00">A new method of weighting query terms for ad-hoc retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,427.40,591.81,97.72,11.90;19,91.43,603.77,404.53,11.90">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,96.40,635.64,428.71,12.00;19,91.43,647.61,433.66,11.90;19,91.43,659.56,179.40,12.00" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,214.00,635.64,272.24,12.00">Usefulness of hyperlink structure for query-biased topic distillation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,507.32,635.64,17.79,11.90;19,91.43,647.61,433.66,11.90;19,91.43,659.56,34.93,11.90">Proceedings of the 27th Annual International SIGIR Conference on Research and Developement in Information Retrieval</title>
		<meeting>the 27th Annual International SIGIR Conference on Research and Developement in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,125.56,428.72,12.00;20,91.43,137.51,433.66,11.90;20,91.43,149.47,138.09,12.00" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="20,220.46,125.56,223.18,12.00">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,464.81,125.56,60.31,11.90;20,91.43,137.51,429.78,11.90">Proceedings of the 21st annual international ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,169.39,428.72,12.00;20,91.43,181.35,398.25,12.00" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="20,257.91,169.39,263.69,12.00">Query performance for tightly coupled distributed digital libraries</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,102.21,181.35,243.24,11.90">Proceedings of the third ACM conference on Digital libraries</title>
		<meeting>the third ACM conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,201.28,428.72,12.00;20,91.43,213.23,433.65,12.00;20,91.43,225.18,139.76,12.00" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="20,281.62,201.28,243.50,12.00;20,91.43,213.23,40.91,12.01">Okapi at TREC-7: Automatic Ad hoc, Filtering, VLC and Interactive</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,151.10,213.23,344.01,11.90">NIST Special Publication 500-242: The Seventh Text REtrieval Confereence (TREC-7)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,245.11,428.68,12.01;20,91.43,257.07,390.98,12.00" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="20,144.78,245.11,179.22,12.01">Relevance Feedback in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,342.53,245.11,182.56,11.90;20,91.43,257.07,128.65,11.90">The SMART Retrieval System-Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,276.99,428.70,12.00;20,91.43,288.95,433.65,12.00;20,91.43,300.90,102.98,12.00" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="20,254.35,276.99,155.99,12.00">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,429.95,276.99,95.15,11.90;20,91.43,288.95,404.53,11.90">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,320.82,428.70,12.00;20,91.43,332.79,151.47,12.00" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="20,133.05,320.82,222.59,12.00">Hierarchical Grouping to optimize an objective function</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,363.51,320.82,161.59,11.90;20,91.43,332.79,20.96,11.90">Journal of American Statistical Associationm</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,96.40,352.71,428.71,12.00;20,91.43,364.66,433.65,12.00;20,91.43,376.62,99.95,12.00" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,268.00,352.71,252.60,12.00">Retrieving Web Pages Using Content, Links, URLs and Anchors</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,102.93,364.66,351.32,11.90">NIST Special Publication 500-250: The Tenth Text REtrieval Conference (TREC 2001)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="663" to="672" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
