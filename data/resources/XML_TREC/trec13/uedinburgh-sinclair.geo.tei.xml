<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,235.44,81.14,141.12,12.91">TREC Genomics 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.56,111.33,64.75,10.76"><forename type="first">Gail</forename><surname>Sinclair</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.19,111.33,79.02,10.76"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,235.44,81.14,141.12,12.91">TREC Genomics 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9059D573DB0A9D1587917505C2BE63FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC Genomics track started in 2003 as the first domain specific track of the Text Retrieval Competition. The aim of the track is to develop various IR tasks specific to the biomedical field. One task of the first year involved the retrieval of documents given a specific gene, while the second task required the extraction a brief description of gene function from documents. This year sees a foray into ad hoc retrieval and a curation and categorization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad hoc Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task description</head><p>It was important that the retrieval tasks mirrored the real life current information needs of biologists. Examples of these needs were ascertained via interviews which were then formulated into queries suitable for ad hoc IR. 50 of these queries were then chosen as the test topics. 5 queries and their selected relevant documents were also later given as training examples. As is standard in TREC ad hoc retrieval, each topic has a title, need and context field, e.g. TITLE: DNA repair and oxidative stress NEED: Find correlation between DNA repair pathways and oxidative stress CONTEXT: Researcher is interested in how oxidative stress effects DNA repair.</p><p>The purpose of the task was then to retrieve only those documents relevant to the queries. Since analysing every document in the document set is so resource intensive and the track has a limited timeline, standard practice was used to evaluated based on sampling. The top 100 documents of each run submitted by the track participants were analysed and judged on relevance. For evaluation, each document judged as relevant was counted as a positive instance and each document judged as not relevant plus all those documents not analysed were counted as negative instances. The nature of this evaluation technique means that there is likely to be some positive examples treated as true negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Methods</head><p>Since there was initially no training data for this task, we decided to use an existing dataset from a related domain.</p><p>The MuchMore<ref type="foot" coords="1,398.28,311.10,3.95,7.17" target="#foot_0">1</ref> corpus contains 25 medical queries and their relevance judgments with respect to almost 8,000 abstracts from 41 journals. These queries differed from the track queries in that they only contained the need and had no title or context field. The abstracts are in English but translated from German. MuchMore is a parallel corpus, with abstracts in both English and German. It is often used for cross lingual IR. The relevance judgments are supplied in a format amenable to TREC evaluation. A version of the corpus is annotated with various linguistic information such as part-of-speech, morphology, UMLS semantic classes. However, for our purposes, the plain version without any annotation was used as this was most similar to the test TREC Genomics queries.</p><p>For retrieval, we used the same system as we did in the 2003 track <ref type="bibr" coords="1,391.35,527.73,91.92,9.82" target="#b1">(Osborne et al., 2003)</ref>, the Lucene retrieval engine and expansion of gene names on the queries. This system performed well in the 2003 track, and so it seemed reasonable to use it again.</p><p>Using default Lucene retrieval and the Much-More queries, the baseline evaluation gave us TREC metrics of MAP 27.7% Recall 55.3% Ave Prec at 0.1 recall 64.6% Prec at 10 docs 51.6%</p><p>This year we focused on how we could differently expand and weight the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Expansion</head><p>The UMLS provides a knowledge server<ref type="foot" coords="2,266.88,87.90,3.95,7.17" target="#foot_1">2</ref> that, given a term or phrase, will search the UMLS according to certain criteria, e.g. exact string match, normalised string match.</p><p>Expanding the queries using UMLS-sourced synonyms for each word in the query increased MAP and recall, while decreasing precision at 0.1 and average precision at 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP</head><p>29.3% Recall 65.9% Ave Prec at 0.1 recall 61.7% Prec at 10 docs 49.6%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Weighting</head><p>Our query expansion from last year used a weighting scheme for the different type of gene representations e.g. official symbol was weighted higher than alias product. Since this year involved sentences rather than just different representations of the same entity, a strategy was required to find out which terms should be weighted higher than others.</p><p>We decided first to weight noun phrases higher than the other words in the query -how much so was left to experimentation. Whereas last year it was found that the weight 2.9 was most useful in performance with regards to gene representations, with MuchMore's type of queries, weighting the nouns 7 times more relevant than the rest of the query was found to best increase performance. However using any weighting at all for this subset of terms significantly bettered the default of no weighting.</p><p>It then seemed appropriate to order the query terms in order of their ability to discriminate between documents. For this, we used a term's frequency in the literature (via PubMed) or on the Web (via a Google API). This strategy was used as a follow on from our success in the BioNLP task at Coling 2004 <ref type="bibr" coords="2,107.80,580.41,86.46,9.82" target="#b0">(Finkel et al., 2004)</ref>.</p><p>The Google API<ref type="foot" coords="2,155.64,590.94,3.95,7.17" target="#foot_2">3</ref> was used to find the frequency of each term across the Web. The terms were then weighted according to this frequency with respect to the other terms in the query -the lower the frequency the greater the weight. The highest frequency term received a weighting of 1.0, with the weights being incremented with each next lower frequency term.</p><p>Similarly each term was individually used as a search term in Pubmed and the number of documents retrieved was automatically recorded. This number of documents was then used to weight the terms in a similar way as above.</p><p>Additionally, to incorporate the fact that a term may have a higher relative frequency to another in a PubMed search than in a Google search, the term orderings determined by the two search strategies were merged, with the weights of each term in a query being averaged, e.g if Google gave term X in a query a weighting of 3 and PubMed gave the same term in the query a weighting of 2, the combined weighting was 2.5.</p><p>Using these weighting schemes, the merged weighting scheme achieved slightly better performance compared to the individual schemes, as follow: The combination of the two weighting decisions was done manually and so for simplicity only Google was used for the test data, as it was the best performer when the two were compared. The test data had many more words and so manually combining the weightings would have been time consuming although automating the process would not have been difficult. When synonyms were used, their associated weighting was the same as the original term from which the synonym was obtained. Frequencies of these new terms could have been found independently however this could have given false emphasis to an irrelevant synonym if it happened to be a rarer term than the original.</p><p>The official runs submitted to TREC involved a combination of the techniques described above. One run used the individual terms, noun phrases and synonyms of both the terms and noun phrases which were then weighted with respect to usage frequency (according to Google). The second run also included the use of stemming.</p><p>Although both runs performed similarly overall, the former technique performed significantly better than the second more often on individual queries than vice versa. This would lead us to surmise that stemming can often be a hindrance, echoing our findings in <ref type="bibr" coords="2,363.98,748.65,122.38,9.82" target="#b2">(Sinclair and Webber, 2004)</ref> 3 Categorization Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>The Mouse Genomics (MGI) team currently manually curate new articles for annotation with Gene Ontology (GO) codes. The Gene Ontology consists of 3 separate vocabularies -one for each of biological process, cellular component and molecular function. The MGI first decides whether each new article is relevant to mouse genomics and so possibly amenable to GO annotation, then any relevant GO codes are assigned together with the evidence for that code.</p><p>The categorization task attempts to imitate this process in three parts: 1) triage task -decision on whether each document contains experimental evidence of mouse genomics and can be considered for annotation.</p><p>2) decision on which vocabularies each article could be annotated with 3) the evidence on which the above decision is based.</p><p>Only our efforts for the first triage part of this task was entered into the track competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triage Task 3.2.1 Division of Text</head><p>To further existing work we have been doing with categorization according to GO codes <ref type="bibr" coords="3,237.04,434.61,59.36,9.82;3,72.00,447.21,62.29,9.82" target="#b2">(Sinclair and Webber, 2004)</ref>, for this task we wanted to compare the information content of the different sections of full text articles.</p><p>The documents were initially classified according to whether they were about mice. The same species classifier used in the 2003 track was used for this <ref type="bibr" coords="3,90.60,522.57,92.05,9.82" target="#b1">(Osborne et al., 2003)</ref>. This classification went further than the initial MGI retrieval in that one simple mention of mouse, mice or murine within the article was not sufficient to classify as being 'about mice'. Any documents considered not about mice were then removed from the document set as they were perceived as not curatable according to MGI's curation process.</p><p>The remaining dataset of full text articles was divided up according to sections. A sample set of the SGML of the articles for the three journal publications used in the dataset was studied and a division strategy devised accordingly, so that appropriate sections could be kept together in the groupings Abstract, Introduction, Methods, Results, Conclusions. (Abstract also includes the article titles.) Unfortunately, any articles not formatted in this way were then lost to the categorization task. Any document omitted in this manner is then deemed to have had a "do not curate" decision made upon it. Although these omissions result in a reduced subset of documents that may be curated, it does not take away from the overall intention of our experimentation -i.e. how 'useful' each section is for decision making.</p><p>The MeSH and RN annotations for the articles were also retrieved and combined with the Abstract documents since these are publicly available. This data was merged so that the less publicly available full text sections could be compared for 'usefulness' with what is already widely available and most extensively used in biomedical information retrieval, i.e PubMed annotations.</p><p>In the training set of 5837 documents, 100% of the articles had an Abstract section, 11 documents did not have any distinct Introduction, ca. 100 documents did not have a distinct Results section, ca. 500 documents did not have a distinct Discussion or Conclusions section. However more than half of the training documents did not have a distinct Methods section.</p><p>In the test document set, all section groupings contained approximately 85% of the articles, except the Discussion grouping which contained 79% of the full test set. It is unclear why so may articles in the training set lack a Methods section. This is particularly regrettable since the Methods section seemed to be the most informative vis-a-vis the curation decision.</p><p>These subsets were then searched for indicators of GO code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">GO Code Identification</head><p>All GO codes were extracted from the ontology and synonymous phrases were looked for in the UMLS Knowledge Server used in the retrieval task. The number of instances of each GO term and associated synonyms were recorded for each article. The training triage decisions were then analysed according to these counts to see if there was any trend in the uses of GO terms in the documents. A lower bound was formulated so that if any document did not contain at least that number of GO terms and/or synonyms then the triage decision would be not to curate.</p><p>At this point the ratio of remaining documents deemed not worthy of curation and those already discarded was significantly different to that of the same ratio in the training set. There was much discussion across Track participants about the ratio of curated to not curated documents in the training and test sets. The ratio in both sets was deemed to be not significantly dissimilar. According to this further filtering was considered appropriate. Although our method of species classification from the 2003 track proved to be very useful, it was not ascertained whether it was significantly better than several other teams' classification by MeSH term as each strategy generated different false positives and negatives. To this end (and at the last minute), the documents were further filtered according to whether the documents had Mice as a MeSH heading. This reduced the aforementioned ratio to a number not so significantly different to that of the training data. In hindsight this was probably a mistake, but the decision was panic-driven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Sections</head><p>As can be seen in Table <ref type="table" coords="4,193.60,478.77,4.07,9.82" target="#tab_1">1</ref>, in all 4 metrics used in this task, Introductions proved the least useful, closely followed by the Discussion sections. The Methods sections proved most informative with respect to Precision and F-score, with the Results sections outperforming the rest with respect to Recall and Utility Measure.</p><p>Although the publicly available data performed well, these results show that it is worthwhile to invest resources in analysing full text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,111.91,466.68,196.67"><head>Table 1 :</head><label>1</label><figDesc>Comparison of article sections with respect to the Track metrics, from highest scoring section (leftmost) to lowest.</figDesc><table coords="4,134.28,111.91,343.28,141.92"><row><cell cols="3">Precision Methods Public</cell><cell>Results Discussion Introduction</cell></row><row><cell>Recall</cell><cell>Results</cell><cell cols="2">Public Methods Discussion Introduction</cell></row><row><cell cols="3">F-Score Methods Results</cell><cell>Public Discussion Introduction</cell></row><row><cell>Utility</cell><cell cols="3">Results Methods Public Discussion Introduction</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,331.08,749.98,89.47,8.07"><p>http://muchmore.dfki.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.08,738.94,94.65,8.07"><p>http://umlsks.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,88.08,749.98,103.86,8.07"><p>http://www.google.com/apis/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,72.00,634.77,224.44,9.82;4,82.92,647.37,213.52,9.82;4,82.92,659.85,213.27,9.82;4,82.92,672.45,213.53,9.82;4,82.92,685.05,213.41,9.82;4,82.92,697.53,213.64,9.82;4,82.92,710.13,45.24,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,143.52,659.85,152.67,9.82;4,82.92,672.45,193.11,9.82">Exploiting context for biomedical entity recognition: From syntax to the web</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shipra</forename><surname>Dingare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gail</forename><surname>Sinclair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,82.92,685.05,213.41,9.82;4,82.92,697.53,213.64,9.82;4,82.92,710.13,41.13,9.82">NLPBA/BioNLP 2004,Joint Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,72.00,723.57,224.54,9.82;4,82.92,736.05,213.44,9.82;4,82.92,748.65,212.96,9.82;4,325.92,327.57,213.50,9.82;4,325.92,340.05,168.12,9.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,211.71,748.65,84.16,9.82;4,325.92,327.57,108.86,9.82">Edinburgh-stanford trec-2003 genomics track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cumiskey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smillie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,454.65,327.57,84.77,9.82;4,325.92,340.05,113.99,9.82">Proc. of the Twelfth Text REtrieval Conference</title>
		<meeting>of the Twelfth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,315.00,353.25,224.43,9.82;4,325.92,365.73,213.27,9.82;4,325.92,378.33,214.00,9.82;4,325.92,390.81,213.53,9.82;4,325.92,403.41,192.53,9.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,503.28,353.25,36.15,9.82;4,325.92,365.73,213.27,9.82;4,325.92,378.33,120.22,9.82">Classification from full text: A comparison of canonical sections of scientific papers</title>
		<author>
			<persName coords=""><forename type="first">Gail</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,468.41,378.33,71.51,9.82;4,325.92,390.81,213.53,9.82;4,325.92,403.41,188.12,9.82">NLPBA/BioNLP 2004,Joint Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
