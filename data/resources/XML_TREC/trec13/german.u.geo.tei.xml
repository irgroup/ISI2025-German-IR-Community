<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,77.48,75.51,456.97,16.61;1,119.24,93.99,373.98,16.61;1,278.36,130.71,55.13,16.61">The GUC Goes to TREC 2004: Using Whole or Partial Documents for Retrieval and Classification in the Genomics Track DRAFT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,204.92,162.10,87.31,12.40"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
							<email>kareem.darwish@guc.edu.eg</email>
						</author>
						<author>
							<persName coords="1,317.96,162.10,89.09,12.40"><forename type="first">Amgad</forename><surname>Madkour</surname></persName>
							<email>amgad.madkour@guc.edu.eg</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Technology</orgName>
								<orgName type="institution">The German University</orgName>
								<address>
									<settlement>Cairo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">th District</orgName>
								<address>
									<settlement>New Cairo, Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,77.48,75.51,456.97,16.61;1,119.24,93.99,373.98,16.61;1,278.36,130.71,55.13,16.61">The GUC Goes to TREC 2004: Using Whole or Partial Documents for Retrieval and Classification in the Genomics Track DRAFT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">28CAEB20524EE3DD5E4B2D975205EB03</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We were interested in examining the relative effect of using parts of the documents, different combinations of parts of the documents, or whole documents on retrieval and classification. We were also interested in the effect of MeSH terms on retrieval. Our experiments show that indexing titles, abstracts, and MeSH terms for adhoc retrieval yielded statistically significantly better results than any other part or combination of parts, with abstracts outperforming any other individual part of the documents. In the triage sub-task, using whole documents for training a classifier outperformed using titles, abstracts, diagram captions, MeSH terms, and windows of text around gene names. However, training a classifier using the combination of titles, abstracts, and MeSH terms produced results comparable to using whole documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The overarching theme of our experiments in the adhoc retrieval and classification tasks was the determination of the relative effectiveness of different parts of the documents or a combination of parts on retrieval and classification.</p><p>For the adhoc retrieval task, we examined the effect of inclusion and exclusion of the Medical Subject Headings (MeSH) terms in the indexed documents on retrieval effectiveness and we compared the relative effectiveness of the MeSH terms compared to the title and abstract fields of the MEDLINE citations. MeSH are hierarchically structured controlled vocabulary terms developed by the National Library of Medicine (NLM) to classify, mostly manually, documents such as the MEDLINE citations.</p><p>For the triage sub-task, we were interested in comparing the relative effect of training a classifier using the full text of a document or alternatively selected parts of the documents. The selected parts of the documents that we experimented with included titles, abstracts, MeSH terms, diagram captions, small windows of text surrounding genes and gene products, and combinations of different parts in the document. We also report here on experiments that we performed for the annotation sub-task.</p><p>The rest of the paper will be organized as follows: section 2 provides background on some results reported in the literature, sections 3 and 4 describe the experimental setup and the results of the experiments respectively, and section 5 concludes the paper.</p><p>Considerable amount of research has focused on the effect of manually assigning MeSH terms to documents in IR applications. Srinivasan <ref type="bibr" coords="2,317.00,118.28,14.08,10.80" target="#b7">[8]</ref> examined the effect of the inclusion and exclusion of MeSH terms on information retrieval using a collection produced by Hersh et al, which will be referred to hence forth as the Hersh collection <ref type="bibr" coords="2,363.32,145.88,12.87,10.80" target="#b3">[4]</ref>. The Hersh collection has 75 topics and 2,344 medline citations, which include the titles and abstracts of the articles along with manually assigned MeSH terms. Srinivasan's experiments showed that the inclusion of the MeSH terms in documents statistically significantly improved retrieval effectiveness. She also demonstrated that the inclusion of MeSH terms can be combined with blind relevance feedback and thesaurus-based query expansion to produce even better retrieval effectiveness. She used a statically generated thesaurus for query expansion. Aronson et al examined the effect of including and excluding MeSH terms in documents on retrieval and confirmed the results obtained by Srinivasan's <ref type="bibr" coords="2,193.40,256.28,12.87,10.80" target="#b0">[1]</ref>. The experiments of Aronson et al showed that MeSH terms statistically significantly improved retrieval effectiveness and that using thesaurus-based expansion further improved retrieval effectiveness. Aronson et al used the UMLS Metathesaurus for query expansion <ref type="bibr" coords="2,170.84,297.80,12.87,10.80" target="#b0">[1]</ref>. French et al examined the effect of augmenting user queries with automatically generated MeSH terms. The MeSH terms were selected using the Entry Vocabulary Indexes technique which employs a probabilistic mapping between natural language text and controlled vocabularies <ref type="bibr" coords="2,228.44,339.08,12.87,10.80" target="#b2">[3]</ref>. French reported that augmenting queries with a small number of MeSH terms statistically significantly improved retrieval effectiveness.</p><p>In the 2002 KDD Cup competition, held in conjunction with SIGKDD, the main task focused on identifying articles for FlyBase, a publicly available database on the genetics and molecular biology of Drosophila (fruit flies), containing experimental evidence of gene expression of gene products, which might then be manually curated <ref type="bibr" coords="2,399.80,407.96,12.69,10.80" target="#b4">[5]</ref>. Some of the problems they faced in the task included the casual mention of genes, the mention of mutated gene expressions (as opposed to natural expressions of genes), and the ambiguity between the gene and its transcript <ref type="bibr" coords="2,155.00,449.48,18.34,10.80" target="#b9">[10]</ref>. The ClearForest and Celera, the group reporting the best results for the task, utilized constrained pattern matching of diagram captions to identify which documents contain experimental evidence. Their justification for using diagram captions relies on the fact that curators who manually select papers look mainly at the diagrams in the paper to ascertain the presence of experimental evidence <ref type="bibr" coords="2,241.16,504.68,18.53,10.80" target="#b9">[10]</ref>. In the absence of diagrams from the provided text only documents, diagram captions provide an indication of the content of the diagrams. For the same task, Shi et al. trained a Na√Øve Bayes Classifier based on the distance between a gene name and keywords suggesting experimental evidence <ref type="bibr" coords="2,287.00,545.96,12.87,10.80" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ad-hoc Retrieval Task</head><p>For the ad-hoc retrieval task, we submitted one official run and conducted a group of posthoc runs as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Official Run</head><p>For the official run, we used PSE which is an open-source retrieval engine that uses OKAPI BM-25 weighting formula <ref type="bibr" coords="3,241.16,119.00,12.87,10.80" target="#b1">[2]</ref>. We indexed the title, abstract, and MeSH fields of the documents and removed extraneous fields such as author names and publication dates. Before indexing the documents, we normalized the case of all the tokens and removed all stopwords based on the stopword list used by PubMed, but we employed no stemming. For the queries, we used the title and need fields of the 50 topics. The queries were processed in the same manner as the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Post-hoc Runs</head><p>For the post-hoc retrieval runs, we used the Lemur toolkit exclusively using the default settings of OKAPI BM-25 weighting formula. Lemur is a language modeling and information retrieval toolkit developed jointly between Carnegie Mellon University and University of Massachusetts at Amherst. When using blind relevance feedback in Lemur, we used the default settings in which a query is augmented with the 20 best terms from the top 5 retrieved documents. For the queries, we used the same queries as those from the official run.</p><p>Our initial attempts to index the entire collection in one index all failed for a reason that we are still investigating. Therefore we split the collection into 5 sub-collections and indexed and searched each sub-collection independently. Since the collection was distributed into 5 document files, each document file was used as a sub-collection. After searching each subcollection separately, the ranked lists for each query were combined into a single list and the documents in the list were sorted based on the scores reported by Lemur. Splitting the collection should not affect term frequencies or document length normalization (as the average document length is fairly consistent across sub-documents), but the splitting might affect document frequencies. The splitting is likely to have an effect on blind relevance feedback, because the feedback is done on each sub-collection separately. The effects of splitting need to be further investigated. The runs we performed had two main aims:</p><p>1. We wanted to reexamine the effect of excluding MeSH terms on retrieval effectiveness.</p><p>To do so, we indexed the collection once using the title, abstract, and MeSH fields, which we will refer to as TWM, and one more time with the title and abstract fields only, which we will refer to as TW. 2. We wanted to examine the effect of indexing three different parts of the documents, namely titles (T), abstracts (W), and MeSH terms (M) on retrieval effectiveness. Even though indexing terms from specific parts of the documents to the exclusion of other parts was expected to yield lower retrieval effectiveness than indexing the full documents, examining the effect of different of parts of the documents may indicate which part contributes the most number of valuable terms, which may consequently be used to improve a processes such as blind relevance feedback by skewing term selection. We examined the impact of the titles, the MeSH terms, and the weightiest 20 terms from the abstract field (term weighting was determined using the OKAPI BM-25 formula).</p><p>The title field represents a natural language summary of the document which was generated by the original author; the MeSH field represents a controlled vocabulary representation of the document which was generated by domain professionals; and the most valuable terms are automatically generated summaries. The indices and subsequent runs will be referred to as T, M, and W for the title, abstract, and MeSH term respectively. All in all the collection was indexed 5 times and was searched with and without blind relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Triage Task</head><p>For the triage task we submitted 5 official runs and conducted a series of additional (unofficial) runs. For all the runs we used SVM Light with either a linear or a polynomial kernel, and we trained SVM Light with all the default parameters using all the provided positive and negative training examples. For all the runs, official and unofficial, the only text processing that we performed was case normalization and we performed no stemming or stopword removal. We opted out of performing stemming and stopword removal because experiments we performed on the training set indicated that neither of them improved filtering effectiveness. We augmented the full length documents with manually assigned MeSH terms from PubMed.</p><p>We performed the following runs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.1">Official runs</head><p>We submitted 5 official runs. In four of the runs, SVM Light was trained using the diagram caption from the papers. The use of diagram captions was inspired by their use in the 2002 KDD Cup by ClearForest and Celera <ref type="bibr" coords="4,279.56,406.28,18.53,10.80" target="#b9">[10]</ref>. Based on some preliminary experiments we performed on the training data, results suggested the effectiveness of captions for training. As a baseline run, we used whole documents for training. Listed below are the names of the runs, the thresholds used, and the part of the documents on which SVM light was trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Kernel Threshold Train on GUClin1260 Linear -0.973 Captions GUClin1700 Linear -1.000 Captions GUCply1260</p><p>Polynomial -0.894 Captions GUCply1700</p><p>Polynomial -0.932 Captions GUCwdply2000 Polynomial -0.950 Whole Document</p><p>Some preliminary experiments we performed on the training examples provided some direction to the choice of threshold, but the experiments provided a direction rather than definite threshold choices. Therefore, our choices of threshold were somewhat arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.2">Unofficial runs</head><p>For the unofficial runs, we tried a variety of setups in which we varied two factors, namely:</p><p>1. Which part of the document SVM Light was trained on. We tried whole documents (WD), titles (T), abstracts (W), MeSH terms (M), captions (C), title + abstract (TW), title + abstract + MeSH terms (TWM), title + abstract + MeSH terms + captions (TWMC), a window 5 words preceding and following each mention of a gene or a gene product (W5), and lastly a window of 10 words preceding and following each mention of a gene or a gene product (W10). Genes and their products were recognized using a modified version of YAGI, which is short for Yet Another Gene Identifier <ref type="bibr" coords="5,421.16,129.80,12.87,10.80" target="#b8">[9]</ref>. 2. The SVM Light cut-off thresholds to find the most effective threshold. We varied the value of the threshold between -0.90 and -1.10 with increments of 0.01.</p><p>A polynomial kernel was used for all the unofficial runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Annotation Task</head><p>For the annotation task, we submitted 5 official runs and did not perform any unofficial runs. We followed three paths in performing the runs as follows:</p><p>1. GUCbase: We performed a baseline run in which all (document, gene name) pairs were annotated with all three possible annotations, namely Biological Process (BP), Molecular Function (MF), and Cellular Component (CC). 2. GUCsvm0 and GUCsvm5: For these two runs, we trained SVM Light for each of the three possible annotations using the GO sub-tree entries. The GO sub-tree entries corresponding to one of the annotations were used as positive examples while the entries in the two other GO sub-trees were used as negative examples. We only used the name and definition fields from the GO entries with case normalization and with no stemming or stopword removal. Then, we concatenated all the paragraphs that mention the gene that we wish to classify from the (document, gene name) pair. We used YAGI to identify genes and we used carriage return to detect paragraphs. We then classified all the concatenated paragraphs using SVM Light. If the lumped paragraphs containing the gene name had a score of 0.0 or -5.0 for the GUCsvm0 and GUCsvm5 respectively for a particular classification, then we generated (document, gene name, classification) tuple. 3. GUCir30 and GUCir50: For these two runs, we used Lemur to index all the entries of the GO with only case normalization and no stemming or stopword removal. Again we used the OKAPI BM-25 weighting formula in Lemur. We used the concatenated paragraphs containing the gene from the (document, gene name) pair as queries. Upon searching the GO entries using our queries, we added the sum of the logs of OKAPI BM-25 scores for the top 30 returned documents for each of the BP, MF, and CC classifications separately. For example, if 5 of the returned entries belonged to the BP sub-tree of GO, then the score of BP is just the sum of the logs of the returned scores for these 5 entries. If the score of any of the classifications was greater than a threshold of 30 and 50 for the GUCir30 and GUCir50 runs respectively, then the (document, gene name, classification) was generated. If the score of all 3 different classifications was less than the threshold then only the classification with the highest score was generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ad-hoc Retrieval Task</head><p>The results in mean average precision of the ad-hoc retrieval tasks were as follows:</p><p>Setting For the official run, our average precision was higher than or equal to the median for 45 out of 50 topics and was the highest for 7 topics out 50. Also, the mean average precision for the official run was identical (to the third significant figure) to that of the equivalent TWM Lemur run. This indicates the collection splitting likely had little effect on retrieval effectiveness. However, blind relevance feedback did not improve retrieval effectiveness for any of the Lemur runs, except for the W run, which is inconsistent with results that we locally obtained on the OHSUMED collection and results reported in the literature. This might indicate that splitting the collection had an adverse effect on blind relevance feedback.</p><p>In comparing the TWM and TW runs, the inclusion of the MeSH terms in the documents statistically significantly improved retrieval effectiveness. Statistical significance is indicated if the p value of a paired two tailed t-test is less than 0.05. This result is consistent with previously reported results in the literature <ref type="bibr" coords="6,225.80,455.72,12.84,10.80" target="#b0">[1,</ref><ref type="bibr" coords="6,241.88,455.72,8.56,10.80" target="#b7">8]</ref>.</p><p>In comparing the title, abstract, and MeSH fields, the abstract field contributes the most number of valuable for retrieval. This can be clearly seen from comparing the "T" run to the "TW" run and from comparing the "T" run to the "W" run. In both comparisons, we can see that using abstracts or a summary of the abstracts statistically significantly increased retrieval effectiveness over the use of titles or MeSH terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Triage Task</head><p>The normalized utility measures for all of our official and unofficial runs are as follows: The results show that training the classifier using the full length documents yielded the best results. However, using the title, abstract, and MeSH term fields yields comparable results. This suggests that using only these fields, which are a part of MedLine citations, is as effective for document classification as full length journal articles. Due to the fact that clearing copyright issues often complicates obtaining full length documents, this result is significant. Also, captions did not perform as well as previous research suggested <ref type="bibr" coords="7,337.64,329.00,18.34,10.80" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Annotation Task</head><p>The summary of our runs are as follows: Although runs showed good recall, they generally suffered from poor precision. Further investigation is required to rectify the poor precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The paper examined the relative effect of using different parts, combinations of parts of the documents, or whole documents on retrieval and classification.</p><p>For the adhoc retrieval task, we compared the effect of including and excluding MeSH terms on retrieval effectiveness and showed that the inclusion of MeSH terms statistically significantly improved retrieval effectiveness. Although this result is consistent with the work reported by Srinivasan and Aronson in the literature, contrary to their results blind relevance feedback did not offer any statistically significant improvement in retrieval effectiveness. This could be due to the fact that we split the document collection affecting our document frequencies. From the experiments presented in the paper, indexing a summary of the abstract field of the documents yielded statistically better retrieval effectiveness than the title or MeSH terms fields. Employing different weights to different portions of the documents or skewing term selection in blind relevance feedback can perhaps maximize the effect of more valuable portions of a document and can potentially lead to better retrieval effectiveness.</p><p>For the triage sub-task, we compared the use of titles, abstracts, diagram captions, small windows of text around genes and gene products, and combinations of the different portions to the use of whole documents. The use of the combination of the title, abstract, and MeSH term fields yielded results comparable to the use of whole documents.</p><p>For the annotation sub-task, we described our experimental procedure and reported the results we obtained. Our results generally suffer from poor precision. We need to investigate methods of improving the annotation task.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,94.46,256.28,412.98,10.80;8,107.96,270.20,316.44,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,244.28,256.28,257.70,10.80">Query Expansion Using the UMLS¬Æ Metathesaurus¬Æ</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rindflesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,107.96,270.20,243.03,10.80">Proceedings of AMIA &apos;97 Annual Fall Symposium</title>
		<meeting>AMIA &apos;97 Annual Fall Symposium</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="485" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,297.80,433.18,10.80;8,107.96,311.48,51.00,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,237.32,297.80,188.87,10.80">Probabilistic Structured Query Methods</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,437.96,297.80,56.19,10.80">SIGIR 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="338" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,339.08,430.14,10.80;8,107.96,352.76,401.76,10.80;8,107.96,366.68,422.28,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,340.04,339.08,184.56,10.80;8,107.96,352.76,274.17,10.80">Exploiting a Controlled Vocabulary to Improve Collection Selection and Retrieval Effectiveness</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Perelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,393.32,352.76,116.40,10.80;8,107.96,366.68,329.17,10.80">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,394.28,423.16,10.80;8,107.96,407.96,380.40,10.80" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,335.72,394.28,181.90,10.80;8,107.96,407.96,267.12,10.80">OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research</title>
		<author>
			<persName coords=""><forename type="first">Hersh</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hickam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,435.56,62.22,10.80;8,107.96,449.48,218.62,10.80" xml:id="b4">
	<monogr>
		<ptr target="http://www.biostat.wisc.edu/~craven/kddcup/" />
		<title level="m" coord="8,107.96,435.56,48.72,10.80">KDD Cup</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,477.08,435.66,10.80;8,107.96,490.76,408.74,10.80;8,107.96,504.68,24.00,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,133.40,490.76,349.77,10.80">A Machine Learning Approach for the Curation of Biomedical Literature</title>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">S</forename><surname>Edwin Rakesh Menon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lixiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><forename type="middle">Y K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hang</forename><forename type="middle">Tong</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,491.00,490.76,25.70,10.80">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,532.28,417.54,10.80;8,107.96,545.96,27.00,10.80" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hull</surname></persName>
		</author>
		<title level="m" coord="8,221.00,532.28,198.75,10.80">The TREC-9 Filtering Track Final Report</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="25" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,573.56,409.02,10.80;8,107.96,587.48,246.36,10.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,174.44,573.56,261.79,10.80">Optimal document-indexing vocabulary for MEDLINE</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,446.36,573.56,57.12,10.80;8,107.96,587.48,137.74,10.80">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="514" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,94.46,615.08,267.10,10.80;8,107.96,628.76,237.82,10.80" xml:id="b8">
	<monogr>
		<ptr target="http://www.cs.wisc.edu/~bsettles/abner/yagi.html" />
		<title level="m" coord="8,107.96,615.08,246.94,10.80">YAGI Tool ( Yet Another Gene Identification Tool</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.96,656.36,420.80,10.80;8,107.96,670.28,181.44,10.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,293.72,656.36,227.04,10.80;8,107.96,670.28,120.88,10.80">ClearForest and Celera, Information Extraction from Biomedical Articles</title>
		<author>
			<persName coords=""><forename type="first">Yizhar</forename><surname>Regev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,236.60,670.28,25.70,10.80">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
