<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.94,99.82,209.19,12.36">TREC NOVELTY TRACK AT IRIT -SIG</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.42,120.16,66.04,11.19"><forename type="first">Taoufiq</forename><surname>Dkaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ISYCOM/GRIMM</orgName>
								<orgName type="institution">Université Toulouse le Mirail</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.76,120.16,65.16,11.19"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Institut Universitaire de Formation des Maîtres Midi-Pyrénées</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.94,99.82,209.19,12.36">TREC NOVELTY TRACK AT IRIT -SIG</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FA64216E594793B41CECAF23A76804E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2004, IRIT modified important features of the strategy that was developed for TREC 2003. Changes include tuning parameter values, topic expansion and exploitation of sentences context.</p><p>According to our method, a sentence is considered as relevant if it matches the topic with a certain level of coverage. This coverage depends on the category of the terms used in the texts. Four types of terms have been defined highly relevant, scarcely relevant, non-relevant (like stop words), highly non-relevant terms (negative terms). Term categorization is based on topic analysis: highly non-relevant terms are extracted from the narrative parts that describe what will be a non-relevant document. The three other types of terms are extracted from the rest of the query. Each term of a topic is weighted according to both its occurrence and the topic part it belongs to (title, descriptive, narrative). Additionally we increase the score of a sentence when either the previous or the next sentence is relevant. When topic expansion is applied, terms from relevant sentences (task 3) or from the first retrieved sentences (task 1) are added to the initial terms.</p><p>With regard to the novelty part, a sentence is considered as novel if its similarity with each of previously processed -and selected as novel-sentences does not exceed a certain threshold. In addition, this sentence should not be too similar to a virtual sentence made of the n bestmatching and previously selected sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>«The TREC novelty track is designed to investigate systems' abilities to locate relevant and new information within the ranked set of documents retrieved in answer to a TREC topic » <ref type="bibr" coords="1,49.68,526.48,59.96,11.19">[trec.nist.gov]</ref>.</p><p>Retrieving relevant texts is traditionally based on computing a similarity between the representations of the information need (or topic) and the texts. This general statement has been applied to full documents as well as chunks of texts (passage retrieval). Intuitively, the same idea can be applied when sentences retrieval is involved. In TREC 2002 IRIT developed a new strategy in order to detect the relevant sentences. This approach has not been used in the general context of document retrieval but we did use it previously and partially in document categorization <ref type="bibr" coords="1,117.06,624.22,65.29,11.19">(Mothe, 2002)</ref> and XML retrieval <ref type="bibr" coords="1,276.15,624.22,65.96,11.19">(Hubert, 2005)</ref>. In our approach a sentence is considered as relevant if it matches the topic with a certain level of coverage. This level of coverage depends on the category of the terms used in the texts. Three types of terms were defined for TREC 2002: highly relevant, scarcely relevant and non-relevant. In TREC 2003 we introduced a new class of terms: highly non-relevant terms. Terms from this category are extracted from the narrative parts of the topics that describe what non-relevant documents are. A negative weight can be assigned to these words. In TREC 2004, IRIT modified important parameter features of this approach. This includes parameters retuning, topic expansion and exploitation of sentences context. When topic expansion is applied, terms from relevant sentences (task 3) or from the first retrieved sentences (task 1) are added to the initial terms. The context of a sentence is taken into account by increasing the score of a sentence when either the previous or the next sentence is relevant.</p><p>With regard to the novelty part, a sentence is considered as novel if its similarity with each previously processed -and selected as novel-sentences does not exceed a certain threshold. In addition, this sentence should not be too similar to a virtual sentence made of the n-bestmatching and previously selected sentences. The similarity function is based on the dot-product function, vectors representing the sentences does not take into account neither the weight of the stems (we use Boolean vectors) nor the context in which the sentence occurs.</p><p>The rest of the paper is organized as follows: in section 2, we describe the method we used, including the way documents and topics are represented and the strategies we developed for the three tasks. In section 3 we present the results and give some comments. Finally, in section 4, we discuss our results and the evolution in Novelty Track results <ref type="bibr" coords="2,328.76,262.12,63.71,11.19">(2002 to 2004)</ref>.</p><p>2 Description of the method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document and topic representation</head><p>In our method, topics and sentences are considered as chunks of text. Each chunk is preprocessed the same way in order to extract representative terms. Terms extracted from a given topic are then categorized into different groups: highly relevant terms (HT), scarcely relevant terms (LT) and highly non-relevant terms (IT). Notice that non-relevant terms (iT) correspond to stop words. Extracted terms are weighted (see below). Each text is finally represented by these sets of weighted terms.</p><p>Note that the values of the different parameters in the formulas are given in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text processing</head><p>Texts are processed using the following method:</p><p>1. Stop words are removed, 2. The remaining words are normalized using a dictionary that provides a common root for different words. This dictionary contains 21291 entries.</p><p>3. Alternatively phrases are extracted. Phrases correspond to frequent sequences of words or frequent sequences of word roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Topic processing</head><p>A topic is pre-processed in order to mark-up its sentences that describe relevant documents and the sentences that describe non-relevant documents (see Figure <ref type="figure" coords="2,389.01,615.10,4.01,11.19" target="#fig_0">1</ref>: NarrativeRel and NarrativeNonRel tags).</p><p>Topic: 59 Title: Payne Steward Plane Crash Type: Event Descriptive: Identify a document that describes the plane crash that killed the golfer Payne Stewart on Oct. 25, 1999.</p><p>Narrative: Details about the crash, who else was aboard, and information about the destination and departure are relevant. The reason for the flight would not be relevant. Time and weather conditions are relevant.</p><p>NarrativeRel: Details about the crash, who else was aboard, and information about the destination and departure are relevant. Time and weather conditions are relevant. NarrativeNonRel: The reason for the flight would not be relevant. Then it is analyzed in order to extract the representative terms (words or phrases) as explained in the previous section. Each term is then weighted and categorized into one of the 3 groups:</p><p>-Highly relevant terms are terms that get a weight greater than H τ ,</p><p>-Scarcely relevant terms are terms that get a weight equal to L τ ,</p><p>-Highly non-relevant terms are terms that are associated with non-relevancy in the narrative part of the documents.</p><p>More precisely, the formula used to compute the term weights is defined as follows:</p><formula xml:id="formula_0" coords="3,49.68,268.65,319.55,47.89">Given k Q a topic and i t a term, { } word stop a not is t Q t T i k i k / ∈ = T k = TT k U TD k U TNR k U TNN k</formula><p>where TT k corresponds to the set of terms extracted from the Title of the topic, TD k from the Descriptive, TNR k from the NarrativeRel and TNN k is the NarrativeNonRel topic part.</p><formula xml:id="formula_1" coords="3,51.06,337.23,287.57,28.63">P k i tf , , is the frequency of i t in the TP k part, { } NN NP D T P , , , ∈</formula><p>The term weight regarding a topic is computed as follows:</p><formula xml:id="formula_2" coords="3,57.78,395.64,416.41,150.04">{ } otherwise if if if Q t weight otherwise and if f where f tf tf H k i L k i k i H k i k i k i NN k i NN k i NN k i k i NN k i NN k i P k i NP D T P P k i 0 0 0 ) , ( 1 0 0 0 ) , ( ) , ( , , , 1 , , 2 , , , , 1 1 , , 2 1 , ,<label>1 , , , , , 2 , , , , , , 1 =</label></formula><formula xml:id="formula_3" coords="3,50.16,379.90,415.32,190.30">&lt; &lt; = = = ≥ = = &lt; &gt; = ⋅ + = ⋅ = ⋅ = ∑ ∈ τ ω τ ω ω τ ω ω µ ω µ ω ω µ ω ω ω µ ω µ ω L τ and H</formula><p>τ are used in order to obtain a significant difference -in terms of importance-between highly relevant terms and scarcely relevant terms. Weights associated to scarcely relevant terms are set to L τ (1 in the experiments submitted to TREC). H τ is set to 3 in the TREC runs. This formula is also used in order to take into account highly non-relevant terms.</p><p>The term weight is used to categorize a term into one of the following groups:</p><formula xml:id="formula_4" coords="3,113.64,627.99,313.56,81.26">{ } { } L k i k k k i i k T t weight and TNR TD TT t t HT τ &gt; ∈ = ) , ( , , / U { } ( ) { } L k i k k k k i i k T t weight and TNR TD TT TNN t t LT τ = - ∈ = ) , ( , , / U { } { } NN NR D T P TP t weight t iT k i i k , , , 0 ) , ( / ∈ ∀ = = { } 0 ) , ( / &lt; ∈ = k i k i i k T t weight and TNN t t IT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Document processing</head><p>Each sentence of a document is considered as a text and the representative terms are extracted as explained in the section 2.1.1. To each term is associated a weight defined as follows:</p><p>Given j S a sentence, i t a term and j i tf , is the frequency of i t in j S .</p><formula xml:id="formula_5" coords="4,49.68,166.10,128.14,40.39">j i j i tf S t weight , ) , ( = 2.2 Relevant sentences</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Without topic expansion</head><p>In order to decide if a sentence is relevant, we associate three components to each sentence:</p><p>-a score that reflect the sentence -topic matching :</p><p>Given a topic k Q and a sentence j S ( )</p><formula xml:id="formula_6" coords="4,148.08,294.15,236.44,30.11">∑ ⋅ = ) , ( ) , ( ) , ( k i j i k j Q t weight S t weight Q S Score</formula><p>-and two groups of terms:</p><formula xml:id="formula_7" coords="4,68.16,347.90,266.09,69.36">{ } ) ( / k i i j HT Sj t t HS ∩ ∈ = { } ) ( / k i i j LT Sj t t LS ∩ ∈ = j</formula><p>HS corresponds to the highly relevant terms from the topic that also occurs in the sentence, j LS corresponds to the scarcely relevant terms from the topic that also occurs in the sentence.</p><p>A given sentence j S is then considered as relevant iff :</p><formula xml:id="formula_8" coords="4,106.92,485.30,327.29,51.03">k j j j k j j j k j LT HS LS HS g HT HS LS LS f Q S Score ⋅         + + ⋅         + &gt; ) , ( + α</formula><p>where X is the number of elements of X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">With topic expansion</head><p>Topic expansion is either based on blind relevance feedback using the first retrieved sentences (task 1) or relevance feedback using the sentences known as relevant (task 3). In both cases the model takes into account topic expansion using the following formula:</p><formula xml:id="formula_9" coords="4,210.30,649.26,186.81,40.83">{ } RP P k i NN NN NP D T P P k i tf µ µ ω + ⋅ = ∑ ∈ , , 2 , 1 , , , ,<label>(5)</label></formula><p>when tf i,k,P &gt; ∆ Where RP is the set of sentences used for topic expansion and ∆ is a constant used as a threshold.</p><formula xml:id="formula_10" coords="4,245.10,363.18,46.05,6.21">P∈{T,D,NR,NN} ) , ( , , 1 , ∑ ∈ = q i i p p P p Sim K β ( { } 5 , 4 ∈ q</formula><p>in the runs sent to TREC)</p><p>p is considered as redundant (not novel) iff:</p><formula xml:id="formula_11" coords="5,162.78,455.85,154.60,39.53">1 τ α ≥ p and 2 τ β ≥ p 1 1 = τ and 6 . 0 2 = τ</formula><p>for the runs sent to TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section presents the results we obtained with the method we developed as described in section 2. The description provides the value of the different parameters of our method:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description of the runs IRIT submitted</head><p>-The first series of values corresponds to the coefficients associated to the different topic parts and that are used to define the class of each extracted term (see section 2.1.2). µT, µD, µNR, µNN, first for terms then for phrases.</p><p>-The second part of the description indicates the function we used to select relevant sentences Func corresponds to ( ) ( )</p><formula xml:id="formula_12" coords="6,175.14,177.70,274.34,42.02">x x g and x x f 5 . 0 85 . 0 5 . 1 2 - = - = , α= 0 Norm corresponds to f(x)=g(x)=0,<label>α=3</label></formula><p>Bonif1: means that we increase by 1 the score of a sentence that follows a relevant sentence</p><p>BonifRetro: means that we increase by 2 a sentence that follows a relevant sentence and by 1 a sentence that precede a relevant sentence.</p><p>-The third and fourth parts correspond to the maximum number of sentences that are taken into account for feedback and the value of ∆ (see section 2.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relevant sentences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Task1</head><p>Figure <ref type="figure" coords="6,82.45,389.44,5.47,11.19" target="#fig_1">3</ref> indicates the number topics for which our best system (or run) has been ranked at the X th position among the 60 runs according to F-Measure. For example, our method obtains the best results for 1 topic, the second position for 2 topics, the third for 2 topics, etc. and has a rank higher than 41 th for only two topic (see figure <ref type="figure" coords="6,307.30,428.80,3.76,11.19" target="#fig_1">3</ref>.a). Figure <ref type="figure" coords="6,366.18,428.80,4.67,11.19" target="#fig_1">3</ref>.b provides a graph that summarizes figure 3.a by grouping together the results obtained for ranges of ranks. Additionally, the cumulative number of topics per range of system position is provided on the same graph.</p><p>For example, we obtained a rank between 1 and 5 for 8 topics. The system obtains a rank equal or lower to 20 for 28 topics.</p><p>This clearly shows that our method is better than average. To be more precise, over the 50 topics, we obtained 41 topics (82%) for which F-measure is higher or equal to the average Fmeasure over the 60 runs. And if we consider the run ranks, we obtained a rank higher or equal to the median (30) for 34 topics. There is no correlation between these results and the type of topic (event or opinion). Our training method seems to be insufficiently efficient compared to others' as our system ranked at a lower position when trained. On average, our system ranked at the 20 th position (over 60 runs) without training and at the 23 rd position (over 40 runs) when trained. Our system obtained a F-measure higher to the average for 29 topics (against 41 without training). However the results are different for event and opinion topics. Among the 29 topics, 12 are events and 17 are opinions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">New sentences</head><p>We present the results obtained when detecting novel sentences the same way (see Figure <ref type="figure" coords="7,459.68,397.00,3.83,11.19" target="#fig_2">4</ref>). We distinguish the results when novel sentences are extracted from the retrieved sentences (TREC task 1 ; figure <ref type="figure" coords="7,123.56,423.28,4.03,11.19" target="#fig_2">4</ref>.a) and when they are extracted from the set of sentences known as relevant (TREC task 2, figure 4.b).</p><p>Regarding the first case (task 1), over the 50 topics, we obtained 43 topics (86%) for which Fmeasure is higher or equal to the average over the 60 runs. And if we consider the run ranks, we obtained a rank higher than the median (30) for 44 topics (88%).</p><p>However, when considering the relevant sentences (task 2), over the 50 topics, we obtained 46 topics (92%) for which F-measure is higher or equal to the average over the 55 runs. Event and opinion topics are evenly distributed (23-23). If we consider the run ranks, we obtained a rank higher than the median (27) for 34 topics (68%). In that case, events and opinions are not evenly distributed .</p><p>The results obtained in Task 3 at the new sub-task, are quite comparable to those obtained at the relevance sub-task (i.e. our system under-performed comparatively to the other systems). This probably does not question the method we define to detect new sentences since it was carried out on a 'noisy' set of sentences (We do not detect relevant sentences well, as a result, F-measure is low for novelty detection.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions</head><p>The results we obtained in TREC 2002 were quite good regarding the 'relevant' subtask. Indeed, for 36 topics (73%), R*P measure was higher or equal to F-measure averaged over the 42 runs that were submitted.</p><p>In TREC 2003, we improved these results as we obtained 46 topics (92%) for which F-measure (2*R*P/(R+P)) was equal or higher to F-measure averaged over the 55 runs submitted. With regard to the 'novelty' part, we also obtained 46 topics (92%) for which F-measure was higher or equal to the average over the 55 runs. An interesting fact is that, relatively to other participants' methods, our method performed better when there was some 'noise' in the sentence set. Indeed the results were better when considering the retrieved sentences than when considering only the relevant sentences, (i.e. our system ranked better over the submitted runs in the case of 'noisy' sentences). We obtained 41 topics (82%) for which F-measure was higher or equal to the average over the 55 runs.</p><p>In TREC 2004, regarding task1 and the detection of the sentence relevancy, for 41 topics (82%), F-measure is higher or equal to F-measure averaged over the 60 submitted runs. 20 of these 41 topics are "events" and 21 are "opinions" (thus event and opinion topics are evenly distributed among these 41 topics).</p><p>With regard to the 'novelty' part (task 1), when considering the retrieved sentences, 43 topics get F-measure higher or equal to F-measure averaged over the runs. However, in this case, the distribution is slightly different, as there are 23 events and 20 opinion topics. Regarding task 2, for which only relevant sentences are considered to detect novelty, 46 topics (92%) get Fmeasure equal or higher than F-measure averaged over the 55 runs (evenly distributed among event and opinion topics). A system that would consider all relevant sentences as novel would get only 29 topics for which F-measure would be higher or equal to average (evenly distributed among event and opinion topics). This is far less than the average performance of all tested systems. This was not the case in 2002 and 2003.</p><p>When some information (relevant sentences) is used for learning purpose (task 3), our system detects relevant sentences better than the average over the 40 systems (runs) for 29 topics and it detects novel sentences better than the average for only 6 topics. However, up to 19 topics are better than the median for the latter sub-task. This means that runs either performed very well or very badly (big standard deviation).</p><p>Last year the system was clearly better detecting relevance than novelty. In TREC 2004, this difference is no clear any more. The results we obtained are better than results averaged over the runs. Regarding relevance detection (task 1), our best run obtains the following results: Average precision 0.32, Average recall 0.74 and Average F-measure 0.404. With regard to the novelty detection (task 1), our best run obtains the following results: Average precision 0.15, Average recall 0.68 and Average F-measure 0.221. When using relevant sentences only, the novelty detection process obtains: Average precision 0.45, Average recall 0.98 and Average F-measure 0.605. Finally, when using relevance judgments to train the system to detect relevance, the system obtains: Average precision 0.29, Average recall 0.68 and Average F-measure 0.372.</p><p>5 References </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,207.00,135.38,125.06,8.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: topic 59 (TREC 2004)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,129.30,238.00,280.44,11.19"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of topics per run rank -relevant sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,126.90,427.42,285.36,11.19"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Number of topics per run rank -summarized results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,49.68,405.04,140.72,11.19;9,49.68,424.00,85.58,11.19;9,153.10,424.00,336.41,11.19;9,118.62,437.14,196.47,11.19;9,49.68,456.10,83.00,11.19;9,153.10,456.10,336.33,11.19;9,118.62,469.24,361.04,11.19;9,49.68,488.20,88.16,11.19;9,153.10,488.20,336.35,11.19;9,118.62,501.34,370.89,11.19;9,118.62,514.48,370.88,11.19;9,118.62,527.56,370.88,11.19;9,118.62,540.70,78.47,11.19"><head>[</head><label></label><figDesc>trec.nist.gov] TREC web site. (Dkaki et al., 2002) T. Dkaki, J. Mothe, J. Augé, Novelty track at IRIT-SIG, Text Retrieval Conference TREC 2002, pp 332-336, 2003. (Dkaki et al. 2004) T. Dkaki, J. Mothe, Combining Positive and Negative Query Feedback in Passage Retrieval, RIAO, pp 661-672, 2004. http://www.irit.fr/~Josiane.Mothe (Mothe et al., 2002) J. Mothe., C. Chrisment, B. Dousset, J. Alaux, DocCube : multidimensional visualisation and exploration of large document sets, Journal of the American Society for Information Science and Technology, JASIST, Special topic section: web retrieval and mining, Guest Editor: Hsinchun Chen, 54 (7), pp. 650-659, March 2003.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The detection of relevant sentences is then based on the formulas described section 2.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Novel sentences</head><p>To decide if a sentence p is to be considered as novel, we compute the similarity between the sentence p and the previous successfully processed sentences i p (novel) and the similarity between the sentence p and a sentence ' P automatically built from the set of i p :</p><p>a set of sentences labeled as novel and</p><p>, ' P is a sentence made of all the sentences from Π , • ( )</p><p>a function that computes the similarity between x and y and • p a sentence for which the system has to decide if it brings new information.</p><p>We first compute the following similarities:</p><p>( )</p><p>We then consider the q best matching sentences:</p><p>is the series of sentences obtained by ordering Π in decreasing order of i p, ω .</p><p>{ }</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
