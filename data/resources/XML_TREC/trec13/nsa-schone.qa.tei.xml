<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_txYTyKq">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dragon Development Corporation</orgName>
								<address>
									<settlement>Columbia</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Johns Hopkins Applies Physics Laboratory</orgName>
								<address>
									<settlement>Laurel</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18A7D1E678EE9377FC6718E29D28317F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We provide a description of the QACTIS question-answering system and its application to and performance in the 2004 TREC question-answering evaluation. Since this was also QACTIS's first year competing at TREC, we provide a complete overview of its purpose, development, structure, and its future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic question answering (QA) has been an area of technical interest at TREC for more than a half decade and it has been the subject of major investment from the government-driven AQUAINT program for almost three years. QACTIS (pronounced like "cactus"), which stands for "Question-Answering for Cross-Lingual Text, Image, and Speech," is a still-in-formulation protoype system that is being developed by the U.S. Department of Defense as a means of gaining greater understanding of QA as a whole while focusing on cross-lingual and cross-media QA--areas which have received less attention from AQUAINT. The final goal for this effort is to develop a prototype which can allow users to ask questions in more than one language (e.g., English and Spanish), interpret those questions regardless of the language, and return answers which have been derived from multilingual and/or multimedia sources. Particular interest for this project is the capability of eventually answering questions from speech which is a virtually unresearched area of study. For the purposes of this TREC competition, however, we, like others, concentrate on English newswire text and the hope we have is that the knowledge gained by this experience can be useful in other languages and other media types.</p><p>In terms of its question-answering ability, QACTIS consists of three major components. Two of these components are competing mechanisms for interpreting questions and postulating possible answers. The third component is a method of validating proposed answers. The first mechanism for answer generation is designed to be general and handle any kind of factoid-style question that might be posed. This technique, which we will here refer to as a "Knowledge-Graph Induction" strategy makes use of sophisticated natural language processing (NLP) techniques to automatically build attributed object-relationship graphs for documents which likely contain answers,. This technique then performs graphical searches to find relevant components to answers and to ensure such components have the interrelationships required by the question. This approach is fairly costly and there are many times when cheaper and more convenient solutions are available. For this reason, QACTIS has a second search mechanism which we call a "Filter Cascade" strategy. This strategy consists of finding potential answers by initially hypothesizing many possible answers and then successively applying increasingly restrictive filters to narrow down potential answers to the one or few most appropriate selections. These filters consist of template-matching, shallow grammatical rule applications, and others. The fusion of results from these two systems represents the prototype's first kind of answers. This fusion can stand on its own as a questionanswerer. However, depending on a user's needs and computing environment, it might be feasible and even desirable to take advantage of the largely unstructured knowledge which exists on the World Wide Web. To satisfy this possibility, the third major component of the prototype is one which will receive answers from either of the individual systems or the fusion thereof and will use the Web to eliminate undesirable potential answers.</p><p>In the sections that follow, we provide a description of these components as well as a description of the foundations which are required in order for these components to function. Additionally, we indicate the level of performance we observed on the TREC 2004 evaluation as well as the successes and the difficulties we experienced in the evaluation. Lastly, we outline the future directions that we plan to undertake between this and the next TREC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Required Foundations</head><p>Figure <ref type="figure" coords="2,85.28,380.55,5.00,9.00" target="#fig_0">1</ref> provides a depiction of QACTIS as a whole. Since this TREC evaluation focuses on the processing of English newswire text, those components that are related to multilingual or multilmedia issues have not been depicted in the diagram and we will not go into a discussion of those. Several of the components which are emphasized have been mentioned before and will be described in greater detail later: namely, the induced knowledge-graph search, the filter cascade strategy, and the web validation. Before we describe those pieces, we will first provide a description of the few remaining groundwork components, namely question interpretation, knowledge incorporation, and document search which provide the foundations to the remaining components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Question Interpretation</head><p>Obviously, it is essential that before a QA system can answer a question, it must be able to receive that question from a user and provide an interpretation of that question. QACTIS can interpret questions either via batch mode or graphical user interface. Figure <ref type="figure" coords="2,186.37,615.55,5.00,9.00" target="#fig_1">2</ref> provides a depiction of QACTIS's GUI and an example question-answer pair (where the answer also includes the corresponding document support).</p><p>Prior to the 2004 TREC QA Evaluation, QACTIS was designed to handle questions in a stateless fashion as if each question were independent of the others. Moreover, the system was primarily designed to handle exclusively factoid-style questions. Since the evaluation for 2004 was to involve question sessions consisting of factoids, lists, and definitions, these pieces has to be incorporated into the system.</p><p>The first issue needing to be addressed was expanding from only factoids to also allow lists and 'other.' Since factoid-style answers are lists with only a single element, and since QACTIS actually produces a list and selects the best answers from that list as factoid answers, the process of deriving lists means that the system only had to be told how many answers to respond with. Based on some empirical calculations, we determined that a fixed number of list answers (between 5 and 7) seemed to provide optimal list results. These are the parameters that we then used in the evaluation.</p><p>Regarding 'other' questions, we recognized from the TREC 2003 evaluation proceedings that the best-valued definition responses were wordy and tended to report most sentences that contained words of interest <ref type="bibr" coords="2,503.56,287.84,10.62,9.00" target="#b0">[1]</ref>. We decided that our primary system would therefore select out the best sentences and return those as answers to 'other' questions. However, since older TREC evaluations had incorporated definition-like questions of the form "Who is X" or "What is X" where a factoid answer might satisfy, we thought to also try answering 'other' questions as if they were list questions of "What is X" questions. We submitted results using both methods.</p><p>The next issue that had to be tackled was to determine how to handle a series of questions within a given 'user session' (that is, under a given topic). We resolved that the easiest strategy for handling such problems would be to cast them as a list of independent questions with resolved anaphora. A source of difficulty here was to determine how complicated the TREC questions might be in terms of resolving anaphora. NIST only provided a few simple examples and it was unclear if this level of simplicity would be comparable to what would be seen at evaluation. We assumed we would need the ability to handle more complex questions. Two members of our team wrote and evaluated a large number of questions in the 'user session' format. There were six main styles of questions that we observed in this analysis. These were:</p><p>(1) non-anaphoric questions; (2) questions where a simple anaphor related directly to the topic; (3) questions requiring more complicated anaphor resolution; (4) questions where neither the topic nor anaphora were indicated; <ref type="bibr" coords="2,528.34,613.84,11.66,9.00" target="#b4">(5)</ref> questions that referred to the answers to previous questions; and <ref type="bibr" coords="2,349.61,637.84,11.66,9.00" target="#b5">(6)</ref> questions that referred to the verbage of previous questions. Though the first kind of such questions is trivial to solve, the rest are more interesting. We here provide examples of each kind: We developed software that we believed could handle the first five kinds of these questions. We anticipated that the sixth kind would be out of scope of this TREC evaluation. At evaluation time, our code was able to handle most of the questions reasonably well, but as will be made more clear in Section 3, there were some catastrophic failures which damaged our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Knowledge Incorporation</head><p>In addition to determining the kind of question and resolving session issues, another component of question interpretation is determining the intent behind the wording of the question. Taxonomic, ontologic, and dictionary-based information can be beneficial for judging the meanings of the question words. To satisfy this need, we made appeal to versions of the English WordNet <ref type="bibr" coords="3,507.12,539.97,10.62,9.00" target="#b1">[2]</ref>. We also made use of an electronic dictionary from one of our earlier efforts <ref type="bibr" coords="3,366.46,563.97,10.62,9.00" target="#b2">[3]</ref>. In addition to accessing meaning, we also needed to contend with issues regarding syntactic variation. We made some use of WordNet's word stemming capability and we also created a stemmer of our own which would provide multiple possible stems or conflations of the words in question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Document Search</head><p>A common QA practice is to initially interpret the incoming question as if it were a request for relevant documents rather than a request for an answer <ref type="bibr" coords="3,454.10,683.97,10.62,9.00" target="#b0">[1]</ref>. Using this strategy allows the system to use typical information retrieval (IR) practices to identify documents that are likely to con- question, an answer, and support.</p><p>tain answers to the user's questions. Although there is no guarantee that this assumption will be valid, this strategy has served useful in most experiements we have tried thus far. In the event that it is unsuccessful, there is not reason to believe that our QA system would have found the answer if the IR could not return appropriate documents. We experimented with several IR systems, including one of our own making. We determined that the Lemur system <ref type="bibr" coords="4,84.65,167.84,11.66,9.00" target="#b3">[4]</ref> provided the best out-of-the-box results to our searches. We therefore incorporated Lemur into QAC-TIS. Although it is possible to massage the question so as to optimize IR results, we have not done this yet in QAC-TIS. Our results therefore reflect the use of Lemur as a black box where a query is supplied as input and a list of documents is generated as output. We use no pseudo-relevance feedback (PRF) in searching since preliminary experiments suggested that although PRF improves mean average precision, there seems to be a diluting amongst the top answer-bearing documents.</p><p>One last comment is in order regarding the kind of IR search QACTIS performs. In the QA community, some believe that the optimal form of IR for improving the ability to answer is to do optimal passage retrieval. This might be a good strategy for QA on factoid-style questions from newswire data where answers are often isolated in a limited number of sentences. However, it is not clear that restrictive passage retrieval is as beneficial when questions are more general, when answers occur across documents, and when the data consists of non-news or non-textual material. We therefore apply IR to full document retrieval which documents then get mined by the specific answer-searcher which later gets applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge-Graph Induction Strategy</head><p>After the introductory steps described above, the next process is to perform a search for the answer. As was mentioned before, QACTIS has a two-phase approach to question answering, namely a knowledge-graph induction strategy and a separate filter cascade strategy. The knowledge graph induction and search is to be general and to potentially handle any kind of questions whereas the filter cascade was developed to handle specific kinds of questions with greater accuracy. We first describe the general knowledge-graph strategy and later discuss the filter cascade mechanism.</p><p>Figure <ref type="figure" coords="4,101.76,611.84,5.00,9.00">3</ref> provides a detailed graphical view of the methodology employed by the knowledge-graph induction strategy. The basic objective of this strategy is to convert the top N potentially relevant documents (as returned by Lemur) into a single, indexed, directed, attributed entity-relationship graph which can be mined to find connected subgraphs containing the desired components of the question. There is insufficient space to describe all of this process in exhaustive detail, so we provide a gen-eral overview of the major system components which allow us to induce and mine such a graph.  </p><formula xml:id="formula_0" coords="4,308.34,121.17,208.03,269.86">Figure 3. Knowledge-Graph Induction/Search &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; Question (Q) Identifinder ParseSelect charniak parser Parsed Docs Documents &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; &lt; &gt; ((( ))( )( ) (( ( ) ) ( ) Reformat &amp; Parse LEMUR Q' Graph Builder Search Builder</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag of Searches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Graph Building</head><p>To begin the graph induction process, we first perform a deep syntactic parse of the Lemur documents using the Charniak parser <ref type="bibr" coords="5,163.77,107.84,10.62,9.00" target="#b4">[5]</ref>. Although we would like parsing to happen at question time, it is currently an extremely slow process (taking about 1 second per sentence on a 2.8 GHz Pentium IV). Therefore, we parse all documents earlier at indexing time and then need only to pull back the correct parses during the query phase. In addition to parsing the documents, we run BBN's Identifinder TM system <ref type="bibr" coords="5,124.04,194.50,11.66,9.00" target="#b5">[6]</ref> to provide entity information which can support answering questions regarding people, places, and times. Identifinder runs sufficiently fast that we apply it at query time. Upon completion of both of these efforts, the system is then ready to induce a graph from this information. We will consider an example of how this is done. Suppose there were a document in our collection: "Johan Vaaler invented the paper clip 90 years ago." Charniak's parser would convert this into something of the form The graphbuilder next converts nouns and noun phrases into entities, verb phrases into relationships, and quantifiers, prepositional phrases, and adjectives into attributes. As it does this, it tries to some degree to resolve anaphora and the meanings of definite articles. When complete, it produces a graph akin to the that in Figure <ref type="figure" coords="5,228.42,546.50,3.89,9.00" target="#fig_4">4</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Creating a Bag of Searches</head><p>In parallel with the graph-building effort, there is a separate process within the graph induction strategy which tries to interpret the user's needs based on the question. If the question were "Who invented the paper clip?," the system attempts first to convert the question into a definitive statement like "Person.Q invented the paper clip." It then parses the statement and applies the graph building process thereto. The major objects (entities, relationships, quantifiers, and attributes) and some relations between them (like quantifier and quantified) are mined from the graph as entities to search for. Next, the system builds a set of searches to process each kind of major object. In the sentence above, the major objects would be the "person.q" and "paper clip" entities and the "invented" relationship. The system determines the kinds of objects that these are and induces a collection of subroutines (one per object) that will be used to test each word/phrase of the top N documents to see if each provides evidence of the object. For example, the subroutine that is induced for the word "paper clip" would test each word in the top N documents to see if they are WordNet synonyms of the noun "paper clip" or if they are some sort of stem or conflation of it. The same is true for the word "invented" except that it knows to be looking for synonyms of a verb. For the word "person.Q," the system knows that words or phrases that satisfy this must at least be entities, but entities that have been marked by Identifinder as people or possibly organizations provide better solutions, as might entities that have been referred to as "he," "she," and so forth. Each different kind of question word, namely "who," "when," "where," "what," "why," and so forth generate a separate type of subroutine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Growth Using Reachability and Distance</head><p>The collection of subroutines is applied to all of the words/phrases within the top N documents and any word that is found to match a subroutine's needs is saved off as well as the location where the it appears. A language model-based score (see <ref type="bibr" coords="5,408.97,539.84,10.62,9.00" target="#b6">[7]</ref>, <ref type="bibr" coords="5,427.24,539.84,11.24,9.00" target="#b7">[8]</ref>) is then used to weight each candidate answer.</p><p>The main task for this system is to determine which objects, from among those that satisfy question words, can best address the needs of the user. This means one needs to identify the objects that are candidate answers and see if, through graph connectivity, it is possible to grow a subgraph which contains all or most of the desired elements from the incoming question. Suppose Figure <ref type="figure" coords="5,535.00,635.84,5.00,9.00" target="#fig_4">4</ref> represents one of the top N documents. Clearly, entity #6 ("paper clip") and relationship #1 ("invented") satisfy the question's requirement for "paper clip" and "invented." Entity #3 ("Johan Vaaler"), according to the graph, was identified as a person ... so this also satisfies the need of "person.Q." However, these three objects by themselves do not solve the user's question. They need to be woven together, if possible, so as to guarantee that the answer is correctly found. Since Figure <ref type="figure" coords="6,188.56,107.84,5.00,9.00" target="#fig_4">4</ref> represents a directed graph, objects can be 'woven together' if an arrow exists between them and if the arrows points in the appropriate direction. From Figure <ref type="figure" coords="6,157.90,143.84,3.75,9.00" target="#fig_4">4</ref>, relationship #1 is reachable from entity #3 and entity #6 is reachable from relationship #1. Hence, we can link together each of the desired terms using this strategy and report, as a final answer, the product of the scores of each object.</p><p>For many graphs, however, it is impossible to obtain reachability between all of the needed components. As a final supplement to the answer search process, we make use of the distance that a missing component is away from a reachable subgraph. The score for incorporating such a word is related to the reciprocal of the square of its distance from the subgraph.</p><p>The scores of the distance-augmented reachable subgraphs are sorted, and the subgraphs with largest scores are reported as answers. For factoid answers, only the best such answer is kept, and for lists, the top m answers are reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cascade of Filters Strategy</head><p>A separate module was developed to provide a more indepth analysis and corresponding answers to certain kinds of questions such as the "How many" and "Other" type questions. This was the initial starting point for QACTIS development and the feeling was that certain types of questions are likely to be amenable to straightforward solutions. The Cascade of Filters approach (CFA) has significantly better MRR scores for questions like the "How many" over those of the Knowledge-Graph Induction algorithm, so QACTIS typically fuses both methods with the idea that specifically-tailored question answering through CFA should be used when available. The CFA uses different filters to identify potential answers and also eliminate others. Any values left at the end of all the filters was considered to be an appropriate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Trigrams n Tags -(TnT)</head><p>The CFA relies on information retrieval using Lemur, Wordnet as a lexical reference system, and TnT for partof-speech tagging. TnT (short for "Trigrams 'n Tags") is an efficient statistical part-of-speech tagger that is trainable on different languages and virtually any tagset <ref type="bibr" coords="6,260.79,619.84,10.62,9.00" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Filters</head><p>The CFA evaluates the top N (N usually set to 30) documents returned by Lemur. (The original question is tokenized using TnT, and the main noun/noun phrase of the question is extracted.) WordNet is also used to generate synonyms for the noun/noun phrase. The retrieved documents are then successively filtered to identify candidate answers. After all candidate answers are scored, the top score is considered the correct answer and ties go to the answer from the highest scoring Lemur document. The CFA filters for "how many" questions are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>(Q)  <ref type="bibr" coords="7,212.33,71.84,4.45,9.00">:</ref> The SEF identifies sentences of each top IR document that contain a match to the question noun phrase or synset synonyms, and also contain a numeric value. The distance in words between the noun and value are then calculated; the shortest distance is the best. A count of the important question words and synset words is also recorded and added to the minimum distance score. Sentences with the highest scores for each of the top N documents are saved in a hashtable with the candidate "how many" numerical value as the key. Each of these sentences are then POS-tagged using TnT and saved in another hashtable ... again using the candidate value as the key. Both hashtables are evaluated by the Template Matcher Filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.2">Template</head><p>Matcher Filter (TMF): Shallow parsing was performed on the question and template match filters were formed. For example, "How many hexagons are on a soccer ball" could generate templates a) "&lt;#&gt; hexagons are on a soccer ball" b) "soccer ball has &lt;#&gt; hexagons" c) "soccer ball contains &lt;#&gt; hexagons" Exact matches indicated a high degree of certainty. All potential answer sentences were tested against the template filters. If any matches were found from the templates, the system progressed to the sentence score recalculation module using these template matched answers, otherwise the Semantic Rules Filter is applied to the hashtable sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.3">Semantic Rules Filter (SRF):</head><p>These filters attempt to use semantic rules for validation. This set of filters eliminates candidates from the hashtable of possibilities. One such filter deals with the verb and its synonyms. For example, given the following two sentences, the first would be eliminated for the question "How many atheletes participated in the 2004 summer olympics games?"</p><p>Potential filtered sentence 1: 103 atheletes of the 2004 summer olympics won medals for the United States. Potential filtered sentence 2: 11,099 atheletes competed for medals in the summer olympics. Both sentences were in the candidate answer hash because they contained the main question word "athelete," had a numeric value with a distance of 1 from that question word, and contained the same number of important question words (summer olympics). In this case the verb "won" did not match "participated" or a synonym of participated (competed), so that sentence was eliminated from the candidate hashtable. The system also accounted for a conjugated form of the verb to appear in the answer as well as a verb to be presented in a different tense than the question verb. If the candidate hashtable still had entries, the processing continued to the next set of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.4">Trigram, Shallow</head><p>Parsing Filter (TSPF): All word-trigrams of the question and candidate answer sentences are computed. If any candidate sentences has trigrams in common with the question, then all sentences that do not have commonality are purged. Next, the POS tags of the question are used to help form a direct object/ verb/value triple from the question. Such triples are also formed for each of the remaining candidate sentences. Again, if there are any candidate answers whose triple matches that of the question, then any candidates that do not have matches are discarded.</p><p>2.3.2.5 Answer reporting: If there are still more than one candidate answer, the system rescores those that remain. The highest-scoring sentence is declared to be the answer. Ties go to the highest Lemur-scoring document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Providing Web Validation</head><p>Several researchers have proposed using large external corpora for the purpose of validating candidate answers to factoid questions <ref type="bibr" coords="7,380.88,303.84,15.33,9.00" target="#b9">[10]</ref>. In particular, the size of the Web and the availability of online search tools make it convenient to use search engines as a data source to confirm answers using the Web as a source for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Magnini et al. compared two different approaches for</head><p>Web-based answer validation: a statistical approach that examined question and answer word co-occurences and a content based method that measures the proximity of question words to an answer in short text extracts. The two methods performed similarly and conferred roughly a 22% increase in performance over a baseline that did not apply answer validation. For our participation in the TREC 2004 QA task we adopted the content-based method of Magnini et al. for both the factoid and list subtasks.</p><p>The algorithm we used is based on the notion that the candidate answer pattern will appear in close proximity to the question terms. A query consisting of the exact answer pattern conjoined with content words from the question was submitted to the Altavista search engine. Thus one web request per candidate answer is required. Like many search engines, the Altavista engine returns a ranked lists of documents along with short textual extracts which contain query terms. It is these textual portions that are examined -not full documents. We want to reward answers that are very close to many question words and which are found with question words in multiple documents. We only considered the top ten responses from our baseline system which found a correct answer in 48% (TREC-9) and 37% (TREC 2003) of cases.</p><p>An answer's score is produced by aggregating scores obtained from individual textual snippets returned by the search engine. The candidate answer string can contain multiple words and may occur more than once the extract. We search for the presence of each non-stopword question word in the snippet. If found, we determine the distance, in 'non-question' words, between question word and answer. (A non-question word is any word not appearing in the question). For each word, the closest location is used. A product is computed over all query terms using 2 1/(dist+1) as factors. This results in a factor of 1 when a term is absent and a factor of 2 when a question word is adjacent to the answer (or separated only by other question words). Values between 1 and 2 are obtained when question words are more distant from the answer location. Per-snippet scores are summed for each candidate answer and the highest score is deemed the most likely response.</p><p>We experimented with this approach using data from the TREC-9 and TREC 2003 QA tracks and found that substantial gains could be obtained. Answer validation enhances overall performance on all factoid questions, but the technique appears more effective with certain question types. We left our baseline system's answers unmodified for amount-type questions (i.e., "how many ..." or "how long is") which make up about 13% of the factoid questions. In Table <ref type="table" coords="8,115.93,338.50,5.00,9.00" target="#tab_4">1</ref> we show the improvement observed for factoid questions from previous TREC QA tracks. We produced ten candidate answers and computed both mean reciprocal rank (MRR) and the number correct (#Corr) at rank 1. Given the improvements that this technique provided at development time, we expected comparable gains to occur at evaluation time as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM EVALUATIONS 3.1. Description of Results</head><p>For TREC-2004, we opted to submit three separate systems. The first system ("nsaqactis1") was our baseline system which represented the output fusion of the Knowledge-Graph and CFA strategies.</p><p>CFA provided the answers to definition and "how many" style questions, and the knowledge graph produced the other types of results. The second system, "nsaqactis2," was the same as the first except web validation was applied afterwards. The third system, "nsaqactis3" used only the knowledge graph and web validation to answer the questions. The performance of the TREC 2004 evaluation is provided in Table 2. To our surprise, our baseline (#1) system performed approximately as well as our Trec 9 baseline had performed. We had hoped that a comparable web-validation improvement would have been seen as well, but to our dismay, web validation actually resulted in a loss of performance on non-list questions, but there was a reasonable gain using web validation on lists. We were also delighted to have "Other" style questions perform very well given that this was a last-minute and untested feature of our system. In the sections that following, we provide a brief analysis of these results to include an indication of where things went as well or better than expected and where the system failed to respond as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Times When Things Go Well</head><p>Perhaps the component of our system whose performance we are most pleased with is that of "Other" answering. Early in the paper, we mentioned that just prior to the TREC evaluation, QACTIS had very little capacity to handle "other" style questions. Through the Knowledge-Induction Search we did have limited capacity of declaring hypernyms of the entities needing definition to be defining statements about those entities, but we did not have the ability to handle definitions in the encyclopedic style that TREC evaluates. For interest sake, we did submit our hypernym output as one version (#3) of definitions, but we had interest in creating definitions that would be more aligned with the evaluation. In last year's TREC evaluations, definitions that consisted of well-chosen sentences seemed to outperform most other types of definitions, so our throught was to build some comparable mechanism. The CFA strategy works primarily from well-chosen sentences, so it seems well-suited to building up our definer. Definitional questions were therefore handled by modifying the Sentence Extractor Filter (2.3.2.1) used in CFA. In particular, sentences from the top N documents were scanned for matches to the target topic and those sentences were saved off. No attempt was made to avoid redundancy with prior questions from the same session. A simple filter was then applied that accepted only those sentences as answer components that had lengths greater than 10 and less than 250 words. The number of sentences per topic was capped at 50.   <ref type="table" coords="9,97.42,71.84,5.00,9.00" target="#tab_5">2</ref> suggests that this strategy seemed to work reasonably well. In our best-scoring run (the baseline, #1), the answers were on average about 2336 characters long and had an average precision of 0.179 and an average recall of 0.485. Since recall played a significant role in performance, and since long answers are well tolerated, this led to the reasonable score of 0.367 which was better than we had hoped to achieve.</p><p>(As a matter of comment, the third system, which scored poorly was based purely on lists of potential hypernyms. In this case, the average answer length was reduced to 226 characters and the precision dropped somewhat to 0.118. Yet the average recall --the most important component of the evaluation --was very low...only 0.066.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Difficulties: The Unexpected</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Series-style Question Problems</head><p>We had expected that our module to convert series-style questions into independent questions would work satisfactorily. There were a number of questions where the system had significant difficulties, however.</p><p>In the 11th squestion-session, the target was "the band Nirvana" and the second and third questions of that session asked about the band members and the band's formation. The resolver recognized that "band" and "band Nirvana" were the same but did not make the corresponding substitutions into the independent questions. Thus, the question did not indicate what kind of band was being looked for and all corresponding answers were missed.</p><p>In this same series, the next questions that were posed used the terms "their" and "they." The resolver assumed that since the former question mentioned band members whereas the topical 'band' is a singular entity, "their" and "they" must refer to the answer of the second question in the series. Thus, instead of using "Nirvana" as a substitution for the anaphora, it used what it thought mught be an answer to the group members question, and it thus substituted "Desmond Chase" for "they" and "their."</p><p>These two phenomena, namely, failing to make the substitution when part of the topic was found in the question and using answers to previous series questions to resolve the anaphors, was a condition which occurred a total of 16 times throughout the question collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Inexact answers</head><p>Another difficulty for our system was that by and large, until the evaluation, we paid little heed to the actual documents where answers appeared and we did not concern ourselves heavily on whether the cleanest answer (i.e., an exact answer) was presented --but only if the right kind of answer was identified. The evaluation, however, was concerned with providing document support and answer exactness.</p><p>The first of these issues is rather complicated to resolve in a system which tries to identify answers across documents --which document did the answer actually appear in? To prepare for the evaluation, we had to equip the system to know, to the best of its ability, where the original answer came from. These modifications were reasonably successful in that only one of the otherwise correct answers we returned was declared to be unsupported.</p><p>On the other hand, the requirement for answer exactness resulted in a high penalty for us. Our best system only produced 47 correct answers. Yet the system produced 12 answers which were deemed to be inexact, thus resulting in significant damage to our overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Experiments and Test in Web Validation</head><p>As mentioned, we applied answer validation on two of three TREC 2004 submissions. Although the method was untested, we also attempted to validate responses to list questions as with factoids.</p><p>Location questions fared worse with Web-validation and 'what' and 'who' questions were less popular in this year's set of questions. These differences in the distribution of question types might account for the observed differences.</p><p>For list questions we returned as many as 7 responses after considering up to 15 possibilities from the baseline system (more or less possibilities may have been provided). Here Web-validation had a significant positive effect, improving the F-score from 0.071 (without validation) to 0.098.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FUTURE DIRECTIONS</head><p>The results we have presented represent our first attempt to participate in the TREC QA Evaluation. Across the course of the next year, we expect that our system will improve as we add functionality for better resolving anaphora, so this will be one of early expansions. Moreover, as we have analyzed documents, we have noted that a significant number of potential answers are not observed since inference is required in order to find them. This, then, will be our next area of concern. Lastly, since multimedia and multilingual QA is of interest to us, we expect to enable the capabilities we have shown here in Spanish and began to try them on non-text media.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,54.00,89.17,111.91,9.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,66.43,359.66,186.92,10.80;3,80.71,371.66,159.83,10.80;3,57.82,72.71,302.07,277.29"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: View of the QACTIS GUI, aquestion, an answer, and support.</figDesc><graphic coords="3,57.82,72.71,302.07,277.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,54.00,318.50,159.42,9.00;5,72.86,330.50,82.77,9.00;5,72.86,342.50,146.06,9.00;5,72.86,354.50,209.82,9.00;5,54.00,370.50,231.66,9.00;5,54.00,382.50,231.66,9.00;5,54.00,394.50,231.66,9.00;5,54.00,406.50,231.66,9.00;5,54.00,418.50,192.47,9.00;5,54.00,434.50,159.42,9.00;5,71.14,446.50,82.77,9.00;5,71.14,458.50,146.06,9.00;5,71.14,470.50,159.00,9.00"><head>(</head><label></label><figDesc>S1 (S (NP (NNP Johan) (NNP Vaaler)) (VP (VBD invented) (NP (DT the) (NN paper) (NN clip)) (ADVP (NP (CD 90) (NNS years)) (RB ago))) (. .))) Identifinder will also indicate that "Johan Vaaler" is a person. The graph builder then reprocesses the string to convert relative times like "90 years ago" into absolute times by using the document's metadata that indicates it was written in 1989, subtract 90 years, and reporting (S1 (S (NP (NNP Johan) (NNP Vaaler)) (VP (VBD invented) (NP (DT the) (NN paper) (NN clip)) (ADVP (PP (IN in) (NN 1899)) )) (. .)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,54.00,566.50,190.41,9.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Indexed, Attributed Entity-Rel Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,308.34,71.84,112.86,9.00"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Cascade of Filters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,63.57,107.23,377.61,610.61"><head>Knowledge Graph Induction &amp; Search Filter Cascade- based Search JOIN Final Answer Question Interpretation Knowledge Incorporation Document Search User's Question Web Validation (Optional)</head><label></label><figDesc></figDesc><table coords="2,308.34,694.80,132.84,23.04"><row><cell>Type 2: Simple Anaphor</cell></row><row><cell>"Kosovo": Where is it located?</cell></row></table><note coords="3,62.57,407.97,179.98,9.00;3,62.57,419.97,188.59,9.00;3,54.00,436.93,127.85,10.80;3,63.29,450.97,222.42,9.00;3,63.29,462.97,52.23,9.00;3,63.29,474.97,116.12,9.00"><p>"Roses": Where were the first ones grown...? "George W Bush": When did Bush take office? Type 3: Complex Anaphor "Synchronized Swimming": What country gave origin to this sport? "Abraham &amp; Mary Lincoln":</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,54.00,474.97,231.66,174.00"><head>When did she marry him? Type 4: No Anaphor nor topic in Question "Philippines":When was the Leyte Gulf invasion? "Climate": When was the last Decadal Oscillation?</head><label></label><figDesc></figDesc><table coords="3,54.00,534.93,231.66,114.04"><row><cell>Type 5: Reference to Previous Answer</cell></row><row><cell>"Spain": Who did Spain want extradited? Where was</cell></row><row><cell>he prior to extradition?</cell></row><row><cell>"Guerrilla": Who are the founders of ...? What is their</cell></row><row><cell>nickname?</cell></row><row><cell>Type 6: Reference to Previous Verbage</cell></row><row><cell>"Venus": When Venus crosses the sun is known as what</cell></row><row><cell>type of eclipse? Prior to 1999, when did the last one</cell></row><row><cell>occur?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,54.00,432.50,227.16,108.00"><head>Table 1 : Effectiveness of Web Validation on Previous QA Data</head><label>1</label><figDesc></figDesc><table coords="8,54.00,465.50,225.92,75.00"><row><cell></cell><cell cols="2">Trec 9 (492)</cell><cell cols="2">Trec2003(380)</cell></row><row><cell></cell><cell>#Corr</cell><cell>MRR</cell><cell>#Corr</cell><cell>MRR</cell></row><row><cell>Baseline</cell><cell>101</cell><cell>.277</cell><cell>49</cell><cell>.190</cell></row><row><cell>Web-Validated</cell><cell>119</cell><cell>.310</cell><cell>73</cell><cell>.231</cell></row><row><cell>(Improvement)</cell><cell cols="4">(+18%) (+12%) (+49%) (+22%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,310.97,237.84,221.65,113.00"><head>Table 2 : TREC 2004 Performance</head><label>2</label><figDesc></figDesc><table coords="8,310.97,258.84,221.65,92.00"><row><cell>Strategy</cell><cell>Factoid List</cell><cell>Other</cell><cell>All</cell></row><row><cell>Knowledge Graph+</cell><cell cols="3">0.204 0.071 0.367 0.211</cell></row><row><cell>Filter Cascade (#1)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>System #1+</cell><cell cols="3">0.187 0.098 0.355 0.207</cell></row><row><cell>Web Validation (#2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Knowledge Graph+</cell><cell cols="3">0.183 0.104 0.062 0.133</cell></row><row><cell>Web Validation (#3)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,310.34,333.84,224.96,169.00"><head>Table 3 : Comparing performance by question type</head><label>3</label><figDesc></figDesc><table coords="9,310.34,354.84,224.96,148.00"><row><cell></cell><cell>Total</cell><cell cols="2">w/o Validation w/ Validation</cell></row><row><cell>Abbrev</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>Amount</cell><cell>31</cell><cell>7</cell><cell>7</cell></row><row><cell>Location</cell><cell>28</cell><cell>8</cell><cell>4</cell></row><row><cell>Miscellany</cell><cell>15</cell><cell>1</cell><cell>1</cell></row><row><cell>Process</cell><cell>2</cell><cell>0</cell><cell>0</cell></row><row><cell>Time</cell><cell>48</cell><cell>19</cell><cell>19</cell></row><row><cell>What</cell><cell>66</cell><cell>6</cell><cell>7</cell></row><row><cell>Who</cell><cell>39</cell><cell>6</cell><cell>5</cell></row><row><cell></cell><cell>230</cell><cell>47</cell><cell>43</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">ACKNOWLEDGMENTS</head><p>The authors would like to thank <rs type="person">Carol Vaness-Dykema</rs> for her efforts to help establish and oversee the foundations of the QACTIS effort. Additional appreciation goes to <rs type="person">John Prange</rs> and the <rs type="programName">AQUAINT program</rs> for providing funding for portions of the work involved in this effort.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_txYTyKq">
					<orgName type="program" subtype="full">AQUAINT program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,69.58,300.84,216.08,9.00;10,71.14,312.84,214.52,9.00;10,71.14,324.84,43.61,9.00" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,136.78,300.84,101.77,9.00">Overview of TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Vorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Gaithersburg, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.02,336.84,217.64,9.00;10,71.14,348.84,214.52,9.00;10,71.14,360.84,214.52,9.00;10,71.14,372.84,22.50,9.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,129.79,348.84,147.74,9.00">WordNet: An online lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,71.14,360.84,153.94,9.00">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.12,384.84,217.54,9.00;10,71.14,396.84,214.52,9.00;10,71.14,408.84,214.52,9.00;10,71.14,420.84,58.33,9.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,268.12,384.84,17.54,9.00;10,71.14,396.84,214.52,9.00;10,71.14,408.84,37.21,9.00">Text Retrieval via Semantic Forests</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,123.26,408.84,105.37,9.00">NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="761" to="773" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>TREC-6, Gaithersburg, MD</note>
</biblStruct>

<biblStruct coords="10,71.15,432.84,93.47,9.00;10,203.02,432.84,82.64,9.00" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Lemur</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>System</surname></persName>
		</author>
		<ptr target="http://www-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,69.77,456.84,215.89,9.00;10,71.14,468.84,212.06,9.00" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,131.21,456.84,146.92,9.00">A maximum-entropy inspired parser</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<idno>CS-99-12</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,68.40,480.84,217.26,9.00;10,71.14,492.84,214.52,9.00;10,71.14,504.84,20.00,9.00" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,230.38,480.84,55.28,9.00;10,71.14,492.84,122.56,9.00">An Algorithm that Learns What&apos;s in a Name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,207.75,492.84,73.56,9.00">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,68.02,516.84,217.64,9.00;10,71.14,528.84,214.52,9.00;10,71.14,540.84,214.52,9.00;10,71.14,552.84,94.99,9.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,177.88,516.84,107.78,9.00;10,71.14,528.84,214.52,9.00;10,71.14,540.84,56.45,9.00">Character N-Gram Tokenization for European Language Text Retrieval: Special Issue on CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,140.54,540.84,85.87,9.00">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.57,564.84,213.09,9.00;10,71.14,576.84,214.52,9.00;10,71.14,588.84,214.52,9.00;10,71.14,600.84,214.52,9.00;10,71.14,612.84,92.49,9.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,192.52,564.84,93.14,9.00;10,71.14,576.84,142.55,9.00">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.10,576.84,58.56,9.00;10,71.14,588.84,214.52,9.00;10,71.14,600.84,159.00,9.00">Proc. of 21st Annual Int&apos;l ACM SIGIR Conference of Research and Development in Information Retrieval</title>
		<meeting>of 21st Annual Int&apos;l ACM SIGIR Conference of Research and Development in Information Retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,69.15,624.84,216.51,9.00;10,71.14,636.84,214.52,9.00;10,71.14,648.84,114.35,9.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,118.02,624.84,159.98,9.00">TnT -a statistical part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,71.14,636.84,214.52,9.00;10,71.14,648.84,84.18,9.00">Proc. of the Sixth Applied Natural Language Processing Conference ANLP</title>
		<meeting>of the Sixth Applied Natural Language essing Conference ANLP</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.30,660.84,210.36,9.00;10,71.14,672.84,214.52,9.00;10,71.14,684.84,214.52,9.00;10,71.14,696.84,214.52,9.00;10,71.14,708.84,96.10,9.00" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,75.97,672.84,209.69,9.00;10,71.14,684.84,170.87,9.00">Comparing Statistical and Content-Based Techniques for Answer Validation on the Web</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.78,684.84,17.88,9.00;10,71.14,696.84,152.55,9.00">Proceedings of the VIII Convegno AI*IA</title>
		<meeting>the VIII Convegno AI*IA<address><addrLine>Siena -Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">September 11-13, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
