<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.76,84.20,407.25,15.49;4,186.52,337.92,9.96,8.97;4,213.88,337.92,9.96,8.97;4,238.96,337.92,9.96,8.97;4,264.05,337.92,9.96,8.97;4,67.32,349.92,50.38,8.97;4,154.06,349.92,13.28,8.97;4,186.42,349.92,9.96,8.97;4,213.78,349.92,9.96,8.97;4,238.87,349.92,9.96,8.97;4,263.95,349.92,13.28,8.97;4,67.32,361.92,50.38,8.97;4,154.06,361.92,13.28,8.97;4,186.42,361.92,9.96,8.97;4,213.78,361.92,35.12,8.97;4,264.03,361.92,9.96,8.97;4,67.32,373.80,57.03,8.97;4,154.11,373.80,13.28,8.97;4,186.47,373.80,9.96,8.97;4,213.83,373.80,60.28,8.97;4,67.32,385.80,53.71,8.97;4,154.03,385.80,13.28,8.97;4,186.39,385.80,37.40,8.97;4,238.91,385.80,9.96,8.97;4,263.99,385.80,9.96,8.97;4,67.32,397.80,60.35,8.97;4,154.07,397.80,13.28,8.97;4,186.43,397.80,37.40,8.97;4,238.96,397.80,35.12,8.97;4,67.32,409.68,60.90,8.97;4,154.02,409.68,13.28,8.97;4,186.38,409.68,62.56,8.97;4,264.06,409.68,9.96,8.97;4,67.32,421.68,67.55,8.97;4,154.07,421.68,13.28,8.97;4,186.43,421.68,87.72,8.97;4,67.32,433.56,100.02,8.97;4,186.42,433.56,101.00,8.97">Microsoft Cambridge at TREC-13: Web and HARD tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,69.72,116.59,73.92,10.76"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
						</author>
						<author>
							<persName coords="1,169.32,116.59,68.74,10.76"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
						</author>
						<author>
							<persName coords="1,266.64,116.59,72.97,10.76"><forename type="first">Michael</forename><surname>Taylor</surname></persName>
						</author>
						<author>
							<persName coords="1,368.28,116.59,54.80,10.76"><forename type="first">Suchi</forename><surname>Saria</surname></persName>
						</author>
						<author>
							<persName coords="1,451.56,116.59,90.67,10.76"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
						</author>
						<title level="a" type="main" coord="1,101.76,84.20,407.25,15.49;4,186.52,337.92,9.96,8.97;4,213.88,337.92,9.96,8.97;4,238.96,337.92,9.96,8.97;4,264.05,337.92,9.96,8.97;4,67.32,349.92,50.38,8.97;4,154.06,349.92,13.28,8.97;4,186.42,349.92,9.96,8.97;4,213.78,349.92,9.96,8.97;4,238.87,349.92,9.96,8.97;4,263.95,349.92,13.28,8.97;4,67.32,361.92,50.38,8.97;4,154.06,361.92,13.28,8.97;4,186.42,361.92,9.96,8.97;4,213.78,361.92,35.12,8.97;4,264.03,361.92,9.96,8.97;4,67.32,373.80,57.03,8.97;4,154.11,373.80,13.28,8.97;4,186.47,373.80,9.96,8.97;4,213.83,373.80,60.28,8.97;4,67.32,385.80,53.71,8.97;4,154.03,385.80,13.28,8.97;4,186.39,385.80,37.40,8.97;4,238.91,385.80,9.96,8.97;4,263.99,385.80,9.96,8.97;4,67.32,397.80,60.35,8.97;4,154.07,397.80,13.28,8.97;4,186.43,397.80,37.40,8.97;4,238.96,397.80,35.12,8.97;4,67.32,409.68,60.90,8.97;4,154.02,409.68,13.28,8.97;4,186.38,409.68,62.56,8.97;4,264.06,409.68,9.96,8.97;4,67.32,421.68,67.55,8.97;4,154.07,421.68,13.28,8.97;4,186.43,421.68,87.72,8.97;4,67.32,433.56,100.02,8.97;4,186.42,433.56,101.00,8.97">Microsoft Cambridge at TREC-13: Web and HARD tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">962FB223D121047687C91865B82E79C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>All our submissions from the Microsoft Research Cambridge (MSRC) team this year continue to explore issues in IR from a perspective very close to that of the original Okapi team, working first at City University of London, and then at MSRC.</p><p>A summary of the contributions by the team, from TRECs 1 to 7 is presented in <ref type="bibr" coords="1,139.40,241.67,10.49,8.97" target="#b2">[3]</ref>. In this work, weighting schemes for ad-hoc retrieval were developed, inspired by a probabilistic interpretation of relevance; this lead, for instance, to the successful BM25 weighting function. These weighting schemes were extended to deal with pseudo relevance feedback (blind feedback). Furthermore, the Okapi team participated in most of the early interactive tracks, and also developed iterative relevance feedback strategies for the routing task.</p><p>Following up on the routing work, TRECs 7-11 submissions dealt principally with the adaptive filtering task; this work is summarised in <ref type="bibr" coords="1,152.98,362.04,10.58,8.97" target="#b4">[5]</ref>. Last year MSRC entered only the HARD track, concentrating on the use of the clarification forms <ref type="bibr" coords="1,79.16,386.04,10.49,8.97" target="#b5">[6]</ref>. We hoped to make use of the query expansion methods developed for filtering in the context of feedback on snippets in the clarification forms. However, our methods were not very successful.</p><p>In this year's TREC we took part in the HARD and WEB tracks. In HARD, we tried some variations on the process of feature selection for query expansion. On the WEB track, we investigated the combination of information from different content fields and from link-based features.</p><p>Section 3 briefly describes the system we used. Section 4 describes our HARD participation and Section 5 our TREC participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System</head><p>The system is the Keenbow experimental environment as described in <ref type="bibr" coords="1,97.00,593.52,10.49,8.97" target="#b5">[6]</ref>. The experiments described here were run using Keenbow on a Microsoft SQL Server, running on an Intel </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quad 700MHz</head><p>Xeon with 3GB RAM. The basic ranking algorithm in Keenbow is the usual Okapi BM25. The collections were preprocessed in a standard manner, using a 126 stopword list and the Porter stemmer (where stemming is used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HARD Track</head><p>For the experiments we submitted to this year's HARD track, we concentrated on methods for query expansion to improve relevance feedback. More formally, our experiments addressed the following problem: given a fixed ranking function , a query É, ranked list of documents É ranked with respect to É, and few ´¼ µ snippets corresponding to documents in É marked by the user as relevant to the query, how can we use the relevant snippets to improve the ranked list É ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature selection methods</head><p>We are interested in two problems: i) selecting new terms to be added to the query, and ii) weighting these terms. For the selection problem, we investigate the use of several functions. We call these functions feature selection functions (noted ´Ø µµ. For the weighting problem we are using standard RSJ feedback weights, except that terms in the query are given artificially high Ê and Ö counts.</p><p>We tried two types of feature selection functions: relative and absolute. Relative feature selection measures produce an ordering on the candidate terms, but they do not give us an indication on the number of terms to be included. The magnitude of the value ´Ø µ depends on the query in an unknown way. We use relative functions by deciding on a fixed number of expansion terms for all queries. Robertson Selection Value is the only relative function we used.</p><p>Absolute feature selection values, on the other hand, can be tested against a threshold (query-independent) to decide how many terms to be included. Typically, queries with a large number of relevant documents can select more terms for inclusion.</p><p>Before discussing the different feature selection functions we used, we introduce some useful notation: ¯AE is the total number of documents in the corpus.</p><p>¯ Î is the number of distinct terms in the corpus.</p><p>¯Ê is the total number of relevant documents (for a fixed query).</p><p>¯Ö is the number of relevant documents in which term Ø appears.</p><p>¯Ò is the total number of documents in which term Ø appears.</p><p>¯Probability of term Ø in relevant class: Ô Ö Ê .</p><formula xml:id="formula_0" coords="2,63.96,148.76,237.07,78.24">¯Probability of term Ø in non-relevant class: Ô Ò Ö AE Ê . ¯Probability of term Ø in corpus: Ô ¼ Ò AE . ¯ Ê Ö Ê Ö ´Ê Ö µ is the number of combinations of Ö in R.</formula><p>We experimented with the following feature selection functions:</p><formula xml:id="formula_1" coords="2,63.96,273.08,227.40,280.92">¯Robertson Selection Value (RSV) [2] ´Ø µ Ố Ô µÐÓ ´Ô ´½ Ô µ ´½ Ô µ Ô µ Ö ÐÓ ´Ô ´½ Ô µ ´½ Ô µ Ô µ ¯Significance Rule (SGN) [4] ´Ø µ Ö ÐÓ AE Ò ÐÓ ´ Ê Ö µ ÐÓ Î ¯Maximum Likelihood Decision Rule (MLDR) ´Ø µ Ö ÐÓ ´Ô ´½ Ô µ ´½ Ô µ Ô µ • Ê ÐÓ ´½ Ô µ • Ò ÐÓ Ô Ô ¼ ¯Simplified Kullback-Leibler Distance (KLD) [1] ´Ø µ Ô ÐÓ Ô Ô ¯Modified Chi-Squared (CHI2) [1] ´Ø µ ´Ô • Ô µ ¾ Ô</formula><p>The MLDR derives from a maximum likelihood argument, which will be explored in a later paper. The KLD is a simplification of the Kullback-Leibler distance between two probability distributions. In this case the distributions relate to the events of (presence, absence) of a given term. The full KL distance would be</p><formula xml:id="formula_2" coords="2,119.64,646.88,111.36,20.40">Ô ÐÓ Ô Ô • 1 Ô µ ÐÓ ½ Ô ½ Ô</formula><p>but the second term is ignored here, as is the fact that the KL distance is asymmetric between the two distributions. The simplification also has the effect that terms will not be selected because they are good indicators of non-relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on Feature Selection</head><p>To test for the performance of the feature selection functions for query expansion, we made an experimental setup similar to the HARD'04 test scenario. We generated clarification forms with snippets and first passages of the top five retrieved documents for each query in the HARD'04 training set. We marked the snippets as relevant only if the corresponding document in the corpus had a positive relevance judgment. Using this, we extracted only queries ´¿¼ ¿¼ ¿¼ ¿½¿ ¿½ ¿½ ¿¾¼ ¿¾ ¿¾ ¿¾ µ for which we got one or more relevant snippets and used this as the set to test for query expansion using different feature selection functions. Figure <ref type="figure" coords="2,415.86,208.08,4.98,8.97" target="#fig_0">1</ref> shows the plot for mean average precision against the number of words in the expanded query for these 10 topics.</p><p>It may be noted that all selection value formulae produce more-or-less comparable peaks. Although two other measures show slightly higher peaks, KL distance (KLD) has least variation in the mean average precision for the different number of words added; in other words, it seems to be less susceptible than the others to non-optimal setting of the threshold. We tested these measures as well on the Reuters Vol.1 corpus and HARD'03 obtaining similar results. For example, DLF consistently gave high mean average precision on query expansion using (1-3) documents for queries in the Reuters collection. Hence, we chose to use KLD for query expansion on our submitted runs to HARD04.</p><p>Intuitively, KL distance values the dissimilarity between the distribution of the term in the relevant class and the entire corpus. In other words, words that are infrequent in the corpus but frequently present in the relevant documents are considered most informative of relevance and hence best candidate terms for inclusion to the query. However, as with all the absolute methods, for a given score threshold there is large variance in the number of words selected for different queries . In particular, we found that it often over-estimates the number of terms to be included in the query. For this reason, we introduced a second parameter which limits the maximum number of words to be included in a query (noted maxT).</p><p>Figure <ref type="figure" coords="2,351.09,531.96,4.98,8.97" target="#fig_1">2</ref> shows the mean average precision over queries when using the KLD selection function against an absolute threshold (plotted in the x-axis), for different values of maxT. The 'infinity' line corresponds to the basic KLD measure without maxT threshold. Note that the use of maxT is beneficial and obtains te best average precision (on the training set at least).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metadata and clarification forms</head><p>The clarification forms contained three forms of data: Snippets, Good Phrases and Bad Phrases. That is, we showed the user a snippet from each of the five top-ranked documents from the baseline search (passage retrieval methods were used to identify appropriate snippets, but we made no further passage retrieval runs, and did not make submissions for the passage  retrieval evaluation). Users were asked whether they would click on each snippet; responses 'Yes' or 'Perhaps' or 'No need' were taken as relevant (the last category means that the user could infer the answer they wanted directly from the snippet, without going to the original document). In addition, they were invited to add up to five words or phrases indicating Good documents, and up to five indicating Bad documents.</p><p>We used these in various ways and in various combinations for feedback. We made minimal use of the metadata as we were primarily interested in assessing relevance feedback. The only form of metadata we used besides the query title, was the 'Description' field. Often, the query title did not convey sufficient information on which the relevance judgements were made. For example, the title for Topic 422 was "Video Game Crash" but the description in the data was "Is the market for interactive software just waiting to crash?" Since our original search was based only on the query title, our results presented on the clarifications forms discussed video game crashes instead of market crash for video game software and hence were all marked as non-relevant. We used the description meta-data along with the query to generate results for such queries in the final run.</p><p>More specifically, metadata and feedback data were used as follows in the submitted runs: If good phrases were used, they were added to the query. If bad phrases were used, they were removed from the query. If snippets were used, all the terms of the snippets were considered for inclusion using the feature selection algorithm described bellow. Finally, if descriptions were used and there were no relevant snippets and there were no good phrases, all the terms in the description were added to the query.</p><p>Terms considered for inclusion (from relevant snippets) were selected by the following procedure (as described in the previous section): PROCEDURE FeatureSelectKLD ( S = set of all words in relevant snippets , ST = score threshold , MaxT = maximum number of terms to be included , )</p><formula xml:id="formula_3" coords="3,321.00,593.76,216.88,116.49">newSet = FOREACH( Ø ¾ Ë ) IF (Ø ST) newSet = newSet Ë ´Ø SKLD´Ø µµ IF ( size(newSet) MaxT) newSet = SORT(newSet, SKLD(Ø ), DESCENDING) newSet=newSet(1..MaxT) RETURN newSet</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Submitted runs</head><p>The parameters we used for the runs include:</p><p>¯BM25 parameters: k1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>Results are shown in Tables <ref type="table" coords="4,166.99,561.84,4.98,8.97" target="#tab_2">2</ref> and<ref type="table" coords="4,191.15,561.84,3.74,8.97">3</ref>.</p><p>The results may be summarised as follows. Query expansion from snippets selected by the user helped this year (in contrast to last year when we failed to get any benefit). Good phrases also helped, even in the relatively simple-minded way that we used them (just to add extra single terms). Bad phrases (in the way that we used them here) did not help; there is clearly much scope for looking at different ways of using this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WEB Track</head><p>In the Web Track we focus on three types of evidence: 2. Link recommendation: PageRank or ClickDistance (defined as the minimum number of hyper-links one needs to follow to go from http://firstgov.gov to the page).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">URL depth: length of URL in characters.</head><p>For the text, we use a new variant of BM25 where weights and length normalisation parameters are distinct per-field (this is discussed in Section 4.1). For link-based and URL features we employ new combination functions, which we find by analysing the relevant and retrieved documents for a set of training queries (this is discussed in Section 4.2).</p><p>We deal with the mixed HP-NP-TD query stream through tuning. We have examples of all three types from TREC-2003, and so we conducted several tuning runs, looking at performances specifically to each task and overall (this is discussed in Section 4.3) The main difference we found across query types is that stemming helps TD and hurts the other two, so we vary the stemming across our runs. Our submission runs and results are discussed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Integrating Content Features Across Multiple Fields</head><p>refer to the different annotated parts of a document, such as title and body, as document fields. Furthermore we use the term anchor field to refer to all the anchor text in the collection pointing to a particular document.</p><p>In previous work we showed that combining BM25 scores across fields can lead to a dangerous over-estimation of the importance of the term <ref type="bibr" coords="5,146.32,161.64,10.49,8.97" target="#b6">[7]</ref>. We proposed to combine the termfrequencies (weighting them accordingly to their field importance) and using the resulting pseudo-frequency in the BM25 ranking function. Furthermore we showed how to adapt automatically the parameters Ã ½ and to changes in the weights.</p><p>In this submission we have modified slightly this approach to take into account fields of extremely different field lengths (such as those of the title and anchor). In particular, we have modified the function so that it can use a different length normalising factor for every field-type. This is done by computing a field-dependant normalised term-frequency:<ref type="foot" coords="5,118.56,291.48,3.49,6.28" target="#foot_0">1</ref> </p><formula xml:id="formula_4" coords="5,66.48,302.48,234.21,52.08">Ü Ø Ü Ø ´½ • ´Ð Ð ½µµ ¾ BODY, TITLE, ANCHOR indicates the field type,</formula><p>Ü Ø is the term frequency of term Ø in the field type of document , Ð is the length of that field, and Ð is the average field length for that field type. is a field-dependant parameter similar to the parameter in BM25. In particular, if ¼ there is no normalisation and if ½ the frequency is completely normalised w.r.t. the average field length.</p><p>These term frequencies can then be combined in a linearly weighted sum to obtain the final term pseudo-frequency, which is then used in the usual BM25 saturating function. This leads the following ranking function, which we refer to as BM25F:</p><formula xml:id="formula_5" coords="5,109.20,479.36,144.12,61.68">Ü Ø Ï ¡ Ü Ø Å¾ ´ µ Ø¾Õ Ü Ø Ã ½ • Ü Ø Û ´½µ Ø</formula><p>where Û ´½µ Ø is the usual RSJ relevance weight for term Ø, which reduces to an idf weight in the absence of relevance information (note that this does not use field information). Note that this function requires one and one Ï parameter per field, plus a single saturating parameter Ã ½ . This constitutes a total of ´ £ ¾ • ½ µ parameters. Because of the dependence structure fo the parameters, we can brake down the global optimisation into smaller optimisation problems of one or two parameters. The optimisation procedure that we followed to is as follows:</p><p>¯ : Independently for every field (setting the other field weights to zero), optimise and Ã½. ¯Ã½ : Setting all field weights Ï to 1, and using all the values previously found, optimise Ã½.</p><p>¯Ï : Setting the body weight to 1, the previously found values of and Ã½, optimise the weights Ï for the title and anchor fields (adapting Ã ½ to each weight setting, as indicated in <ref type="bibr" coords="5,390.73,293.64,10.38,8.97" target="#b6">[7]</ref>). This constitutes ( • ½ ) optimisations in 2 dimensions (2D) and one optimisation in 1D. 2D and 1D optimisations were done by a robust line-search type algorithm, optimising Precision@10 on a set of training topics.</p><p>In order to evaluate our approach against standard BM25 and <ref type="bibr" coords="5,328.79,375.00,11.50,8.97" target="#b6">[7]</ref> we used 4-fold cross-validation over a set of TREC topics. We did this with the 2003 Name Page (NP) and 2003 Topic Distillation (TD) topics sets separately (each has 50 topics). A full optimisation run took approximately 3 days to complete (running on top of Keenbow without making any effort to optimise the code for this task). The resulting performances were significantly better for a range of measures, so we decided to use BM25F for our final submissions.</p><p>Since the variance of the parameters was not large across cross-validation sets, we used for our final submissions the average parameter values obtained in the cross-validation. The values of these parameters are indicated in table 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Integrating Document Features</head><p>Our new combination functions are suggested based on analysis of the relevant and retrieved documents for a set of training queries. Examining such sets is reminiscent of Singhal et al <ref type="bibr" coords="5,311.04,592.92,10.49,8.97" target="#b7">[8]</ref>.</p><p>In general, we address the problem of choosing a score contribution function for adding static scores Ë to existing document scores , with ÓÑ Ò Ë ÓÖ</p><p>• ´Ëµ. As an illustration, we consider the case were is BM25 and Ë is PageRank (see Figure <ref type="figure" coords="5,400.45,653.16,3.60,8.97" target="#fig_3">3</ref>).</p><p>For a set of training queries we identify two sets of documents. The relevant set Ê is the set of known relevant documents for the training queries. The retrieved set Ì is the set of top-ranked documents if we retrieve using only. In this example we use the top Ö documents for each query, where Ö is the number of known-relevant documents for that query, which makes Ì the same size as Ê.</p><p>Assuming that and Ë are independent, the correct score contribution for Ë × is ÐÓ È´Ë × Êµ È´Ë × Êµ . In practice we ap- proximate the set of irrelevant documents Ê using the whole collection . Based on our training set, the score contribution is given by the line labelled Ê in Figure <ref type="figure" coords="6,236.31,469.44,3.74,8.97" target="#fig_3">3</ref>. The upwards slope of the line indicates that PageRank is a useful relevance indicator.</p><p>However, and Ë are not independent, and we can see this if we plot Ì in the same manner (also Figure <ref type="figure" coords="6,234.54,517.20,3.60,8.97" target="#fig_3">3</ref>). The upwards slope of the line indicates that BM25 is already retrieving high-PageRank pages. The difference between the curves Ê Ì suggests the shape of . We tried log PageRank, since Ê Ì looks quite straight in the region of interest:</p><formula xml:id="formula_6" coords="6,111.00,579.68,190.06,15.84">´È Ê Ò µ Û ÐÓ ´È Ê Ò µ (1)</formula><p>We also tried a sigmoid of log PageRank, since Ê Ì does appear to flatten out. Tuning the sigmoid of log PageRank gave us our final ´Ëµ for PageRank:</p><formula xml:id="formula_7" coords="6,95.40,638.84,205.66,27.60">´È Ê Ò µ Û ½ • ´ ÐÓ ´È Ê Ò µ• µ (2)</formula><p>although due to time constraints we fixed ½ .</p><p>Similar analysis for Ë Ð ×Ø Ò suggested:</p><p>´ Ð ×Øµ Û</p><p>• Ð ×Ø </p><formula xml:id="formula_9" coords="6,362.64,215.84,195.34,22.26">´ÍÊÄÄ Ò Ø µ Û • ÍÊÄÄ Ò Ø<label>(4)</label></formula><p>and this performed better than the negative linear function used in previous experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Preliminary Experiments</head><p>To evaluate the combination of content and link features we used a mixed set of 120 TREC 2003 Web queries ( 40 from TD, 40 from NP and 40 from Home Page (HP)). We used the BM25F parameters obtained optimising Prec@10 on TD and NP. We tuned the PageRank (PR) weights Û and (keeping ½) and similarly we tuned and Û for ClickDistance (CD). URL-length (URLl) parameters were tuned for PageRank and ClickDistance afterwards on the same set (we checked the risk of over-fitting by validating on the remaining 2003 queries). Finally, we tested several rank combination methods without much success. The only interesting result we found was to interleave the documents of two disparate runs (removing duplicates from the bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Submitted Runs and Results</head><p>Our submission runs were: ¯MSRC04B1S: BM25 NP tuning + PR + URLl ¯MSRC04B2S: BM25 NP tuning stem + PR + URLl ¯MSRC04B1S2: BM25 NP tuning + CD + URLl ¯MSRC04B3S: BM25 TD tuning + CD ¯MSRC04C12: interleave MSRC04B1S and MSRC04B2S</p><p>Our results are summarised in Table <ref type="table" coords="6,468.49,629.63,3.74,8.97" target="#tab_3">5</ref>. Based on our training runs we expected stemming to help TD and hurt the other two. Actually the first two lines of the table show that turning on stemming helped slightly across all tasks. Since the differences were small, interleaving the two (MSRC04C12) only had the effect of adding some (slightly positive) noise. The runs with click distance were somewhat worse than the PageRank runs, but they performed two do similar jobs.</p><p>In the HARD Track, we have succeeded in our relevance feedback mechanisms (in particular, the rules for query expansion) in such a way that we can benefit from user feedback on snippets. Other matters have been left in abeyance, including (a) the use of active learning techniques for choosing the snippets to present to the user (as we did last year), (b) the use of negative information (words or phrases specified by the user as negative indicators of relevance), and (c) passage retrieval.</p><p>In the Web Track we did not find new forms of evidence or new ways of dealing with mixed query streams. Instead we concentrated on dealing well with the different text fields in BM25 and adding static information using appropriate functions. It appears that there was still room for gains in these areas, and our final results were very satisfactory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,54.00,323.76,245.94,8.97;3,54.00,335.64,245.74,8.97;3,54.00,347.64,246.11,8.97;3,54.00,359.52,57.80,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Mean average precision variation as selected words are added to the query. The baseline result of search using only the original query is shown as the first data point with number of words = 2.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,54.00,671.88,245.97,8.97;3,54.00,683.76,245.65,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean average precision variation as the KLD threshold is varied, for different number of maximum terms (maxT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,372.00,59.28,40.27,8.97;5,432.31,59.28,64.33,8.97;5,372.00,66.08,8.52,15.84;5,380.52,75.98,3.96,4.56;5,437.04,71.16,17.43,8.97;5,477.51,71.16,12.45,8.97;5,379.56,82.04,21.12,12.48;5,437.04,83.16,17.43,8.97;5,477.51,83.16,12.45,8.97;5,385.92,93.92,6.00,12.48;5,439.56,95.04,12.45,8.97;5,477.57,95.04,12.45,8.97;5,385.56,105.92,32.52,12.48;5,439.56,107.04,12.45,8.97;5,477.57,107.04,12.45,8.97;5,372.00,113.96,30.48,16.44;5,437.04,119.04,17.43,8.97;5,475.11,119.04,17.43,8.97;5,372.00,125.84,21.72,16.44;5,439.56,130.92,12.45,8.97;5,477.57,130.92,12.45,8.97;5,372.00,137.84,47.88,16.44;5,440.76,142.92,9.96,8.97;5,475.20,142.92,17.43,8.97;5,311.04,169.56,245.83,8.97;5,311.04,181.56,246.33,8.97;5,311.04,193.44,76.53,8.97"><head>Table 4 :</head><label>4</label><figDesc>BM25F parameters obtained by optimising the Topic Distillation (TD) and Name Page (NP) TREC tasks of 2003. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,54.00,296.04,246.07,8.97;6,54.00,307.92,89.82,8.97;6,204.59,307.92,95.15,8.97;6,54.00,321.12,134.49,8.97;6,197.04,313.76,35.88,12.48;6,197.04,322.52,35.88,12.48;6,237.84,321.12,63.05,8.97;6,54.00,338.84,7.80,7.68;6,69.72,330.44,35.64,12.48;6,69.72,339.20,35.64,12.48;6,110.28,337.80,189.83,8.97;6,54.00,351.36,245.70,8.97;6,54.00,363.36,60.32,8.97;6,116.72,358.28,29.61,15.84;6,57.32,43.96,243.51,236.10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Web Track: Analysis for finding the score contribution function for The horizontal axis is log(PageRank). The plots are ÐÓ È´Ë × Êµ È´Ë × Êµ (labelled as R), ÐÓ È´Ë × Ìµ È´Ë × Ìµ (labelled as T) and the difference between the two (R-T). The choice of score contribution function is based on the shape of Ê Ì.</figDesc><graphic coords="6,57.32,43.96,243.51,236.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,318.48,64.08,211.87,367.04"><head>Table 2 :</head><label>2</label><figDesc>HARD Track, hard relevance evaluation</figDesc><table coords="4,318.48,85.20,211.87,345.93"><row><cell></cell><cell>AveP</cell><cell>P@10 RPrec</cell></row><row><cell>MSRCBaseline</cell><cell cols="2">0.2098 0.2733 0.2336</cell></row><row><cell>MSRCh4SD</cell><cell cols="2">0.2511 0.3533 0.2539</cell></row><row><cell>MSRCh4SG</cell><cell cols="2">0.2581 0.3400 0.2752</cell></row><row><cell>MSRCh4SGB</cell><cell cols="2">0.2585 0.3400 0.2763</cell></row><row><cell>MSRCh4SSn</cell><cell cols="2">0.2428 0.2622 0.2621</cell></row><row><cell>MSRCh4SSnB</cell><cell cols="2">0.2396 0.2489 0.2557</cell></row><row><cell>MSRCh4SSnG</cell><cell cols="2">0.2836 0.3044 0.2981</cell></row><row><cell>MSRCh4SSnGB</cell><cell cols="2">0.2839 0.3044 0.2992</cell></row><row><cell cols="3">MSRCh4SSnGBD 0.2841 0.3111 0.2966</cell></row><row><cell cols="3">Table 3: HARD Track, hard relevance evaluation</cell></row><row><cell></cell><cell>AveP</cell><cell>P@10 RPrec</cell></row><row><cell>MSRCBaseline</cell><cell cols="2">0.2077 0.3444 0.2409</cell></row><row><cell>MSRCh4SD</cell><cell cols="2">0.2544 0.4133 0.2857</cell></row><row><cell>MSRCh4SG</cell><cell cols="2">0.2490 0.4333 0.2875</cell></row><row><cell>MSRCh4SGB</cell><cell cols="2">0.2506 0.4289 0.2889</cell></row><row><cell>MSRCh4SSn</cell><cell cols="2">0.2329 0.3311 0.2591</cell></row><row><cell>MSRCh4SSnB</cell><cell cols="2">0.2284 0.3178 0.2525</cell></row><row><cell>MSRCh4SSnG</cell><cell cols="2">0.2612 0.3911 0.2938</cell></row><row><cell>MSRCh4SSnGB</cell><cell cols="2">0.2615 0.3911 0.2938</cell></row><row><cell cols="3">MSRCh4SSnGBD 0.2631 0.3978 0.2955</cell></row><row><cell cols="2">1. Text: title, body and anchor.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,311.04,59.24,246.10,147.78"><head>Table 5 :</head><label>5</label><figDesc>Web Track results. The 'average' is just an average of the other three columns.</figDesc><table coords="6,318.48,59.24,232.65,136.20"><row><cell>Run</cell><cell cols="4">Average TD MAP NP MRR HP MRR</cell></row><row><cell>MSRC04B1S</cell><cell>0.5392</cell><cell>0.159</cell><cell>0.719</cell><cell>0.741</cell></row><row><cell>MSRC04B2S</cell><cell>0.5461</cell><cell>0.162</cell><cell>0.731</cell><cell>0.745</cell></row><row><cell>MSRC04B1S2</cell><cell>0.4985</cell><cell>0.136</cell><cell>0.709</cell><cell>0.651</cell></row><row><cell>MSRC04B3S</cell><cell>0.4601</cell><cell>0.121</cell><cell>0.674</cell><cell>0.585</cell></row><row><cell>MSRC04C12</cell><cell>0.5458</cell><cell>0.165</cell><cell>0.724</cell><cell>0.749</cell></row><row><cell>Analysis for</cell><cell></cell><cell cols="3">Å¾ • È Ê Ò and Ë</cell></row></table><note coords="6,311.04,192.86,167.61,14.16"><p>ÍÊÄÄ Ò Ø suggested a similar function:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,68.40,705.16,234.15,7.17;5,54.00,714.64,249.01,7.17"><p>The equation for normalised term-frequency and that for BM25 contain errors in the printed version, which have been corrected in this online version.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,70.42,304.92,229.74,8.97;7,70.56,316.92,229.24,8.97;7,70.56,328.80,229.71,8.97;7,70.56,340.80,59.72,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,287.99,304.92,12.17,8.97;7,70.56,316.92,229.24,8.97;7,70.56,328.80,29.30,8.97">An information-theoretic perspective to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Moro</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,116.16,328.80,179.85,8.97">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.42,361.56,229.56,8.97;7,70.56,373.56,183.69,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,139.65,361.56,156.05,8.97">On term selection for query expansion</title>
		<author>
			<persName coords=""><surname>S E Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,70.56,373.56,102.66,8.97">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.42,394.32,229.69,8.97;7,70.56,406.20,229.41,8.97;7,70.56,418.20,229.90,8.97;7,70.56,430.08,228.96,8.97;7,70.56,442.08,22.42,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,192.25,394.32,107.85,8.97;7,70.56,406.20,3.74,8.97">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,256.05,406.20,43.91,8.97;7,70.56,418.20,229.90,8.97;7,70.56,430.08,57.74,8.97">The Eighth Text REtrieval Conference (TREC-8), NIST Special Publication 500-246</title>
		<editor>
			<persName><forename type="first">E M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.42,462.84,229.71,8.97;7,70.56,474.84,221.21,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,186.14,462.84,113.99,8.97;7,70.56,474.84,30.64,8.97">Threshold setting in adaptive filtering</title>
		<author>
			<persName coords=""><forename type="first">S E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,108.08,474.84,102.54,8.97">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="312" to="331" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.42,495.60,229.78,8.97;7,70.56,507.48,229.50,8.97;7,70.56,519.48,229.36,8.97;7,70.56,531.48,229.84,8.97;7,70.56,543.36,228.96,8.97;7,70.56,555.36,22.42,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,70.56,507.48,211.83,8.97">Microsoft Cambridge at TREC 2002: Filtering track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>S E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,230.56,519.48,69.36,8.97;7,70.56,531.48,229.84,8.97;7,70.56,543.36,57.74,8.97">The Eleventh Text REtrieval Conference, TREC 2002, NIST Special Publication 500-251</title>
		<editor>
			<persName><forename type="first">E M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.42,576.12,229.52,8.97;7,70.56,588.12,229.52,8.97;7,70.56,600.00,229.96,8.97;7,70.56,612.00,229.63,8.97;7,70.56,623.88,210.45,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,260.65,576.12,39.29,8.97;7,70.56,588.12,152.85,8.97">Microsoft Cambridge at TREC 2003: Hard track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>S E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,189.18,600.00,111.34,8.97;7,70.56,612.00,43.58,8.97">The Twelfth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct coords="7,70.42,644.64,229.50,8.97;7,70.56,656.64,229.83,8.97;7,70.56,668.64,229.21,8.97;7,70.56,680.52,50.95,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,271.69,644.64,28.23,8.97;7,70.56,656.64,180.65,8.97">Simple BM25 Extension to Multiple Weighted Fields</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>S E Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,258.89,656.64,41.50,8.97;7,70.56,668.64,224.26,8.97">Thirteenth Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,70.42,701.28,229.59,8.97;7,70.56,713.28,229.16,8.97;7,327.60,57.24,229.23,8.97;7,327.60,69.24,229.50,8.97;7,327.60,81.12,102.75,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,283.64,701.28,16.37,8.97;7,70.56,713.28,145.50,8.97">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,239.17,713.28,60.55,8.97;7,327.60,57.24,229.23,8.97;7,327.60,69.24,200.78,8.97">Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 19th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
