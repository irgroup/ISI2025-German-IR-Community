<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.92,93.40,427.47,12.93;1,297.48,109.36,64.76,12.93">Answering multiple questions on a topic from heterogeneous resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.76,138.15,60.87,10.77"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.48,138.15,94.08,10.77"><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.09,138.15,64.67,10.77"><forename type="first">Sue</forename><surname>Felshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.03,138.15,97.44,10.77"><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.12,152.07,114.15,10.77"><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.19,152.07,67.74,10.77"><forename type="first">Roni</forename><surname>Katzir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.27,152.07,61.56,10.77"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.11,152.07,78.41,10.77"><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.52,165.99,93.65,10.77"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.57,165.99,81.57,10.77"><forename type="first">Federico</forename><surname>Mora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.73,165.99,84.41,10.77"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.92,93.40,427.47,12.93;1,297.48,109.36,64.76,12.93">Answering multiple questions on a topic from heterogeneous resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">433E79A6040467B43E15CA885A272F79</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction MIT CSAIL's entry into this year's TREC Question Answering track focused on the conversational aspect of this year's task, on improving the coverage of our list and definition systems, and on an infrastructure to generalize our TREC-specific tools for other question answering tasks.</p><p>While our overall architecture remained largely unchanged from last year, we have built on our strengths for each component: our web-based factoid engine was adapted for input from a new web search engine; our list engine's knowledge base expanded from 150 to over 3000 lists; our definitional nugget extractor now has expanded and improved patterns with improved component precision and recall.</p><p>Beyond their internal improvements, these components were adapted to a larger conversational framework that passed information about the topic<ref type="foot" coords="1,176.52,507.73,4.19,5.56" target="#foot_0">1</ref> to factoids and lists. Answer selection for definitional<ref type="foot" coords="1,215.64,521.29,4.19,5.56" target="#foot_1">2</ref> questions newly took into account the prior questions and answers for duplicate removal.</p><p>Our factoid engine, Aranea <ref type="bibr" coords="1,244.07,563.38,76.76,10.91" target="#b8">(Lin et al., 2002;</ref><ref type="bibr" coords="1,103.20,576.94,81.92,10.91" target="#b5">Katz et al., 2003)</ref>, used the World Wide Web to find candidate answers to the given question, and then projects its best candidates onto the corpus, choosing the one best supported. This year, instead of using only Google for web search, we integrated results from the Teoma search engine as well.</p><p>Our list engine, Pauchok <ref type="bibr" coords="1,250.57,671.98,70.42,10.91;1,103.19,685.54,23.95,10.91" target="#b9">(Tellex et al., 2003)</ref>, retrieved passage-sized chunks of text relevant to the question using information re-trieval techniques, and projected onto them the fixed lists associated with the question focus. This year we used several new techniques and knowledge sources to gather many times more fixed lists than we had last year.</p><p>Our definition engine, Col. ForBIN <ref type="bibr" coords="1,523.36,298.54,28.12,10.91;1,338.40,312.10,89.81,10.91" target="#b3">(Hildebrandt et al., 2004;</ref><ref type="bibr" coords="1,431.81,312.10,78.20,10.91" target="#b2">Fernandes, 2004)</ref>, inspects the text collection for syntactic patterns often associated with a definitional context, and extracts pairs of targets and definitional nuggets. Topics are then matched against a database of target-nugget pairs. We have expanded the number of patterns and their complexity, yielding improved extraction performance, but changed target matching in a way that caused a net loss in accuracy. A new anaphorresolution engine improved our final score.</p><p>We have made several infrastructure improvements to the original AQUAINT data set: we made it XML-compliant, separated conjoined articles, extracted metadata, and removed meta-text. We also created a standoff XML annotation architecture for storing intermediate processing stages (e.g., POS tags), that was used by both the list and definition engines.</p><p>Many unforseen technical challenges forced us to cut integration and testing short, so that many new features were never compared to old ones. This caused, in some cases, no answers, or remarkably poor answers, which were easily fixed after the fact.</p><p>We will expand on each of these topics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Question Analysis</head><p>The test collection contained 65 "targets", which in this paper we will call "topics" to dif-ferentiate them from the "focus" (which has been called the "target" of a factoid or list question) of each individual question within the topic. The primary tool we used for analyzing each question in the context of its topic was the START Natural Language Question Answering System<ref type="foot" coords="2,140.76,168.37,4.19,5.56" target="#foot_2">3</ref>  <ref type="bibr" coords="2,150.00,169.54,88.99,10.91" target="#b4">(Katz et al., 2002;</ref><ref type="bibr" coords="2,244.14,169.54,25.87,10.91;2,52.20,183.10,24.55,10.91" target="#b6">Katz, 1988;</ref><ref type="bibr" coords="2,80.95,183.10,54.01,10.91" target="#b7">Katz, 1997)</ref>. Three of its internal functions were exposed in a TREC-specific API, and enhanced to work with a wider array of questions:</p><p>• Noun-phrase parsing for the topic itself,</p><p>• anaphoric substitution to place the topic into each question as appropriate, and</p><p>• focus extraction to find for each question the type of answer sought.</p><p>For example, START would analyze "boxer Floyd Patterson" as an occupation and a person, choosing to substitute only the name into a question: "How old was Floyd Patterson when Floyd Patterson won the title?", or "List the names of boxers Floyd Patterson fought." The algorithm is shown in Figure <ref type="figure" coords="2,215.22,411.94,4.19,10.91">1</ref>.</p><p>In the case above, the factoid question would get passed to Aranea with the occupation "boxer" appended: Aranea analyzes only the beginning of the question to find the expected answer-type, and uses just keywords thereafter.</p><p>The list question is more closely coupled, and our list engine Pauchok was told via the API that "boxer(s)" was the focus of the question, and separately that Floyd Patterson's occupation was boxer.</p><p>The definition processor was given the topic unanalyzed.</p><p>If the query-analysis algorithm failed to find a substitution for the topic into a query, then both the factoid and list engines simply appended the topic to the query for document retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Factoid Questions</head><p>We have been using the Aranea system for question answering for three years, and were</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">List Questions</head><p>We retrieved passage-sized chunks of text using information retrieval techniques, and projected fixed lists, whose annotations matched the question focus, onto the passages. The most significant change this year was our accumulation of 20 times the number of fixed lists used last year. The lists were compiled from several sources and provided the backbone of the list question answering mechanism. Examples of the lists extracted are given in Figure <ref type="figure" coords="2,496.73,309.34,4.19,10.91">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Focus Identification</head><p>The first step for answering list questions is to identify the focus of the question, the "target" in previous years' terminology, which is indicative of the expected answer type. After START has incorporated the topic into the question,<ref type="foot" coords="2,330.12,416.17,4.19,5.56" target="#foot_7">6</ref> it also identifies several candidate structures as possible focus candidates. It provides these to the list engine in the form of a list of strings ordered by specificity. <ref type="foot" coords="2,456.96,456.85,4.19,5.56" target="#foot_8">7</ref>For example the list for "Name famous people who have been Rhodes scholars" contained:</p><p>• "famous people who have been Rhodes scholars"</p><p>• "famous people"</p><p>• "Rhodes scholars"</p><p>• "people"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Noun-phrase Annotations</head><p>Second, each candidate may be associated with several fixed lists, compiled a priori by matching noun-phrase annotations for each list. Each of our 3301 separate lists has at least one associated noun-phrase annotation 1. Find generalizations of the topic • either from the structure of the topic e.g., "Hale-Bopp comet" is a "comet"; "senator Jim Inhofe" is a "senator",</p><p>• or from pre-existing knowledge of the name e.g., a "boll weevil" is a beetle, an insect, an arthropod, ...</p><p>2. Make one of the following substitutions, or return failure:</p><p>• any pronoun, respecting gender if available, but not number e.g., substitute "Hale-Bopp comet" for "it" in "How often does it approach the earth?"</p><p>• partial topic for whole topic, preserving possessive e.g., substitute full topic "Fred Durst" for "Durst" in "Where was Durst born?"</p><p>• topic for generalization e.g., substitute "the Berkman Center for Internet and Society", for "the center" in "Where is the center located?"</p><p>Figure 1: START's algorithm for query analysis.</p><p>that identifies the list, and is matched against the focus identified above. Continuing with the example above, we do not have a list of famous Rhodes scholars, nor of Rhodes scholars, but we do have lists of famous people (78000+ from START's preexisting biography.com knowledge source) and of people (using heuristic name matching).</p><p>If a list matched a focus, then elements of that list that appear in the retrieved passages are scored based on the rank of their passage.<ref type="foot" coords="3,316.56,461.17,4.19,5.56" target="#foot_9">8</ref> Items from each focus backoff are strictly preferred to items in later backoffs.</p><p>The fixed and dynamic lists that this method relies on are described in Sections 4.3 and 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Expanded Fixed Lists</head><p>Last year we used about 150 manuallycompiled lists in a similar list-answering process. This year we used nearly 3300 fixed lists. We semi-automatically extracted lists in three ways: we found hyponyms of words appearing in "Which X" context; we found common descriptions of people in first sentences of WorldBook Encyclopedia articles; and we used new semi-structured online resources to compile further categories.</p><p>Our process for "wrapping" semi-structured online resources is well described in papers about our Omnibase system <ref type="bibr" coords="3,490.92,353.86,65.28,10.91;3,338.40,367.42,23.95,10.91" target="#b4">(Katz et al., 2002)</ref>. This process contributed 171 of our lists.</p><p>The WorldBook Encyclopedia's first sentences often contain very salient descriptions, especially for people, that serve as category names. For example from the entry for Mac-Dowell: "MacDowell, Edward Alexander, was an American composer and pianist." we can put him into three categories of famous people: "American composer", "composer", and "pianist". We used a context-free grammar to parse all first-sentences for people, and with some manual cleanup generated 730 lists.</p><p>From the corpus itself, we selected category names by looking for "Which X" and "What X" surface patterns at the beginnings of sentences. We associated these category names with instances of their immediate WordNet hyponyms that appeared in the corpus. This process generated 11,000 lists, from which we manually selected 2360, based on a subjective coherence of the list elements, subjective quality of the list name as a description of the elements, and having more than one proper-noun list element. that could be used to ask about the list.</p><p>The final set of 3091 automatically extracted lists contain a total of 30,112 symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dynamic Lists</head><p>Some "lists" are actually scripts. For example the annotation that matches books by an author goes to Barnes-and-Noble.com to identify the author's works, and returns these works as the "fixed" list to work from.</p><p>In addition, if we did not have a noun-phrase annotation match for any of the focus backoffs, but any of the backoffs were in WordNet, then we treated its WordNet hyponyms as if they were a known fixed list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Guess Answers</head><p>Despite a far greater number of fixed lists than last year, we cannot anticipate every category a question might be asked about. When we do not match any annotation for a focus, we use heuristics to find reasonable candidates, called "guess answers".</p><p>A guess answer can be a noun phrase or a quoted title. To be selected, the guess answer must appear within a short fixed linear distance of a focus string.</p><p>For example, if we are asked for kinds of grapes, provided we have no list of grape types, we prefer the noun phrases "chardonnay grapes" and "grape juice" over "grapes like pinot and chardonnay", and those over "grapes of distinction like pinot and chardonnay", where the grape types receive no score.<ref type="foot" coords="4,500.88,306.25,4.19,5.56" target="#foot_10">9</ref> </p><p>Our two runs varied how guess answers were used. In our first run, guess answers were provided for every question. In our second run, guess answers were used only when no fixed lists were identified to match the question focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Passage Retrieval</head><p>Passages to project lists onto were retrieved with a much simpler algorithm than last year: we indexed groups of paragraphs that were at least 500 characters long as documents for IR, and then used document retrieval technologies described in <ref type="bibr" coords="4,350.76,510.46,99.15,10.91" target="#b1">(Bilotti et al., 2004)</ref> to retrieve these chunks, treating them as passages.</p><p>We did a preliminary investigation to see how much worse this method was than the method we used last year, and found anecdotally that it was comparable. Because of the difficulty we anticipated in adapting last year's passage retrieval code to the changed query expansion and document retrieval modules, we chose to focus our efforts elsewhere.</p><p>We chose 500 characters as a minimum passage size in order to avoid paragraphs separated for effect, but not containing enough text to give context to its contents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Answer Selection</head><p>Answer selection was based on duplicate removal and IR-based scoring. Two answers were considered duplicates if all of their content words (stopwords and punctuation removed, case ignored) were the same. If one set of content words subsumed the other, the longer answer was chosen. In both of our runs, the top 25% of guess answers were to be returned if START did not identify a focus. In our mit1 run, that is the only time that guess answers were returned. In our mit2 run, the top 25% of guess answers were to be returned for every question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Definition Questions</head><p>Our architecture for answering definitional questions is shown in Figure <ref type="figure" coords="5,250.06,566.74,4.19,10.91" target="#fig_0">3</ref>. Unlike last year, the topic to define is provided. That topic is then passed to three parallel techniques for finding definitional "nuggets" of the topic: lookup in a database of relational information created from the AQUAINT corpus, lookup in a Web dictionary followed by answer projection, and lookup directly in the AQUAINT corpus with information retrieval techniques. Answers from the three different sources are merged to produce the final system output. The following subsections briefly describe each of these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Target Analysis</head><p>This year the targets to define are not couched in a question, but they may still not be in a form that would likely be found in the corpus or in a dictionary. For example "the band" in "the band Nirvana" serves to disambiguate the sense of "Nirvana", but mentions of that band will more frequently appear as just "Nirvana" than as the whole phrase. Looking for "Nirvana" alone, we still must find instances that refer to the band.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Database Lookup</head><p>The use of surface patterns for answer extraction has proven to be an effective strategy for question answering. We began last year to use surface patterns to extract a database of definitional nuggets from the corpus from which to later answer questions, and have expanded on the sophistication and variety of those surface patterns for this year. We look for definitional patterns a priori to overcome a preponderance of non-definitional contexts for target words in document retrieval results; compile-time processing gives us better recall. We have reimplemented last year's system, and given it the name Col. ForBIN.<ref type="foot" coords="5,429.23,445.45,8.38,5.56" target="#foot_11">10</ref> Last year we had copular, appositive, and occupation patterns, and a few verb patterns ("became", "founded", "invented", etc.). <ref type="bibr" coords="5,366.77,501.34,123.23,10.91" target="#b3">(Hildebrandt et al., 2004)</ref> This year we have sixteen classes of patterns. They are comprised of cascades of regular expression patterns, that capture among other things: base noun phrases, single-level, two-level, and recursive noun phrases, prepositional phrases, relative clauses, and tensed verbs with modals. The new patterns allowed us to identify the target and nugget as any constituents of the matched pattern, so we were able to focus on finding exact definitional nuggets rather than windows of definitional contexts.</p><p>Finally, we have incorporated BBN's Iden-tiFinder program <ref type="bibr" coords="5,424.99,677.98,89.81,10.91" target="#b0">(Bikel et al., 1999)</ref> to iden-tify named entities. Most of the patterns require that their target entity be an Identi-Found named entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Improvement in Patterns</head><p>To measure precision of the pattern extraction engine, we asked students to annotate at least 200 sentences marked by the pattern extractor for each pattern. To measure recall, we asked students to annotate randomly selected sentences from documents judged relevant to TREC12 definition questions. See Figure <ref type="figure" coords="6,248.63,238.66,5.39,10.91">4</ref> for component results.</p><p>In each case, they were asked to mark target-nugget pairs, where the nugget was a good description of the target. Because the patterns between the two versions are different, the precision judgements on each pattern were not comparable, so we effectively had different test sets for TREC12 precision and Col. ForBIN precision measurements. Another problem comparing precision was that we used data from the TREC12 judgements, and possibly<ref type="foot" coords="6,74.40,400.57,8.38,5.56" target="#foot_12">11</ref> some of the data from the newer pattern judgements, in the evaluation. The recall judgements are the same for both cases, and were unseen by developers.</p><p>Much of the improved precision came from restricting targets to named entities, and from ensuring that those targets were the thing described rather than being simply linearly adjacent. Much of the improved recall came from the new ability to associate multiple nuggets, and non-adjacent nuggets, with a target.</p><p>Examples of sentences from the corpus matching each pattern are shown in Figure <ref type="figure" coords="6,261.57,565.30,4.19,10.91">5</ref>, with emphasis on targets from this year's competition. The composition of the patterns, the testing methodology, and the results, are detailed in <ref type="bibr" coords="6,95.07,619.54,82.86,10.91" target="#b2">(Fernandes, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Referring Expression Resolution</head><p>We used a simple rule-based referring expression resolution engine to assign full names to definite noun phrases, partial names, and pro-nouns in the entire corpus using an algorithm very similar to that used in query analysis. Referring expressions had to match their referent's gender and number where available, and preference was given to referents based on linear distance to their latest mention.</p><p>As in query analysis, partial names were expanded to their full known name, but without any further known description (e.g., "Floyd Patterson" would be substituted for "Patterson", sans "boxer". Unlike query analysis, no ontological information was used: only simple definite noun phrase references and occupation references were expanded.</p><p>Component performance was evaluated against the MUC-7 data set, yielding 71% precision and 23% recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Dictionary Lookup</head><p>Another component of our system for answering definitional questions utilizes an existing Web-based dictionary for nuggets. This component is largely unchanged from last year. Obviously, such an approach cannot be applied directly, because all nuggets must originate from the AQUAINT corpus. So we use answer projection techniques to "map" dictionary definitions onto AQUAINT documents.</p><p>Given the topic, our dictionary lookup engine goes to the Merriam-Webster online dictionary for its definitions. Keywords from the definition are used in a Lucene query to retrieve documents, and to score sentences based on keyword overlap with the dictionary definition. Sentences are trimmed to 250 bytes around the topic, containing the beginning or end of sentence if possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Document Lookup</head><p>Finally, our system looks for answers in the AQUAINT corpus itself, using the topic as a Lucene query, and selecting sentences that contain the topic. As in dictionary lookup, these sentences are trimmed to 250 bytes around the topic, containing the beginning or end of sentence if possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Version</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head><p>Precision exact inexact exact inexact TREC 12 144 / 483 = .30 175 / 483 = .36 1410/7061 = .19 3114/7061 = .44 Col. ForBIN 156 / 483 = .32 186 / 483 = .39 2527/4190 = .60 2669/4190 = .64 Figure <ref type="figure" coords="7,137.69,156.22,4.19,10.91">4</ref>: Comparison of definition extraction component from TREC 12 to present. Precision is evaluated on separate but comparable data sets. Recall is measured on one set of data from articles judged relevant to TREC12 definition questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Topic Matching and Answer Selection</head><p>Like last year, we looked for answers in parallel in the database described above, from a dictionary source, and from the corpus itself. Unlike last year, we did not prefer database answers strictly over dictionary answers, and those in turn over plain corpus answers. Rather, we weighted candidate sentences from the database at three times their score, and from dictionary at twice their score, and let them mix.</p><p>The answers were then presented by target quality. All answers matching a better target were presented before any matching a worse one. Unfortunately we were unable to use START's backoff mechanism to identify the relevant portion of the topic (e.g., "Floyd Patterson", then "Patterson" for "boxer Floyd Patterson"). Instead, we used a combination of candidate target precision and recall for quality. This made no distinction between "Fred" and "Durst" as backoffs for the singer.</p><p>Within each target, answers were ranked by novelty-the amount of word overlap between that answer and any previous answer. The base score of each word was its idf in the corpus, but this was boosted if the answer appeared often in the candidate matches yet to be printed (as a measure of salience), or if it was all lower-case (to promote answers with more English text in them over lists of names).</p><p>The maximally novel answer was selected at each step, and novelties recalculated with that answer now among the set of previous answers.</p><p>The novelty scoring was initialized with the previous questions and answers to avoid duplication of answers already given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Infrastructure Improvements</head><p>One of the most challenging components of any question answering endeavor is the complexity of the input data. When the complexity is in language, the challenge is welcome and exciting. Complexity in the input format is simply frustrating. We made a strong effort this year to clean the AQUAINT corpus, making the following changes:</p><p>• transform character entities and tags from SGML to XML • separate the title into its own tag • separate any article abstract into its own tag • separate metadata such as:</p><p>-Reporter -Location -Source</p><p>• remove comments to/from editors • separate documents that contain summaries on multiple news stories into a document for each story • remove duplicate documents (leaving pointers to the documents they duplicate)</p><p>We plan to make the scripts for cleaning and access available to other TREC participants. We hope that this common cleaned corpus will lower one barrier to entry into the competition.</p><p>Another infrastructure improvement we made was to create a suite of standoff XML tools in perl and java to manipulate, serialize, and display XML annotations in the text. This is undergoing revisions from lessons learned, but we also hope to make this code available. pattern example copular:</p><p>Ray Rhodes was coach of the year. We will be able to make available our standoff annotations for, e.g., Brill tags over the corpus (62Gb). We have found it much faster to read such tags from a file than to regenerate them on the fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We submitted two runs, summarized in Figure <ref type="figure" coords="9,121.41,470.02,4.19,10.91">6</ref>, in which we tested the effect of referring expression resolution on definition questions, and the effect of using or not using the best guess answers returned by our list component.</p><p>The referring expression resolution component (Section 5.4) improved recall for five definition questions and lowered recall for four. The paired difference between F-measures of the runs was .0029 ± .0153 (p-value: .353) and so was not statistically significant.</p><p>The use of guess answers (Section 4.7) improved both precision and recall for two questions, 62.4 and 63.3, but the paired difference between the two runs was still not significant: 0.0058 ± 0.0082; p-value 0.0798.</p><p>In hindsight, we believe that some promising ideas were overshadowed by mistakes in our software engineering process, primarily in insufficient integration and testing, and we look forward to fielding a more robust entry in the next competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,103.20,276.58,217.83,10.91;5,103.20,290.14,46.93,10.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture for answering definition questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,338.40,702.22,218.00,37.91"><head></head><label></label><figDesc>During this manual process, we also added synonymous nounphrase annotations, other than the list name, Examples of lists extracted from the corpus. The noun-phrase annotation is shown in bold. Multiple entries on a line indicate "synonyms", of which only one form will ever be reported, even if both are found.</figDesc><table coords="4,52.20,85.30,452.42,159.35"><row><cell>Italian region</cell><cell>cellular phone manufacturers, cell phone manufacturers</cell><cell>labor leader, leader</cell></row><row><cell>Abruzzi Basilicata, Lucania Calabria Campania Emilia-Romagna ...</cell><cell>Nokia Samsung Motorola Nextel Sprint</cell><cell>Sidney Hillman Leonard Woodcock Samuel Gompers Elizabeth Gurley Flynn James Riddle Hoffa David Dubinsky</cell></row><row><cell>from corpus and WordNet</cell><cell>from internet sources</cell><cell>from World Book</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,52.20,116.40,455.11,609.81"><head></head><label></label><figDesc>APW19990102.0072 copular w/anaphora: He [Franz Kafka] was one of the best-known Czech authors of early 20th century. APW20000425.0204 affiliation: When a note detailing the idea reached GE chairman Jack Welch, Adams, 30, a convicted murderer, was fatally shot in March 1994 while fighting with another prisoner. APW19990101.0028 entity in appositive: The disease, sporadic fatal insomnia, is caused by ... Dean, who died at age 24 in a 1955 car crash, is ... verb-passive: Franz Kafka was born in Prague, Czechoslovakia, in 1883 and died a month before his 41st birthday, having long suffered from tuberculosis. Mrs. Dole served as transportation secretary for President Reagan and labor secretary for President Bush. APW19990105.0044 verb-np-generic:Harding, who denied advance knowledge, received probation after pleading guilty to conspiracy to hinder prosecution. APW19990105.0220Figure5: Sample nuggets extracted from the AQUAINT corpus using surface patterns. The target terms are in bold, the nuggets are in italics.</figDesc><table coords="8,58.08,173.40,449.23,474.93"><row><cell></cell><cell></cell><cell>mit1</cell><cell></cell><cell>mit2</cell><cell></cell></row><row><cell></cell><cell>Factoid</cell><cell cols="4">Aranea was used unmodi-</cell></row><row><cell></cell><cell></cell><cell cols="3">fied for both runs.</cell><cell></cell></row><row><cell></cell><cell>List</cell><cell cols="2">always guess</cell><cell cols="2">guess unless</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">known list</cell></row><row><cell cols="3">Definition without</cell><cell></cell><cell>with</cell><cell>refer-</cell></row><row><cell></cell><cell></cell><cell cols="4">NYT19991021.0177 reference ence resolu-</cell></row><row><cell>occupation:</cell><cell></cell><cell cols="4">When a note detailing the idea reached GE chairman Jack Welch, resolution tion</cell></row><row><cell cols="6">NYT19991021.0177 Figure 6: Differences between the mit1 and occupation: Singer-choreographer Fred Durst wants a multimedia empire... NYT19990705.0170 mit2 runs.</cell></row><row><cell>age:</cell><cell></cell><cell cols="4">Adams, 30, a convicted murderer, was fatally shot in March 1994 while Factoid List Other Final</cell></row><row><cell></cell><cell>best</cell><cell cols="4">fighting with another prisoner. APW19990101.0028 .770 .622 .460</cell></row><row><cell>appositive:</cell><cell>mit1</cell><cell>.313</cell><cell>.119</cell><cell>.184</cell><cell>.232</cell></row><row><cell></cell><cell>mit2</cell><cell>.313</cell><cell>.113</cell><cell>.186</cell><cell>.231</cell></row><row><cell cols="2">median</cell><cell>.170</cell><cell>.094</cell><cell>.184</cell><cell></cell></row><row><cell cols="6">APW19990526.0110 ...caused by the same type of deformed proteins, known as prions, that Figure 7: Overall system performance also known as:</cell></row><row><cell></cell><cell></cell><cell cols="3">... APW19990526.0110</cell><cell></cell></row><row><cell>also called:</cell><cell></cell><cell cols="4">Some women in Beijing have established a non-governmental organi-</cell></row><row><cell></cell><cell></cell><cell cols="4">zation called Global Village to increase awareness of environmental</cell></row><row><cell></cell><cell></cell><cell cols="2">protection.</cell><cell></cell><cell></cell></row><row><cell>named:</cell><cell></cell><cell cols="4">In the early hours of June 8, 1924, a 38-year-old British schoolteacher</cell></row><row><cell></cell><cell></cell><cell cols="4">named George Mallory set forth... NYT19990504.0349</cell></row><row><cell>like:</cell><cell></cell><cell cols="4">Jiang has shown every sign that he aspires to enter the pantheon of great</cell></row><row><cell></cell><cell></cell><cell cols="4">Communist philosopher-leaders like Mao and Deng. NYT19990503.0106</cell></row><row><cell cols="2">like (false positive):</cell><cell cols="4">But although he's a high-wire act onstage, like Iggy Pop, Durst comes</cell></row><row><cell></cell><cell></cell><cell cols="4">across as mellow offstage. NYT19990618.0182</cell></row><row><cell>such as:</cell><cell></cell><cell cols="4">In the past, researchers had tested various single nutrients, such as</cell></row><row><cell></cell><cell></cell><cell cols="4">calcium, magnesium and potassium, to find clues about what affects</cell></row><row><cell></cell><cell></cell><cell cols="2">blood pressure,</cell><cell></cell><cell></cell></row><row><cell>became:</cell><cell></cell><cell cols="4">Jennifer Capriati became the youngest Grand Slam semifinalist and</cell></row><row><cell></cell><cell></cell><cell cols="4">beat five top-10 players in her first year. APW19990108.0333</cell></row><row><cell>was named:</cell><cell></cell><cell cols="4">A year later he [George W. Bates] was named managing editor of</cell></row><row><cell></cell><cell></cell><cell cols="4">the International Herald Tribune in Paris. APW19990107.0283</cell></row><row><cell cols="2">relative clause:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">APW20000223.0092</cell><cell></cell></row><row><cell>verb-pp:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,119.76,720.12,185.40,8.97"><p>"target" in the Guidelines' terms. See Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,115.56,729.64,3.60,4.17;1,119.76,730.68,132.63,8.97"><p>2 "other" in the Guidelines' terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,68.76,731.18,182.55,8.26;2,287.40,88.30,217.63,10.91;2,287.40,101.86,217.90,10.91;2,287.40,115.42,77.98,10.91"><p>http://www.ai.mit.edu/projects/infolab/ able to deploy it in our updated architecture with few changes. Where it used to send the query to Google</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,370.56,115.42,130.10,10.91"><p>, it now sends it to Teoma</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,287.40,128.86,217.87,10.91;2,287.40,142.42,159.51,10.91"><p>as well, and makes no distinction between the two sources in further processing.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5" coords="2,304.08,678.84,99.65,8.97"><p>http://www.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6" coords="2,304.08,689.52,99.05,8.97"><p>http://www.teoma.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7" coords="2,304.08,700.20,201.45,8.97;2,287.40,710.16,23.26,8.97"><p>or failed to incorporate it and provided it separately</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8" coords="2,299.88,719.68,3.60,4.17;2,304.08,720.72,201.42,8.97;2,287.40,730.68,151.44,8.97"><p>  7  It also provides the parsed data structure, but Pauchok currently uses the list of strings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9" coords="3,119.76,710.76,201.71,8.97;3,103.20,720.72,218.00,8.97;3,103.20,730.68,205.89,8.97"><p>This implies scoring based on document query backoff, because sets of retrieved document chunks for each expanded query are appended to one another.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_10" coords="4,304.08,720.72,201.31,8.97;4,287.40,730.68,195.10,8.97"><p>Of course the noun phrase "grapes of distinction" would be incorrectly collected as a guess answer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_11" coords="5,355.08,700.80,201.20,8.97;5,338.40,710.76,218.27,8.97;5,338.40,720.72,218.25,8.97;5,338.40,730.68,22.90,8.97"><p>Phish's song Col. Forbin's Ascent claims: "Col. Forbin, I know why you've come here, / And I'll help you with the quest to gain the knowledge that you lack."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_12" coords="6,68.76,710.76,201.57,8.97;6,52.20,720.72,218.03,8.97;6,52.20,730.68,95.93,8.97"><p>The students doing annotation and development worked together; we were not sufficiently careful to keep the tasks separate.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,338.40,141.45,218.26,9.96;9,349.32,152.37,207.19,9.96;9,349.32,163.41,207.21,9.96;9,349.32,174.33,71.89,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,476.42,152.37,80.09,9.96;9,349.32,163.41,108.95,9.96">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,473.68,163.41,78.33,9.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,193.05,218.13,9.96;9,349.32,204.09,207.39,9.96;9,349.32,215.01,207.31,9.96;9,349.32,225.93,207.15,9.96;9,349.32,236.97,207.27,9.96;9,349.32,247.89,92.80,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,378.80,204.09,177.91,9.96;9,349.32,215.01,207.31,9.96;9,349.32,225.93,21.94,9.96">What works better for question answering: Stemming or morphological query expansion?</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,390.81,225.93,165.65,9.96;9,349.32,236.97,207.27,9.96;9,349.32,247.89,64.00,9.96">Proceedings of the SIGIR 2004 Workshop IR4QA: Information Retrieval for Question Answering</title>
		<meeting>the SIGIR 2004 Workshop IR4QA: Information Retrieval for Question Answering</meeting>
		<imprint>
			<date type="published" when="2004-07">2004. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,266.61,218.10,9.96;9,349.32,277.65,207.16,9.96;9,349.32,288.57,168.32,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,457.57,266.61,98.93,9.96;9,349.32,277.65,203.27,9.96">Answering definitional questions before they are asked. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,307.41,218.12,9.96;9,349.32,318.33,207.32,9.96;9,349.32,329.25,207.16,9.96;9,349.32,340.29,207.02,9.96;9,349.32,351.21,207.26,9.96;9,349.32,362.13,207.30,9.96;9,349.32,373.05,146.15,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,377.00,318.33,179.64,9.96;9,349.32,329.25,102.57,9.96">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,476.90,329.25,79.59,9.96;9,349.32,340.29,207.02,9.96;9,349.32,351.21,207.26,9.96;9,349.32,362.13,207.30,9.96;9,349.32,373.05,111.70,9.96">Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting, HLT/NAACL-04</title>
		<meeting>the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting, HLT/NAACL-04</meeting>
		<imprint>
			<date type="published" when="2004-04">2004. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,391.89,218.24,9.96;9,349.32,402.81,207.30,9.96;9,349.32,413.85,207.32,9.96;9,349.32,424.77,207.28,9.96;9,349.32,435.69,207.16,9.96;9,349.32,446.73,207.24,9.96;9,349.32,457.65,207.29,9.96;9,349.32,468.57,53.94,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,528.93,413.85,27.71,9.96;9,349.32,424.77,207.28,9.96;9,349.32,435.69,82.33,9.96">Omnibase: Uniform access to heterogeneous data for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alton</forename><surname>Jerome Mc-Farland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baris</forename><surname>Temelkuran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,458.07,435.69,98.41,9.96;9,349.32,446.73,207.24,9.96;9,349.32,457.65,207.29,9.96;9,349.32,468.57,22.47,9.96">Proceedings of the 7th International Workshop on Applications of Natural Language to Information Systems (NLDB 2002)</title>
		<meeting>the 7th International Workshop on Applications of Natural Language to Information Systems (NLDB 2002)</meeting>
		<imprint>
			<date type="published" when="2002-06">2002. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,487.41,218.23,9.96;9,349.32,498.33,207.22,9.96;9,349.32,509.25,207.18,9.96;9,349.32,520.29,207.29,9.96;9,349.32,531.21,207.41,9.96;9,349.32,542.13,207.02,9.96;9,349.32,553.17,164.59,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,438.75,520.29,117.86,9.96;9,349.32,531.21,202.82,9.96">Integrating web-based and corpus-based techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,364.31,542.13,192.03,9.96;9,349.32,553.17,109.86,9.96">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003-11">2003. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,571.89,218.21,9.96;9,349.32,582.81,207.15,9.96;9,349.32,593.85,207.27,9.96;9,349.32,604.77,145.53,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,420.87,571.89,135.74,9.96;9,349.32,582.81,40.65,9.96">Using English for indexing and retrieving</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,410.23,582.81,146.24,9.96;9,349.32,593.85,207.27,9.96;9,349.32,604.77,88.47,9.96">Proceedings of the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</title>
		<meeting>the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>RIAO &apos;88</note>
</biblStruct>

<biblStruct coords="9,338.40,623.49,218.20,9.96;9,349.32,634.53,207.17,9.96;9,349.32,645.45,207.14,9.96;9,349.32,656.37,111.25,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,419.43,623.49,137.17,9.96;9,349.32,634.53,100.22,9.96">Annotating the world wide web using natural language</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<idno>RIAO97</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,476.19,634.53,80.31,9.96;9,349.32,645.45,207.14,9.96;9,349.32,656.37,64.70,9.96">Proceedings of the Conference on the Computer-Assisted Searching on the Internet</title>
		<meeting>the Conference on the Computer-Assisted Searching on the Internet</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.40,675.21,218.32,9.96;9,349.32,686.13,207.41,9.96;9,349.32,697.05,207.42,9.96;9,349.32,708.09,207.20,9.96;9,349.32,719.01,207.01,9.96;9,349.32,729.93,135.46,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,510.22,686.13,46.51,9.96;9,349.32,697.05,207.42,9.96;9,349.32,708.09,166.01,9.96">Extracting answers from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,537.47,708.09,19.06,9.96;9,349.32,719.01,207.01,9.96;9,349.32,729.93,80.73,9.96">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)</meeting>
		<imprint>
			<date type="published" when="2002-11">2002. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,52.20,89.01,218.33,9.96;10,63.12,99.93,207.19,9.96;10,63.12,110.85,207.25,9.96;10,63.12,121.89,207.05,9.96;10,63.12,132.81,207.04,9.96;10,63.12,143.73,207.03,9.96;10,63.12,154.77,149.72,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,236.73,99.93,33.57,9.96;10,63.12,110.85,207.25,9.96;10,63.12,121.89,99.11,9.96">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,189.62,121.89,80.55,9.96;10,63.12,132.81,207.04,9.96;10,63.12,143.73,207.03,9.96;10,63.12,154.77,121.12,9.96">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</meeting>
		<imprint>
			<date type="published" when="2003-07">2003. July</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
