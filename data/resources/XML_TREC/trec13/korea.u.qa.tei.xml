<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.76,164.96,335.74,15.49;1,264.24,186.80,82.89,15.49">Korea University Question Answering System at TREC 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.24,219.31,81.02,10.76"><forename type="first">Kyoung-Soo</forename><surname>Han</surname></persName>
							<email>kshan@nlp.korea.ac.kr</email>
						</author>
						<author>
							<persName coords="1,269.88,219.31,73.97,10.76"><forename type="first">Hoojung</forename><surname>Chung</surname></persName>
							<email>hjchung@nlp.korea.ac.kr</email>
						</author>
						<author>
							<persName coords="1,352.68,219.31,72.38,10.76"><forename type="first">Sang-Bum</forename><surname>Kim</surname></persName>
							<email>sbkim@nlp.korea.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Young-In Song</orgName>
								<orgName type="department" key="dep2">Hae-Chang Rim Natural Language Processing Lab. Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Joo-Young Lee</orgName>
								<address>
									<country>Korea University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>5-ga, Anam-dong, Seongbuk-gu</addrLine>
									<postCode>136-701</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.76,164.96,335.74,15.49;1,264.24,186.80,82.89,15.49">Korea University Question Answering System at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D6FC6655E2381869CFB47AA78CAC357</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Proximity Based Answer Extraction Redundancy Based Answer Extraction Answer Extraction Question Classification WordNet NE tagger Conexor FDG Parser Question Analysis Passage Retrieval Answer Question AQUAINT Proximity Based Answer Extraction Redundancy Based Answer Extraction Answer Extraction Question Classification</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our QA system consists of two different components. One is for the factoid and list questions, and the other is for the other questions. The components are processed individually, and each result is combined into our submitted run.</p><p>For the factoid questions, we have tried to find answers by proximity-based named entity search. Given a question, fine-grained named entities for candidate answers are selected, and all the extracted passages containing the named entities and question keywords are scored by a proximity-based measure. List questions are processed in a similar way to the factoid questions, but we empirically give a threshold value to obtain only top Ò candidate answers.</p><p>For other questions, relevant phrases consisting of noun phrases and verb phrases are extracted using a dependency relationship to the question target from the initially retrieved sentences. After redundant phrases are eliminated from the answer candidates, final answers are selected using several selection criteria including the term statistics from an encyclopedia.</p><p>Section 2 summarizes our system for factoid and list questions, and Section 3 for other questions. In Section 4, the TREC evaluation results are analyzed, and Section 5 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Factoid and List Questions</head><p>To put it briefly, our system extracts answer candidates from AQUAINT documents according to the expected answer types of a question, and uses a confidence score to rank the answer candidates. The highest ranked answer candidate is selected as an answer for the factoid question, while answer candidates with a higher score than a Figure <ref type="figure" coords="2,162.36,335.28,3.90,8.97">1</ref>: Overall architecture for question answering system for factoid and list questions certain pre-defined threshold value are chosen for list questions. As shown in Figure <ref type="figure" coords="2,133.80,389.16,3.77,8.97">1</ref>, there are following three steps in answering factoid and list questions: a question analysis step, a passage retrieval step, and an answer extraction step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>In question analysis phase, we first perform syntactic parsing using Conexor FDG parser <ref type="bibr" coords="2,156.89,460.32,11.55,8.97" target="#b0">[1]</ref>. Then we categorize given question into one of the predefined question classes. If the question is categorized into how-, when-, where-question, predefined named entity types for each question class is assigned to the given question. Otherwise, we further extract question focus, which is about what the question is about. Once the question focus is extracted, expected answer type is determined considering question focus. Finally, main verb is extracted from the syntactic information of the question sentence and heuristic rules. The main verb is used as a essential clue word in answer extraction phase.</p><p>For example, for the question "What American commodore demanded that Japan trade with the United States?", we can obtain the following analysis results:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUESTION FOCUS American commodore</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Answer Type PERSON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Verb demand</head><p>We manually constructed a n:n mapping table, which links between expected answer types and representative words for each type. In case of the above example, we first extract question focus, and provide the head word "commodore" of the question focus. Since "commodore" is one of the hyponyms of person in WordNet and the word "person" is linked to PERSON in our mapping table, we assign the expected answer type PERSON. Thus, it is possible to assign several expected answer types given a question if the head word of the question focus have several hypernyms in WordNet.</p><p>When we define expected answer types, we consider the named entity classes provided by BBN IdentiFinder <ref type="bibr" coords="3,241.93,199.80,12.18,8.97" target="#b1">[2]</ref> which we used in our answer selection phase. However, since BBN IdentiFinder attaches only coarse-grained classes to named entities, we have further subcategorized the BBN IdentiFinder classes into finer-grained ones. The finegrained named entity tagging was done with a named entity dictionary. The dictionary entries were automatically extracted from AQUAINT corpus by using simple patterns, and then some entries were added manually. Moreover, in order to cover some questions for the names of artworks such as books and movies, we have developed a title named entity tagger and tagged the title of artworks. The detail of this tagger is described in <ref type="bibr" coords="3,175.56,295.44,10.60,8.97" target="#b2">[3]</ref>, and we listed some named entity tags we used in Table <ref type="table" coords="3,414.71,295.44,3.77,8.97" target="#tab_0">1</ref>. Query words for document retrieval are also extracted in this phase by removing stopwords from the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>The relevant passages for a given question are retrieved by a document retrieval system and passage selection rules. The document retrieval system which uses Okapi ranking function retrieves AQUAINT documents relevant to the given question. Among the retrieved documents, each sentence that contains ¯the expected answer type of the given question, and ¯one or more question keywords, and ¯any proper noun in the given question is selected as a passage.</p><p>In addition, we also extract the passages including anaphora for the proper noun in the question. If a sentence has an appropriate anaphora for the proper noun in the question, and also contains the expected answer type and one or more keywords in question, then we check the preceding sentence. If the preceding sentence has only the proper noun in the question, we decide to extract both sentences as a passage since it is most probably that the anaphora is referring the proper noun in the preceding sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction</head><p>Every named entity which matches to the expected answer type is selected as an answer candidate among the retrieved passages, which is NE tagged and dependency-parsed. Also every noun in the passages is considered as an answer candidate if the noun is a hyponym of the expected answer type in WordNet. Based on the observation that some types of questions which contain only the question focus and target words<ref type="foot" coords="4,245.88,532.08,3.49,6.28" target="#foot_0">1</ref> can be easily answered only with the co-occurrence frequency between an answer candidate and named entities in the question, we ranked answer candidates for this type of question according to their frequencies in the retrieved passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Otherwise, we use a confidence score for answer candidates to rank them. Our confidence score is calculated by linearly combining two scores: surface distance score and syntactic similarity score, which are described below subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Surface Distance Score</head><p>We give surface distance score to each retrieved passage-answer candidate pair. The surface distance score between question É and passage È containing answer candidate , is calculated as follows:</p><formula xml:id="formula_0" coords="5,196.44,186.14,281.12,33.54">ËÓÖ ´É È µ Õ ¾É ÁÑÔ Ø´Õ Éµ ¡ Ï Ø´Õ ¼µ ×Ø Ò ´Õ È µ<label>(1)</label></formula><p>where</p><formula xml:id="formula_1" coords="5,153.12,234.98,324.44,37.50">ÁÑÔ Ø´Õ Éµ an impact factor of Õ in É (2) ×Ø Ò ´Õ È µ Ñ Ò Ø ¾Ö Ð´Õ µ</formula><p>word distance between Ø and candidate answer</p><p>An impact factor is about how important the word Õ is in question É. We empirically set the impact factor, for instance:</p><p>¯If Õ is the main verb in question É, we set 1.5. ¯If Õ has capital initial letter or Õ is superlative form, we set 1.2</p><p>In equation ( <ref type="formula" coords="5,200.24,357.00,3.56,8.97" target="#formula_2">3</ref>), Ö Ð´Õ µ is a set of related words of Õ . This is another resource we used in answer selection. In our system, a related word of word Ø means a morphological derivational form of Ø or a special type of derivations such as (Korea, Korean) or (Europe, European). Õ ¼ is a selected word in ×Ø Ò measure, and Ï Ø´Õ ¼µ is 1 if Õ ¼ is Õ itself, or 0.3 otherwise. If there is no related word of Õ in passage È , ×Ø Ò ´Õ È µ is defined as an infinite number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Syntactic Similarity Score</head><p>Our syntactic similarity score measures the possibility that each retrieved passage contains the proper answer for given question by comparing syntactic dependency pairs in question and passage. Among several sentences in each passage, we check only one sentence having expected answer type, that is, candidate answer.</p><p>A syntactic similarity between question É and sentence Ë ËÝÒË Ñ´É Ëµ is calculated as follows:</p><formula xml:id="formula_3" coords="5,158.64,527.66,292.68,38.40">ËÝÒË Ñ´É Ëµ È Õ Õ ¾É Û Û ¾Ë Û Ø Ô ´Õ Õ Û Û µ É</formula><p>where term Õ is the head of term Õ in the question, term Û is the head of term Û in the sentence Ë, and Û Ø Ô means the similarity between each dependency unit in the sentence Ë and the question É.</p><p>Û Ø Ô values are determined by checking the following heuristic rules step by step:</p><p>(1) If ´Õ Õ µ and ´Û Û µ are same, Û Ø Ô value for the two dependencies is 1.0.</p><p>(</p><formula xml:id="formula_4" coords="6,142.08,125.30,335.62,44.64">) If Õ Û , Õ Û , but Û has a indirect relation with Õ via Ò intermediate words, Û Ø Ô of the dependencies is ½ ¼ Ò £ ¼ ¼ (3) If Õ is a synonym of Û , or Õ is a synonym of Û , Û Ø Ô value between<label>2</label></formula><p>the dependencies is 0.7. We have used WordNet to check whether two words has a synonym relation.</p><p>(4) If Õ Û and Õ Û , that is, only the direction of dependency is reversed, Û Ø Ô value between the dependencies is 0.6</p><p>(5) If Õ Û , and it is the name of location or date, Û Ø Ô value between the dependencies is 0.5</p><p>With these two score measures, we find morphologically and syntactically similar passages to a question, and the answer candidate from the passages get higher confidence scores. The highest ranked answer candidate is selected as an answer for the factoid question, while answer candidates which have higher scores than an empirically determined threshold value are chosen for the list questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Other Questions</head><p>Figure <ref type="figure" coords="6,161.88,373.20,4.98,8.97" target="#fig_0">2</ref> shows the overview of our definition question answering system developed for the other questions. Our system extracts answer candidates consisting of noun and verb phrases related to the question target, ranks the candidates using several evidences, and returns Ò top-ranked candidates whose scores are higher than a pre-defined threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Analysis</head><p>Contrary to the factoid and list questions, other questions do not have an expected answer type such as a location or a person name. In the question analysis phase for the definition questions, the head word of the target is extracted and the target type is identified. In order to identify the head word, we syntactically parsed the question target using the Conexor FDG parser. This head word is used for a query for sentence retrieval. We have performed named entity tagging to the question target, and classified the target using the named entity of the target head word into one of three target types: person, organization, and other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieval</head><p>In order to answer the question, we have to retrieve relevant information to the question target in the passage retrieval phase. Because the target tends to be used with different expression in documents from that in the question, a lot of relevant information could not be retrieved or lots of irrelevant information would be retrieved by one phase passage retrieval. For example, for a target Bill Clinton, it would be expressed in a text by Clinton, president Clinton, he, etc. A strict phrase query bill clinton would suffer  from low recall because of differently-represented phrases in documents, while a relaxed query clinton would be overloaded by a plenty of irrelevant information such as George Clinton. Therefore, our passage retrieval engine for the other questions consists of two phases: document retrieval phase and sentence retrieval phase. Our intention is that we firstly retrieve only relevant documents to the target by relatively strict query, and then extract relevant sentences by using more relaxed query. The query for the document retrieval consists of the words and phrases of the target filtered with a stopword list. The phrase is used for a sequence of words with an initial capital letter, and the word itself is used otherwise. For example, for a target Bill Clinton, a phrase bill clinton is used as the query, and for a target Berkman Center for Internet and Society, the query would include a phrase berkman center and two words internet and society.</p><p>For the sentence retrieval, only the head word of the target is used as the query. In other words, sentences containing the head word are extracted from the retrieved documents. Sentences in which the head word is represented as an anaphora are also extracted by using simple rules. We observed that an anaphora refers to the target if the anaphora is used as a subject and the target is also used as a subject in the previous sentence. As shown in the following example, he in the sentence (b) refers to Clinton in the sentence (a), so both sentences are extracted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Candidate Extraction</head><p>For generating a more fine-grained answer, phrases are extracted from the retrieved sentences by using the syntactic dependency relations. In order to identify the syntactic relations, we also syntactically parsed the retrieved sentences using the Conexor FDG parser. Following syntactic patterns are used for extracting the answer candidates from the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noun phrases modifying the question target Noun phrases that have a direct syntactic relation to the question target</head><p>Relative pronoun phrases Verb phrases that a nominative or possessive relative pronoun modifies directly the question target Participle phrases Present or past participles that modify directly without its subject the question target or the main verb directly related to the question target.</p><p>Copulas Noun phrases used as a complement of a verb be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General verb phrases</head><p>Verb phrases whose head word is not stop verbs.</p><p>The stop verbs mean the functional verbs, which is not informative one such as be, say, talk, and tell.</p><p>In the above example, the following phrases would be extracted by applying the syntactic patterns:</p><p>(1) Former president (2) born in Hope, Arkansas (3) named William Jefferson Blythe IV after his father, William Jefferson Blythe III The syntactic information induced from the syntactic parser has many errors or there are sentences from which the information is not obtained. In order to alleviate the problem, we complement the syntactic information with POS information.</p><p>¯If any word between the first word and the last of a phrase in the sentence is not extracted, it is also extracted to the phrase.</p><p>¯If the last word of extracted phrase is labeled with noun-dependent POSs such as adjective, determiner and preposition, the immediate noun phrase is put together into the extracted phrase.</p><p>¯If the extracted phrase is incomplete one, that is, ended with one of the POSs such as conjunction or relative pronoun, the last word is removed from the extracted phrase.</p><p>Because all extracted phrases are not useful for answer candidates, it is necessary to check the answer validity. The phrases which contain more than two content words and at least one noun or number is considered to be valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Redundancy Elimination</head><p>We also eliminated redundant information among the candidates, the extracted noun and verb phrases, based on word overlap and the semantic class of the main head word in WordNet. If two candidates share 70% or more words, the lowerly-ranked candidate is eliminated. If the word overlap does not amount to 30%, the two candidates are determined to be nonredundant. Otherwise, the semantic class of the main head word is checked. If each main head word of the two candidates is contained in a synset in WordNet, the two candidates are determined to be redundant, and the lowerly-ranked one is eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Selection</head><p>Our system used several evidences to rank answer candidates: head word redundancy, term statistics in the relevant passages, and the biographical term weight. We have selected final answers up to 24 candidates. The highly-ranked answer candidates having a higher score than pre-defined threshold were included into the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Head Word Redundancy</head><p>The important facts or events are usually mentioned repeatedly, and the head word is the core of each answer candidate. Therefore, we consider the redundancy of head word of answer candidate by using following formula.</p><formula xml:id="formula_5" coords="9,224.52,390.86,156.22,41.22">Ö ´ µ ´Ö Ò if is noun phrase Ö Ú</formula><p>if is verb phrase where Ö represents the number of the retrieved sentences in which the head word of the answer candidate is used as a head word, Ò is the total number of answer candidates as a noun phrase, and Ú is the total number of answer candidates as a verb phrase. The eliminated candidates in the redundancy elimination phase also contribute to the calculation of the values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Local Term Statistics</head><p>In addition to the head word, the frequent words in the retrieved passages are important. The ÐÓ´ µ presents a local weight based on the term statistics in the retrieved sentences, and is calculated as follows:</p><formula xml:id="formula_6" coords="9,207.84,569.18,188.52,31.44">ÐÓ´ µ È Ø ¾ Û Ø ÐÓ ´Ø µ È Ø ¾ × Ñ Ü×</formula><p>where × is the number of sentences in which the term Ø is occurred, Ñ Ü× is the maximum value of × among all terms, and is the number of all content words in the answer candidate .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Biographical Term Weight</head><p>The biographical term weight is calculated only for the person target. The documents in an encyclopedia describe whole life of a person including personal identity and events. In other words, the encyclopedia can be a good training data for learning about the biographical definition.</p><p>The probability that a term occurs in the encyclopedia can be used for the term importance measure. The idea is that the more frequent word in the encyclopedia will be more important in the biographical definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Û</head><p>Ø Ô ´Øµ Ô ÒÝ ´Øµ Ø Ì where Ø is the term frequency in the encyclopedia, and Ì is the total occurrences of all terms.</p><p>In addition to the encyclopedia, we also utilized a general text. The term probability ratio assigns the high weight to the term occurring much more frequently in the encyclopedia than in the general text by using the following formula:</p><formula xml:id="formula_7" coords="10,249.12,325.82,111.72,26.28">Û Ø ÔÖ Ø Ó ´Øµ Ô ÒÝ ´Øµ Ô Ò ´Øµ</formula><p>It is similar to the TextMap <ref type="bibr" coords="10,238.27,359.40,13.61,8.97" target="#b3">[4]</ref>. The biographical term weight is calculated using the above weights as follows:</p><formula xml:id="formula_8" coords="10,157.68,376.58,297.58,86.16">Ó ½ ´ µ È Ø ¾ ÐÓ ¾ ´Û ØÔ ´Ø µ¢Û ØÔÖ Ø Ó´Ø µ•½µ if is noun phrase È Ø ¾ ÐÓ ½¼ ´Û ØÔ ´Ø µ•½µ if is verb phrase Ó ¾ ´ µ È Ø ¾ Û Ø Ô ´Ø µ</formula><p>Refer to the <ref type="bibr" coords="10,198.24,476.52,11.60,8.97" target="#b4">[5]</ref> for more detail comparison between these weights.</p><p>Our system KUQA1 and KUQA2 use the weights Ó ½ ´ µ and Ó ¾ ´ µ respectively.</p><p>KUQA3 does not use this biographical term weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Combination</head><p>The evidence mentioned so far is combined with linear interpolation.</p><p>×ÓÖ</p><formula xml:id="formula_9" coords="10,224.88,558.86,184.56,15.60">´ µ « ¢ Ö ´ µ • ¬ ¢ ÐÓ´ µ • ¢ Ó´ µ</formula><p>where « ¬ are tuning parameters satisfying « • ¬ • ½, and are determined</p><p>empirically.</p><p>The answer candidates whose score is higher than a threshold are returned as final answers. In order to eliminate the nuggets redundant to the factoid and list questions, we calculate a redundancy by content word overlap. If an answer candidate overlaps with previous questions (e.g. factoid and list questions) more than 65% or with previous answers (e.g. answers for the factoid and list questions) more than 60%, then the answer candidates are not selected as final answers.</p><p>We have submitted three runs : KUQA1, KUQA2, and KUQA3. For the factoid questions, KUQA1 is the result by using dependence information, while KUQA2 and KUQA3 are not.</p><p>For other questions, we used a public online encyclopedia, the Columbia Encyclopedia <ref type="foot" coords="11,155.52,196.56,3.49,6.28" target="#foot_1">2</ref> . A part of AQUAINT corpus is used for the general text. It consists of 5,268 APW articles in July 2000, 12,843 NYT ones in March 1999, and 9,727 XIE ones in December 1998.</p><p>As shown in Table <ref type="table" coords="11,226.20,234.00,3.77,8.97" target="#tab_2">2</ref>, we can conclude that dependence information is very useful to find an answer. Since the performances for the list questions are superior to the median performance, we can also claim that our proximity-based ranking strategy is quite effective. For other questions, KUQA1 and KUQA2 used the biographical term weight, Ó ½ ´ µ and Ó ¾ ´ µ respectively, based on the encyclopedia, and KUQA3 does not used the biographical term weight. It seems that Ó ¾ ´ µ is not appropriate for the biographical term weight because it cannot properly normalize the score. Unexpectedly, the results show that the system KUQA1 using the encyclopedia does not outperform the system KUQA3 not using it. By analyzing the TREC evaluation results, the matching decision between KUQA1 and KUQA3 was not consistent; nuggets regarded to be matched in KUQA3 were not regarded to be matched in KUQA1. KUQA1 seems to be evaluated more strictly than KUQA3. Table <ref type="table" coords="11,324.00,482.64,4.98,8.97" target="#tab_3">3</ref> shows the characteristics for the two systems. In spite of the inconsistent matching decision, KUQA1 using the encyclopedia information can achieve higher precision at little cost of recall with shorter answer than KUQA3. Our system cannot answer the other questions when the parsing errors for answer-containing sentences are not successfully dealt with and any conditions for phrase extraction are not satisfied. It is necessary to relax the conditions for the more answer coverage. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,164.04,360.36,283.15,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture for definition question answering system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,142.68,629.52,253.89,8.97;7,142.08,649.44,335.58,8.97;7,158.64,661.44,41.61,8.97"><head></head><label></label><figDesc>(a) Former president Bill Clinton was born in Hope, Arkansas. (b) He was named William Jefferson Blythe IV after his father, William Jefferson Blythe III.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,161.28,128.88,288.81,369.81"><head>Table 1 :</head><label>1</label><figDesc>Part of the named entity tag set used in our system</figDesc><table coords="4,161.28,128.88,287.01,153.57"><row><cell></cell><cell cols="2">Sub-category Examples</cell></row><row><cell>Location</cell><cell>Country</cell><cell>Korea, Italy, Mauritius, Mexico, ...</cell></row><row><cell></cell><cell>Continent</cell><cell>Africa, Asia, ...</cell></row><row><cell></cell><cell>City</cell><cell>San Francisco, Jakarta, ...</cell></row><row><cell></cell><cell>Planet</cell><cell>Earth, Jupiter, ...</cell></row><row><cell></cell><cell>Mountain</cell><cell>Pikes Peak, Mt. Everest, ...</cell></row><row><cell></cell><cell>Sea</cell><cell>Red Sea, Pacific, ...</cell></row><row><cell>Person</cell><cell></cell><cell>Louis Armstrong, George Bush, ...</cell></row><row><cell cols="2">Organization Event</cell><cell>Vietnam war, World War I, ...</cell></row><row><cell></cell><cell>Games</cell><cell>Super Bowl, U.S. Open, World Cup, ...</cell></row><row><cell></cell><cell>Univ.</cell><cell>University of Mississippi, ...</cell></row><row><cell></cell><cell>Company</cell><cell>Coca-Cola Co., Dell, ...</cell></row><row><cell></cell><cell>Sports Team</cell><cell>Baltimore Orioles, Seattle Mariners, ...</cell></row></table><note coords="4,161.28,285.84,18.82,8.97;4,289.92,285.84,41.13,8.97;4,161.28,297.84,26.70,8.97;4,289.92,297.84,56.25,8.97;4,161.28,309.72,16.62,8.97;4,289.92,309.72,98.37,8.97;4,161.28,321.72,20.74,8.97;4,289.92,321.72,80.01,8.97;4,161.28,333.72,28.26,8.97;4,289.92,333.62,78.33,9.09;4,161.28,345.60,30.01,8.97;4,289.92,345.60,72.86,8.97;4,161.28,357.96,37.72,8.97;4,289.92,357.96,160.17,8.97;4,161.28,370.32,32.84,8.97;4,289.92,370.32,93.45,8.97;4,225.00,382.32,28.38,8.97;4,289.92,382.32,122.97,8.97;4,225.00,394.20,27.25,8.97;4,289.92,394.20,50.22,8.97;4,225.00,406.20,28.57,8.97;4,289.92,406.20,97.77,8.97;4,225.00,418.20,24.49,8.97;4,289.92,418.20,71.97,8.97;4,225.00,430.08,24.42,8.97;4,289.92,430.08,73.29,8.97;4,225.00,442.08,121.45,8.97;4,225.00,454.08,30.94,8.97;4,289.92,454.08,61.36,8.97;4,161.28,465.96,26.68,8.97;4,289.92,465.96,98.01,8.97"><p>Date July 14, ... Month April, June, ... Day Thursday, Wednesday, ... Time 1:05 p.m., 7 p.m., ... Money $35, $5.4 billion, ... Percent 10 percent, 16.5% Artworks City of Angels, Notes of a Native Son, ... Number one, two, 323, 32.35, ... Length 1,893 yards, 382 kilometers, ... Height 37-foot-high Weight 1 ton, 52kg, 0.5 gram, ... Count 3 times, 7times, ... Speed 17kph, 100Kmh,... Temperature 24 degree, 53f Volume 500cc, 1.5-liter Others Pentium III Xeon, PC, ...</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,217.44,300.72,176.58,69.57"><head>Table 2 :</head><label>2</label><figDesc>Summary of the performances</figDesc><table coords="11,217.44,312.60,176.58,57.69"><row><cell>Run</cell><cell>factoid</cell><cell>list</cell><cell>other</cell><cell>final</cell></row><row><cell cols="5">KUQA1 0.222 0.159 0.246 0.212</cell></row><row><cell cols="5">KUQA2 0.187 0.157 0.229 0.190</cell></row><row><cell cols="5">KUQA3 0.187 0.157 0.247 0.195</cell></row><row><cell>Median</cell><cell cols="3">0.170 0.094 0.184</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,182.28,585.24,246.87,45.21"><head>Table 3 :</head><label>3</label><figDesc>Comparison between our systems for other questions</figDesc><table coords="11,227.64,597.12,156.18,33.33"><row><cell>Run</cell><cell cols="3">recall precision length</cell></row><row><cell cols="2">KUQA1 0.261</cell><cell>0.279</cell><cell>649.15</cell></row><row><cell cols="2">KUQA3 0.262</cell><cell>0.271</cell><cell>666.34</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,148.08,624.76,255.02,7.17"><p>For example, Which continent is India in? or Who is a mayor of San Francisco?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="11,148.08,654.64,100.51,7.17;11,263.76,654.64,213.67,7.17;11,133.80,664.12,90.89,7.17"><p>The Columbia Encyclopedia. Columbia University Press, New York, 6th edition, 2004, http://www.bartleby.com/65/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We have presented an overview of our QA system for TREC 2004. Our QA system used the dependency information for all questions, and the results imply that the dependency information is useful for answer selection. Moreover, the encyclopedia was helpful to select proper answer for other questions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,150.36,242.04,327.19,8.97;12,150.36,254.04,327.27,8.97;12,150.36,265.92,52.53,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,298.56,242.04,142.72,8.97">A non-projective dependency parser</title>
		<author>
			<persName coords=""><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Jarvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,459.71,242.04,17.84,8.97;12,150.36,254.04,296.71,8.97">Proceedings of the 5th Conference on Applied Natural Language Processing</title>
		<meeting>the 5th Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,150.36,285.96,327.31,8.97;12,150.36,297.84,290.85,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,423.48,285.96,54.19,8.97;12,150.36,297.84,108.77,8.97">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,267.36,297.84,72.05,8.97">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,150.36,317.76,327.30,8.97;12,150.36,329.76,327.39,8.97;12,150.36,341.64,315.81,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,174.48,329.76,234.54,8.97">Title recognition using lexical pattern and entity dictionary</title>
		<author>
			<persName coords=""><forename type="first">Joo-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young-In</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoojung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hae-Chang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,428.16,329.76,49.59,8.97;12,150.36,341.64,221.66,8.97">Proceedings of the 1st Asia Information Retrieval Symposium (AIRS)</title>
		<meeting>the 1st Asia Information Retrieval Symposium (AIRS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="345" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,150.36,361.56,327.21,8.97;12,150.36,373.56,327.18,8.97;12,150.36,385.56,327.57,8.97;12,150.36,397.44,22.65,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,266.88,373.56,191.24,8.97">Multiple-engine question answering in textmap</title>
		<author>
			<persName coords=""><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.36,385.56,256.93,8.97">Proceedings of the 12th Text Retrieval Conference (TREC-2003)</title>
		<meeting>the 12th Text Retrieval Conference (TREC-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="772" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,150.36,417.36,327.19,8.97;12,150.36,429.36,327.33,8.97;12,150.36,441.36,327.27,8.97;12,150.36,453.24,62.49,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,446.52,417.36,31.04,8.97;12,150.36,429.36,323.07,8.97">Answer selection for biographical definition questions using biographical term weighting</title>
		<author>
			<persName coords=""><forename type="first">Kyoung-Soo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young-In</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hae-Chang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,162.48,441.36,284.42,8.97">Proceedings of the 1st Asia Information Retrieval Symposium (AIRS)</title>
		<meeting>the 1st Asia Information Retrieval Symposium (AIRS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
