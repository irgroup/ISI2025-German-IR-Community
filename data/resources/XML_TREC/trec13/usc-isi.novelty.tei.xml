<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,194.04,68.29,233.21,11.47">ISI Novelty Track System for TREC 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.66,120.63,65.50,9.84"><forename type="first">Soo-Min</forename><surname>Kim</surname></persName>
							<email>skim@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.00,120.63,108.23,9.84"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Science Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.62,120.63,65.55,9.84"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Science Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way Marina del Rey</addrLine>
									<postCode>90292-6695</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,194.04,68.29,233.21,11.47">ISI Novelty Track System for TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">03E723937960CEDACB53B9341B1AD7B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our system developed at ISI for the Novelty track at TREC 2004. The system's two modules recognize relevant event and opinion sentences respectively. We focused mainly on recognizing relevant opinion sentences using various opinionbearing word lists. Of our 5 runs submitted for task 1, the best run provided an F-score of 0.390 (precision 0.30 and recall 0.71).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Novelty Track is designed to investigate systems' abilities on two separate tasks: locating relevant or new information within a set of documents relevant to a TREC topic. Identifying relevant sentences is a sentence retrieval task similar to passage retrieval, where relevance is defined as both relating to a given topic and bearing an opinion about the topic. Identifying new sentences is defined as retrieving sentences about a given topic that contain information that has not appeared previously in this topic's set of documents. This year, opinion topics include "Gay Boy Scouts Banned", "Military Action Kosovo", and "Banning Tobacco Advertisements", while event topics include "India and Pakistan Nuclear Tests", "East Timor Independence", and "Payne Steward Plane Crash".</p><p>For both tasks, systems are given the topic and a set of relevant documents ordered by date, and must return only the appropriate sentences. Unlike last year's task, the initial documents set may include irrelevant documents, which systems must identify and ignore. We used a probabilistic Bayesian inference network model, as implemented in the search engine software package INQUERY <ref type="bibr" coords="1,176.51,685.14,95.36,9.01" target="#b0">(Callan et al. 1992)</ref>, to identify the relevant documents.</p><p>There are four subtasks that vary the kinds of data available to systems and the kinds of results that must be returned. Among these four tasks, ISI participated in the first: identify all relevant and novel sentences given the full set of documents for the given topic.</p><p>We describe document filtering in Section 2 and our methods for identifying relevant sentences from opinion topic documents in Section 3. Section 4 explains the method we used for event topics. System results are reported in Section 5 and conclusions appear in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Filtering</head><p>Each topic contained 25 relevant documents, possibly mixed with additional irrelevant documents. Thus, before proceeding to the next phase we had to separate relevant documents from irrelevant documents. We treat this problem as a standard Information Retrieval (IR) procedure. This approach is motivated by years of research in IR. It is a well known truism that simple techniques in IR often yield better results than sophisticated and deep linguistic analysis. We are interested in analyzing the results from other participants to empirically verify whether this belief holds in the novelty track document filtering task.</p><p>We use the example in Figure <ref type="figure" coords="1,475.11,537.30,5.17,9.01" target="#fig_0">1</ref> to illustrate out document filtering process.</p><p>To perform document filtering, we used the description &lt;desc&gt; field as our IR query. (We initially tried various other fields, including narrative &lt;narr&gt; and title &lt;title&gt;, but quickly abandoned them after noticing that the narrative field contains information which is very hard for computational treatment, such as negation.) To perform IR we use a probabilistic Bayesian inference network model as implemented in the search engine software package INQUERY. For each query we perform the standard procedure of stop-word removal and stemming. Using an OR query, we select the top 25 documents returned by the search engine as the relevant set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Opinion Topics</head><p>Identifying relevant sentences from opinion topic documents is different from identifying sentences from event topic documents. Relevant sentences from opinion documents should be relevant to the topic and opinion-bearing at the same time. Unlike event topics, we assume that whether a sentence is opinion-bearing or not is more important than its relevance to the topic in the opinion topic case, assuming that most opinions expressed a document are relevant to its topic. In other words, opinion documents such as editorials could contain sentences that describe irrelevant facts but the opinion sentences in these documents are likely relevant to the topic.</p><p>The most important part of opinion-bearing sentence recognition is identifying so-called subjectivity clues in a sentence. There are many approaches to building and using subjectivity clues. <ref type="bibr" coords="2,79.32,518.94,64.83,9.01" target="#b8">Turney (2002)</ref> and <ref type="bibr" coords="2,173.82,518.94,61.80,9.01" target="#b9">Wiebe (2000)</ref> focused on learning adjectives and adjectival phrases. <ref type="bibr" coords="2,268.01,530.76,24.70,9.01;2,79.32,542.70,51.13,9.01">Riloff et al. (2003)</ref> extracted nouns and <ref type="bibr" coords="2,219.98,542.70,72.76,9.01;2,79.32,554.58,27.61,9.01">Riloff and Wiebe (2003)</ref> extracted patterns for subjective expressions using a bootstrapping process.</p><p>We used unigrams as subjectivity clues and built four different systems to generate opinion-bearing word lists. After building these unigram lists, we checked each sentence in the relevant documents for the presence of opinion-bearing words. Sections 3.1 through 3.4 describe the four methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ISI Opinion-Bearing Word List</head><p>We developed a system to classify words as either opinion-bearing or non-opinion-bearing using WordNet and Wall Street Journal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Using WordNet</head><p>In pursuit of accuracy, we first manually collected a set of opinion-bearing words (34 adjectives and 44 verbs). Early classification trials showed that precision was very high (the system found only opinion-bearing sentences), but since the list was so small, recall was very low. We therefore used this list as seed words for expansion using WordNet.</p><p>Our assumption was that synonyms and antonyms of an opinion-bearing word could be opinion-bearing as well, as for example "nice, virtuous, pleasing, well-behaved, gracious, honorable, righteous" as synonyms for "good", or "bad, evil, disreputable, unrighteous" as antonyms.</p><p>However, not all synonyms and antonyms could be used: some such words seemed to exhibit both opinion-bearing and non-opinionbearing senses, such as "solid, hot, full, ample" for "good". This indicated the need for a scale of opinion valence (good or bad) strength. If we can measure the 'opinion-based closeness' of a synonym or antonym to a known opinion-bearing word, then we can determine whether to include it in the expanded set. To develop such a scale, we first created a non-opinion-bearing word list manually and produced related words for it using WordNet. To avoid collecting uncommon words, we started with a basic/common English word list compiled for foreign students preparing for the TOEFL test. From this we randomly selected 462 adjectives and 502 verbs for human annotation. Human1 and human2 annotated 462 adjectives and human3 and human2 annotated 502 verbs, labeling each word as either opinion-bearing or nonopinion-bearing. Now, to obtain a measure of opinion/nonopinion strength, we measured the WordNet distance of a target (synonym or antonym) word to the two sets of manually selected seed words plus their current expansion words. We assigned the new word to the closer category. The following equation represents this approach:</p><p>(1) ) ... where k f is the k th feature of category c which is also a member of the synonym set of the target word w, and count(f k ,synset(w)) means the total number of occurrences of f k in the synonym set of w. The motivation for this model is document classification. (Although we used the synonym set of seed words derived from WordNet, we could instead have obtained word features from a corpus.) After expansion, we obtained 2682 opinion-bearing and 2548 non-opinion-bearing adjectives, and 1329 opinion-bearing and 1760 non-opinion-bearing verbs, with strength values. Using these words as features we built a Naive Bayesian classifier and classified 32373 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Using WSJ Data</head><p>Experiments with the above set did not provide very satisfactory results on arbitrary text. For one reason, WordNet's synonym connections are simply not extensive enough. However, if we know the relative frequency of a word in opinionbearing texts compared to non-opinion-bearing text, we can use the statistical information instead of lexical information. For this, we collected a huge amount of data in order to make up for the limitations of collection 1.</p><p>Following the insight of <ref type="bibr" coords="3,250.05,440.76,12.63,9.01;3,277.74,440.76,14.92,9.01;3,79.32,452.64,100.48,9.01" target="#b11">Yu and Hatzivassiloglou (2003)</ref>, we made the basic and rough assumption that words that appear more often in newspaper editorials and letters to the editor than in non-editorial news articles could be potential opinion-bearing words (even though editorials contain sentences about factual events as well). We used the TREC collection to collect data, extracting and classifying all Wall Street Journal documents from it either as Editorial or nonEditorial based on the occurrence of the keywords "Letters to the Editor", "Letter to the Editor" or "Editorial" present in its headline. This produced in total 7053 editorial documents and 166025 non-editorial documents.</p><p>We separated out opinion from non-opinion words by considering their relative frequency in the two collections, expressed as a probability, using SRILM, SRI's language modeling toolkit (http://www.speech.sri.com/projects/srilm/). For every word W occurring in either of the document sets, we computed We used Kneser-Ney smoothing <ref type="bibr" coords="3,458.84,93.72,74.01,9.01;3,319.38,105.66,24.09,9.01" target="#b2">(Kneser and Ney, 1995)</ref> to handle unknown/rare words. Having obtained the above probabilities we calculated the score of W as the following ratio:</p><formula xml:id="formula_0" coords="3,367.26,143.53,126.92,21.72">alProb(W) nonEditori rob(W) EditorialP ) ( = W Score</formula><p>Score(W) gives an indication of the bias of each word towards editorial or non-editorial texts. We computed scores for 86,674,738 word tokens.</p><p>Naturally, words with scores close to 1 were untrustworthy markers of opinion valence. To eliminate these words we applied a simple filter as follows. We divided the Editorial and the non-Editorial collections each into 3 subsets. For each word in each {Editorial, non-Editorial} subset pair we calculated Score(W). We retained only those words for which the scores in all three subset pairs were all greater than 1 or all less than 1. In other words, we only kept words with a repeated bias towards Editorial or non-Editorial. This procedure helped eliminate some of the noisy words, resulting in 15568 words. Table <ref type="table" coords="3,464.21,360.18,5.17,9.01" target="#tab_2">1</ref> shows the top 10 words with strong biases in each direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Editorial</head><p>Non </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Merger of WordNet and WSJ Lists</head><p>So far, we have classified words as either opinion-bearing or non-opinion-bearing by two different methods. The first method calculates the degrees of closeness to manually chosen sets of opinion-bearing and non-opinion-bearing words in WordNet and decides its class and strength. When the word is equally close to both classes, it is hard to decide its subjectivity, and when WordNet does not contain a word or its synonyms, such as the word "antihomosexsual", we fail to classify it.</p><p>The second method, classification of words using WSJ texts, is less reliable than the lexical method.</p><p>However, it does for example successfully handle "antihomosexual". Therefore, we combined the results of the two methods, since their different characteristics compensate for each other.</p><p>After merging two opinion-bearing word lists, we experimented with a cut-off parameter value to select only strong opinion bearing words. Finally, 10682 words were selected and applied for our run ISIRUN204.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using the General Inquirer Dictionary</head><p>We created a third list of words similarly, by selecting positive and negative valence words from the General Inquirer Dictionary<ref type="foot" coords="4,219.60,209.93,3.30,5.76" target="#foot_0">1</ref> . Any sentence that contains either positive or negative words was selected as a relevant opinion.</p><p>Initial results using this list were unsatisfactory since it contained only 1,915 positive and 2,291 negative words. This motivated us to apply the same method we used for Section 3.  We tested this list on the TREC 2003 Novelty Track sentences of opinion topics. Table <ref type="table" coords="4,258.27,475.68,5.17,9.01" target="#tab_4">2</ref> shows the results. We also applied a stemmer to avoid strict string matching. (It is not suprising that only using Inquirer words without WordNet expansion performed poorly given its small size.) We applied this list for the run ISIRUN304.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using TREC 2003 Data</head><p>We used the official relevant sentences from the 2003 Novelty Track as training data for our run ISIRUN404. Using the file "qrels.relevant.03.txt", which contains all relevant sentences that were selected by human annotators at NIST, we divided all sentences in relevant documents into two categories: relevant (R) and non-relevant (NR). Then we applied the Brill tagger to locate all verbs, adjectives, nouns and modal verbs that we believed played important roles in determining relevance the standard procedure of stop-word removal and stemming. Having performed the search we return all sentences that have non-zero scores as the final answer. For example, issuing the query "Arrest of former Chilean dictator, General Augusto Pinochet, in London. He was charged with murder, torture, genocide, and terrorism during his regime in Chile." we obtain the following sentences as output:</p><p>1. Garzon's request for Pinochet's arrest and extradition on charges of genocide, torture and terrorism led to the dictator's detention in London last October.</p><p>2. The warrant said the general was wanted for questioning for "crimes of genocide and terrorism that includes murder''.</p><p>3. Prosecutors are arguing that the genocide, mass murder and torture charges against the former dictator should overrule Britain's immunity for former heads of state.</p><p>4. Pinochet was arrested at the instigation of a Spanish magistrate, who is seeking the general's extradition on charges of genocide, murder and torture during his 17-year-rule. . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submitted 5 Runs</head><p>We submitted 5 runs for Task 1. As a baseline system to compare our methods with, we produced ISIALL04 by marking every sentence in relevant documents as RELEVANT. Surprisingly, this baseline performed relatively well. For other 4 runs, we combined the event topic result described in Section 4 with the 4 opinion topic methods described in Section 3, since we focused mainly on opinion topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we presented our work on the Novelty track at TREC 2004. This track presents several challenges in terms of treating a sentence as information unit. We focused on recognizing relevant sentences from opinion type topics. Unlike event topics, we did not consider whether a sentence contains any phrase referring to the topic. We assumed that whether a sentence is opinionbearing or not is much more important. We are curious to learn about other methods for document filtering and the effect on the overall performance of a system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,137.04,264.84,97.96,8.22"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Topic example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,453.92,597.06,5.53,9.65;2,424.74,597.06,2.77,9.65;2,403.92,597.06,2.21,9.65;2,393.66,597.06,3.68,9.65;2,368.40,597.06,19.05,9.65;2,354.36,597.06,14.13,9.65;2,406.62,572.76,3.68,9.65;2,395.40,572.76,2.21,9.65;2,385.14,572.76,3.68,9.65;2,359.82,572.76,19.05,9.65;2,345.78,572.76,14.13,9.65;2,441.60,602.00,3.87,6.75;2,420.96,602.00,3.87,6.75;2,471.18,602.00,3.87,6.75;2,368.46,609.38,3.44,6.75;2,359.94,585.14,3.44,6.75;2,457.98,597.06,14.75,9.65;2,428.46,597.06,14.75,9.65;2,408.54,597.06,14.75,9.65;2,397.14,597.06,4.91,9.65;2,387.24,597.06,6.76,9.65;2,400.02,572.76,7.38,9.65;2,388.56,572.76,4.91,9.65;2,378.72,572.76,6.76,9.65;2,345.96,593.20,6.06,13.54;2,319.38,617.70,213.39,9.01;2,319.38,628.98,213.36,9.64;2,319.38,640.26,213.27,9.01;2,319.38,651.60,213.40,9.01;2,319.38,662.88,141.32,9.01;3,275.76,110.68,12.51,9.36;3,207.06,110.68,3.57,9.36;3,197.58,110.68,2.15,9.36;3,181.08,110.68,3.57,9.36;3,154.50,110.68,3.57,9.36;3,145.32,110.68,3.57,9.36;3,116.70,110.68,18.48,9.36;3,100.74,110.68,13.70,9.36;3,263.16,80.86,3.57,9.36;3,253.62,80.86,2.15,9.36;3,166.08,80.86,3.57,9.36;3,154.50,80.86,3.57,9.36;3,145.32,80.86,3.57,9.36;3,116.70,80.86,18.48,9.36;3,100.74,80.86,13.70,9.36;3,262.50,58.54,3.57,9.36;3,253.02,58.54,2.15,9.36;3,239.64,58.54,3.57,9.36;3,228.06,58.54,3.57,9.36;3,218.94,58.54,3.57,9.36;3,190.32,58.54,18.48,9.36;3,174.30,58.54,13.70,9.36;3,159.36,58.54,3.57,9.36;3,147.42,58.54,2.15,9.36;3,136.56,58.54,3.57,9.36;3,107.94,58.54,18.48,9.36;3,91.92,58.54,13.70,9.36;3,167.40,123.99,3.13,5.46;3,262.56,108.69,4.17,5.46;3,255.06,108.69,2.08,5.46;3,236.34,108.69,1.57,5.46;3,226.56,108.69,2.08,5.46;3,227.40,83.79,4.70,5.46;3,224.16,83.79,3.13,5.46;3,204.36,83.79,3.13,5.46;3,184.92,83.79,3.13,5.46;3,158.16,104.18,13.25,19.70;3,164.16,121.81,3.43,7.66;3,92.10,106.93,5.88,13.14;3,92.10,77.11,5.88,13.14;3,165.72,54.79,5.88,13.14;3,162.84,102.63,4.52,5.46;3,160.38,123.99,2.78,5.46;3,257.88,108.69,4.18,5.46;3,238.38,108.69,15.30,5.46;3,230.46,108.69,1.74,5.46;3,211.26,108.69,13.91,5.46;3,191.04,116.31,2.78,5.46;3,117.00,122.25,2.78,5.46;3,248.28,83.79,3.13,5.46;3,117.00,92.37,2.78,5.46;3,190.62,70.11,2.78,5.46;3,108.24,70.11,2.78,5.46;3,232.80,111.43,1.99,3.90;3,201.78,110.68,4.77,9.36;3,187.20,110.68,2.98,9.36;3,173.88,110.68,6.56,9.36;3,149.22,110.68,4.77,9.36;3,138.12,110.68,6.56,9.36;3,257.88,80.86,4.77,9.36;3,233.34,80.86,14.31,9.36;3,208.98,80.86,14.31,9.36;3,189.00,80.86,14.31,9.36;3,170.28,80.86,14.31,9.36;3,158.88,80.86,6.56,9.36;3,149.22,80.86,4.77,9.36;3,138.12,80.86,6.56,9.36;3,257.22,58.54,4.77,9.36;3,243.84,58.54,7.16,9.36;3,232.44,58.54,6.56,9.36;3,222.78,58.54,4.77,9.36;3,211.74,58.54,6.56,9.36;3,152.04,58.54,7.16,9.36;3,140.40,58.54,4.77,9.36;3,129.36,58.54,6.56,9.36"><head></head><label></label><figDesc>a category (opinion-bearing or nonopinion-bearing), w is the target word, and syn n is the synonyms or antonyms of the given word by WordNet. To compute equation (1), we built a classification model, equation (2):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,334.50,396.30,183.13,156.36"><head>Table 1 : Top 10 words of collection 2 for each category.</head><label>1</label><figDesc></figDesc><table coords="3,468.26,396.30,44.23,9.01"><row><cell>-Editorial</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,99.90,447.06,172.11,8.22"><head>Table 2 : Results Using Inquirer Dictionary</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,79.32,543.77,213.38,142.86"><head>Table 4 : Performance of 5 submitted runs</head><label>4</label><figDesc>Table4shows the performances of the 5 runs.</figDesc><table coords="5,83.46,581.27,199.91,84.74"><row><cell>Run</cell><cell>Precision</cell><cell>Recall</cell><cell>F-score</cell></row><row><cell>ISIALL04</cell><cell>0.26</cell><cell>0.84</cell><cell>0.371</cell></row><row><cell>ISIRUN204</cell><cell>0.30</cell><cell>0.74</cell><cell>0.385</cell></row><row><cell>ISIRUN304</cell><cell>0.30</cell><cell>0.73</cell><cell>0.387</cell></row><row><cell>ISIRUN404</cell><cell>0.30</cell><cell>0.71</cell><cell>0.390</cell></row><row><cell>ISIRUN504</cell><cell>0.30</cell><cell>0.74</cell><cell>0.385</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,94.20,705.90,146.55,8.22;4,95.64,717.66,195.46,8.22"><p>http://www.wjh.harvard.edu/~inquirer/ http://www.wjh.harvard.edu/~inquirer/homecat.htm</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and subjectivity of a sentence. For each word, we calculated the probability of the word being relevant and non-relevant based on a model described by the following formulas. After collecting these words, we checked each sentence in the test data and marked it as "relevant" if it contains at least one of them. For the official run, we experimentally selected Î»=0.0001 based on development test data of Novelty 2003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combination of All Three</head><p>For the run ISIRUN504, we simply combined all words from 3.1 through 3.3 and used them to select relevant sentences from the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Event Topics</head><p>We treat event identification as a traditional document IR task. The goal here is to identify those event sentences that are relevant to the given event from the given list of documents. For the IR part we treat each sentence independently of other sentences and index them accordingly. We thus reduce the problem of event identification to that of sentence retrieval.</p><p>We choose the description &lt;desc&gt; field for formulating the query. To perform IR we use a probabilistic Bayesian inference network model as implemented in the search engine software package INQUERY. For each query we perform</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,319.38,246.42,213.28,9.02;5,330.00,258.30,202.68,9.02;5,330.00,270.18,202.68,9.02;5,330.00,282.12,180.79,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,330.00,258.30,142.02,9.02">The INQUERY Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,480.96,258.30,51.72,9.02;5,330.00,270.18,202.68,9.02;5,330.00,282.12,176.57,9.02">Proceedings of DEXA-92, 3rd International Conference on Database and Expert Systems Applications</title>
		<meeting>DEXA-92, 3rd International Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,299.64,213.34,9.02;5,330.00,311.57,202.68,9.02;5,330.00,323.46,80.30,9.02" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="http://www.cogsi.princeton.edu/~wn" />
		<title level="m" coord="5,330.00,311.57,111.68,9.02">Adjectives in WordNet</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,340.98,213.33,9.02;5,330.00,352.92,202.69,9.02;5,330.00,364.80,116.70,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,450.84,340.98,81.88,9.02;5,330.00,352.92,143.58,9.02">Improved Backingoff for n-gram Language Modeling</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,480.96,352.92,51.73,9.02;5,330.00,364.80,41.38,9.02">Proceedings of ICASSP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,382.38,213.35,9.02;5,330.00,394.26,202.65,9.02;5,330.00,406.13,202.73,9.02;5,330.00,418.01,104.82,9.02" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="http://www.cogsi.princeton.edu/~wn" />
		<title level="m" coord="5,423.57,394.26,109.08,9.02;5,330.00,406.13,135.01,9.02">Introduction to WordNet: An On-Line Lexical Database</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,435.59,213.37,9.02;5,330.00,447.53,202.78,9.02;5,330.00,459.41,130.38,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,449.50,435.59,83.25,9.02;5,330.00,447.53,198.39,9.02">Learning Extraction Patterns for Opinion-bearing Expressions</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,330.00,459.41,124.80,9.02">Proceedings of the EMNLP-03</title>
		<meeting>the EMNLP-03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,476.93,213.29,9.02;5,330.00,488.81,202.75,9.02;5,330.00,500.75,175.56,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,495.38,476.93,37.30,9.02;5,330.00,488.81,202.75,9.02;5,330.00,500.75,56.83,9.02">Learning Subjective Nouns Using Extraction Pattern Bootstrapping</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,393.84,500.75,106.53,9.02">Proceedings of CoNLL-03</title>
		<meeting>CoNLL-03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,518.27,213.42,9.02;5,330.00,530.21,202.73,9.02;5,330.00,542.09,52.05,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,465.20,518.27,67.59,9.02;5,330.00,530.21,124.18,9.02">Overview of the TREC 2003 Novelty Track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,465.78,530.21,66.96,9.02;5,330.00,542.09,46.84,9.02">Proceedings of TREC-2003</title>
		<meeting>TREC-2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,559.61,213.39,9.02;5,330.00,571.55,202.77,9.02;5,330.00,583.43,202.68,9.02;5,330.00,595.37,52.55,9.02" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<title level="m" coord="5,412.92,559.61,119.85,9.02;5,330.00,571.55,202.77,9.02;5,330.00,583.43,198.27,9.02">SRILM -An Extensible Language Modeling Toolkit. Proceedings of the Intl. Conf. Spoken Language Processing</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,612.89,213.26,9.02;5,330.00,624.77,202.73,9.02;5,330.00,636.65,202.76,9.02;5,330.00,648.59,10.33,9.02;5,340.32,646.37,5.16,5.76;5,349.92,648.60,182.81,9.02;5,330.00,660.48,38.76,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,396.38,612.89,136.26,9.02;5,330.00,624.77,202.73,9.02;5,330.00,636.65,109.47,9.02">Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,449.70,636.65,83.06,9.02;5,330.00,648.59,10.33,9.02;5,340.32,646.37,5.16,5.76;5,349.92,648.60,118.56,9.02">Proceedings of the 40 th Annual Meeting of the ACL</title>
		<meeting>the 40 th Annual Meeting of the ACL<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,319.38,678.06,213.33,9.02;5,330.00,689.88,157.30,9.02;5,487.26,687.71,5.16,5.76;5,496.50,689.88,36.16,9.02;5,330.00,701.82,202.69,9.02;5,330.00,713.70,96.25,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,403.48,678.06,129.24,9.02;5,330.00,689.88,54.11,9.02">Learning subjective adjectives from corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,392.46,689.88,94.84,9.02;5,487.26,687.71,5.16,5.76;5,496.50,689.88,36.16,9.02;5,330.00,701.82,164.53,9.02">Proceedings of the 17 th National Conference on Artificial Intelligence</title>
		<meeting>the 17 th National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.32,56.16,213.39,9.02;6,90.00,68.04,202.72,9.02;6,90.00,79.98,77.00,9.02" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,246.19,56.16,46.52,9.02;6,90.00,68.04,117.22,9.02">Annotating Opinions in the World Press</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,214.38,68.04,78.34,9.02;6,90.00,79.98,71.96,9.02">Proceedings of the ACL SIGDIAL-03</title>
		<meeting>the ACL SIGDIAL-03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.32,97.44,213.39,9.02;6,90.00,109.38,202.67,9.02;6,90.00,121.26,202.69,9.02;6,90.00,133.20,202.65,9.02;6,90.00,145.08,23.22,9.02" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,256.44,97.44,36.26,9.02;6,90.00,109.38,202.67,9.02;6,90.00,121.26,202.69,9.02;6,90.00,133.20,80.33,9.02">Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,181.68,133.20,110.97,9.02;6,90.00,145.08,18.58,9.02">Proceedings of EMNLP-2003</title>
		<meeting>EMNLP-2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
