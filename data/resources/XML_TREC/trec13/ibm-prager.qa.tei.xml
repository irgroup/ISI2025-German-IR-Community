<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.24,87.16,263.54,16.65">IBM&apos;s PIQUANT II in TREC2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,90.00,124.04,97.26,10.80"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
						</author>
						<author>
							<persName coords="1,195.42,124.04,78.04,10.80"><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
							<email>kczuba@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,282.14,124.04,55.42,10.80"><forename type="first">John</forename><surname>Prager</surname></persName>
							<email>jprager@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,385.04,124.04,116.01,10.80"><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
							<email>sashabg@cs.columbia.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Abraham Ittycheriah Dept. of Computer Science</orgName>
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University Yorktown Heights</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>New York</settlement>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.24,87.16,263.54,16.65">IBM&apos;s PIQUANT II in TREC2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFB1B1929FC8230CADD9B691651CB351</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>PIQUANT II, the system we used for TREC 2004, is a completely reengineered system whose core functionalities for answering factoid and list questions remain largely unchanged from previous years <ref type="bibr" coords="1,161.27,250.22,111.16,9.94" target="#b3">[Chu-Carroll et al, 2003</ref><ref type="bibr" coords="1,272.43,250.22,96.61,9.94" target="#b9">, Prager et al, 2004]</ref>. We continue to address these questions using our multi-strategy and multi-source approach. For "other" questions, we experimented with two alternative approaches, one that uses statistical collocation information for extracting prominent passages related to the target, and the other which is a slight variation of the QA-by-Dossier approach we employed last year <ref type="bibr" coords="1,318.08,300.86,92.81,9.94" target="#b9">[Prager et al, 2004]</ref> that asks a set of subquestions of interest about the target and returns a set of relevant passages that answer these subquestions. In addition, to address this year's new question format, we developed a question preprocessing component to interpret individual questions against the given target to generate a selfcontained natural language question as expected by subsequent components of our QA system. NIST assessed scores showed substantial improvement of our new PIQUANT II system over earlier versions of our QA system both in terms of absolute scores as well as relative improvement compared to the best and median scores in each of the three component subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The PIQUANT II System</head><p>In an effort to create an efficient development and test platform for question answering, we devoted a significant amount of time to system re-engineering. The primary goals of this reengineering effort were to address the portability, component reusability, and speed of our original PIQUANT system. The system that we used for the TREC-2004 evaluation was built within our new PIQUANT II architecture framework <ref type="bibr" coords="1,363.40,486.14,98.31,9.94">[Czuba, forthcoming]</ref>. The main characteristics of the framework are:</p><p>1. Full Java implementation. 2. Plug-and-play architecture. 3. Well-defined, standardized APIs for typical QA system components. 4. Distributed client-server deployment.</p><p>Our new architecture continues to support our multi-agent approach to QA where different strategies are employed to address different question types. Figure <ref type="figure" coords="1,403.44,599.90,5.52,9.94" target="#fig_0">1</ref> shows a diagram of the PIQUANT II framework as instantiated in the factoid and list subtasks of our TREC 2004 runs. The box labeled "Answer Agents" in the middle of the diagram is the focus of our framework, which allows for plug-and-play of multiple answering agents that make up the core of a QA system. The PIQUANT II framework provides the machinery to execute these answering agents in parallel, and to accumulate and send the results from individual agents to the answer resolution component. The answer resolution component combines top candidate answers from each agent and produces the top n answers that represent the answers of the QA system as a whole. In our TREC 2004 runs, the answer resolution component adopts an equal a priori probability weighting scheme and thus simply sums the agent confidence scores of all semantically equivalent answers and outputs the answer with the highest combined score. The answer justification component attempts to locate a passage in a reference corpus that justifies a given answer to a question. This component is useful when an answer to a question was found via other means (such as through a database lookup or on the web), but a "trusted source" is needed to justify the answer. In the case of PIQUANT II in TREC, it is used when an answer is found by our KSP agent, which performs a database lookup of the attribute value of an object for questions such as "What is the capital of Canada?," and a document in the reference corpus is required to support the answer.</p><p>The question preprocessor performs any transformation needed on the system input to ensure that answering agents receive their expected input (usually self-contained natural language questions). In the next section, we describe the question preprocessor we developed to address this year's QA task, as well as the new answering agents we have developed in this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIQUANT II Components in TREC 2004</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Preprocessor</head><p>Since this year's track introduced the notion of a question target and a set of questions related to it, a question preprocessor was required to perform anaphora resolution on each question against the target to produce a self-contained natural language question for subsequent processing. Since we did not have a full anaphora resolution module available, we developed a set of heuristics to handle the different kinds of anaphora we thought were likely to occur. However, prior to performing anaphora resolution, we analyze the target to extract appositional constructs. The target analysis task focused primarily on extracting proper names from appositional constructs. For example, the target the band Nirvana was preprocessed to a modifier the band and Nirvana that became our new target. The new target was used in anaphora resolution as described below, whereas the modifier was kept and added to the search query for disambiguation purposes.</p><p>Our answer selection ranking algorithm makes use of syntactic relationships extracted from the question text; it is thus highly desirable that the question text contains the actual target instead of an anaphor. In order to satisfy this requirement we addressed two anaphora types in questions:</p><p>1. Pronominal anaphora including possessives. 2. Definite NPs.</p><p>We divided anaphoric pronouns into two groups, those that are unambiguous <ref type="bibr" coords="3,431.32,225.98,93.66,9.94;3,90.00,238.70,55.86,9.94">(him, his, hers, it, its, their, theirs)</ref>, and those that are ambiguous (her: possessive vs. accusative). For unambiguous pronouns, we replaced them in the question text with an appropriate form of the target. E.g. for the target Nirvana, the question What is its biggest hit? is transformed to What is Nirvana's biggest hit? For the ambiguous her, the question is first parsed using IBM's ESG parser <ref type="bibr" coords="3,481.92,276.62,43.04,9.94;3,90.00,289.34,25.83,9.94" target="#b7">[McCord, 1989]</ref> to determine whether it is an accusative or possessive pronoun and the question is transformed appropriately. We assume that only one pronoun is likely to occur in a question, and thus we replace the pronoun earliest in the sentence. We also assume that the pronoun refers to the question target as opposed to an entity in a preceding question.</p><p>If the question does not contain a pronoun, it is checked for the presence of definite noun phrases. We assume that definite noun phrases (which we in turn assume begin with a definite article) are all referential if the question target does not appear in the question. In order to enable the right set of relationships to be build for such NPs, we add an of-PP to the question. E.g., for the target IBM, the question Where are the company's headquarters? is transformed to Where are the company of IBM's headquarters? Although the resulting English is somewhat odd, our parser handles the transformed questions correctly and an appropriate set of syntactic relationships is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answering Agents</head><p>For the most part, the answering agents we employed this year are algorithmically the same as the agents we used last year, re-engineered to conform to our new architecture. The two exceptions are the Juru-based predictive annotation agent and the profile agent, whose development illustrates some of the key features of the PIQUANT II architecture. The modular architecture and standardized APIs in the PIQUANT II framework enabled component reuse and thus allowed us to rapidly deploy new agents. This was crucial for the development of the profile agent given the short time we had to address the "other" question type before the submission deadline. By reusing existing components as building blocks, the first version of the profile agent was literally built within hours. We then spent the next two weeks experimenting with various concept and passage extraction strategies to obtain the version of the agent used in our submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Juru-Based Predictive Annotation Agent</head><p>The Juru-based predictive annotation agent implements our previously reported Predictive Annotation strategy <ref type="bibr" coords="3,185.04,641.42,96.03,9.94" target="#b8">[Prager et al., 2000]</ref> in our new framework. In doing so, we adopted components that conform to IBM's Unstructured Information Management Architecture <ref type="bibr" coords="3,481.72,653.90,40.24,9.94;3,90.00,666.62,75.05,9.94" target="#b6">[Ferrucci and Lally, 2004]</ref>, including various named entity annotators, parser, and search engine. The resulting answering agent ended up being significantly different in terms of component strategies and performance from the earlier implementation that was deployed in previous years' TREC evaluations. The primary difference is the use of the Juru search engine developed at IBM's Haifa Labs, which placed first in the "precision at 10" category in the TREC web track in 2001 <ref type="bibr" coords="4,485.76,74.30,36.06,9.94;4,90.00,87.02,51.84,9.94" target="#b1">[Carmel et al., 2002]</ref>. In particular, we used JuruXML <ref type="bibr" coords="4,300.06,87.02,90.68,9.94" target="#b2">[Carmel et al., 2003]</ref>, an extension of Juru, which supports queries over XML fragments, and enables us to implement our predictive annotation scheme. Juru/JuruXML has different characteristics than the GuruQA passage retrieval engine we previously used in several regards. First, it adopts a tf*idf based ranking scheme, rather than the weighted Boolean query scheme in GuruQA. As a result, the documents/passages retrieved for the same query could differ dramatically. Second, our previous query building process relied quite heavily on morphological and synonym expansions of keywords. However, Juru query syntax does not support treating a group of words/phrases as synonyms in its scoring process. For these reasons and in a general effort to improve the quality of our information retrieval, we developed a new query building component.</p><p>Our approach in creating this new query builder was to make a parameterized component which would allow us to easily explore the effects of different query building strategies. These parameters controlled how elements of our question analysis output such as predicted answer type, question keywords/phrases or question semantic type would be included, excluded or required in the query. Using data from previous TRECs, we analyzed the effect of these settings in terms of document/passage recall (i.e. the proportion of documents/passages known to contain correct answers which were retrieved) and end-to-end MRR performance. Although work to optimize these parameters is ongoing, we observed that certain settings improved results on training data, and used those settings for our submitted run. Settings which improved results included:</p><p>? Requiring a predictive-annotation token of the predicted answer type ? Requiring the highest-idf single word from the question, based on the idea that this term often serves as an anchor or "selector" term (cf. <ref type="bibr" coords="4,338.47,379.34,121.57,9.94" target="#b10">[Ramakrishnan et al., 2004]</ref>) ? Including all question keywords (i.e. not using stoplisting but rather allowing Juru's tf*idf ranking to determine term significance) ? Excluding WordNet-based synonym or hypernym expansions (various restrictions and strategies were applied, none improved results)</p><p>After Juru returns the top-ranked documents from the query, we apply density-based passage retrieval to identify the most promising passages, and lastly perform answer selection<ref type="foot" coords="4,465.60,467.33,3.48,6.26" target="#foot_1">2</ref> .</p><p>In terms of accuracy, the new predictive annotation agent is comparable to our previous implementation. However, because of the difference in search engine behavior, there is substantial non-overlap in the actual questions on which each agent scored, we therefore included both implementations of the agent in the QA system we used in our submission.</p><p>In terms of run time, the new Java implementation on Linux (with a couple of Windowsdependent components running as servers) averages 7 seconds per question, compared with 2 minutes and 26 seconds per question on our old implementation in Perl/C/Java on AIX.<ref type="foot" coords="4,492.00,581.09,3.48,6.26" target="#foot_2">3</ref> This significant improvement in system speed, although not a factor in the TREC QA track evaluation, is crucial to end-user experience, to enabling more rapid regression tests, and to allowing for more complex processing mechanisms to be adopted in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Profile Agent</head><p>The profile agent was the first new agent developed within the PIQUANT II framework, and was designed to select information of interest about a given target from a reference corpus. In contrast to the approach taken by many other systems to date where the system attempts to identify both the definition of the target (expressed, for instance, in a copula construction or as an apposition) and other prominent information of interest (cf. <ref type="bibr" coords="5,356.16,142.94,137.98,9.94" target="#b0">[Blair-Goldensohn et al., 2004;</ref><ref type="bibr" coords="5,497.52,142.94,24.43,9.94;5,90.00,155.42,42.43,9.94" target="#b11">Xu et al., 2004;</ref><ref type="bibr" coords="5,136.08,155.42,93.71,9.94" target="#b5">Echihabi et al., 2004]</ref>), our profile agent adopts a statistical collocation-based method and focuses on identifying passages that convey prominent information associated with the target that is difficult to discover based on syntactic information alone.</p><p>The profile agent extracts relevant information in a three-stage process. In the first stage, short passages about the target are extracted from the reference corpus. In the second stage, entities that are strongly associated with the target are identified from within these passages. In the third stage, a subset of the extracted passages is chosen to convey the relationship between the selected entities and the target. We experimented with various strategies in all three stages, and in this paper, we briefly outline the strategies used in our submitted run. Note that since we only started to implement this agent two weeks prior to the QA evaluation period, we experimented with different strategies to the best of our abilities at that time. We are aware that there is significant room for future experimentation and improvement and intend to further develop this agent.</p><p>The passage extraction process reuses the search component in our Juru-based predictive annotation agent. A search query is formulated based on the given target to retrieve the 100 most relevant documents, from which up to 500 one-sentence passages relevant to the target are selected. In the entity selection phase, the extracted passages are processed by the ESG parser and all common nouns are identified and normalized. The occurrence count for each normalized noun is compared against the expected count based on its idf value in the corpus, and a score is assigned for each noun which is the difference between these two counts. The system then selects as candidate concepts to be included in the output those nouns whose score exceeds a certain predetermined threshold. In the final stage, the profile agent selects as its output a subset of the extracted passages that best conveys the candidate concepts. These passages are selected by considering each candidate concept in ranked order: of all passages that include the currently highest ranked candidate concept, the passage that contains the maximum number of other candidate concepts is selected. All concepts mentioned in the selected passage are removed from the candidate list and the process iterates until either the candidate concept list is exhausted or the maximum number of passages is reached (in our run, the maximum passage count is 20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA-by-Dossier Agent</head><p>Except in one respect, our use of QA-by-Dossier (QbD) was identical to last year's. QbD consists of building ahead of time a set of auxiliary questions for each broad target type. For example, for PERSON there was "When was X born?", for ORGANIZATION "Who is the CEO of X?", for THING "What does X do?". QbD then calls our QA system recursively so that the factoid-oriented agents can answer these questions. We had about a dozen auxiliary questions per type. Acknowledging that some important nuggets might be missed by this agent (but possibly found by others), we used this approach because analysis of biographies and obituaries had determined that a large percentage of the de facto important definitional information was in the form of answers to such boilerplate questions.</p><p>The major difference between our QbD run this year and last (other than the different platform, and that we had no time to learn new thresholds) was that we returned entire sentences instead of exact answers. Our factoid QA-system returns exact answers, and last year for definitional questions we returned the exact answers prefixed by a term indicating the relationship in the question, for example "born: 1900". Our definitional question results last year were disappointing, and we wondered whether the short nature of our returned answers hurt us because the context was missing, so this year we returned the entire sentence that contained the exact answer. As will be shown in the Analysis section below, we did better with long answers but for an entirely different reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Performance and Analysis</head><p>We submitted two runs to the TREC 2004 QA track whose only difference is in the strategy adopted for answering "other" questions. For both runs, the factoid questions are answered by submitting each question to all available factoid agents and weighing the answers proposed by each agent in proportion to their respective confidence scores. We did not adopt a NIL strategy and therefore returned our best answer for each question. For list questions, we observed from last year's results that our system has relatively high precision in its top answers, but does not fare well in recall even if more answers are returned; therefore, we chose to target precision by returning our top 5 answers for each list question. For the "other" questions, run IBM1 adopted the profile agent for selecting significant passages related to the target, while run IBM2 used the QA-by-Dossier agent to return passages that answer each sub-question about the target. The results for these runs as scored by the NIST assessors are shown in Table <ref type="table" coords="6,414.48,309.98,4.20,9.94" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Analysis</head><p>Since our entry this year was based on a re-engineered system with several new strategies employed, we are particularly interested in how our system performed relative to our old system which had been in continuous development over 5 years. Table <ref type="table" coords="6,394.08,447.50,5.52,9.94" target="#tab_2">2</ref> summarizes the results of comparing our higher-scoring run this year against our performance from TREC 2003. In an attempt to account for possible differences in task difficulty, we further contrast our system performance changes between 2003 and 2004 against changes in the best and median scores of all submitted runs for each of the three subtasks. Our analysis shows that in absolute numbers, our new system performs better than the old system on all three subtasks, and that in terms of relative improvement, the percent increase figures range widely from 5% to 160%. A closer examination of the performance in each subtask shows that for factoid questions, for which the main difference from last year is the addition of a new implementation of our predictive annotation agent employing a different search strategy, our percent change falls in between the percent changes of the best and median scores, suggesting that our improvement is on target based on the global trend. For list questions, we inherited the improvement in factoid question answering, as well as changed our thresholding strategy to target high precision in our returned answer set. These two factors combined turned out to have a dramatic impact, achieving significantly better performance improvement over the median scoring system (our system's list score last year was only slightly above median). Finally, for the "other" category, our system, through a combination of adopting different selection strategies and returning passage-length answers, achieved a 61% relative improvement over last year, while the best and median scores both decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA-by-Dossier Performance</head><p>We sought to determine whether our long answers to the "other" questions performed better than our exact answers to definition questions last year -in particular we were concerned that the brevity of our exact-answer format might have hurt us in the judging. Last year we achieved 32 vital nuggets out of a total of 207, giving a recall of 0.155. To compute a comparable figure for this year, we needed to simulate last year's operational setup and scoring. To do that, we had to add to the vital nuggets those factoid questions that were paraphrases of those in the QbD auxiliary question sets (since if the same answer is provided in the "other" section as in the factoid, the one in the factoid section gets the score). By our count, we got 46 factoid questions that were in the QbD auxiliary set, plus 12 vital nuggets that were answers to QbD questions. There were a total of 146 QbD factoid questions, and 234 acceptable vital nuggets, giving a recall of (46+12)/(146+234) = 0.153. This is eerily close to last year's figure, but due to the number of variable factors involved, the safest conclusion is simply that there was no perceptible gain in the assessor's ability to detect correct nuggets from sentence-length answers.</p><p>However, we did perform better with longer answers than last year, but for a different reason.</p><p>Our auxiliary questions scored only12 vital nuggets that were direct answers to these questions, but 64 vital nuggets that were in the answer sentences "by chance". Likewise, we scored 18 "okay" nuggets as direct answers to the QbD questions, and 76 "by chance". This reinforces a phenomenon that we've been observing in our work with QA, which is that exact-answer QA retrieval systems can be used effectively for passage-retrieval applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>In this paper, we described our PIQUANT II system as configured in the TREC 2004 QA runs.</p><p>Our effort this year focused on a the reengineering of our QA system in order to 1) have a welldesigned QA architecture for future development and 2) significantly reduce system response time. For our submitted runs, one of the primary improvements we made was the development of the profile agent, which achieved a 61% relative improvement over our score in the corresponding subtask last year when the trend in all submitted runs shows a decrease. Our change in strategy for the list questions to target high precision in the answer set also appeared to have a significant impact, while our improvement in the factoid task appears to be on target compared to best and median score changes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,109.44,372.08,393.14,9.07"><head>Figure 1</head><label>1</label><figDesc>Figure 1 PIQUANT II Framework as Instantiated for TREC 2004 Factoid &amp; List Questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,335.99,388.33,52.60"><head>Table 1 Assessed Scores for Submitted Runs</head><label>1</label><figDesc></figDesc><table coords="6,90.00,335.99,388.33,34.46"><row><cell>Run</cell><cell>Factoid</cell><cell>List</cell><cell>Other</cell><cell>Overall</cell></row><row><cell>IBM1</cell><cell>.313</cell><cell>.200</cell><cell>.285</cell><cell>.278</cell></row><row><cell>IBM2</cell><cell>.313</cell><cell>.200</cell><cell>.227</cell><cell>.263</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,523.91,420.27,191.08"><head>Table 2 Comparison of PIQUANT Results against Best and Median Scores of All Runs</head><label>2</label><figDesc></figDesc><table coords="6,90.00,523.91,420.27,160.22"><row><cell></cell><cell>Best Score</cell><cell>Median Score</cell><cell>PIQUANT Score</cell></row><row><cell>Factoid</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2003</cell><cell>0.700</cell><cell>0.177</cell><cell>0.298</cell></row><row><cell>2004</cell><cell>0.770</cell><cell>0.170</cell><cell>0.313</cell></row><row><cell>% change</cell><cell>+10%</cell><cell>-4%</cell><cell>+5%</cell></row><row><cell>List</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2003</cell><cell>0.396</cell><cell>0.069</cell><cell>0.077</cell></row><row><cell>2004</cell><cell>0.622</cell><cell>0.094</cell><cell>0.200</cell></row><row><cell>% change</cell><cell>+57%</cell><cell>+36%</cell><cell>+160%</cell></row><row><cell>Other</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2003</cell><cell>0.555</cell><cell>0.192</cell><cell>0.177</cell></row><row><cell>2004</cell><cell>0.460</cell><cell>0.184</cell><cell>0.285</cell></row><row><cell>% change</cell><cell>-17%</cell><cell>-4%</cell><cell>+61%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,96.00,698.96,404.45,9.07;1,90.00,710.48,101.54,9.07"><p>Part of this work was conducted while Sasha Blair-Goldensohn was a summer intern at the IBM T. J. Watson Research Center.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,95.97,653.12,422.68,9.07;4,90.00,664.40,127.37,9.07"><p>Our question analysis and answer selection components are algorithmically the same as their counterparts in the previous implementation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,95.90,675.92,403.19,9.07;4,90.00,687.44,427.69,9.07;4,90.00,698.96,415.66,9.07;4,90.00,710.48,422.68,9.07"><p>Although the predictive annotation agent is a general purpose agent, it does not attempt to answer all question types. To be more comprehensive in coverage, a QA system will include more specialized agents in addition to this agent, which may increase response time (however, as the agents are now executed in parallel, the overall execution time is the maximum time needed by all agents plus some slight overhead).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Dave Ferrucci</rs> for helpful discussions, and <rs type="person">Elena Filatova</rs> for her help in testing the question pre-processing component.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,90.00,166.94,431.30,9.94;8,90.00,179.66,352.78,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,367.32,166.94,153.98,9.94;8,90.00,179.66,85.40,9.94">Answering Definitional Questions: A Hybrid Approach</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schlaikjer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,186.24,179.66,170.23,9.94">New Directions in Question Answering</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,205.10,432.05,9.94;8,90.00,217.58,394.08,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,490.66,205.10,31.39,9.94;8,90.00,217.58,195.92,9.94">Juru at TREC 10 --Experiments with Index Pruning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hersovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,293.52,217.58,114.96,9.94">Proceedings of TREC2001</title>
		<meeting>TREC2001</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="228" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,243.02,431.98,9.94;8,90.00,255.50,326.16,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,448.18,243.02,73.80,9.94;8,90.00,255.50,135.71,9.94">Searching XML documents via XML fragments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mandelbrod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,233.28,255.50,119.07,9.94">Proceedings of SIGIR 2003</title>
		<meeting>SIGIR 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,280.94,432.00,9.94;8,90.00,293.42,409.02,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,424.60,280.94,97.40,9.94;8,90.00,293.42,207.14,9.94">A Multi-Strategy and Multi-Source Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,305.04,293.42,114.96,9.94">Proceedings of TREC2002</title>
		<meeting>TREC2002</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,318.86,431.99,9.94;8,90.00,331.58,59.49,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,149.04,318.86,367.92,9.94">Extendable Plug-and-Play Architecture Framework for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,356.78,431.91,9.94;8,90.00,369.50,308.15,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,480.76,356.78,41.15,9.94;8,90.00,369.50,176.24,9.94">Multiple-Engine Question Answering in TextMap</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,277.44,369.50,114.96,9.94">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2003">2004. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,394.70,431.35,9.94;8,90.00,407.42,432.23,9.94;8,90.00,420.14,94.55,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,235.68,394.70,285.67,9.94;8,90.00,407.42,228.79,9.94">UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,327.84,407.42,189.65,9.94">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,445.34,431.94,9.94;8,90.00,458.06,240.00,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,177.84,445.34,344.10,9.94;8,90.00,458.06,175.84,9.94">Slot grammar: A system for simpler construction of practical natural language grammars. Natural Language and Logic</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mccord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="118" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,483.26,431.90,9.94;8,90.00,495.98,313.43,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,366.00,483.26,155.90,9.94;8,90.00,495.98,47.34,9.94">Question-Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,147.84,495.98,119.13,9.94">Proceedings of SIGIR 2000</title>
		<meeting>SIGIR 2000<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,521.18,431.87,9.94;8,90.00,533.90,238.79,9.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,493.25,521.18,28.63,9.94;8,90.00,533.90,109.29,9.94">IBM&apos;s PIQUANT in TREC2003</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mahindru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,208.08,533.90,114.95,9.94">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2003">2004. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,559.10,431.60,9.94;8,90.00,571.82,251.58,9.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,411.90,559.10,109.70,9.94;8,90.00,571.82,186.39,9.94">Is Question Answering a Required Skill? Proceedings of WWW</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Paranjpe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,597.02,431.08,9.94;8,90.00,609.74,173.03,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,302.25,597.02,218.82,9.94;8,90.00,609.74,41.91,9.94">TREC2003 QA at BBN: Answering Definitional Questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Licuanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.32,609.74,114.96,9.94">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2003">2004. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
