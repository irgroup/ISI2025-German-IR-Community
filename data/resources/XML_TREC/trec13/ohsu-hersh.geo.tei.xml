<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.48,69.17,395.94,14.87">and conceptual drift for biomedical document triage</title>
				<funder ref="#_YPTjqQQ">
					<orgName type="full">National Library of Medicine</orgName>
				</funder>
				<funder ref="#_HmvzWZ6">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.96,95.63,62.81,11.66"><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics and Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.43,95.63,85.37,11.66"><forename type="first">R</forename><forename type="middle">T</forename><surname>Bhupatiraju</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics and Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.77,95.63,59.99,11.66"><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics and Clinical Epidemiology</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.48,69.17,395.94,14.87">and conceptual drift for biomedical document triage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F29646CFAFEB4DA32352F0A55CF5BEE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature generation</term>
					<term>feature selection</term>
					<term>classifiers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We approached the problem of classifying papers for the TREC 2004 Genomics Track triage task as a four step process: feature generation, feature selection, classifier training, and finally, classification. Section specific binary features that discriminated significantly between positive and negative training samples were chosen using the Chisquare statistic. Three classifiers were trained on this feature set: a simple Naive Bayes classifier, the SVMLight support vector machine implementation, and a voting perceptron extended to support variable learning rates. Comparing the classifiers on the training data we found that neither Naive Bayes nor SVMLight was able to adequately account for the factor of 20 in the utility function. The voting perceptron classifier performed much better at this. The performance on the test collection was lower for all classifiers, although consistent with the relative values of the training cross-validation. Feature subsetting showed no significant differences in precision or recall, implying that there was some redundancy among the features. We also examined how well the feature set derived from the 2002 training collection represented the papers in the 2003 test collection, and found a low level of similarity between feature sets derived from the two collections. This supports the hypothesis that important classification terms change quickly over time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The 2004 Text Retrieval Conference (TREC) Genomics track was divided into two main tasks: categorization, and ad-hoc retrieval. The categorization task was composed of a document triage subtask and an annotation subtask to detect the presence of evidence in the document for each of the three main Gene Ontology (GO) code hierarchies. Our work focused on the document triage subtask. We also participated in the ad-hoc retrieval task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Document classification is a common problem in biomedicine. Training a support vector machine (SVM) on vectors created from stemmed and/or stopped document word counts has proven to be a basic and typically successful approach <ref type="bibr" coords="1,194.64,663.52,89.18,9.07" target="#b6">(Yeh et al., 2003)</ref>. However we believed that the triage problem posed here had several distinctive features that would require modification to the basic approach.</p><p>First, the number of true positives in both the training and test collection was known to be small, between 6-7%. Second, the utility function chosen as the metric of record was heavily weighted to reward recall and not precision. This was based on an analysis of the current working procedures of the annotators at the Mouse Genome Institute (MGI), and an approximation of how they currently value false negative and false positive classification. The official utility function weights a false negative as twenty times more serious than a false positive. By this measure the current work practice of MGI, which is read all documents in test collection, has a utility of 0.25.</p><p>Additionally, the training and test collections were not randomly drawn from the same sample but instead were collected from documents published in two sequential years. While this is a more realistic simulation of a system as it would be put into use at MGI, it raises the issue of how well the feature set derived from one year of literature represents the literature of subsequent years.</p><p>Because of these issues our approach included a rich set of feature types, statistically based feature selection, several classifiers, and an analysis of how well the feature set derived from the year 2002 corpus represented the documents in the 2003 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM AND METHODS</head><p>We approached the triage problem in four stages: Feature generation, feature selection, classifier selection and training, and finally, test document classification. The first three steps were performed using only the training set. The final step was performed on the test collection to generate the submitted results.</p><p>During system development we used ten-fold crossvalidation on the training set to compare approaches and set system parameters. This entailed performing the first two steps on the entire training collection. Then 90% of the training data was used to train the classifiers which were then applied to the remaining 10% of the training data. This was repeated nine more times, so that all of the training data was classified once. The results were then aggregated to compute cross validation metrics on the training corpus. Figure <ref type="figure" coords="1,443.04,652.00,5.04,9.07">1</ref> presents this process diagrammatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1.</head><p>Step-wise approach to text classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature generation</head><p>The full text corpus with SGML mark-up provided opportunity to investigate the use of many types of features. While many text classification approaches treat the text as a "bag-of-words", we chose to use the information contained in the SGML mark-up to generate section type specific features. Because we combined features that could occur multiple times in a single document with features that could occur only once, after some initial testing we choose to treat each feature as binary, that is, each feature was either present in a document or it was absent.</p><p>One type of feature that we generated consisted of pairs of section names and stemmed words, using the Porter stemming algorithm. After applying a stop list of the 300 most common English words, individual parts of the document processed were processed to include sections for the abstract, body paragraphs, captions, and section titles. We also created similar combination section, stemmed word features using the stopped and stemmed section title in combination with the stopped and stemmed words within the named section.</p><p>In addition we downloaded the corresponding MEDLINE records from PubMed. The corresponding MeSH headings were extracted for each article. We included MeSH-based feature types based on the full MeSH headings, the MeSH main headings, and the MeSH subheadings.</p><p>Finally we included feature types based on information in the references section of each document. The main author of each reference was taken as a feature type. We also included long form of references as a feature type, by including the primary author, the journal name, volume, year, and page number.</p><p>Running the feature generation process on the full set of 5837 training documents produced over 100,000 potentially useful features along with counts of the number of documents containing each feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature selection</head><p>We chose to use the Chi-square selection method to select the features that best discriminated between positive and negative documents in the training corpus. The 2x2 Chisquare table is constructed as shown in Table <ref type="table" coords="2,510.72,298.00,3.78,9.07" target="#tab_0">1</ref>, using the document counts obtained in the previous step. During system tuning it was found that an alpha value of 0.025 produced the best results. Using this value as a cut-off, 1885 features were selected as the most significant. The number and type of each feature found significant and used in the following steps are shown in Table <ref type="table" coords="2,512.66,370.00,3.78,9.07" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classifier selection and training</head><p>We applied three different classifiers to the problem: Naive Bayes, SVM, and Voting Perceptron. While it is commonly thought that the best classifiers are based on the SVM approach of Vapnik <ref type="bibr" coords="2,469.92,439.12,64.22,9.07" target="#b5">(Vapnik, 2000)</ref>, the distinctive aspects of the current classification problem discussed above motivated us to apply three different classifiers. By using the same feature set with each of the classifiers, this allowed us to compare the effectiveness of the classifier algorithms for the particular requirements of the triage task. Neither Naive Bayes, nor the implementation of SVM we used, SVMLight <ref type="bibr" coords="3,139.20,273.52,67.38,9.07" target="#b3">(Joachims, 2004)</ref>, provided adequate means of adjusting for the low frequency of positives and the high utility of true positives relative to false positives. For Naive Bayes, we used our own implementation. Naive Bayes provides a classification probability threshold that can be used to trade off between precision and recall. However, this is an indirect means of compensation, and in practice for this classification task, we found adjusting the probability threshold did not have a significant effect.</p><p>We fully expected SVMLight to perform better than Naive Bayes, since it included a cost factor parameter that can be adjusted to allow unequal penalties for false positives and false negatives. However, we found that the amount of influence this parameter has is small, and was inadequate to compensate for the factor of 20 difference between the cost of false positives and negatives. Since neither Naive Bayes nor one of the most popular implementations of SVM addressed our requirements, something else was needed.</p><p>A review of the classification literature shows significant work in modifying the classic Perceptron algorithm of Rosenblatt <ref type="bibr" coords="3,161.06,537.52,77.49,9.07" target="#b4">(Rosenblatt, 1958)</ref> to achieve performance at or near that of SVM for many problems. One algorithm in particular, the Voting Perceptron algorithm <ref type="bibr" coords="3,97.92,573.52,119.91,9.07" target="#b2">(Freund and Schapire, 1999)</ref>, has very good performance, is quite fast, and is easy to implement. While the algorithm as published does not include a means of compensating for asymmetric false positive and negative penalties, we have created a modification to the algorithm that does.</p><p>A perceptron is essentially an equation for a linear combination of the values of the feature set. There is one term in the perceptron for each feature in the feature set, plus an optional bias term. A document is classified by taking the dot product of a document's feature vector with the perceptron, and adding in the bias term. If the result is greater than zero, then the document is classified as positive, if less than or equal to zero, then the document is classified as negative.</p><p>Rosenblatt's original algorithm trained the perceptron by applying it to each sample in the training data. If the sample was incorrectly classified, the perceptron was modified by adding or subtracting the a sample back into the perceptron, adding when the sample was a true positive, and subtracting when the sample was a true negative. Over a large number of training samples the perceptron converges on the solution that best approximates the separation between positive and negative documents in the training set.</p><p>Freund and Schapire improved the perceptron's performance by modifying the algorithm to produce a series of perceptrons, each which makes a prediction on the class of each document and gets a number of "votes" depending upon how many documents that perceptron classified correctly in the training set. The class with the most votes is the predicted class assigned to the document.</p><p>Our extension to this algorithm is based upon adjusting the learning rate of the perceptron differently for false negatives and false positives. While in the typical implementation, incorrectly classified samples are directly added or subtracted back into the perceptron, we first multiply the sample by a factor known as the learning rate. Furthermore, we use different learning rates for false positives and false negatives. Given the definition of the utility function, we expected that the optimal learning rate for false negatives to be about 20 times that of false positives. This is indeed what we found during training. We used a learning rate of 20.0 for false negatives, and 1.0 for false positives.</p><p>Each of the three classifiers was applied to the training corpus. Ten-fold cross-validation was used to optimize any free parameters. The Naive Bayes classifier had one free parameter, the probability classification threshold. This was left at the default value of 0.50. The SVM-Light classifier settings chosen used the linear kernel and a cost factor of 20.0. The Voting Perceptron classifier was used with a linear kernel, and the learning rates were given above. This created a trained classifier model for each of the three methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification of test documents</head><p>Finally, the models created by the Naive Bayes, SVM, and Voting Perceptron classifiers were applied to the test corpus. This is done in two steps. First the documents in the test corpus were analyzed for the presence or absence of the significant features found during the feature selection step. This created a feature vector for each test document. Then the documents were classified by applying each of the three trained classifiers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation of conceptual drift</head><p>One important issue in applying text classification systems to documents of interest to curators and annotators is how well the available training data represents the documents to be classified. When classifying a biomedical text, the available training documents must have been written before the text to be classified. However, by its very nature the field of science changes over time, as does the language used to describe it. How rapidly the written literature of science changes has a direct influence on the development of biomedical text classification systems in terms of how features are generated and chosen, how often the systems need to be retrained, how large the training increment should be, and may effect the maximum performance that can be expected out of these systems. We wanted to begin to understand this important issue of conceptual drift in the biomedical literature. In order to measure how well the features chosen from the training collection represented the information important in classifying the document in the test collection, we performed additional steps of feature generation and selection on the test collection. The exact same system and parameters were applied to the test collection as the training collection. Then we measured how well the training collection feature set represented the test collection feature set by computing similarity metrics between the two sets <ref type="bibr" coords="4,139.44,620.80,64.46,9.07" target="#b1">(Dunham, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification performance</head><p>The results of applying our classification systems to both the training and test collections are presented in Table <ref type="table" coords="4,550.61,130.23,3.78,9.07" target="#tab_3">3</ref>. As previously stated, ten-fold cross validation was used to evaluate performance on the training data. The results on the test data were created blindly, running the algorithms on the test corpus and sending the results in for evaluation by the TREC 2004 Genomics Track staff.</p><p>As can be seen in the The same relative performance was obtained for all three algorithms for precision, recall, F-score, and utility on the test collection, however the actual numbers are much lower. Voting Perceptron utility fell to 0.4983, and recall fell to 0.6571. SVMLight's precision and F-score fell to 0.2309 and 0.2790 respectively. Also the F-score of the Voting Perceptron algorithm on the test data was 0.2719, almost equal to that of SVMLight.</p><p>Figure <ref type="figure" coords="4,363.60,406.24,5.04,9.07">2</ref> presents the results of applying the Voting Perceptron classifier to the training data using ten-fold cross validation and leaving out feature types. Each vertical row shows the recall and precision obtained when leaving out one type of feature. For comparison, the leftmost column shows the performance of the full feature set. No significant differences in recall or precision were found by leaving out single feature types. This may indicate that there is redundancy in the feature set. In fact there is some textual overlap between the body paragraph stemmed words feature type and the section title with stemmed section words feature type, and also between the author of referenced work and reference feature types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Conceptual drift</head><p>Because the metrics on the test collection are much lower than the cross validation on the training set, it useful to understand how well the feature set extracted from the training collection represents the test collection. We performed the same process of feature generation and selection on the test collection. The process generated a set of 1899 significant features, a quantity very close to the 1885 features extracted from the training collection. We computed similarity measures between these two sets of features. The Dice similarity coefficient was 0.2489, the Jaccard similarity was 0.1422, cosine similarity was 0.2489, and the overlap measure was 0.2499. All similarity measures showed a low level of similarity between the two sets. This conceptual drift is not simply a reflection of a wholesale change in vocabulary. We performed equivalent similarity measures on the individual word frequencies in the training and test collection, filtered out common English words as before, and sorted the words most frequent to least frequent for both sets. Computing similarity measures between the top 100, 1000, and 10,000 words in both sets showed consistently high similarity measures, with the maximum being the Dice similarity coefficient of 0.9618 at 100 words, and the minimum being a Jaccard similarity of 0.9232 at 10,000 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>These results show that document classification can be a useful technique to aid the MGI curators in screening documents for annotation. The utility of the Voting Perceptron system on the test collection is about twice that of the estimate of the current work practice of MGI, and is ~14% better than the next best classifier. Clearly it is important for biomedical document classifiers to be able to flexibly incorporate utility measures specific to the task, as we have done here with the Voting Perceptron classifier.</p><p>Nevertheless, the performance measures on the test collection are significantly lower than those on the training set. One possible explanation for this can be found in the low similarity between the two sets of significant features extracted for the two text corpi. The maximum similarity metric, the overlap measure, only shows about 25% overlap between the two sets. Therefore, many of the features found significant during training were in fact not significant for triaging the test collection.</p><p>There may be many factors influencing why this is so. As the vocabulary similarity measures show, it is not  </p><formula xml:id="formula_0" coords="5,71.36,288.04,439.63,101.59">F U L L _ F E A T U R E _ S E T M I N U S _ A B S T R A C T M IN U S _ B O D Y _ P A R A G R A P H M IN U S _ C A P T IO N M I N U S _ M E S H _ F U L L _ H E A D I N G S M I N U S _ M E S H _ H E A D IN G S M IN U S _ M E S H _ S U B H E A D IN G S M I N U S _ R E F _ A U T H O R M IN U S _ R E F E R E N C E M IN U S _ S E C T I O N M I N U S _ S E C T IO N _ T I T L E Feature Set</formula><formula xml:id="formula_1" coords="5,74.72,288.04,436.27,101.59">U L L _ F E A T U R E _ S E T M I N U S _ A B S T R A C T M IN U S _ B O D Y _ P A R A G R A P H M IN U S _ C A P T IO N M I N U S _ M E S H _ F U L L _ H E A D I N G S M I N U S _ M E S H _ H E A D IN G S M IN U S _ M E S H _ S U B H E A D IN G S M I N U S _ R E F _ A U T H O R M IN U S _ R E F E R E N C E M IN U S _ S E C T I O N M I N U S _ S E C T IO N _ T I T L E Feature Set</formula><p>Precision Recall simply a wholesale change in the language used in journal articles. The cause is something more subtle, and more specific to the terms and concepts that are important in the classification of these documents for annotation triage. One possible explanation is that the important words and concepts that signify inclusion in the positive triage set changed between the years during which the documents in the test and training sets were written, 2002 and 2003 respectively. This may be due to authors using new concepts or different language. It may also signify that the criteria used by the annotators when triaging a document has changed.</p><p>Clearly this issue needs more study if we are to apply text classification in a manner that best addresses the needs of annotators. Document triage systems may need to be re-trained more frequently, or even continuously trained. It may also be important to develop methods of extracting sets of features that have greater longevity than the Chi-square method used here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>Automated document triage as presented here can be a useful aid to the MGI triage process. The current state of the art provides a notable increase in utility above the current work process. However, more work needs to be done to verify that the utility metric used here actually represents value as perceived by the MGI curators. Furthermore, the best means of deriving and updating the classification feature set over time is an open question and needs further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">AD-HOC INFORMATION RETRIEVAL TASK</head><p>OHSU also took part in the ad hoc retrieval task of the Genomics Track. For the task, we decided to see how known simple but effective indexing and retrieval strategies would fare with the test collection. As such, we used the Lucene system, which is part of the Jakarta open source distribution of Web tools. Lucene implements a variant of TF*IDF term weighting that includes additional parameters for query term boosting and document length normalization (Apache Software Foundation, 2004). We did not use boosting and manual inspection showed length normalization to be detrimental. Therefore our runs were based on TF*IDF term weighting for document ranking. We submitted two official runs, one that used just the information needs statement of the query (OHSUNeeds) and the other, which used all of the text, including the title, information need, and context (OHAUAll). Both of these runs performed above the median in mean average precision (MAP), with the OHSUNeeds run scoring slightly better. Determining why these simple approaches worked better than many others requires further analysis, but could be due to more elaborate methods having detrimental effects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,324.00,534.91,183.06,7.13"><head>Table 1 .</head><label>1</label><figDesc>2x2 arrangement for testing feature significance</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,70.56,76.66,485.91,606.73"><head>Feature is the one under test?</head><label></label><figDesc></figDesc><table coords="2,70.56,76.66,485.91,606.73"><row><cell>Training Corpus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature Generation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature Selection</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classifier Selection</cell><cell></cell><cell></cell><cell></cell></row><row><cell>&amp; Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Corpus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Document Classification</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training document is triage positive?</cell><cell>Yes</cell><cell>Yes Number of times feature seen in positive documents</cell><cell>No Number of times other features seen in positive documents</cell></row><row><cell></cell><cell></cell><cell>Number of times</cell><cell>Number of times</cell></row><row><cell></cell><cell></cell><cell>feature seen in</cell><cell>other features</cell></row><row><cell></cell><cell>No</cell><cell>negative</cell><cell>seen in</cell></row><row><cell></cell><cell></cell><cell>documents</cell><cell>negative</cell></row><row><cell></cell><cell></cell><cell></cell><cell>documents</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,54.00,72.67,137.64,7.13"><head>Table 2 .</head><label>2</label><figDesc>Number and type of features used</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,54.00,106.27,211.68,139.61"><head>Table 3 .</head><label>3</label><figDesc>Performance of classification system on test and training corpi</figDesc><table coords="3,54.00,106.27,211.68,139.61"><row><cell>Feature type</cell><cell>Number</cell></row><row><cell></cell><cell>significant</cell></row><row><cell>Abstract stemmed words</cell><cell>127</cell></row><row><cell>Body paragraph stemmed words</cell><cell>778</cell></row><row><cell>Caption stemmed words</cell><cell>291</cell></row><row><cell>MeSH full headings</cell><cell>15</cell></row><row><cell>MeSH main headings</cell><cell>52</cell></row><row><cell>MeSH subheadings</cell><cell>5</cell></row><row><cell>Author of referenced work</cell><cell>35</cell></row><row><cell>Reference</cell><cell>4</cell></row><row><cell>Section title stemmed words</cell><cell>69</cell></row><row><cell>Section title with stemmed section words</cell><cell>509</cell></row><row><cell>Total number of features significant features</cell><cell>1885</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,324.00,202.24,234.24,105.07"><head></head><label></label><figDesc>table, the Voting Perceptron algorithm had the best utility of 0.6600, Naive Bayes next, and SVMLight worst on both the training and test corpi. The highest recall obtained was 0.8453 by the Voting Perceptron algorithm on the training collection. The highest precision of 0.3140 was obtained by SVMLight on the training collection, and the highest overall F-score was obtained by SVMLight also on the training collection.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by Grant number <rs type="grantNumber">2 T15 LM07088-11</rs> from the <rs type="funder">National Library of Medicine</rs>, and Grant <rs type="grantNumber">ITR-0325160</rs> from the <rs type="funder">National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YPTjqQQ">
					<idno type="grant-number">2 T15 LM07088-11</idno>
				</org>
				<org type="funding" xml:id="_HmvzWZ6">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,324.00,199.35,215.40,9.07;6,333.12,211.35,73.08,9.07;6,333.12,223.36,224.06,9.07;6,333.12,235.35,106.00,9.07" xml:id="b0">
	<monogr>
		<ptr target="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/search/Similarity.html" />
		<title level="m" coord="6,473.04,199.35,66.36,9.07;6,333.12,211.35,68.49,9.07">Class Similarity, Lucene 1.4.2 API</title>
		<imprint>
			<publisher>Apache Software Foundation</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,324.00,247.35,213.59,9.07;6,333.12,259.36,201.72,9.07;6,333.12,271.36,100.68,9.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,419.03,247.35,118.56,9.07;6,333.12,259.36,62.85,9.07">Data mining introductory and advanced topics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Dunham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Prentice Hall/Pearson Education</publisher>
			<pubPlace>Upper Saddle River, N.J.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,324.00,283.36,209.77,9.07;6,333.12,295.36,188.76,9.07;6,333.12,307.36,130.20,9.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,478.57,283.36,55.20,9.07;6,333.12,295.36,184.35,9.07">Large Margin Classification Using the Perceptron Algorithm</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,333.12,307.36,72.35,9.07">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="277" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,324.00,319.36,233.17,9.07;6,333.12,331.36,116.32,9.07" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="http://svmlight.joachims.org/" />
		<title level="m" coord="6,406.80,319.36,145.70,9.07">SVM-Light Support Vector Machine</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,324.00,343.35,213.57,9.07;6,333.12,355.36,214.97,9.07;6,333.12,367.36,156.12,9.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,411.62,343.35,125.95,9.07;6,333.12,355.36,214.97,9.07;6,333.12,367.36,19.30,9.07">The perceptron: A probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,358.80,367.36,85.68,9.07">Psychological Review</title>
		<imprint>
			<biblScope unit="page" from="386" to="407" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,324.00,379.36,217.69,9.07;6,333.12,391.36,114.60,9.07" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,412.08,379.36,129.61,9.07;6,333.12,391.36,23.76,9.07">The nature of statistical learning theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,324.00,403.36,211.92,9.07;6,333.12,415.36,210.97,9.07;6,333.12,427.36,186.36,9.07;6,333.12,439.36,143.16,9.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,333.12,415.36,210.97,9.07;6,333.12,427.36,181.59,9.07">Evaluation of text data mining for database curation: lessons learned from the KDD Challenge Cup</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,333.12,439.36,57.90,9.07">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
