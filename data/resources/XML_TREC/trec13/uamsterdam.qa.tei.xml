<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.25,154.89,298.75,15.49">Using Wikipedia at the TREC QA Track</title>
				<funder ref="#_kM5etAN #_m3hCuMf #_nekvbMn #_3VUYw5Z #_by4mtPB #_yd9yhfC #_W3hTA2v">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_4d7X67t">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.00,187.37,109.87,10.76"><forename type="first">David</forename><forename type="middle">Ahn</forename><surname>Valentin</surname></persName>
						</author>
						<author>
							<persName coords="1,301.85,187.37,79.55,10.76"><forename type="first">Jijkoun</forename><surname>Gilad</surname></persName>
						</author>
						<author>
							<persName coords="1,384.39,187.37,37.86,10.76;1,170.20,201.32,30.56,10.76"><forename type="first">Mishne</forename><surname>Karin</surname></persName>
						</author>
						<author>
							<persName coords="1,203.74,201.25,137.34,10.82"><forename type="first">M</forename><surname>Üller Maarten De Rijke</surname></persName>
						</author>
						<author>
							<persName coords="1,353.04,201.32,88.01,10.76;1,441.05,199.02,1.49,7.86"><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Free University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.25,154.89,298.75,15.49">Using Wikipedia at the TREC QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F4732FA2274BC10B319BF9F0477D45E2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC 2004 Question Answering track. We provide a detailed account of the ideas underlying our approach to the QA task, especially to the so-called "other" questions. This year we made essential use of Wikipedia, the free online encyclopedia, both as a source of answers to factoid questions and as an importance model to help us identify material to be returned in response to "other" questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe our participation in the TREC 2004 Question Answering track; our participation in the Web and Terabyte tracks is described elsewhere <ref type="bibr" coords="1,265.43,480.36,10.58,8.97" target="#b7">[8]</ref>. This year, we had several aims for the Question Answering track. One was to extend our QA system to handle this year's more complex question presentation, and to see how our existing modules cope with this new setting. Another was to make good use of the English edition of Wikipedia (http://en.wikipedia.org), the open domain encyclopedia, both as an additional stream for answering factoid questions, and as an importance model to help us answer "other" questions. Let's explain the latter point before we continue.</p><p>When the QA track at TREC was introduced, it focused on so-called "factoid" questions (typically having a short named entity as an answer) such as How many people live in Tokyo? or When is the Tulip Festival in Michigan?. As the track evolved, it was argued that this type of questions does not accurately model the needs of real users of QA technology. In addition to named entities as answers, users often search for definitions of concepts, or for summaries of important information about them. As a result, in 2003 TREC introduced definition questions-questions for which the answer is not a single named entity, but a list of information nuggets <ref type="bibr" coords="1,404.22,384.23,15.27,8.97" target="#b13">[14]</ref>.</p><p>We were glad to see that at the TREC 2004 QA track this was taken a step further. The questions were now clustered in small groups, organized around the same topic. For example, the topic Concorde included questions such as How many seats are in the cabin of a Concorde? and What airlines have Concordes in their fleets?. Finally, for every topic, the track guidelines required participants to supply "additional important information found in the corpus about the target, that was not explicitly asked." This last requirement has been dubbed "other" questions. In our view, the task presented at the TREC 2004 QA track, and the introduction of the "other" questions makes a big step towards more realistic user scenarios. According to our own analysis of web query logs, users tend to ask much more "knowledge gathering" questions than factoid questions about specific facts.</p><p>This new type of "other" questions puts more emphasis on the user aspect in the QA process-an issue that has mostly been neglected in the QA community. The TREC criteria for what is a good answer to a given question has so far been rather vague, but QA systems dealt with this vagueness fairly effectively for factoid questions. With the "other" questions, where systems are required to re-turn only important information, there is an implicitly assumed user model that can discriminate between important and unimportant facts about a topic. For example, for the topic Clinton, his birthday might be considered important, while the day of the week when he left Mexico probably is not. In order to give reasonable responses to "other" questions, a QA system needs to model such preferences.</p><p>We present an approach for answering "other" questions using an explicit "importance" model. We describe a method for gathering important facts about an entity from a collection of documents and for ranking the facts with respect to their importance for the user. We show that our ranking improves over plain retrieval of facts from the corpus. The core idea of our method is to estimate the importance of facts found in the target collection by using external "reference" corpora, high-quality sources of information that model a user's ability to distinguish between important and unimportant facts. The proposed method is our first step towards user-oriented QA, and further refinements of the underlying techniques are needed. We identify additional areas where this method is or may be helpful, and discuss its strengths, weaknesses and directions for further research.</p><p>The rest of this paper is organized as follows. We give separate accounts of our approaches for answering factoid/list questions and for answering "other" questions. First, though, we address a complication in the presentation of questions in this year's QA task: the grouping of questions by target. We then describe our runs, present our results, and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Handling Targets</head><p>Each target is given explicitly as a phrase, and the questions for the target are presented in sequence. The possibility of anaphoric dependencies of the questions on the target or on preceding questions is thus introduced. We use an anaphoric resolution module to resolve pronouns occurring in the questions. Our module is simple: each pronoun is resolved to the highest ranked compatible antecedent in the antecedent list. The antecedent list consists of the target and all noun chunks occurring in preceding questions (with previously resolved pronouns replaced by their antecedents).</p><p>Our heuristic is to rank the target highest and to rank the other noun chunks according to their occurrence order. Compatibility is determined according to a simple type system: pronouns are marked human (he, she, etc.), non-human (it, this, that, etc.), or unknown (they, etc.), while other noun chunks are unmarked until they are resolved to a pronoun. These simple heuristics appear to work well with the question groups, where few entities are introduced and where there is a strong tendency to refer to the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Factoid Questions</head><p>Our approach for answering factoid questions is largely based on QUARTZ, our QA system used for experiments in the TREC 2003 QA track <ref type="bibr" coords="2,422.65,317.47,11.62,8.97" target="#b6">[7]</ref> and the CLEF 2004 Question Answering track <ref type="bibr" coords="2,399.55,329.42,10.58,8.97" target="#b5">[6]</ref>. We use an architecture where several streams run in parallel: each is based on a different approach to QA and is a self contained QA system in itself. A final step of merging the results of the streams is based on both redundancy of answers between streams and a process of learning the strengths and weaknesses of each of them <ref type="bibr" coords="2,364.55,401.16,10.58,8.97" target="#b2">[3]</ref>.</p><p>This year, apart from minor technical modifications of these streams, we employed two new components in QUARTZ: an additional stream exploiting an opendomain encyclopedia and a mechanism for type checking of the answer candidates generated by each stream; see Figure <ref type="figure" coords="2,339.03,473.17,3.74,8.97" target="#fig_0">1</ref>. We also implemented a number of simple filtering mechanisms that serve as sanity checks for the answer candidates and performed a number of experiments in our answer justification module. We give an overview of the new components here and refer the reader to <ref type="bibr" coords="2,495.33,520.99,10.79,8.97" target="#b2">[3,</ref><ref type="bibr" coords="2,509.42,520.99,7.47,8.97" target="#b3">4,</ref><ref type="bibr" coords="2,520.18,520.99,7.47,8.97" target="#b5">6,</ref><ref type="bibr" coords="2,530.95,520.99,8.30,8.97" target="#b6">7]</ref> for an account of the rest of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encyclopedia Stream</head><p>Many systems participating in the TREC QA track use not only the local (AQUAINT) corpus, but also additional knowledge sources such as the web and various gazetteers <ref type="bibr" coords="2,354.15,618.12,15.27,8.97" target="#b14">[15]</ref>. The use of external resources (such as the web) in QUARTZ has proved to be beneficial, and we have therefore decided to employ an additional source of external knowledge into the system: a corpus specifically designed to address the information needs expressed by  the open-domain questions appearing in TREC-an encyclopedia. We used the English edition of Wikipedia (http://en. wikipedia.org), a free-content encyclopedia: among the reasons to use it are its relatively wide coverage, its availability in a standard database format, and the fairly structured format of its entries.</p><p>We adopt a simplification of techniques reported already in <ref type="bibr" coords="3,109.91,486.62,10.58,8.97" target="#b8">[9]</ref>. Given a question and the question topic, we first extract the Wikipedia entry for the topic. The QUARTZ question classifier module identifies the named entity type that should be returned as an answer to the question; a named entity tagger then identifies potential answers in the encyclopedic entry. The list of answers is then ranked according to two factors: the prior answer confidence, which is an estimate of how likely it is that the named entity is an answer to any question, and a posterior answer confidence, which is an estimate of the likelihood of the named entity to be an answer to the question at hand. For estimating the prior confidence, we use layout information about the Wikipedia format, basically giving more confidence to named entities appearing earlier in the entry; for the posterior estimations, we calculate a sentence-level similarity score between the question and sentence containing the answer, based on the Jaccard measure. The final ranking of the answers is a combination of the prior and posterior estimations, with more weight given to the posterior one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Type Checking</head><p>In question analysis, search, and extraction of answer candidates, QUARTZ, like other QA systems, applies a recall oriented strategy. The underlying assumption is that recall can be maintained at an acceptable level in the early steps of the QA process because possible noise will be filtered out in the final filtering step.</p><p>Answer type checking-checking whether a given answer candidate belongs to the expected semantic type (or set of types)-is one filtering method that we explored further in this year's TREC evaluation. The factoid questions used in the TREC QA track are associated with a small number of semantic types-the expected types of the correct answers. On top of the coarsegrained expected answer types used to extract answer candidates (such as PERSON, LOCATION, DATE or ORGA-NIZATION), we found it useful to identify more precisely whether we are looking for, e.g., an ACTOR, a CAPITAL, a YEAR, or an NGO (Non-Governmental Organization).</p><p>We extended previous experiments in domain-specific type checking <ref type="bibr" coords="4,132.92,151.94,16.60,8.97" target="#b12">[13]</ref> to an open-domain type checker by combining two fitering approaches, ontology-based and redundancy-based. First, we extract a WordNet synset as the required expected answer type of a question. For each candidate answer, we then calculate the probability that it has an expected type, based on word co-occurrence of the answer and the expected type on the web. Finally, an answer is filtered unless it is more likely to be of the expected answer type than of one of its WordNet siblings. Due to space restrictions, a more detailed description and a formal evaluation of this new type checker will be published elsewhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional Filtering</head><p>In previous evaluations, we have encountered the problem of "junk"-ungrammatical answers resulting mainly from the combination of different streams and the heavy usage of n-gram techniques in QUARTZ <ref type="bibr" coords="4,240.81,367.03,10.58,8.97" target="#b4">[5]</ref>. This year we employ a simple web hit count filter to cope with this phenomenon. Each candidate answer is sent, as a phrase search, to Google, and phrases that have no results at all are considered to be incorrectly formed answers and removed from the candidate list.</p><p>Additionally, since our system also relies on external knowledge such as the web and Wikipedia, it often obtains answers even for questions with no answer in the collection (NIL questions). We attemp to detect such questions using another simple filter, a collection hit count filter: each question topic is searched for in the collection and, if no documents are retrieved, a NIL response is given for the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Justification</head><p>An additional substantial problem found in previous evaluations is that of unsupported answers, i.e., correct answers with an incorrect supporting document. We have invested some effort in improving our answer justification mechanism, with an improvement of more than 20% on training data; even so, unsupported answers still account for half of our total number of correct answers.</p><p>Previously, we used Okapi-based retrieval for answer projection, with the query formed from the question and the answer. The Okapi model's good performance on early precision allowed us to take the top retrieved document as the supporting document. For this year, we still base our projection mechanism on retrieval only, but have moved from Okapi to a vector space model with extensive usage of various query operators in the query. We issue the answer as a phrase term, identify phrases in the question and issue them as phrase terms as well, and use boolean operators for various terms in the query. These are techniques that are known to increase early precision, and, as mentioned, we have indeed noticed an improvement on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">List Questions</head><p>As in the previous TREC QA track, we have not implemented a specific mechanism to handle list questions, but rather used our factoid approach for these questions, as well. The top ranking answers according to this approach are given as the answer to the list question; the number of answers depends on a confidence drop in the scores assigned to the candidates; in the absence of such a drop, a fixed threshold is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answering "Other" Questions with an Importance Model</head><p>In this section we provide the details of our method for extracting, ranking, and re-ranking information nuggets from a corpus. In a nutshell, after identifying a suitable "reference" corpus for our domain (our user model), we first use IR and NLP methods to identify information nuggets-short excerpts of text-related to the topic, both from the given document collection and from the "reference" corpus. Then, we use sentence-similarity metrics to rank the nuggets from the collection: the facts similar to those found in the "reference corpus" are considered more important and ranked higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Target Corpus and Reference Corpus</head><p>In the TREC QA task, answers to questions (including "other" questions) must be found in a given text corpus. In recent years, this corpus has been a part of the AQUAINT corpus, containing more than 1 million newswire documents, and a total of 3.1GB of text. In our experiments this corpus is used as the target corpus, where important information nuggets have to be located. The corpus is unstructured: we do not know beforehand which articles or passages contain "important" information about a topic.</p><p>The "reference" corpus to be used should be a relatively small, high-quality collection of documents, which is catalogued in a way that facilitates selecting documents which contain important information for a given topic. Typical corpora that can be used for such reference purposes are encyclopedias (e.g., biography pages from http://biography.com) and various knowledge bases (e.g., the Internet Movie Database http://www. imdb.com). Since TREC QA is an open domain task, we used the English edition of Wikipedia (http://en. wikipedia.org), an open domain encyclopedia. The version we used contained 768,000 entries (including placeholders and disambiguation entries), for a total of 900 MB of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mining Facts from the Target Corpus</head><p>When answering an "other" question for a given topic, we use IR to locate documents containing information about the topic, and then split the sentences from the retrieved documents into more easily "digestable" shorter nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval</head><p>First, from the target collection we retrieve the top 20 documents containing the topic as a phrase, using a traditional vector space model for the retrieval. Our collection is composed of news articles with headlines. Since an occurrence of a topic in a headline can be very indicative of the document's importance for the topic, we indexed the headlines and the article bodies separately, and calculate the retrieval score as a combination of the different representations; this is a common technique for semi-structured IR <ref type="bibr" coords="5,148.92,594.96,15.27,8.97" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraction</head><p>Since a response to an "other" question is a list of short nuggets, we have to split the retrieved documents into separate facts. This raises several problems. First, we observed a notorious use of referential NPs: even in highly focused documents the topic is introduced initially, and then referred to with pronouns or definite NPs (e.g., "PRESIDENT CLINTON arrived today at the . . . HE will leave to Mexico on Monday"). We therefore resolve pronouns in the documents using the simple anaphora resolution module described in Section 2. Then, we extract all sentences which contain the topic (either originally or after the resolution); this is a natural way to restrict our attention to document sections which potentially include facts about the entity.</p><p>Still, the sentences are often too long to be presented as nuggets. Moreover, as the next step of our method involves comparison of nuggets, we need to keep them atomic, i.e., as short as possible. We observed that most facts in the extracted sentences could be described with simple predicates (e.g. "[President Clinton] will leave to Mexico"). We therefore parse the sentences with Minipar-a wide-coverage dependency parser <ref type="bibr" coords="5,498.30,348.09,20.47,8.97" target="#b9">[10]</ref>-and consider as a fact nugget every predicate (usually, a verb) with all its arguments and modifiers. Table <ref type="table" coords="5,484.04,372.01,4.01,8.97" target="#tab_1">1</ref>(a) shows an example for the topic Cassini space probe.</p><p>Finally, every extracted fact is given a prior importance estimation: the retrieval score of the document from which the fact was extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mining Facts from the Reference Corpus</head><p>In order to obtain a list of "good" facts for a given topic, we now repeat the fact extraction stage, with slight modifications, for the reference corpus. First, we extract a high-quality document (i.e., an encyclopedia entry) for the topic. We then apply the anaphora resolution and sentence splitting methods described in the previous section.</p><p>Next, we assign importance to each fact, based on layout cues in the document, such as proximity to the beginning of the entry. These heuristics are based on the fact that in encyclopedia entries, important information is typically given first, data in tables is usually significant, and so on. An example of facts extracted from an encyclopedia entry is given in  Re-ranked facts • Cassini will be carrying 12 separate packages of scientific instruments a probe <ref type="bibr" coords="6,486.90,544.92,11.62,8.97" target="#b2">[3]</ref> • Saturn's largest moon <ref type="bibr" coords="6,261.79,557.87,11.62,8.97" target="#b0">[1]</ref> • department to find out what was happening with Cassini <ref type="bibr" coords="6,399.12,570.82,11.62,8.97" target="#b2">[3]</ref> • the instruments on Cassini to provide pictures of Saturn Nearly seven meters' rings moons radar to pierce the orange [1] • . . . (c) Re-ranked facts from the target corpus (with the id of the most similar reference fact in brackets).</p><p>Table <ref type="table" coords="6,232.58,663.69,3.88,8.97" target="#tab_1">1</ref>: Fact extraction and re-ranking in action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Estimating Importance of Facts</head><p>At this stage, we have two lists of nuggets: facts from the target corpus, with prior importance estimation, and reliable facts from the reference corpus, each with its importance value. To refine the importance estimation for the target facts, we calculate sentence-level similarity between the target and reference nugget lists: we exhaustively compare each target fact to each fact from the reference corpus. We experimented with two types of sentence-level similarity measures: lexical and semantic.</p><p>We measure lexical similarity by determining the word overlap between the sentences, using metrics such as Jaccard <ref type="bibr" coords="7,92.22,287.33,11.62,8.97" target="#b1">[2]</ref> to normalize over the sentence lengths. Prior to the comparison we use standard stemming and stopword removal on both sentences to increase the morphological uniformity. As to semantic similarity between sentences, we use linguistically motivated techniques to find similarities also between sentences which do not match on the surface level. We use two types of metrics; the first is the total WordNet distance of words appearing in the sentences, based on methods described in <ref type="bibr" coords="7,228.54,382.97,10.58,8.97" target="#b0">[1]</ref>. Alternatively, we use similarity scores between pairs of words derived from proximities and co-occurrence in large corpora, described in <ref type="bibr" coords="7,114.72,418.84,15.27,8.97" target="#b10">[11]</ref>, and sum the total proximity measure for the words in the two segments.</p><p>In the experiments described below we used the lexical similarity with Jaccard metric. Later we found that cooccurrence-based measures seem to give better estimates of sentence similarity. A careful evaluation of different measures for this task is in our future plans.</p><p>Let {t i } denote the list of facts extracted from the target corpus and {r j } the reliable facts from the reference corpus. We denote the similarity between two facts as sim(t i , r j ), the prior importance estimation of a target fact as I pr (t i ) and the importance of the reliable fact as I(r j ). Then the updated, posterior importance estimation of a target fact is calculated as follows:</p><formula xml:id="formula_0" coords="7,104.82,608.84,163.01,14.97">I post (t i ) = I pr (t i ) • max j (I(r j ) • sim(t i , r j )) .</formula><p>We sort the target facts by decreasing posterior importance and present the top N as the key facts about the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Removing Redundant Facts</head><p>At the TREC 2004 QA track, each "other" question was asked after a sequence of factoid questions, all about a given topic. Therefore, an additional requirement was set on the response to the "other" question: the retrieved facts should not duplicate the information conveyed by (answers to) the factoid questions.</p><p>To avoid such duplication, we performed another filtering step: from the ranked list, we omit nuggets that are similar to other nuggets higher in the ranking, or to one of the factoid questions together with its answer (as found by our factoid-QA system). We use the same sentence-level similarity measure sim(•, •) as for the posterior importance estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Runs</head><p>We submitted 3 runs differing only in the final sanity checking for answer candidates. Our aim here was to compare different options for the answer filtering.</p><p>uams04raw No answer type checking or other filtering mechanisms employed.</p><p>uams04tc1 Answer type checking and web and collection hit-count filters described above used.</p><p>uams04tc2 Same as previous run, but Wikipedia not used for the 'other' questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Table <ref type="table" coords="7,334.78,518.22,4.98,8.97" target="#tab_2">2</ref> gives the combined results for the 3 QA tasks (accuracy for factoids, F score for list and "other" questions) and the final scores of our runs. The results are disappointing, especially in light of our recent good performance for Dutch Question Answering <ref type="bibr" coords="7,327.41,665.94,11.62,8.97" target="#b5">[6]</ref> (where the questions are easier than the TREC questions, and rather similar to the TREC8 or TREC9 QA track). An error analysis shows that most errors are due to the insufficiently fine-grained question classification and entity extraction (the current system uses only 37 question types and 5 NE types). Also, we note the high rate of unsupported answers: without the answer justification requirement, our performance would double. Furthermore, our type-checking module does not seem robust enough yet to improve the performance of the system.</p><p>Let's take a closer look at our performance on the "other" questions. We compare two runs, a baseline run (uams04tc2) and a re-ranked run (uams04raw). In the former we extracted nuggets from the target collection as described above, and used prior estimates of the facts to rank the nuggets; 20 or fewer nuggets were submitted. For the latter, we used posterior estimates of the nugget importance (I post ) instead; also 20 or less facts were submitted per topic. The results of the runs are given in Table <ref type="table" coords="8,293.17,331.97,3.74,8.97" target="#tab_3">3</ref>. For comparison, the best system at TREC 2004 achieved an F-measure of 0.46, while the median F-measure over all 63 submitted runs is 0.184. Note that the version of the F-measure used at TREC 2004 is biased towards recall. As is clear from Table 3, our re-ranking method substantially improves both recall and precision, but more so for precision.</p><p>A further per question breakdown of the change of performance in terms of F-measure is given in Figure <ref type="figure" coords="8,271.63,533.67,4.98,8.97" target="#fig_2">2</ref> (top), indicating that while the gain in F-measure averaged over all questions is positive, there are questions whose score is affected negatively by our re-ranking mechanism. The results in Table <ref type="table" coords="8,135.66,581.49,4.98,8.97" target="#tab_3">3</ref> indicate that our re-ranking mechanism affects precision and recall differently; this is reflected in Figure <ref type="figure" coords="8,101.67,605.40,4.98,8.97" target="#fig_2">2</ref> (middle) and (bottom), where we provide perquestion breakdowns for precision and recall. For 26 questions the F-measure of the original sentences (without re-ranking) is 0, preventing any improvement from our re-ranking method.</p><p>An analysis of the assessed runs revealed that often good nuggets were in the collection, but not in the top 20 documents we used for the extraction. Indeed, the threshold of 20 was set mainly for computational reasons, and further experiments with higher thresholds has shown clear improvements. Since the evaluation of new runs has to be done manually, we have no numerical support for this claim. Another major source of errors was the similarity measure (normalized word overlap) used in our submitted runs. Because of its sparsity, often the decision about a match was based on a single common word, as, e.g., for the third nugget in Table <ref type="table" coords="9,231.10,247.51,3.71,8.97" target="#tab_1">1</ref>(c). Again, experimenting with more suitable measures is hindered by the lack of automatic evaluation methodology: unlike the factoid questions at TREC, for "other" questions it is difficult to create patterns of correct answers. Developing such effective automatic evaluation methods is essential for improving the systems. Re-ranking errors were also caused by our sentence splitting and anaphora resolution methods. For example, for the topic "Carlos the Jackal" one of the important nuggets "the man known as Carlos the Jackal, once considered the world's most wanted terrorist, is serving a life sentence there" was discarded after re-ranking, although the reference corpus provided the nugget "on December 23 he was found guilty and sentenced to life imprisonment." Although both contain the key words sentence and imprisonment, the nugget from the target collection was too long for the similarity to be detected by our method. A better sentence splitter (capable of ignoring reduced relative clauses) could make nuggets shorter and the similarity more obvious. Another reason for discarding the snippet was the incorrectly resolved referential "there." Had it been resolved to its true antecedent "La Sante," the snippet could have matched the reference nugget "he was sent to La Santé de Paris prison to await trial."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper we have described our participation in the TREC 2004 Question Answering track. This year, our work for the Question Answering track was largely motivated by the wish to extend our QA system to handle the new, more complex question presentation, and to exploit Wikipedia at various stages of our QA architecture, for factoids and for "other" questions.</p><p>While the new setting proved tractable, a variety of bugs in the "core" of our QA engine lead to rather disappointing scores on the factoids.</p><p>As to the "other" questions, by comparing facts extracted from a target collection to the information from a reference resource (Wikipedia), we identified those facts that are potentially important for the user. Even with a simple word overlap-based similarity measure, this method shows reasonable performance: applying it to answer "other" questions in the TREC 2004 QA track, we show substantial improvements over the baseline. Our analysis of the TREC 2004 results on "other" questions suggests experimenting with more sophisticated sentencelevel similarity measures and improving sentence splitting for extraction of atomic facts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,230.52,357.10,150.22,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: QUARTZ System Overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,220.97,322.56,169.31,8.97;6,90.06,344.45,431.13,8.97;6,179.05,356.40,342.14,8.97;6,179.05,368.36,342.14,8.97;6,179.05,380.31,342.14,8.97;6,179.05,392.27,205.04,8.97;6,90.06,404.62,59.38,8.97;6,174.57,411.41,346.62,8.97;6,198.98,423.37,83.86,8.97;6,174.57,436.32,193.30,8.97;6,174.57,449.27,163.25,8.97;6,174.57,462.22,223.01,8.97;6,174.57,475.18,346.62,8.97;6,198.98,487.13,26.01,8.97;6,213.42,516.24,184.42,8.97"><head></head><label></label><figDesc>(a) Extracting facts from the target corpus. Encyclopedia entry Cassini-Huygens is a joint NASA/ESA unmanned space mission intended to study Saturn and its moons. The spacecraft consists of two main elements: the Cassini orbiter and the Huygens probe. The spacecraft was launched on October 15 , 1997 and entered Saturn's orbit on July 1 , 2004. October 15, is the first spacecraft to orbit Saturn and just the fourth spacecraft to visit Saturn. Extracted facts 1. Cassini -Huygens a joint NASA/ESA unmanned space mission intended to study Saturn and its moons 2. The spacecraft consists of two main elements 3. the Cassini orbiter the Huygens probe 4. The spacecraft entered Saturn's orbit on July 1, 2004 5. October 15, is the first spacecraft to orbit Saturn just the fourth spacecraft to visit Saturn (b) Extracting facts from the reference corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,310.61,623.68,228.64,8.97;8,310.61,635.64,228.64,8.97;8,310.61,647.59,99.33,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Per-question breakdown of effect of re-ranking on "other" questions: F-measure (top), precision (middle), and recall (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,354.21,665.94,43.47,8.97"><head>Table 1</head><label>1</label><figDesc>(b). Document text The Cassini space probe , due to be launched from Cape Canaveral in Florida of the United States at dawn , is carrying 33 kg of plutonium needed to power A rocket's seven-year journey to Venus and Saturn . Local mass media quoted opponents of Cassini as saying at the weekend that the mission will cross Panama , the Caribbean , Southern Africa and Madagascar be fore hurtling into space . Foreign affairs spokesman Pieter Swanepoel said neither had anything's department received any request or contacted the American authorities to find out what was happening with Cassini . . . . Extracted facts • The Cassini space probe : due to be launched from Cape Canaveral in Florida of the United States at dawn • Local mass media quoted opponents of Cassini as saying at the weekend the mission will cross Panama • Foreign affairs spokesman Pieter Swanepoel said neither had anything's • department to find out what was happening with Cassini • . . .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,316.98,562.62,215.89,63.33"><head>Table 2 :</head><label>2</label><figDesc>Results for the QA track</figDesc><table coords="7,316.98,573.65,215.89,52.30"><row><cell>Run</cell><cell>A</cell><cell>F</cell><cell>F</cell><cell>Overall</cell></row><row><cell>identifier</cell><cell cols="3">(Exact,Lenient) (List) (Oth)</cell><cell></cell></row><row><cell>uams04raw</cell><cell>0.135 , 0.287</cell><cell cols="2">0.094 0.210</cell><cell>0.143</cell></row><row><cell>uams04tc1</cell><cell>0.126 , 0.269</cell><cell cols="2">0.085 0.207</cell><cell>0.136</cell></row><row><cell>uams04tc2</cell><cell>0.126 , 0.269</cell><cell cols="2">0.087 0.184</cell><cell>0.131</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,78.38,390.54,215.89,52.37"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results for "other" questions.</figDesc><table coords="8,78.38,401.57,215.89,41.34"><row><cell>Measure</cell><cell>Baseline</cell><cell>Re-ranked</cell></row><row><cell>Precision</cell><cell>0.176</cell><cell>0.220 (+25%)</cell></row><row><cell>Recall</cell><cell>0.208</cell><cell>0.237 (+14%)</cell></row><row><cell>F-measure</cell><cell>0.184</cell><cell>0.210 (+14%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, and <rs type="grantNumber">612.069.006</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4d7X67t">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_kM5etAN">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_m3hCuMf">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_nekvbMn">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_3VUYw5Z">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_by4mtPB">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_yd9yhfC">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_W3hTA2v">
					<idno type="grant-number">612.069.006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,332.18,455.33,207.06,8.97;9,332.18,467.29,207.06,8.97;9,332.18,479.24,207.07,8.97;9,332.18,491.20,207.06,8.97;9,332.18,503.15,22.42,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,458.63,455.33,80.61,8.97;9,332.18,467.29,207.06,8.97;9,332.18,479.24,60.42,8.97">The use of Sentence Similarity as a Semantic Relevance Metric for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,415.14,479.24,124.11,8.97;9,332.18,491.20,202.55,8.97">Proceedings of the AAAI Symposium on New Directions in Question Answering</title>
		<meeting>the AAAI Symposium on New Directions in Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,332.18,525.54,207.06,8.97;9,332.18,537.49,160.25,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,377.38,525.54,161.87,8.97;9,332.18,537.49,17.04,8.97">The distribution of the flora of the alpine zone</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,357.07,537.49,63.85,8.97">New Phytologist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="1912">1912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,332.18,559.87,207.06,8.97;9,332.18,571.83,207.06,8.97;9,332.18,583.78,207.07,8.97;9,332.18,595.74,207.06,8.97;9,332.18,607.69,207.07,8.97;9,332.18,619.65,96.18,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,451.00,559.87,88.24,8.97;9,332.18,571.83,207.06,8.97;9,332.18,583.78,13.08,8.97">Answer selection in a multi-stream open domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,503.18,583.78,36.07,8.97;9,332.18,595.74,207.06,8.97;9,332.18,607.69,68.81,8.97">Proceedings 26th European Conference on Information Retrieval (ECIR&apos;04)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<meeting>26th European Conference on Information Retrieval (ECIR&apos;04)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2997</biblScope>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,332.18,642.03,207.06,8.97;9,332.18,653.99,207.06,8.97;9,332.18,665.94,207.07,8.97;10,93.58,127.96,207.06,8.97;10,93.58,139.92,92.98,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,491.66,642.03,47.58,8.97;9,332.18,653.99,207.06,8.97;9,332.18,665.94,122.47,8.97">Information extraction for question answering: Improving recall through syntactic patterns</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,478.34,665.94,60.91,8.97;10,93.58,127.96,207.06,8.97;10,93.58,139.92,63.79,8.97">Proceedings of the 20th International on Computational Linguistics (COLING 2004)</title>
		<meeting>the 20th International on Computational Linguistics (COLING 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,158.51,207.06,8.97;10,93.58,170.47,207.06,8.97;10,93.58,182.42,123.94,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,258.34,158.51,42.30,8.97;10,93.58,170.47,83.27,8.97">How frogs built the Berlin Wall</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,200.76,170.47,94.69,8.97">Proceedings CLEF2003</title>
		<meeting>CLEF2003</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,201.02,207.06,8.97;10,93.58,212.97,207.06,8.97;10,93.58,224.93,207.06,8.97;10,93.58,236.88,207.07,8.97;10,93.58,248.84,62.27,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,187.34,212.97,113.30,8.97;10,93.58,224.93,64.30,8.97">The university of amsterdam at qa@clef 2004</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,93.58,236.88,177.09,8.97">Working Notes for the CLEF 2004 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,267.44,207.06,8.97;10,93.58,279.39,207.06,8.97;10,93.58,291.35,207.06,8.97;10,93.58,303.30,207.06,8.97;10,93.58,315.26,22.42,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,207.73,279.39,92.91,8.97;10,93.58,291.35,207.06,8.97;10,93.58,303.30,21.15,8.97">The University of Amsterdam at the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,133.92,303.30,96.91,8.97">Proceedings TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="586" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,333.85,207.06,8.97;10,93.58,345.81,207.06,8.97;10,93.58,357.77,44.00,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,261.36,333.85,39.28,8.97;10,93.58,345.81,148.71,8.97">Language models for searching in web corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,264.59,345.81,17.17,8.97">This</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,376.36,207.06,8.97;10,93.58,388.32,207.06,8.97;10,93.58,400.27,207.07,8.97;10,93.58,412.23,207.06,8.97;10,93.58,424.18,207.06,8.97;10,93.58,436.14,139.45,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,146.67,376.36,153.97,8.97;10,93.58,388.32,207.06,8.97;10,93.58,400.27,11.00,8.97">Murax: a robust linguistic approach for question answering using an on-line encyclopedia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kupiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,122.18,400.27,178.47,8.97;10,93.58,412.23,207.06,8.97;10,93.58,424.18,113.95,8.97">Proceedings of the 16th annual international ACM SIGIR conference on Research and development information retrieval</title>
		<meeting>the 16th annual international ACM SIGIR conference on Research and development information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,454.73,207.06,8.97;10,93.58,466.69,207.07,8.97;10,93.58,478.65,207.06,8.97;10,93.58,490.60,116.78,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,125.90,454.73,174.74,8.97;10,93.58,466.69,87.42,8.97">PRINCIPAR -an efficient, broad-coverage, principle-based parser</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,203.24,466.69,97.41,8.97;10,93.58,478.65,207.06,8.97;10,93.58,490.60,86.96,8.97">Proceedings of the 15th International Conference on Computational Linguistics (COLING-94)</title>
		<meeting>the 15th International Conference on Computational Linguistics (COLING-94)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,509.20,207.06,8.97;10,93.58,521.15,207.06,8.97;10,93.58,533.11,99.35,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,175.86,509.20,124.78,8.97;10,93.58,521.15,76.74,8.97">Discovery of inference rules for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,182.72,521.15,117.92,8.97;10,93.58,533.11,11.42,8.97">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,551.70,207.06,8.97;10,93.58,563.66,207.07,8.97;10,93.58,575.61,207.06,8.97;10,93.58,587.57,207.06,8.97;10,93.58,599.53,111.99,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,194.99,551.70,105.65,8.97;10,93.58,563.66,138.87,8.97">Combining document representations for known-item search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,251.29,563.66,49.36,8.97;10,93.58,575.61,207.06,8.97;10,93.58,587.57,207.06,8.97;10,93.58,599.53,32.66,8.97">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.58,618.12,207.06,8.97;10,93.58,630.08,207.06,8.97;10,93.58,642.03,207.06,8.97;10,93.58,653.99,207.07,8.97;10,93.58,665.94,66.97,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,280.96,618.12,19.68,8.97;10,93.58,630.08,185.62,8.97">Type checking in open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Olsthoorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,93.58,642.03,207.06,8.97;10,93.58,653.99,137.31,8.97">Proceedings of the 16th European Conference on Artificial Intelligence (ECAI 2004)</title>
		<meeting>the 16th European Conference on Artificial Intelligence (ECAI 2004)</meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="398" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.18,127.96,207.06,8.97;10,332.18,139.92,207.07,8.97;10,332.18,151.87,207.06,8.97;10,332.18,163.83,22.42,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,392.73,127.96,146.52,8.97;10,332.18,139.92,82.38,8.97">Overview of the TREC 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,437.44,139.92,101.81,8.97;10,332.18,151.87,143.77,8.97">Proceedings Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting>Twelfth Text Retrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,332.18,183.75,207.06,8.97;10,332.18,195.71,207.06,8.97;10,332.18,207.66,103.11,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,391.97,183.75,147.27,8.97;10,332.18,195.71,61.96,8.97">Overview of the trec 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,411.78,195.71,127.47,8.97;10,332.18,207.66,73.53,8.97">The Twelfth Text REtrieval Conference (TREC-03)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
