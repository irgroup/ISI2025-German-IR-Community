<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,79.62,76.00,452.74,16.72;1,162.54,96.76,286.84,16.72">Initial Results with Structured Queries and Language Models on Half a Terabyte of Text</title>
				<funder ref="#_U5P78JH">
					<orgName type="full">Dept. of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.00,132.86,130.75,11.15"><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.60,132.86,62.67,11.15"><forename type="first">Paul</forename><surname>Ogilvie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,381.11,132.86,67.99,11.15"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,79.62,76.00,452.74,16.72;1,162.54,96.76,286.84,16.72">Initial Results with Structured Queries and Language Models on Half a Terabyte of Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">654A2EC59789E8A91CA91AF9E4DD4011</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The CMU Distributed IR group's experiments for the TREC 2004 Terabyte track are some of the first to use Indri, a new indexing and retrieval component developed by the University of Massachusetts for the Lemur Toolkit <ref type="bibr" coords="1,54.00,314.31,10.63,8.74" target="#b3">[2]</ref>. Indri combines an inference network with a languagemodeling approach and is designed to scale to terabytesized collections.</p><p>Our goals for this year's Terabyte track were modest: to complete a set of simple baseline runs successfully using the new Indri software, and to gain more experience with Indri's retrieval model, the track's GOV2 corpus, and terabyte-scale collections in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COLLECTION AND TASK SUMMARY</head><p>The Terabyte track this year used the GOV2 corpus, which is made up of about 25 million Web documents crawled from the .gov Web domain, comprising about 426 Gb of document source.</p><p>The task for the Terabyte track is ad-hoc retrieval. There are 50 queries or 'topics'. Each topic is presented in the usual TREC format, as a tagged document with three fields that summarize the information need at different levels of detail: a short 'title' consisting of just a few terms, a longer, more detailed 'description' field, and a multi-sentence 'narrative' field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING ENVIRONMENT</head><p>We indexed all of the GOV2 corpus documents, submitting complete documents to the indexer. We did no special processing for titles or other document structure. After running each document through Indri's HTML parser and stripping out tags, terms were normalized by removing punctuation, converting to lowercase, and performing Krovetz stemming <ref type="bibr" coords="1,131.79,663.45,10.65,8.74" target="#b7">[6]</ref>. Indri has a utility to extract anchor text, but a working version of this was not available at the time of our experiments, so no anchor text (or other linkderived features) was included. We did not use an indextime stop list or acronym list.</p><p>We partitioned the GOV2 corpus into six roughly equalsized pieces and built a separate index for each subset. There were two reasons for doing this. First, it was our intent to distribute retrieval for the corpus across multiple machines: Indri has a facility for transparently federating retrieval results across the network. Second, it helped reduce the time to find and fix problems with interim Indri releases (typically, pathological HTML cases that would cause the parser to crash).</p><p>For various reasons, we ended up using a single PC for both indexing and retrieval (though not at the same time). The PC had dual Xeon 3.2GHz CPUs with 2Gb RAM running Red Hat Linux release 9. The file system used an ACNC SATA Raid array, specifically Jetsor III Raid/150869 with 128mb dual SCSI host 16 slots, with 8x250GB RAID-5 drives in the box and 6x146GB RAID-5 disks internal to the PC. All disks were 7200 RPM.</p><p>We used a modified version of Indri build 20040830-1620 to build the index set. (The changes were to work around various parsing bugs which have since been fixed.) Total indexing time was 20.1 hours and the resulting index files were 138 Gb in size. The index referenced 25,205,168 documents. Indri also generated a compressed version of the tokenized collection, which for the GOV2 corpus was 107 Gb in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RETRIEVAL ENVIRONMENT 4.1 Topic Processing and Query Formation</head><p>We converted topics to Indri structured queries as follows.</p><p>For title-only runs, we parsed each title field into its constituent unigrams and removed stop words using a stop list of 447 words derived from the standard list used with INQUERY <ref type="bibr" coords="1,370.99,638.91,10.62,8.74" target="#b6">[5]</ref>.</p><p>The remaining terms were simply combined using the Indri #weight operator, with all terms given equal weight. (These prior weights are in addition to any term weighting done by the retrieval model.)</p><p>For runs using all three topic fields, we tried a very simple heuristic term matching scheme to identify unigrams, bigrams, and trigrams that recurred across fields -the idea being that concepts occurring in multiple fields would be more likely to be central to the information need. There were three phases: 1) extracting candidate terms (unigrams, bigrams, and trigrams) from each field, 2) matching up candidates across fields while computing a weighted score for each candidate, and 3) selecting the top N candidates with scores above an empirically derived threshold. For scoring, fields were associated with weights giving a rough prior estimate of their importance, as follows: title: 0.50, narrative: 0.35, and description: 0.15. The combined score for a candidate term was obtained by summing the weights for the fields in which it occurred. Scores for all candidates were then normalized so that the highest-scoring candidate(s) were given a weight of 1.0. We used the same term selection threshold of 0.200 for all our experiments, since this value seemed to give the best relative performance in most cases. While our algorithm here was simplistic, it may be easily generalized to a more powerful translation model in which approximate term matching is done with scores obtained via a translation cost function.</p><p>The final all-field structured queries were formed by combining the selected candidate terms and their scores with the #weight operator. Within the #weight operator, phrases were mapped to proximity operators. We tried different window sizes ranging from 3 words up to 20 words. We settled on an unordered proximity window of 8 words for both bigrams and trigrams since this appeared to give slightly better accuracy in the cases we tried. An example of the resulting query is shown in Figure <ref type="figure" coords="2,256.05,420.16,3.77,8.74">1</ref>. </p><formula xml:id="formula_0" coords="2,72.00,434.59,27.00,10.19">&lt;top&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Model Smoothing</head><p>We used Dirichlet smoothing <ref type="bibr" coords="2,446.87,88.89,11.71,8.74">[8]</ref> for all submitted runs, with µ = 2500, using the WT10g collection <ref type="bibr" coords="2,499.01,100.59,11.71,8.74" target="#b4">[3]</ref> to estimate optimal values for µ and other retrieval parameters. Indri allows different smoothing settings for term and window language models, but for our baseline we used the same setting for all language models. We also looked briefly at learning query-specific µ values (see Section 5).</p><p>More details on the Indri retrieval model are available in the UMass TREC 2004 paper <ref type="bibr" coords="2,439.16,186.51,10.63,8.74" target="#b1">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query Expansion</head><p>We used the default pseudo-relevance feedback algorithm in the Indri runquery utility (as of build 20040909-1720) to perform query expansion. This algorithm is a variant of a language-modeling approach to pseudo-feedback described by Lafferty and Zhai <ref type="bibr" coords="2,410.20,269.07,10.63,8.74" target="#b8">[7]</ref>. The final query is a weighted combination of the original and expanded queries, which in our experiments were weighted equally. We filtered the expansion terms by using a modified stop list that included Web-specific noise terms such as 'pdf', 'http', 'www', and so on. The best performance on WT10g given the other query processing parameters was obtained using the document count ('docs') and term count ('terms') parameters shown in Table <ref type="table" coords="2,428.75,362.67,3.77,8.74" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>We submitted two title-only runs, with and without pseudorelevance feedback (cmutufs2500 and cmutuns2500 respectively), and one run using all topic fields with feedback (cmuapfs2500). A summary of the runs and their performance is given in Table <ref type="table" coords="2,444.30,445.23,3.77,8.74" target="#tab_1">1</ref>. Our best performing run was cmuapfs2500, which ranked 3 rd overall out of 71 runs in terms of MAP, R-precision, and bpref <ref type="bibr" coords="2,483.15,468.63,11.71,8.74" target="#b5">[4]</ref> scores.</p><p>Results for the WT10g collection on topics 451-550 are also given, in Table <ref type="table" coords="2,412.78,498.03,3.76,8.74" target="#tab_2">2</ref>, showing the effects of adding various enhancements to the baseline method. The most significant improvements came from switching to Dirichlet smoothing with tuned µ parameter, and using terms from all fields instead of just the title. The best performing run used all fields, unigram terms, Dirichlet smoothing, and pseudo-feedback and obtained a MAP of 0.2524. Our heuristic phrase selection algorithm appeared to give slightly worse results than using unigrams. Investigating the reasons for this and finding improvements to our topic analysis, is the subject of future work.</p><p>We did some preliminary investigation into varying µ for each query as a function of simple features such as number of non-stopwords and mean log frequency of the query terms in general English, but the results were inconclusive. We also calculated an oracle run for the all-field topics by selecting the individual values for µ giving the best performance for each query. (µ was varied from 500 to 5000 in increments of 500.) This oracle run obtained a MAP of 0.2618. The comparable run with best fixed µ=2500 obtained a MAP of 0.2304.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The GOV2 collection was the largest processed by the CMU DIR group to date. We achieved our main goal for this track, which was to obtain a set of baseline results with Indri, a new indexing and retrieval component in the Lemur Toolkit <ref type="bibr" coords="3,115.97,375.51,10.63,8.74" target="#b3">[2]</ref>. These results made basic use of Indri's combined language modeling-based retrieval and inference network features. Indri provides a much richer set of query operators than we used for our experiments. For example, it has the ability to build language models for arbitrary fields, including fields for document structure. This should prove quite useful for many IR tasks and future work.</p><p>The only significant problem we encountered was the slow speed of long, expanded queries on a collection of this size, with such queries taking several minutes to complete. We believe this problem had two causes: first, because of system constraints, we ended up using an inefficient configuration, i.e. federated retrieval with multiple collection partitions on a single machine instead of multiple machines. Second, some significant speed improvements have since been made in Indri's query processing. We expect speed to be less of an issue in future experiments.</p><p>Other problems, such as handling the wide range of HTML in such a large corpus, were relatively minor and an expected part of adopting early builds of complex software.</p><p>Overall, Indri proved to be reliable and scalable for this task, and we believe it represents a promising new tool for future large-scale retrieval experiments. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,59.58,76.78,228.20,134.13"><head>Table 1 : Submitted runs for the GOV2 corpus using TREC topics 701-750 and top 10,000 documents. All runs used Dirichlet smoothing with µ = 2500.</head><label>1</label><figDesc></figDesc><table coords="3,59.58,76.78,227.39,85.97"><row><cell>Run</cell><cell>Fields Used</cell><cell>Terms</cell><cell>QE parameters</cell><cell>MAP</cell></row><row><cell>cmutuns2500</cell><cell>Title</cell><cell>Unigrams</cell><cell>No expansion</cell><cell>0.2071</cell></row><row><cell>cmutufs2500</cell><cell>Title</cell><cell>Unigrams</cell><cell>docs=5, terms=10</cell><cell>0.2481</cell></row><row><cell>cmuapfs2500</cell><cell>All</cell><cell>Phrases</cell><cell>docs=5, terms=20</cell><cell>0.2843</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,327.36,75.34,221.21,201.95"><head>Table 2 : Mean average precision results on WT10g, using TREC topics 451 -550 and top 1000 documents.</head><label>2</label><figDesc></figDesc><table coords="3,478.86,75.34,60.24,20.44"><row><cell>Title</cell><cell>All fields</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGEMENTS</head><p>This work was supported by <rs type="funder">Dept. of Education</rs> grant <rs type="grantNumber">R305G03123</rs>. Any opinions, findings, conclusions, or recommendations expressed in this material are the authors', and do not necessarily reflect those of the sponsors. The authors thank <rs type="person">Trevor Strohman</rs> and <rs type="person">Don Metzler</rs> for their extensive help with Indri.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_U5P78JH">
					<idno type="grant-number">R305G03123</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,322.38,383.15,92.18,10.46" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,396.10,219.47,7.85;3,335.88,406.66,209.76,7.85;3,335.88,417.22,196.03,7.85" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
		<idno>UMass at TREC2004: Notebook. TREC 2004</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,427.78,201.08,7.85" xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.88,427.78,77.48,7.85">Conference Notebook</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,442.36,215.93,7.85;3,335.88,452.91,222.05,7.85;3,335.88,463.47,212.53,7.85;3,335.88,474.03,176.45,7.85;3,335.88,483.70,151.20,7.07" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/˜lemur" />
		<title level="m" coord="3,482.21,463.47,66.20,7.85;3,335.88,474.03,173.22,7.85">The Lemur toolkit for language modeling and information retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,497.62,202.06,7.85;3,335.88,508.18,216.33,7.85;3,335.88,518.74,211.58,7.85;3,335.88,529.30,60.56,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,487.56,497.62,50.39,7.85;3,335.88,508.18,212.47,7.85">Engineering a multi-purpose test collection for Web retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,335.88,518.74,148.19,7.85">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="853" to="871" />
			<date type="published" when="2003-11">November 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,543.88,213.63,7.85;3,335.88,554.43,205.81,7.85;3,335.88,564.99,65.00,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,458.14,543.88,91.37,7.85;3,335.88,554.43,83.46,7.85">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,427.62,554.43,72.37,7.85">Proc. of SIGIR 2004</title>
		<meeting>of SIGIR 2004<address><addrLine>Sheffield, U.K</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,579.57,221.01,7.85;3,335.88,590.13,204.18,7.85;3,335.88,600.69,212.67,7.85;3,335.88,611.25,127.24,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="3,500.14,579.57,56.75,7.85;3,335.88,590.13,55.85,7.85">The INQUERY retrieval system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,400.20,590.13,139.86,7.85;3,335.88,600.69,208.99,7.85">Proceedings of the Third International Conference on Database and Expert Systems Applications</title>
		<meeting>the Third International Conference on Database and Expert Systems Applications<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,625.83,201.65,7.85;3,335.88,636.39,215.46,7.85" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="3,380.04,625.83,157.49,7.85;3,335.88,636.39,36.21,7.85">Word Sense Disambiguation for Large Text Databases</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Mass., Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Doctoral thesis</note>
</biblStruct>

<biblStruct coords="3,335.88,650.97,215.05,7.85;3,335.88,661.53,201.98,7.85;3,335.88,672.09,211.83,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,425.15,650.97,125.79,7.85;3,335.88,661.53,198.74,7.85">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.88,672.09,72.37,7.85">Proc. of SIGIR 2002</title>
		<meeting>of SIGIR 2002<address><addrLine>New Orleans, U.S.A.</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,335.88,686.67,211.04,7.85;3,335.88,697.23,205.36,7.85;3,335.88,707.79,211.86,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="3,422.89,686.67,124.04,7.85;3,335.88,697.23,202.12,7.85">A study of smoothing methods for language models applied to ad-hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,335.88,707.79,72.37,7.85">Proc. of SIGIR 2002</title>
		<meeting>of SIGIR 2002<address><addrLine>New Orleans, U.S.A</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
