<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.98,109.51,399.47,15.70;1,248.64,140.83,98.05,15.70">Meiji University Web, Novelty and Genomics Track Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.17,180.92,92.04,11.15"><forename type="first">Tomoe</forename><surname>Tomiyama</surname></persName>
							<email>tomiyama@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.87,180.92,71.21,11.15"><forename type="first">Kosuke</forename><surname>Karoji</surname></persName>
							<email>karoji@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.05,180.92,75.84,11.15"><forename type="first">Takeshi</forename><surname>Kondo</surname></persName>
							<email>t-kondo@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.29,180.92,72.14,11.15"><forename type="first">Yuichi</forename><surname>Kakuta</surname></persName>
							<email>kakuta@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.82,196.58,88.65,11.15"><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
							<email>takagi@cs.meiji.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.02,259.16,71.35,11.15"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.00,321.80,103.37,11.15"><forename type="first">Teruhito</forename><surname>Kanazawa</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">KYA group Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.98,109.51,399.47,15.70;1,248.64,140.83,98.05,15.70">Meiji University Web, Novelty and Genomics Track Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DA478754935AC98327A1454680C7C95</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in Novelty track, the topic distillation task of Web track and ad hoc task of Genomic Track. Our main challenge is to deal with meaning of words and improve retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conceptual Fuzzy Sets</head><p>To make computers understand a language, the most difficult problem is the ambiguity of the language. In Information Retrieval, a presence of multisense words causes retrieval performance decrement. According to the theory of meaning representation of use, proposed by Wittgenstein, the various meanings of a word can be represented by other words, and when a decentralized knowledge representation is formed, the meaning of the word is clear.</p><p>For instance, "typhoon" can be expressed as "violent tropical storm" or "tropical cyclone". "cyclone" can in turn be expressed by other words. In summary, the concept of the word "typhoon" can be expressed by other words ("violent", "tropical", and, "storm").</p><p>Conceptual Fuzzy Sets (CFSs) has been proposed to deal with the ambiguity problem i ii . In CFSs, to represent of meaning of words, prototype concepts are represented by the activity values of words, and CFSs use overlap of the activity values of the prototype concepts. The concept of an arbitrary input is represented by overlapping the distributions of the activity values of prototype concepts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Example of CFSs</head><p>Let us consider a concrete example of a CFS in Figure <ref type="figure" coords="2,337.89,119.53,3.94,9.16">2</ref>.1. The noun "java" has three senses: "coffee", "island", and "programming language". However, if "C" and "Java" appear together in the same document, the context is recognized as "computer". Therefore, "Java" probably means "programming". In the same way, if the context is "island", "Java" probably means "island". If the context is "coffee", "java" probably means "coffee". The words related to the context, such as "Mocha", "Kona", and "coffee" had high activity values. Thus, if the context can be specified by words, the ambiguity of the meanings of words can be eliminated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Web Track</head><p>We submitted five runs for the mixed query task and one manual run for the query classification task. We had two main challenges, which were the query expansion using CFS and the document modification by RS model. The RS model is described in 3.2, and CFS is described in 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Engine System Overview</head><p>R 2 D 2 is designed as a full-text retrieval system based on the vector space model iii . Formal definition of the vector space model is the following. The query Q consists of searching terms { } m q q q , , , 2 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>. The similarity between the query Q and the document j d is defined as:</p><formula xml:id="formula_0" coords="2,246.84,641.08,103.67,49.83">∑ = = m i j j i w S 1 ) , ( ) d , (Q , ) ( ) , ( ) , ( i f i j f j i w D T ⋅ ≡ .</formula><p>T f : factor based on the term frequency in a document. In R 2 D 2 , D T f f and are calculated using the RS model described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The RS model</head><p>We have proposed a method named the Relevance-based Superimposition (RS) model to solve the semantic ambiguity problem in information retrieval. A query usually provides only a very restricted means to represent the user's intention. Query expansion is a method for semantic disambiguation on query issuing phase. It includes index terms related to the original query expression, thus assisting novice users who have limited vocabulary in the target field. However, it is difficult to choose terms that represent the user's intention automatically and carefully. Therefore, pragmatically effective retrieval can only be achieved by adjusting many parameters depending on the database iv . Document feature vector modification is one of the methods that use information extracted from the documents for semantic disambiguation in index generation phase. We believe it achieves higher recall without losing precision of retrieval, because documents usually have much more information than a query.</p><p>The RS model is designed using the document feature vector modification approach This model partitions the documents so that the relevant documents dealing with the same topic fall into the same cluster. However, the idea is different from the traditional cluster-based methods v,vi in which the document clusters are usually mutually exclusive. These methods assume that documents can be classified into orthogonal topics; however, it is natural to assume that a document can belong to several topics. This difference in assumptions will reflect on the retrieval. The details of the RS model has already been reported in vii and viii . We have evaluated the effectiveness of this model using TREC San Jose Mercury consisting of news articles and NTCIR 1/2 test set consisting of scientific papers. The experimental results showed that the RS model improves the average precisions by 7%, which can be considered significant (5-10% is generally required for significant improvement ix ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Model overview</head><p>The RS model is designed using the document feature vector modification approach, as described in Figure <ref type="figure" coords="3,127.59,522.78,3.94,9.17">3</ref>.1. This model partitions the documents so that the relevant documents dealing with the same topic fall into the same cluster. However, the idea is different from the traditional cluster-based methods in which the document clusters are usually mutually exclusive. These methods assume that documents can be classified into orthogonal topics; however, it is natural to assume that a document can belong to several topics. This difference in assumptions will reflect on the retrieval.</p><p>Let us define the RS model formally. In the RS model, each document is represented by a feature vector. Term frequencies are often used as the features. Suppose that a document database contains a set of documents } , , , { Note that a document may be contained in more than one cluster in the RS model, whereas clusters in other methods are often mutually exclusive. At this point, we must decide what type of relevance we will use to make clusters. The principle of the RS model is independent of the source of relevance information, and our choice will depend on the type of database and the types of elements in it. For instance, the following elements included in the database can be candidate sources for relevance information and used for document clustering:</p><p>• keywords given by the authors or automatically extracted; • references, hyperlinks;</p><p>• bibliographic information, such as author name, publication date, and journal title. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Representative vector generation</head><p>When clusters representing topics are given, the document feature vector is modified in two steps: (1) representative vector (RV) generation for each cluster, and then, (2) feature vector modification by RVs. We can design a statistical method so that the RV can be considered to accurately represent overall characteristics of the documents that belong to the same cluster. Next, the modification method should properly perform the superimposition of features represented by RVs so that the topics of each cluster are reflected in the modified document feature vectors, thereby reducing the ambiguity of retrieval caused by expressional mismatches between the query and the documents. RV r of cluster C is constructed from the feature vectors of the documents in C . Currently, we have tested five types of representative-vector-generator (RVG) functions, derived from the α -family distributions x , where the i th component r i of RV r is defined as follows:</p><formula xml:id="formula_1" coords="4,251.34,654.33,99.77,38.99">1 1 1 2 2 1 1 α α - ∈ -         ≡ ∑ C i i d C r d .</formula><p>here d i denotes the i th component of the feature vector of document d and |C| denotes the number of documents contained in cluster C. The dimension of RV is equal to that of document feature vectors. We empirically evaluated an appropriate value forα 1 and identified α 1 =-3 as the most effective parameter xi . Whenα 1 =-3, the variation of the α-family distributions is called 'root-meansquare':</p><formula xml:id="formula_2" coords="5,261.72,131.43,66.68,28.84">∑ ∈ ≡ C i i d C r d 2 1</formula><p>.</p><formula xml:id="formula_3" coords="5,192.96,180.30,211.98,131.74">document set by keyword A document set by keyword B Keyword A A A, B B B Representative Vector Document Feature Vector RVG d1 d2 d3 d4 d5 C A C B r 1 r 2</formula><p>Figure 3.2: Representative vector generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Document feature vector modification</head><p>The second step is modification of the document feature vector using the RVs of the clusters to which the document belongs.</p><p>We assume that important index terms for a document d are any terms that occur frequently in any cluster to which d belongs, as well as other terms occurring frequently in d itself. This ( ) ( )</p><formula xml:id="formula_4" coords="5,95.64,527.97,412.29,67.03">2 2 1 2 2 1 1 α α - ∈ -         ≡ ∑ d d S i i r S s r Let ) , , , ( 2 1 I d d d L represent the feature vector of document d and let ) , , , ( 2 1 I S S S L</formula><p>represent the vector of the cluster set D (d). The modified document feature vector d ′ is then</p><formula xml:id="formula_5" coords="5,85.08,628.33,203.92,13.65">defined as )) , ( , ), , ( ), , ( ( 2 2 1 1 I I s s s s d f s d f s d f L</formula><p>, where s f is the superimposing function defined as:</p><formula xml:id="formula_6" coords="5,224.22,675.21,150.77,44.09">3 3 3 1 2 2 1 2 1 2 1 ) , ( α α α - - -                   + ≡ y x y x f s .</formula><p>We have evaluated some members of the α -family distributions for s and s f , and identified α 2 =-3 (root-mean-square) and α 3 = -∞(maximum) as the most effective parameters, respectively xi .</p><p>We usedα 1 =-3 for r ,α 2 =-3 for S and α 3 = -∞ for s f in the following evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Automatic Keyword Extraction</head><p>In the previous section, we described how to make document clusters using the well-chosen keywords given by the authors of the documents. However, we must also consider archives where no explicit keywords are given for clustering. There are two possible answers: one is automatic unsupervised keyword extraction and the other is to find another clue of relevance. We investigated the former approach in the evaluation. Details are described in xii .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query expansion using Conceptual Fuzzy Sets (CFS)</head><p>We used CFS to expand queries. When users want to search for something, their requests are not always clear. When users input query into search engines, they are imaging a concept related to the query terms. Traditional search engines do not retrieve based on concepts, but based on input query terms, so such systems may return results that are different from users' concepts. To solve this problem, our system represented users' queries as concepts. In CFS, the concepts are represented by some words and their degree of relationship. To construct CFS, we need a dictionary in which the concepts are represented by some words and their degree of relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dictionary</head><p>A dictionary contains knowledge that is necessary to achieve CFS. It is defined as a set of prototype vectors. A prototype vector is represented by words and their weights. We automatically built a dictionary from HTML documents in the .GOV. To build the dictionary, we classified the documents using RS Model as described in the section 3.2. A generated document cluster can be considered to have meanings. The centered vector of each cluster was used as the prototype vector. As a result, we got a dictionary consisting of 73,827 prototype vectors. We used about the top 150 words in each centered vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Query Expansion</head><p>We used the prototype vectors to expand input queries. Each prototype vector was activated depending on users' queries. Prototype vectors that were related to the query were activated strongly and prototype vectors that had none of the query terms were not activated. Overlapping activated prototype vectors generated new concepts, and the queries were represented by them. The expanded query vector is computed by summing weighted concept vectors:</p><formula xml:id="formula_7" coords="6,257.16,599.36,80.21,90.45">i C q C q i w V V V V S i i ⋅ ⋅ = ∑ ⋅ = i C i q i V S V '</formula><p>where S i is the activation value of the prototype vector C i , V q is the word vector that every query word's value is 1.0, V ci is the word vector that represents the prototype vector C i , and w i is the weight based on the number of query terms that is contained in the prototype vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiments</head><p>We used document structures and out-degree reranking for all runs. The description of each run is shown in Table <ref type="table" coords="7,152.49,135.19,3.94,9.16" target="#tab_0">3</ref>.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Indexing</head><p>We used modified document vectors by RS model in meijihil3, meijihil5. In the other runs, we used ordinary tf-idf method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Document scoring</head><p>First, our system classified queries into four types. If a query was the TD type, the query would be expanded by CFS. To score a document, we used content of the document, its structures, and link information. Summary of our searching procedure is shown in Figure <ref type="figure" coords="7,379.28,373.75,3.94,9.16">3</ref>.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Query classification</head><p>We classified queries into four types that are the topic distillation (TD) type, the named page (NP) type, the home page (HP) type and the unknown (UK) type in order to apply CFS for only TD type queries. One reason was that CFS was effective to only TD type queries on the experiments using TREC2003 data. Moreover, when users are searching for known items, the relevant documents are thought to include the query terms. Thus, we assumed that such queries would not need to be expanded. Consequently, we classified the queries into each type. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Results</head><p>Our query classification procedure is as follows. First, we classified query into NP type, HP type and other type. This was based on appearing in the home page finding queries and the named page finding queries from last year's queries. Then, we classified other type query into TD type and UK type. The method was based on template-based phrase list, and used verbs and abbreviations list. As a result of classifying this year's query, TD type queries were 103, HP type queries were 32, NP type queries were 8 and UK type queries were 82.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Using document structure</head><p>We used three types of information in order to calculate the document structure score. These were anchor text of incoming links, URL length of the document, and title text of the document. The document structure score were added to the document-query similarity:</p><formula xml:id="formula_8" coords="8,172.98,273.57,251.58,17.32">ititle iurl ianchor i i S S S D Query Similarity S γ β α + ⋅ + = ) ) , ( (</formula><p>, where S i is the initial document score of D i , S ianchor is the anchor text score of D i and S iurl is the URL length score of D i , S ititle is the title text score of D i ,α is the weight for S ianchor , β is the weight for S iurl , γ is the weight for S ititle. We empirically determined β α , and γ based on the data on TREC2003. The parameters for each document structure were decided according to each type as shown in Table <ref type="table" coords="8,151.58,403.57,3.94,9.16" target="#tab_0">3</ref>.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Outlink score</head><p>To give the higher score homepage of the site, the system added outlink score to initial document score. The outlink score is based on the assumption that relevant homepages have links to many relevant pages. We defined it as follows:</p><p>, where S j is the score of initial search result in document j and N i is the number of outlinks in document j. In our experiment on the TREC2003 Topic Distillation Task, this method was 36% more precise than baseline. However, on the TREC2003 NP/HP finding task, MAP did not improve much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑</head><formula xml:id="formula_9" coords="8,255.06,595.51,108.69,32.39">+ = i N j i i j i i N N S S S</formula><p>) log ( '</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Result and discussion</head><p>Our official results are shown in Table <ref type="table" coords="9,263.45,119.53,3.94,9.16" target="#tab_0">3</ref>.3. In the mixed query task, the best result was meijihil1. Our query classification method was failed. This method based on last year's data did not fit to this year's. 86 of 225 queries correctly classified. From comparison meijihil2 and meijihil3, we found that the RS Model improved the Suc@1 score, while it got the Suc@10 score worse. In traditional Information Retrieval, like TREC ad-hoc task and NTCIR, the RS Model made approximately opposite effects, which means improving recall precision in top ranking without decreasing relevant precision. We suppose the RS Model could retrieve some relevant documents that could not be retrieved by the general IR Model however it did not be adapted to this task efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Additional experiment</head><p>As shown in official results, the query expansion using CFS method did not performed well. In our experiments, we could expand only 78% of 75 TD queries, so we did not find the effect of CFS. Therefore, to expand all TD queries, we tested to expand all of 225 queries. The result was shown in the last line in Table <ref type="table" coords="9,173.47,486.31,3.94,9.16" target="#tab_0">3</ref>.4. We found that CFS had bad influences in HP and NP type queries, while it slightly improved MAP in TD queries. Our approaches were effective to the topic distillation task on TREC2003, however the approaches did not perform well on this year's data. We assumed that there were many difficult queries in this year for CFS. For example, the query of "information security", "the arts in education" and so on could not expand well. Our future project is to build a more robust dictionary that can deal with such query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NOVELTY TRACK</head><p>Our main challenge in Novelty Track is concept-based expansion of words and sentence using the CFSs. We thought that the concept-based expansion of words and sentence can represent meanings of the words and the sentence more exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept-based Expansion</head><p>We used an RBF network to mount CFSs on the computer. Our CFSs, based on the RBF network, had the structure shown in  There are the two required elements for the query expansion model using the CFSs.</p><p>(1) Prototype concepts, c i</p><p>(2) The degrees of relationships between each prototype concept and each word, a ij</p><p>We approached the construction of c i by clustering the documents in a corpus. The corpus is Reuters corpus xiii or Open Directory Project (ODP) xiv . Then, the degrees of relationship (weights) a ij were learned using the least square minimum method. Supervisor data was needed to learn. It was made by using topics and tagged documents in the Reuters corpus. Each document was tagged to topic it belonged to, and the supervisor vector of each topic was made by using the tags. We used transposed matrix of c as the weights of the CFSs using the ODP. An arbitrary input, x was expanded as follows:</p><p>1. Calculate the similarities S i , between input vector, x and each prototype concept, c i .</p><formula xml:id="formula_10" coords="10,275.40,657.04,42.88,33.52">i i c x c x ⋅ = i S</formula><p>2. After expanding an arbitrary input, x, using CFSs, the activity value, f ｊ (x) of word, w ｊ is calculated by</p><formula xml:id="formula_11" coords="10,168.72,259.35,246.34,137.94">Input Output Σ Σ Σ ・ ・ ・ ・ ・ ・ X Similarity (x, c1) Similarity (x, c 2 ) Similarity (x, cm) a mn a m1 a 1n a 12 ･ ･ ･ ･ ･ ･ . ) ( ∑ = i i ij j S a f x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relevant Sentences Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Relevant Detection</head><p>We treated relevant detection as binary classification problem. Each topic which is made with the topic title, description, and narrative, was expanded using CFSs.</p><p>We calculated Term-Frequency (TF) and Document-Frequency (DF) considering the sentence as a very short document. When classifying the sentences as relevant or non-relevant, a similarity between the topic and the sentences was calculated. The similarity is calculated by using vector space model (VSM). If the similarity exceeded a certain threshold, the sentence was classified as relevant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Threshold Learning</head><p>In this system, we must set an appropriate threshold to distinguish relevant sentences from non-relevant ones. In Task1, The threshold was learned using the TREC2002 Novelty Track corpus. In Task3, The threshold of each topic was learned using the relevant sentences in the first 5 documents which were provided. If the first 5 documents include some non-relevant documents, the supervisor data decreases. Therefore, the supervisor data of all topics was used in the topic without (or small number) the supervisor data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">New Sentences Retrieval</head><p>We used three criteria in order to detect new sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Sentence Weight Score</head><p>A new sentence which would be presented to a user should be itself important sentence in text. In order to detect new in news stream, to consider locally is more effective rather than to consider globally. Therefore we used sentence weight proposed by Zechner xv , we improved it. The score can be calculated using N-window-idf which is document (sentence) frequency in past N sentences. By this means, weights of frequent words decrease; sentence weights represent local importance.  </p><formula xml:id="formula_12" coords="12,90.90,128.36,415.96,96.79">( log ) ( t df window N N t idf window N - - = - - . ) ( ) ( ) ( 1 ) ( i i i t idf window N t tf s Length s ightScore SentenceWe - - × × = ∑ .</formula><p>where Length(s) is the length of a sentence. The score is canonicalized by the length. tf(t i ) is the frequency of a word, t i in the sentences. N is the window size. N-window-df(t) is the sentence frequency of a word, t i in past N sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Scarcity Score</head><p>An appearance of the word which has not appeared before may contribute to the novelty. So we used term frequency in past N sentences. The score is as follows:</p><formula xml:id="formula_13" coords="12,142.14,343.88,318.77,34.79">∑ - - × = i i t tf window N s Lenght s ore ScarcitySc ) ( 1 ) ( 1 ) ( .</formula><p>where, N-window-tf(t) is the term frequency of the word, t i in past N sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Redundancy Score</head><p>A new sentence probably differs from the sentences judged to be new in the past. Concept-based expansion using the CFS is available. Therefore the formula is as follows: where we used the cosine similarity as Similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">New Detection</head><p>We used the Support Vector Machine (SVM) for classification of new or non-new. We used the SvmLight program xvi to train the SVM models. In Task 1, 2, and, 3, the SVM model was trained using the TREC2003 Novelty corpus. In Task4 the SVM model was trained using the TREC2004 corpus. The time window N was experientially set to 200. The Novelty Detection system we constructed is shown in Figure <ref type="figure" coords="12,216.69,599.71,3.94,9.16">4</ref>.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Novelty Result</head><p>We submitted three to five runs to Task 1-4. Table <ref type="table" coords="13,310.53,313.33,4.37,9.16" target="#tab_4">4</ref>.1 and 4.2 show our results. Novelty in Table <ref type="table" coords="14,126.07,103.87,4.37,9.16" target="#tab_4">4</ref>.1 show whether the expansion using the CFSs applied or not. "yes" shows that the concept-based expansion was applied in Topic or Sentence. "no" shows that the concept-based expansion was not applied. The N-window, Sentence Weight, and Scarcity column show whether each algorithm was applied or not. The redundancy score used in all submission results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Novelty Conclusions</head><p>Results show that our concept-based expansion using the CFSs was not working well in both Relevant and Novelty. Table <ref type="table" coords="14,216.63,203.77,4.37,9.16" target="#tab_4">4</ref>.2 shows that N-window is useful in new detection. The difference between MeijiHIL2WRS and MeijiHIL2WR shows that slightly improve performance. To represent novel feature, we used three criteria, and to distinguish new sentences, we used SVM. The results showed that this approach was effective in Novelty Detection that is binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Genomics Track</head><p>This is the first year that our group participates in the Genomics track of the TREC. Here we report our system and a method on the ad hoc task. Our method is to use the model expressing the meaning of words to implement CFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The model expressing the meaning of words to implement CFS</head><p>Within one document, any words have only one meaning. we assumed. That is, we expected that the document expresses the meaning of words. Based on this assumption, we created the model.</p><p>Using this assumption, a meaning of words is expressed from relationship between vast quantities of words and documents. Specifically, first, we constructed bipartite graph to define relations between words and documents as shown in Figure <ref type="figure" coords="14,301.00,503.47,3.93,9.16">5</ref>.1. Second, we obtained N documents which are received highest relation value from input words. Finally, relation values of words were calculated from N documents. We regarded relation values of words as Semantic Vector of input words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents words</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Relation between documents and words which were used in the system</head><p>To get relation between documents and words, we used tf-idf value calculated from &lt;ArticleTitle&gt; and &lt;AbstractText&gt;. We normalized values which calculated based on tf-idf. And if values were over a certain threshold, we presupposed that relations between words and documents exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">System</head><p>To weigh the model expressing the meaning of word to implement CFS, we used simple system in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Create word vector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MedlineCitation</head><p>Word vector of MedlineCitation was created using tf-idf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head><p>We compared tf-idf and Semantic Vector, and selected the higher one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity calculation</head><p>We used cosine measure for calculating relevancy score of MedlineCitation to Topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Conclusion</head><p>Results are shown in Table <ref type="table" coords="16,229.36,338.40,4.32,10.05" target="#tab_6">5</ref>.1. To compare the results, we superimpose result which used only tf-idf. The system using the model failed to improve retrieval performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,228.72,432.06,137.87,9.57"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Example of CFSs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,93.12,747.42,5.08,6.13;2,88.86,742.50,271.83,9.17;2,284.22,340.20,20.20,9.57;2,226.02,256.73,29.52,11.18;2,251.58,395.92,53.32,7.85;2,301.74,261.34,26.53,7.85;2,187.62,356.03,45.60,11.18;2,238.86,340.78,6.50,7.85;2,244.50,292.60,20.98,7.85;2,343.74,306.76,27.52,7.85;2,343.74,348.29,28.93,11.18;2,165.30,387.76,3.99,6.96;2,165.30,371.98,3.99,6.96;2,165.30,356.21,3.99,6.96;2,165.30,340.36,3.99,6.96;2,165.30,324.59,3.99,6.96;2,165.30,308.81,3.99,6.96;2,165.30,292.73,3.99,6.96;2,165.30,276.95,3.99,6.96;2,165.30,261.17,3.99,6.96;2,110.04,387.76,23.04,6.96;2,106.50,371.98,30.14,6.96;2,99.84,356.20,43.47,6.96;2,118.92,340.36,5.32,6.96;2,112.02,324.58,19.14,6.96;2,114.90,308.80,13.31,6.96;2,111.66,292.73,19.85,6.96;2,112.92,276.95,17.33,6.96;2,110.46,261.17,22.19,6.96;2,484.14,387.16,14.01,6.96;2,484.14,371.26,14.01,6.96;2,484.14,355.30,13.71,6.96;2,484.14,339.46,14.01,6.96;2,484.14,323.50,14.01,6.96;2,484.14,307.66,14.01,6.96;2,484.14,291.76,14.01,6.96;2,484.14,275.80,14.01,6.96;2,484.14,261.16,14.01,6.96;2,407.28,387.16,23.05,6.96;2,407.28,371.26,31.94,6.96;2,407.28,355.30,43.95,6.96;2,407.28,339.46,5.32,6.96;2,407.28,323.50,19.56,6.96;2,407.28,307.66,13.31,6.96;2,407.28,291.76,21.65,6.96;2,407.28,275.80,17.33,6.96;2,407.28,261.16,22.19,6.96;2,116.94,236.36,27.19,9.81;2,436.44,236.90,35.16,9.81"><head>Df</head><label></label><figDesc>: factor based on the document frequency containing the term.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,167.76,438.60,259.84,9.57"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Figure 3.1: The process flow of the RS model approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,85.08,444.54,56.04,9.17;5,159.01,444.54,7.02,9.17;5,183.98,444.54,45.46,9.17;5,247.41,444.54,8.14,9.17;5,273.45,444.54,9.93,9.17;5,301.33,444.54,58.57,9.17;5,377.79,444.54,14.60,9.17;5,410.34,444.54,18.03,9.17;5,446.28,444.54,33.30,9.17;5,497.47,444.54,12.87,9.17;5,85.08,458.29,425.32,9.82;5,85.08,473.95,425.23,9.82;5,86.58,498.60,62.92,9.17;5,161.22,503.53,1.95,6.12;5,156.72,498.60,216.07,9.17"><head></head><label></label><figDesc>-vector-modifier (DVM) function. Let D(d) be the set of clusters to which the document d belongs. Let S(d) denote the set of RVs that belong to the clusters in D(d) . Then the i th component i s of the vector of D (d) can be defined as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,210.48,613.68,174.38,9.57"><head>Figure 3 . 3 :</head><label>33</label><figDesc>Figure 3.3: Our searching procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,238.18,219.43,45.80,9.16"><head></head><label></label><figDesc>Figure 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,163.44,428.94,268.42,9.57"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Structure of CFSs based on the RBF network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="11,169.98,444.60,255.38,9.57"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Architecture of Relevant Detection System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="11,209.04,336.82,35.17,7.85;11,207.12,352.48,39.07,7.85;11,189.54,368.14,74.17,7.85;11,195.00,383.68,63.23,7.85;11,356.34,353.14,36.01,7.85;11,354.60,368.74,39.54,7.85;11,446.22,334.65,35.62,8.74;11,438.60,398.41,58.06,9.16;11,106.92,331.45,32.64,9.16;11,103.26,388.54,36.03,7.85;11,298.56,344.26,19.45,7.85;12,85.08,103.49,352.84,10.46"><head></head><label></label><figDesc>Sentence Weight Score and N-window-idf(t) as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,420.42,148.58,4.74,12.41"><head>)</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="13,172.74,266.40,249.90,9.57"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Architecture of Novelty Detection System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="14,267.24,693.37,67.10,6.28"><head>Figure</head><label></label><figDesc>Figure 1 : Bipartite Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="15,85.08,516.99,43.67,8.74"><head>Figure</head><label></label><figDesc>Figure 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,130.44,156.90,335.29,96.23"><head>Table 3 .1: The description of our runs.</head><label>3</label><figDesc></figDesc><table coords="7,130.44,172.93,335.29,80.20"><row><cell>Run ID</cell><cell cols="2">Query Classification Document modification</cell><cell>Query Expansion</cell></row><row><cell>meijihil1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>meijihil2</cell><cell>O</cell><cell></cell><cell></cell></row><row><cell>meijihil3</cell><cell>O</cell><cell>O</cell><cell></cell></row><row><cell>meijihil4</cell><cell>O</cell><cell></cell><cell>O</cell></row><row><cell>meijihil5</cell><cell>O</cell><cell>O</cell><cell>O</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,173.70,425.28,247.58,82.01"><head>Table 3 .2: The parameter for score of each structure.</head><label>3</label><figDesc></figDesc><table coords="8,173.70,441.57,237.89,65.72"><row><cell></cell><cell>Anchor text</cell><cell>url length</cell><cell>Title</cell></row><row><cell>TD</cell><cell>0.6</cell><cell>0</cell><cell>0</cell></row><row><cell>HP</cell><cell>0.5</cell><cell>0.8</cell><cell>0.5</cell></row><row><cell>NP</cell><cell>1.0</cell><cell>0</cell><cell>0.4</cell></row><row><cell>UK</cell><cell>0.5</cell><cell>0.2</cell><cell>0.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,93.90,172.50,405.21,124.61"><head>Table 3 .3: Official results.</head><label>3</label><figDesc></figDesc><table coords="9,93.90,193.93,405.21,103.18"><row><cell>Run ID</cell><cell>All topics (Normalized result)</cell><cell>Success @ 1</cell><cell cols="4">Success @ 10 TD/MAP NP/MRR HP/MRR</cell></row><row><cell>meijihil1</cell><cell>0.69</cell><cell>0.3644</cell><cell>0.7378</cell><cell>0.1099</cell><cell>0.6111</cell><cell>0.4728</cell></row><row><cell>meijihil2</cell><cell>0.68</cell><cell>0.3556</cell><cell>0.7156</cell><cell>0.1101</cell><cell>0.5841</cell><cell>0.4585</cell></row><row><cell>meijihil3</cell><cell>0.69</cell><cell>0.3689</cell><cell>0.7111</cell><cell>0.1148</cell><cell>0.5913</cell><cell>0.4568</cell></row><row><cell>meijihil4</cell><cell>0.56</cell><cell>0.3156</cell><cell>0.5600</cell><cell>0.0949</cell><cell>0.4339</cell><cell>0.4165</cell></row><row><cell>meijihil5</cell><cell>0.54</cell><cell>0.2933</cell><cell>0.5867</cell><cell>0.0712</cell><cell>0.4726</cell><cell>0.4258</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,145.44,508.02,293.05,57.81"><head>Table 3 .4: Additional experiment.</head><label>3</label><figDesc></figDesc><table coords="9,145.44,523.98,293.05,41.85"><row><cell>runID</cell><cell>TD/MAP</cell><cell>HP/MRR</cell><cell>NP/MRR</cell></row><row><cell>meijihil1</cell><cell>0.1099</cell><cell>0.6111</cell><cell>0.4728</cell></row><row><cell>cfs(additional)</cell><cell>0.1115</cell><cell>0.3530</cell><cell>0.2859</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,99.96,350.70,395.43,136.80"><head>Table 4 .1: Official Results of Novelty Task1, 3.</head><label>4</label><figDesc></figDesc><table coords="13,99.96,382.71,395.43,104.79"><row><cell></cell><cell>Run ID</cell><cell>CFSs type</cell><cell>Topic expansion</cell><cell>Sentence expansion</cell><cell>P</cell><cell>Relevant R</cell><cell>F</cell><cell>P</cell><cell>New R</cell><cell>F</cell></row><row><cell></cell><cell>MeijiHIL10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.29</cell><cell cols="5">0.76 0.384 0.14 0.58 0.212</cell></row><row><cell>Task1</cell><cell>MeijiHILcfs</cell><cell>Reurters</cell><cell>yes</cell><cell>no</cell><cell>0.24</cell><cell cols="5">0.92 0.357 0.12 0.75 0.194</cell></row><row><cell></cell><cell>MeijiHILodp</cell><cell>ODP</cell><cell>yes</cell><cell>no</cell><cell>0.23</cell><cell cols="5">0.96 0.349 0.12 0.77 0.187</cell></row><row><cell></cell><cell>MeijiHIL3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.31</cell><cell cols="5">0.62 0.376 0.14 0.41 0.190</cell></row><row><cell>Task3</cell><cell>MeijiHIL3Tc</cell><cell>Reurters</cell><cell>yes</cell><cell>no</cell><cell>0.30</cell><cell cols="5">0.54 0.339 0.13 0.41 0.174</cell></row><row><cell></cell><cell cols="2">MeijiHIL3STc Reurters</cell><cell>yes</cell><cell>yes</cell><cell>0.27</cell><cell cols="5">0.62 0.339 0.12 0.46 0.166</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,85.08,116.62,425.28,628.33"><head>Table 4 .2: Official Results of Novelty Task2, 4.</head><label>4</label><figDesc></figDesc><table coords="13,100.86,541.95,394.71,140.13"><row><cell></cell><cell>Run ID</cell><cell cols="4">CFSs type N-window Sentence Weight Scarcity</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell></cell><cell cols="2">MeijiHIL2WCS Reurters</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>0.49</cell><cell cols="2">0.85 0.608</cell></row><row><cell></cell><cell>MeijiHIL2CS</cell><cell>Reurters</cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>0.42</cell><cell cols="2">1.00 0.580</cell></row><row><cell>Task2</cell><cell>MeijiHIL2WRS</cell><cell>-</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>0.48</cell><cell cols="2">0.93 0.619</cell></row><row><cell></cell><cell>MeijiHIL2RS</cell><cell>-</cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>0.46</cell><cell cols="2">0.97 0.609</cell></row><row><cell></cell><cell>MeijiHIL2WR</cell><cell>-</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>0.48</cell><cell cols="2">0.93 0.617</cell></row><row><cell></cell><cell>MeijiHIL4WRc</cell><cell>Reurters</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>0.49</cell><cell cols="2">0.64 0.525</cell></row><row><cell></cell><cell>MeijiHIL4RSc</cell><cell>Reurters</cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>0.40</cell><cell cols="2">0.97 0.544</cell></row><row><cell>Task4</cell><cell>MeijiHIL4WRS</cell><cell>-</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>0.54</cell><cell cols="2">0.51 0.492</cell></row><row><cell></cell><cell>MeijiHIL4RS</cell><cell>-</cell><cell>no</cell><cell>no</cell><cell>yes</cell><cell>0.44</cell><cell cols="2">0.89 0.566</cell></row><row><cell></cell><cell>MeijiHIL4WR</cell><cell>-</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>0.5</cell><cell cols="2">0.61 0.522</cell></row></table><note coords="13,85.08,704.47,425.28,9.16;13,85.08,720.13,425.26,9.16;13,85.08,735.79,425.23,9.16"><p><p><p><p><p>where the Run ID column in Table</p>4</p>.1 and 4.2 shows the actual Run ID in TREC. The CFSs type column in Table</p>4</p>.1 and 4.2 shows the corpus to construct the CFSs. "-" means that concept-based expansion using the CFSs was not available. The Topic expansion and Sentence expansion column</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="16,254.82,391.62,85.72,9.57"><head>Table 5 .1: Results.</head><label>5</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,99.00,527.59,411.30,9.16;16,99.00,539.65,394.95,9.16;16,85.08,549.06,3.94,6.13;16,99.78,551.77,410.62,9.16;16,99.78,563.83,410.56,9.16;16,99.78,575.89,111.41,9.16" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,342.16,527.59,168.15,9.16;16,99.00,539.65,94.92,9.16;16,341.55,551.77,168.85,9.16;16,99.78,563.83,207.92,9.16">Conceptual Fuzzy Sets as a Meaning Representation and their Inductive Construction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Imura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ushida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamaguchi ; Ii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Imura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ushida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,200.52,539.65,181.32,9.16;16,316.74,563.83,189.12,9.16">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="929" to="945" />
			<date type="published" when="1995">1996. 1995</date>
		</imprint>
	</monogr>
	<note>International Journal of Intelligent Systems</note>
</biblStruct>

<biblStruct coords="16,85.08,585.24,5.92,6.13;16,101.58,587.95,395.34,9.16" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<title level="m" coord="16,213.12,587.95,191.07,9.16">Introduction to Modern Information Retrieval</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,97.68,600.07,408.64,9.16;16,97.68,612.13,62.97,9.16" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,261.37,600.07,165.02,9.16">Improving Automatic Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,445.02,600.07,41.36,9.16">SIGIR &apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="206" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,85.08,621.48,3.52,6.13;16,98.34,624.19,402.87,9.16;16,98.34,636.25,376.77,9.16" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,145.09,624.19,356.13,9.16;16,98.34,636.25,51.44,9.16">The Retrieval Effectiveness of Five Clustering Algorithms as a Function of Indexing Exhaustivity</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Burgin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,156.66,636.25,195.74,9.16">In J. American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="562" to="572" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,99.00,648.31,381.57,9.16;16,99.00,660.43,115.78,9.16" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,214.01,648.31,266.55,9.16;16,99.00,660.43,26.29,9.16">Reexamining the cluster hypothesis: Scatter/Gather on retrieval results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,143.04,660.43,41.36,9.16">SIGIR &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,99.78,672.49,403.50,9.16;16,99.78,684.55,330.99,9.16" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,268.07,672.49,235.20,9.16;16,99.78,684.55,89.76,9.16">A Relevancebased Superimposition Model for Effective Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Takesu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,196.31,684.55,80.84,9.16">IEICE Transactions</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2152" to="2160" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
	<note>E83-D</note>
</biblStruct>

<biblStruct coords="16,99.84,696.61,371.16,9.16;16,99.00,708.73,375.36,9.16;16,99.00,720.79,366.63,9.16;16,99.00,732.85,96.53,9.16" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,321.85,696.61,149.15,9.16;16,99.00,708.73,274.12,9.16">The Effects of the Relevance-based Superimposition Model in Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Takasu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,391.14,708.73,83.22,9.16;16,99.00,720.79,304.69,9.16">Proc. 5th European Conference on Research and Advanced Technology for Digital Libraries</title>
		<meeting>5th European Conference on Research and Advanced Technology for Digital Libraries<address><addrLine>Darmstadt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">Sept. 2001</date>
			<biblScope unit="page" from="312" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,85.08,742.20,5.50,6.13;16,101.16,744.91,403.20,9.16;16,245.70,421.32,14.66,9.57;16,301.41,421.32,23.81,9.57;16,346.41,421.32,38.49,9.57;16,204.78,437.46,22.61,9.57;16,245.70,437.52,42.25,9.57;16,306.24,437.46,30.28,9.57;16,360.25,437.46,30.28,9.57;16,204.78,453.60,19.56,9.57;16,245.71,453.60,90.86,9.57" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="16,158.33,744.91,330.48,9.16">Variations in Relevance Judgements and the Measure of Retrieval Effectivenss</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<idno>run MAP R-PREC tf-idf unofficial 0.2401 0.2883 CFS MeijiHilG 0.0924</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="16,365.34,453.60,25.21,9.57;17,100.44,114.73,129.50,9.16" xml:id="b8">
	<monogr>
		<title level="m" coord="16,370.38,453.60,20.17,9.57;17,100.44,114.73,41.36,9.16">1409 SIGIR &apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,99.18,126.79,400.12,9.16;17,100.44,138.85,242.90,9.16" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,150.26,126.79,333.33,9.16">On a New Data Model suitable for Intellectual Accesses by Personal Preference</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="17,100.44,138.85,63.50,9.16">IPSJ SIG Notes</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="388" />
			<date type="published" when="1998-07">July 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,101.16,150.91,396.99,9.16;17,99.78,162.97,397.83,9.16;17,99.78,175.09,119.62,9.16" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,269.93,150.91,228.21,9.16;17,99.78,162.97,102.90,9.16">Effect of the Relevance-based Superimposition Model on Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Takasu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Adachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,220.80,162.97,129.79,9.16">IPSJ Database workshop 2000</title>
		<title level="s" coord="17,357.90,162.97,61.66,9.16">IPSJ SIG Notes</title>
		<meeting><address><addrLine>Iwate</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,85.08,184.44,7.48,6.13;17,103.14,187.15,406.09,9.16;17,99.78,199.21,289.57,9.16;17,85.08,208.56,9.46,6.13;17,99.84,211.27,182.07,9.16;17,100.86,223.39,182.20,9.16;17,100.86,235.45,226.91,9.16;17,85.08,248.94,82.85,9.83;17,85.08,260.46,7.06,6.13;17,102.72,263.17,384.78,9.16;17,99.78,275.23,385.59,9.16;17,99.78,287.29,179.14,9.16" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,274.81,187.15,234.42,9.16;17,99.78,199.21,173.33,9.16;17,154.71,263.17,332.79,9.16;17,99.78,275.23,80.11,9.16">Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Takasu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Adachi</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/data/reuters/reuters.htmlhttp://about.reuters.com/researchandstandards/corpus/xivhttp://dmoz.org/xvK.Zechner" />
	</analytic>
	<monogr>
		<title level="m" coord="17,85.08,208.56,9.46,6.13;17,99.84,211.27,182.07,9.16;17,198.42,275.23,286.95,9.16;17,99.78,287.29,91.43,9.16">Proceedings of the 16th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 16th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="1996">2004. May 2004. 1996</date>
			<biblScope unit="page" from="986" to="989" />
		</imprint>
	</monogr>
	<note>xiii Reuters Corpus @ NIST or Reuters Corpus</note>
</biblStruct>

<biblStruct coords="17,99.42,299.41,387.53,9.16;17,99.00,311.47,406.74,9.16;17,99.00,323.53,23.67,9.16" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,154.62,299.41,187.09,9.16">Making large-Scale SVM Learning Practical</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,359.88,299.41,127.07,9.16;17,99.00,311.47,101.73,9.16">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="184" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
