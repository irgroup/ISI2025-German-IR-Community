<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.40,181.86,340.44,18.18;1,130.87,206.77,349.51,18.18;1,258.13,231.67,94.99,18.18">Novelty, Question Answering and Genomics: The University of Iowa Response</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,125.94,271.54,102.62,12.58"><forename type="first">David</forename><surname>Eichmann</surname></persName>
						</author>
						<author>
							<persName coords="1,247.21,271.54,57.37,12.58"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.92,271.54,118.11,12.58"><forename type="first">Shannon</forename><surname>Bradshaw</surname></persName>
						</author>
						<author>
							<persName coords="1,453.70,271.54,83.49,12.58"><roleName>Xin</roleName><forename type="first">Ying</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,117.33,289.47,48.57,12.58"><forename type="first">Li</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.25,289.47,118.90,12.58;1,299.15,287.26,13.86,8.74"><forename type="first">Padmini</forename><surname>Srinivasan Abc</surname></persName>
						</author>
						<author>
							<persName coords="1,322.08,289.47,133.45,12.58"><forename type="first">Aditya</forename><surname>Kumar Sehgal</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa Iowa City</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Novelty</settlement>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,468.91,289.47,80.77,12.58"><forename type="first">Hudon</forename><surname>Wong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa Iowa City</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Novelty</settlement>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.40,181.86,340.44,18.18;1,130.87,206.77,349.51,18.18;1,258.13,231.67,94.99,18.18">Novelty, Question Answering and Genomics: The University of Iowa Response</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9AF0B5C3FAA871AD6BB85FD185B0477</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our system for novelty this year comprises three distinct variations. The first is a refinement of that used for last year involving named entity occurrences and functions as a comparative baseline. The second variation extends the baseline system in an exploration of the connection between word sense and novelty. The third variation involves more statistical similarity schemes in the positive sense for relevance and the negative sense for novelty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variations 1 and 2</head><p>Our general approach involves establishing a similarity threshold for sentence relevance and an new entity threshold for novelty. If this exceeded a threshold, a sentence is declared relevant. Additionally, if the number of novel elements present in the sentence is above a declared number, the sentence is declared novel. 'Element' here can be a noun phrase or a named entity.</p><p>Our first two variations for this year operate on a composite precondition of simple similarity matches between the topic definition and the candidate document and the topic and the candidate sentence. If both measures exceed the declared threshold, a sentence is declared relevant. For the available training topics, our relevance and novelty strategies proved to be remarkably responsive to tuning between precision-focused runs and recall-focused runs for novelty as well as the more predictable relevance decision. Our official runs involved both noun phrases and named entities. The second variation has two alternatives. The first attempts to address the semantics of novelty by expanding all noun phrases (and contained nouns) to their corresponding synset IDs, and subsequently using synset IDs for novelty comparisons. Our conjecture here is that it is possible to conflate variations in wording and hence improve novelty precision without compromising recall. The risk of over-expansion of word-sense is likely to be minimal within the confines of a single topic and a limited number of sentences. The second alternative performs word sense disambiguation using an ensemble scheme to establish whether the additional computational overhead is warranted by an increase in performance over simple sense expansion. The runs submitted within these variations are UIowa04Nov11, UIowa04Nov12, UIowa04Nov13 for task 1, UIowa04Nov21, UIowa04Nov22 for task 2, UIowa04Nov31, UIowa04Nov32 for task 3 and UIowa04Nov41, UIowa04Nov42 for task 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variation 3</head><p>The third variation employs SMART for vector similarity based decisions. First, the SMART system then generated one term vector for each sentence. The topic documents were also fed into the SMART system. For each topic, the title, description and narrative fields were included to generate the topic vector. Specifically for the narrative field, the paragraph was firstly split into sentences. The sentences containing terms like irrelevant or not relevant were discarded. The weighting scheme for both sentence and topic vectors is nnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1</head><p>If the total number of documents of a topic exceeds 25, the top 25 documents with highest similarity to the topic were retrieved. Only sentences within these 25 documents were considered as relevant candidates. We set up two relevant sentence retrieval thresholds for two runs: mean(ST SIM -2 * std(ST SIM ) and mean(ST SIM -std(ST SIM ), here ST SIM is the vector of cosine similarity between the candidate sentences and the topic. After retrieving all the relevant sentences, we extracted the novel sentences by the following procedure: 1. Mark the first relevant sentence as novel. Set the Current Knowledge vector as the term vector of the first sentence. 2. Get the next relevant sentence. Compute the cosine similarity between the sentence and the Current Knowledge SKSIM (i). 3. Expand the Current Knowledge vector by adding in the sentence vector. 4. Go to step 2 if there are more relevant sentences left unprocessed. Stop otherwise. 5. Set novelty threshold as mean(ST SIM -2 * std(ST SIM ) for run UIowa04Nov14 mean(ST SIM -std(ST SIM ) for UIowa04Nov15 and sentences with SKSIM (i) below the thresholds are retrieved as novel sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 2</head><p>The same novel retrieval strategy as task 1 was applied on the provided relevant sentence set to get novel sentences. Three runs were preformed on this task (UIowa04Nov23, UIowa04Nov24, UIowa04Nov25). The UIowa04Nov25 run used threshold mean(ST SIM ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 3</head><p>In task 3, the relevant and novel sentences in the first 5 documents are given. Firstly, the topic vectors were expanded by including the provided relevant sentences. Secondly, the number of relevant documents among the first 5 documents DREL5 can be figured out and hence there will be exactly 25-DREL5 relevant documents in the remaining ones. The top 25-DREL5 documents with highest similarity to the expanded topic were retrieved. Thirdly, we tried to get the optimal relevant sentence threshold for each topic by enumerating different thresholds ranging from 0.05 to 0.25 on the relevant documents in the first 5 documents. The threshold producing highest F score in the first 5 documents was applied to the remaining documents. Note that here the original topic vector was used to compute sentence topic similarity. Finally the sentences in the remaining relevant documents with higher-than-threshold similarity score were retrieved as relevant sentences. Similar threshold optimization was performed on novel retrieval. For each topic, the threshold with highest novel F score for the first 5 documents was picked. The search range for the best novel threshold is from 0.2 to 0.4 with step 0.05. However, there are cases that no relevant sentences are found in the first 5 documents. If that happens, the same strategies as for task 1 were applied on the remaining documents. Runs UIowa04Nov33, UIowa04Nov34 and UIowa04Nov35 represent these strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 4</head><p>Use the same novel threshold search technique as task 3 and novel sentences were retrieved by task 1 procedure using the obtained best threshold. Runs UIowa04Nov43, UIowa04Nov44 and UIowa04Nov45 represent these strategies. Our results are presented in Table <ref type="table" coords="4,330.15,151.51,4.55,10.48" target="#tab_0">1</ref>. It may be observed that our first two variations perform the best. However, it is interesting to observe that a simple, word based similarity approach is not too far behind our more sophisticated approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Genomics Task 1</head><p>We used the Lucy/Zettair search engine as the retrieval system for the Ad Hoc task because of its ability to handle the large size of the collection. Lucy/Zettair supports Boolean, ranked and phrase querying. To construct an inverted index using Lucy/Zettair, the dataset was first converted to TREC format. In addition to AB, TI, MH Medline fields, RN and GS fields were included in indexing. All the fields in the query topics were included. Gene names were automatically expanded using synonyms from the LocusLink database. Then the queries were processed to produce of phrases and words. All gene names were regarded as phrases. We also tried Boolean search using only the title but the results were not good on the five samples. We were interested in the combination of the Boolean search and ranked search but found it difficult to automatically construct a satisfying Boolean query using all the fields in the topics. Our final strategy consisted of ranked searches using the extracted phrases and words. Table <ref type="table" coords="5,407.87,479.76,5.86,10.48">2</ref> below gives the results for our one submission which was called UIowaGN1. The table essentially compares our performance with the summary of the performance for all submissions for this task. Our results indicate that we did fairly well for precision at top 10. However, since we essentially did not consider thresholding strategies, and instead submitted upto 1000 documents for each topic, our performance in the other measures has suffered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Genomics Task 2</head><p>The categorization task requires us to first decide if a test document is about mouse genomics biology. If it is, we need to categorize the document into one or more of the three Gene Ontology categories: biological processes, celluar components, and molecular funtions.</p><p>Our basic idea to tackle the categorization task is to utilize the GO terms that have been used to annotate mouse genomics biology documents, and the MeSH terms from all the training documents, to perform retrieval from the test documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GO Terms Retrieval</head><p>We collected a complete set of GO terms from each of the three GO vocabularies. These GO terms are the ones that have been used to annotate mouse genomics biology documents in MGI. We indexed all test documents with all individual words using our own information indexing/retrieval system created by Shannon Bradshaw at University of Iowa. We calculated the proportion of each categories' document in the training data set and estimated the number of testing documents that could belong to each of the three categories. Then we used each of the three categories' GO terms to perform retrieval on all testing documents, and used the estimated proportion as a threshold to select the top set of documents for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Terms Retrieval</head><p>From the training documents, we picked the documents that are annotated to belong to only one of the three GO categores. We then collected the mesh terms from these three sets of documents' MEDLINE records. We used these three sets of mesh terms as the three GO categories' mesh term vocabulary, and perform retrieval on the testing documents using our own indexing/retrieval system. We used the same estimated proportion as in the GO terms retrieval as thresholds to select the top set of documents for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining the GO terms retrieval results and the Mesh terms retrieval results</head><p>For both retrieval experiments, we used the given assignement of the gene names to the testing documents as the gene name assignment to our retrieved documents. So, if a testing document is assigned with two genes, and our retrieval puts this document into two categories, then we assigned both genes to this document as triaged to two categoreis. We realized this strategy is problematic. We had planned to investigate a better way to associate genes with document categories but we ran out of time in the end.</p><p>In our experiments using the training data, we observed that a certain way of combining the results from the GO-terms based retrieval and the MeSH-terms based retrieval could out-perform the individual retrieval using GO terms or MeSH terms alone. After a series of experiments, we decided on the following combination strategies:</p><p>• iowarun1: In our experiments using the training data, using the GO terms retrieval result for cellular component category was better than when using the combination of GO terms and Mesh terms retrieval for the cellular component category. So in our submitted run one on the test data, we use GO terms retrieval result for cellular component category. For the biological processes and the molecular functions categories, we get the retrieval results from both the GO-terms based retrieval and the MeSH-terms based retrieval given the same threshold, and then we take the union of these two sets of results as our final result for these two categories.</p><p>• iowarun2: Using the same GO terms and MeSH terms based retrieval strategies, here we only vary our thresholds for selecting the top sets of documents for each category. We set these to be 75 percent of our estimated proportion. Then we combine these two retrieval results the same way as designed in iowarun1. So the difference between iowarun1 and iowarun2 is only in the thresholds set for the GO-terms retrieval and the MeSH-terms retrieval.</p><p>• iowarun3: Using the same retrieval thresholds as in iowarun1, we use the union of the GO term retrieval and the MeSH term retrieval results for each category as our final sumbitted results for these categories. So the difference between iowarun1 and iowarun3 is only in the cellular component category. In iowarun3, the results for cellular component categories are also a union of results from the two retrieval strategies, instead of from GO-terms retrieval only.</p><p>• iowarun4 Here we decided to explore a completely different approach using a simple strategy based on citation sentences. We first used PubMed and the publisher sites linked from PubMed to gather articles citing the documents in both the training and test collections. We then extracted the paragraph within each citing article containing a citation to an article in the training collection. The paragraphs we extracted contained approximately three sentences on average. We indexed each training document in a retrieval system using individual words found in paragraphs citing that document. For each of the three categories BP, MF, and CC we identified the most strongly associated index terms.</p><p>To identify these "category terms" we chose terms that were used at least twice as frequently to cite articles in one category as they were to cite articles in either of the other two categories.</p><p>We then indexed all test documents in our retrieval system using the same technique as for training documents using a standard vector space approach. Using the category terms identified from the training collection, we queried the test collection. For each category we choose the top k documents so that the distribution of documents falling into each category matched the distribution found in the training collection. Unfortunately, this technique did not perform well. However, the citation sentences we selected did identify some useful discriminating terms for each category. We believe that further efforts employing more robust text classification techniques based on citation sentences would yield better results.</p><p>Table <ref type="table" coords="8,172.81,251.52,5.86,10.48" target="#tab_1">3</ref> provides a summary of our results for our Task 2 runs. Before we decided on the strategies for the above runs (especially the first 3 task 2 runs), we had conducted numerous experiments exploring the best way to tackle the triage subtask. Unfortunately, due to a mistake made in evaluating our performance which we only found out the day before submission, we wrongfully underestimated all our previous experiments and therefore possibly failed to identify the best strategy from our studies. Specifically, when we used 2/3 of the given training data as our 'training data' and 1/3 for testing, instead of limiting the gold standard data to these 1/3 data points we used the complete training data as our answer file. Therefore, we found for example that our very first strategy achieved an F-score of only 0.1386, while the true performance if using the correct answer file is 0.3674. Although we did not have enough time to reevaluate all our previous tests we provide a summary of our experiments which may merit further investigation if we tackle a similar problem in the future:</p><p>• Test 1: We used the abstracts of the documents to implement a decisiontree kind of 4 steps of classification:</p><p>Step 1, classify if a document is positive or negative document. (By positive, we mean that the document is about some GO category of a certain gene. By negative, we mean that the document is not about any GO category).</p><p>Step 2, if a document is positive, we classify if this document is about "cellular component" (CC) or not.</p><p>Step 3, For the same document, we classify if this document is about "molecular funtion" (MF) or not.</p><p>Step 4, For the same document, we classify if this document is about "biological processes" (BP) or not. This 4-stage decision tree will provide eight leaves which are the eight possible categorizations for a given document: all 3 categories, CC and MF, CC and BP, CC only, MF and BP, MF only, BP only, and negative.</p><p>For this test, we used all words from the document set to perform indexing and retrieval. The retrieval step gives us a ranked list of the test documents. We used the proportion of the positive training data as a threshold to classify the test documents. The step 1 classification is trained on positive and negative documents in the training set. The step 2 to 4 classification are trained on the training documents that belong to only one category: CC or MF or BP. This strategy in our testing has a performance of 0.3674 F-score.</p><p>• Test 2: The same as test 1 but we omitted step 1, which can still produce eight possible categoriztions for a given document. We estimated the performance would not surpass that of test 1.</p><p>• Test 3: The same decision-tree classification strategy as in test 1, but in the indexing and retrieval step, we used GO terms to index and all words from documents to retrieve. The performance seemed to be worse than test 1.</p><p>• Test 4: The same decision-tree classification strategy and the same indexing and retrieval method as in test 1, but we worked on the methodology sections extracted from the full text of all the training documents.</p><p>The performance seemed to be comparable to test 1.</p><p>• Test 5: The same implementation as in test 2, but we worked on the methodology sections instead. The performance seemed to be comparable to test 2.</p><p>• Test 6: Similar to test 5, the only difference is that we used GO terms for indexing but all words from documents for retrieval. The performance is estimated to be worse that test 5.</p><p>• Test 7: The above tests are basically document-level retrieval. We also performed sentence-level retrieval as a strategy to classify documents.</p><p>The rational of this strategy is similar to test 2, but we worked on the sentence level indexing/retrieval instead. We first index each sentence in our testing set of documents with all its words. Then for each of the three GO cateogory, we collect the training documents that belong to only one category, and used all the words in the training documents sentences to perform retrieval on the sentences on the testing set of documents. Since each sentence is associated with a document, the ranked list of sentence can also provide us with a ranked list of documents. Then we used a threshold (estimated proportion of documents belonging to a category) to classify a top set of testing documents as belonging to a category. We also tried some variations on this strategy by filtering the terms used for retrieval. Instead of using all terms from the training documents, we tried using the top 30 terms of a certain frequecy, and the top 100 terms of a certain frequcy, for retrieval. The top 30 terms of frequency 10 seems to perform better but not significantly better among all our previous tests.</p><p>Again, due to the evaluation mistake, we were not able to investigate the true performance of these tests before we submitted our official runs. Given these problems we are pleased with the final results that we have obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,117.33,221.91,423.35,392.63"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,117.33,221.91,423.35,392.63"><row><cell></cell><cell cols="3">: Novelty Track Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Relevant</cell><cell></cell><cell></cell><cell>Relevant</cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F</cell><cell>Precision</cell><cell>Recall</cell><cell>F</cell></row><row><cell></cell><cell></cell><cell cols="2">Task 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UIowa04Nov11</cell><cell>0.31</cell><cell>0.82</cell><cell>0.42</cell><cell>0.15</cell><cell>0.71</cell><cell>0.229</cell></row><row><cell>UIowa04Nov12</cell><cell>0.31</cell><cell>0.82</cell><cell>0.42</cell><cell>0.15</cell><cell>0.67</cell><cell>0.23</cell></row><row><cell>UIowa04Nov13</cell><cell>0.31</cell><cell>0.82</cell><cell>0.42</cell><cell>0.15</cell><cell>0.71</cell><cell>0.227</cell></row><row><cell>UIowa04Nov14</cell><cell>0.29</cell><cell>0.74</cell><cell>0.392</cell><cell>0.12</cell><cell>0.68</cell><cell>0.188</cell></row><row><cell>UIowa04Nov15</cell><cell>0.29</cell><cell>0.74</cell><cell>0.392</cell><cell>0.11</cell><cell>0.58</cell><cell>0.175</cell></row><row><cell></cell><cell></cell><cell cols="2">Task 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UIowa04Nov21</cell><cell></cell><cell></cell><cell></cell><cell>0.48</cell><cell>0.9</cell><cell>0.609</cell></row><row><cell>UIowa04Nov22</cell><cell></cell><cell></cell><cell></cell><cell>0.46</cell><cell>0.92</cell><cell>0.604</cell></row><row><cell>UIowa04Nov23</cell><cell></cell><cell></cell><cell></cell><cell>0.42</cell><cell>0.96</cell><cell>0.569</cell></row><row><cell>UIowa04Nov24</cell><cell></cell><cell></cell><cell></cell><cell>0.42</cell><cell>0.87</cell><cell>0.553</cell></row><row><cell>UIowa04Nov25</cell><cell></cell><cell></cell><cell></cell><cell>0.44</cell><cell>0.57</cell><cell>0.482</cell></row><row><cell></cell><cell></cell><cell cols="2">Task 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UIowa04Nov31</cell><cell>0.29</cell><cell>0.91</cell><cell>0.407</cell><cell>0.13</cell><cell>0.74</cell><cell>0.208</cell></row><row><cell>UIowa04Nov32</cell><cell>0.29</cell><cell>0.91</cell><cell>0.407</cell><cell>0.13</cell><cell>0.79</cell><cell>0.207</cell></row><row><cell>UIowa04Nov33</cell><cell>0.32</cell><cell>0.64</cell><cell>0.398</cell><cell>0.12</cell><cell>0.62</cell><cell>0.188</cell></row><row><cell>UIowa04Nov34</cell><cell>0.32</cell><cell>0.64</cell><cell>0.398</cell><cell>0.12</cell><cell>0.61</cell><cell>0.187</cell></row><row><cell>UIowa04Nov35</cell><cell>0.33</cell><cell>0.62</cell><cell>0.396</cell><cell>0.12</cell><cell>0.55</cell><cell>0.178</cell></row><row><cell></cell><cell></cell><cell cols="2">Task 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UIowa04Nov31</cell><cell></cell><cell></cell><cell></cell><cell>0.44</cell><cell>0.9</cell><cell>0.57</cell></row><row><cell>UIowa04Nov32</cell><cell></cell><cell></cell><cell></cell><cell>0.43</cell><cell>0.92</cell><cell>0.567</cell></row><row><cell>UIowa04Nov33</cell><cell></cell><cell></cell><cell></cell><cell>0.39</cell><cell>0.97</cell><cell>0.538</cell></row><row><cell>UIowa04Nov34</cell><cell></cell><cell></cell><cell></cell><cell>0.39</cell><cell>0.96</cell><cell>0.536</cell></row><row><cell>UIowa04Nov35</cell><cell></cell><cell></cell><cell></cell><cell>0.39</cell><cell>0.96</cell><cell>0.535</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,139.62,283.63,328.11,169.42"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="8,139.62,283.63,328.11,169.42"><row><cell cols="4">: Genomics Track -Task 2 Results</cell><cell></cell></row><row><cell>Run</cell><cell cols="4">iowarun1 iowarun2 iowarun3 iowarun4</cell></row><row><cell>True positive</cell><cell>269</cell><cell>223</cell><cell>297</cell><cell>66</cell></row><row><cell>False positive</cell><cell>529</cell><cell>362</cell><cell>629</cell><cell>324</cell></row><row><cell>False negative</cell><cell>226</cell><cell>272</cell><cell>198</cell><cell>426</cell></row><row><cell>Precision</cell><cell>0.3371</cell><cell>0.3812</cell><cell>0.3207</cell><cell>0.1692</cell></row><row><cell>Recall</cell><cell>0.5434</cell><cell>0.4505</cell><cell>0.6000</cell><cell>0.1333</cell></row><row><cell>F-score</cell><cell>0.4161</cell><cell>0.4130</cell><cell>0.4180</cell><cell>0.1492</cell></row><row><cell>Utility Factor</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>Raw Utility</cell><cell>4851</cell><cell>4098</cell><cell>5311</cell><cell>996</cell></row><row><cell>Max Utility</cell><cell>9900</cell><cell>9900</cell><cell>9900</cell><cell>9900</cell></row><row><cell>Normalized Utility</cell><cell>0.4900</cell><cell>0.4139</cell><cell>0.5365</cell><cell>0.1006</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
