<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,194.45,71.04,222.90,20.74">RMIT University at TREC 2004</title>
				<funder>
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-01-31">January 31, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.67,104.79,78.58,14.41"><forename type="first">Bodo</forename><surname>Billerbeck</surname></persName>
							<email>bodob@cs.rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.21,104.79,73.95,14.41"><forename type="first">Adam</forename><surname>Cannane</surname></persName>
							<email>cannane@cs.rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.12,104.79,80.55,14.41"><forename type="first">Abhijit</forename><surname>Chattaraj</surname></persName>
							<email>abhijit@cs.rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.63,104.79,75.28,14.41"><forename type="first">Nicholas</forename><surname>Lester</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,145.23,118.74,78.50,14.41"><forename type="first">William</forename><surname>Webber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.69,118.74,86.08,14.41"><forename type="first">Hugh</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.73,118.74,62.02,14.41"><forename type="first">John</forename><surname>Yiannis</surname></persName>
							<email>jyiannis@cs.rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.70,118.74,58.71,14.41"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<addrLine>GPO Box 2476V</addrLine>
									<postCode>3001</postCode>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,194.45,71.04,222.90,20.74">RMIT University at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-01-31">January 31, 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">BC29BEFE82E869DC4AE466C06FEA0A75</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>RMIT University participated in two tracks at TREC 2004: Terabyte and Genomics, both for the first time. This paper describes the techniques we applied and our experiments in both tracks, and discusses the results of the genomics track runs; the terabyte track results are unavailable at the time of manuscript submission. We also describe our new zettair search engine, in use for the first time at TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Zettair Search Engine</head><p>The zettair search engine is a publicly available system developed by the Search Engine Group at RMIT. It is intended to be a straightforward implementation of effective and efficient query evaluation techniques based on current information retrieval research. Zettair is available under a BSD license from http: //www.seg.rmit.edu.au/zettair. This is the first year that zettair has been used at TREC. The latest release version (0.6.1) is capable of indexing the GOV2 collection on a single machine, assuming sufficient disk space. Zettair is written in C and has been ported to many different operating systems, including Linux, Microsoft Windows, Sun Solaris, FreeBSD, and Mac OS X (Darwin).</p><p>Zettair can extract text from SGML-based languages -although it makes no pretense at full SGML processing -with direct support for indexing TREC collections. Users can select which portions of SGMLbased languages are indexed by editing a configuration file. Zettair implements an efficient index construction algorithm <ref type="bibr" coords="1,150.34,468.55,87.62,12.00" target="#b3">(Heinz &amp; Zobel 2003)</ref>, with the exception that in-place merging <ref type="bibr" coords="1,409.74,468.55,86.32,12.00" target="#b6">(Moffat &amp; Bell 1995)</ref> is not currently implemented and a variable-byte compression scheme is used instead of Golomb coding <ref type="bibr" coords="1,485.04,480.51,35.80,12.00;1,90.71,492.46,131.72,12.00" target="#b9">(Scholer, Williams, Yiannis &amp; Zobel 2002)</ref>. Indexes contain full word positions, which allow the resolution of phrase queries in addition to simple ranked queries. The indexing algorithm used is single-pass, and generates compressed postings in-memory and writes them to disk to limit the amount of memory used during indexing. Our implementation currently allows a (rough) configurable limit on the amount of memory used during indexing. On-disk postings are merged together to keep the number of different postings runs manageable, and then a final merge creates the final index from the on-disk postings. The final merge ensures that all postings for each term are stored in a single, contiguous list, and bulk-builds a B + -tree vocabulary structure.</p><p>Multiple ranking metrics are supported, including pivoted Cosine (Singhal, Buckley &amp; Mitra 1996) and Okapi BM25 <ref type="bibr" coords="1,147.65,588.10,141.02,12.00" target="#b5">(Jones, Walker &amp; Robertson 2000)</ref>. Our submissions use the Okapi metric, with the exact ranking formula: bm25(q, d) = t∈q log N -f t + 0.5 f t + 0.5</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where terms t appear in query q; there are N documents in the collection; a term occurs in f t documents and in a particular document f d,t times; K is k 1 ((1 -b) + b × L d /AL); constants k 1 and b are set to 1.2 and 0.75 respectively; L d is the length of a particular document as measured in bytes, and AL is the average document length over the collection. In this formulation, term contributions for terms that occur in more than half the documents in the collection are negative. In this case, a small positive term contribution is used instead.</p><p>Zettair can read directly from TREC query files and produce ranked output that is compatible with trec eval. When it has access to the original source text, zettair can provide query-biased summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parsing</head><p>The zettair document parsing system is designed to make a single pass over the source document with only minimal buffering necessary for correct parsing. Due to the frequently malformed nature of the HTML that the search engine is required to process -for TREC tracks and other tasks -the HTML parser validates each tag by ensuring that each open tag character (&lt;) has a corresponding close tag character (&gt;) within a configurable numbers of characters, currently set to 999. It also ensures that no un-escaped additional start tags occur within this span. HTML comments, whose content is ignored during indexing, are also validated in this way (matching &lt;!--to --&gt; without additional occurrences of &lt;!--), as are entity references (such as matching &amp; to ;). Limited standard HTML entity references are recognised and are substituted by their entities. Zettair does not currently support internationalised text, so only entities within the ASCII character set are translated. Unrecognised entites or invalid constructs are treated as if they are plain text. A primary aim of this treatment by the parser is to limit the amount of material that malformed markup can cause the parser to ignore during indexing.</p><p>Once tags and other HTML entities are recognised within the source text, a configuration file is used by the parser to determine which tags to index content between. The default is to index any material that is not explicitly excluded. Our current implementation indexes all material in source documents with the exception of material contained within comments, JavaScript, and script, style, and vbscript tags. Material contained within TREC document header tags is also excluded from indexing.</p><p>Documents are delimited within TREC collections by the &lt;/doc&gt; tag. This is not foolproof, both because some of the TREC collections contain spurious empty documents that consist of only a &lt;/doc&gt; tag, and because the source documents may also contain the sequence &lt;/doc&gt;. The result of this, and possibly other factors, is that some documents lack a document number identifier, which makes retrieving them in response to queries problematic. In the rare case that a document that does not have a document number is retrieved in response to a TREC query, the document number of the prior document (or closest prior document with a document number) in the same source file is returned instead.</p><p>Another problem that the parsing system attempts to work around is to avoid parsing binary documents. Some previous TREC collections, notably WT10g and .GOV, have contained numerous binary documents, and GOV2 is the same. The parser ignores binary documents using heuristics, and all content in those documents is ignored until the next &lt;/doc&gt; tag. The heuristics include searching for well-known byte sequences near the start of the document, in the same manner as the file(1) utility on Unix-like systems. A small subset of heuristics for document types judged most likely to occur in the TREC collections, such as PDF and Microsoft Word documents, was imported from the FreeBSD implementation of the file(1) utility. To apply these heuristics it is necessary to find the start of a document within the byte stream. This is difficult, as TREC collections have different formats for delimiting the content and the meta-data associated with each document. The parsing system treats any construct within the set of tags with prefix "doc" at the start of the file as header information.</p><p>Text that is not removed by HTML parsing is tokenised into separate terms for indexing. Our termbreaking heuristic ends a term when any of several criteria are met: a whitespace character is encountered, a HTML tag is encountered, control or extended ASCII characters are encountered, or two successive punctuation marks are encountered. Only letters and digits are preserved within the term for indexing purposes.</p><p>3 Terabyte-track runs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline</head><p>Our baseline run used the Okapi BM25 formulation described in the previous section. Given the system-tosystem differences of participants, such as specifics of parsing, our expectation is that this run will allow us to identify correspondences and differences between zettair and other runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Anchortext</head><p>This run aggregated the results of a standard zettair evaluation, as described above, with those from a parallel anchortext index. This aggregate index was then used as the base index for the fuzzy phrase and priority compound metrics described below.</p><p>The anchortext index was constructed as follows. For each document d in the collection, the texts of all links pointing to d from other documents in the collection were concatenated into a parallel (possibly empty) anchortext document A(d). All parallel anchortext documents were then written out as a document collection in TREC format, with each A(d) having the same TREC document identifier as its corresponding d. This parallel collection was then indexed in the standard way to create a parallel anchortext index.</p><p>Query evaluation then involved submitting the query to both the main and the anchortext indexes, and combining the results. The main index used the default Okapi metric described previously. The anchortext index used a metric for anchortext collections <ref type="bibr" coords="3,274.86,269.49,143.79,12.00" target="#b2">(Hawking, Upstill &amp; Craswell 2004)</ref>, which we refer to as the Hawkapi metric. Taking the final number of results required as n, a total of 5n results were extracted from the main index, and the same number again from the anchortext index. For each main index result, if it also occurred in the anchortext results, its anchortext score multiplied by a weighting factor was then added to its main score. These results were then sorted, and the top n results returned.</p><p>Two points should be noted about the combined metric. First, a document cannot appear in the final results if it only appears in the anchortext results, regardless of its score. Second, a main result can only have its score supplemented by its anchor result if it is in the top 5n anchor results; anchor results that did not meet this threshold were not considered.</p><p>Several different anchor weightings were tried and their precision calculated using our own evaluations, based on a limited manual evaluation of old .GOV queries on the new .GOV2 collection. The best weighting using this method was 0.2, which was the weighting used for the final submission. Since the Okapi BM25 and Hawkapi metrics are not normalized against each other, no particular significance should be placed on the precise value of this metric.</p><p>Our sample evaluations showed the anchortext-enhanced index provided better precision than the plain index. There are several directions for future work on enhancing results with anchortext. First, a method for combining plain and anchortext results that did not require a document to appear in the top 5n plain results in order to make it to the final results should be considered. Second, the Okapi and Hawkapi metrics could be normalised, making the precise values used for weighting the anchortext enhancement more meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fuzzy Phrase</head><p>We have had positive experiences with the use of phrases in past work, and so we submitted a run in which each query was modified by repeating it as a fuzzy phrase. A fuzzy or sloppy phrase is a phrase query in which the terms of the phrase do not have to appear precisely in sequence; there can be gaps between them, up to a certain limit, and possibly their order can also be rearranged.</p><p>Our implementation of fuzzy phrases allowed a sloppiness factor s to be specified at query-time. This sloppiness factor allows words in a fuzzy phrase to be up to s places from their correct position. The syntax used is "term1 term2 term3" [sloppy: s]. For example, the phrase "A C" <ref type="bibr" coords="3,385.67,603.62,45.20,11.90">[sloppy: 2]</ref> </p><formula xml:id="formula_0" coords="3,90.72,603.62,430.17,23.96">would match A C, A x C, A x x C, or C A, but not C x A or A x x x C.</formula><p>The fuzzy phrase metric was implemented by taking the query, and then modifying it by appending to the end of the query a copy of the query as a phrase with a sloppiness of 5. So, for example, the query A B C becomes A B C "A B C" <ref type="bibr" coords="4,193.14,35.90,44.24,11.90">[sloppy: 5]</ref>. This modified query was then submitted to the base anchortext index described in the previous section.</p><p>We allowed the fuzzy phrase metric to use a different anchortext weighting from that used for the base anchortext run. Several different anchor weightings were tried for the fuzzy phrase metric and their precision calculated using our own evaluations. The best weightings were in the range 0.6 to 1.0; we chose 1.0 for our submitted run and, again, no significance should be placed upon the precise value chosen. However, we note that the anchortext weighting here is much higher than for the base scheme. We also observed in our sample evaluations that, as we expanded the pool of evaluated documents and allowed for recall to be calculated to a greater number of results, the optimal anchortext weighting decreased. The reasons for this are unclear. We will wait for the official results and relevance judgements to investigate this further.</p><p>Our sample evaluations showed the fuzzy phrase metric performing somewhat worse than the base anchortext metric, and even than the plain (non-anchortext) metric, which was a surprise to us. We await the official results to confirm this observation.</p><p>Several modifications to our fuzzy phrase metric suggest themselves. First, we would like to be able to specify the weight to give to the phrase-term as against the plain terms in the combined query. Second, it might be desirable to give a higher score for more exact phrases than for more sloppy ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Priority</head><p>We also experimented with preferencing or giving priority to documents that matched one or more criteria. The candidate criteria were: contains the query as a phrase; contains the query as a sloppy phrase; contains all query terms in the first N words of the document; and, contains all query terms in the HTML title of the document. Experiments on the .GOV collection suggested that the best combination of criteria was sloppy phrase (with a sloppiness of 5) and all query terms in the first 50 words of the document. In the actual submission, this scheme was denoted FunkyZ.</p><p>The metric was implemented as follows. Taking the final number of results required as n, we retrieve 5n results from the plain (non-anchortext) index. We take the score of the top result as our boost amount, b. Then, we check each of these result documents to see if it meets our criteria. For each criterion it meets, we add b to its score. We then supplement these results with scores from the parallel anchortext index; the anchortext weighting used was 0.2.</p><p>With this metric, any document meeting M criteria -no matter how low its base score but provided it appears in the top 5n results -will be ranked higher than any document meeting only M -1 criteria. This is regardless of its base score.</p><p>Although it had been quite promising in the experiments on .GOV, this priority scheme performed terribly using our sample evaluations on .GOV2. Examining the actual documents returned, we observed that these documents did indeed (as expected) have the query terms at the top of the document as or nearly as a phrase. The results, however, contained a high proportion of link documents, that is, pages containing lists of hypertext links to other pages on the subject. Since the relevance judging criteria required the page itself to directly contain the information being searched for, this may have been one reason why the priority scheme performed so badly in our sample runs.</p><p>This implementation of the priority scheme was simplistic. Consideration should be given to making the boost amount smaller, so that otherwise high-ranking documents that do not meet the extra criteria can actually remain highly-ranked. The top-N (for us, top-50) criterion could be changed to take account of the document length, although not as a linear proportion of it. In evaluating the top-N criteria, we would also like to be able to strip out navigational text, although it is not clear whether this was a serious issue for the .GOV2 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Query expansion</head><p>For the expansion run we used the local analysis method proposed by <ref type="bibr" coords="4,368.52,620.46,109.12,12.00" target="#b8">Robertson &amp; Walker (1999)</ref>, where 25 terms with the lowest term selection value are chosen from the top 10 ranked documents: </p><formula xml:id="formula_1" coords="5,258.76,191.82,86.89,26.51">T SV t = f t N rt R r t</formula><p>where term t appears in f t of all N documents in the collection, and locally in r t of the top R documents. Expansion terms were added to the query, and their weight <ref type="bibr" coords="5,332.63,238.17,112.71,12.00" target="#b8">(Robertson &amp; Walker 1999)</ref> was derived using the formula:</p><formula xml:id="formula_2" coords="5,139.68,249.28,267.57,57.10">1 1 3 × log (r t + 0.5)/(R -r t + 0.5) (f t -r t + 0.5)/(N -f t -R + r t + 0.5)</formula><p>Although we have previously found that the number of top ranked documents used and terms chosen for expansion is highly collection-dependent <ref type="bibr" coords="5,255.44,326.23,108.75,12.00" target="#b0">(Billerbeck &amp; Zobel 2004a)</ref>, in the absence of additional collection information (such as relevance judgements of previous years) we used the default values.</p><p>We found in unpublished experiments that placing the restriction on expansion terms that they need to occur in a minimum number of ranked documents can be beneficial; for example, it can be effective to select only the terms that fulfil the condition r t &gt; 2. However, in our preliminary experiments this restriction did not seem to help with this collection. We therefore did not restrict the selection of expansion terms.</p><p>No use was made of anchor text or any other query-independent additional information for the query expansion run; documents were ranked using only their full text.</p><p>We plan to validate alternative and more efficient methods of expansion <ref type="bibr" coords="5,406.02,421.87,110.16,12.00" target="#b1">(Billerbeck &amp; Zobel 2004b</ref>) using the relevance judgements for this collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Results</head><p>Results are shown in Table <ref type="table" coords="5,199.40,481.03,3.73,12.01" target="#tab_0">1</ref>. We were surprised that no technique is on average significantly better than full text or full text with anchortext. For example, query expansion is the subject of a long-term research project at RMIT and is well-understood. It works well for some topics (the highest MAP for 4 topics, and above median for 34); but has not worked well on others. The fuzzy phrase method was particularly surprising, as it was the most effective on our training runs, where admittedly the number of queries was small, and in previous TREC environments.</p><p>To explore these results further, we used the relevance judgements to see if we had, for example, made poor choices of parameters. However, these experiments only yielded small improvements, and in no instance exceeded our baseline run.</p><p>We also discovered that our baseline run was less effective than similar baselines reported by other groups at TREC. This appears to be due to differences in elements such as parsing and stemming rather than the strategies used for similarity measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Genomic-track Runs</head><p>We participated in the Genomic track for the first time and submitted two runs for the ad hoc retrieval task. Similarly to our terabyte-track experiments, our baseline run used the Okapi BM25 formulation. Given the system-to-system differences of participants, this should again allow us to compare our second run to other non-RMIT runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Structure of Genomic Data</head><p>In structured document search, it is important to identify what parts of the document should be searched (and which should be ignored) in response to ranked queries. The strictly-defined structure of genomic data lends itself well to the process of selecting the best possible representative elements from queries and data. This approach has been shown at TREC 2003 to be highly effective <ref type="bibr" coords="6,362.44,177.09,158.42,12.00;6,90.71,189.05,207.73,12.00" target="#b7">(Osborne, Cuminskey, Sinclair, Smillie, Webber, Chang, Mehra, Rotemberg &amp; Altman 2003)</ref>.</p><p>The process of selecting fields requires careful consideration. In developing our techniques, we performed exhaustive experiments to assess which combinations of fields from queries and data leads to the most effective representation. With the training queries, we found accuracy improved by almost 1% by simply ignoring 18 fields from the Medline records<ref type="foot" coords="6,294.64,236.01,3.48,8.40" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Expansion</head><p>Variation exists in how concepts are expressed in queries and documents. These variations range from simple differences such as changes in case and word-separating punctuation, to more complex differences such as the use of abbreviations, acronyms, synonyms, plurals, and tense. Variations give rise to ambiguity in the data, and typically result in false negatives.</p><p>Case-folding overcomes differences between terms by representing all terms uniformly in a single case. We case-fold in our experiments. Punctuation variations can be resolved using pattern recognition and replacement schemes; we use the zettair parser in its default mode as described previously. More complex techniques are required for resolving synonyms, abbreviations, and acronyms. Stemming can resolve most issues with plurals; resolving tense is more difficult. We do not consider stemming or tense in our work.</p><p>Synonym expansion has been shown to yield significant improvements in accuracy <ref type="bibr" coords="6,447.48,391.66,73.39,12.00;6,90.71,403.62,17.90,12.01" target="#b4">(Hull &amp; Waldman 2003</ref><ref type="bibr" coords="6,108.61,403.62,86.91,12.00" target="#b7">, Osborne et al. 2003)</ref>. We address the problem of synonyms by attempting to unify the different terms used for species names: when a species from a hand-crafted list is found, it is supplemented with all variants. For example, a search for the term "Mus musculus" does not match a document that contains only the terms "mouse" or "mice". Using our approach, all appearances of the term "Mus musculus" -in queries and documents -are supplemented with the additional terms "mouse" and "mice".</p><p>A hand-crafted list of 66 synonyms was created to expand terms relating to species. First, a list of species terms from past queries was obtained. Second, an online dictionary was searched for each species term and a list of words related to the species was created. Further words were added to the list by selecting unique terms from the MESH field and article abstracts from Medline data. Last, the list of terms was manually inspected to create the final set of synonyms we use in our experiments<ref type="foot" coords="6,338.73,510.36,3.48,8.40" target="#foot_2">3</ref> . Documents and the 2004 test queries were then processed to supplement each appearance of a target-term with its synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Collection Partitioning</head><p>Conventional techniques for searching text typically examine each document in a collection for possible matches with the query. Collection partitioning is aimed at reducing the search space based on existing knowledge that a subset of documents cannot be relevant to a query. For example, a query that specifies the answer must describe a human gene cannot match documents that describe other species or do not concern genes. To reduce the search space, a set of partitioning terms need to be chosen. These terms should unambiguously divide a collection into the set of documents that may be relevant and those that are not; this occurs through a query pre-processing step using a Boolean AND of partitioning terms. When such terms are known, a query that contains a partitioning term can be evaluated on a subset of the collection, and the remainder of the collection ignored. Ideally, the terms should appear frequently in a small, but significant subset of the collection.</p><p>The search process with collection partitioning requires two processes: the first step is to execute queries that contain partitioning terms on a subset collection that contains those terms; and, the second step resolves the remaining queries -those that do not contain partitioning terms -using the entire collection. We use only names of species as the basis for partitioning, and details are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Queries supplied in 2004 are more structured than those used in 2003. Each contains a unique identifier, a brief description of the information need (the Query-title), an expanded description of the information need (the Information-need), and information to provide context for the search task (the Context). Using the training queries, we found that using all fields from each query was most effective. However, in our secondary experiment, we chose to use only Query-title and Information-need, based on our expectation this combination would work best.</p><p>Our approach to term expansion is aimed at supplementing selected terms in queries and documents with synonyms. Unfortunately, despite careful design of our synonym list, only one of the five training queries was expanded at all and this lead to poor performance for that query. Despite this, we used the technique in our submission.</p><p>We use names of species as partitioning terms for partitioning the collection in our submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submissions</head><p>RMITa Our primary run for this track uses default zettair settings, ignores 18 selected fields from each Medline record, and uses all fields from the query.</p><p>RMITb Our secondary run combines several approaches. Collection partitioning is performed on the collection, and only the Title and Information-need fields are used from the query. Species specific term expansion is applied to both the queries and the collection.</p><p>Table <ref type="table" coords="7,129.96,552.57,4.97,12.01" target="#tab_1">2</ref> presents accuracy results for the RMITa and RMITb runs on the 2004 test data. RMITa performed the best of the two submissions, achieving a mean average precision 7.4% better than RMITb. Based on overall results from the ad-hoc task, RMITa achieved average precision that was the same as or better than the median for 32 of the 50 queries in the test set, and scored the highest for 2 queries. For precision-at-10 documents, RMITa scored better than the median for 30 queries and achieved the highest score for 12 of the queries. In contrast, RMITb did not perform well: both the average precision and precision-at-10 values for this run obtained or exceeded the median score for only 25 of the 50 queries. For precision at 10, RMITb achieved the best result for 9 queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The plain text run, RMITa, performed better than the combined collection partitioning and species term expansion approach, RMITb. Partitioning approaches work well when there is sufficient knowledge or context regarding the query and documents. In the absence of such advanced knowledge, ranked retrieval techniques are more effective. Further, only 25 of the 50 queries contained species specific data: as a result, the combination of collection partitioning and species term expansion performed poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>RMIT participated in the Genomics and Terabyte tracks for the first time in 2004, and used the zettair search engine for the first time at TREC. Results for the Terabyte track are unavailable, but our Genomics track results suggest that zettair is an effective text retrieval engine. In contrast, our special-purpose techniques for genomic IR were less effective, and refinement continues. We eagerly await the results of our Terabyte track runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,173.26,42.71,265.35,98.97"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of terabyte runs</figDesc><table coords="5,173.26,64.72,265.35,76.96"><row><cell>Technique</cell><cell>R</cell><cell>Average</cell><cell>Precision</cell></row><row><cell></cell><cell cols="2">Precision Precision</cell><cell>at 10</cell></row><row><cell>Plain (ZETPLAIN)</cell><cell>0.2986</cell><cell>0.4898</cell><cell>0.2230</cell></row><row><cell>Query expansion (ZETBODOFFFF)</cell><cell>0.2780</cell><cell>0.5122</cell><cell>0.2189</cell></row><row><cell>Combined (ZETANCH)</cell><cell>0.2940</cell><cell>0.5020</cell><cell>0.2168</cell></row><row><cell>Priority (ZETFUNKYZ)</cell><cell>0.2854</cell><cell>0.4592</cell><cell>0.2067</cell></row><row><cell>Fuzzy phrase (ZETFUZZY)</cell><cell>0.1965</cell><cell>0.3653</cell><cell>0.1306</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,194.75,42.71,222.27,66.09"><head>Table 2 :</head><label>2</label><figDesc>Overall effectiveness of runs, on 2004 test data    </figDesc><table coords="7,194.91,64.72,222.01,44.09"><row><cell>Technique</cell><cell>R</cell><cell>Average</cell><cell cols="2">Precision Precision</cell></row><row><cell></cell><cell cols="2">Precision Precision</cell><cell>at 10</cell><cell>at 20</cell></row><row><cell>RMITa</cell><cell>0.3199</cell><cell>0.2796</cell><cell>0.5120</cell><cell>0.4700</cell></row><row><cell>RMITb</cell><cell>0.2508</cell><cell>0.2059</cell><cell>0.4560</cell><cell>0.4060</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.06,632.39,265.36,9.60"><p>The fraction 1/3 was recommended by the authors in unpublished correspondence.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,105.06,638.04,343.94,9.60"><p>The ignored fields can be found at: http://www.cs.rmit.edu.au/∼abhijit/ignored-fields.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,105.06,647.77,415.17,9.60"><p>The complete list of 66 synonyms can be found at: http://www.cs.rmit.edu.au/∼abhijit/2004species-synonyms.txt</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported by the <rs type="funder">Australian Research Council</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,90.71,306.52,430.01,10.80;8,110.63,317.48,304.77,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,216.27,306.52,267.28,10.80">Questioning query expansion: An examination of behaviour and parameters</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Billerbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,501.87,306.52,18.85,10.80;8,110.63,317.48,96.78,10.80">Proc. Australasian Database Conf</title>
		<meeting>Australasian Database Conf</meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2004">2004a</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,332.42,430.04,10.80;8,110.63,343.39,189.23,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,216.54,332.42,142.81,10.80">Techniques for efficient query expansion</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Billerbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,377.83,332.42,142.92,10.80;8,110.63,343.39,51.03,10.80">Proc. String Processing and Information Retrieval Symp</title>
		<meeting>String essing and Information Retrieval Symp<address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004b</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,358.33,430.09,10.80;8,110.63,369.29,307.60,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,267.86,358.33,127.44,10.80">Toward better weighting of anchors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Upstill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,415.21,358.33,105.59,10.80;8,110.63,369.29,197.36,10.80">Proc. ACM-SIGIR Int. Conf. on Research and Development in Information Retrieval</title>
		<meeting>ACM-SIGIR Int. Conf. on Research and Development in Information Retrieval<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="512" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,384.23,430.04,10.80;8,110.63,394.87,203.37,11.67" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,200.55,384.23,205.47,10.80">Efficient single-pass index construction for text databases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,415.67,384.23,105.08,10.71;8,110.63,395.19,143.69,10.71">Jour. of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="713" to="729" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,410.13,430.08,10.80;8,110.63,421.09,410.21,10.80;8,110.63,432.05,202.95,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,207.55,410.13,223.35,10.80">Recognizing gene and protein function in MEDLINE abstracts</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Waldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.45,410.13,71.34,10.80;8,110.63,421.09,70.10,10.80">Proc. Text Retrieval Conference (TREC)</title>
		<meeting>Text Retrieval Conference (TREC)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="93" to="97" />
		</imprint>
		<respStmt>
			<orgName>NIST-National Institute of Standards and Technology, National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,446.99,430.06,10.80;8,110.63,457.63,340.89,11.67" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,287.03,446.99,233.75,10.80;8,110.63,457.96,127.69,10.80">A probabilistic model of information retrieval: development and comparative experiments. Parts 1&amp;2</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,249.41,457.96,142.44,10.71">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="779" to="840" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,472.90,430.06,10.80;8,110.63,483.54,132.04,11.67" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,222.62,472.90,165.48,10.80">In situ generation of compressed inverted files</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A H</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,396.39,472.90,124.39,10.71;8,110.63,483.86,72.36,10.71">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="537" to="550" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,498.80,430.02,10.80;8,110.63,509.76,410.19,10.80;8,110.63,520.71,410.19,10.80;8,110.63,531.68,106.81,10.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,149.25,509.76,171.12,10.80">Edinburgh-stanford TREC-2003 genomics track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cuminskey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smillie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,338.78,509.76,143.09,10.80">Proc. Text Retrieval Conference (TREC)</title>
		<meeting>Text Retrieval Conference (TREC)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="622" to="630" />
		</imprint>
		<respStmt>
			<orgName>NIST-National Institute of Standards and Technology, National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,546.62,430.09,10.80;8,110.63,557.58,214.52,10.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,225.57,546.62,96.59,10.80">Okapi/Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,341.31,546.62,120.76,10.80">Proc. Text Retrieval Conf. (TREC)</title>
		<meeting>Text Retrieval Conf. (TREC)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,572.53,430.03,10.80;8,110.63,583.48,410.16,10.80;8,110.63,594.44,15.66,10.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,303.11,572.53,204.93,10.80">Compression of inverted indexes for fast query evaluation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yiannis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,114.40,583.48,299.78,10.80">Proc. ACM-SIGIR Int. Conf. on Research and Development in Information Retrieval</title>
		<meeting>ACM-SIGIR Int. Conf. on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.71,609.38,430.08,10.80;8,110.63,620.34,320.26,10.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,256.11,609.38,140.28,10.80">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,415.81,609.38,104.98,10.80;8,110.63,620.34,197.36,10.80">Proc. ACM-SIGIR Int. Conf. on Research and Development in Information Retrieval</title>
		<meeting>ACM-SIGIR Int. Conf. on Research and Development in Information Retrieval<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
