<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.77,76.73,446.54,18.59">Question Answering using the DLT System at TREC 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.05,109.70,125.04,13.44"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.97,109.70,65.88,13.44"><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.93,109.70,76.03,13.44"><forename type="first">Kieran</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.77,126.14,92.70,13.44"><forename type="first">Aoife</forename><surname>O'gorman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.13,126.14,97.15,13.44"><forename type="first">Michael</forename><surname>Mulcahy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.77,76.73,446.54,18.59">Question Answering using the DLT System at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C7E5941C9FC03D3C7B9C4ED0AD2C549F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article outlines our participation in the Question Answering Track of the Text REtrieval Conference organised by the National Institute of Standards and Technology. We first provide an outline of the system. We then describe the changes made relative to last year. After this we summarise our results before drawing conclusions and identifying some next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Outline of System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Strategy</head><p>In common with most other QA systems, the following stages are at the core of our approach:</p><p>• Question analysis: Process the input query attempting to find its type (e.g. who or colour) and to identify significant phrases.</p><p>• Document retrieval: Formulate a search query based on the results of the previous stage. Use this together with a search engine indexed on the document collection to produce a list of candidate documents which are likely to contain answers to the question.</p><p>• Named entity recognition: Based on the query type identified in the first stage, search for corresponding named entities (NEs) in the candidate documents which co-occur with terms derived from the query.</p><p>• Answer selection: Decide which NE (or NEs) should be chosen as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Factoids</head><p>Most of the questions in the TREC task are factoid. They ask for straightforward pieces of information which can be extracted from free text fairly readily. Our approach to these is conventional and relies on two simple ideas: Firstly, the required NE type can be predicted from the question itself, and secondly NEs can be identified in the text by simple means which are determined in advance. Two key stages in this process are the formulation of the search query during Document Retrieval, and the choice of correct answer from a list of candidates during Answer Selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lists</head><p>We have not so far devoted much attention to list questions so the approach adopted is simply to treat them as factoids and to return all answers which exceed a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Definitions (i.e. type 'Other')</head><p>Definitions are approached differently from factoids and lists. Having returned documents which contain the appropriate target phrase, we search for instances of phrasal patterns which can indicate the presence of important information about the target. Each sentence containing such a phrase is then returned. The phrases used were adapted from another project <ref type="bibr" coords="2,285.77,481.57,127.35,10.57" target="#b2">(Gabbay and Sutcliffe, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Question Groups</head><p>For this year the overall task changed. Previously the object was to answer 500 individual questions which could each be of factoid, list or definition type. No link existed between questions. However, for this year the task was based around groups of questions each concerning an overall topic which was identfied by a marked-up target. There were 65 targets in total and their breakdown by topic can be seen in Table <ref type="table" coords="2,113.25,582.97,4.14,10.57" target="#tab_0">1</ref>. The vast majority (51) concern organisations and persons. Within each group, a question is explicitly marked-up as being of type Factoid, List or Other (i.e. Definition).</p><p>While the grouping of questions allows interesting experiments concerning interactions between answers, we did not attempt this. Instead we adopted the simplistic approach of adding the target terms to the end of each query within a group and then proceeding to process the individual questions using the same basic method as last year. As it turned out, this was not a bad strategy.</p><p>In the next section we describe the major additions which were made to the system relative to the last TREC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>George Warrington</head><p>Table 2: Question Types used in the DLT system. The second column shows a sample question for each type. The third column shows the correct answer, not necessarily that produced by our system. 27 question types are listed here plus 'unknown'. A further 52 question types handled by the system were not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DLT System Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Summary of Enhancements</head><p>Three main changes were made this year. Firstly, new query types were added together with a method for recognising them. This necessitated the development of some new NEs. Secondly, the question analysis stage was refined and this led to changes in document retrieval. Thirdly, we indexed the document collection by sentence rather than by paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Types and their Identification</head><p>Last year' s system could recognise 47 question types plus the default type 'unknown' . Instances of 29 types occurred in the test queries. This year, a further 32 types were added, bringing the total up to 79 plus 'unknown' . These types are shown in Table <ref type="table" coords="4,292.77,240.13,4.14,10.57">2</ref>, together with examples of each, taken from the 2004 test questions. It is interesting to observe that while last year 29 out of 47 types in the system were actually used in the runs, this year only 28 out of 79 types were used, 14 of which were new. Perhaps the new style of grouping questions makes them less diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Analysis</head><p>Following type identification the query is subjected to a detailed analysis to assist in the process of search expression formulation. Last year we attempted to identify capitalised sequences and other simple constructs in order to use these as search phrases. Following work on CLEF this year <ref type="bibr" coords="4,468.57,354.49,71.52,10.57;4,72.09,367.45,20.61,10.57" target="#b3">(Sutcliffe et al., 2004</ref>) and a study we undertook concerning the relationship between query terms and effective search terms <ref type="bibr" coords="4,99.38,380.41,121.57,10.57" target="#b5">(White and Sutcliffe, 2004)</ref> we decided adopt the CLEF strategy which uses the following steps:</p><p>• Tag the query for part-of-speech using <ref type="bibr" coords="4,262.89,407.05,56.98,10.57" target="#b6">Xelda (2003)</ref>;</p><p>• Recognise instances of 9 different constructions;</p><p>• Weight these according to their importance;</p><p>• Order them according to weight;</p><p>• Use the conjunction of these as the initial search expression.</p><p>The nine constructions are shown in Table <ref type="table" coords="4,263.25,511.69,5.52,10.57" target="#tab_2">3</ref> together with their weights and an example of each. Weights are assigned using a scheme reminiscent of <ref type="bibr" coords="4,265.65,524.65,91.99,10.57" target="#b4">Magnini et al. (2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Search Expression Formulation</head><p>Based on the results of query analysis a search expression is composed. All searches are boolean. Constructs as identified in the previous stage are ordered by increasing score and then joined with AND operators to make a single boolean query. This is then used as the starting point of a search for documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Document Retrieval</head><p>Last year, we indexed Aquaint by &lt;p&gt; tag, i.e. treating each paragraph as a separate document. The use of &lt;p&gt; tags is uneven in the collection so we decided instead to split the entire collection into separate sentences and then to index by these instead. Thus each sentence was considered a separate document. This effectively ensures that all search terms must occur in close proximity to each other. Of course, it also means that where a concept is mentioned explicitly and then referred to anaphorically in a local context containing other search terms (and indeed a candidate answer) that such a sentence will not be retrieved and hence the correct answer may be lost. However the <ref type="bibr" coords="5,385.17,300.73,125.96,10.57" target="#b5">White and Sutcliffe (2004)</ref> study investigated this point among a number of other related issues for a sample set of TREC queries, and found that it is too infrequent to have a significant impact on our existing system.</p><p>During retrieval a boolean query as formulated in the previous stage is submitted to the search engine and the first n matching documents (i.e. sentences) are returned. n was set to 30 for Run 1 and 100 for Runs 2 and 3. (Results reported here are Run 2.) If no documents are returned in the search, the least significant term (i.e. the first) is removed and the search is repeated. This process continues until at least one document is returned or no terms remain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Named Entity Recognition</head><p>NE recognition is similar to last year and uses our own module which is based on grammars together with some exhaustive lists. Some new NEs were added this year for basic entities such as museums. Following <ref type="bibr" coords="5,121.17,479.89,87.07,10.57" target="#b0">Clarke et al. (2003)</ref>, queries of unknown type are answered by searching for general names. We attempted to restrict these names relative to last year by disallowing instances of places, person names, mountains, mountain ranges, bodies of water and companies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Answer Selection</head><p>In order to decide which NE candidate (or candidates in the case of list questions) should be returned, two strategies for answer selection were used. The first is highest_scoring, where we return the NE occurring in a context which matches terms in the query best. The second is highest_google which uses a similar algorithm to <ref type="bibr" coords="5,162.57,594.25,91.99,10.57" target="#b4">Magnini et al. (2002)</ref>.</p><p>During answer selection for factoid questions the candidate with the best score is chosen, this being either highest_scoring or highest_google. For list questions, up to 20 answers which are of the correct NE type(s) and which co-occur with at least one other query term are returned. Definition answers are not subjected to any selection -the answer is the concatenation of all sentences matching a definition pattern, however many there happen to be.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Runs and Results</head><p>Three runs were submitted. The first and second used highest_scoring for answer selection while the third used highest_google. Run 1 used n=30 for the (maximum) number of documents analysed while Runs 2 and 3 used n=100. Run 2 was the best and the results for it are shown in Table <ref type="table" coords="6,469.53,613.69,5.52,10.57" target="#tab_3">4</ref> for queries of factoid type only. The first two columns show the numbers of queries which were classified correctly (C) and incorrectly (NC) broken down by query type. A significant number of factoid queries i.e. 43 out of 230 or 19% are in fact unclassifiable by the system because no appropriate category exists for them. If the system assigns 'unknown' to a query appropriately it is technically correct behaviour but of course is unlikely to result in the correct answer being returned. Considering all such assignments as wrong, therefore, the classifier accuracy is 70%. Considering them right when they are technically correct, the classifier accuracy is 89%. As noted above, no fewer than 52 out of the system's 79 query types did not</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,166.05,94.94,60.85,9.53"><head></head><label></label><figDesc>abbrev_expand</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,108.09,93.97,396.03,208.26"><head>Table 1 : Breakdown of Question Groups by Target Types</head><label>1</label><figDesc></figDesc><table coords="2,150.93,93.97,295.13,158.11"><row><cell>organisation</cell><cell>26</cell><cell>Rat Pack</cell></row><row><cell>person</cell><cell>25</cell><cell>James Dean</cell></row><row><cell>man_made_object</cell><cell>5</cell><cell>USS Constitution</cell></row><row><cell>animal</cell><cell>2</cell><cell>agouti</cell></row><row><cell>substance</cell><cell>2</cell><cell>prions</cell></row><row><cell>art_work</cell><cell>1</cell><cell>Tale of Genji</cell></row><row><cell>medical_condition</cell><cell>1</cell><cell>cataract</cell></row><row><cell>natural phenomenon</cell><cell>1</cell><cell>Hale Bopp comet</cell></row><row><cell>political_agreement</cell><cell>1</cell><cell>Good Friday Agreement</cell></row><row><cell>scandal</cell><cell>1</cell><cell>Teapot Dome scandal</cell></row><row><cell>Total</cell><cell>65</cell><cell></cell></row></table><note coords="2,360.20,269.02,143.83,10.29;2,108.09,280.94,396.03,9.53;2,108.09,292.70,205.23,9.53"><p>. This shows an analysis of the 65 targets in TREC 2004 into a representative set of types. As can be seen, the vast majority of targets are organisations (interpreted broadly) and persons.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,108.09,78.34,396.03,168.09"><head>Table 3 :</head><label>3</label><figDesc>Construct Types used in Query Analysis. The second column is the integer weight assigned to the construct and the third shows a sample phrase for the type. All come from this year's queries except those marked with * which did not occur and are thus taken from last year.</figDesc><table coords="5,161.37,78.34,283.40,119.01"><row><cell>Construct</cell><cell>Weight</cell><cell>Example</cell></row><row><cell>quote</cell><cell>80</cell><cell>"Cold Mountain" *</cell></row><row><cell>all_cap_wd</cell><cell>60</cell><cell>AARP</cell></row><row><cell>cap_dot_wd</cell><cell>1</cell><cell>U.S. *</cell></row><row><cell>cap_nou_prep_det_seq</cell><cell>40</cell><cell>President of the United States</cell></row><row><cell>cap_wd_seq</cell><cell>40</cell><cell>Black Panthers</cell></row><row><cell>number</cell><cell>20</cell><cell>first</cell></row><row><cell>low_adj_low_nou</cell><cell>40</cell><cell>annual revenue</cell></row><row><cell>non_cap_nou_seq</cell><cell>40</cell><cell>cataract surgery</cell></row><row><cell>wd</cell><cell>20</cell><cell>year</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,108.09,511.90,396.03,33.21"><head>Table 4 : Results by Query Type for Run 2.</head><label>4</label><figDesc>The columns C and NC show the numbers of queries of a particular type which were classified correctly and not correctly. Those classified correctly are then broken down into Right, ineXact, Unsupported and Wrong.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusions</head><p>Our performance is still poor but doubled since last year. This can be attributed mainly to indexing the document collection by sentence rather than by paragraph, and identifying appropriate search phrases within the query prior to document retrieval. In addition, many extra query types were added to the system and there were a few small refinements such as the tightening of the definition of a general name.</p><p>The main weakness of the system is the search component. Next steps should include improvements to the term expansion and query relaxation strategies together with detailed evaluation experiments concerning these. In addition we should take greater advantage of the target field which effectively states the topic of a group of queries. Another area for further work is to reduce the number of unknown queries from their present level of 63 out of 230 i.e. 27%. Some of these could be handled by new question types together with refinements to the query categorisation module. Examples of others which remain very difficult can be seen in Table 5.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>XIE19970328.0083 &lt;P&gt; --The 39 men and women who committed suicide were members of a cult known as Heaven' s Gate, the authorities said. They left behind detailed videotapes describing their intention and appeared to believe that the Hale-Bopp Comet now streaking across the sky was their ticket to heaven. &lt;/P&gt; come up at all. Naturally these have all occurred in previous TRECs, suggesting that there may be fewer query types in the new 'grouped question' task of TREC 2004 than were found in previous years.</p><p>The columns marked R, X, U and W show the numbers of answers judged Right, ineXact, Unsupported and Wrong by the NIST assessors. The overall rate of success was thus 39 out of 230 (17%) or 41 out of 230 (18%) including inexact answers. This is approximately twice the performance of our system last year (8% or 9% including inexact answers). Regarding the list and definition questions, the mean results were F=0.101 and F=0.171 respectively. Last year the scores were 0.034 and 0.133 so this is also a small improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Many thanks to Ken Litkowski for providing answer patterns and supporting documents for the question collection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.09,297.85,468.00,10.57;8,72.09,310.81,468.00,10.57;8,72.09,323.77,468.04,10.57;8,72.09,336.73,468.00,10.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,109.41,310.81,374.20,10.57">Statistical Selection of Exact Answers (MultiText Experiments for TREC 2002)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kemkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Tilker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,245.37,323.86,294.76,10.36;8,72.09,336.82,23.70,10.36">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2003. November 19-22, 2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.09,349.69,419.76,10.57;8,72.09,375.61,173.43,10.57" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Gaithersburg</surname></persName>
		</author>
		<ptr target="www.dtsearch.com" />
	</analytic>
	<monogr>
		<title level="j" coord="8,72.09,375.61,44.76,10.57">DTSearch</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.09,401.53,467.98,10.57;8,72.09,414.49,468.00,10.57;8,72.09,427.54,468.05,10.36;8,72.09,440.41,244.16,10.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,250.41,401.53,289.66,10.57;8,72.09,414.49,116.33,10.57">Comparing Scientific and Journalistic Texts from the Perspective of Extracting Definitions</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,214.53,414.58,325.56,10.36;8,72.09,427.54,37.28,10.36;8,199.05,427.54,341.09,10.36;8,72.09,440.50,180.67,10.36">the 42nd Meeting of the Association for Computational Linguistics, Forum Convention Centre Barcelona 21-26 July</title>
		<imprint>
			<date type="published" when="2004-07-25">2004. 25 July, 2004. 2004</date>
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
	<note>Proceedings of the Workshop on Question Answering in Restricted Domains</note>
</biblStruct>

<biblStruct coords="8,72.09,466.33,468.00,10.57;8,72.09,479.29,467.98,10.57;8,72.09,492.25,339.20,10.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,396.09,466.33,144.00,10.57;8,72.09,479.29,275.59,10.57">Cross-Language French-English Question Answering using the DLT System at CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mulcahy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>O'gorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,371.97,479.38,168.10,10.36;8,72.09,492.34,78.56,10.36">Proceedings of the Cross Language Evaluation Forum</title>
		<meeting>the Cross Language Evaluation Forum<address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09-17">2004. 2004. 16-17 September, 2004</date>
			<biblScope unit="page" from="305" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.09,518.17,468.00,10.57;8,72.09,531.13,467.97,10.57;8,72.09,544.09,300.32,10.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,348.93,518.17,191.16,10.57;8,72.09,531.13,161.92,10.57">Is it the Right Answer? Exploiting Web Redundancy for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,243.45,531.22,296.61,10.36;8,72.09,544.18,115.97,10.36">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-06">2002. July 6-12, 2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.09,570.01,468.00,10.57;8,72.09,582.97,468.00,10.57;8,72.09,596.02,467.95,10.36;8,72.09,608.89,187.64,10.57" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,263.25,570.01,276.84,10.57;8,72.09,582.97,91.85,10.57">Seeking an Upper Bound to Sentence Level Retrieval in in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,189.81,583.06,350.28,10.36;8,72.09,596.02,45.04,10.36;8,212.73,596.02,256.59,10.36">Proceedings of the Workshop IR4QA: Information Retrieval for Question Answering</title>
		<meeting>the Workshop IR4QA: Information Retrieval for Question Answering<address><addrLine>University of Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-25">2004. 29 July 2004. July 25. 2004</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
	<note>the 27th Annual International ACM SIGIR Conference</note>
</biblStruct>

<biblStruct coords="8,72.09,634.81,171.64,10.57" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Xelda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>www.temis-group</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
