<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.26,72.23,418.70,12.58;1,265.98,88.31,63.35,12.58">National University of Singapore at the TREC-13 Question Answering Main Task</title>
				<funder ref="#_mQuvC4B">
					<orgName type="full">Singapore Millennium Foundation Scholarship</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,114.66,115.62,45.54,9.88"><forename type="first">Hang</forename><surname>Cui</surname></persName>
							<email>cuihang@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,182.23,115.62,37.59,9.88"><forename type="first">Keya</forename><surname>Li</surname></persName>
							<email>likeya@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.28,115.62,51.61,9.88"><forename type="first">Renxu</forename><surname>Sun</surname></persName>
							<email>sunrenxu@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.44,115.62,71.16,9.88"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.63,115.62,65.08,9.88"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.26,72.23,418.70,12.58;1,265.98,88.31,63.35,12.58">National University of Singapore at the TREC-13 Question Answering Main Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D739EDF629732B61261886276C522366</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction .</p><p>Our participation at TREC in the past two years <ref type="bibr" coords="1,56.70,238.74,98.02,9.88" target="#b5">(Yang et al., 2002</ref><ref type="bibr" coords="1,169.21,238.74,25.63,9.88" target="#b5">(Yang et al., , 2003) )</ref> has focused on incorporating external knowledge to boost document and passage retrieval performance in event-based open domain question answering (QA). Despite our previous successes, we have identified three weaknesses of our system with respect to this year's task guidelines. First, our system works at the surface level to extract answers, by picking the first occurrence of a string that matches the question target type from the highest ranked passage. As such, our answer extraction relies heavily on the results of passage retrieval and named entity tagging. However, a passage that contains the correct answer may contain other strings of the same target type <ref type="bibr" coords="1,256.11,415.80,27.46,9.88;1,56.70,428.46,52.69,9.88" target="#b4">(Light et al., 2001)</ref>, which means an incorrect string may be extracted. A technique to select the answer string that has the correct relationships with respect to the other words in the question is needed. Second, our definitional QA system utilizes manually constructed definition patterns. While these patterns are precise in selecting definition sentences, they are strict in matching (i.e., slot-byslot matching using regular expressions), failing to match correct sentences with minor variations. Third, this year's guidelines state that factoid and list questions are not independent; instead, they are all related to given topics. Under such a contextual QA scenario, we need to revise our framework to exploit the existing topic-relevant knowledge in answering the questions.</p><p>Accordingly, we focus on the following three features in this year's TREC:</p><p>(1) To provide appropriate evidence for answer extraction, we use grammatical dependency relations among question terms to reinforce answer selection. In contrast to previous work in matching dependency relations, we propose measuring the similarity between relations to rank answer strings.</p><p>* These two authors are listed in the alphabetical order of their last names.</p><p>(2) To obtain higher recall in definition sentence retrieval, we adopt soft matching patterns <ref type="bibr" coords="1,506.88,208.08,31.77,9.88;1,311.82,220.74,48.36,9.88">(Cui et al., 2004a)</ref>. Unlike conventional lexico-syntactic patterns matched by regular expressions (i.e., hardmatching patterns), soft patterns represent each slot as a vector of words and syntactic classes with their distributions, rather than generalizing specific instances. This allows us to probabilistically match test instances against the training data.</p><p>(3) To answer topically related factoid and list questions, we first combine sentences from our definition sentence retrieval module with downloaded definitions from external resources. This sentence base is used to answer factoid and list questions. Although using such a definition sentence base restricts recall in passage retrieval, it improves efficiency and effectiveness in answering common questions about people and organizations.</p><p>This paper is organized as follows: In the next section, we present the overall architecture of our system. In Sections 3, 4 and 5, we give the details of the above three features. In Section 6, we conclude the paper with future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>In Figure <ref type="figure" coords="1,367.14,517.02,4.12,9.88">1</ref>, we illustrate the architecture of our QA system. We have leveraged our prior work in question analysis, document retrieval, query expansion and passage retrieval to build the system. In our comprehensive pre-processing step, we store a named entity profile and a full parse of each article in the TREC corpus. The offline processing greatly accelerates answer extraction.</p><p>Our framework functions as follows: • Target analysis and document retrieval: First, the user submits a topic, e.g., "Aaron Copland", to the system. Lucene<ref type="foot" coords="1,459.48,656.65,3.51,6.32" target="#foot_0">1</ref> is used to index and retrieve the relevant documents. Topics are often coupled with qualifiers, for instance, "skier Alberto Tomba". We rely on the Web to separate the qualifiers from the main topic words, e.g., "Alberto Tomba" in the above example.</p><p>Figure <ref type="figure" coords="2,197.53,304.14,4.12,9.88">1</ref>. The illustration of the TREC QA system architecture Specifically, we calculate the pointwise mutual information (PMI) 2 between each pair of topic terms based on the hits returned by Google when using the topic terms as query. Terms with PMI values beyond a pre-defined threshold are grouped together. To construct a suitable Lucene query, terms in the same group are first connected by "AND", and then different groups are connected by "OR". To handle errors or infrequent expressions in the given topics, we replace our original query by any query suggestion from Google 3 . For instance, our system automatically changes "Harlem Globe Trotters" to "Harlem GlobeTrotters" according to Google's result. From the document retrieval on the NE pre-tagged corpus, we get a set of NE tagged relevant documents related to the given topic. • Passage retrieval and query expansion for factoid and list questions: To answer topically related factoid and list questions, we perform passage retrieval on two sources: the topic's relevant document set and the definition sentence set produced by the definition generation module. In our submissions to this year's TREC, the first and second runs for factoid questions used the whole relevant document set for passage retrieval; in the third run, we experimented with using only definition sentences to find answers to factoid questions.</p><p>We use a simple linear expansion strategy for query expansion. The method picks expansion terms from Google snippets according to the terms' co-occurrences with the question terms in the snippets. The passage retrieval module takes in expanded queries as input, and performs densitybased lexical matching to rank passages, which consist of a window of three sentences. We list the detailed algorithm for passage retrieval in Appendix 1.</p><p>• Answer extraction: We perform rule-based question analysis to assign question target type to each question. Before question typing, we substitute the topics for all pronouns in the questions. For example, the question: "What is their gang color?", for the topic "Crips", is transformed into: "What is Crips' gang color?" This step facilitates dependency relation parsing in later steps. Highly ranked passages are fed into the answer extraction module. Both the question and candidate answer passages are parsed by MiniPar <ref type="bibr" coords="2,311.82,748.42,53.67,9.91" target="#b5">(Lin, 1998)</ref>, a robust parser for grammatical dependency relations. The module ranks all possible strings of the appropriate type by how closely they model relations to other question terms as encountered in training. We will discuss the ranking of answer strings using approximate dependency relation matching in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approximate Dependency Relation Matching for Answer Extraction</head><p>By analyzing a subset of TREC-9 questions, <ref type="bibr" coords="3,56.70,165.88,87.53,9.91" target="#b4">Light et al. (2001)</ref> estimated an upper bound of 70% for the performance of a question answering system under the condition of perfect passage retrieval, named entity detection and question typing. Given the fact that there is always error in syntactic parsing and passage retrieval, the actual performance of answer extraction is worse. The ceiling on performance is created when many named entities of the same type appear close to each other, confusing answer selection. Without any knowledge of syntactic relations between the entities, a system might select the named entity nearest to the question terms. In addition, some questions, such as: "What does AARP stand for?" have no known named entity types to represent the question target. We believe the key to overcoming such linguistic ambiguity is to use deep syntactic analysis on both the question and answer text. To this end, we extract grammatical dependency relations between entities and use approximate matching of such relations in answer evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting Dependency Relation Triples</head><p>Combining dependency relations in question answering is not a new idea. PIQASso <ref type="bibr" coords="3,236.80,468.76,46.65,9.91;3,56.70,481.42,48.14,9.91" target="#b0">(Attardi et al., 2001)</ref> tested usage of syntactic relations generated by Minipar, a free robust dependency parser, in their QA system. However, their system produced low recall on the TREC data set, due to their use of keyword-based document retrieval <ref type="bibr" coords="3,56.70,544.66,97.57,9.91" target="#b3">(Katz and Lin, 2003)</ref>. In contrast, <ref type="bibr" coords="3,221.19,544.66,62.28,9.91;3,56.70,557.32,29.37,9.91" target="#b3">Katz and Lin (2003)</ref> implemented a system to index and match all syntactic relations on the whole corpus. The weakness of existing systems that try to incorporate dependency parsing is that they use exact matching of relations to locate answers. Although such exact indexing and matching of relations result in high precision, they fare poorly in recall due to variations in both lexical tokens and syntactic trees.</p><p>Following the approaches taken by the existing work, we extract all relation path triples generated by the Minipar dependency parser from a given question and a candidate answer sentence. A relation triple is the smallest representation of a dependency path embedded in the parsing tree of a sentence. Each triple consists of two slots and one path of relations between them:</p><formula xml:id="formula_0" coords="3,128.40,772.72,93.35,10.24">&lt;Slot 1 , Path, Slot 2 &gt;</formula><p>where slots are either open-class words, like nouns and verbs, or named entities. A path represents the relation vector, consisting of a series of relations without taking their end nodes extracted from the parsing tree. For example, given the question: "What American revolutionary general turned over West Point to the British?" and answer sentence: "… Benedict Arnold's plot to surrender West Point to the British", we get the following triples<ref type="foot" coords="3,529.80,160.27,3.51,6.32" target="#foot_1">4</ref> :</p><formula xml:id="formula_1" coords="3,311.82,185.19,221.25,49.50">q1) General sub obj West Point q2) West Point mod pcomp-n British s1) Benedict Arnold poss s sobj West Point s2) West Point mod pcomp-n British</formula><p>It is difficult to find identical relation structures between questions and answers. This is seen in the case above, where a correct answer is given but the relation structures differ. Although the triple (s2) matches the triple (q2) from the question, the string "Benedict Arnold" would not be selected as answer according to existing techniques because there is no match for the triple (q1). Approximate matching is needed to evaluate candidate answers. Clearly, we need a similarity measurement to represent how likely the two paths, namely "sub obj" and "poss s sobj", refer to the same relation chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Relation Similarity</head><p>Common dependency relations are used interchangeably. Due to the variations existing in natural language text, the same relation may be phrased differently for questions and answer sentences. For instance, the appositive relation that appears frequently in news texts could correspond to other relations in a question. To obtain similarity measures among paths, we adopt a statistical method to learn the relatedness of relations from training data.</p><p>We accumulate around 1,000 factoid question-answer pairs from the past two years' TREC QA tasks to build our statistical model. We use Minipar to parse all the questions and their correspondent answer sentences. For each question-answer pair, relation paths from the question triples are aligned with those from the answer sentence if their slot fillers are the same after stemming. In order to get relations between answers and other question terms, we substitute a general tag for those question targets in questions and those answer strings in answer sentences. This results in 2,557 relation path pairs for model construction. The relatedness of two relations is measured by their co-occurrences in both question relation paths and answer relation paths. We employ a variation of mutual information to represent relation co-occurrences. Unlike normal mutual information, we account for path length in our calculation. Specifically, we discount the cooccurrence of two relations in long paths. The mutual information is presented as:</p><formula xml:id="formula_2" coords="4,58.68,194.85,219.78,31.75">) (Re ) (Re ) Re , (Re log ) Re , (Re 1 0 1 0 1 0 l f l f l l l l MI A Q × × = ∑ δ α (1)</formula><p>where Rel 0 and Rel 1 are two relations extracted from question paths and answer paths respectively. f Q (Rel) and f A (Rel) represent the number of occurrences of Rel in question paths and answer paths. ) Re , (Re 1 0 l l δ is 1 when the relations Rel 0 and Rel 1 occur in a question path and its corresponding answer path respectively, and 0 otherwise. α is the inverse proportion of the lengths of the question path and the answer path.</p><p>We calculate pairwise similarity for all dependency relations based on this equation. These relation similarities form the basis for calculating relation path similarity in the evaluation of answer strings. Figure <ref type="figure" coords="4,123.81,400.36,5.50,9.91">2</ref> shows an excerpt of the similarity measures between different relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Excerpt of similarity measures between relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating Answer Strings</head><p>To ensure high recall, we feed the top 50 sentences from the passage retrieval module into the answer evaluation module. We consider two issues in selecting the correct answer: the correct named entity type as determined by question typing, and the similarity of paths between candidate answers and question terms in the question and the candidate answer sentence. For questions with an unknown target type, we examine all noun and/or verb phrases in the given sentences. We first align the relation paths anchored by matched question terms from the combine the similarities of all relation paths. We rank the candidate answers by:</p><formula xml:id="formula_3" coords="4,312.66,82.96,139.22,15.12">∑ = Q Ans P P Sim Ans Weight , ( ) ( ( ,*) (</formula><p>question and the answer sentence. We then Recall that a relation path lations along the path in the parsing tree. To measure the similarity of two relation paths, we combine the similarities between their relations. In our submissions, we experimented with two different methods in aligning relations when calculating path similarities.</p><p>First, we treat relations quence of tokens and consider all possible alignments of relations between two paths without actually aligning any relations. We term this total path matching, which is similar to IBM's Model 1 statistical translation model <ref type="bibr" coords="4,442.37,298.36,86.82,9.90" target="#b1">(Brown et al. 1993</ref>).</p><p>In our case, we use simple mutual information to represent their "translation probability". The path similarity is calculated by: ∏∑</p><formula xml:id="formula_4" coords="4,313.56,345.76,223.32,27.99">+ = P P Sim ) , ( ε j i A j Q i A P len Q A Q l l MI P len ) Re , (Re ) ( 1 ( ) (<label>(3)</label></formula><p>In the second configuration, which is called relation triple matching, we count only the similarities of individual relations that have the same slot fillers. In other words, only the relations between adjacent nodes that contain the question terms in the parsing tree are considered in the path similarity calculation. In this case, the alignments of relations are judged by their two end slot fillers. We combine all similarities of matched triples to rank candidate answers:  After ranking e select the highest ranked answer string, which has the appropriate target type and also falls into the verification list, as the final answer.</p><formula xml:id="formula_5" coords="4,313.50,501.69,218.26,29.58">∏ ∈ + = P P Sim ) , ( ε M j A j Q i M N Q A Q l l MI P len ) Re , (Re ) ( 1 ( ) (<label>(4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Results and Discussio</head><p>We submitted three runs for factoid que l employing approximate dependency relation matching in answer extraction. The highest average accuracy of 0.625 was achieved by the configuration that used total path matching. The performance obtained by relation triple matching (average accuracy of 0.600) was close to it. We note that using triple matching to align Further examination reveals that our new method of measuring dependency relation path similarity in answer extraction outperforms our previous system, in which the first occurrence of a named entity with the correct type is returned for questions with a known answer type.</p><p>For all non-NE questions (in which the system is unsure of the question target type), the module picks the most probable noun phrase which is nearest to all question terms in the top ranked passage. These non-NE questions account for 69 out of the total 230 factoid questions according to our question typing module. We used our previous system as the baseline and compared it with the new answer extraction module in the first two runs we submitted. We list the results in Table <ref type="table" coords="5,253.97,573.76,4.12,9.90" target="#tab_2">1</ref>. The table shows that leveraging more syntactic relations boosts the performance of answer string selection, especially where non-NE answers are involved.</p><p>We also analyzed the distribution of this year's factoid questions. We illustrate the distribution of questions according to the number of runs that gave the correct answers in Figure <ref type="figure" coords="5,219.33,674.92,4.12,9.90">3</ref>. We include the number of questions that were answered correctly by NUSCHUA1 in the figure as well. The X axis in Figure <ref type="figure" coords="5,159.18,712.90,5.50,9.90">3</ref> stands for the number of runs with correct answers for the corresponding number of questions (as showed by axis Y) in all submitted runs to TREC. The left end of the X axis represents that no runs gave correct answers to the questions. As illustrated in Figure <ref type="figure" coords="5,475.06,308.14,4.13,9.90">3</ref>, our system does not perform well in answering difficult questions. As illustrated in the figure, it misses all questions that are correctly answered by one or two runs. This shows that although we have improved on our previous system by incorporating more complicated relation matching techniques, the system still has much room for improvement. One serious problem is the lexical gap, i.e., the difference in vocabulary used to express the questions and those used in the passages. Our relation matching is conducted only when some question words are matched in the candidate passages. In future work, we may incorporate approximate matching of question terms in relation matching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Definition Generation for Topics</head><p>To facilitate the answering of topic-related factoid and list questions as well as to provide sentences for answering Other questions, we deem it important to identify precise and complete definition sentences for the given topics. In last year's TREC definitional QA task, top ranked groups utilized a relatively uniform architecture for extracting definition sentences: (1) finding additional information for the topics from external web sites or thesauri; and (2) employing manually constructed definition patterns to identify sentences. Enlightened by our previous experimental results <ref type="bibr" coords="6,150.73,135.23,82.52,9.90">(Cui et al., 2004b)</ref>, we try to improve our previous system by using: (1) existing definitions from specific web sites, rather than generic web search; and (2) machine learned soft matching definition patterns, instead of manually constructed hard matching patterns represented in regular expressions. We combine the use of these two techniques to identify precise definition sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical Ranking of Definition Sentences with External Knowledge</head><p>To ensure recall, for each topic, we construct two data sets as the basis for selecting definition sentences: one based on the TREC corpus and the other from external knowledge. The TREC set is constructed by relevant documents determined by the document retrieval module using the topic as the query. We retrieve up to 800 documents for each topic. These documents are split into sentences. To construct the external knowledge set, we accumulate existing definitions for the topics from six specific web sites and glossaries. The external resources and their coverage of topics are listed in Table <ref type="table" coords="6,126.91,438.16,4.12,9.90" target="#tab_3">2</ref>. The definitions are downloaded through pre-written wrappers for these sources. As Biography.com and S9 are dedicated biographical web sites, we do not search for definitions of organizations and other objects at these two sites. We first perform statistical weighting of sentences on both of the data sets to find the sentences relevant to the given topics. When ranking sentences with corpus word statistics, we employ the centroid-based ranking method, which has been used in other definitional QA systems (e.g., <ref type="bibr" coords="6,337.92,59.33,70.32,9.90" target="#b5">Xu et al., 2003)</ref>. We select a set of centroid words (excluding stop words) which co-occur frequently with the search target in the input sentences. To select centroid words, we use mutual information to measure the centroid weight of a word w as follows:</p><formula xml:id="formula_6" coords="6,312.42,132.50,225.23,22.33">) ( ) 1 ) _ ( log( ) 1 ) ( log( ) 1 ) _ , ( log( ) ( w idf term sch sf w sf term sch w Co w Weight centroid × + + + + = (5)</formula><p>where Co(w, sch_term) denotes the number of sentences where w co-occurs with the search term sch_term, and sf(w) gives the number of sentences containing the word w. We also use the inverse document frequency of w, idf(w)<ref type="foot" coords="6,491.82,207.97,3.51,6.32" target="#foot_2">5</ref> , as a measurement of the global importance of the word. Words whose centroid weights exceed the average plus a standard deviation are selected as centroid words.</p><p>The weighting of centroid words can be improved by using external knowledge. We augment the weight of the centroid words which also appear in the definitions from the external knowledge data set. We form centroid words into a centroid vector, which is then used to rank input sentences by their cosine similarity with the vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Soft Matching Definition Patterns</head><p>By doing statistical ranking, we obtain a list of highly ranked sentences that are potential definition sentences. These sentences are closely relevant to the given topic but may not be necessarily definition sentences. Definition sentences, such as "Gunter Blobel, a molecular biologist …" are often written in a certain style or pattern.</p><p>Definition patterns in most TREC systems are manually constructed, which is labor intensive. These patterns are usually represented and matched using regular expressions. We consider these techniques hard matching because they require definition sentences to match exactly. The use of hard pattern rules fails to capture the variations in vocabulary and syntax that are often exhibited in definitions sentences; the method also cannot recognize definition patterns which are not explicitly found in training. To overcome this problem, we have proposed a probabilistic soft matching technique which computes the degree of match between test sentences and training instances <ref type="bibr" coords="6,359.43,664.96,92.09,9.90">(Cui et al., 2004a)</ref>. Given a set of training instances, a virtual vector representing the soft definition pattern Pa is generated by aligning the training instances according to the positions of &lt;SCH_TERM&gt;:</p><formula xml:id="formula_7" coords="7,56.70,84.28,226.80,18.17">&lt;Slot -w , … Slot -2 , Slot -1 , SCH_TERM , Slot 1 , Slot 2 , … Slot w : Pa&gt;</formula><p>where Slot i contains a vector of tokens with their probabilities of occurrence derived from the training instances.</p><p>The test sentences are first preprocessed in a manner similar to the preprocessing of labeled definition sentences. Using the same window size w, the token fragment S surrounding the &lt;SCH_TERM&gt; is retrieved:</p><formula xml:id="formula_8" coords="7,56.70,229.12,226.88,19.08">&lt;token -w , …, token -2 , token -1 , SCH_TERM, token 1 , token 2 , … token w : S&gt;</formula><p>The degree of match between the test sentence and the generalized definition patterns is measured by the similarity between the vector S and the virtual soft pattern vector Pa, which accounts for similarity of individual slots as well as the sequence of slots. Our soft matching technique is described in detail in <ref type="bibr" coords="7,151.49,338.75,75.67,9.90">Cui et al. (2004a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3</head><p>Manually Constructed Patterns In addition to centroid-based weighting and soft pattern matching, we also use a set of manually constructed definition patterns, which is a subset of patterns we used for last year's TREC definitional QA task. These patterns, mainly consisting of appositives and copulas, are high-precision patterns represented in regular expressions, for instance "&lt;SEARCH_TERM&gt; is DT$ NNP". The purpose of using such hard matching patterns in addition to soft matching patterns is to capture those well-formed definition sentences that are missed due to the imposed cut-off of ranking scores by soft pattern matching and centroid-based weighting.</p><p>Therefore, the system works in stages: it ranks all sentences using centroid-based ranking and soft pattern matching, and takes the top ranked sentences as candidate definition sentences. It then examines those lower ranked sentences which are not included in the candidate definition sentences and adds in those sentences matched by any of the manually constructed patterns. In this way, we boost the recall of definition sentences identified by the sentence extraction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4</head><p>Redundancy Removal and Answer String Extraction As the TREC QA guideline suggests, to answer Other questions, the nuggets that have been covered by those topic-related factoid/list questions are to be removed. Our system performs a two-stage redundancy check when selecting definition sentences into the final answer. Suppose we are to select N sentences for the final answer, the selection process works as follows:</p><p>Here, answer_stc refers to those sentences that have been previously selected as part of the answer for Other questions. Factoid_stc refers to those sentences that produce the answers for those factoid or list questions. We measure the similarity between two sentences using the simple cosine similarity which weights unigrams by their inverse document frequency (IDF). We apply a stricter similarity threshold for sentences used to answer factoid/list questions as the answers to such questions tend to amount for very small portion of the sentences.</p><p>In addition to full definition sentences, we also develop a set of heuristic rules to extract fragments from sentences in order to shorten the final answers. These heuristic rules are adopted from the system we developed last year. For instance, for a definition sentence that contains the appositive of the topic, only the appositive part is extracted. To avoid introducing confusion, all starting topic words of each sentence are also removed. For example, "TB, also known as tuberculosis …" is transformed into "also known as tuberculosis …" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Results</head><p>We submitted three runs for Other questions. The runs differed only in the length of the cut-off criterion applied. The summary of the three runs is listed in Table <ref type="table" coords="7,377.51,592.37,4.13,9.90" target="#tab_5">3</ref>.</p><p>Our second run achieved the highest score in the Other task. Due to the change of the F 5 measure to the F 3 measure, the length of the answers plays a more important role in the evaluations. It is crucial for us to develop a more systematic method for selecting definition answers than the current heuristics that we employ. This year's Other task cannot be considered identical to last year's definitional QA task because it requires us to exclude all nuggets that have been covered by the topic-related factoid/list questions. This makes the evaluation of the Other task more difficult. Based on our observation, the essential aspects about a topic have been covered by the factoid/list questions. For instance, given a query on a singer, questions about standard topics of interest (such as his/her birthday, songs and band) are already posed through specific factoid or list questions. Thus, it is very difficult for a system to determine what "other" information is most important about the topic. We believe that this is the main reason that causes the overall scores of this task to decline. We also experimented with using definition sentences to answer topic-related factoid/list questions. Our third run for factoid question answering illustrates such an idea. In this run, the definition sentence extraction module sent its topranked sentences to the passage retrieval module. The passage retrieval module ranked these definition sentences according to specific factoid/list questions. This approach, while efficient and effective in extracting answers to common questions about persons and organizations, tended to miss peripherally relevant passages. This run achieved an average precision of 0.50, which was lower than the runs that used the whole relevant document for passage retrieval. We conjecture that the cut-off threshold of selecting definition sentences leads to lower recall that affects passage retrieval.</p><p>We also incorporate existing definitions from external web sites and thesauri in answering certain types of list questions. Specifically, we utilize a set of manually constructed wrappers to acquire certain aspects of a person or a corporation. One of the wrappers is for extracting the names of a person's works, including his/her songs, movies, books and plays, which are often listed in a specific format in web sites. In this way, we can obtain a list of names or works directly from these sites. In addition, such lists of works are often presented in a uniform manner: they are often enclosed by quotation marks and consist of several capitalized words. Although these extracted lists may contain noise, false matches can be discarded by validating the list against existing definitions. As such, we have achieved high precision and recall for the eight list questions on people's songs, albums and books, with an average F measure of 0.81 and 0.73 respectively for the two runs. In addition to works, we have also pre-compiled a list of structured patterns for extracting product names of a company and working positions for a person. In future work, we plan to extend our soft matching patterns to accomplish this task to handle variations in news articles.</p><p>In addition, we have found that many fields of simple facts about a person can be extracted directly from existing definitions, such as birth/death date, birthplace and career. We believe that developing such a set of wrappers to mine simple facts would improve both the effectiveness and efficiency of the QA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have reviewed the newly-adopted techniques in our QA system. They include measuring relation path similarity in answer extraction, soft matching patterns for identifying definition sentences, and using definitions about topics to answer topicrelated factoid/list questions. While these techniques have improved our previous QA system, we note that more improvements may be pursued in future work. First, the mismatch of question terms is still a serious problem. It is crucial to devise a framework that can align semantically related words and calculate relation path similarities. Second, a generic method for selecting appropriate text fragments from definition sentences is necessary. The main challenge here is to identify relevant parts of the definition sentence when the match is only partial. Third, the performance gain obtained using definitions to answer common questions about a person or an organization still remains to be explored. More experiments should be conducted to figure out what kind of specific questions can be correctly answered by automatically generated and manually constructed definitions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,56.70,541.74,226.84,13.44;2,56.70,557.94,226.79,9.88;2,56.70,570.60,226.82,9.88;2,56.70,583.26,226.80,9.88;2,56.70,595.86,226.77,9.88;2,56.70,608.52,42.78,9.88;2,117.19,608.52,42.14,9.88;2,177.20,608.52,23.83,9.88;2,218.80,608.52,64.75,9.88;2,56.70,621.18,226.89,9.88;2,56.70,633.84,226.78,9.88;2,56.70,646.50,226.77,9.88;2,56.70,659.10,226.84,9.88;2,56.70,671.76,226.70,9.88;2,66.60,739.15,3.51,6.32;2,133.74,748.06,3.67,9.91;2,120.36,748.06,3.67,9.91;2,139.32,732.40,3.67,9.91;2,128.10,732.40,2.75,9.91;2,114.84,732.40,3.67,9.91;2,125.16,748.36,6.73,9.60;2,113.28,748.36,6.73,9.60;2,131.70,732.70,6.12,9.60;2,119.64,732.70,6.73,9.60;2,107.76,732.70,6.73,9.60;2,74.82,735.88,29.13,13.47;2,66.60,762.55,216.90,9.97;2,56.70,775.02,26.15,9.02"><head>•</head><label></label><figDesc>Definition generation: The relevant document set for the given topic is the basis for generating the definition for that topic. The definition generation module first extracts definition sentences from the document set. It identifies definition sentences using centroid-based weighting and definition pattern matching. It also leverages existing definitions from external resources. We discuss definition sentence extraction in Section 4. After redundancy removal, the module produces the definition for the topic. as when Google returns : "Did you mean: XXX"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,470.52,86.51,3.66,9.90;4,462.42,92.68,6.50,5.40;4,519.84,86.51,12.85,9.90;4,321.72,108.65,216.97,9.90;4,311.82,121.25,9.77,9.90;4,452.16,146.57,86.54,9.90;4,311.82,159.23,8.55,9.90;4,445.67,235.12,93.00,9.90;4,311.82,247.78,9.16,9.90;4,382.26,98.30,10.67,5.23;4,452.76,85.16,3.67,5.23;4,452.94,92.84,9.00,5.23;4,321.59,121.25,217.18,9.90;4,311.82,133.91,126.15,9.90"><head>)</head><label></label><figDesc>one slot being the question target or a candidate answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,311.82,523.12,223.88,153.25"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of two submitted runs</figDesc><table coords="5,311.82,548.11,223.88,128.26"><row><cell></cell><cell>Baseline</cell><cell>NUSCHUA1</cell><cell>NUSCHUA2</cell></row><row><cell>Overall</cell><cell></cell><cell></cell><cell></cell></row><row><cell>average</cell><cell>0.51</cell><cell>0.62</cell><cell>0.60</cell></row><row><cell>accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>For</cell><cell></cell><cell></cell><cell></cell></row><row><cell>questions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>with NE</cell><cell>0.68</cell><cell>0.78</cell><cell>0.75</cell></row><row><cell>typed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>targets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>For</cell><cell></cell><cell></cell><cell></cell></row><row><cell>questions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>without</cell><cell>0.29</cell><cell>0.42</cell><cell>0.41</cell></row><row><cell>NE typed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>targets</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,56.70,514.07,226.93,177.73"><head>Table 2 .</head><label>2</label><figDesc>List of external resources for definitions and their coverage of topics.</figDesc><table coords="6,60.30,538.99,219.65,152.80"><row><cell></cell><cell>Coverage</cell><cell>of</cell></row><row><cell>External Resource Names</cell><cell cols="2">Topics (out of 65</cell></row><row><cell></cell><cell>topics)</cell></row><row><cell>Biography.com (http://www.biography.com/)</cell><cell>19</cell></row><row><cell>S9 (http://s9.com/biography/index.html)</cell><cell>15</cell></row><row><cell>Wikipedia</cell><cell></cell></row><row><cell>(http://en.wikipedia.org/wiki/Main_Page)</cell><cell>63</cell></row><row><cell>Bartleby.com (http://www.bartleby.com/)</cell><cell>37</cell></row><row><cell>Google Glossary (search by "define: &lt;term&gt;"</cell><cell></cell></row><row><cell>in Google)</cell><cell>25</cell></row><row><cell>WordNet Glossary</cell><cell>13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,56.70,185.81,226.85,167.16"><head>Table 3 .</head><label>3</label><figDesc>Summary of submitted runs for Other questions.</figDesc><table coords="8,56.70,210.99,226.79,141.97"><row><cell>Runs</cell><cell cols="2">Answer string extraction applied</cell><cell cols="2">Average length (in bytes)</cell><cell>Final F 3 score</cell></row><row><cell>NUSCHUA1</cell><cell></cell><cell>No</cell><cell>2079</cell><cell>0.448</cell></row><row><cell>NUSCHUA2</cell><cell></cell><cell>Yes</cell><cell>1973</cell><cell>0.460</cell></row><row><cell>NUSCHUA3</cell><cell></cell><cell>Yes</cell><cell>2505</cell><cell>0.379</cell></row><row><cell cols="2">5 Exploiting</cell><cell cols="2">Definitions</cell><cell>to</cell><cell>Answer</cell></row><row><cell cols="4">Factoid/List Questions</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,343.14,763.50,195.47,9.02;1,311.82,775.02,134.40,9.02"><p>http://jakarta.apache.org/lucene/docs/index.html. Lucene performs Boolean search.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,327.96,729.00,210.64,9.02;3,311.82,740.52,226.81,9.02;3,311.82,752.04,226.80,9.02;3,311.82,763.56,226.87,9.02;3,311.82,775.02,20.88,9.02"><p>We list only part of the extracted triples for the sake of space. A path exists between any pair of two open class words or named entities. We also restrict the length of the path to seven relations between the two slots.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="6,327.72,740.52,207.78,9.02;6,311.82,752.04,99.23,9.02;6,311.82,763.56,209.17,9.02;6,311.82,775.02,48.94,9.02"><p>We use the statistics from the Web Term Document Frequency and Rank site (http://elib.cs.berkeley.edu/docfreq/) to approximate words' IDF.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="9,333.42,197.88,45.84,9.88;9,311.82,216.54,215.01,9.88"><p>Appendix Appendix 1. Algorithms for Passage Retrieval</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgement</head><p>The authors are grateful to <rs type="person">Shi-Yong Neo</rs>, <rs type="person">Victor Goh</rs> and <rs type="person">Yee-Fan Tan</rs> for their help with migrating the previous year's subsystems. We also thank <rs type="person">Hui Yang</rs> for sharing her experience in participating in TREC QA. Thanks also go to <rs type="person">Alexia Leong</rs> for proofreading this paper. The first author is supported by the <rs type="funder">Singapore Millennium Foundation Scholarship</rs> (Ref No: <rs type="grantNumber">2003-SMS-0230</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mQuvC4B">
					<idno type="grant-number">2003-SMS-0230</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We employ a density-based passage retrieval algorithm that is reinforced by query expansion. To select expansion terms for a factoid or list question, we submit the question to Google and select the top 10 weighted terms from returned snippets as expansion terms based on the following weighting scheme:</p><p>where IDF(t) denotes the inverse document frequency of term t and |snippets(t)| gives the number of snippets that contain t. As we filter out those snippets that do not contain any target term, the counting of snippets is equivalent to the cooccurrence of term t with the target.</p><p>We take sentences as passages. The reason is that we employ Minipar in final answer extraction and Minipar can only resolve dependency relations within a sentence. We weight a sentence as the combination of three partial scores: word overlap, word density and weights of its adjacent sentences. In other words, a sentence's weight could be augmented by adjacent sentences that have high weight. It is similar to the idea used in SiteQ <ref type="bibr" coords="9,518.48,517.24,20.15,9.90;9,311.82,529.90,60.75,9.90" target="#b3">(Lee et al., 2001)</ref> that rank n-sentence passages. In particular, we weight sentence S i as follows: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,56.70,59.40,51.26,9.88;9,56.70,77.99,226.90,9.90" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Attardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,56.70,90.65,226.80,9.90;9,56.70,103.25,226.82,9.90;9,56.70,115.91,226.82,9.90;9,56.70,128.57,193.18,9.90;9,56.70,153.89,226.83,9.90;9,56.70,166.55,226.78,9.90;9,56.70,179.50,42.81,9.59;9,115.41,179.50,37.28,9.59;9,168.60,179.50,51.98,9.59;9,236.48,179.50,47.06,9.59;9,56.70,191.81,226.89,9.90;9,56.70,204.47,66.95,9.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,217.62,90.94,65.88,9.59;9,56.70,103.54,126.22,9.59;9,182.88,166.84,100.60,9.59;9,56.70,179.50,42.81,9.59;9,115.41,179.50,37.28,9.59;9,168.60,179.50,51.98,9.59;9,236.48,179.50,47.06,9.59;9,56.70,192.10,44.17,9.59">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Formica</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,193.13,103.25,90.40,9.90;9,56.70,115.91,97.18,9.90">Proceedings of text Retrieval Conference</title>
		<meeting>text Retrieval Conference<address><addrLine>Gaithersburg(MD)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">November 13-16, 2001. 1993. 1993</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
	<note>NIST</note>
</biblStruct>

<biblStruct coords="9,56.70,229.78,226.83,9.90;9,56.70,242.38,226.79,9.90;9,56.70,255.04,226.89,9.90;9,56.70,267.70,226.79,9.90;9,56.70,280.36,209.30,9.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,87.66,242.68,195.83,9.59;9,56.70,255.34,137.32,9.59">Unsupervised learning of soft patterns for definitional question answering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,201.60,255.04,81.99,9.90;9,56.70,267.70,187.42,9.90">Proceedings of the Thirteenth World Wide Web Conference</title>
		<meeting>the Thirteenth World Wide Web Conference<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2004-05-17">2004. 2004. May 17-22, 2004</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,56.70,305.62,226.82,9.90;9,56.70,318.28,226.80,9.90;9,56.70,331.30,226.90,9.59;9,56.70,343.60,226.74,9.90;9,56.70,356.26,226.83,9.90;9,56.70,368.92,24.76,9.90;9,56.70,394.18,226.93,9.90;9,56.70,407.20,226.87,9.59;9,56.70,419.50,226.81,9.90;9,56.70,432.16,226.78,9.90;9,56.70,444.76,12.83,9.90;9,88.38,444.76,39.74,9.90;9,146.96,444.76,51.05,9.90;9,216.86,444.76,23.17,9.90;9,258.87,444.76,24.72,9.90;9,56.70,470.08,226.85,9.90;9,56.70,482.74,226.83,9.90;9,56.70,495.40,226.81,9.90;9,56.70,508.42,226.84,9.59;9,56.70,521.02,226.89,9.59;9,56.70,533.32,226.84,9.90;9,56.70,545.98,192.54,9.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,121.32,318.58,162.18,9.59;9,56.70,331.30,221.92,9.59;9,56.70,407.20,226.87,9.59;9,56.70,419.80,99.77,9.59;9,255.42,495.70,28.09,9.59;9,56.70,508.42,226.84,9.59;9,56.70,521.02,226.89,9.59;9,56.70,533.62,17.18,9.59">SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-S</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">K</forename><surname>Sheffield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename><forename type="middle">;</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
		<idno>TREC-10</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,56.70,343.60,226.74,9.90;9,56.70,356.26,148.23,9.90;9,164.81,419.50,118.71,9.90;9,56.70,432.16,226.78,9.90;9,56.70,444.76,12.83,9.90;9,88.38,444.76,39.74,9.90;9,146.96,444.76,45.94,9.90;9,97.42,533.32,186.12,9.90;9,56.70,545.98,50.77,9.90">Proceedings of the EACL-2003 Workshop on Natural Language Processing for Question Answering</title>
		<meeting>the EACL-2003 Workshop on Natural Language Processing for Question Answering</meeting>
		<imprint>
			<date type="published" when="2001">2004. 2004. 2003. April 2003. 2001. 2001</date>
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
	<note>Proceedings of the Tenth Text REtrieval Conference</note>
</biblStruct>

<biblStruct coords="9,56.70,571.30,226.88,9.90;9,56.70,583.90,226.80,9.90;9,56.70,596.56,226.83,9.90;9,56.70,609.22,187.00,9.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,131.28,584.20,152.22,9.59;9,56.70,596.86,136.83,9.59">Analysis for elucidating current question answering technology</title>
		<author>
			<persName coords=""><surname>Light</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,201.83,596.56,81.70,9.90;9,56.70,609.22,97.98,9.90">Journal of Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<publisher>Fall-Winter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,61.03,634.54,222.51,9.90;9,56.70,647.20,226.88,9.90;9,56.70,659.80,201.76,9.90;9,56.70,685.12,226.90,9.90;9,56.70,697.78,226.83,9.90;9,56.70,710.44,226.83,9.90;9,56.70,723.04,226.86,9.90;9,56.70,735.70,55.96,9.90;9,56.70,761.02,226.81,9.90;9,56.70,773.68,226.90,9.90;9,311.82,59.32,226.88,9.90;9,311.82,71.98,226.87,9.90;9,311.82,84.64,139.63,9.90;9,311.82,109.90,226.79,9.90;9,311.82,122.92,226.85,9.59;9,311.82,135.22,226.88,9.90;9,311.82,147.88,226.82,9.90;9,311.82,160.54,226.86,9.90;9,311.82,173.14,105.16,9.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,147.84,634.84,135.70,9.59;9,56.70,647.50,54.47,9.59;9,174.19,698.08,109.33,9.59;9,56.70,710.74,108.96,9.59;9,311.82,59.62,189.08,9.59;9,522.12,110.20,16.49,9.59;9,311.82,122.92,226.85,9.59;9,311.82,135.52,176.57,9.59">The Integration of Lexical Knowledge and External Resources for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin ; Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,136.32,647.20,147.26,9.90;9,56.70,659.80,70.28,9.90;9,184.02,710.44,99.51,9.90;9,56.70,723.04,204.72,9.90;9,525.24,59.32,13.46,9.90;9,311.82,71.98,226.87,9.90;9,311.82,84.64,58.48,9.90;9,525.24,135.22,13.46,9.90;9,311.82,147.88,226.82,9.90;9,311.82,160.54,113.34,9.90">the Proceedings of the Eleventh Text REtrieval Conference (TREC&apos;2002)</title>
		<editor>
			<persName><forename type="first">Yang</forename></persName>
		</editor>
		<meeting><address><addrLine>Granada, Spain; Maryland, USA; Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05">1998. May, 1998. 2003. 2003. 2003. 2003. 19-22 Nov 2002</date>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
	<note>the notebook of the 12th Text REtrieval Conference (TREC&apos;2003)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
