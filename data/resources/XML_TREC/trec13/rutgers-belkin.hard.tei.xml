<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,175.86,87.31,299.10,12.19">Rutgers&apos; HARD Track Experiences at TREC 2004</title>
				<funder ref="#_guFuCG3">
					<orgName type="full">Advanced Research and Development Activity (ARDA)&apos;s Advanced Question Answering for Intelligence (AQUAINT) Program</orgName>
				</funder>
				<funder ref="#_H5GrWdK">
					<orgName type="full">Swedish Institute of Computer Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.24,105.06,46.95,9.57"><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
							<email>belkin@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.37,105.06,43.67,9.57"><forename type="first">I</forename><surname>Chaleva</surname></persName>
							<email>chaleva@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.74,105.06,33.50,9.57"><forename type="first">M</forename><surname>Cole</surname></persName>
							<email>mcole@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.72,105.06,32.08,9.57"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.52,105.06,24.70,9.57"><forename type="first">L</forename><surname>Liu</surname></persName>
							<email>luliu@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.29,105.06,39.06,9.57"><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
							<email>yhliu@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.68,117.24,50.08,9.57"><forename type="first">G</forename><surname>Muresan</surname></persName>
							<email>muresan@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.77,117.24,47.69,9.57"><forename type="first">C</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
							<email>csmith@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.87,117.24,25.99,9.57"><forename type="first">Y</forename><surname>Sun</surname></persName>
							<email>ysun@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.44,117.24,43.70,9.57"><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
							<email>xjyuan@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.17,117.24,56.03,9.57"><forename type="first">X.-M</forename><surname>Zhang</surname></persName>
							<email>xzhang]@scils.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,175.86,87.31,299.10,12.19">Rutgers&apos; HARD Track Experiences at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DE5F7BB51C20631696C30ACDA33620A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The goal of our work in the HARD track was to test techniques for using knowledge about various aspects of the information seeker's context to improve IR system performance. We were particularly concerned with such knowledge which could be gained through implicit sources of evidence, rather than explicit questioning of the information seeker. We therefore did not submit any clarification form<ref type="foot" coords="1,182.82,213.31,3.24,5.65" target="#foot_0">1</ref> , preferring to rely on the categories of supplied metadata concerning the user which we believed could, at least in principle, be inferred from user behavior, either in the past or during the current information seeking episode.</p><p>The experimental condition of the HARD track was for each site to submit at least one baseline run for the set of 50 topics, using only the title and (optionally) description fields for query construction. The results of the baseline run(s) were compared with the results from one or more experimental runs, which made use of the supplied searcher metadata, and of a clarification form submitted to the searcher, asking for whatever information each site thought would be useful in improving search results. We used only the supplied metadata, for the reasons stated above, and especially because we were interested in how to make initial queries better, rather than in how to conduct a dialogue with a searcher. There were five categories of searcher metadata for each topic (not all topics had values for all five): Genre, Familiarity, Geography, Granularity and Related text(s), which were intended to represent aspects of the searcher's context which might be useful in tailoring retrieval to the individual, and the individual situation. We made the assumption that at least some of these categories would be available to the IR system prior to (or in conjunction with) the specific search session, either through explicit or implicit evidence. Therefore, for us the HARD track experimental condition was designed to test whether knowledge of these contextual characteristics, and our specific ways of using that knowledge, would result in better retrieval performance than a good IR system without such knowledge.</p><p>We understood that there would be, in general, two ways in which to take account of the metadata. One would be to modify the initial query from the (presumed) searcher, before submitting it for search; the other would be to search with the initial query, and then to modify (i.e. re-rank) the results before showing them to the searcher. We used both, but mainly concentrated on the latter of these techniques in taking account of the different types of metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Hypotheses for How to Take Account of Metadata Categories and Values</p><p>Our general approach was to generate hypotheses about how to take account of each of the categories of metadata information in order to improve retrieval effectiveness, to operationalize them in the TREC HARD setting, to test each hypothesis individually on the training corpus, and then to combine the best-performing ones from each category to generate a final result list for the test corpus. Below are summarized the various improvement methods used based on metadata: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Familiarity</head><p>With respect to familiarity, we had two basic ideas. The first is that people who are familiar with a topic will want to see documents which are detailed and terminologically specific, and people who are unfamiliar with a topic will want to see general and relatively simple documents. This we operationalized in two ways. One was that readability, as measured by the Flesch Reading Ease Score 2 , would approximate terminological specificity or generality, with documents with low readability being suitable for people with high familiarity with a topic, and documents with high readability being suitable for people with low familiarity with a topic. The second operationalization was to use one factor in the Flesch Reading Ease Score, the mean number of syllables per word in a document, as an indication of terminological specificity. There is good evidence that the number of syllables in a word correlates negatively very highly with frequency of the word in English. We took this as a measure of terminological specificity.</p><p>The second idea concerning familiarity is based on research indicating differences in the processing of concrete and abstract words in texts <ref type="bibr" coords="2,124.54,113.43,100.60,8.74" target="#b0">(Audet &amp; Burgess, 1999;</ref><ref type="bibr" coords="2,227.64,113.43,147.16,8.74" target="#b1">Barsalou &amp; Wierner-Hastings, 2004;</ref><ref type="bibr" coords="2,377.30,113.43,133.10,8.74" target="#b2">Burgess, Livesay, &amp; Lund, 1998;</ref><ref type="bibr" coords="2,512.83,113.43,64.52,8.74;2,65.94,124.47,44.68,8.74" target="#b10">Schwanenflugel et al, 1988;</ref><ref type="bibr" coords="2,113.10,124.47,90.98,8.74" target="#b9">Schwanenflugel, 1991)</ref>. One research finding is that people are more easily able to provide distinct contexts for concrete words as compared to abstract words and that comprehension of concrete words takes place more quickly. This led us to hypothesize that people who are unfamiliar with a topic will have difficulty understanding documents that treat a topic abstractly, and will prefer documents that treat the topic with concrete terminology, hence, people with low familiarity with a topic will prefer documents which have a high proportion of concrete terms. Similarly, we hypothesize that people with high familiarity with a topic will prefer documents that have a high proportion of abstract terms.</p><p>We used Martindale's Regressive Image Dictionary (RID) for our model of concrete and abstract expression <ref type="bibr" coords="2,502.81,196.95,73.73,8.74" target="#b7">(Martindale, 1990)</ref>.</p><p>The RID is a taxonomy of words and word stems arranged in accordance with a psychological theory of consciousness and expression. The theory entails a definition of states of consciousness that lie along a continuum, from cognition as regressive, analogical, and concrete (Primary) to cognition as analytical, logical, and abstract (Secondary). For our purposes, we utilized only the sub-segments of the RID related to expression characterized specifically as concrete and abstract. The concrete terms are in the part of the dictionary related to "deep regression," and specifically connote spatial references, such as at, where, over, out, and long). Abstraction is in the part of the dictionary termed "secondary process," which is "an inverse indicator of regression"; the list includes terms such as know, may, thought, and why <ref type="bibr" coords="2,306.42,274.47,73.78,8.74" target="#b6">(Martindale, 1975)</ref>.</p><p>Our application of the RID was relatively straightforward. Texts were analyzed for term frequency using the Concrete/Abstract word lists. The log of the ratio of concrete to abstract words was calculated for each text (ConAbsFreq), as was the log of the inverse ratio (AbsConFreq) and these numbers were normalized across the corpus to give "concreteness scores" and "abstractness scores".</p><p>We thus have three different hypotheses with respect to how to take account of a searcher's familiarity with a topic:</p><p>H1: Assessors with low familiarity with a topic are more likely to find that documents on topic that have high readability are relevant, and those with low readability are not relevant; assessors with high familiarity, vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2:</head><p>Assessors with low familiarity with a topic are more likely to find that documents on topic with a low average number of syllables per word are relevant, and those with a high average number of syllables per word are not relevant; assessors with high familiarity, vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3:</head><p>Assessors with low familiarity with a topic are more likely to find that documents on topic which have a high concreteness score are relevant, and those with a high abstractness score are not relevant; assessors with high familiarity, vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Genre</head><p>For taking account of genre, we had two basic ideas. The first was that news-report documents are objective by nature, and that opinion-editorial documents are subjective by nature. In another research project at Rutgers, HITIQA, a linear regression model had been developed, using a variety of linguistic and formal features of documents, which had proven reasonably accurate in predicting the degree of objectivity or subjectivity of a document <ref type="bibr" coords="2,328.65,512.73,42.09,8.74" target="#b8">(Ng, 2003)</ref>. The features used include: average length of paragraphs in words, number of paragraphs, frequency of possessive pronouns, frequency of plural proper nouns, frequency of comparative adjectives, frequency of model auxiliary, frequency of question marks, frequency of distinct organizations, frequency of distinct person names and frequency of currency.</p><p>We therefore used this model, trained to the HARD-04 training topics and a set of 1000 documents from the New York Times (NYT) sub-collection which we manually classified according to genre, to classify the documents in the retrieved lists according to whether they were objective (for topics with news-report as genre), subjective (for topics with op-ed as genre), or both (for topics with other as genre), in this case classifying what was left as "other". Since the classification is based on a scale, we were able to assign explicit values indicating how well the particular document met each criterion. We also used a Support Vector Machine with linguistic features to accomplish the classification, on the grounds that SVMs usually outperform other learning methods. We saw this as an alternative method of classification, using features similar to those of the regression model.</p><p>Our second basic idea for genre was that different document genres can be identified by their vocabularies. This we operationalized by constructing language models for each genre based on the training topics (all the documents in the training collection which shared the same genre) and on our set of manually classified documents. We also constructed a background language model, which consisted of all the documents retrieved in the training collection plus the 1000 NYT documents that we judged ourselves. Then, for each test topic, we constructed a background model for the topic which was all of the retrieved documents for that topic. These data were used in two ways. One was to determine the Kullback-Leibler distance between the language model for each document in the retrieved list for each topic which specified a particular desired genre, and the relevant genre language model. This procedure resulted in a score for each document, indicating its closeness to that genre. The other way we used the data was to identify terms which occurred most frequently in each genre language model, and also terms which occurred significantly more frequently in the genre language model than was expected, given the background model for the training collection. These terms can then be cumulated, and a single list manually constructed of terms likely to be present in documents of the particular genre. These terms can then added to the baseline queries for the test topics to which they were relevant, and the queries run on the original baseline results lists. This will result in new scores, and a new ranking for each affected list.</p><p>Thus, our hypotheses with respect to genre are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H4:</head><p>The subjectivity or objectivity of a document will determine its membership in a genre; modifying the ranks of documents according to how well they fit to the desired genre will increase performance.</p><p>H5: Classification of documents by subjectivity or objectivity using an SVM procedure will lead to better performance than classification using a linear regression model.</p><p>H6: Documents of a single genre will have language models characteristic of that genre; the Kullback-Leibler (KL) divergence between a document's language model and the relevant genre language model indicates whether that document is a member of the genre; modifying the ranks of documents according to how well they fit the desired genre will increase performance.</p><p>H7: Documents of a single genre will include terms characteristic of that genre; re-ranking lists of documents retrieved according to a topically-based query by running a query containing both the topical terms and the relevant genre terms will increase performance.</p><p>Time constraints allowed us to complete evaluation of only H6, the use of language models to detect genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Geography</head><p>Similarly to the approach taken for genre, we hypothesized that the vocabularies of documents are specific for the geographic area that they refer to. More specifically, US documents should be distinguishable from non-US documents based on their language models (although a significant amount of noise was expected due to documents that covered US and non-US topics, such as world reactions to an event that happened in the US, or trade relations between US and other countries). Our main approach was to build language models based on the training data (positive examples of US documents and positive examples of non-US documents) and to assign "US scores" and "non-US scores" to all documents in the baseline, in order to re-rank the baseline according to how well each document fits the Geography requirement for hard relevance.</p><p>In addition, we manually extended the language models by providing geographic names taken from online atlases: for the US model we added names of US states, their capitals and other important cities, while for the non-US model we added all the names of countries and peoples in the world. Our hypotheses related to geography were:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H8:</head><p>The vocabulary of documents is specific for the geographic area that they refer to. More specifically, US documents can be distinguished from non-US documents.</p><p>H9: Names of US states and important US cities are an indication of US documents; names of other countries are an indication of non-US documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The effect of baseline results quality</head><p>In order to check the effect of the baseline quality on the effectiveness of our methods for improving retrieval hard effectiveness, we used a range of baselines, generated with different IR systems, or with different parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H10:</head><p>A better baseline provides better training data and, therefore, it generates metadata scores that better predict document relevance in terms of metadata.</p><p>Time constraints did not allow us to complete the evaluation of H10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental setting</head><p>We considered two different approaches for using metadata information in order to improve baselines. One was to do query expansion based on training data, and to re-run the query. The other was to assign metadata scores for each metadata type and value ("news-report score", "opinion-editorial score", "US score", …) to each document in the baseline, indicating how well a document fits the model built for that metadata. These metadata scores can then be combined with the baseline scores, using different weights that indicate our levels of confidence in different metadata models, to generate final scores. Although both approaches have advantages and disadvantages, we concentrated on the latter due to its flexibility in changing the level of contribution for each metadata: it is possible to simply change the value of weight cells in a spreadsheet, while the former approach requires a re-run of a search. Also, in the latter approach the order in which different metadata models are considered is irrelevant. Note that some metadata models were built based on training data (hard-relevance judgments) and some, such as abstractness/concreteness or readability, were independent of training data.</p><p>We used two IR tools to implement our approaches. One was the Lemur IR toolkit<ref type="foot" coords="4,396.06,133.39,3.24,5.65" target="#foot_2">3</ref> , which was used (i) to generate baseline result lists; and (ii) to build language models based on the training data for Genre and Geography and generate sets of metadata scores indicating how well each baseline document matched those language models. Based on this approach, we submitted a baseline and two official runs. The second tool was InQuery <ref type="bibr" coords="4,309.48,168.81,77.18,8.74" target="#b3">(Callan et al, 1992)</ref> Release 3.2, for which we submitted one baseline run. However, we were unable to complete InQuery test runs using the metadata in time for submission of official test results. The CMU-Cambridge statistical language model toolkit<ref type="foot" coords="4,320.58,188.77,3.24,5.65" target="#foot_3">4</ref> was used for generating Genre language models for the documents in the Inquery baseline.</p><p>In both cases, we addressed only the document retrieval problem, and did not attempt passage-retrieval. Also, for both Lemur and InQuery runs, the initial query was constructed using both title and description fields of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combination of Evidence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The approach</head><p>Figure <ref type="figure" coords="4,94.56,297.33,5.01,8.74">1</ref> depicts, in principle, our approach to combining metadata evidence with the original baseline scores. Based on our research hypotheses, we build metadata models, which attempt to capture characteristics of documents that satisfy a certain metadata requirement. Some of these models are based on data external to the HARD TREC setting (e.g., readability models, objectivity/subjectivity models, concreteness/abstractness models) or at least not based on official LDC relevance judgments (e.g. Genre models based on our own judgments on a random sample of NYT documents). Other models are based on training data provided by LDC, specifically on positive and negative examples of documents that belong to certain categories of Genre and Geography, as derived from the training relevance judgments.</p><p>We then use the training judgments to evaluate these metadata models in terms of how well they support our research hypotheses. The better a model supports the corresponding hypothesis(es), the higher the confidence level that we attached to that model. Each metadata model contributes, according to its confidence level, to altering the baseline scores and to the generation of the final scores for the HARD TREC runs. Finally, by comparing the baseline run with the metadata-aware runs, based on LDC judgments, we can draw conclusions with regard to our hypotheses.</p><p>Note that confidence levels are relative, rather than absolute, indicating that some metadata models are somewhat more reliable than others. The researchers are expected to try various values for these confidence levels, rather than expect the model evaluation to set exact values. Moreover, using training judgments to evaluate the quality models derived from those some judgments will obviously generate over-inflated confidence levels. While not trusting these evaluation results completely, we can still observe whether the models can capture document characteristics that can predict metadata.</p><p>In order to evaluate a metadata model, we propose a two-stage approach:</p><p>1. Evaluate the quality of the metadata scores. This can be done by applying a t-test (or its non-parametric equivalent if the data is not normally distributed) to verify that documents that are hard-relevant tend to have better scores than documents that are non-relevant. Alternatively, the problem can be viewed as classification into a relevant and a nonrelevant category, so ROC curves can be used to verify how well the metadata scores can predict the metadata category for each document.</p><p>2. Apply the metadata scores to the baseline (as described in the next section) and check whether the new scores provide better ranking. The quality of ranking can be estimated, based on LDC's test judgments, (i) by using trec_eval and looking at various measures of effectiveness, in particular R-precision; (ii) by using a measure somewhat more sensitive to effects on individual topics, such as the difference in the sum of the ranks of the relevant documents in the original list, and in the re-ranked list, or, in order to give more prominence to documents at the top of the result list, the mean reciprocal rank (MRR).</p><p>It is important to separate the two stages of evaluation, in order to distinguish between the quality of metadata scores and the formulae for combining baseline and metadata scores in order to obtain final rankings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The formulae for combining relevance evidence</head><p>Here we describe how we generated "new scores" for each of the baseline ranked lists by combining the evidence provided by the original or "old" score of the baseline run for each document with evidence generated by our operationalizations of the various hypotheses with respect to the metadata. We considered two general methods for combining this evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Simple weighted average</head><p>The new scores are the weighted average between the old score and various sets of metadata scores The weights of the metadata scores reflect the confidence that we have in that metadata to improve the baseline. This is the method that we used to generate our official runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Z-scores based combination</head><p>This method offers a principled way to re-rank the baseline. The assumption is that the metadata scores are normally distributed and that values within one standard deviation from the mean should have little or no influence on the baseline. Values above one standard deviation will have an effect (positive or negative) on the baseline that increases with the value. The formula applied is: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Official Runs</head><p>While successfully preparing the groundwork for such a high number of hypotheses, we were unable to complete the experimental process described in the previous section before the deadline for submitting official runs. In the rest of this paper we concentrate on describing and analyzing our official runs, and describe plans for future work in view of generating and analyzing unofficial runs, which will potentially shed light on the accuracy of our hypotheses.</p><p>We submitted two official runs obtained by combining the Lemur baseline scores with Genre and Geography scores obtained by building language models based on the LDC training judgments, and assigning each document sets of metadata scores indicating how well the document fit the model. We had not managed to compute confidence levels or to implement the planned combination of metadata scores, so we simply applied a weighted average of scores, with a weight of 1.0 assigned to the baseline score (indicating topicality) and a weight of 0.1 for the first run, respectively 0.2 for the second run, to all the metadata scores.  <ref type="table" coords="7,91.24,74.19,5.01,8.74" target="#tab_1">2</ref> depicts our results. As expected, the three performance measures (average precision (AvgPrec), relevance at top 10 documents (Rel10), and R-precision (RPrec)) for the baseline and two experimental runs were not normally distributed. Tests for the significance of mean differences were therefore done using non-parametric methods, specifically Wilcoxon and Kruskall-Wallis.</p><p>For our first experimental run (Run 1), soft relevance performance for was superior for all three. For Run 1, HARD relevance performance was also greater for AvgPrec and Rel10 measures. Our second experimental run resulted in no significant difference in performance relative to baseline, but the performance was consistently below the baseline.</p><p>We analyzed Run 1 HARD relevance performance, looking for differences related to Genre, Geography, and Familiarity metadata. In a comparison of the four types of genre preference, performance did not vary significantly within the metadata group. No significant performance differences were found in a comparison of the two familiarity levels. Geography was the only metadata type for which a significant performance difference was found between specified values (US, non-US, and any) (χ 2 = 8.263, df = 2, p &lt; .05). Visual inspection of box-plots for geography revealed that topics specifying non-US geography appear to have been the major factor differentiating performance among the three geography preferences. Our official runs reflect a very narrow part of our investigation, namely that related to Genre hypothesis H6 and Geography hypothesis H8. The results indicate support for H8 in Run1, i.e. taking Geography into account can potentially improve effectiveness. However, when too much importance is given to metadata compared to topicality, as in Run2, performance actually decreases.</p><p>The purpose of the HARD TREC experiment is to try and improve retrieval effectiveness, as measured by hard relevance, with the expectation that soft (topical) effectiveness will be sacrificed. It is surprising, therefore, to observe that our test run RUN1 shows improvement over the baseline in terms of both soft and hard effectiveness. This could be attributed to poor relevance judgments, but it is more likely due to a correlation between Genre and topicality; for example, some topics may be likely to appear in US documents, while other topics may be more typical for non-US documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of metadata scores</head><p>In order to have a better understanding of these results, we also analyzed the quality of metadata scores, as reflected by evaluation judgments from LDC. The ROC curves below depict the capacity of metadata scores to distinguish between documents judged relevant or non-relevant in terms of their metadata (Genre, Geography, Familiarity). For some of the metadata, the training and testing sets had large imbalances of positive and negative judgments, so the interpretation of the ROC curves for general comment is difficult. For example, in the case of the opinion-editorial genre there were 123 positive assessments and 1185 negative assessments. In general though, if the scores curve is under the LDC judgment line, it means the document scoring has performed poorly and high document scores are not well correlated with the metadata judgments. Such a result should raise serious doubts as to whether that metadata can be reliably detected and so whether reference to that metadata should be included in a new relevance prediction system. However, if a significant portion of the curve is better than the LDC judgment prior, it may indicate that a range of scores is a good signal for the metadata.  It seems that the geography metadata, especially non-US, were well-detected in the Lemur testing runs. The genre metadata is another matter. The "news-report" and "other" scores are anti-correlated at the highest scoring levels, while "opinion-editorial" does not seem to have any effect. A possible explanation is that, as the language models were built based on the training judgments, they captured the 23 training topics rather than the genre of the documents (i.e. the style of the vocabulary). As the test topics are distinct from training topics, not only cannot we expect an improvement in performance, but a decrease in performance will result, as documents with the wrong topic are pushed towards the top of the ranked result list. Discussion and Conclusions With regard to running the HARD TREC experiment, we have learnt that planning and prioritizing are essential. We had an extensive set of research hypotheses based on an extensive literature survey, had an elaborate plan of work for systematically exploring our hypotheses, spent time creating our own Genre annotations for a random sample of 1000 documents, and even used two different IR systems in order to make our investigation more complete. Progressing in parallel on all the fronts proved to be the wrong approach: before the submission deadline we were way beyond the plan and had to submit runs based on guesswork rather than on the planned intermediary results. More specifically, we had no estimation of confidence levels for different metadata scores and simply assigned all metadata the same importance. In retrospect, an iterative approach could have worked better, with the most important hypotheses tested first; if time allows it, the experiment can be extended to test other hypotheses.</p><p>With regard to building models for personalization, we can conclude that, once a searcher's preferences are known based on some implicit sources of evidence (implicit relevance feedback), models built independently of the learning sample seem more reliable than those based on exemplar documents. For example, instead of using language models that may capture the topics of the sample documents offered as exemplars of relevant documents, one should seek document features that can capture the style, rather than the topic of such documents: readability, measures of concreteness/abstractness, etc.</p><p>Using language models to capture Genre preference was a complete failure, presumably because the language models captured the topics of the training documents. Somewhat surprising was the success of language models for Geography metadata; we need to investigate further, in order to understand the difference between Genre and Geography models.</p><p>Flesch readability scores were successful in detecting the simplest documents for people with low familiarity to the topic. We were less successful in providing the right documents for people with high familiarity to the topic. This may be due to the existence of a large number of "bad" documents (in a foreign language, or containing just list of articles, or lists of phone numbers) in the corpus, and our model may not have distinguished between them and valid but complex documents. More work on examining the results and filtering out bad documents may help. Some future work that we plan to do in order to extend this work and better understand our results is describe below:</p><p>• When indexing the document collection, we applied stopword removal. It later occurred to us that the distribution of function words in documents may be an indication of the documents' genre. Therefore, we plan to re-run the part of the experiment that employs language models for generating genre scores, this time without removing stopwords. • The hypotheses that document vocabulary properties will determine membership in a certain genre seem to be promising and we plan to continue our work to determine if genre-specific terms exist and whether genre can be detected by measures of the subjectivity or objectivity of the genre documents. We also plan to run experiments using classifiers other than the simple linear regression model, for example SVMs. Application of language models to characterize terms specific to a genre will also be pursued, with more attention given to the experimental design, so that the training captures the genres rather than the topics of the exemplary documents. • When using Lemur to assign Genre and Geography scores to each document in a baseline, we used smoothing based on the baseline. We plan to repeat that part of the experiment, but to use smoothing based on the entire HARD-04 document collection; this way we hope that the language models will better capture the genre of the documents used as positive example, rather than their topics. • On a related issue we plan to try and build Genre language models based on the random sample of 1000 NYT documents that we judged ourselves, instead of the training documents judged and provided by LDC. The LDC documents were all results of searches based on 23 training queries; using for training a random sample of documents will increase the topical diversity of the sample and will increase the chances that the language models capture genre rather than topicality.</p><p>Finally, we suggest that the quality of the test collection be improved for future similar experiments. The collection contained a high number of bad documents, which should have been filtered out, the quality of the training judgments was rather poor (only two documents were rejected based on familiarity mismatch, only two positive examples of "opinion-editorial" documents were offered, etc.), and some of the relevance judgments used in evaluation were clearly questionable (some documents were rejected on grounds of genre mismatch, even if the request was for "any").</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,261.54,74.31,127.86,8.74;5,165.36,89.10,320.16,459.72"><head>Figure</head><label></label><figDesc>Figure 1. Combining evidence</figDesc><graphic coords="5,165.36,89.10,320.16,459.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,309.96,170.62,3.00,10.48;6,296.10,170.62,6.01,10.48;6,281.10,170.62,6.01,10.48;6,226.38,170.62,6.01,10.48;6,160.74,170.62,6.01,10.48;6,303.78,170.62,5.34,10.48;6,289.32,170.62,4.67,10.48;6,272.28,170.62,6.01,10.48;6,234.84,170.62,26.21,10.48;6,207.84,170.62,15.37,10.48;6,169.20,170.62,26.21,10.48;6,139.74,170.62,19.18,10.48;6,83.94,191.73,27.24,8.74;6,83.94,208.77,327.60,8.74;6,265.80,233.22,2.63,9.17;6,215.88,241.80,5.26,9.17;6,217.20,226.68,21.88,9.17;6,158.58,226.68,23.92,9.17;6,201.90,241.80,5.26,9.17;6,241.14,226.68,22.67,9.17;6,183.90,226.68,22.67,9.17;6,139.26,233.22,5.26,9.17;6,272.76,235.77,197.20,8.74;6,83.94,259.05,146.34,8.74;6,247.50,283.95,2.62,9.13;6,198.00,277.41,5.24,9.13;6,230.34,273.76,5.74,12.82;6,147.18,280.30,5.74,12.82;6,205.26,277.41,22.53,9.13;6,156.42,277.41,38.96,9.13;6,139.92,283.95,4.07,9.13;6,253.98,286.41,72.48,8.74;6,453.24,285.21,2.43,8.49;6,423.18,277.89,4.87,8.49;6,409.74,293.13,4.87,8.49;6,429.96,277.89,20.88,8.49;6,385.20,277.89,36.13,8.49;6,364.38,281.81,5.34,11.92;6,355.80,281.26,5.61,12.60;6,101.94,322.71,114.50,8.74;6,387.54,322.34,2.62,9.15;6,377.10,314.42,3.49,9.15;6,330.78,314.42,5.25,9.15;6,285.72,314.42,3.49,9.15;6,381.72,312.87,2.95,5.15;6,327.48,330.86,5.25,9.15;6,337.98,314.42,22.30,9.15;6,289.68,314.42,38.55,9.15;6,362.76,310.75,5.75,12.84;6,257.04,318.67,5.75,12.84;6,370.62,310.16,6.04,13.57;6,246.60,318.08,6.33,13.57;6,101.94,361.53,105.25,8.74;6,462.72,361.18,2.55,8.89;6,452.58,353.50,3.39,8.89;6,423.42,353.50,5.09,8.89;6,368.28,353.50,3.39,8.89;6,324.54,353.50,5.09,8.89;6,457.02,351.98,2.87,5.00;6,354.36,351.98,2.87,5.00;6,364.50,369.46,5.09,8.89;6,431.04,353.50,21.62,8.89;6,384.12,353.50,37.44,8.89;6,331.50,353.50,21.68,8.89;6,285.24,353.50,37.50,8.89;6,360.66,349.94,5.58,12.47;6,256.74,357.62,5.58,12.47;6,246.54,357.04,6.14,13.18;6,83.94,387.27,498.35,8.74;6,101.94,398.37,476.51,8.74;6,101.94,409.41,259.41,8.74;6,83.94,426.51,468.89,8.74;6,65.94,451.65,5.01,8.74"><head></head><label></label><figDesc>the average score difference between adjacent documents in the baseline: a coefficient that indicates how confident we are in the metadata and implicitly what influence it should have on the baseline; one can start with value 1, and then increase it; k-values corresponding to different metadata can be chosen to reflect the confidence level attached to the corresponding model; -+ or -is determined according to specific characteristics of the metadata values and the desired re-ranking effect 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,230.16,247.35,190.60,8.74"><head>Fig</head><label></label><figDesc>Fig. 2 Performance and geography metadata</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,103.86,74.31,443.19,8.74;8,90.52,96.48,200.49,236.94"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Abstract and concrete word frequencies are poor predictors of documents selected for familiarity.</figDesc><graphic coords="8,90.52,96.48,200.49,236.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,65.94,430.35,511.62,8.74;8,65.94,441.45,285.89,8.74;8,90.58,468.54,200.49,232.44"><head>Figure 4</head><label>4</label><figDesc>Figure 4 The highest readability scores have some correlation with documents relevant for readers with low familiarity to the topic and are neutral for readers expressing high familiarity.</figDesc><graphic coords="8,90.58,468.54,200.49,232.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,178.92,87.57,293.08,8.74;9,92.64,110.16,201.21,233.64"><head>Figure 5 Figure 6</head><label>56</label><figDesc>Figure 5 Language model prediction of geography may be promising</figDesc><graphic coords="9,92.64,110.16,201.21,233.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,65.94,508.35,506.14,55.35"><head>Table 1 . Methods for improving retrieval effectiveness based on metadata.</head><label>1</label><figDesc></figDesc><table coords="1,78.12,532.71,493.96,31.00"><row><cell></cell><cell></cell><cell>Genre</cell><cell></cell><cell></cell><cell></cell><cell>Familiarity</cell><cell></cell><cell>Geography</cell></row><row><cell>Regression</cell><cell>KL</cell><cell>Query</cell><cell>Flesch</cell><cell>Language</cell><cell>Readability</cell><cell>Syllables</cell><cell>Abstract /</cell><cell>Language</cell></row><row><cell>model</cell><cell>distance</cell><cell>expansion</cell><cell>scores</cell><cell>models</cell><cell></cell><cell>per word</cell><cell>concrete words</cell><cell>models</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,109.20,580.41,424.71,111.04"><head>Table 2 . Effectiveness of baseline and test runs(* p&lt; .05, ** p &lt; .01)</head><label>2</label><figDesc></figDesc><table coords="6,109.20,604.77,424.71,86.68"><row><cell></cell><cell></cell><cell>HARD</cell><cell></cell><cell></cell><cell>SOFT</cell><cell></cell></row><row><cell></cell><cell>Avg. Prec.</cell><cell>Rel10</cell><cell>R-Prec.</cell><cell>Avg. Prec.</cell><cell>Rel10</cell><cell>R-Prec.</cell></row><row><cell>Baseline</cell><cell>0.1697</cell><cell>2.356</cell><cell>0.1957</cell><cell>0.1690</cell><cell>2.867</cell><cell>0.1957</cell></row><row><cell>(SD)</cell><cell>(0.1999)</cell><cell>(2.789)</cell><cell>(0.2133)</cell><cell>(0.1841)</cell><cell>(3.020)</cell><cell>(0.1809)</cell></row><row><cell>Run 1</cell><cell>0.1845**</cell><cell>2.644*</cell><cell>0.1867</cell><cell>0.1788**</cell><cell>3.133*</cell><cell>0.2103**</cell></row><row><cell>(SD)</cell><cell>(0.2065)</cell><cell>(3.053)</cell><cell>(0.2196)</cell><cell>(0.1861)</cell><cell>(3.209)</cell><cell>(0.1888)</cell></row><row><cell>Run 2</cell><cell>0.1624</cell><cell>2.333</cell><cell>0.1672</cell><cell>0.1608</cell><cell>2.911</cell><cell>0.1911</cell></row><row><cell>(SD)</cell><cell>(0.1983)</cell><cell>(2.852)</cell><cell>(0.2080)</cell><cell>(0.1746)</cell><cell>(3.029)</cell><cell>(0.1666)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,65.94,74.19,22.82,8.74"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,71.70,688.95,397.34,8.74"><p>See Allan, this volume, for detailed information about the goals and conditions of the HARD track.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,71.70,700.05,326.14,8.74;1,65.94,711.09,366.96,8.74"><p>Computed using algorithms implemented in Perl by Kim Ryan (Available online: http://aspn.activestate.com/ASPN/CodeDoc/Lingua-EN-Fathom/Fathom.html#SYNOPSIS)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,71.70,700.05,135.54,8.74"><p>http://www-2.cs.cmu.edu/~lemur/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,71.70,711.09,175.07,8.74"><p>http://mi.eng.cam.ac.uk/~prc14/toolkit.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Jussi Karlgren</rs>, of the <rs type="funder">Swedish Institute of Computer Science</rs>, collaborated with us in investigating determination of genre. <rs type="person">David Harper</rs>, of the <rs type="institution">Robert Gordon University, Aberdeen, Scotland</rs>, helped us with useful suggestions, Perl scripts, and many hours of fruitful discussions. The <rs type="projectName">HITIQA</rs> project is supported by the <rs type="funder">Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program</rs> under contract number <rs type="grantNumber">2002-H790400-000</rs> to <rs type="institution">SUNY Albany</rs>, with Rutgers as a subcontractor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_H5GrWdK">
					<orgName type="project" subtype="full">HITIQA</orgName>
				</org>
				<org type="funding" xml:id="_guFuCG3">
					<idno type="grant-number">2002-H790400-000</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to test if our explanation is reasonable, we will build Genre models independent of the training topics and judgments; for example, we can use the random sample of 1000 Documents that whose genre we judged. As those documents were picked at random, a diversity of topics can be expected, so the model should not capture some training topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3</head><p>Using KL divergence to detect genre and re-rank documents</p><p>To test hypothesis H6, we used the CMU-Cambridge statistical LM toolkit <ref type="bibr" coords="10,368.22,138.45,125.92,8.74">(Clarkson.and Rosenfeld, 1997)</ref>. to construct a language model for each genre based on training topics. We also constructed a background language model for all the retrieved documents. Based on these language models, we can use KL divergence to evaluate the difference between them. KL divergence measures the distance between two probability distributions. Smaller KL divergence values indicate greater similarity between the two distributions. For two identical distributions the KL divergence is equal to 0.</p><p>Given two distributions q and r, the KL distribution is calculated <ref type="bibr" coords="10,327.60,199.83,44.83,8.74" target="#b5">(Lee, 2001)</ref>.</p><p>, where y ranges over all words in the vocabulary In our case, q(y) is the distribution for all the retrieved documents and r(y) is the distribution for the relevant genre language model. This basic KL divergence calculation cannot be applied directly to our problem because a given word from the retrieved documents may not exist in the genre model vocabulary. In such a case log r(y) is infinite and the formula breaks down. A skewed KL divergence formula <ref type="bibr" coords="10,194.87,318.33,46.69,8.74" target="#b5">(Lee, 2001)</ref> can be used to avoid this problem. It replaces r with an average of r and q in the original KL divergence formula:</p><p>where y ranges over all words in the vocabulary and α is a constant.</p><p>Using the skewed KL divergence formula, we calculated a new score for the LDC training and re-ranked the documents according to the new score. The effectiveness of the treatment is evaluated as: Effectiveness = (sum of the ranks of hard relevant documents with old scores) -(sum of the ranks of hard relevant documents with new scores) , so a positive result indicates an improvement in document rankings.</p><p>To our surprise, we found the value of the effectiveness measure to be negative. The re-ranking treatment pushed most hard relevant documents to higher ranks, towards the bottom of the ranked list. These results go against our intuition, so we plan more experiments to understand what happened.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,65.94,601.77,506.68,8.74;11,65.94,612.81,494.10,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,202.08,601.77,370.54,8.74;11,65.94,612.81,22.49,8.74">Using a high-dimensional memory model to evaluate the properties of abstract and concrete words</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Audet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,105.69,612.81,307.24,8.74">Proceedings of the 20th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 20th Annual Conference of the Cognitive Science Society<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,65.94,627.87,482.55,8.74;11,65.94,638.97,502.96,8.74;11,65.94,650.01,23.69,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,254.52,627.87,107.24,8.74;11,65.94,638.97,359.94,8.74">Grounding cognition: The role of perception and action in memory, language, and thought</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barsalou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wierner-Hastings</surname></persName>
		</author>
		<editor>D. Pecher and R. Zwaan</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Situating abstract concepts</note>
</biblStruct>

<biblStruct coords="11,65.94,665.13,514.62,8.74;11,65.94,676.17,35.91,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,236.22,665.13,238.90,8.74">Explorations in Context Space: Words, sentences, discourse</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Livesay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,481.66,665.13,79.75,8.74">Discourse Processes</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="211" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,65.94,691.23,506.92,8.74;11,65.94,702.33,235.78,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,338.16,691.23,127.30,8.74">The INQUERY Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">C</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,482.57,691.23,90.29,8.74;11,65.94,702.33,231.31,8.74">Proceedings of the 3rd International Conference on Database and Expert Systems</title>
		<meeting>the 3rd International Conference on Database and Expert Systems</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,74.19,505.03,8.74;12,65.94,85.29,71.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,230.13,74.19,257.80,8.74">Statistical language modeling using the CMU-Cambridge toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,494.03,74.19,76.94,8.74;12,65.94,85.29,46.71,8.74">Proceedings ESCA Eurospeech</title>
		<meeting>ESCA Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,102.33,510.91,8.74;12,65.94,113.43,60.88,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,129.24,102.33,299.31,8.74">On the effectiveness of the skew divergence for statistical language analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,434.97,102.33,141.88,8.74">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,128.43,455.07,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,158.16,128.43,228.70,8.74">Romantic progression: The psychology of literary history</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Martindale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,143.55,381.43,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,158.16,143.55,229.38,8.74">The clockwork muse: The predictability of artistic change</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Martindale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,158.61,517.63,8.74;12,65.94,169.71,483.52,8.74;12,65.94,180.81,306.06,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,529.74,158.61,53.83,8.74;12,65.94,169.71,217.04,8.74">Identification of effective predictive variables for document qualities</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rittman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,300.03,169.71,249.43,8.74;12,65.94,180.81,146.55,8.74">Proceedings of 2003 Annual Meeting of American Society for Information Science and Technology</title>
		<meeting>2003 Annual Meeting of American Society for Information Science and Technology<address><addrLine>Medford, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Information Today, Inc</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,195.87,502.00,8.74;12,65.94,206.91,213.29,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,177.54,195.87,190.48,8.74">Why are abstract concepts hard to understand?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schwanenflugel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,493.03,195.87,74.90,8.74;12,65.94,206.91,57.52,8.74">The psychology of word meaning</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Schwanenflugel</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="223" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,65.94,222.03,502.99,8.74;12,65.94,233.07,214.73,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,299.46,222.03,269.47,8.74;12,65.94,233.07,22.49,8.74">Context availability and lexical decisions for abstract and concrete words</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schwanenflugel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Harnishfeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Stow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,95.46,233.07,127.22,8.74">Journal of Memory &amp; Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="499" to="520" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
