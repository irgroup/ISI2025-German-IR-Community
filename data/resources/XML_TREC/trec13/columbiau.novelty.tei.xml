<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,71.88,72.34,465.55,16.59">Columbia University in the Novelty Track at TREC 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.16,117.92,85.27,11.06"><forename type="first">Barry</forename><surname>Schiffman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University New York</orgName>
								<address>
									<postCode>10027</postCode>
									<region>N.Y</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.04,117.92,117.42,11.06"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University New York</orgName>
								<address>
									<postCode>10027</postCode>
									<region>N.Y</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,71.88,72.34,465.55,16.59">Columbia University in the Novelty Track at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22EB9DBE175376D9D6FB1A5B639389E2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our system for the Novelty Track at TREC 2004 looks beyond sentence boundaries as well as within sentences to identify novel, nonduplicative passages. It tries to identify text spans of two or more sentences that encompass mini-segments of new information. At the same time, we avoid any pairwise comparison of sentences, but rely on the presence of previously unseen terms to provide evidence of novelty. The system is guided by a number of parameters, both weights and thresholds, that are learned automatically with a randomized hill-climbing algorithm. During learning, we varied the target function to produce configurations that emphasize either precision or recall. We also implemented a straightforward vector-space model as a comparison and to test a combined approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The novelty detection problem seeks an automatic way of identifying any new information in a document, or documents, on a given topic. It is a recent area of inquiry in the Natural Language Processing and Information Retrieval communities and has been explored at the last three meetings of the Text Retrieval Conference (TREC), in the Novelty Track.</p><p>After the first Novelty Track in 2002, the National Institute of Standards and Technology (NIST) separated the track into four tasks, two of which combined passage retrieval and novelty filtering and two of which concentrated on novelty filtering, giving participants the choice of whether to do the combined tasks (Tasks 1 and/or 3) or whether to focus on the novelty detection alone (Tasks 2 and/or 4). In the combined tasks, participants have to first choose the sentences that are "relevant" to a given topic from a set of documents, and then make a second pass to remove duplicates. In the pure novelty task, participants are given an ordered set of relevant sentences and must filter them to choose all those with "new" information -that is the information that has not appeared previously in the set <ref type="bibr" coords="1,194.90,616.33,13.33,8.97" target="#b10">[10]</ref>.</p><p>Both the retrieval and filtering tasks are quite difficult in themselves, and it is problematic to join them and force the filtering systems to use the experimental output of the retrieval systems. The noisy input clouds what can be learned about determining novelty. We did only Task 2 of the Novelty Track this year, since it is most closely related to our ongoing research into creating updates or bulletin summaries for an on-line news browsing system.</p><p>Our submission for the Novelty Track, called SumSeg, is based on observations of the data we collected for the development of our update summarizer. We saw that new information sometimes appears in passages that are two or more sentences long, and sometimes only in clauses embedded in a sentence. (Task 4 is similar to Task 2, but it allows the systems to see the novel sentences from the first five documents. Time constraints prevented us from submitting runs for it that would have made use of additional input.)</p><p>In order to recognize novelty in both cases -segments of two or more sentences, and embedded clauses that are only part of a sentence -we avoid direct sentence similarity measures, and consider previously unseen words to be the main evidence of novelty. SumSeg has a number of thresholds for deciding how much novelty is necessary to trigger a novel classification. We implemented a randomized hill climbing algorithm to learn thresholds for how many new words would trigger a novel classification. We also sought to learn different weights for different types of nouns, for example, persons, or locations or common nouns. In addition, we included a mechanism to allow sentences that had few strong content words to continue the classification of the previous sentence. The basic SumSeg system is described in <ref type="bibr" coords="1,527.64,450.01,9.09,8.97" target="#b8">[8]</ref>. Finally, we used two statistics, derived from analysis of the full Aquaint corpus, to eliminate low-content words.</p><p>For TREC 2004, we submitted a total five runs: the first two used learned parameters that aimed at high precision output, and the third at high recall. The fourth run was a straightforward vector-space model, with a cosine similarity metric, used as a baseline, and the fifth was a combination of the high recall run with the vector-space model. Training was done on the 2003 TREC novelty data.</p><p>Over all, we were most interested in trying to improve precision. It seemed from the experiences of the participants at TREC and from our own work that precision was extremely difficult to increase much beyond a random selection of relevant sentences. In the 2003 Novelty Track, the top precision was 0.80 although 66% of the relevant sentences were novel. The median precision among all 45 runs in 2003 was 0.70, and the average 0.635. If we remove the five runs by one participant that were in the 0.2 range, possibly because of some misunderstanding, the median is still only 0.71 and the average 0.687. In our summarization work, we especially value conciseness and our long-term goal would be to find the minimal output of a novelty system.</p><p>The next section will review related work. Section 3 will describe the system, and Section 4 will discuss our experiments. Finally Section ?? will preview our performance in this year's Novelty Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Much of the work in this area has been done for the Novelty Track. A number of groups experimented with matrix-based methods. The group from the University of Maryland and the Center for Computing Sciences there used three techniques that operate on term-sentence matrices, QR decomposition, pivoted QR decomposition: QR algorithm, and singular value decomposition <ref type="bibr" coords="2,174.75,184.94,9.09,8.97" target="#b3">[3]</ref>. The University of Maryland, Baltimore County, worked with clustering algorithms and singular value decomposition in sentence-sentence similarity matrices <ref type="bibr" coords="2,117.79,216.38,9.09,8.97" target="#b6">[6]</ref>.</p><p>Topic words were used to cluster candidate sentences by the information retrieval group at Tsinghua University <ref type="bibr" coords="2,275.82,247.70,13.33,8.97" target="#b14">[14]</ref>. The clusters then restrict the word overlap comparisons to reduce redundancy.</p><p>The Institute of Computing Technology, the Chinese Academy of Sciences, experimented with varying the number of novel sentences by the ordering of the source documents. They also tried maximal marginal relevance, and word overlap, and found that word overlap was the most effective <ref type="bibr" coords="2,263.97,331.46,13.33,8.97" target="#b11">[11]</ref>.</p><p>Meiji University embellished pairwise similarity calculations with co-occurrence data from a background corpus. It restricted the novelty comparisons to a time window for the publication dates and included an idf term in scoring sentences <ref type="bibr" coords="2,82.67,394.21,13.33,8.97" target="#b13">[13]</ref>. The national University of Taiwan also used term expansion to inform sentence similarity measures <ref type="bibr" coords="2,275.84,404.65,13.33,8.97" target="#b12">[12]</ref>. The University of Iowa based its novelty decisions on a count of new named entities and noun phrases in a sentence <ref type="bibr" coords="2,273.82,435.97,9.09,8.97" target="#b5">[5]</ref>.</p><p>An interesting approach at TREC 2002 was done by a group at CMU <ref type="bibr" coords="2,79.87,467.41,14.60,8.97" target="#b2">[2]</ref>, which used WordNet to identify synonyms and a graph-matching algorithm to compute similar structure between sentences.</p><p>Using the TREC 2002 data, Allan <ref type="bibr" coords="2,191.55,509.29,9.59,8.97" target="#b1">[1]</ref> compared a number of sentence-based models ranging in complexity from a count of new words and cosine distance, to a variety of sophisticated models based on KL divergence with different smoothing strategies and a "core mixture model" that considers the distribution of the words in the sentence with the distributions in a topic model and a general English model.</p><p>Our system is closest to the Iowa system since it pays a large amount of attention to a count of new named entities and noun phrases, but we give different weights to different types of named entities. We also calculate the weights of common nouns with respect to their frequency in a large background corpus and in the document set for the current topic, as does Allan's core mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM</head><p>This section will introduce the general outline of the system. The major components will be detailed in the subsections below.</p><p>Our system was tailored to the problem posed in the Task 2 of the TREC Novelty Track. For each of the 50 topics, participants were given a set of sentences that have been judged relevant to the topic and were required to return a new list that contains no sentences that were covered by information seen earlier in the input. The relevant sentences were all drawn from a set of documents, at least 25 for each topic. Some topics had additional documents, some not relevant to the topic, that were included to increase the difficulty of the tasks, but these would have no impact on Task 2. The topics were evenly divided between opinion and events.</p><p>Our chief intuition about the problem is that contextual features are important in classifying the sentences. At first we tried to leave the sentences in their original context. This strategy incurred a considerable amount of additional processing. By limiting the input to the relevant sentences, we found that our results did not deteriorate since we had the sentence indices so the program could determine when two were adjacent or not. In a typical discourse, a segment might be introduced with sentence composed of words that clearly indicate novelty, but the sentences that follow immediately after are likely to use shorthand references, such as pronouns, to realize the entities in the introductory sentence. These subsequent sentences can be hard to compare to sentences from the previous documents if the references are left unresolved.</p><p>An analysis by the TREC organizers at NIST suggests that a system should look at consecutive sentences. They determined that 84% of the relevant sentences in 2003 were immediately adjacent to another relevant sentence and that the average length of a run of relevant sentences was 4.252 <ref type="bibr" coords="2,316.80,402.13,13.33,8.97" target="#b10">[10]</ref>.</p><p>Table <ref type="table" coords="2,343.07,423.13,4.55,8.97" target="#tab_0">1</ref> shows that more than half the novel sentences at TREC 2004 appear in consecutive runs of two or more. This circumstance poses a dilemma. A pairwise comparison of sentences can fail on sentences that continue the discussion of a novel subtopic, without explicit references to the novel entities. Yet it seems to be beyond the state of the art to perform a deep analysis, like anaphora resolution, of all the documents in this task. Our solution was to utilize a surface analysis of the sentences, marking named entities, common nouns and verbs, using a chunker to locate noun phrases and prepositional phrases. After this was done, we scanned the sentences in the document sets, building tables of terms that were previously seen. A sentence with a sufficient number of terms that were previously unseen -or new -was considered novel. The thresholds were learned, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Length of</head><p>At the TREC 2003, the group from the University of Iowa <ref type="bibr" coords="3,53.76,151.10,9.59,8.97" target="#b5">[5]</ref> had the highest-precision submission using just counts of named entities and nouns. We elaborated on this approach, using the named entity recognizer in a way that provides reasonably accurate cross-document coreference, separating classes of named entities and using separate thresholds for each class, people, organizations, locations, unspecified names, common nouns, cash amounts, and verbs. Some sentences that are not rich in such discriminating words continue a discussion of a subtopic from the previous sentence. We looked for these by examining the previous sentences. When we encountered a sentence rich in terms that we could identify as either new or old, we updated the current focus accordingly. Separate thresholds were used to identify shifts to novel and those returning to not novel. In that way, we tried to handle these sentences that did not clearly indicate if they were new or old on their own. For example, if we found a personal pronoun at the beginning of the main thought. we followed the established focus. We use the chunker output here to determine the probable main subjects.</p><p>We used a greedy, hill-climbing algorithm to determine effective values. In all, we have 11 features, either weights for the nominal classes and thresholds for segmentation, creating a potential search space of millions of configurations.</p><p>Our learner starts with a randomly selected set of values.</p><p>It chooses the next weight to update randomly, keeping changes that do not harm the score, discarding those that diminish it. Our evaluation function is the TREC score, i.e., the F-measure combination of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Analysis</head><p>We used the Talent tool from IBM <ref type="bibr" coords="3,199.70,522.25,9.59,8.97">[7]</ref> for sentence boundaries, part-of-speech tagging, word lemmas and named-entity recognition. By concatenating the input documents into a single file, we have Talent perform cross-document coreference. This way we got a single identifier for each named entity. Talent identifies people, organizations and locations, and labels others as "names". The tagged documents were then fed into a finite state transducer that located the phrase boundaries.</p><p>In addition, common nouns are weighted by a score combining the document frequencies from a large background corpus with the document frequencies in the topic set. For the background, corpus, we used all the New York Times articles from 1998, 1999 and 2000 that were in the AQUAINT data. We counted the uninflected lemmas to combine the obvious morphological variations. We use a log scale for the document frequencies to create broad categories. The score is the product of the two values:</p><formula xml:id="formula_0" coords="3,339.24,69.73,194.28,20.90">W = (1 -( 1 log(dfset) ))( 1 (log(int(df background ))) )</formula><p>Thus a strong presence in the current document set would get added value, but not enough to outweigh the second term in the equation above, which would be near 0 for the most common words. In revising the system for this year's TREC evaluation, we added a new feature, promiscuity, which is derived from an analysis of the APW portion of the Aquaint corpus. The goal is the same as our use of document frequencies, but the method for identifying these words is their contextual distribution. The idea is that words occurring in too many different contexts will not be of much use in classifying the sentences.</p><p>The promiscuity values seek to eliminate words that are too vague to count for similarity/dissimilarity judgments. They are constructed by analyzing tables of document cooccurrences and deciding which are closely bound to a large number of other words. The base statistic used is the log likelihood ratio <ref type="bibr" coords="3,380.83,285.38,9.09,8.97" target="#b4">[4]</ref>.</p><formula xml:id="formula_1" coords="3,316.80,301.58,165.60,36.57">λ = maxω∈Ω 0 H(w; k) maxω∈Ω H(ω; k) ,<label>where</label></formula><formula xml:id="formula_2" coords="3,345.24,343.18,170.87,20.00">H(ω; k) = H(p; n, k) = p k (1 -p) n -k " n k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>«</head><p>We took the λ for each pair of words that co-occurred in documents, and then computed the mean and variance of matches for a word, plus a count of the number of significant matchups. The statistics were separated by part of speech, so the results for nouns co-occurring with other nouns was separate from those for nouns with verbs. The values represent a combination of the learning algorithms over the different categories. We used a threshold of 0.55. Here, if the value exceeds that threshold, the word is eliminated from consideration. Table <ref type="table" coords="3,405.91,465.62,4.55,8.97">2</ref> shows some of the rejected nouns, the middle column the verbs and the right column, adjectives. Table <ref type="table" coords="3,345.81,568.30,4.13,8.08">2</ref>: A sampling of nouns, verbs and adjectives that were found to be used in too many contexts to convey much meaning on their own.</p><p>We are not arguing that these words have no content or meaning, but that they are either intrinsically vague or are commonly used in structures that are semantically dominated by another word, like "a type of vehicle". The word type provides information about the object, but vehicle is the word we want.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segmentation</head><p>We made use of the part-of-speech tags and phrase boundaries in the input texts to determine when the focus of the discourse shifts, and thus approximate topical boundaries within a document. The segments in this case were labeled as either novel or not novel. We made no attempt to find or label topical boundaries or differentiate between novel segments. We were only interested in distinguishing between new and old. In examining the sequences of noun phrases in a sentence, we imposed three tests on each sentence. 4. The default is to continue the focus, whether novel or old.</p><p>The idea is to make the easier decisions first. The ordering of the tests was determined experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Machine Learning</head><p>We opted for a hill-climbing approach to find effective parameters for the system. These parameters can be divided into two kinds: the weights on the classes of words, like people or locations, and the thresholds for deciding if enough of the content is novel. These values interact with each other dynamically. The decision on novelty for sentence Si not only depends on the weights for the words it contains, but on the decision made for the previous sentence, Si-1, and possibly further back.</p><p>The learner (see Figure <ref type="figure" coords="4,158.11,605.89,4.08,8.97" target="#fig_1">1</ref>) is similar to neural networks where only one weight is altered at a time, and to genetic algorithms, where changes to the hypothesis are selected at random and evaluated. If the change does not hurt results, it is accepted, otherwise the program backtracks and chooses another weight to update. At first, we required the new configuration to produce a score greater than the previous one before we accepted it. But we altered this to accept configurations that produce scores equal to the previous one. The choice of which weight to update is made at random, in an effort to avoid local minima in the search space, but with  The configurations usually converge well within 100 iterations. We experimented with ways to initialize the starting values. We first tried handpicked values and then uniform weights, but found convergence was usually faster with random starting values.</p><p>In training on the 2003 data, the biggest problem was to find a way to deal with the large percentage of novel sentences. About 65% of the instances are positive, so that a random system achieves a relatively high F-measure by increasing the number of sentences it calls novel -until recall reaches 1.0. At the other extreme, a system that exclusively chose the sentences in the first document would achieve a high recall -more than 90% of the relevant sentences in the first document for each topic were considered novel.</p><p>In the Novelty Track the F-measure was set to give equal weight to precision and recall, but we wanted to be able to coax the learner to give greater weight to either precision or by adjusting the F-measure computation:</p><formula xml:id="formula_3" coords="4,400.44,504.02,69.46,24.62">F = 1 β prec + (1-β)</formula><p>recall β is a number between 0 and 1. The closer it gets to 1, the more the formula favors precision.</p><p>The design was motivated by the need to explore the problem more fully and inform the algorithm for deciding novelty as much as to find optimal parameters for the values. Thus we wanted to be able to record all the steps the learner made through the search space, and to save the intermediate states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Vector-Space Module</head><p>Our vector-space module, which assigned all non-stop-words a value of 1, and used the cosine distance metric to compute similarity. We classified a sentence as similar to another if its cosine score exceeded some threshold, T .  The dots are the performance of all the submissions at TREC. The solid line shows the performance of our baseline unweighted vector-space module with a list of stop words, and the dotted line the same system using part-of-speech tags.</p><formula xml:id="formula_4" coords="5,53.76,318.38,229.37,89.68">Cos(u, v) = u • v u • v and N ovel(si)  true if Cos(si, sj) &lt; T, f orj = 1 . . . i -1 f alse otherwise</formula><p>If a sentence failed to be similar to any of the sentences previously seen, we classified as novel.</p><p>When we set T at .9, we found that we had a precision of .71 and a recall of 0.98, indicating that about 6% of the sentences were quite similar to some preceding sentence (See Figure <ref type="figure" coords="5,82.18,488.66,3.56,8.97" target="#fig_2">2</ref>). After that, each point of precision was very costly in terms of recall. Our experience was mirrored by the participants at TREC 2003.</p><p>In practice, the range of recall was much greater than precision. Judging from the experiences of the participants at TREC and our own exploratory experiments, it is difficult to push precision above 0.80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We decided to use only the 2003 Novelty Track data. NIST changed the source and type of data, and altered both the way the topics were presented and the judgments that were made, compared with the 2002 Novelty Track. While the genre remained news, the source was changed from the last two TREC collections to the AQUAINT collection. In addition, the topics were divided between opinion and event types in 2003. The ordering of the documents was changed so that they were presented in chronological order, instead by relevance to the topic.</p><p>In our initial exploration, where we wanted to see how well our hypothesis might generalize, we divided the data into a training set of 25 topics and a testing set of 25 topics, in such a way to preserve the proportion of 56% events and 44% opinions. Our training topics had a total of 8,090 relevant sentences and 5,490 new sentences, and our testing topics, had 7,467 sentences and 4,736 new ones. The proportions of novel to relevant of 67.8% for the training set and 63.4% for the testing set are close to the combined proportion of 65.7%.</p><p>Before testing, we made several initial runs to observe the learner on the training data only, we made several decisions about the learning procedure and one substantial change to the novelty algorithm.</p><p>With respect to the learner, we decided to use random values for the initial set of weights, instead of handpicked values or some uniform value, and to allow the program to choose these anew for each run. That way we got more insight into the behavior of the evaluation function.</p><p>At first, we set the learning rate at 0.1, but later increased the adjustment to 0.25. We allowed the updated weights to increase or decrease by this amount, wit. The choice of weight to receive the increment or decrement is also made at random. Because the algorithm is greedy, we wanted to dampen the tendency for the program to push a particular weight too fast, falling into local minima. We restricted the choice of the next weight by prohibiting the selection of any weight changing in the last n moves. For the final experiments we set n to 3.</p><p>We began by backtracking from any changed that failed to improve the previous score, but the results were prone to falling into local minima. Later, we altered the policy to accept any change that at least equalled the previous best score. Over all we saw a reduction of only a few points when we applied the configurations learned on the training sets to the testing sets (See Figure <ref type="figure" coords="5,456.02,454.46,3.56,8.97" target="#fig_3">3</ref>). The figure also shows the backtracking that occurs, especially toward the area of convergence. The most immediate problem facing the learner was the large proportion of positive examples. The learner could be set to search for either the best precision or the best recall. Recall searches invariably turned out to be trivial since the system converged on configurations that simply classified a large number of examples as novel. Precision searches were better as they found configurations that achieved precision rates of more than 0.9, but at such low recall to be of little value. We then returned to using the F-measure as an evaluation function, but varying the β weight. With β weights of 0.8 to 0.97, we were able to find configurations that produced results at higher precisions than any of the participants in the 2003 Novelty Track (See Figure <ref type="figure" coords="6,261.46,171.98,3.56,8.97" target="#fig_5">4</ref>).  At this point, we added the vector-space results, computed in parallel, reasoning that different approaches that produced high recall results might combine to achieve higher precision without deterioration in recall. The intersection of these two systems might be considerably better than either of the components.</p><p>Our vector-space module could achieve arbitrary high recall rates, with precision consistently above random. It operated completely on the basis of surface analysis, using only the words in the documents. It however encountered a relatively low ceiling on precision, dropping straight down around 0.78.</p><p>To make the combination work, we needed higher recall scores from the segmentation module. So we began reducing the β values from 0.8 to 0.6 and then to 0.5, but this time were interested in the configurations that were discovered earlier in the learning search, those with moderate precision and recall scores. By 100 iterations, these searches would often converge to a configuration of weights that produced a precision near random, and a recall near perfect, but earlier iterations on the testing sets often produced relatively high recall at precisions above 0.75. By themselves, these were similar to several of the stronger submissions in the Novelty Track.</p><p>But when we combined the two modules by taking intersections of their selection, we saw substantial improvements in results (See Figure <ref type="figure" coords="6,131.11,710.54,4.52,8.97" target="#fig_6">5</ref>).One of the stronger combinations was  to take the intersection of a recall-oriented run of SumSeg after 50 iterations with the vector-space model at a cosine similarity threshold of 0.40. The result achieved 0.80 precision, with 0.54 recall on the unseen test examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fine Tuning</head><p>Since we added the promiscuous words features, we wanted to re-examine the results of learning, and we wanted to retrain on the all the 2003 data, but we felt we could build on the results from the earlier training. We began with several configurations learned in the first round of experimentsthose that held up well on the test examples after training. We added the new feature, and allowed the novelty and focus shift thresholds to change. Since we were only dealing with the list or relevant sentences, we thought these parameters might benefit from change. Along with these, we tried restoring the verb weights, which had been zeroed out in round 1, and trying higher values for common nouns and verbs than before. The reasoning was that with the promiscuous words features, the content of the allowed words would be more reliable.</p><p>Table <ref type="table" coords="6,342.11,537.98,4.55,8.97" target="#tab_5">3</ref> shows the final configuration used for Trec 2004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>Our results are encouraging, especially since the configurations that were oriented toward higher precision, indeed, achieved the best precision scores in the evaluation, with our best precision run about 20% higher in precision than the best of all non-Columbia runs (See Figure <ref type="figure" coords="6,493.44,616.34,3.56,8.97" target="#fig_7">6</ref>.) Meanwhile, our recall-oriented run was one of eight runs that were in a virtual tie for achieving the top f-measure. These eight runs were within 0.01 of one another in the measure.</p><p>Table <ref type="table" coords="6,342.71,668.66,4.55,8.97" target="#tab_6">4</ref> shows the numbers of our performance of our five submissions. Prec1 had an F-score close to the average of 0.577 for all systems, while while Prec2 was 50% ahead of a random selection in accuracy.Both our Combo system and our baseline Cosine were above average in F-measure. emphasis on precision is justified in a number of ways, although the official yardstick was the F-measure.</p><p>First, we approach the problem from the summarization task, where compression of the report is valuable. Table <ref type="table" coords="7,288.08,417.62,4.55,8.97" target="#tab_6">4</ref> shows the lengths of our returns. It is impossible to compare these precisely with other systems, because the averages given by NIST are averages of the scores for each of the 50 sets, and we do not have the breakdown of the numbers by set for any submissions but our own. However, we can estimate the size of the other output by considering average precision and recall as if they were computed over the total number of sentencesin all 50 sets. This computation shows an average length of about 6,500 sentences and a median of 6,981 -out of a total of 8,343 sentences. However, this total includes some amount of header material, not only the headline, but the document ID and other identifiers, the date and some shorthand messages from the wire services to its clients. In addition, a number of the sets had near perfect duplicate articles. We contend there is little value in a system that does no more than weed out some nonnarrative material and very simple cases, even though they might have achieved high F-measures.</p><p>Second, our experience, and the results of other groups, shows that it is much more difficult to achieve high precision than high recall. In all three years of the Novelty Track, precision scores tended to hover in a narrow band just above what one would get by mechanically selecting novel for all sentences. This phenomenon was apparent in 2002 when more than 90% of the relevant sentences were novel, and in 2003 when about 65% of the relevant sentences were novel and in 2004 when only 41% were novel. The graph shows all 54 submission in Task 2 for the Novelty Track, with our five submissions labeled. Our precision-oriented runs were well ahead of all others in precision, while our recall-oriented run was in a large group that reached about 0.5 precision with relatively high recall.</p><p>Finally, the F-measure is problematic in this task, as NIST concedes in its overview <ref type="bibr" coords="7,416.94,333.86,9.09,8.97" target="#b9">[9]</ref>, because the same score can be achieved by vastly different systems. In cases where the targets are relatively few, accuracy and coverage are better matched in difficulty. A comparison with the 2003 results is difficult. In 2004, there were 8,343 relevant sentences, but only 3,454, or 41.3% were judged novel, a sharp drop from the previous year, when 65% of the relevant sentences were judged novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-Id</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We built a system that combines that is capable of being tuned to emphasize either precision or recall, using machine learning to find effective parameters. Although we cannot compare the results to runs submitted by other groups, it seems that we did well. We have already incorporated some of the strategies that worked here into our larger system.</p><p>Our study of the data and our experiments have given us many interesting insights into the problem. A completely naïve approach can produce a competitive score, but the relatively high F score is produced by returning a very large percentage of the sentences. It seems that brevity deserves a premium here. Some measurement of the relative importance of the passages would greatly enhance the utility of the system and we would also like to look at ways to factor in the importance of our selections.</p><p>The input sets in the Novelty Track have changed greatly over the three years. The proportion of novel sentences to relevant sentences has steadily dropped. The first year, more than 90% of the relevant sentences were novel; then 65% and now 41%. There is considerable variation among the sets in any one year, and we are curious about finding a way to automatically categorize sets and adjust the parameters of the classifier. Our precision systems seem to do better on the sets with a lower proportion of novel to relevant, but they are erratic in sets with higher proportions. We have found no clear difference between event and opinion sets, but that may be an additional area of inquiry.</p><p>Finally we would like to find an efficient way of trying more complete reference resolution, as that could make the notion of novel segments stronger.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,316.80,173.38,238.78,8.08;4,316.80,183.82,189.52,8.08"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The learning algorithm uses a randomized hill climbing approach with backtracking</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,53.76,232.18,238.93,8.08;5,53.76,242.62,238.82,8.08;5,53.76,253.06,238.79,8.08;5,53.76,263.50,238.84,8.08;5,53.76,273.94,219.04,8.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The dots are the performance of all the submissions at TREC. The solid line shows the performance of our baseline unweighted vector-space module with a list of stop words, and the dotted line the same system using part-of-speech tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,316.80,671.62,238.79,8.08;5,316.80,682.06,209.68,8.08"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Showing the difference between the training and testing for the segmentation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,53.76,368.74,238.94,8.08;6,53.76,379.18,238.94,8.08;6,53.76,389.62,73.38,8.08"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparing the segmentation module with learned weights against the submissions at the TREC meeting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,316.80,232.18,238.92,8.08;6,316.80,242.62,238.92,8.08;6,316.80,253.06,238.92,8.08;6,316.80,263.50,204.52,8.08"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The chart shows the benefit of combining the learned scores with the vector-space model. The combination is done by taking the intersection of the sentences labeled as novel by both modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,316.80,232.18,239.04,8.08;7,316.80,242.62,238.94,8.08;7,316.80,253.06,238.90,8.08;7,316.80,263.50,238.89,8.08;7,316.80,273.94,238.79,8.08;7,316.80,284.50,151.88,8.08"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6:  The graph shows all 54 submission in Task 2 for the Novelty Track, with our five submissions labeled. Our precision-oriented runs were well ahead of all others in precision, while our recall-oriented run was in a large group that reached about 0.5 precision with relatively high recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,345.96,456.25,180.56,175.81"><head>Table 1 :</head><label>1</label><figDesc>Novelty often comes in bursts</figDesc><table coords="2,414.00,456.25,69.53,155.49"><row><cell></cell><cell>Run Count</cell></row><row><cell>1</cell><cell>1338</cell></row><row><cell>2</cell><cell>421</cell></row><row><cell>3</cell><cell>132</cell></row><row><cell>4</cell><cell>72</cell></row><row><cell>5</cell><cell>43</cell></row><row><cell>6</cell><cell>22</cell></row><row><cell>7</cell><cell>11</cell></row><row><cell>8</cell><cell>2</cell></row><row><cell>9</cell><cell>3</cell></row><row><cell>10</cell><cell>3</cell></row><row><cell>11</cell><cell>2</cell></row><row><cell>12</cell><cell>2</cell></row><row><cell>15</cell><cell>2</cell></row><row><cell>17</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,64.56,148.94,228.33,257.49"><head></head><label></label><figDesc>1. We begin by checking if the sum of the weights of the novel content words (including named entities) exceeds a threshold, T novel . If it does, the sentence is considered novel. If the previous focus was old, this indicates the focus has shifted to a novel segment. 2. If novel words do not exceed T novel , we examine the weight of the already-seen content words against a separate threshold, T old . If they do, the sentence is considered old. If the previous focus was novel, this means the focus has shifted to an old segment. 3. The next test is threefold: (a) If the sum of old content words and novel content words is below a threshold, T keep , we assume the prior focus, novel or old, is kept.</figDesc><table /><note coords="4,79.20,321.01,213.55,8.97;4,96.00,331.45,196.65,8.97;4,96.00,341.89,196.66,8.97;4,96.00,352.33,29.26,8.97;4,80.16,366.13,212.60,8.97;4,96.00,376.57,196.89,8.97;4,96.00,387.01,196.76,9.02;4,96.00,397.45,65.23,8.97"><p><p>(b) If the first noun phrase that is not contained in a prepositional phrase is a third person personal pronoun, we assume the prior focus, novel or old is kept.</p>(c) If none of the tests above are triggered, a second test for old content is applied, and if the value exceeds a secondary threshold, T shif t , a novel focus is shifted to old.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,539.85,710.54,15.94,8.97"><head>Table 3 :</head><label>3</label><figDesc>A comparison of the three SumSeg configurations for the 2004 TREC Novelty Track. Prec1 and Prec2 were two selected for ability to raise precision. In the initial training, from the first round, the fitness function for the learning algorithm favored precision over recall. For recall, the f-measure was emphasized, which tends to favor high recall systems. Note that the novelty threshold, nov, is relatively high in the precision-biased configurations, while the weights on classes of words are relatively low.</figDesc><table coords="6,539.85,710.54,15.94,8.97"><row><cell>Our</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,316.80,386.18,239.06,140.77"><head>Table 4 :</head><label>4</label><figDesc>Comparison of results of Columbia's five runs, compared to a random selection of sentences, and the overall average F-scores by all 55 submissions.</figDesc><table coords="7,339.24,386.18,194.15,88.53"><row><cell></cell><cell cols="4">Precision Recall F-meas Length</cell></row><row><cell>Prec1</cell><cell>0.57</cell><cell>0.58</cell><cell>0.562</cell><cell>3276</cell></row><row><cell>Prec2</cell><cell>0.61</cell><cell>0.45</cell><cell>0.506</cell><cell>2372</cell></row><row><cell>Recall</cell><cell>0.50</cell><cell>0.85</cell><cell>0.617</cell><cell>5603</cell></row><row><cell>Cosine</cell><cell>0.49</cell><cell>0.81</cell><cell>0.599</cell><cell>5537</cell></row><row><cell>Combo</cell><cell>0.53</cell><cell>0.73</cell><cell>0.598</cell><cell>4578</cell></row><row><cell>All Nov</cell><cell>0.41</cell><cell>1.00</cell><cell>0.581</cell><cell>8343</cell></row><row><cell>Average</cell><cell>0.46</cell><cell>0,86</cell><cell>0.577</cell><cell>6500</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,58.25,299.17,96.59,10.76" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,311.05,197.16,8.97;8,72.60,321.49,216.31,8.97;8,72.60,331.93,188.10,8.96;8,72.60,342.37,173.59,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,215.67,311.05,54.08,8.97;8,72.60,321.49,152.62,8.97">Retrieval and novelty detection at the sentence level</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bolivar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,243.37,321.49,45.54,8.96;8,72.60,331.93,188.10,8.96;8,72.60,342.37,146.08,8.96">Proceedings of the ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,360.01,194.98,8.97;8,72.60,370.45,216.31,8.97;8,72.60,381.01,208.83,8.97;8,72.60,391.45,109.70,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,114.69,370.45,174.22,8.97;8,72.60,381.01,77.64,8.97">Information filtering, novelty detection and named-page finding</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,168.52,381.01,112.91,8.96;8,72.60,391.45,81.48,8.96">Proceedings of the 11th Text Retrieval Conference</title>
		<meeting>the 11th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,409.09,200.01,8.97;8,72.60,419.53,204.56,8.97;8,72.60,429.97,72.16,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,72.60,419.53,120.86,8.97">From trec to duc to trec again</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,212.18,419.53,64.98,8.96;8,72.60,429.97,44.06,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,447.61,204.69,8.97;8,72.60,458.17,211.61,8.97;8,72.60,468.61,72.89,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,125.24,447.61,152.04,8.97;8,72.60,458.17,96.38,8.97">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,176.10,458.17,104.30,8.96">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,486.25,220.18,8.97;8,72.60,496.69,195.71,8.97;8,72.60,507.13,216.91,8.97;8,72.60,517.57,152.13,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,207.04,496.69,61.27,8.97;8,72.60,507.13,212.86,8.97">Experiments in novelty, genes and questions at the university of iowa</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eichmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Arens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sehgal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,84.11,517.57,112.51,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,535.33,217.54,8.97;8,72.60,545.77,205.88,8.97;8,72.60,556.21,202.41,8.97;8,72.60,566.65,72.16,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,127.28,556.21,64.24,8.97">Umbc at trec 12</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kallurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Cost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rajavaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanbhag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhatkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ogle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,210.15,556.21,64.86,8.96;8,72.60,566.65,44.06,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,584.29,220.06,8.97;8,72.60,594.73,204.40,8.97;8,72.60,605.17,156.95,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,229.70,584.29,62.95,8.97;8,72.60,594.73,93.86,8.97">Disambiguation of proper names in text</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ravin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wacholder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,184.71,594.73,92.29,8.96;8,72.60,605.17,128.73,8.96">Proceedings of the 17th Annual ACM-SIGIR Conference</title>
		<meeting>the 17th Annual ACM-SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,622.93,212.26,8.97;8,72.60,633.37,216.21,8.97;8,72.60,643.81,199.76,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,215.90,622.93,68.95,8.97;8,72.60,633.37,171.26,8.97">Machine learning and text segmentation in novelty detection</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<idno>CUCS-036-04</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,72.59,661.45,205.75,8.97;8,72.60,671.89,205.50,8.97;8,72.60,682.33,124.42,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,120.44,661.45,157.91,8.97;8,72.60,671.89,19.18,8.97">Draft overview of the trec 2004 novelty track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,110.25,671.89,167.85,8.96;8,72.60,682.33,97.21,8.96">The Thirteenth Text Retrieval Conference (TREC 2004) Noteboo k</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.59,700.09,217.76,8.97;8,72.60,710.53,210.28,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,186.63,700.09,103.72,8.97;8,72.60,710.53,51.19,8.97">Overview of the trec 2003 novelty track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.26,710.53,112.39,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,56.89,185.18,8.97;8,335.64,67.45,212.86,8.97;8,335.64,77.89,202.73,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,440.82,67.45,107.68,8.97;8,335.64,77.89,44.31,8.97">Trec-2003 novelty and web track at ict</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.75,77.89,112.51,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,95.53,191.40,8.97;8,335.64,105.97,205.32,8.97;8,335.64,116.41,206.48,8.97;8,335.64,126.85,111.23,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,437.12,105.97,103.84,8.97;8,335.64,116.41,161.53,8.97">Similarity computation in novelty detection and generif annotation</title>
		<author>
			<persName coords=""><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,515.98,116.41,26.15,8.96;8,335.64,126.85,83.01,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,144.49,215.36,8.97;8,335.64,154.93,181.65,8.97;8,335.64,165.49,72.16,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,395.83,144.49,155.16,8.97;8,335.64,154.93,97.78,8.97">Meiji university web and novelty track experiments at trec 2003</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>University</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.44,154.93,64.86,8.96;8,335.64,165.49,44.06,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,335.63,183.13,218.88,8.97;8,335.64,193.57,216.56,8.97;8,335.64,204.01,140.62,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,335.64,193.57,200.86,8.97">Thuir at trec 2003: Novelty, robust, web and hard</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.64,204.01,112.51,8.96">TREC Notebook Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
