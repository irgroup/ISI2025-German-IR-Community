<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,229.92,84.56,151.98,15.41;1,221.52,100.40,168.80,15.41">JHU/APL at TREC 2004: Robust and Terabyte Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.44,131.26,68.28,11.96"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.75,131.26,66.69,11.96"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.96,131.26,63.62,11.96"><roleName>Scott.Cost}@jhuapl</roleName><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.54,131.26,44.71,11.96"><forename type="first">Scott</forename><surname>Cost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,140.16,181.66,73.47,11.96"><forename type="first">{christine</forename><surname>Piatko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,457.50,181.66,14.34,11.96"><surname>Edu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,229.92,84.56,151.98,15.41;1,221.52,100.40,168.80,15.41">JHU/APL at TREC 2004: Robust and Terabyte Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FEF85527F044D9A931A515A35E21A2FB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The Johns Hopkins University Applied Physics Laboratory (JHU/APL) focused on the Robust and Terabyte Tracks at the 2004 TREC conference.</p><p>For initial ranked retrieval, we continue to use a statistical language model to compute query/document similarity values. Hiemstra and de Vries <ref type="bibr" coords="1,256.75,275.50,12.75,11.96" target="#b2">[3]</ref> describe such a linguistically motivated probabilistic model and explain how it relates to both the Boolean and vector space models. The model has also been cast as a rudimentary Hidden Markov Model <ref type="bibr" coords="1,242.13,300.94,11.90,11.96" target="#b3">[4]</ref>. Although the model does not explicitly incorporate inverse document frequency, it does favor documents that contain more of the rare query terms. The similarity measure can be computed as</p><formula xml:id="formula_0" coords="1,163.42,352.79,241.58,72.93">€ Sim(q,d) = α ⋅ f (t,d) + (1-α) ⋅ f (t,C) ( ) t ∈q ∏ f (t,q )</formula><p>Equation 1. Similarity calculation.</p><p>where α is the probability that a query word is generated by a document-specific model, and (1α) is the probability that it is generated by a generic language model. f(t,C) denotes the mean relative document frequency of term t. We have observed that aggregate performance using this model is fairly insensitive to the precise value of α that is used; however, higher values of alpha tend to result in selecting documents that contain a greater number of the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Track</head><p>In last year's TREC Robust Track, we investigated merging (many) disparate run files, using automated techniques such as SVM classification to create a single, robust run. While we had modest success improving our runs over our baseline, we did not approach the theoretical maximum of an oracle choosing the best run per query.</p><p>We examined about half of the 50 difficult queries from TREC 2003 by hand (similar to an effort done by a larger team <ref type="bibr" coords="1,131.08,601.42,11.70,11.96" target="#b0">[1,</ref><ref type="bibr" coords="1,145.47,601.42,7.65,11.96" target="#b1">2]</ref>). Our findings were much the same.</p><p>We observed that relevant documents for these difficult queries did contain concepts related to each title word. In addition, these title concepts could be found "fairly close together" in the relevant document. For difficult queries, our system sometimes discounted one of the title concepts (it was not found at all in a highly ranked document). Other times it found documents with most of the terms, but widely scattered in the returned documents.</p><p>For example, for a query such as "Hubble Telescope Achievements," both query expansion and term weighting seemed to reduce the importance of the concept of "Achievements." In fact, one of the relevant documents has the phrase "achievements of the US Hubble Space Telescope," but for our system was not always a top scoring document.</p><p>We also noted, even for difficult queries, it seemed that a good fraction of relevant documents did appear top 1000 documents, they were just not ranked highly enough. We thus chose to focus this year on ways to rerank an existing run to try to improve performance. We focused on boosting documents with more title concepts appearing closer together.</p><p>We did not make use of any external resources, such as the Web, which were shown to be quite beneficial in the TREC-2003 Robust Track.</p><p>We reused indices from last year that used various tokenization methods. Summary information for the indices that we used is shown Table <ref type="table" coords="2,230.08,249.58,4.05,11.96" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimal Matching Span</head><p>We opted to try applying the Minimal Matching Weighting of Monz to re-rank, hoping to improve our typical good runs by favoring documents with more query terms appearing closer together.</p><p>Monz applied this scoring method to improve QA performance in his thesis work <ref type="bibr" coords="2,445.76,424.78,11.76,11.96" target="#b4">[5]</ref>, since he had not seen much benefit using fixed length overlapping passages for retrieval to improve QA performance. The Minimal Matching Weighting score is a linear combination of the retrieval system score (in our case, a scaled language model score) with a Minimal Matching Span Score, related to the number of matching terms in the document and the length of the closest span in which they appear together.</p><p>Roughly speaking, the minimal matching span (MMS) of a set of terms is the minimal length of a consecutive set of document terms containing at least one occurrence of each term in the set.</p><p>If there is more than one matching query term in the document, the new minimal span weighting score (MSW) is computed by interpolating between a weighted version of minimal matching span of matching query terms and the normalized language model score (S). If there are q terms in the query and q matching is the set of query terms that appear in the document, MSW is as follows:</p><formula xml:id="formula_1" coords="2,163.68,607.07,284.38,40.62">MSW = λ S + (1 -λ ) (|q matching |/MMS) α (|q matching | / |q|) β Equation 2. Monz Minimal Span Weighting Score</formula><p>Monz empirically determined parameters λ = 0.4 α= 1,8; β = 1 based on TREC-9 data, and we reused these values for most runs (we used λ = 0.5 for a TDN run based on performance on TREC2003 data).</p><p>Prior to submission we estimated our performance examining performance on the previous queries used in the TREC 2003 evaluations. We observed some benefits to reranking using this rescoring Minimal Matching Weighting, using matching title words (or stems) for both title-only and TDN runs for various robust measures, and this was confirmed in our officially submitted runs (see the Robust Runs Performance section below).</p><p>We focused on improving the hardest topics, as suggested in the Robust Track guidelines, since MAP-Hardest is most affected by the most difficult topics. We did not focus on high mean average precision (averaged over all topics) in our base runs and primarily concerned ourselves with improvements of robust measures of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Runs</head><p>We used five baseline runs (not submitted): Ts (title-only, stem index), Tw (title-only, word index), D (desc-only, word index), TDN (title-desc-narr, word index), and TDNf (title-desc-narr using relevance feedback, word index). For each baseline run, we normalized the language model scores of the top 1000 documents retrieved to between 0 and 0.9. We submitted five runs, reranking each of the baseline runs.</p><p>apl04rsTs (Tsr) is the combination of two runs, using the stem index, the (normalized as described above) language model scores, the Title topic field only, and this run reranked with minimal matching span using the Title only. No relevance feedback was applied. apl04rsTw (Twr) is the same as above, using the word index. apl04rsDw (Dr) is the combination of two runs, using the stem index, the (normalized as described above) language model scores, the Title topic field only, and this run reranked with minimal matching span using the Description only. This was our mandatory description-only run. To perform better we should have chosen a subset of "important" words from the description. No relevance feedback was applied. apl04rsTDNw5 (TDNr) is a combination of two runs, one using the words index and the (normalized as described above) language model scores on the Title, Description and Narrative topic fields and this run reranked with minimal matching span using the Title only. No relevance feedback was applied, and for this run only λ was chosen based on the previous TREC data to 0.5. apl04rsTDNfw (TDNfr) is a combination of two runs, one using the words index and the (normalized as described above) language model scores on the Title, Description and Narrative topic fields. Relevance feedback was applied. This run was reranked with minimal matching span using the Title only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Runs Performance</head><p>Overall, we observed modest improvements in most robust measures using the reranking approach. Below are tables of performance of officially submitted runs for various subsets of topics. In each of the tables, grey boxes indicate where the measure improves after using minimal span reranking.</p><p>The tables below generally show improvement for the measures of mean average precision (MAP), precision at 10 (P(10)), and Area (which is described in the Robust Track Overview). The largest and most dramatic increases are in precision at 10. Given that the increases are mainly in precision at 10, it would be interesting to incorporate this approach with one round of relevance feedback. Selecting key description terms (as opposed to all non stop words) would also improve the performance of description-only reranking. We did not make use of our aggressive run combination approach from TREC-2003 to get the best possible baselines for our T and D runs, so those runs remained roughly median. It will be interesting to try our reranking technique on top title-only TREC 2004 submissions to see if the approach still provides any boost. Our TDN runs (see Table <ref type="table" coords="5,243.04,401.50,4.55,11.96" target="#tab_5">6</ref>) did compare reasonably well to all submissions, and reranking still showed measurable improvements, particularly for precision at 10, so we are optimistic the technique will apply even with higher baseline runs.</p><p>We feel our preliminary results show the value of reranking favoring query concept terms appearing closer together in top-ranked documents. Further experiments are needed on a wider variety of base runs to confirm the general applicability of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Topic Difficulty Prediction Results</head><p>We used an average span statistic of top documents as an estimate of topic difficulty. We averaged just the minimal span part of the Monz score (without the normalized language model score interpolation) over the top 10 documents. These scores were then sorted to produce the topic ranks required by the Robust Track. We did not use this statistic for prediction in our runs. We observed only a very weak correlation between topic hardness and this average span statistic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Terabyte Track</head><p>Given the difficulties of indexing a collection as large as the TREC terabyte collection, we were interested in performing the terabyte evaluation without indexing the collection. That is, in as few passes over the data as possible we sought to score every document on every query without building any index structures. This effectively treats the task as a routing problem. It makes sense to index a smaller collection if it will be searched more than a few times. However, on a Terabyte scale collection, when evaluating hundreds of queries in parallel, the point at which it makes sense to build an index is not so clear.</p><p>Our approach is to reduce the set of queries to a deterministic finite-state automaton (DFA) that processes text one character at a time and identifies each occurrence of each query term in the collection. First, we generate the cross-product of the following sets:  the fifty topics for the track;  five query length combinations: topic-only, topic and description, topic, description and narrative, description-only and narrative-only; and  words, character 4-grams and character 5-grams. This generates 750 queries from the fifty topics. Next, we build a non-deterministic finite-state automaton (NDFA) that recognizes each term in each query as it is fed characters from the document stream. This NDFA is then converted to a DFA. We now have a deterministic way to identify each occurrence of each query term. The DFA is run over each document in the collection. The document is scored, and the result is placed in the appropriate heap (each query maintains its own heap of its top scoring documents). Term statistics are required to calculate the language model similarity metric; we calculated term statistics over a small portion of the collection for this purpose. We implemented the system in Perl, using the PerlIO::gzip package to uncompress the data on-the-fly. We checkpointed the results every so often to guard against system crashes. The system ran on two Sun systems, each with 4G of main memory, using three CPUs on each. Unfortunately, serious system problems prevented us from processing the entire collection by the submission date; we processed only 1% of the collection. The system has a larger memory footprint than we had anticipated. While no significant memory allocation is performed in the portion of the code that we wrote, it is possible that the package we are using to uncompress the data uses memory dynamically.</p><p>After the submission deadline, we ran the system on the entire collection. Mean average precision for the resulting runs are shown in Table <ref type="table" coords="6,232.28,496.30,4.12,11.96" target="#tab_8">9</ref>. These runs use no blind relevance feedback; we expect that the scores could be significantly improved with such feedback, but doing so would entail a second pass over the data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Our Robust Track results give some evidence that reranking of our runs, using Monz's Minimal Matching Span scores, improves robustness. This benefit needs to be confirmed on other systems. For example, we would like to try reranking other systems' runs from the TREC 2004 Robust Track, and confirm robustness performance improvements on these runs as well.</p><p>We hope to expand our reranking approach to use more general concept matching, instead of exact title words or stems. A similar idea of conceptual indexing was explored by Sun researchers <ref type="bibr" coords="7,475.92,167.50,11.71,11.96" target="#b5">[6]</ref>. We began some initial experiments with title word expansion using WordNet that did not complete in time for official submissions. However we do believe title (or short query)-to-concept expansion and reranking to favor documents having more concepts appearing closer together could further improve robustness.</p><p>The routing approach to retrieval over the Terabyte collection is attractive, in that it scales linearly with the collection size, and consumes no additional disk resources. It can handle many queries in parallel, and in theory can maintain a fixed memory footprint. Unfortunately, Perl seems not to have been kind to this approach. We suspect that recoding in a lower level language might ameliorate some of these difficulties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,180.24,274.78,251.64,64.04"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="2,205.92,274.78,225.96,64.04"><row><cell cols="2">. Index Statistics for the Robust Track Collection</cell></row><row><cell></cell><cell># Terms Index Size</cell></row><row><cell>words</cell><cell>w 554751 373 MB</cell></row><row><cell cols="2">stems (Snowball) s 455803 320 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,45.84,85.18,509.58,116.36"><head>Table 2 .</head><label>2</label><figDesc>Effect of Span Reranking using T on Tw (word) Run</figDesc><table coords="4,45.84,110.86,509.58,90.68"><row><cell>Topic Set</cell><cell></cell><cell>MAP</cell><cell></cell><cell>P (10)</cell><cell cols="2"># no-rel@10</cell><cell></cell><cell>Area</cell></row><row><cell></cell><cell>Tw</cell><cell>Twr</cell><cell>Tw</cell><cell>Twr</cell><cell>Tw</cell><cell>Twr</cell><cell>Tw</cell><cell>Twr</cell></row><row><cell>200 old topics</cell><cell>.1970</cell><cell>.2078</cell><cell>.3303</cell><cell>.3815</cell><cell>25 (12.5%)</cell><cell>29 (14.5%)</cell><cell>.0086</cell><cell>.0092</cell></row><row><cell>49 new topics</cell><cell>.2366</cell><cell>.2462</cell><cell>.3252</cell><cell>.3571</cell><cell>6 (12.2%)</cell><cell>6 (12.2%)</cell><cell>.0136</cell><cell>.0166</cell></row><row><cell>50 hard topics</cell><cell>.1031</cell><cell>.1107</cell><cell>.2173</cell><cell>.2660</cell><cell>9 (18.0%)</cell><cell>8 (16.0%)</cell><cell>.0062</cell><cell>.0063</cell></row><row><cell>249 all topics</cell><cell>.2048</cell><cell>.2154</cell><cell>.3293</cell><cell>.3767</cell><cell>31 (12.4%)</cell><cell>35 (14.1%)</cell><cell>.0090</cell><cell>.0101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,45.84,230.14,509.58,116.36"><head>Table 3 .</head><label>3</label><figDesc>Effect of Span Reranking using T on Ts (stem) Run</figDesc><table coords="4,45.84,255.82,509.58,90.68"><row><cell>Topic Set</cell><cell></cell><cell>MAP</cell><cell></cell><cell>P (10)</cell><cell cols="2"># no-rel@10</cell><cell></cell><cell>Area</cell></row><row><cell></cell><cell>Ts</cell><cell>Tsr</cell><cell>Ts</cell><cell>Tsr</cell><cell>Ts</cell><cell>Tsr</cell><cell>Ts</cell><cell>Tsr</cell></row><row><cell>200 old topics</cell><cell>.2207</cell><cell>.2388</cell><cell>.3480</cell><cell>.4080</cell><cell>23 (11.5%)</cell><cell>27 (13.5%)</cell><cell>.0105</cell><cell>.0129</cell></row><row><cell>49 new topics</cell><cell>.2566</cell><cell>.2701</cell><cell>.3374</cell><cell>.3857</cell><cell>6 (12.2%)</cell><cell>5 (10.2%)</cell><cell>.0122</cell><cell>.0209</cell></row><row><cell>50 hard topics</cell><cell>.0947</cell><cell>.1125</cell><cell>.2027</cell><cell>.2640</cell><cell>9 (18.0%)</cell><cell>7 (14.0%)</cell><cell>.0066</cell><cell>.0086</cell></row><row><cell>249 all topics</cell><cell>.2278</cell><cell>.2449</cell><cell>.3459</cell><cell>.4036</cell><cell>29 (11.6%)</cell><cell>32 (12.9%)</cell><cell>.0104</cell><cell>.0137</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,45.84,375.10,509.58,116.12"><head>Table 4 .</head><label>4</label><figDesc>Effect of Span Reranking using D on D (word) Run</figDesc><table coords="4,45.84,400.78,509.58,90.44"><row><cell>Topic Set</cell><cell></cell><cell>MAP</cell><cell></cell><cell>P (10)</cell><cell cols="2"># no-rel@10</cell><cell></cell><cell>Area</cell></row><row><cell></cell><cell>D</cell><cell>Dr</cell><cell>D</cell><cell>Dr</cell><cell>D</cell><cell>Dr</cell><cell>D</cell><cell>Dr</cell></row><row><cell>200 old topics</cell><cell>.1990</cell><cell>.1915</cell><cell>.3243</cell><cell>.3510</cell><cell>23 (11.5%)</cell><cell>30 (13.5%)</cell><cell>.0071</cell><cell>.0067</cell></row><row><cell>49 new topics</cell><cell>.2500</cell><cell>.2373</cell><cell>.3293</cell><cell>.3633</cell><cell>3 ( 6.1%)</cell><cell>4 ( 8.2%)</cell><cell>.0203</cell><cell>.0221</cell></row><row><cell>50 hard topics</cell><cell>.1031</cell><cell>.1073</cell><cell>.2133</cell><cell>.2640</cell><cell>9 (18.0%)</cell><cell>8 (16.0%)</cell><cell>.0053</cell><cell>.0054</cell></row><row><cell>249 all topics</cell><cell>.2091</cell><cell>.2006</cell><cell>.3253</cell><cell>.3534</cell><cell>26 (10.4%)</cell><cell>34 (13.7%)</cell><cell>.0078</cell><cell>.0077</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,45.84,520.06,511.68,120.68"><head>Table 5 .</head><label>5</label><figDesc>Effect of Span Reranking using T on TDN (word) Run</figDesc><table coords="4,45.84,545.74,511.68,95.00"><row><cell>Topic Set</cell><cell></cell><cell>MAP</cell><cell></cell><cell>P (10)</cell><cell cols="2"># no-rel@10</cell><cell></cell><cell>Area</cell></row><row><cell></cell><cell>TDN</cell><cell>TDNr</cell><cell>TDN</cell><cell>TDNr</cell><cell>TDN</cell><cell>TDNr</cell><cell>TDN</cell><cell>TDNr</cell></row><row><cell>200 old topics</cell><cell>.2608</cell><cell>.2768</cell><cell>.4210</cell><cell>.4970</cell><cell>9 ( 4.5%)</cell><cell>12 ( 6.0%)</cell><cell>.0200</cell><cell>.0219</cell></row><row><cell>49 new topics</cell><cell>.3017</cell><cell>.3075</cell><cell>.4054</cell><cell>.4490</cell><cell>2 ( 4.1%)</cell><cell>2 ( 4.1%)</cell><cell>.0503</cell><cell>.0587</cell></row><row><cell>50 hard topics</cell><cell>.1370</cell><cell>.1526</cell><cell>.2947</cell><cell>.3780</cell><cell>1 ( 2.0%)</cell><cell>4 ( 8.0%)</cell><cell>.0127</cell><cell>.0122</cell></row><row><cell>249 all topics</cell><cell>.2689</cell><cell>.2828</cell><cell>.4179</cell><cell>.4876</cell><cell>11 ( 4.4%)</cell><cell>14 ( 5.6%)</cell><cell>.0227</cell><cell>.0260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,45.84,72.46,515.27,120.68"><head>Table 6 .</head><label>6</label><figDesc>Effect of Span (0.5) Reranking using T on TDN (word) Relevance Feedback Run</figDesc><table coords="5,45.84,98.14,515.27,95.00"><row><cell>Topic Set</cell><cell></cell><cell>MAP</cell><cell></cell><cell>P (10)</cell><cell cols="2"># no-rel@10</cell><cell></cell><cell>Area</cell></row><row><cell></cell><cell>TDNf</cell><cell>TDNfr</cell><cell>TDNf</cell><cell>TDNfr</cell><cell>TDNf</cell><cell>TDNfr</cell><cell>TDNf</cell><cell>TDNfr</cell></row><row><cell>200 old topics</cell><cell>.2936</cell><cell>.3078</cell><cell>.4490</cell><cell>.5100</cell><cell>22 (11.0%)</cell><cell>23 (11.5%)</cell><cell>.0149</cell><cell>.0208</cell></row><row><cell>49 new topics</cell><cell>.3720</cell><cell>.3557</cell><cell>.4313</cell><cell>.4837</cell><cell>2 ( 4.1%)</cell><cell>1 ( 2.0%)</cell><cell>.0617</cell><cell>.0697</cell></row><row><cell>50 hard topics</cell><cell>.1461</cell><cell>.1618</cell><cell>.3053</cell><cell>.3620</cell><cell>8 (16.0%)</cell><cell>10 (20.0%)</cell><cell>.0089</cell><cell>.0105</cell></row><row><cell>249 all topics</cell><cell>.3091</cell><cell>.3172</cell><cell>.4455</cell><cell>.5048</cell><cell>24 ( 9.6%)</cell><cell>24 ( 9.6%)</cell><cell>.0187</cell><cell>.0255</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,72.00,259.66,434.58,77.24"><head>Table 7 .</head><label>7</label><figDesc>Comparing TDN Results to Median</figDesc><table coords="5,72.00,285.58,434.58,51.32"><row><cell>250 topics</cell><cell></cell><cell>Median AP</cell><cell></cell><cell>MAP</cell><cell>#(%)no-rel@10</cell><cell>Area</cell></row><row><cell>Runtag</cell><cell>Best</cell><cell>(&gt;/=/&lt;)</cell><cell>Worst</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TDNr</cell><cell>1</cell><cell>146/3/100</cell><cell>0</cell><cell>.2828</cell><cell>14 (5.6%)</cell><cell>0.0260</cell></row><row><cell>TDNfr</cell><cell>8</cell><cell>173/0/76</cell><cell>0</cell><cell>.3172</cell><cell>24 (9.6%)</cell><cell>0.0255</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="5,160.08,596.86,291.86,103.64"><head>Table 8 .</head><label>8</label><figDesc>Hardness Correlation using Average Top-10 Span Statistic</figDesc><table coords="5,227.76,622.78,153.42,77.72"><row><cell>Runtag</cell><cell>Kendall correlation</cell></row><row><cell>Ts</cell><cell>0.200</cell></row><row><cell>Tw</cell><cell>0.172</cell></row><row><cell>D</cell><cell>0.178</cell></row><row><cell>TDNr</cell><cell>0.162</cell></row><row><cell>TDNfr</cell><cell>0.175</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,182.40,546.94,246.81,91.64"><head>Table 9 .</head><label>9</label><figDesc>Mean average precision for Terabyte Track runs</figDesc><table coords="6,229.20,572.86,153.60,65.72"><row><cell></cell><cell cols="2">4-grams Words</cell></row><row><cell>T</cell><cell>13.08</cell><cell>16.14</cell></row><row><cell>TD</cell><cell>18.72</cell><cell>21.99</cell></row><row><cell>TDN</cell><cell>25.69</cell><cell>29.18</cell></row><row><cell>Merged TDN</cell><cell>29.89</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,90.00,352.06,450.04,11.96;7,72.00,364.54,368.94,11.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,155.54,352.06,128.39,11.96">Why current IR engines fail</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,295.78,352.06,244.26,11.96;7,72.00,364.54,276.87,11.96">Proceedings of the 27th International Conference on Research and Development in Information Retrieval (SIGIR-04)</title>
		<meeting>the 27th International Conference on Research and Development in Information Retrieval (SIGIR-04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="584" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,389.98,450.00,11.96;7,72.00,402.70,304.38,11.96" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
		<ptr target="http://nrrc.mitre.org/NRRC/Docs_Data/RIA_2003/ria_final.pdf" />
		<title level="m" coord="7,262.07,389.98,268.23,11.96">Reliable Information Access Final Workshop Report</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,427.90,450.03,11.96;7,72.00,440.62,352.45,11.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,237.17,427.90,302.86,11.96;7,72.00,440.62,113.91,11.96">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="7,90.00,465.82,449.96,11.96;7,72.00,478.54,468.04,11.96;7,72.00,491.02,233.67,11.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,322.81,465.82,217.15,11.96;7,72.00,478.54,29.48,11.96">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,130.01,478.54,410.03,11.96;7,72.00,491.02,144.30,11.96">the Proceedings of the 22nd International Conference on Research and Development in Information Retrieval (SIGIR-99)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,95.42,516.46,444.56,11.96;7,72.00,529.18,140.09,11.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,145.68,516.46,389.31,11.96">From Document Retrieval to Question Answering. ILLC dissertation series 2003-04</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.51,554.38,452.43,11.96;7,72.00,567.10,362.98,11.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,168.82,554.38,277.40,11.96">Conceptual Indexing: A Better Way to Organize Knowledge</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
		</author>
		<idno>SMLI TR-97-61</idno>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
			<pubPlace>Mountain View, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Sun Microsystems Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
