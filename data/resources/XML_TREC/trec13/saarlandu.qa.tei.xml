<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.78,154.89,337.71,15.11;1,221.38,176.81,168.48,15.11">Question Answering by Searching Large Corpora with Linguistic Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.19,209.29,78.96,10.48"><forename type="first">Michael</forename><surname>Kaisser</surname></persName>
							<email>mkaisser@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University / DFKI GmbH</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.11,209.29,74.95,10.48"><forename type="first">Tilman</forename><surname>Becker</surname></persName>
							<email>becker@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University / DFKI GmbH</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.78,154.89,337.71,15.11;1,221.38,176.81,168.48,15.11">Question Answering by Searching Large Corpora with Linguistic Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD036674776F75859CFF1ACE720846AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the QuALiM Question Answering system which uses linguistic analysis of questions as well as candidate sentences in its answer finding process. To this end we have developed a rephrasing algorithm based on linguistic patterns that describe the structure of questions and candidate sentences and where precisely to find the answer in the candidate sentences. With this method and a fall-back strategy, both using the web as their primary data source, we participated in TREC 2004. We present our official results and a follow-up evaluation to elucidate the contribution of the methods used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At least since Frege and Wittgenstein we understand how to interpret sentences in natural language as expressing facts. In a sentence, different entities are referred to by linguistic expressions and the structure of the sentence describes the relationship between them. When someone asks a question, he asks not so much for a word or a few words that make up the answer, but rather for a fact that he has some partial knowledge of, but where he is lacking an important entity to describe it completely. E.g., in the question "When was Franz Kafka born?" the asker already knows that there is a person called Franz Kafka, and that this person must, as all persons, be born sometime. What he lacks, and asks for, is the date of birth. The fact the person looks for can in a davidsonian way be described like this: born("Franz Kafka",e) AND born("July 3, 1883",e) The knowledge the question asker has is this: born("Franz Kafka",e) AND born(X,e)</p><p>Sentences containing the facts and entities that are sought can be expressed with a wide variety of linguistic means. However, a simple sentence containing the answer to the question might be:</p><p>"Franz Kafka was born on July 3, 1883."</p><p>The observation one can make here is that certain words from the question reoccur in the answer sentence. In this case "Franz" "Kafka", "was" and "born". This is the basic observation most IR systems, and therefore also most QA systems, make use of. When searching for an answer to a question like the one described these systems search for the (nonstop) words in the question in a document collection, and if they find as many as possible closely together, they look for a date which is also close to these words and return this as the answer. However, they cannot be sure that the answer they found is the correct one: "Franz Kafka was not born on July 3, 1884." What these systems are not taking into account is that natural language has a structure and that it is this structure that determines in which relation certain linguistic entities in a sentence stand to each other. A related aspect is the variety of linguistic means to refer to entities. E.g., the correct answer is also contained in:</p><p>"Franz Kafka's birthday was July 3, 1883."</p><p>These are the basic observations that lead to the development of QuALiM: "Question Answering with Linguistic Methods".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>In this section we will give an short overview of how QuALiM answers questions. We will then in the following sections describe the different modules in more detail. Then, the TREC results and a post-TREC evaluation with its discussion are presented. After the conclusion, an appendix lists the third-party applications that were used in QuALiM.</p><p>QuALiM heavily relies on patterns. When it is asked a question it searches for a syntactic description in one of these patterns that matches the question. Most of these patterns also contain syntactic descriptions of potential answer sentences. This information can be used to predict the surface structure of possible answer sentences. Google is used to search for answer sentences and the retrieved sentences are then checked on whether they really match the syntactic structure proposed in the first place. To do this the sentences are parsed and tagged. If a candidate sentence still matches, the system knows which phrase in the sentence is the answer. The answer itself is then checked on its semantic type by using a Named Entity Recognition system.</p><p>QuALiM also implements a fallback mechanism, which does not propose reformulations, but instead sends queries created from key words and key phrases in the question to Google. From the returned snippets n-grams are mined, which are also checked on their semantic type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Rephrasing Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Strict Pattern Matching</head><p>As described above, the rephrasing part of QuALiM relies on patterns which are used to define linguistic constraints on questions, potential answer sentences to these questions and the answers themselves. In detail a pattern used in QuALiM consists of three parts:</p><p>• Sequences are used to classify the questions according to their linguistic (mostly syntactic) structure.</p><p>• Targets are used to describe the linguistic (mostly syntactic) structure of potential answer candidates.</p><p>• AnswerType elements express semantic constraints on the answers. Figure <ref type="figure" coords="2,352.00,206.81,4.98,8.74" target="#fig_0">1</ref> gives an example of such a pattern. Each question that the system is asked is checked whether it matches one of the sequences in the pattern files. The sequence which can be seen in figure <ref type="figure" coords="2,310.61,254.77,4.98,8.74" target="#fig_0">1</ref> matches any question that starts with the word "When", followed by the word "did", followed by an NP, followed by a verb in its infinitive form, followed by an NP or a PP, followed by a question mark which has in addition to be the last element in the question.</p><p>In the TREC 2004 question set, this sequence matched five questions:</p><p>• When did Floyd Patterson win the title?</p><p>• When did Amtrak begin operations?</p><p>• When did Jack Welch become chairman of General Electric?</p><p>• When did Jack Welch retire from GE?</p><p>• When did the Khmer Rouge come into power?</p><p>For the TREC 2004 runs QuALiM used 157 patterns (with 157 sequences) to classify incoming questions. If a question matches a sequence, the targets are used to predict the linguistic structure of potential answer sentences. Two targets are shown in figure 1. For the question "When did Amtrak begin operations?", they would suggest the following answer sentences (or answer sentence parts):</p><p>1. Amtrak began operations in ANSWER 2. In ANSWER (,) Amtrak began operations</p><p>The numbers in the ref elements are variables that point back to the sequence element with the corresponding id attribute. Note, that QuALiM copies the complete linguistic information for the sequence element into the target, not just its surface appearance. Beside the target elements that can be seen in the &lt;pattern name="When+did+NP+Verb+NPorPP" level="5"&gt; &lt;sequence&gt; &lt;word id="1"&gt;When&lt;/word&gt; &lt;word id="2"&gt;did&lt;/word&gt; &lt;parse id="3"&gt;NP&lt;/parse&gt; &lt;morph id="4"&gt;V INF&lt;/morph&gt; &lt;parse id="5"&gt;NP|PP&lt;/parse&gt; &lt;final&gt;?&lt;/final&gt; &lt;/sequence&gt; &lt;target name="target1"&gt; &lt;ref&gt;3&lt;/ref&gt; &lt;ref morph="V PAST"&gt;4&lt;/ref&gt; &lt;ref&gt;5&lt;/ref&gt; &lt;word&gt;in&lt;/word&gt; &lt;answer&gt;NP&lt;/answer&gt; &lt;/target&gt; &lt;target name="target2"&gt; &lt;word&gt;in&lt;/word&gt; &lt;answer&gt;NP&lt;/answer&gt; &lt;punctuation optional="true"&gt;,&lt;punctuation&gt; &lt;ref&gt;3&lt;/ref&gt; &lt;ref morph="V PAST"&gt;4&lt;/ref&gt; &lt;ref&gt;5&lt;/ref&gt; &lt;/target&gt; ... more targets ... &lt;answerType phrases="NP|PP"&gt; &lt;built-in weight="2"&gt;dateComplete&lt;/built-in&gt; &lt;namedEntity weight="4"&gt;Date&lt;/namedEntity&gt; &lt;built-in weight="3"&gt;year|in_year&lt;/built-in&gt; &lt;other ignore="true"/&gt; &lt;/answerType&gt; &lt;/pattern&gt; example (ref, word, punctuation and answer ), three others exist: pos, parse and unknown, but the latter two are-up to now-just implemented with dummy functionality and could therefore not be used for the TREC runs.</p><p>Although the targets describe linguistic structures they can also be used to propose surface structures of the potential answer sentences. Some target elements can be used for this, namely ref, word and punctuation, while the others cannot. With this information search queries can be created that are send to Google. Unfortunately, as Google ignores punctuation in queries, this information is also not useful when creating the queries. But the ref and word elements in the targets seen in figure <ref type="figure" coords="3,465.10,619.56,4.98,8.74" target="#fig_0">1</ref> provide enough information to generate the following two queries:</p><p>1. "Amtrak began operations in" 2. "In" "Amtrak began operations" QuALiM will send these queries to Google and harvest the first 40 snippets returned. It will then try to extract sentences from the snippets that contain all words from the search query. For the first query listed above, the first five sentences QuALiM can extract are:</p><p>• "Since Amtrak began operations in 1971, federal outlays for intercity rail passenger service have been about $18 billion."</p><p>• "Amtrak began operations in 1971."</p><p>• "Amtrak of the obligation to operate the basic system of routes that was largely inherited from the private railroads when Amtrak began operations in 1971."</p><p>• "Amtrak began operations in 1971, as authorized by the Rail Passenger Service Act of 1970."</p><p>• "A comprehensive history of intercity passenger service in Indiana, from the mid-19th century through May 1, 1971, when Amtrak began operations in the state."</p><p>QuALiM will now parse and tag the candidate sentences and check weather the linguistic structure described in the target really matches the sentences. If the system finds this structure, it also knows which part of the sentence must be the answer. In the first four examples given above it is "1971", in the last "the state" (which is sorted out in a later processing step, when the system recognizes that "the state" is not an appropriate answer for this type of question).</p><p>The system will place all answers it has found in a weighted sequence bag, a bag of word sequences, where each word sequence has a weight attached. In the given example the weighted sequence bag will look like this: 4: "1971" 1: "the state"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fuzzy Pattern Matching</head><p>Experiments during the construction phase of QuALiM showed that the described constraints are sometimes too strict. It happens that the retrieved sentences contain the correct answer, but not exactly at the position described by the targets. For the second target shown in figure 1 a possible answer sentence received from Google might be:</p><p>"In 1971, the railroad company Amtrak began operations."</p><p>This sentence does not match the target because no single NP is placed between the word "In" and the NP "Amtrak". In such a case QuALiM will extract the string where it actually expected the NP, here: "1971, the railroad company". This string contains two NPs: "1971" and "the railroad company". With this information another weighted sequence bag is created:</p><p>1: "1971", "the railroad company"</p><p>The results form both pattern matching algorithms are then combined, but before this is done the weights from the exact target matching algorithm are multiplied with 5. So the outcome looks like this: 21: "1971" 5: "the state" 1: "the railroad company" This method of combining the results proofed to be quite effective. If QuALiM can find many sentences that match the targets exactly, the fuzzy results are of nearly no importance. If there are no or only a few exact matching sentences found, the fuzzy results will become more important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Google Fallback Mechanism</head><p>QuALiM implements a second mechanism to find answers. It constructs three search queries that are combinations from all NPs in the question and all non stop words:</p><p>"When was Jim Inhofe first elected to the senate?" becomes 1. Jim Inhofe senate first elected 2. "Jim Inhofe" "the senate" 3. "Jim Inhofe" "the senate" first elected These queries are sent to Google and from the snippets returned, n-grams are mined. These n-grams are placed in a weighted sequence bag, where the weights initially show how often an n-gram has been found in the snippets. The results form the third query are doubled. The weights are then modified, so that ngrams with more words receive a bonus over shorter n-grams.</p><p>The results from the Google fallback algorithm are combined with the results from the rephrasing algorithms. But before this is done the weights of the rephrasing algorithms are multiplied with 4. (So the overall ratio strict:fuzzy:fallback is 20:4:1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Type Checking</head><p>What remains to do is to check the combined output of the three answer finding mechanisms on their type. To do this the answerType element in the XML structure is used. Several information sources can be combined here: named entity recognition, Word-Net or built-in named entity recognition features, which can be used to recognize standard date specifications, year specifications, numbers, number/unit compounds etc.</p><p>The textual content of an answer type element specifies the condition an item in the sequence bag must fulfill, to match the answer type element. Each element from the sequence bag is matched against all answer type elements. All weights from the matching answer type elements are summed up, and finally the value of the sequence bag element is multiplied with that sum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">List and Definition Questions</head><p>The approach QuALiM uses to answer list questions is a simple modification of the factoid approach. The system tries to put a question in the singular form and it will then return all answers above a given cutoff level. It is possible to write special list patterns (similar to factoid patterns as seen in figure <ref type="figure" coords="5,508.76,151.87,3.88,8.74" target="#fig_0">1</ref>), but only two patterns existed for the TREC runs.</p><p>Definition questions are processed as follows: The series' target is send to Google, and as with the Google fallback mechanism a weighted sequence bag is created, with the most frequent words or word sequences occurring in the snippets at the top. The system then looks for sentences (or sentence parts) in the AQUAINT document collection that contain many of the entries in the weighted sequence Bag. For each sentence, the weights of the entries in the sequence bag that occur in the sentence lead to a score which will be divided through the character length of the sentence. The sentence with the best score is selected. Then, all values of all entries in the sequence bag that occurred in the selected sentence are divided by three and again all sentences are scored and the best one is selected. This is repeated until the sentences selected so far exceeded a certain character length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>QuALiM builds on two pillars: Creative use of the web, especially by using a search engine (Google in our case) as an Information Retrieval system and by using linguistic methods to analyze questions and candidate sentences in order to locate the exact answers in the latter.</p><p>Using the web for Question Answering is an idea that became more and more important in recent years. AnswerBus <ref type="bibr" coords="5,394.82,533.94,21.04,8.74">[Ans]</ref>, BrainBoost <ref type="bibr" coords="5,477.60,533.94,20.21,8.74">[Bra]</ref>, IONaut [ION] and START <ref type="bibr" coords="5,387.34,545.90,28.99,8.74">[STA]</ref> are systems that are online and that search the web for an answer to a question a user has asked. Furthermore quite a few research papers exist on that topic: [DBB + 02, KEW01, RFQ + 04] to name just a few.</p><p>Our way to use the web shows similarities to [DBB + 02]. In this paper the authors also present a way to reformulate questions as possible answer sentences, exploiting redundancy in large corpora such as the web. However, their reformulations are not based on the syntactic structure, but rather on sim-ple string manipulations. Reformulations based on the syntactic structure offer a number of benefits: The reformulations are more exact, more reformulations can be described and the knowledge about the sentence structure can be used to locate the exact answer in candidate sentences.</p><p>In general not much work has been done so far on using linguistic knowledge to find candidate sentences, on making use of this knowledge to evaluate their credibility and to locate the part of the sentence that contains the answer.</p><p>There are three systems we want to mention here which actually parse candidate sentences and make use of this information, although in quite different ways: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We see our syntactic patterns as a generic approach to handle various linguistic phenomena:</p><p>• We can combine different variants of the same underlying question in one pattern.</p><p>(The rephrasing algorithm allows to have patterns with more than one sequence, although this feature was not used in the TREC 2004 runs.)</p><p>• We can distinguish a question like "Who is the Queen of England" (Who+is+NP+?) form "Who is Albert Einstein?" (Who+is+Person Name+?).</p><p>This can be done because information from named entity recognition can be included in the sequences and it is is useful because both questions require different answer types.</p><p>• We can perform reformulations that introduce words which are not in the question into the answer sentence: E.g, we can reformulate "When+was+Person+born+?" to "Per-son+'s+birthday+is/was+ANSWER".</p><p>• Almost all syntactic phenomena the parser can handle can be described with the patterns: appositions, active/passive transformations, dative shift, subordinate clauses, NP-gerunds and many more.</p><p>Although we use the complete syntactic structure returned from the parser to determine phrase boundaries, we abstract away from that by using flat descriptions in our patterns. As a result, we cannot describe some linguistic phenomena, for example PPattachment.</p><p>Furthermore, currently all patterns have to be written manually, which takes up a lot of time. This is the reason why we performed the TREC runs with only 157 patterns. We simply had no time to write more of them.</p><p>9 Evaluation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">TREC Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Post-TREC Evaluation</head><p>After TREC 2004 we conducted several evaluations here at DFKI. We were mainly interested in the per- formance and behavior of the different algorithms implemented. For the additional runs performed and evaluated at DFKI, we used the TREC 2004 question set, but we resolved the questions manually. So we changed the Question "When was he born?" in the "Franz Kafka" series to "When was Franz Kafka born?" and told the system to process the questions as they are in the file and not to combine them with the target. Furthermore, we turned the document localization module off. So the system returned just answers, but no AQUAINT documents. This means that there can be no "NIL" answers and no "unsupported" judgments. Naturally-as we disabled two error sources-the results we received are better than the results we received in TREC 2004. <ref type="foot" coords="7,226.90,420.37,3.97,6.12" target="#foot_0">1</ref> We performed four more runs, telling the system it should only answer factoid questions and ignore list and definitional questions.</p><p>Here is a short description of the runs:</p><p>strict In this run we only used the rephrasing algorithm in its strict version as described in section 3.1.</p><p>fuzzy For this runs we used the fuzzy version of the rephrasing algorithm. See section 3.2 (Note that NPs at the exact position still received a weight four times higher as NPs not being at the exact position.)</p><p>fallback This run used only the Google fallback mechanism as described in section 4.</p><p>combined This run combined the lazy rephrasing algorithm with the Google fallback mechanism.</p><p>Table <ref type="table" coords="7,347.97,322.11,4.98,8.74" target="#tab_2">2</ref> shows the results we obtained.</p><p>After the system answered all 230 factoid questions, we sorted the results by their confidence values. (Similar to the TREC 2002 QA task, where all participants had to do this, see <ref type="bibr" coords="7,455.76,370.16,29.30,8.74" target="#b10">[Vor02]</ref>.) Then we calculated what fraction of the best x answers are correct answers. The results can be seen in figure <ref type="figure" coords="7,531.49,394.07,3.88,8.74" target="#fig_1">2</ref>. If you draw for example an imaginary vertical line from 41 on the x axis, it will cut the strict curve at 0.94, meaning that 94% of the 41 answers the system was most sure of, were correct. This line cuts the fallback curve at 0.72, so only 72% of the best 41 answers from the fallback mechanism are correct. Figure <ref type="figure" coords="7,342.63,477.75,4.98,8.74" target="#fig_2">3</ref> is based on the same data as figure <ref type="figure" coords="7,512.51,477.75,3.88,8.74" target="#fig_1">2</ref>, but this time we computed the fraction of correct and inexact answers. The little diamonds that can be seen on the curves mark the last correct answer the system returned, while the boxes mark the point from where on the system found no more data to process and therefore could only return an "answer unknown" response with a confidence value of 0.</p><p>We think that these results are quite interesting:</p><p>• Generally, it has to be said that the fallback mechanism performs better than the rephrasing algorithm, at least when the results are measured using accuracy as in TREC 2004. (In the diagrams accuracy can be seen at the rightmost point of the diagram, where the fraction of the top 230 (i.e., of all) questions was computed.  • The rephrasing algorithms run out of data very fast. This due to the fact that the TREC 2004 runs were performed with only 157 patterns. For most of the questions this approach did simply not catch. Either, because there was no pattern for this question type, or because the reformulations could not be found on the web. It is left for further work to evaluate how much performance gain can be achieved with more patterns.</p><p>• As mentioned, the rephrasing algorithms do not return that many answers, but a large fraction of them is correct. Also, the confidence values the algorithms return seem to be really useful. Answers with a high confidence value are almost always correct. Of the 38 answers the strict rephrasing algorithm was most sure of, 36 were correct, two were inexact and none were wrong.</p><p>Of the next 35 answers 10 were correct, four were inexact and 21 were wrong. 2 After these 73 questions, the algorithm returned no more answers.</p><p>• The confidence values of the fallback mechanism are not that useful. The mechanism returned 215 answers, but it is difficult for the system to determine whether it has found a correct answer or not.</p><p>• Especially for the rephrasing algorithms, the first non-correct answers the system returns, are not completely wrong, but rather inexact. (See the difference between figures 2 and 3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion and Further Directions</head><p>Most QA systems use keyword approaches combined with named entity recognition techniques to locate the answer to a question in a given document collection. In this paper we presented a new way to locate</p><p>2 The confidence values of the rephrasing algorithms reflect the amount of answer sentences that were found on the web and how useful they were judged when being checked on the correct syntax and on the correct answer type. So low confidence values indicate that there is something wrong: Either not much data was found or the syntactic structure or the answer type did not match.</p><p>answers: Feeding syntactic reformulations into an Information Retrieval system and-more importantusing a syntactic analysis of the results in order to find the exact answer to a question.</p><p>Our experiments suggest that linguistic processing of candidate sentences results in a better knowledge of when an answer is correct or not. If we adopt the terms precision and recall from Information Retrieval (which are not commonly used in Question Answering), we can conclude that making use of a syntactic analysis of candidate sentences results in good precision values. But, because natural language offers so many different ways to express answers, it is not easy to obtain good recall values.</p><p>This last point leads directly to our future plans: We want to explore how recall can be improved, without sacrificing the good precision values (or even find ways to improve precision as well). The most obvious way to do this is to increase the size of the pattern set. Because it is very time consuming to write the patterns manually, we are currently thinking about making use of Machine Learning techniques in order to do this.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,127.68,527.96,355.89,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example pattern as used in the current version of the QuALiM system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,72.00,372.49,467.24,8.74;8,72.00,384.45,29.11,8.74;8,93.03,124.80,425.21,232.58"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fraction of correct answers of the top x answers returned, when ordering them by their confidence values.</figDesc><graphic coords="8,93.03,124.80,425.21,232.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,72.00,639.94,467.25,8.74;8,72.00,651.89,101.09,8.74;8,93.03,392.24,425.21,232.58"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Fraction of correct and inexact answers of the top x answers returned, when ordering them by their confidence values.</figDesc><graphic coords="8,93.03,392.24,425.21,232.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,310.61,455.72,228.64,150.34"><head>Table 1 :</head><label>1</label><figDesc>TREC 2004 results for the QuALiM system.</figDesc><table coords="6,310.61,455.72,228.64,128.48"><row><cell cols="3">Table 1 shows the results QuALiM received in TREC</cell></row><row><cell cols="3">2004. The runs differed in parameter settings (e.g.</cell></row><row><cell cols="3">the number of NIL answers returned for factoid ques-</cell></row><row><cell cols="3">tions, the length of the answer list when answering</cell></row><row><cell cols="2">list questions etc.)</cell><cell></cell></row><row><cell></cell><cell cols="2">run 1 run 2 run 3 rank</cell></row><row><cell>Factoid</cell><cell cols="2">0.343 0.339 0.343 4th</cell></row><row><cell>List</cell><cell>0.096</cell><cell>0.111 0.125 9th</cell></row><row><cell>Other</cell><cell>0.145</cell><cell>0.181 0.211 10th</cell></row><row><cell cols="2">Combined 0.232</cell><cell>0.242 0.256 6th</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,96.05,126.76,419.15,104.72"><head>Table 2 :</head><label>2</label><figDesc>Results of the QuALiM system in our post-TREC evaluations. See the text for details.</figDesc><table coords="7,178.77,126.76,253.72,82.86"><row><cell></cell><cell cols="4">strict fuzzy fallback combined</cell></row><row><cell># correct</cell><cell>46</cell><cell>58</cell><cell>91</cell><cell>105</cell></row><row><cell># inexact</cell><cell>6</cell><cell>10</cell><cell>9</cell><cell>7</cell></row><row><cell># wrong</cell><cell>20</cell><cell>31</cell><cell>114</cell><cell>106</cell></row><row><cell># answers returned</cell><cell>72</cell><cell>98</cell><cell>214</cell><cell>218</cell></row><row><cell># no answer found</cell><cell>158</cell><cell>132</cell><cell>16</cell><cell>11</cell></row><row><cell>accuracy</cell><cell cols="2">0.200 0.252</cell><cell>0.395</cell><cell>0.456</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,87.24,601.05,213.40,6.99;7,72.00,610.52,228.64,6.99;7,72.00,619.98,125.88,6.99"><p>In our additional runs, when all questions finding methods are combined, we receive an accuracy of 0.456. We think that this is quite reasonable. In TREC</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2004" xml:id="foot_1" coords="7,220.42,619.98,80.23,6.99;7,72.00,629.45,228.64,6.99;7,72.00,638.91,228.64,6.99;7,72.00,648.37,228.64,6.99;7,72.00,657.84,228.64,6.99;7,72.00,667.30,207.65,6.99"><p>our best run returned 79 correct and 20 unsupported answers. As we performed no document localization we can expect an accuracy of roughly 99/230=0.430. The additional 0.026 points result from the fact that we used resolved questions. (We actually would have expected that this results in a larger performance gain.)</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Technical Details</head><p>QuALiM is written in Java 1.4. It uses the following 3rd party modules: • ANNIE (a named Entity Recognition system): http://gate.ac.uk/ie/annie.html • Google (accessed through the GoogleAPI): http://www.google.com/apis/ • Link Parser: http://www.link.cs.cmu.edu/link/ • QTag (a POS tagger): http://web.bham.ac.uk/o.mason/software/tagger/ • WordNet: http://www.cogsci.princeton.edu/˜wn/ • XTAG morphology database: http://www.cis.upenn.edu/˜xtag/</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>You can find more information about QuALiM here: http://www.dfki.de/˜mkaisser/ For example an electronic copy of my Master's Thesis: <ref type="bibr" coords="10,89.82,164.31,30.99,8.74" target="#b4">[Kai04]</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,122.89,220.81,177.75,8.74;10,122.89,232.77,123.57,8.74" xml:id="b0">
	<monogr>
		<ptr target="http://www.answerbus.com" />
		<title level="m" coord="10,122.89,220.81,172.88,8.74">AnswerBus Question Answering System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,122.89,253.66,177.76,8.74;10,122.89,265.61,32.80,8.74;10,122.89,277.57,125.18,8.74" xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://www.brainboost.com" />
	</analytic>
	<monogr>
		<title level="j" coord="10,122.89,253.66,177.76,8.74;10,122.89,265.61,28.11,8.74">BrainBoost -Question Answering Search Engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,78.12,298.46,18.37,8.74;10,96.49,296.89,6.12,6.12;10,103.11,298.46,197.54,8.74;10,122.89,310.42,177.75,8.74;10,122.89,322.37,177.75,8.74;10,122.89,334.33,174.09,8.74" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Bankom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<idno>DBB + 02</idno>
		<title level="m" coord="10,281.27,310.42,19.37,8.74;10,122.89,322.37,177.75,8.74;10,122.89,334.33,143.45,8.74">Web Question Answering: Is More Always Better? Proceedings of UAI 2003</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,355.22,26.56,8.74;10,98.57,353.65,6.12,6.12;10,105.18,355.22,195.45,8.74;10,122.89,367.18,177.75,8.74;10,122.89,379.13,177.75,8.74;10,122.89,391.09,177.75,8.74;10,122.89,403.04,177.75,8.74;10,122.89,415.00,177.75,8.74;10,122.89,426.95,177.75,8.74;10,122.89,438.91,22.70,8.74;10,72.00,459.80,24.36,8.74;10,122.89,459.80,177.75,8.74;10,122.89,471.76,153.03,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,268.46,379.13,32.18,8.74;10,122.89,391.09,177.75,8.74;10,122.89,403.04,141.59,8.74">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Bensley</surname></persName>
		</author>
		<idno>HMC + 03</idno>
		<ptr target="http://www.ionaut.com" />
	</analytic>
	<monogr>
		<title level="m" coord="10,283.84,403.04,16.81,8.74;10,122.89,415.00,177.75,8.74;10,122.89,426.95,114.17,8.74;10,161.40,459.80,139.24,8.74;10,122.89,471.76,14.53,8.74">The Proceedings of the 2003 Edition of the Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">8400</biblScope>
		</imprint>
	</monogr>
	<note>The IO question answering system</note>
</biblStruct>

<biblStruct coords="10,122.89,492.65,71.60,8.74;10,241.27,492.65,59.37,8.74;10,122.89,504.60,177.75,8.74;10,122.89,516.56,177.75,8.74;10,122.89,528.51,102.02,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,241.27,492.65,59.37,8.74;10,122.89,504.60,177.75,8.74;10,122.89,516.56,109.39,8.74">Answering by Searching Large Corpora with Linguistic Methods. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kaisser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Saarland University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,122.89,549.41,177.75,8.74;10,122.89,561.36,177.76,8.74;10,122.89,573.32,177.75,8.74;10,122.89,585.27,177.75,8.74;10,122.89,597.23,134.77,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,177.45,549.41,123.19,8.74;10,122.89,561.36,122.39,8.74">Annotating the World Wide Web using Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,264.73,561.36,35.92,8.74;10,122.89,573.32,177.75,8.74;10,122.89,585.27,177.75,8.74;10,122.89,597.23,104.74,8.74">Proceedings of the 5th RIAO conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97)</title>
		<meeting>the 5th RIAO conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,122.89,618.12,177.76,8.74;10,122.89,630.08,177.75,8.74;10,122.89,642.03,177.75,8.74;10,122.89,653.99,177.75,8.74;10,122.89,665.94,90.72,8.74;10,310.61,127.96,24.49,8.74;10,335.10,126.39,6.12,6.12;10,341.71,127.96,197.54,8.74;10,361.50,139.92,177.75,8.74;10,361.50,151.87,177.75,8.74;10,361.50,163.83,177.75,8.74;10,361.50,175.78,177.75,8.74;10,361.50,187.74,177.75,8.74;10,361.50,199.69,177.75,8.74;10,361.50,211.65,177.75,8.74;10,361.50,223.60,109.16,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,204.26,630.08,96.38,8.74;10,122.89,642.03,86.37,8.74;10,427.20,163.83,112.04,8.74;10,361.50,175.78,177.75,8.74;10,361.50,187.74,43.42,8.74">Omnibase: Uniform Access to Heterogeneous Data for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Cody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deniz</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alton</forename><forename type="middle">Jerome</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baris</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Temelkuran</surname></persName>
		</author>
		<idno>KFY + 02</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,237.85,642.03,62.80,8.74;10,122.89,653.99,177.75,8.74;10,437.04,187.74,102.21,8.74;10,361.50,199.69,177.75,8.74;10,361.50,211.65,177.75,8.74;10,361.50,223.60,20.12,8.74">Proceedings of the 7th International Workshop on Applications Of Natural Language to Information Systems</title>
		<meeting>the 7th International Workshop on Applications Of Natural Language to Information Systems</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2001">2001. 2001. 2002. 2002</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 10th World Wide Web Conference</note>
</biblStruct>

<biblStruct coords="10,310.61,243.53,24.08,8.74;10,334.68,241.95,6.12,6.12;10,341.30,243.53,197.95,8.74;10,361.50,255.48,177.75,8.74;10,361.50,267.44,177.75,8.74;10,361.50,279.39,177.75,8.74;10,361.50,291.35,177.75,8.74;10,361.50,303.30,55.03,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,514.73,255.48,24.52,8.74;10,361.50,267.44,172.22,8.74">Probabilistic Question Answering on the Web</title>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiguo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harris</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amardeep</forename><surname>Grewal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,361.50,279.39,177.75,8.74;10,361.50,291.35,152.57,8.74">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>JA-SIST</publisher>
		</imprint>
	</monogr>
	<note>RFQ + 04</note>
</biblStruct>

<biblStruct coords="10,361.50,323.23,17.16,8.74;10,396.12,323.23,33.07,8.74;10,446.63,323.23,33.52,8.74;10,497.60,323.23,41.65,8.74;10,361.50,335.18,38.81,8.74;10,429.99,335.18,45.46,8.74;10,505.15,335.18,34.09,8.74;10,361.50,347.14,181.35,8.74" xml:id="b8">
	<monogr>
		<ptr target="http://www.ai.mit.edu/projects/infolab/" />
		<title level="m" coord="10,361.50,323.23,17.16,8.74;10,396.12,323.23,33.07,8.74;10,446.63,323.23,33.52,8.74;10,497.60,323.23,41.65,8.74;10,361.50,335.18,38.81,8.74;10,429.99,335.18,45.46,8.74;10,505.15,335.18,29.22,8.74">The START Natural Language Question Answering System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,361.50,367.06,177.76,8.74;10,361.50,379.02,177.75,8.74;10,361.50,390.97,177.75,8.74;10,361.50,402.93,177.75,8.74;10,361.50,414.88,177.74,8.74;10,361.50,426.84,177.75,8.74;10,361.50,438.79,49.15,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,452.43,379.02,86.82,8.74;10,361.50,390.97,177.75,8.74;10,361.50,402.93,172.30,8.74">Combining Linguistic Processing and Web Mining for Question Answering: ITC-irst at TREC-2004</title>
		<author>
			<persName coords=""><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,375.30,414.88,163.94,8.74;10,361.50,426.84,141.40,8.74">The Proceedings of the 2004 Edition of the Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,361.50,458.72,177.75,8.74;10,361.50,470.68,177.75,8.74;10,361.50,482.63,177.74,8.74;10,361.50,494.59,177.75,8.74;10,361.50,506.54,49.15,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,463.66,458.72,75.59,8.74;10,361.50,470.68,173.18,8.74">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Vorheese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,375.30,482.63,163.94,8.74;10,361.50,494.59,141.40,8.74">The Proceedings of the 2002 Edition of the Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
