<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.08,101.33,423.83,15.15;1,266.65,121.26,150.70,15.15">Juru at TREC 2004: Experiments with prediction of query difficulty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,164.49,181.84,68.85,8.77"><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
							<email>yomtov@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Labs University</orgName>
								<address>
									<addrLine>Campus Haifa</addrLine>
									<postCode>31905</postCode>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.23,181.84,45.46,8.77"><forename type="first">Shai</forename><surname>Fine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Labs University</orgName>
								<address>
									<addrLine>Campus Haifa</addrLine>
									<postCode>31905</postCode>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.54,181.84,67.50,8.77"><forename type="first">David</forename><surname>Carmel</surname></persName>
							<email>carmel@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Labs University</orgName>
								<address>
									<addrLine>Campus Haifa</addrLine>
									<postCode>31905</postCode>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.53,181.84,67.50,8.77"><forename type="first">Adam</forename><surname>Darlow</surname></persName>
							<email>darlow@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Labs University</orgName>
								<address>
									<addrLine>Campus Haifa</addrLine>
									<postCode>31905</postCode>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,451.45,181.84,68.06,8.77"><forename type="first">Einat</forename><surname>Amitay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Haifa Research Labs University</orgName>
								<address>
									<addrLine>Campus Haifa</addrLine>
									<postCode>31905</postCode>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.08,101.33,423.83,15.15;1,266.65,121.26,150.70,15.15">Juru at TREC 2004: Experiments with prediction of query difficulty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23AD533A752D9828B840F1854C625EF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our experiments in the Robust track this year focused on predicting query difficulty and using this prediction for improving information retrieval. We developed two prediction algorithms and used the subsequent prediction in several ways in order to improve the performance of the search engine. These included modifying the search engine parameters, using selective query expansion, and switching between different topic parts. We also experimented with a new scoring model based on ideas from the field of machine learning. Our results show that query prediction is indeed efficient in improving retrieval, although further work is needed in order to improve the performance of the prediction algorithms and their uses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most search engines respond to user queries by generating a list of documents deemed relevant to the query. In this work we suggest to give the user additional information by predicting query difficulty; namely, how likely it is that the documents returned by the search engine are relevant to the query. This information is advantageous for several applications, among them are:</p><p>1. Simple evaluation of the query results. 2. Feedback to the user: The user can rephrase the query so as to improve query prediction. 3. Feedback to the search engine: The search engine can use the query predictor as a target function for optimizing the query, for example by adding terms to the query. 4. Use of different ranking functions for different queries based on their predicted difficulty: It is well-known in the Information Retrieval community that methods such as query expansion can help "easy" queries but are detrimental to "hard" queries <ref type="bibr" coords="1,429.49,589.57,9.96,8.74" target="#b4">[5]</ref>. The predictor can be used to mark easy queries on which the search engine should use query expansion. Our initial experiments suggest that such a method indeed improves the performance of the search engine. 5. For distributed information retrieval: As a method for deciding which search engine to use:</p><p>Given a query and several search engines (e.g. Google, Yahoo, HotBot, etc), the predictor can estimate the results of which search engine are best for the given query. Additionally, prediction can be used as a method for merging the results of queries performed on different datasets by weighing the results from each dataset.</p><p>The goal in our participation in the Robust Track this year was to experiment with using prediction of difficulty as a means for improving information retrieval. Our observations, which we describe in detail below, show that queries that are answered well by search engines are those whose keywords agree on most of the returned documents. Agreement is measured by the number of documents ranked best using the full query that were also ranked best by the sub-queries. Difficult queries (i.e. queries for which the search engine will return mostly irrelevant documents) are those where either all keywords agree on all results or cannot agree on them. The former is usually the case where the query contains one rare keyword that is not representative of the whole query and the rest of the query terms appear together in many irrelevant documents.</p><p>For example, consider the TREC query "What impact has the Chunnel had on the British economy and/or the life style of the British?". In this query many irrelevant documents, selected by the search engine, will contain the words British, life, style, economy, etc. But the gist of the query, the Chunnel, is lost. Another type of difficult query is one where the query terms do not agree on the target documents and each contributes very few documents to the final results. An example of such a case is the TREC query "Find accounts of selfless heroic acts by individuals or small groups for the benefit of others or a cause". In this query there are no keywords that appear together in the relevant documents and thus the final result set is poor.</p><p>Our observations have led us to suggest methods by which query difficulty can be predicted. The proposed methods are a wrapper to the search engine, i.e. they is not limited to a specific search engine or search method. Since they do not intervene in the workings of the search engine, they can be applied to any search engine. The main advantages of these algorithms, in addition to possible applications described above, are that they are simple, and can be applied by the search engine during query execution, since most of the data they use for their operation are generated by the search engine during its normal mode of operation.</p><p>As far as we know, the only attempt to measure query difficulty (albeit for a different reason) is <ref type="bibr" coords="2,544.72,389.01,9.97,8.74" target="#b0">[1]</ref>. In this work, prediction of query difficulty is attempted for a specific scoring model (i.e. A specific search engine). The purpose of this research is to decide on a query-by-query basis if query expansion should be used. In contrast, the proposed method is not limited to a specific search engine, and thus has many additional applications (As outlined above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Predicting query difficulty</head><p>The basic idea behind the prediction is to measure the contribution of each query element to the final ranking of documents. The query elements are the keywords (i.e. the words of the query, after discarding stopwords) and lexical affinities, which are closely related keywords found in close proximity to each other <ref type="bibr" coords="2,231.32,521.11,9.97,8.74" target="#b4">[5]</ref>. We define a sub-query as the ranking of the documents returned by the search engine when a query element is run through it.</p><p>The features used for learning the estimator are:</p><p>1. The overlap between each sub-query (a query based on one query term) and the full query. The overlap between two queries is defined as the size of intersection between the top N results of the two queries. Thus, the overlap is in the range of [0, N ]. 2. The rounded logarithm of the document frequency, log(DF ), of each of the sub-queries.</p><p>Learning to estimate query difficulty using the aforementioned data presents two challenges. The first is that the number of sub-queries is not constant, and thus the number of features for the estimator varies from query to query. In contrast, most techniques for estimation are based on a fixed number of features. The second problem in learning to estimate is that the significance of the sub-queries is not given, so any algorithm which performs a comparison or a weighted average on an ordered feature vector is not directly applicable.</p><p>In the following, we propose two solutions for these problems, based on finding a canonic representation for the data. The first solution bunches the features using a histogram, and the second one uses a modified tree-based estimator. We show that a histogram is useful when the number of sub-queries is large (i.e. there are many keywords and lexical affinities in the query). The tree-based classifier useful primarily for short queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query predictor using a histogram</head><p>The histogram-based algorithm generates the prediction in the following manner:</p><p>1. Find the top N results for the full query and for each of the sub-queries. 2. Build a histogram of the overlaps. Denote this histogram by h (i) , i = 0, 1, ..., N . Entry h (i) counts the number of sub-queries which agree with the full query on exactly i documents in the top N results. Unless otherwise stated, we used N=10. 3. Make the histogram into a binary histogram, that is:</p><formula xml:id="formula_0" coords="3,161.87,276.90,94.93,19.70">h b (i) = 1 h (i) &gt; 0 0 h (i) = 0</formula><p>4. Sort the binary histogram, retaining the rank of the ordered vector. For example, if h b = [1 0 0 1 1], the result of the sorting is r = [2 3 1 4 5] 5. Predict query difficulty by multiplying the resulting rank r, by a linear weight vector c such that P red = c T • r. The method by which this weight vector is computed is described below.</p><p>This algorithm can be improved significantly by using as features both the overlaps and log (DF ) of the terms. For canonical representation we split log (DF ) into 3 discrete values<ref type="foot" coords="3,469.86,371.92,3.97,6.12" target="#foot_0">1</ref> 0 -1, 2 -3, 4+. In this case, the algorithm is modified so that in stage ( <ref type="formula" coords="3,358.88,384.46,3.88,8.74">1</ref>), a two-dimensional histogram is generated, where entry h(i, j) counts the number of sub-queries with log (DF ) = i and j overlaps with the full query. For example, suppose a query has 4 sub-queries, an overlap vector ov(n) = [2 0 0 1] and a corresponding log (DF ) vector log (DF (n)) = [0 1 <ref type="bibr" coords="3,361.02,417.33,15.59,8.74">1 2]</ref>. The two-dimensional histogram for this example would be:</p><formula xml:id="formula_1" coords="3,126.00,444.87,136.49,43.61">h(DF, Overlap) = 0 1 2 0 0 0 1 1 2 0 0 2 0 1 0</formula><p>Before stage (3), the histogram is made into a vector by concatenating the lines of the histogram, corresponding to the overlap histogram at each log (DF ) of the term, one after the other. In the example above, the corresponding vector for the linear estimator (after making the histogram into a binary histogram and concatenating rows) is h(i) = [0 0 1 1 0 0 0 1 0];</p><p>The linear weight vector can be estimated in several ways, depending on the objective of the estimator. If the object of the estimator is to predict the P@10 or MAP of a query, a logical error function would be the Minimum Mean Square Error (MMSE), in which case the weight vector is computed using the Moore-Penrose pseudo-inverse <ref type="bibr" coords="3,296.58,580.17,12.71,8.74" target="#b5">[6]</ref>:</p><formula xml:id="formula_2" coords="3,292.96,596.85,265.04,13.11">c = R • R T -1 • R • t T (1)</formula><p>where R is a matrix whose columns are the ranking vectors r for the training queries, computed as described above, and t is a vector of the target measure (P@10 or MAP) for those queries. However, the objective might also be to rank the queries according to their expected P@10 or expected MAP (maximizing Kendall's-τ between estimated and actual order), in which case a more suitable optimization strategy is to modify the above equation as suggested in <ref type="bibr" coords="4,434.75,86.12,9.97,8.74" target="#b8">[9]</ref>. The idea is that a linear weight vector will maximize Kendall's-τ , if for each query i which is ranked higher than query j, we enforce the constraint c T • r i &gt; c T • r j . This constraint can be rewritten in the form: c T (r i -r j ) &gt; 0.</p><p>In practice, it is better to modify the constraint such that we demand that c T (r i -r j ) ≥ 1 in order to fix the scaling of c <ref type="bibr" coords="4,219.48,131.22,14.61,8.74" target="#b9">[10]</ref>. Therefore, instead of using the ranking vectors r directly, the matrix R is modified to a matrix whose columns are the differences between the ranking vectors; for each pair of training queries (i, j) the k-th column is:</p><formula xml:id="formula_3" coords="4,272.70,170.40,285.30,9.65">R k = r i -r j ∀i, j = 1, 2, . . . N r<label>(2)</label></formula><p>where N r is the number of training queries. In this case, the target vector is modified as follows:</p><formula xml:id="formula_4" coords="4,253.25,205.57,304.75,20.61">t k = +1 if t i &gt; t j -1 if t j ≤ t i ∀i, j = 1, 2, . . . N r<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query predictor using a modified decision tree</head><p>The histogram-based estimator can be useful only when enough data is available to construct it. If the query is short, there are few sub-queries with which to build it, and the resulting canonical representation is too sparse to be of use for estimation. In contrast, as we will show below, a treebased classifier can make use of much sparser data and thus it is useful for estimating the difficulty of shorter queries. The decision tree learning algorithm we present is similar to the CART algorithm <ref type="bibr" coords="4,126.00,317.48,9.97,8.74" target="#b1">[2]</ref>, with modification described below.</p><p>The suggested tree is a binary decision tree. Each node consists of a weight vector and a score. Sub-queries are sorted according to increasing DF. Starting at the root, the estimation algorithm moves along the branches of the tree, using one sub-query for deciding the direction of movement at each node. Movement is terminated when no more sub-queries exist or when a terminal node is reached. The estimation of query difficulty is the score at the terminal node. The algorithm takes a left branch if the multiplication of the weights at the current node by the overlap and logarithm of DF of the current sub-query is smaller than a threshold, or a right branch if it is larger than the threshold.</p><p>During training of the decision tree, a linear classifier (such as the above-mentioned Moore-Penrose pseudo-inverse) is trained at each node to try and split the number of training queries equally among the branches of the tree according to their rank (i.e., their respective ranking according to P@10 or MAP). This is done by training the weight vector of the classifier so that it gives a negative value when it is multiplied by the patterns from those queries with a lower rank and a positive value when multiplied by the patterns from those queries that have a higher rank. Thus, low-ranked queries will have a negative result and be classified to the left child node and those with the higher rank will have a positive value for the multiplication and be classified to the right child node.</p><p>Scores are assigned to nodes so that the left part of the tree will have a lower score compared to its right part. This is achieved by the following method: The root node has a score of 1. Taking a left branch implies a division of the score by 1.5, while a right branch multiplies it by 1.2. These values were found through a heuristic search.</p><p>It is well-known in literature that better decision trees can be obtained by training a multitude of trees, each in a slightly different manner or using different data, and averaging the estimated results of the trees. This concept is known as a Random Forest <ref type="bibr" coords="4,368.05,593.44,9.97,8.74" target="#b2">[3]</ref>. Thus, we trained 50 different trees using a modified resampling of the training data, obtained via a modification of the AdaBoost algorithm <ref type="bibr" coords="4,126.00,615.36,9.97,8.74" target="#b5">[6]</ref>. The modified AdaBoost algorithm resamples the training data to improve the final performance of the estimator. Assume each query from the training set is characterized by the pair {x, r}, where x is the data from a single training query (i.e., pairs of log (DF ) and overlaps for each sub-query, and r is the ranking (MAP or P10) of the query. The algorithm for training a single tree is shown below:</p><p>1. Initialize D = (x, r) 1 , . . . , (x, r) n , k max , δ (Minimum difference threshold), W 1 (i) = 1/n, i = 1, . . . , n, k ← 0. 2. Train a decision tree DT k using D sampled according to W k (i). 3. Measure the absolute difference in the location of each training example classified by the decision tree and its correct rank. Denote these differences by d(i). 4. Compute E k , the training error of DT k , measured on D, E k = i (d(i) &gt; δ). 5. Compute the importance of the hypothesis:</p><formula xml:id="formula_5" coords="5,149.14,164.15,85.15,27.46">α ← 1 2 ln (1-E k ) E k 6.</formula><p>Update the weights:</p><formula xml:id="formula_6" coords="5,149.14,192.77,240.48,48.19">W k+1 (i) ← W k (i) × e α if d(i) &lt; δ e -α if d(i) ≥ δ 7. Normalize weights: W k+1 (i) = W k+1 (i)/ i W k+1 (i) 8.</formula><p>Repeat stages 2-7 for k max iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluating the performance of the query predictor</head><p>The query prediction algorithm was tested using the Juru search engine <ref type="bibr" coords="5,448.21,284.60,10.51,8.74" target="#b3">[4]</ref> on two document collections from the TREC competition <ref type="bibr" coords="5,285.11,295.56,13.34,8.74" target="#b7">[8,</ref><ref type="bibr" coords="5,300.10,295.56,11.63,8.74" target="#b11">12]</ref>: The TREC collection and the WT10G collection. The TREC collection comprises of 528,155 documents. The WT10G collection encompasses 1,692,096 documents. The testing was performed using 200 TREC topics over the TREC collection and 100 topics from the Web track of TREC over the WT10G collection. Each topic contains two parts: A short (1-3 word) title and a longer description (Usually a complete sentence).We experimented with short queries based on the topic title and with long queries based on the topic description. Four-fold cross-validation was used for assessing the algorithm, i.e.: The topics were divided into four equal parts. A predictor was trained using three of the four parts, and was then applied to the remaining part for estimation of its performance. The process was repeated for each of the parts, and the quoted result is an average of the four results.</p><p>The set of queries used for evaluating our data, sorted by predicted MAP (or P10), can be thought of as a ranked list. The distance between the predicted ranked list and the real ranked list can be measured using Kendall's-tau (KT) distance.</p><p>The results of this test are shown in 1. Note that a random rank would result in a Kendall-tau distance of zero. Both predictors show a reasonable approximation of the correct ranking. Shorted queries (i.e. Title queries) are estimated better using the tree-based classifier, while long queries are addresses by the histogram-based classifier. The last rows of 1 showed the ability to use a single predictor for both databases. The results demonstrate that there was an improvement in performance through the use of both datasets. This is attributed to the fact that more data was available for building the predictors, and thus they performed better.</p><p>The weights calculated for the linear regression in the histogram-based predictor represent the relative significance of the entries in the binary histogram (h b (i) = 1 if there is a least one sub-query that agrees with the full query on i top results, otherwise h b (i) = 0). It is difficult to interpret the weights obtained by the linear regression. However, since there are a finite number of possible states that the binary histogram can have, some intuition regarding the weights (and the algorithm) can be gained by computing the estimated precision for each possible state of the histogram. If an overlap of 10 is considered between the full query and the sub-queries, there are a total of 2 11 = 2048 possible histogram states, corresponding to 2 11 different binary histograms.</p><p>Thus we computed the predicted P10 for each such state using the linear prediction vector obtained using the description part of 200 topics from the TREC collection, ordered the states according to the prediction, and averaged each group of 128 consecutive states. The average histogram values for several slices (Each slice is an average of 128 individual histograms) are shown in figure <ref type="figure" coords="5,501.14,659.20,3.88,8.74" target="#fig_0">1</ref>. This figure demonstrates that the easy queries have some overlaps between sub-queries and the full query at both high and medium ranges of overlap. Queries that perform poorly have their overlaps clustered in one area of the histogram, usually the lower one. This suggests that a good pattern of overlap would be one where the query is not dominated by a single keyword. Rather, all (or at least most) keywords contribute somewhat to the final results. 4 Experiments in the Robust Track: Improving information retrieval using query prediction</p><p>As mentioned above, query prediction is useful not only as feedback to the user, but also as a method for improving information retrieval. In this section we describe several ways in which query prediction was used for this goal.</p><p>The results of these methods on the older 200 TREC queries (301-450, 601-650) are shown in table <ref type="table" coords="6,126.00,549.50,3.88,8.74">2</ref>. These results were obtained using four-fold cross-validation. Figure <ref type="figure" coords="6,428.86,549.50,4.98,8.74">2</ref> shows a comparison of these results and other TREC participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Description of the runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base runs</head><p>We performed five basic runs using Juru without any modification. These runs were:</p><p>1. Title-only run (Run name: JuruTit ). 2. Description-only run (Run name: JuruDes ).</p><p>3. Title and description run (Run name: JuruTitDes ).</p><p>4. Title-only with Query Expansion run (Run name: JuruTitQE ).</p><p>5. Description-only with Query Expansion run (Run name: JuruDesQE ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective query expansion</head><p>Query expansion (QE) is a method for improving retrieval by adding terms to the query based on frequently-appearing terms in the best documents retrieved by the original query. However, this technique works only if the search engine was able to give a high rank to relevant documents using the original query. If this is not the case, QE will add irrelevant terms, causing a decrease in performance <ref type="bibr" coords="7,126.00,198.47,9.97,8.74" target="#b4">[5]</ref>.</p><p>Thus, it is not beneficial to use QE on every query. Instead, it is advantageous to have a switch that will predict when QE will improve retrieval, and when it would be detrimental to it.</p><p>Using the same features utilized for the histogram-based predictor we trained an SVM classifier <ref type="bibr" coords="7,547.48,243.30,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="7,126.00,254.26,12.73,8.74" target="#b10">11]</ref> to decide when to use QE and when not to. The SVM was trained with an RBF (Gaussian) kernel, with a width of 0.5.</p><p>There were two queries of this type. In the first the input was the description part of the topic. This run is labeled JuruDesSwQE . In the second the input was the title part of the topic. This run is labeled JuruTitSwQE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term selection</head><p>An alternative use for the predictor is to discover queries for which the search engine will return no or very few relevant results. Our working assumption was that if this is the case in a description query, the cause is likely to be noise arising from the length of the query. Therefore, for such queries, only a few terms were chosen in the query and the remaining terms were filtered out completely. Terms were chosen by two methods attempted to keep only terms which are similar in order to focus on the topic of the query. One used a hand-crafted rule and the other a tree-based classifier similar to the one described for the prediction:</p><p>1. In the hand-crafted method, for each query, the similarity between each pair of terms was calculated. The similarity measure used was a vector cosine between the terms (represented as sets of documents) divided by the log-distance between the occurrences of the terms in the query. The two pairs with the highest similarities were chosen, meaning either three or four terms.</p><p>2. In the automatic method, for each query, each term was given a list of features. This list included its occurrence's offset in the query, its vector cosine similarity to each term in the query and the distance between its occurrence and the occurrences of each term in the query.</p><p>Several classified examples were used to train a tree-based classifier which was then used to choose the terms in each query.</p><p>Terms which were chosen by either of the methods were kept and those which were chosen by neither were removed from the query.</p><p>The input for this run was the description part of the topic. This run is labeled JuruDesTrSl .</p><p>Deciding which part of the topic should be used TREC topics contain two relevant parts: The short title and the longer description. Our observations have shown than in general queries that are not answered well by the description and better answered by the title part of the topic.</p><p>The predictor was used to decide which part of the topic should be used. The title part of the topic was used for the queries ranked (by the predictor) in the lower 15% of the topics, while the description part was used for the remaining 85% of topics.</p><p>This run is labeled JuruTitSwDs .</p><p>Vary the parameters of the search engine according to query prediction</p><p>The Juru search engine uses both keywords and lexical affinities for ranking documents. Usually each lexical affinity is given a weight of 0.25 compared to 0.75 for regular keywords. However, this value is an average that can address both difficult and easy queries. In fact, difficult queries can be answered better by assigning greater weight to lexical affinity, while easy queries are improved by assigning lexical affinities a lower weight.</p><p>Unfortunately, the predictor is not sensitive enough to be used to modify these weights across the whole spectrum of difficulty. Instead we use a weight of 0.1 for the queries ranked (by the predictor) in the top 15% of the queries, while the regular weight of 0.25 was used for the remaining 85% of queries.</p><p>The input for this run was the description part of the topic. This run is labeled JuruDesLaMd .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank aggregation</head><p>Rank aggregation is a method designed to combine ranking results from various sources. The main applications include building meta-search engines, combining ranking functions, selecting documents based on multiple criteria, and improving search precision through word associations. Previous attempts <ref type="bibr" coords="8,160.06,346.90,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="8,172.24,346.90,7.75,8.74" target="#b8">9]</ref> to implement this method were somewhat "passive" in their design of the functions that provide the ranking results, and instead focus on techniques that combine the ranks provided by the available sources.</p><p>Our rank aggregation method adapted a more "active" approach: Similarly to our query prediction technique, we automatically constructed a set of sub-queries, based on the original query elements (keywords and lexical affinities), and used Juru to get ranking scores for every sub-query. We then used the resulting rank scores of the sub-queries as features in a training set, namely each document was represented as a vector of scores. Finally, we trained a non-linear SVM to learn how to re-rank documents (employing the strategy suggested in <ref type="bibr" coords="8,346.67,440.55,10.29,8.74" target="#b8">[9]</ref>), where the reference order for training was based on the original (full) query ranks. Thus, the learning system did not get the true order at any time, and in essence this scheme may be viewed as an "unsupervised learning" framework.</p><p>The input for this run was the description part of the topic. This run is labeled JuruDesAgg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" coords="8,153.51,526.69,4.98,8.74">2</ref> shows the results of running the methods described above on the older 200 TREC queries, using four-fold cross-validation. Note that for a fair comparison, runs should be compared according to the parts of the topics they use.</p><p>Selective QE is the most efficient method for improving queries based solely on the description. When both topic parts are used, it is evident that the switch between topic parts is better than using both at the same time or just one of the parts.</p><p>Figure <ref type="figure" coords="8,158.84,604.40,4.98,8.74">2</ref> shows a histogram of the difference between the P10 obtained using the experimental methods and the median score of all the 2004 Robust Track runs. As this figure shows none of the methods performed as good as the best TREC submission. Of our runs, the best runs were the run which applied a switch between regular runs and those that used QE and the term-selection runs. However, the differences between all the runs were relatively minor, with an average difference of only 0.33.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,126.00,369.39,432.00,7.89;6,126.00,379.38,432.00,7.86;6,126.00,389.34,432.00,7.86;6,126.00,399.30,432.00,7.86;6,126.00,409.27,432.00,7.86;6,126.00,419.23,242.63,7.86;6,231.21,296.70,72.01,58.66"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of the average weights as a function of the predicted P10. The leftmost figure shows a pattern which generated lower prediction for P10 (An average P10 of 3.1). This figure shows that a low prediction is generated when most overlaps are low, specifically here most sub-queries have an overlap of 2. The center figure are the average weights of representing an average predicted P10 of 7.3 and the rightmost figure the average weights representing a predicted P10 of 9.96. In all three figures the horizontal axis is the number of overlaps and the vertical axis the average weight.</figDesc><graphic coords="6,231.21,296.70,72.01,58.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,145.23,75.19,390.47,112.38"><head>Table 1 .</head><label>1</label><figDesc>Kendall-tau scores for query prediction evaluated on the TREC and the Web datasets.</figDesc><table coords="6,363.34,75.19,96.17,6.14"><row><cell>Tree</cell><cell>Histogram</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,135.96,659.88,253.45,7.86"><p>In cases where the DF of a term is zero, we take log (DF ) = 0.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="9,155.04,175.69,4.12,7.89">2</ref>. Improvements in retrieval using different techniques. The measures used for assessing the performance are MAP, P@10, and % no. The last are the percentage of queries for which there was not a single relevant document in the top 10 documents. The Kendall-tau score between the MAP obtained in our 10 runs and the predictions generated by the suggested algorithms is shown in table <ref type="table" coords="9,317.29,466.51,4.98,8.74">3</ref> (The displayed results are different from the original submitted runs due to a bug discovered after the runs were submitted). As can be seen the histogrambased predictor achieved good, consistent, predictions. Note the similarity between Kendall-tau for the older 200 queries and the 49 new queries, which demonstrate the robustness of the histogram predictor. The results of these predictions on all queries are much better than the median of all runs submitted to the Robust Track (At this time we do not yet know the median and best prediction as measured over the 49 new queries). The tree-based predictor obtained inferior results. This is likely not the results of over-fitting, since the prediction we submitted was the results of four-fold cross-validation. Instead, we suspect that this is due to an inherent difficulty in prediction of short queries. As shown above, this difficulty can be mitigated by using more training data (i.e. more queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>Our experiments in the Robust Track of TREC centered on predicting query difficulty and using this prediction for improving information retrieval. We developed two algorithms for predicting query difficulty and used this prediction in several novel ways. Our results demonstrate that query prediction is easier for longer queries compared to shorter ones. Our histogram-based predictor is able to predict query difficulty very well, but the use of these predictions for significant improvement of retrieval tasks is still an open issue for research.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,134.19,297.98,423.80,7.86;10,142.76,307.95,415.25,7.86;10,142.76,317.91,195.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,312.36,297.98,245.64,7.86;10,142.76,307.95,38.06,7.86">Query difficulty, robustness and selective application of query expansion</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.05,307.95,348.76,7.86">Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004)</title>
		<meeting>the 25th European Conference on Information Retrieval (ECIR 2004)<address><addrLine>Sunderland, Great Britain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,331.86,423.81,7.86;10,142.76,341.82,61.18,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,380.04,331.86,132.61,7.86">Classification and regression trees</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,355.77,283.28,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,201.13,355.77,61.97,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,270.75,355.77,70.87,7.86">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,369.72,423.80,7.86;10,142.76,379.68,415.24,7.86;10,142.76,389.64,298.12,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,539.42,369.72,18.58,7.86;10,142.76,379.68,197.25,7.86">Juru at TREC 10 -Experiments with Index Pruning</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Einat</forename><surname>Amitay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miki</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoelle</forename><forename type="middle">S</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yael</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,364.09,379.68,193.91,7.86;10,142.76,389.64,39.14,7.86">Proceeding of Tenth Text REtrieval Conference (TREC-10</title>
		<meeting>eeding of Tenth Text REtrieval Conference (TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology. NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,403.59,423.81,7.86;10,142.76,413.55,415.25,7.86;10,142.76,423.51,402.95,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,393.32,403.59,164.69,7.86;10,142.76,413.55,161.76,7.86">Automatic query refinement using lexical affinities with maximal information gain</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eitan</forename><surname>Farchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yael</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,324.74,413.55,233.27,7.86;10,142.76,423.51,260.60,7.86">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,437.46,423.81,7.86;10,142.76,447.42,45.56,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,310.11,437.46,84.15,7.86">Pattern classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New-York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,461.37,423.81,7.86;10,142.76,471.33,63.62,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,366.28,461.37,109.64,7.86">Rank aggregation revisited</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,500.15,461.37,57.85,7.86;10,142.76,471.33,32.96,7.86">Proceedings of WWW10</title>
		<meeting>WWW10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,485.28,423.80,7.86;10,142.76,495.25,415.25,7.86;10,142.76,505.21,160.32,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,267.35,485.28,162.00,7.86">Overview of the TREC-2001 Web Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.84,495.25,248.44,7.86">Proceedings of the Tenth Text Retrieval Conference (TREC-10</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text Retrieval Conference (TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology. NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,134.19,519.16,423.81,7.86;10,142.76,529.12,379.31,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,197.31,519.16,199.44,7.86">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,415.27,519.16,142.73,7.86;10,142.76,529.12,199.67,7.86">Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<publisher>Association of Computer Machinery</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.85,543.07,424.14,7.86;10,142.76,553.03,237.81,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,266.34,543.07,291.65,7.86;10,142.76,553.03,65.14,7.86">Leaning with kernels: Support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.85,566.98,424.15,7.86;10,142.76,576.94,109.32,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,251.10,566.98,212.86,7.86">Computer manual to accompany pattern classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New-York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.85,590.89,424.14,7.86;10,142.76,600.85,415.24,7.86;10,142.76,610.81,104.19,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,291.69,590.89,248.56,7.86">Overview of the Tenth Text REtrieval Conference (TREC-10)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,142.76,600.85,253.79,7.86">Proceedings of the Tenth Text Retrieval Conference (TREC-10</title>
		<meeting>the Tenth Text Retrieval Conference (TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
