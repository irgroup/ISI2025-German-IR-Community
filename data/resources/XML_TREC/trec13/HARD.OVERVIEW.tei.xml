<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,172.27,112.05,267.49,15.11;1,161.58,133.97,288.83,15.11">HARD Track Overview in TREC 2004 High Accuracy Retrieval from Documents</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_ZBwxqeG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_VyMMFdE">
					<orgName type="full">SPAWARSYSCEN-SD</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,274.51,166.45,62.98,10.48"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,172.27,112.05,267.49,15.11;1,161.58,133.97,288.83,15.11">HARD Track Overview in TREC 2004 High Accuracy Retrieval from Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D557D3A8FD743AD81E5604F4524BB77D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The HARD track of TREC 2004 aims to improve the accuracy of information retrieval through the use of three techniques: (1) query metadata that better describes the information need, (2) focused and time-limited interaction with the searcher through "clarification forms", and (3) incorporation of passage-level relevance judgments and retrieval. Participation in all three aspects of the track was excellent this year with about 10 groups trying something in each area. No group was able to achieve huge gains in effectiveness using these techniques, but some improvements were found and enthusiasm for the clarification forms (in particular) remains high. The track will run again in TREC 2005.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The High Accuracy Retrieval from Documents (HARD) track explores methods for improving the accuracy of document retrieval systems. It does so by considering three questions:</p><p>1. Can additional metadata about the query, the searcher, or the context of the search provide more focused and therefore accurate results? These metadata items generally do not directly affect whether or not a document is on topic, but they do affect whether it is relevant. For example, a person looking for introductory material will not find an on-topic but highly technical document relevant.</p><p>2. Can highly focused, short-duration, interaction with the searcher be used to improve the accuracy of a system? Participants created "clarification forms" generated in response to a query-and leveraging any information available in the corpus-that were filled out by the searcher. Typical clarification questions might ask whether some titles seem relevant, whether some words or names are on topic, or whether a short passage of text is related.</p><p>3. Can passage retrieval be used to effectively focus attention on relevant material, increasing accuracy by eliminating unwanted text in an otherwise useful document? For this aspect of the problem, there are challenges in finding relevant passages, but also in determining how best to evaluate the results.</p><p>The HARD track ran for the second time in TREC 2004. It used a new corpus and a new set of 50 topics for evaluation. All topics included metadata information and clarification forms were considered for each of them. Because of the expense of sub-document relevance judging, only half of the topics were used in the passage-level evaluation. A total of 16 sites participated in HARD, up from 14 the year before. Interest remains strong, so the HARD track will run again in TREC 2005, but because of funding uncertainties will only address a subset of the issues. Exactly what is included and how it takes place will be determined by interested participants. Information about the track will be available at the track's Web page, http://ciir.cs.umass.edu/research/hard (the contents of the site are not predictable after 2005).</p><p>Topic creation, clarification form entry, and relevance judging were all carried out by the Linguistic Data Consortium (LDC) at the University of Pennsylvania (http://www.ldc.upenn.edu). The annotation work was supported in part by the DARPA TIDES project. Evaluation of runs using the judgments from the LDC was carried out by NIST. The remainder of this document discusses the HARD 2004 track and provides an overview of some of its results. Additional details on results are available in the TREC papers from the participating sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HARD Corpus</head><p>The HARD 2004 evaluation corpus itself consisted entirely of English text from 2003, most of which is newswire. The specific sources and approximate amounts of material are: This information was made available to participating sites with a research license. The data was provided free of charge, though sites interested in retaining the data after the HARD track ended were required to make arrangements with the LDC to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>Topics were an extension of typical TREC topics: they included (1) a statement of the topic and (2) a description of metadata that a document must satisfy to be relevant, even if it is on topic. The topics were represented in XML and included the following components:</p><p>• number is the topic's number-e.g., HARD-003.</p><p>• title is a short, few word description of the topic.</p><p>• description is a sentence-length description of the topic.</p><p>• topic-narrative is a paragraph-length description of the topic. This component did not contain any mention of metadata restrictions. It is intended purely to define what is "on topic."</p><p>• metadata-narrative is a topic author's description of how metadata is intended to be used. This description helps make it clear how the topic and metadata were intended to interact.</p><p>• retrieval-element indicates whether the judgments (hence retrieval) should be at the document or passage level. For HARD 2004, half of the topics were annotated at the passage level.</p><p>• The following metadata fields were provided:</p><p>familiarity had a value of little or much. It affected whether a document was relevant, but not whether it was on topic.</p><p>genre had values of news-report, opinion-editorial, other, or any. It affected whether a document was relevant, but not whether it was on topic.</p><p>geography had values of US, non-US, or any. It affected whether a document was relevant, but not whether it was on topic.</p><p>subject describes the subject domain of the topic. It is a free-text field, though the LDC attempted to be consistent in the descriptions it used. It affected whether or not a document was on-topic.</p><p>related-text.on-topic provided an example of text that the topic's author considered to be on-topic but not relevant.</p><p>related-text.relevant provided an example of text that the topic's author considered to be relevant (and therefore also on-topic).</p><p>During topic creation, the LDC made an effort to have topics vary across each of the indicated metadata items.</p><p>The following is a sample topic from the evaluation corpus (topic HARD-428). Some particularly long sections of the topic have been elided. &lt;topic&gt; &lt;number&gt; HARD-428 &lt;/number&gt; &lt;title&gt; International organ traffickers &lt;/title&gt; &lt;description&gt; Who creates the demands in the international ring of organ trafficking? &lt;/description&gt; &lt;topic-narrative&gt; Many countries are institutionalizing legal measures to prevent the selling and buying of human organs. Who, in the ring of international organ trafficking, are the "buyers" of human organs? Any information that identifies 'where' they are or 'who' they may be will be considered on topic; the specificity of info does not matter. Also, the story must be about international trafficking. Stories that only contain information about the "sellers" of organs or those that focus on national trafficking will be off topic. &lt;/topic-narrative&gt; &lt;metadata-narrative&gt; Subject (CURRENT EVENTS) is chosen as it is expected that such articles will have more information about the identities of the parties involved. Genre (NEWS) is expected to exclude stories that tends to focus on ethical matters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relevance judgments</head><p>For each topic, documents that are annotated get one of the following judgments:</p><p>• OFF-TOPIC means that the document does not match the topic. (As is common in TREC, a document without any judgment is assumed to be off topic for evaluation purposes.)</p><p>• ON-TOPIC means that the document does match the topic but that it does not satisfy the provided metadata restrictions. Given the metadata items listed above, that means it either does not satisfy the FAMILIARITY, GENRE, or GEOGRAPHY items (note that SUBJECT affects whether a story is on topic).</p><p>• RELEVANT means that the document is on topic and it satisfies the appropriate metadata.</p><p>In addition, if the retrieval element field is passage then each judgment comes with information that specifies which portions of the documents are relevant.</p><p>To specify passages, HARD used the same approach used by the question answering track <ref type="bibr" coords="4,468.26,556.32,69.18,8.74" target="#b10">[Voorhees, 2005]</ref>. A passage is specified by its byte offset and length. The offset will be from the "&lt;" in the "&lt;DOC&gt;" tag of the original document (an offset of zero would mean include the "&lt;" character). The length will indicate the number of bytes that are included. If a document contains multiple relevant passages, the document will be listed multiple times.</p><p>The HARD track used the standard TREC pooling approach to find possible relevant documents. The top 85 documents from one baseline and one final run from each submitted system were pooled (i.e., 85 times 16 times 2 documents). The LDC considered each of those documents as possibly relevant to the topic.</p><p>Across all topics, the LDC annotated 36,938 documents, finding 3,026 that were on topic and relevant and another 744 that were on topic but not relevant. Topics ranged from one on topic and relevant document to 519; from 1 on topic but not relevant document to 70.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training data</head><p>The LDC provided 20 training topics and 100 judged documents per topic. The topics incorporated a selection of metadata values and came with relevance judgments.</p><p>In addition, the LDC provided a mechanism to allow sites to validate their clarification forms. Sites could send a form to the LDC and get back confirmation that the form was viewable and some "random" completion of the form. The resulting information was sent back to the site in the same format that was used in the evaluation. (No one took advantage of such a capability.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Clarification forms</head><p>A unique aspect of the HARD track is that it provides access to the person who formulated the query and will be doing the annotation. It allows sites to get a small amount of additional information from that person by providing a small Web page as a form with clarification questions, check boxes, etc. for the searcher to fill in.</p><p>The assessor spent no more than three (3) minutes filling out the form for a particular topic. If some portions of a form were not filled out when the time expired, those portions were left blank. Sites were aware of the time limit and were encouraged to keep their forms small-however, several (perhaps most) sites built longer forms intending to get whatever they could within three minutes rather than building forms designed to be filled in quickly.</p><p>In order to avoid implementation issues, systems were required to restrict the forms to simple HTML without Javascript, images, and so on. They were also told what would be the hardware configuration used by annotators, so they could tailor the presentation appropriately if desired.</p><p>The LDC reported that the annotators enjoyed filling out clarification forms immensely-if only because it was an entirely new type of annotation task for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results format</head><p>Results were returned for evaluation in standard TREC format extended, though, to support passage-level submissions since it possible that the searcher's preferred response is the best passage (or sentence or phrase) of relevant documents. Results included the top 1000 documents (or top 1000 passages) for each topic, one line per document/passage per topic. Each line had the format: topic-id Q0 docno rank score tag psg-offset psg-length where:</p><p>• topic-id represents the topic number from the topic (e.g., HARD-001)</p><p>• "Q0" is a constant provided for historical reasons</p><p>• docno represents the document that is being retrieved (or from which the passage is taken)</p><p>• rank is the rank number of the document/passage in the list. Rank should start with 1 for the document/passage that the system believes is most likely to be relevant and continue to 1000.</p><p>• score is a system-internal score that was assigned to the document/passages. High values of score are assumed to be better, so score should generally drop in value as rank increases.</p><p>• tag is a unique identifier for this run by the site.</p><p>• psg-offset indicates the byte-offset in document docno where the passage starts. A value of zero represents the "&lt;" in "&lt;DOC&gt;" at the start of the document. A value of negative one (-1) means that no passage has been selected and the entire document is being retrieved.</p><p>• psg-length represents how many bytes of the document are included in the passage. A value of negative one (-1) must be supplied when psg-offset is negative one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation approach</head><p>Results were evaluated at the document level, both in light of (HARD) and ignoring (SOFT) the query metadata. Ranked lists were also evaluated incorporating passage-level judgments. We discuss each evaluation in this section.</p><p>Five of the 50 HARD topics <ref type="bibr" coords="6,216.21,132.84,129.64,8.74">(401, 403, 433, 435, and 450)</ref> had no relevant (and on topic) documents. That is, although there were documents that matched the topics, no document in the pool matched the topic and the query metadata. Accordingly, those five topics were dropped from both the HARD and SOFT evaluations. (They could have been kept for the SOFT evaluation, but then the scores of the two evaluations would not have been comparable.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Document-level evaluation</head><p>In the absence of passage information, evaluation was done using standard mean average precision. There were two variants, one for HARD judgments and one for SOFT.</p><p>Some of the runs evaluated in this portion were actually passage-level runs and could therefore include a document at multiple points in the ranked list-i.e., because more than one passage was considered likely to be relevant. For the document-level evaluation, only the first occurrence of a document in the ranked list was considered. Subsequent occurrences were "deleted" from the ranked list. (That meant that it was possible for a site to submit 1000 items in a ranked list, but have fewer than 1000 documents ranked.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Passage-level evaluation</head><p>Two passage measures were explored for HARD 2004. The first was the same one used in HARD 2003, passage R-precision. Some research at UMass Amherst demonstrated an extremely strong bias in favor of short passages, so a second measure was also explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Passage R-Precision</head><p>In a nutshell, this evaluation measure considers the "true" relevant R passages as found by annotators. It considers the top R passage returned by a system and counts the proportion of characters that overlap relevant passages. It incorporates a penalty for repeating text in multiple passages. More details are provided below.</p><p>The passage level evaluation for a topic consists of values for passage recall, passage precision, and the F score at cutoff 5, 10, 15, 20, 30, 50, and 100, plus a R-precision score. As with standard document level evaluation, a cutoff is the rank within the result set such that passages at or above the cutoff are "retrieved" and all other passages are not retrieved. So, for example, if the cut-off is 5 the passage recall and precision are computed over the top 5 passages. R-precision is defined similarly to the document level counterpart: it is the passage precision after R passages have been retrieved where R is the number of relevant passages for that topic. We are using passage R-precision as an evaluation measure reported for the track because it is a cutoff-based measure that tracks mean average precision extremely closely in document evaluations.</p><p>The following is an operational definition of passage recall and precision as used in the evaluation. For each relevant passage allocate a string representing all of the character positions contained within the relevant passage (i.e., a relevant passage of length 100 has a string of length 100 allocated). Each passage in the retrieved set marks those character positions in the relevant passages that it overlaps with. A character position can be marked at most once, regardless of how many different retrieved passages contain it. (Retrieved passages may overlap, but relevant passages do not overlap.) The passage recall is then defined as the average over all relevant passages of the fraction of the passage that is marked. The passage precision is defined as the total number of marked character positions divided by the total number of characters in the retrieved set. The F score is defined in the same way as for documents, assigning equal weight to recall and precision: F = (2*prec*recall)/(prec+recall) where F is defined to be 0 if prec+recall is 0. We included the F score because set-based recall and precision average extremely poorly but F averages well. R-precision also averages well.</p><p>In all of the above, a document is treated as a (potentially long) passage. That is, the relevant "passage" starts at the beginning of the document and is as long as the document. (These are represented in the judgment file as passages with -1 offset and -1 length, but are treated as described above.) For any topic, a retrieved document (i.e., where offset and length are -1) is again just a passage with offset 0 and length the length of the document.</p><p>Using the above definition of passage recall, passage recall and standard document level recall are identical when both retrieved and relevant passages are whole documents. That is not true for this definition of passage precision. Passage precision will be greater when a shorter irrelevant document is retrieved as compared to when a longer irrelevant document is retrieved. This makes sense, but is different from standard document level precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Passage-level bpref</head><p>Some explorations at UMass Amherst showed that passage R-precision could be improved dramatically by splitting existing passages into smaller pieces. For example, by splitting the top-ranked passages into 32 pieces and then using the top R of those (rather than the top R original passages), the value of passage R-precision increased by 128%.</p><p>Although numerous measures were considered, a variation of bpref <ref type="bibr" coords="7,391.84,274.86,123.27,8.74" target="#b2">[Buckley and Voorhees, 2004</ref>] was finally selected. In this measure, the top 12,000 characters of the system's ranked list of passages was considered (intended to correspond roughly to 10 normal sized passages).</p><p>As a document evaluation measure, bpref considers two sets of documents: a relevant set and a nonrelevant set. The assumption is that if a document A is taken from the first set and B is taken from the second, then the user has a binary preference that A be ranked higher than B. The measure counts the proportion of times that the user's implied set of preferences is satisfied. A perfect system would rank all known relevant documents above all known non-relevant documents, would thereby satisfy all of the user's preferences, and receive a score of 1.0. The worst possible score is zero, and systems will normally score somewhere in the middle.</p><p>To extend this measure to passages, we consider character-level preferences. We assert that all relevant characters should be presented before any non-relevant characters and count the proportion of preferences that are satisfied. Note that the choice of character as the base unit is arbitrary and made for reasons of simplicity. It could have been word, phrase, or even sentence, but each of those would require algorithmic decisions about boundaries between units that are not necessary for character-level decisions. We believe (though have not investigated) that different units will merely change the scale of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Protocol</head><p>The HARD 2004 track ran from May through August of 2004. On June 25th, sites received the 50 evaluation topics, but without any of the metadata fields provided. That is, they received just the title, description, and narrative information, a format consistent with past "ad hoc" TREC tracks.</p><p>Using that base information, sites were asked to do their best to rank documents for relevance and return the ranked list of documents (not passages). These were the "baseline runs" and were due to NIST on July 9th.</p><p>In addition, sites could optionally generate up to two clarification forms that the LDC annotators would fill out. These forms were due to the LDC on July 16th</p><p>On July 29th, the filled-out forms were returned to sites and the metadata fields of the topics were released to all sites, regardless of whether they used clarification forms. Sites could use any of that information to produce improved ranked lists. The final runs, incorporating everything they could, were due to NIST on August 5th.</p><p>As described above, one baseline run and one final run were used from each site. The top 85 documents from each of those runs were pooled together and used by the LDC for judging. For topics that required passage-level judgment, the annotator marked passages as relevant as soon as a relevant document was found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Participation</head><p>The following 16 sites participated in the HARD track of TREC 2004. The first three columns indicate whether the site used metadata values, clarification forms, or passage retrieval in any of their submitted runs. (No information was reported for Tsinghua University or the University of Cincinnati, and they did not provide a paper on this track to TREC for publication.) It is interesting to note the wide range of ways that the different purposes of the track were exploited. Only three sites used all three possible components of the track. The clarification forms were the most popular, but not by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta CF Psgs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Results</head><p>This section provides a sketch of some of the results found by participating sites. Further and more detailed information is available in the sites individual papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Use of metadata</head><p>For the most part, sites built models for the geography, genre, and subject metadata categories. They typically used text classification techniques to decide whether a document matched the category. Some sites used the Web to collect more data relevant to the category. And some built manual term lists for classification (mostly for geography information).</p><p>In general, sites were unable to demonstrate substantial gains in effectiveness using metadata. Since metadata differentiated between relevant and merely on-topic documents, a run using metadata should score much better on "hard" measures (where only relevant documents are counted as relevant) and "soft" measures (where on-topic documents are also counted as relevant). Several runs were able to improve in that direction, though not by huge margins.</p><p>Some of these results are because topics tended not to require the metadata to improve performance. For example, AIDS in Africa is obviously a non-US topic, and being told that it is not US is of little value.</p><p>The University of North Carolina asked (in clarification forms) the user how many times they had searched before for each topic. They then showed that users who had claimed low familiarity in metadata also had not previously searched often for this topic. They did not use the metadata to aid retrieval, but cleverly used the clarification form to show how familiarity metadata could be collected <ref type="bibr" coords="8,421.16,703.23,78.51,8.74" target="#b7">[Kelly et al., 2005]</ref>.</p><p>The University of Waterloo also did not use metadata for retrieval, but did a very nice analysis using the familiarity metadata. Users with low familiarity selected fewer phrases in Waterloo's clarification forms. User's with low familiarity were helped by the clarification forms but users with much familiarity were hurt <ref type="bibr" coords="9,72.00,111.02,170.69,8.74" target="#b9">[Vechtomova and Karamuftuoglu, 2005]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Use of clarification forms</head><p>Clarification forms allowed sites to ask the user anything about the topic that could be expressed in simple HTML. Most requested information asked for judgments on keywords, documents, or passages. One site asked whether presented passages were of about the right length, presumably to get a handle on the right amount of information that should be returned. Several sites included free-form entry of phrases, other keywords, or related text at the end of their clarification forms.</p><p>When sites asked for keywords, they had usually found words or phrases that their system suspected were related to the topic. These might be words or phrases appearing in top-ranked documents, synonyms of query words found using Wordnet (for example), extracted noun phrases or named entities, or ranges of time that where relevant material would appear.</p><p>Document-style requests generally asked for a judgment of relevant for the passage. That was often the title and a few keywords from a document, the passage most likely to be relevant ("best passage"), or a cluster of documents represented by titles and/or key words. The set of documents, passages, or clusters chosen for presentation were either the top-ranked set or a set modified to incorporate some notion of novelty-i.e., do not present two highly similar documents for judgment.</p><p>Clarification forms were very popular, very fun, provided an open ended framework for experimentation, and were by those counts very successful. On the other hand, most sites limited themselves to keyword and text relevance feedback rather than trying more novel techniques, so the "open ended" nature has not (yet) encouraged new ideas.</p><p>The value of clarification forms remains elusive to determine. Many sites saw some gains from their clarification forms, but there were several sites that achieved their best performance-or nearly their beston the baseline runs. Unquestionably work should consider on clarification forms because they are popular, though until more impressive gains are seen, their value will debatable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Use of passages</head><p>As described in Section 8, two measures for passage retrieval were considered, but others were compared. Two get a sense of how similar they were, we investigated the correlation between bpref at 12,000 characters. (That measure was declared "primary" in the track guidelines, but sufficiently late in the process that some sites fit to the passage R-precision measure.)</p><p>• Precision at 12,000 characters measured the proportion of characters that were relevant in the top 12,000 characters. It showed a 99% correlation.</p><p>• Character R-precision (similar to passage R-precision, but a character-oriented evaluation where R is the total number of relevant characters not passages). It showed an 88% correlation.</p><p>• Passage F1 at 30 passages retrieved showed a 90% correlation.</p><p>• Passage precision at 10 passages showed an 80% correlation.</p><p>• Passage R-precision (last year's official measure) showed a 45% correlation.</p><p>If nothing else, these results should suggest that sites training their systems to optimize passage R-precision should not be expected to do well on the character bpref measure. Passage retrieval systems often use fixed-length passages of some number of words or characters, treating those passages as if they were documents. Some sites tried to generate appropriately sized passages using HMMs, retrieving and then merging highly ranked adjacent sentences, or looking for runs of text where the query terms are highly dense. Most sites scored passages and then combined the passage score with the document score in one way or another.</p><p>There was substantially more activity in passage retrieval for HARD 2004 than in 2003. However, the issue of how best to resolve variable-length passage retrieval with variable-length passage "truth" judgments remains open and begs for substantially more exploration. There are clear problems with the passage Rprecision measure, but the character bpref is also not without issues. Unfortunately, the HARD 2005 track will be dropping passage retrieval because of funding issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Overall results</head><p>When measured by topicality (i.e., when on-topic and/or relevant documents are the target), the top runs were all automatic and used both the title and description. Some top runs used clarification forms, passage retrieval, and the (hard) related text information. A few top runs used the geography and genre metadata fields and a couple used the topic narrative and (soft) related text.</p><p>When measured by relevance (i.e., only relevant documents were the target), the top runs used similar information, though all top runs used the (hard) related text.</p><p>For passage retrieval evaluation, the best runs were usually automatic (though the second ranked run was manual), used the title and scription, incorporated a clarification form, and did passage retrieval. Interestingly, the fifth ranked run was a document run with no passages marked. Some sites were able to find advantage to the geography and genre metadata, and some used related text and narrative. Note that related text (of both kinds) was more often used in top performing document retrieval systems than in top performing passage retrieval systems.</p><p>No top run by any of the measures used the familiarity field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusion</head><p>The second year of the HARD track appears to have been much more productive for most sites. With better training data and a clearer task definition earlier, groups were able to carry out more careful and interesting research.</p><p>The HARD track will continue in TREC 2005. Funding considerations have forced the removal of passage retrieval from the evaluation. Topics deemed by the Robust track to be difficult will be used rather than developing new topics, though they will be judged against a new corpus. Familiarity metadata will be collected, but not used in any particular way by the annotators.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The coordination of the HARD track at the CIIR would not have been possible without the help of <rs type="person">Fernando Diaz</rs>, <rs type="person">Mark Smucker</rs>, and <rs type="person">Courtney Wade</rs>. The way the track was organized was by consensus of participating researchers and they all deserve credit for the shape the track eventually took.</p><p>The work at the CIIR was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by <rs type="funder">SPAWARSYSCEN-SD</rs> grant number <rs type="grantNumber">N66001-02-1-8903</rs>, and in part by <rs type="funder">NSF</rs> grant number <rs type="grantNumber">IIS-9907018</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the author's and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VyMMFdE">
					<idno type="grant-number">N66001-02-1-8903</idno>
				</org>
				<org type="funding" xml:id="_ZBwxqeG">
					<idno type="grant-number">IIS-9907018</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,76.44,620.48,463.56,8.74;10,82.52,632.43,457.48,8.74;10,82.52,644.39,104.19,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,211.66,632.43,185.46,8.74">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">Abdul-Jaleel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,420.31,632.43,115.07,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="10,72.00,664.31,468.01,8.74;10,82.52,676.27,457.48,8.74;10,82.52,688.22,228.52,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,309.17,676.27,212.92,8.74">Rutgers&apos; HARD track experiences at TREC 2004</title>
		<author>
			<persName coords=""><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,82.52,688.22,115.29,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,75.16,468.00,8.74;11,82.52,87.11,224.44,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,381.46,75.16,158.54,8.74;11,82.52,87.11,49.01,8.74">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">Voorhees ;</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,152.82,87.11,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.00,107.04,468.00,8.74;11,82.52,118.99,457.49,8.74;11,82.52,130.95,53.19,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,118.67,118.99,223.69,8.74">TREC-2004 HARD-track experiments in clustering</title>
		<author>
			<persName coords=""><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,365.04,118.99,116.49,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,150.87,468.01,8.74;11,82.52,162.83,457.48,8.74;11,82.52,174.78,163.59,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,132.02,162.83,323.87,8.74">The Robert Gordon University&apos;s HARD track experiments at TREC 2004</title>
		<author>
			<persName coords=""><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,478.38,162.83,61.62,8.74;11,82.52,174.78,50.36,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,194.71,467.99,8.74;11,82.52,206.66,457.48,8.74;11,82.52,218.62,131.74,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,524.78,194.71,15.21,8.74;11,82.52,206.66,339.56,8.74">Improving passage retrieval using interactive elicitation and statistical modeling</title>
		<author>
			<persName coords=""><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,445.79,206.66,94.21,8.74;11,82.52,218.62,18.51,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,238.54,468.00,8.74;11,82.52,250.50,396.63,8.74;11,72.00,270.42,468.00,8.74;11,82.52,282.38,240.98,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,426.68,238.54,113.32,8.74;11,82.52,250.50,147.11,8.74;11,304.69,270.42,228.83,8.74">York University at TREC 2004: HARD and genomics tracks</title>
		<author>
			<persName coords=""><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,250.62,250.50,115.29,8.74;11,94.97,282.38,115.29,8.74">Proceedings of TREC 2004</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</editor>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2005. 2005</date>
		</imprint>
	</monogr>
	<note>Proceedings of TREC 2004. Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,302.31,468.00,8.74;11,82.52,314.26,367.80,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,348.35,302.31,191.65,8.74;11,82.52,314.26,117.87,8.74">University of North Carolina&apos;s HARD track experiments at TREC 2004</title>
		<author>
			<persName coords=""><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,221.79,314.26,115.29,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,334.19,468.00,8.74;11,82.52,346.14,163.59,8.74;11,72.00,366.07,467.99,8.74;11,82.52,378.02,332.93,8.74;11,72.00,397.95,468.00,8.74;11,82.52,409.90,175.30,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,232.87,334.19,223.93,8.74;11,141.91,346.14,104.19,8.74;11,72.00,366.07,85.72,8.74;11,357.73,366.07,182.26,8.74;11,82.52,378.02,83.68,8.74;11,318.01,397.95,154.08,8.74">Conceptual language models for contextaware text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G.-A</forename><surname>Levow ; Levow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rode</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,478.48,334.19,61.52,8.74;11,82.52,346.14,50.36,8.74;11,186.92,378.02,115.29,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2005. 2005. 2005. 2005</date>
		</imprint>
	</monogr>
	<note>ISCAS at TREC-2004: HARD track. In Proceedings of TREC 2004. Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,429.83,468.01,8.74;11,82.52,441.78,457.48,8.74;11,82.52,453.74,104.19,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,457.00,429.83,83.01,8.74;11,82.52,441.78,315.26,8.74">Approaches to high accuracy retrieval: Phrase-based search experiments in the HARD track</title>
		<author>
			<persName coords=""><forename type="first">Karamuftuoglu ;</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karamuftuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,419.56,441.78,115.82,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,473.66,468.00,8.74;11,82.52,485.62,213.49,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,259.31,473.66,238.29,8.74">Overview of the TREC 2004 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees ; Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,520.89,473.66,19.11,8.74;11,82.52,485.62,100.26,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,72.00,505.54,468.00,8.74;11,82.52,517.50,457.48,8.74;11,82.52,529.45,64.82,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,82.52,517.50,277.50,8.74">WIDIT in TREC-2004 genomics, HARD, robust and web tracks</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,380.49,517.50,114.74,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

<biblStruct coords="11,72.00,549.38,468.00,8.74;11,82.52,561.33,457.48,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,498.95,549.38,41.05,8.74;11,82.52,561.33,210.47,8.74">Microsoft Cambridge at TREC-13: Web and HARD tracks</title>
		<author>
			<persName coords=""><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.32,561.33,114.60,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Appears in this</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
