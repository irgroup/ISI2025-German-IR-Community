<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,184.20,107.33,243.60,12.64;1,144.24,123.41,323.65,12.64">Approaches to High Accuracy Retrieval: Phrase-Based Search Experiments in the HARD track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.12,166.28,76.38,8.96"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences University of Waterloo Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.12,166.28,96.86,8.96"><forename type="first">Murat</forename><surname>Karamuftuoglu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,184.20,107.33,243.60,12.64;1,144.24,123.41,323.65,12.64">Approaches to High Accuracy Retrieval: Phrase-Based Search Experiments in the HARD track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B743959200591D50415D56A933C9979</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our main research focus this year was on the use of phrases (or multi-word units) in query expansion. Multi-word units (MWUs) comprise a number of lexical units, such as named entities ("United Nations"), nominal compounds ("amusement park") and phrasal verbs ('check in'). Although MWUs can belong to different parts of speech, our focus was on nominal MWUs. We used a combined syntactico-statistical approach for selecting nominal MWUs. In the first selection pass we obtained valid noun phrases, and in the second pass we used statistical measures to select strongly bound MWUs. We have experimented with using two statistical measures of selecting MWUs from text: the C-value <ref type="bibr" coords="1,123.60,340.76,50.10,8.96" target="#b4">(Frantzi and</ref><ref type="bibr" coords="1,177.72,340.76,122.85,8.96">Ananiadou 1996, Vintar 2004</ref>) and the Log-Likelihood ratio <ref type="bibr" coords="1,433.80,340.76,63.91,8.96" target="#b3">(Dunning 1993)</ref>. Selected MWUs were then suggested to the user for interactive query expansion. Two main research questions were studied in these experiments:</p><p>-Whether nominal MWUs which exhibit strong degree of stability in the corpus are better candidates for interactive query expansion than nominal MWUs selected by the frequency parameters of the individual terms they contain; -Whether nominal MWUs are better candidates for interactive query expansion than single terms.</p><p>In more detail these experiments are presented in section 2.2.</p><p>The second focus of this work is on studying the effectiveness of noun phrases in document ranking. We have developed a new method of phrase-based document re-ranking, which addresses the problem of weighting overlapping phrases in documents, which in statistical IR models, such as probabilistic leads to the over-inflation of the document score. The method is described in detail in section 2.3.1. In section 3 we present the evaluation results, and in section 4 we discuss the differences in query expansion and retrieval performance between queries formulated by users with low and high familiarity with the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline run</head><p>We submitted two baseline runs: in the first one (UWATbaseTD) we used all non-stopword terms, extracted from the title and description fields of the topic, in the second (UWATbaseT) -all terms from the title field only. For both runs we used Okapi BM25 search function <ref type="bibr" coords="1,245.52,628.16,102.79,8.96" target="#b8">(Sparck-Jones et al. 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clarification forms</head><p>According to track instructions, a clarification form (CL form) could be used by participants to elicit any feedback or additional search criteria from users. Our main interest in using clarification forms was to evaluate different techniques for selecting MWUs and phrases for interactive query expansion.</p><p>We used 25 top-ranked documents retrieved in the UWATbaseTD run for selecting query expansion units. In each of these documents 2 best sentences were selected using the same technique that we used in our HARD track participation last year <ref type="bibr" coords="1,161.40,743.24,100.99,8.96" target="#b9">(Vechtomova et al. 2004)</ref>. Two main factors influenced sentence selection: (1) the idf weights of the original query terms present in the sentence, and (2) information value of the sentence, i.e. the combined tf.idf value of its words. We did not experiment with other passage-selection techniques or with full-text.</p><p>As last year, we applied Brill's rule-based tagger <ref type="bibr" coords="2,281.28,106.16,49.16,8.96" target="#b1">(Brill 1995)</ref> and BaseNP noun phrase chunker <ref type="bibr" coords="2,478.92,106.16,61.02,8.96;2,72.00,117.68,57.44,8.96" target="#b7">(Ramshaw and Marcus 1995)</ref> to extract noun phrases from these sentences. Multi-word units are then selected from the list of obtained noun phrases using different statistical techniques, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarification form 1</head><p>MWUs are characterised foremost by relative stability in the corpus. Some of the noun phrases output by the NP chunker are chance word groupings, and not stable MWUs. We were interested in exploring the value of MWUs compared to all noun-phrases in representing useful query expansion concepts to the user. The method of selecting stable MWUs from noun phrases using C-value is outlined below.</p><p>Noun phrases output by the NP chunker are ranked by the average idf of their constituent terms. For each phrase we generate the list of all phrases that it subsumes, i.e. contiguous or non-contiguous combinations of words in forward order, including the original complete phrase. For each subphrase, the C-value is calculated. The C-value is a measure of stability of an n-gram in the corpus <ref type="bibr" coords="2,268.08,267.08,121.27,8.96" target="#b4">(Frantzi and Ananiadou 1996)</ref>. The C-value formula we used is as follows:</p><p>Where:</p><p>t(a) is the frequency of the n-gram in longer n-grams; c(a) is the number of longer n-grams including a;</p><p>All subphrases for a given phrase are ranked by the C-value. The top ranked subphrase is then used to replace the original phrase in the list of candidate query expansion terms. The original complete phrase may get a higher Cvalue than any of its subphrases, in which case it is kept without a change.</p><p>For example, in our experiment, the bigram "World Cup" received the highest C-value out of all its subphrases generated from the phrase "grueling IAU 100-kilometer World Cup" and as a consequence was selected for the phrase list. The obtained phrases were then ranked by their C-value, top 75 of which were shown to the user in the clarification form 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarification form 2</head><p>One of the research questions that we wanted to explore was whether phrases are better candidates for interactive query expansion than single terms. A phrase carries more context and information, so it should be easier for the user to select more good phrases than single terms for query expansion. To test this hypothesis we took the phrases we selected for the previous set of clarification forms, and produced a list of single terms by splitting them and removing duplicates. The terms were then shown to the user for selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarification form 3</head><p>In this form we included top 75 phrases which were output by the NP chunker and ranked by the average idf of their constituent terms. By comparing query expansion with the phrases selected from this clarification form with the phrases selected from the 1 st and the 4 th clarification forms we aim to answer the question whether the application of the measures of phrase stability in the corpus lead to better phrases for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarification form 4</head><p>In the final set of clarification forms we experimented with selecting noun phrases using the Log-Likelihood measure <ref type="bibr" coords="2,108.12,689.60,58.50,8.96" target="#b3">(Dunning 1993</ref>). Log-Likelihood has been used extensively for the identification of statistically significant word collocations in text and has shown good results in English (e.g., <ref type="bibr" coords="2,352.20,701.12,111.31,8.96" target="#b5">Manning and Schütze 1999)</ref>.</p><p>We calculate Log-Likelihood for bigrams, using the Ngram Statistics Package <ref type="bibr" coords="2,395.52,724.16,121.16,8.96" target="#b0">(Banerjee and Pedersen 2003)</ref>. The phrase weighting was then done as follows: first, from each phrase output by the NP chunker we derived contiguous bigrams. For each bigram, its Log-Likelihood score was calculated. The highest Log-Likelihood score of any bigram</p><p>1 This was the maximum number of phrases that could be displayed in the clarification form.</p><p>(</p><formula xml:id="formula_0" coords="2,95.40,299.11,361.36,28.27">) ) ) ( ) ( ) ( )( 1 ) ( ( ) ( a c a t a freq a length a value C - - = -<label>1</label></formula><p>derived from the phrase was taken as the phrase weight. We then displayed the top 75 phrases in the CL form. This is a rather crude selection method, but it does reward phrases which contain a strongly bound collocation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental runs</head><p>Five experimental runs were conducted. Runs UWATexp1, UWATexp2, UWATexp3 and UWATexp4 used the feedback provided by the users to the 1 st , 2 nd , 3 rd and 4 th sets of clarification forms accordingly. In each run the query was constructed by splitting the phrases selected by the user from the corresponding clarification form into single terms and adding them to the original query terms. Each term in the expanded query was weighted in Okapi. The BM25 document retrieval function was used for topics requesting documents and BM250 passage retrieval function was used for topics requesting passages.</p><p>Run 5 (UWATexp5) was conducted using phrase search. Here for each topic we take the top 1000 documents retrieved in the UWATexp1 run and re-rank it using the phrase search method presented in section 2.3.1 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Phrase search</head><p>Following the interactive query expansion stage where the users select query expansion phrases, the next step is to use them in search. Intuitively using them as phrases in search should lead to better precision than if we split them into single words. One problem associated with the use of phrases in a statistical IR model, such as probabilistic <ref type="bibr" coords="3,72.00,336.08,104.60,8.96" target="#b8">(Sparck-Jones et al. 2000)</ref> is that some terms may occur in multiple phrases, for example let us assume there are two phrases in the query: "air traffic" and "traffic control", and two documents: the first containing one phrase "air traffic control", and the second -two phrases "air traffic" and "traffic control". How should they be weighted? If we calculate weights of each phrase in the document separately and then add them up to get the document score, as is currently done in the probabilistic model for single terms, then two documents will get equal scores. That obviously shouldn't be the case. But then how should the phrase weight be calculated for the first document? The situation gets more complex if we allow for non-contiguous word combinations, i.e. matching the following: "1 air 2 traffic 10 control" (numbers here denote positions of the words in text). Allowing match on non-contiguous word combinations is good for recall as it relaxes search constraints, but the distance between the phrase elements should be inversely related to the phrase weight. Therefore, the two main issues to be addressed by the phrase search algorithm are:</p><p>-remove the problem of overlapping phrases; -reflect the distance between the phrase elements in the phrase weight.</p><p>We have developed the following phrase search algorithm, which attempts to address these problems:</p><p>In the first step we retrieve documents by using a query which consists of all single terms extracted from the userselected phrases from the CL form 1 (UWATExp1 run). The next step is to re-rank these documents by using phrase information. We take top 1000 documents per topic in the retrieved set, stem the terms in each document and create a document representation, consisting only of the stemmed occurrences of terms from the query in their original order and their sequential position number in text.</p><p>For each query phrase, all possible subphrases (i.e. contiguous and non-contiguous combinations of words) are generated and ranked in the descending order of their length. For each subphrase in the list we use cgrep -a pattern matching program for extracting minimal matching strings <ref type="bibr" coords="3,311.88,635.12,56.36,8.96" target="#b2">(Clarke 1995)</ref> to extract the minimal spans of text in the document containing the subphrase. Each time cgrep returns matching strings, they are removed from the document representation and the procedure is repeated with the same phrase. If no matching string is found, the program attempts to match the next phrase in the list, and so on. In this way we can match progressively longer spans containing the phrase or its subphrases. An example of extracted windows for the phrase "practical implementation" is given in figure <ref type="figure" coords="3,147.24,692.60,4.98,8.96" target="#fig_0">1</ref> (the number preceded by the '#' sign is the sequential position of the following word in the original document text). Windows extracted using the above method might overlap. Our approach is to eliminate overlaps in windows in a two-step process: (1) rank the windows by their weight and (2) remove overlapping words from the lower ranked windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Window weighting</head><p>In this approach the window weight is calculated from the combination of idf weights of individual query terms occurring in it. The following formula was used:</p><p>Where:</p><p>i -word in the window a; n -number of words in the window a; span = pos(n) -pos(1) where: pos(i) -position number of the i th word in the window; p -tuning parameter 2 . So, the more informative the words in the window are, the shorter the span is, and the more words there are in the window, the higher is the weight of the window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removing duplicate windows</head><p>After the windows are ranked, we remove overlapping words by doing pairwise comparison of all windows. If two windows have overlapping query word(s), these words are removed from the lower ranked window. The windows shown in figure <ref type="figure" coords="4,137.16,494.48,4.98,8.96" target="#fig_0">1</ref> after the removal of overlapping words are illustrated in figure <ref type="figure" coords="4,397.44,494.48,3.77,8.96" target="#fig_1">2</ref>. All windows extracted from the document for every query phrase are then added to the same list, weighted using the formula (2) above and have the overlapping words removed. For each window we also keep the index of the phrase which was used to extract it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculating document scores</head><p>The next step is to calculate document scores. First, for each phrase in the query we calculate its weight in the document as follows:</p><p>2 Experiments showed that 0.2 gives the best performance on the HARD 2003 corpus. Document score is then calculated as the sum of PhraseWeight values for all query phrases that occur in the document:</p><p>Where: a -the query phrase occurring in the document d; n -number of query phrases occurring in the document d.</p><p>Finally, the top 1000 documents in the originally retrieved set are re-ranked by the new document scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>The results of the document-level evaluation based on 46 topics 5 are presented in table 2. All expanded runs significantly improved the average precision (Soft-rel) over the baseline run UWATbaseTD, and all runs except UWATexp5 significantly improved P@10 (Soft-rel) over the baseline (t-test at .05 significance level). Retrieval performance of the expanded queries created from the user feedback to clarification forms 1 and 2 is very similar. This suggests that users tend to select similarly good terms, whether they are shown to them in the context of phrases or on their own. On average users selected 21 phrases from the 1 st clarification form and 27 single terms 3 Experiments showed that k=1.2 gives the best performance on the HARD 2003 corpus. 4 Sparck-Jones et al. have experimentally determined that 0.75 gives best results on TREC data. 5 Four topics had no relevant documents and were, therefore, excluded from the evaluation.</p><formula xml:id="formula_1" coords="5,74.16,211.26,411.40,167.99">AveDocLen Doclen b b NF × + - = ) 1 ( (4) (5) ) ( ) ( 1 a ht PhraseWeig d ore DocumentSc n a = =</formula><p>from the 2 nd form. There were 675 phrase-terms selected only from the 1 st form, 384 terms selected only from the 2 nd form and 921 terms selected from both forms.</p><p>There is a very small difference between the performance of the queries from phrases selected using the average idf of their terms (UWATExp3) and queries from phrases selected using the measures of phrase stability in the corpus: the C-value (UWATExp1) and the Log-Likelihood ratio (UWATExp4). UWATExp3 gives somewhat better R-Precision and AveP results in Hard-rel evaluation. The R-precision (Hard-rel) of UWATExp3 is 5% higher than UWATExp1 and 9% higher than UWATexp4, neither of which is statistically significant. Similar performances of these three runs suggest that the statistical component of phrase selection does not play an important role in choosing query expansion phrases when it is combined with syntactical phrase selection techniques, such as POStagging and NP-chunking.</p><p>The phrase search algorithm (UWATExp5) did not demonstrate improvement over the performance of the singleterm search method (UWATExp1). While average precision (Soft-rel) increased slightly, the precision at 10 documents and R-precision deteriorated. The use of phrases improved average precision in 17 topics and degraded precision in 28 topics (Soft-rel evaluation). The average gain was 56%, while the average loss was 24%. We have also analysed performance of the phrase-search and single-term search methods in topics formulated by users with little and much familiarity, which is discussed in the next section.</p><p>One of the problems that might have caused the overall low performance of the phrase-search method is that we did not set the span limit. The rationale for that was to capture not only phrasal, but also topical relations between terms. However, this approach may be more useful with long multi-topic documents, rather than short documents. Since HARD track collection consisted mainly of short news articles, this aspect of the phrase search method is unlikely to help distinguish between relevant and non-relevant documents more than single-term document retrieval techniques.</p><p>Table <ref type="table" coords="6,98.64,393.40,5.01,9.01" target="#tab_1">3</ref> shows the results of the passage-level evaluation, averaged over 25 topics, which requested passages as retrieval elements. UWATExp3 gave best results in all passage evaluation measures but one. The phrase search run UWATExp5 did better than the single-term search method UWATExp1 in CharRPrec by 7.8%, but similar or slightly worse in other measures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The effect of familiarity on phrase selection and retrieval performance</head><p>The familiarity metadata was used in the HARD track to indicate the extent to which the searchers formulating the topic were familiar with it. Out of 46 topics, which were used in the evaluation, there were 25 topics with user familiarity "little" and 21 topics with familiarity "much".</p><p>We have analysed the effect of the searcher familiarity with the topic on two variables:</p><p>-the number of phrases selected for query expansion; -the performance of different search methods.</p><p>We hypothesise that the more familiar the searcher is with the subject of the query, the more phrases they are able to choose for query expansion. Our experimental results support this hypothesis. In all four clarification forms users familiar with the topic selected substantially more QE terms and phrases than the less familiar users (Table <ref type="table" coords="6,502.32,711.64,3.63,9.01" target="#tab_3">4</ref>). The difference observed in all clarification forms but one, CL1 (C-value selected phrases), was statistically significant (using t-test at .05 significance level).  Next, we hypothesise that the more familiar the searchers are with the topic, the better the performance of their unexpanded and expanded queries should be. The results of all baseline and experimental runs support this hypothesis: in all runs topics with "much" familiarity show higher Mean Average Precision, as shown in Table <ref type="table" coords="7,518.04,272.92,3.77,9.01" target="#tab_5">5</ref>.  The analysis of search results by familiarity reveals very interesting patterns in the performance of the phrase-based document re-ranking method. As mentioned in the previous section, overall the phrase-based run (UWATExp5) did not improve performance over the single-term search (UWATExp1). By analysing topics with different familiarity levels, we can see, however, that our phrase-based document re-ranking method improves the Average Precision of topics with "little" familiarity by 10%, and deteriorates the Average Precision of topics with "much" familiarity by 5.7%. The recall-precision graph in figure <ref type="figure" coords="7,216.60,757.48,9.48,9.01" target="#fig_3">3a</ref> shows another interesting phenomenon: for topics with familiarity "little" our phrase-based document re-ranking method has lower precision than single-term search at low recall levels and higher precision at higher recall levels, beginning from around 20% recall. Similar, but weaker pattern is evident in topics with familiarity "much" (figure <ref type="figure" coords="8,226.56,117.64,8.11,9.01" target="#fig_3">3b</ref>): phrase-based re-ranking has lower precision than single-term search up to around 60% recall level, and then starts getting slightly better. The pattern of lower precision at high recall and higher precision at high recall levels was also observed by <ref type="bibr" coords="8,307.44,140.56,73.53,9.01" target="#b6">Mitra et al. (1997)</ref> in their experiments with phrase search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we presented a comparative evaluation of different phrase selection techniques in interactive query expansion and a phrase-based document reranking method. A combined syntactico-statistical method was used for the selection of phrases. First, noun phrases were selected using a part-of-speech tagger and a noun-phrase chunker, and secondly, different statistical measures were applied to select phrases for query expansion. Three selection methods were used: C-value, Log-Likelihood ratio and the average idf of phrase terms to select phrases, which were then shown to the user for interactive query expansion. Evaluation experiments did not demonstrate substantial difference between these statistical methods in their effect on the retrieval performance.</p><p>We also studied whether users select better terms when they are shown in the context of phrases, than separately. The users were asked to select query expansion items from two clarification forms: one with the complete phrases selected by the C-value, and the other with the single terms from these phrases. The two query expansion runs gave very similar results, which suggests that presenting terms in the context of phrases does not provide much help to the users in selecting good query expansion terms. However, a large proportion of terms was only selected from one of the clarification forms.</p><p>The phrase-based document re-ranking method did not demonstrate overall improvement over the single-term search technique. However, it improved the Average Precision of topics formulated by users with low familiarity. As discussed earlier in the paper, phrases differ by their stability in the corpus, therefore they should not be treated uniformly in search. For example, a document which has a partial match on a non-compositional or idiomatic phrase (e.g. "Salt Lake City") is more likely to be non-relevant, than a document that has a partial match on a non-idiomatic expression (e.g. "organic product"). Therefore the weight of the partially matching phrase should be reduced more in the first case than in the second. The continuation of this work will be to use measures of phrase stability to estimate phrase weights in the documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,97.32,172.04,417.22,8.96"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of windows extracted from a document for the phrase "practical implementation"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,155.88,583.40,300.09,8.96"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of windows after the removal of overlapping words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,265.08,304.25,3.50,6.31;4,178.80,287.81,3.50,6.31;4,176.04,311.57,1.95,6.31;4,201.96,303.05,1.95,6.31;4,221.28,306.24,22.69,10.81;4,240.60,289.08,6.01,10.81;4,188.28,296.64,12.70,10.81;4,151.08,296.64,6.01,10.81;4,136.68,296.64,9.34,10.81;4,72.72,296.64,63.49,10.81;4,259.32,306.24,4.00,10.81;4,254.28,306.24,6.01,10.81;4,216.72,306.24,4.00,10.81;4,157.44,296.64,4.00,10.81;4,146.76,296.64,4.00,10.81;4,182.04,311.57,3.50,6.31;4,246.36,302.31,6.60,15.65;4,207.36,292.71,6.60,15.65;4,164.52,292.71,6.60,15.65;4,178.56,309.28,3.85,9.13;4,457.20,302.52,13.96,10.81;4,464.40,713.52,13.96,10.81;4,252.96,735.05,5.00,9.01;4,230.04,735.05,12.72,9.01;4,216.36,735.05,4.44,9.01;4,287.88,710.69,6.68,9.01;4,275.64,710.69,7.82,9.01;4,222.36,710.69,53.00,9.01;4,180.12,710.69,4.44,9.01;4,156.60,727.13,5.00,9.01;4,144.48,727.13,7.82,9.01;4,95.76,727.13,48.68,9.01;4,213.48,701.10,3.50,6.31;4,209.64,724.74,4.67,6.31;4,245.52,731.77,5.49,13.04;4,222.60,731.77,5.49,13.04;4,201.96,707.41,5.49,13.04;4,187.08,707.41,5.49,13.04;4,167.64,723.85,5.49,13.04;4,214.56,722.45,6.98,9.13;4,294.36,710.69,3.33,9.01;4,284.04,710.69,3.33,9.01;4,197.52,710.69,3.33,9.01;4,193.32,710.69,5.00,9.01;4,176.52,710.69,3.33,9.01;4,161.88,727.13,3.33,9.01;4,153.00,727.13,3.33,9.01;5,72.00,106.13,29.42,9.01;5,108.00,117.65,184.10,9.01;5,108.00,129.05,201.86,9.01;5,108.00,140.57,268.78,9.01;5,108.00,152.09,166.29,9.01;5,274.32,149.94,3.24,5.83;5,277.56,152.09,2.50,9.01;5,72.00,175.13,468.08,9.01;5,72.00,186.65,121.22,9.01;5,72.00,249.05,29.42,9.01;5,108.00,260.57,162.98,9.01;5,108.00,272.09,211.22,9.01;5,108.00,283.61,76.34,9.01;5,184.44,281.46,3.24,5.83;5,187.68,283.61,2.50,9.01"><head></head><label></label><figDesc>, extracted for the query phrase a; n -number of windows extracted for the phrase a; NF -document length normalisation factor (see equation 5 below). k -phrase frequency normalisation factor 3 .The document length normalisation factor was calculated in the same way as in the BM25 document ranking function (Sparck-Jones 2000):Where:Doclen -document length (word count); AveDoclen -average document length in the corpus; b -tuning constant 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,166.20,723.04,279.57,9.01;7,207.36,734.44,197.26,9.01"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Recall-precision graphs (soft-rel) of the runs for topics with (a) familiarity "little" and (b) familiarity "much".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.72,520.51,466.50,166.66"><head>Table 2 .</head><label>2</label><figDesc>Document-level results of the runs, averaged over all topics.</figDesc><table coords="5,72.72,520.51,466.50,143.32"><row><cell>Run</cell><cell></cell><cell>Soft-rel</cell><cell></cell><cell></cell><cell>Hard-rel</cell><cell></cell></row><row><cell></cell><cell>P@10</cell><cell>R-precision</cell><cell>AveP</cell><cell>P@10</cell><cell>R-precision</cell><cell>AveP</cell></row><row><cell>Title terms (UWATbaseT)</cell><cell>0.3089</cell><cell>0.2499</cell><cell>0.2196</cell><cell>0.2422</cell><cell>0.2298</cell><cell>0.2185</cell></row><row><cell>Baseline, Title + Description (UWATbaseTD)</cell><cell>0.42</cell><cell>0.3011</cell><cell>0.2693</cell><cell>0.3444</cell><cell>0.2744</cell><cell>0.2636</cell></row><row><cell>Single-term search, Query expansion with phrases from clarification form 1 (UWATExp1)</cell><cell>0.4889</cell><cell>0.3381</cell><cell>0.3176</cell><cell>0.4044</cell><cell>0.2971</cell><cell>0.2817</cell></row><row><cell>ExpRun1 reranked using the phrase-search algorithm (UWATExp5)</cell><cell>0.4422</cell><cell>0.3258</cell><cell>0.3233</cell><cell>0.3711</cell><cell>0.2854</cell><cell>0.2888</cell></row><row><cell>Single-term search, Query expansion with terms from clarification form 2 (UWATExp2)</cell><cell>0.48</cell><cell>0.3283</cell><cell>0.3026</cell><cell>0.4</cell><cell>0.2807</cell><cell>0.2695</cell></row><row><cell>Single-term search, Query expansion with phrases from clarification form 3 (UWATExp3)</cell><cell>0.4911</cell><cell>0.3352</cell><cell>0.3191</cell><cell>0.3978</cell><cell>0.3131</cell><cell>0.3128</cell></row><row><cell>Single-term search, Query expansion with phrases from clarification form 4 (UWATExp4)</cell><cell>0.4689</cell><cell>0.3256</cell><cell>0.3019</cell><cell>0.4</cell><cell>0.2875</cell><cell>0.2689</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,115.68,451.15,380.58,108.10"><head>Table 3 .</head><label>3</label><figDesc>Passage-level results    </figDesc><table coords="6,115.68,451.15,380.58,84.88"><row><cell>Run</cell><cell></cell><cell>Bpref@XChars</cell><cell></cell><cell></cell><cell>Prec@XChars</cell><cell></cell><cell>CharRPrec</cell></row><row><cell></cell><cell>6000</cell><cell>12000</cell><cell>24000</cell><cell>6000</cell><cell>12000</cell><cell>24000</cell><cell></cell></row><row><cell>UWATBaseT</cell><cell>0.209</cell><cell>0.204</cell><cell>0.171</cell><cell>0.223</cell><cell>0.221</cell><cell>0.185</cell><cell>0.163</cell></row><row><cell>UWATBaseTD</cell><cell>0.207</cell><cell>0.198</cell><cell>0.189</cell><cell>0.232</cell><cell>0.23</cell><cell>0.228</cell><cell>0.166</cell></row><row><cell>UWATExp1</cell><cell>0.286</cell><cell>0.26</cell><cell>0.22</cell><cell>0.308</cell><cell>0.289</cell><cell>0.245</cell><cell>0.166</cell></row><row><cell>UWATExp5</cell><cell>0.267</cell><cell>0.245</cell><cell>0.223</cell><cell>0.279</cell><cell>0.275</cell><cell>0.25</cell><cell>0.179</cell></row><row><cell>UWATExp2</cell><cell>0.267</cell><cell>0.26</cell><cell>0.231</cell><cell>0.291</cell><cell>0.286</cell><cell>0.261</cell><cell>0.169</cell></row><row><cell>UWATExp3</cell><cell>0.285</cell><cell>0.272</cell><cell>0.243</cell><cell>0.304</cell><cell>0.306</cell><cell>0.268</cell><cell>0.192</cell></row><row><cell>UWATExp4</cell><cell>0.22</cell><cell>0.222</cell><cell>0.21</cell><cell>0.25</cell><cell>0.247</cell><cell>0.246</cell><cell>0.179</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,180.84,215.44,250.53,20.41"><head>Table 4 .</head><label>4</label><figDesc>The average number of QE phrases/terms selected by users with "little" and "much" familiarity.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,172.68,450.40,266.73,20.53"><head>Table 5 .</head><label>5</label><figDesc>Mean average precision (soft-rel) of topics formulated by users with "little" and "much" familiarity.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,508.60,434.26,9.01;8,72.00,520.12,456.70,9.01;8,72.00,531.52,118.78,9.01" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,223.92,508.60,277.86,9.01">The Design, Implementation and Use of the Ngram Statistics Package</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,72.00,520.12,452.80,9.01">Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<meeting>the Fourth International Conference on Intelligent Text Processing and Computational Linguistics<address><addrLine>Mexico City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-02">2003. 2003. February</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,554.56,468.09,9.01;8,72.00,566.08,238.66,9.01" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,138.84,554.56,401.25,9.01;8,72.00,566.08,58.08,9.01">Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,136.68,566.08,105.08,9.01">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="565" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,589.00,455.97,9.01;8,72.00,600.52,443.37,9.01;8,72.00,612.04,22.66,9.01" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,214.08,589.00,214.45,9.01">On the use of Regular Expressions for Searching Text</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<idno>number CS-95-07</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,444.24,600.52,27.76,9.01">Canada</title>
		<imprint>
			<date type="published" when="1995-02">February 1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo Computer Science Department ; University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,72.00,635.08,457.30,9.01;8,72.00,646.60,50.98,9.01" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,123.24,635.08,248.52,9.01">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,378.60,635.08,104.90,9.01">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,669.52,329.49,9.01;8,401.52,667.38,5.04,5.83;8,409.08,669.52,120.26,9.01;8,72.00,681.04,130.66,9.01" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,231.84,669.52,118.84,9.01">Extracting nested collocations</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">T</forename><surname>Frantzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,367.92,669.52,33.57,9.01;8,401.52,667.38,5.04,5.83;8,409.08,669.52,120.26,9.01;8,72.00,681.04,83.88,9.01">Proc. 16 th Conference on Computational Linguistics, COLING</title>
		<meeting>16 th Conference on Computational Linguistics, COLING</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,704.08,452.14,9.01;8,72.00,715.60,85.30,9.01" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="8,180.84,704.08,224.80,9.01">Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,738.52,463.85,9.01;8,72.00,750.04,424.64,9.01" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,276.12,738.52,193.26,9.01">An Analysis of Statistical And Syntactic Phrases</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,486.84,738.52,49.01,9.01;8,72.00,750.04,42.47,9.01">Proceedings of RIAO97</title>
		<meeting>RIAO97<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="200" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,106.12,468.00,9.01;9,72.00,117.64,209.26,9.01" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,232.92,106.12,221.18,9.01">Text Chunking Using Transformation-Based Learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,462.24,106.12,77.76,9.01;9,72.00,117.64,204.08,9.01">Proceedings of the Third ACL Workshop on Very Large Corpora, MIT</title>
		<meeting>the Third ACL Workshop on Very Large Corpora, MIT</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,140.56,417.38,9.01;9,72.00,152.08,465.33,9.01;9,72.00,163.60,50.38,9.01" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,303.24,140.56,186.14,9.01;9,72.00,152.08,169.94,9.01">A probabilistic model of information retrieval: development and comparative experiments</title>
		<author>
			<persName coords=""><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,248.76,152.08,164.69,9.01">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="809" to="840" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Part 1. Part 2</note>
</biblStruct>

<biblStruct coords="9,72.00,186.64,467.98,9.01;9,72.00,198.04,467.98,9.01;9,72.00,209.56,181.54,9.01" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,294.00,186.64,241.69,9.01">Interactive Search Refinement Techniques for HARD Tasks</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karamuftuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Buckland</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Editors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,72.00,198.04,219.49,9.01">Proceedings of the Twelfth Text Retrieval Conference</title>
		<meeting>the Twelfth Text Retrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2004. November 18-21, 2003</date>
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,232.60,468.09,9.01;9,72.00,244.12,467.98,9.01;9,72.00,255.64,358.42,9.01" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,148.20,232.60,305.41,9.01">Comparative Evaluation of C-Value in the Treatment of Nested Terms</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Vintar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,477.36,232.60,62.73,9.01;9,72.00,244.12,467.98,9.01;9,72.00,255.64,226.51,9.01">Proceedings of MEMURA 2004 Workshop (Methodologies and Evaluation of Multiword Units in Real-world Applications), Language Resources and Evaluation Conference (LREC)</title>
		<meeting>MEMURA 2004 Workshop (Methodologies and Evaluation of Multiword Units in Real-world Applications), Language Resources and Evaluation Conference (LREC)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="54" to="57" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
