<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.58,86.54,410.22,11.10">The Robert Gordon University&apos;s HARD Track Experiments at TREC 2004</title>
				<funder ref="#_RytDS4J">
					<orgName type="full">Scottish Higher Education Funding Council</orgName>
					<orgName type="abbreviated">SHEFC</orgName>
				</funder>
				<funder>
					<orgName type="full">Research Development Initiative of The Robert Gordon University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,151.74,103.50,67.35,9.88"><forename type="first">David</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
							<email>david.harper@smartweb.rgu.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<addrLine>St Andrew Street</addrLine>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.87,103.50,85.78,9.88"><forename type="first">Gheorghe</forename><surname>Muresan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<addrLine>St Andrew Street</addrLine>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.77,103.50,51.86,9.88"><forename type="first">Bicheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<addrLine>St Andrew Street</addrLine>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.75,103.50,57.83,9.88"><forename type="first">Ivan</forename><surname>Koychev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<addrLine>St Andrew Street</addrLine>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.76,116.16,96.15,9.88"><forename type="first">Dietrich</forename><surname>Wettschereck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<addrLine>St Andrew Street</addrLine>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.68,116.16,88.89,9.88"><forename type="first">Nirmalie</forename><surname>Wiratunga</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">The Robert Gordon University</orgName>
								<address>
									<addrLine>St Andrew Street</addrLine>
									<settlement>Aberdeen</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.58,86.54,410.22,11.10">The Robert Gordon University&apos;s HARD Track Experiments at TREC 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">368F94A045B09B501F2F34B22C24984C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and Motivation</head><p>The High Accuracy Retrieval from Documents (HARD) track explores methods of improving the accuracy of document retrieval systems. As part of this track, the participants have investigated how information about a searcher's context can be used to improve retrieval performance <ref type="bibr" coords="1,430.50,242.88,53.34,9.02" target="#b0">[Allan, 2003;</ref><ref type="bibr" coords="1,90.00,254.40,48.83,9.02">Allan, 2004]</ref>. Searchers, referred to as assessors in this track, produce TREC-style search topics. Additionally, assessors are asked to specify values from pre-defined categories of metadata, that relate to the context of the search, as opposed to the topic of the search. The categories and values of the metadata used for the HARD track in 2004 are:</p><p>• Desired genre of documents, with values news, opinion-editorial, other, and any.</p><p>• Desired geographic treatment, with values USA, non-USA, and any. (Documents satisfying the USA metadata value may also refer to non-USA geographic areas, and vice versa.) • Assessor's familiarity with the topic, with values much and little (or equivalently high and low). • One or more documents related to the topic, but not from the collection to be searched.</p><p>• Desired granularity of response, with values passage and document.</p><p>The work reported in this paper focuses on exploiting the genre, geographic and familiarity metadata. We refer to the combination of metadata category and value, e.g. genre/news, as a meta-pair.</p><p>The experimental protocol for the track is described in the overview paper <ref type="bibr" coords="1,389.41,442.98,45.28,9.02">[Allan 2004</ref>]. Track participants were provided with a set of twenty training topics, including the associated metadata relating to context, plus 100 training documents per topic and relevance assessments. The training documents were retrieved from the HARD 2004 test corpus, derived from a number of different news databases. Each document is assessed as being not relevant, soft relevant or relevant (which we refer to as hard relevant). Not relevant means the document is not on topic. Soft relevant means a document is on topic but does not meet all the metadata specified; hard relevant (or simply relevant) means a document is both on topic and meets the metadata specified. For soft relevant documents, the metadata categories they did not satisfy were also indicated.</p><p>After a training period, fifty unseen test topics were distributed, without the metadata. Participants then searched the test corpus using their local systems, and submitted baseline search results, 1000 retrieved documents per topic. After submission of the baselines, the metadata for the test topics was distributed. Participants could use this metadata in any way in order to produce a new set of final search results, which were then submitted for evaluation. Both baseline and final search results were evaluated by the assessors for each topic, using three-valued relevance assessment. The effectiveness of the use of metadata was determined by comparing the baseline search results with final search results, especially with respect to difference in hard relevance, using a variety of TREC measures. The HARD track gave prominence to R-Precision, which emphasizes high accuracy retrieval. This was the first time that RGU had participated in the HARD track, and indeed in TREC. We were interested in investigating the effect of exploiting the topic metadata to re-rank our initial baseline run, in a similar fashion to that of Rutgers in TREC 2003 <ref type="bibr" coords="1,302.46,698.94,75.97,9.02" target="#b3">[Belkin et al, 2003]</ref>. We used the Lemur toolkit (LTK) to obtain a baseline ranking, using title and description for each topic, and using OKAPI BM25 weighting (with default LTK settings). Then, we focussed on re-ranking this baseline for each topic, based on queries generated specifically to rank separately by genre, geography, and familiarity, using the LTK re-ranking capability (ranking of so-called "working set"). The baseline and metadata-derived rankings were then combined using an evidence combination approach. Our experiments were motivated by several interests. First, we were interesting in re-ranking the topicality-derived baseline based on queries generated specifically for each meta-pair, i.e. pair of metadata category/metadata value. This enabled us to investigate the effect of each meta-pair source individually, and better understand the effectiveness of the approach used for that meta-pair. Moreover, the separate rankings provided a good basis for our subsequent approaches to evidence combination, in which we combine the various sources of evidence from both the baseline ranking and meta-pair re-rankings.</p><p>Second, we were interested in a variety of approaches for generating queries based on the various metapair specifications. We explored machine learning approaches, and specifically the use of relevance feedback, based on the training data. We also generated manual queries for some meta-pairs. And, we devised a novel topic-specific approach based on language modelling and the Kullback-Leibler divergence, for ranking documents by familiarity.</p><p>Third, we wanted to explore a number of principled approaches to combining the evidence provided by the baseline and metadata-derived rankings. These included Dempster-Shafer evidence combination, and a fusion technique based on normalising scores across rankings using rank position.</p><p>Fourth, we were interested in the challenge of evaluating, and understanding, the potentially complex interactions between the various approaches we used. Specifically, we were interested in evaluating the individual effects of the various approaches used for metadata-based re-ranking, and the overall effect of evidence combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ranking using Metadata</head><p>Three basic approaches were considered in re-ranking the topicality-based initial runs using the metadata provided for each topic<ref type="foot" coords="2,221.64,424.15,3.24,5.83" target="#foot_2">2</ref> , these being:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Relevance feedback approach using relevance assessments from the training data; • Manual generation of queries for (some of) the meta-pair combinations; and • A novel approach for generating familiarity-specific queries based on building topic models for sets for pseudo-relevant documents from the baseline, and selecting terms based on the computing the Kullback-Leibler divergence between a topic model and the overall collection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevance Feedback Approach</head><p>Using the training data, we were able to produce sample sets of hard and soft relevant documents for each meta-pair. For example, from those topics with meta-pair genre/news, we were able to generate a sample of hard relevant documents, and a sample of soft relevant documents. We conjectured that, by using the meta-pair samples in a relevance feedback process, we should be able to improve the baseline for topics with that specification. The effectiveness of this process will depend on at least two factors.</p><p>We need to obtain a large enough sample of hard relevant documents for each meta-pair. And, given that the sample is derived from a number of topics, we need to ensure that the sample is not biased towards particular topics (i.e. topicality-biased). Alternatively, we need ways of factoring topicality from metadata-ness in each sample. Given the small number of topics and training documents, we did not expect to satisfy these constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Manual Query Generation</head><p>This approach was effectively a fallback position, if the relevance feedback approach did not work. We generated a set of working conjectures for manual generation of queries, based in part on the characteristics of the training data, and in part on the characteristics of the corpus. These conjectures relate to the individual meta-category/metadata pairs, and we describe both the conjecture and our approach to query generation:</p><p>Genre/news Given the corpus is a news corpus, and thus dominated by news-type stories, it is highly likely that this specification would be met almost by default. We felt that in effect genre/news might be treated by the users as if it were genre/any. Therefore, we decided not to generate queries of this type.</p><p>Genre/oped Given this requires a very specific kind of document (opinion/editorial), it is likely that the user would satisfy themselves that this criteria was met. We generated a manual query based on inspection of the small sample of genre/oped documents in the training data. We focussed on words expressing the "first" person, opinions and views, and topical words typical of reviews, e.g. book, film, etc.</p><p>Genre/other We thought that "other" would be applied to very specialised or technical topics, and conjectured that the topicality parts of the topic might prove sufficient in retrieving appropriate documents. Therefore, we did not generate a manual query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geography/(US and non-US)</head><p>For the geography specification, we conjectured that US (resp. non-US) geography could be approximated using a query comprising US (resp. non-US) places names. We generated two queries using the names of US states, state capitals, and state mnemonics for the "US" query, and country names and "nationality" for the non-US query, (e.g. "Iran, Iranian/ France, French, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Familiarity/(little and much)</head><p>We did not generate manual queries as we developed an automatic procedure for generating familiarity-specific queries (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Familiarity Ranking using Topic Specificity or Generality</head><p>We conjecture that people with low familiarity with a topic will prefer documents which are in general representative of the topic as a whole, whereas people with high familiarity with a topic will prefer documents which are specific to particular aspects of the topic. This is related to the intuition, and common experience, that people with little knowledge are better off reading something general about a topic, rather than something very specific. We attempted to approximate these characteristics by identifying documents which contain terms which are representative of a topic for users with low familiarity, and documents in which highly discriminative terms occur for users with high familiarity.</p><p>Identifying such terms, with respect to any specific topic, gives us a set of terms which can be used as a query, for re-ranking the baseline search results. We thus formulated the following hypothesis concerning familiarity:</p><p>Users unfamiliar with a topic will prefer documents in which highly representative terms occur, and users familiar with a topic will prefer documents in which highly discriminating terms occur.</p><p>If we can identify these sets of representative and discriminating terms for a topic, then the term sets could be used as a query to re-rank the baseline according to familiarity.</p><p>We used the divergence between the collection language model and a topic language model as the basis for identifying representative and discriminating terms as follows. We define the collection model to be the probability distribution P(t|C), where for every term t in the vocabulary, the model gives the probability that we would observe term t if we randomly sampled from the collection C. A topic model, P(t|T), is constructed using the top-ranking K documents from the baseline, which are assumed to be relevant (pseudo relevant). The Kullback-Leibler divergence has been used extensively in applications of language modelling in information retrieval <ref type="bibr" coords="4,329.08,74.34,101.96,9.02">[Croft and Lafferty 2003;</ref><ref type="bibr" coords="4,433.57,74.34,70.50,9.02;4,90.00,85.86,21.54,9.02" target="#b8">Lafferty and Zhai 2001]</ref>. In this approach, we compute the Kullback-Leibler divergence between a topic model and the collection model, and we restrict the computation to the terms occurring in the topic model, VT, as follows:</p><formula xml:id="formula_0" coords="4,119.10,132.48,200.74,41.01">( ) ∑ ∈ = VT i t C i t p T i t p T i t p C T KL ) | ( ) | ( log ) | ( ) || (</formula><p>We ranked the terms in VT by their individual contribution to KL (T || C), and selected the top-ranked L terms for further processing. We refer to the expression before the log as the KL-representation, as this gives a measure of the term density, and thus generality. We refer to the log expression as the KLdiscrimination, as this gives a measure of term discrimination, and thus specificity.</p><p>Finally, the familiarity hypothesis is operationalised in the following way. KL-representative and KLdiscriminative terms for each topic were identified by constructing a language model (using Lemur) for the first ten documents in the baseline results, and a language model for the entire HARD corpus. Then, the top 50 terms contributing to KL (T || C) were selected, and the KL-representation and KLdiscrimination expressions were computed for those terms. For each topic with familiarity level low (or little), the 20 top-ranked KL-representation terms were selected to construct a query which was then used to re-rank the baseline results for that topic. For each topic with familiarity level high (or much), the 20 top-ranked KL-discrimination terms were chosen to construct a query used to re-rank the baseline results for those topics. Evidently, this procedure is heavily dependent on the whether an adequate sample of "on topic" (they may indeed be soft or hard relevant) documents is obtained from the baseline. We know the pseudorelevance feedback process can actively harm retrieval if this sample is poor in relevant documents. It is likely that for topics that perform poorly in the baseline run, our approach may not improve the baseline with respect to familiarity, and indeed may harm it.</p><p>The familiarity-specific queries are purposefully topic-specific, and hence good queries may improve the baseline in two ways. First, they may improve the soft effectiveness, by promoting soft relevant documents in the re-ranking. Second, if our conjecture holds, they may improve hard effectiveness, by promoting hard relevant documents in the re-ranking.</p><p>We also conjectured that the familiarity/much specification is likely to be considered more important by the users than the familiarity/little specification. By their nature, news articles (in the general sense of all kinds of news output), are written on the assumption that the reader will indeed have little familiarity with any given topic. Therefore, we believe that most documents will meet the familiarity/little specification almost by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evidence Combination</head><p>Before describing our approaches to evidence combination, we wish to discuss ranking-based versus filter-based use of metadata. In our ranking approach, we use each source of evidence for the metapairs to generate a separate re-ranking of the baseline. In a filter-based approach, one might use a given source of evidence to filter (i.e. remove) documents from the baseline ranking, e.g. filter baseline based on geography/US (say). We prefer the ranking approach for two reasons. First, we want to preserve any data obtained from a source of evidence for as long as possible in our process, and essentially let the evidence combination deal with poor scoring documents. The risk with filtering is that a document may score highly based on most sources, and be removed based on a poor source of evidence. This is particularly an issue when some of our approaches to metadata are very speculative, or indeed very simplistic. Second, we wanted to explore principled approaches to evidence combination (or fusion), and in part this means that all sources of evidence should be considered in toto.</p><p>In previously reported work on the HARD track, metadata evidence was used to adjust the baseline ranking in a relatively ad hoc way <ref type="bibr" coords="5,228.56,85.86,75.97,9.02" target="#b3">[Belkin et al, 2003]</ref>. Thus, baseline scores were adjusted based on heuristics developed for each sources of evidence, e.g. readability scores used to rank by Flesch Reading Ease Score<ref type="foot" coords="5,170.52,106.75,3.24,5.83" target="#foot_3">3</ref> . In this work, we wished to explore principled approaches in which each source of evidence was treated alike, although we not necessarily with equal weight.</p><p>We explored two approaches to evidence combination, namely Dempster-Shafer evidence combination, and an approach based on normalising scores based on rank position and weighted combination of the resultant normalised scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dempster-Shafer</head><p>This approach was first proposed for use in an IR context by <ref type="bibr" coords="5,334.52,213.48,96.08,9.02" target="#b6">[Jose and Harper, 1997]</ref> for combining sources of evidence for image retrieval, and has subsequently been used for in other image retrieval research <ref type="bibr" coords="5,125.79,236.46,109.36,9.02" target="#b2">[Aslandogan and Yu, 2000]</ref>, and in <ref type="bibr" coords="5,269.12,236.46,70.02,9.02" target="#b9">[Urban et al, 2003</ref>].</p><p>We will not describe Dempster-Shafer evidence combination in detail, but rather give the reader a flavour for how it can be applied in document retrieval. Essentially, each source of evidence (e.g. a set of document scores in a given ranking) can be viewed as providing support for so-called singleton<ref type="foot" coords="5,482.40,280.39,3.24,5.83" target="#foot_4">4</ref> sets comprising a set containing each individual document. In Dempster-Shafer, for each source of evidence, we have a confidence level between zero and one for each source. A confidence level of one means we have complete confidence in that sources, and zero that we have no confidence. Suppose for a given source, we have confidence C. Further, each source of evidence has a base probability assignment (BPA) or mass, which in our application is a set of normalised scores summing to C for the documents in a given ranking. Strictly, (1-C) is that part of the BPA that is unassigned to any proposition set.</p><p>Let us assume we have generated the BPAs for two sources A and B, as follows, and we have confidence C A and C B in these sources as shown:</p><formula xml:id="formula_1" coords="5,90.00,415.85,235.51,42.04">m A = (a 1 , a 2 , …., a n ), confidence C A , A i C a = ∑ m B = (b 1 , b 2 , …., b n ), confidence C B , B i C b = ∑</formula><p>where n is the number of documents in the source/ranking, a i is the score for document i according to source A, and similarly for b i and source B.</p><p>Let m A (i) denote a i, similarly m B (i). The (simplified) rule for combining singleton sources of evidence to obtain the new BPA (or mass) is:</p><formula xml:id="formula_2" coords="5,90.00,541.08,228.17,32.73">m Comb (i) = m A (i) m B (i) + (1-C A ) m B (i) + (1-C B ) m A (i), and confidence C Comb = C A + C B -C A C B</formula><p>This combination rule has some very nice properties <ref type="bibr" coords="5,302.19,587.04,95.63,9.02" target="#b6">[Jose and Harper, 1997;</ref><ref type="bibr" coords="5,400.26,587.04,43.73,9.02" target="#b5">Jose, 1998]</ref>, when you explore the various limiting values. For example, when C A =C B =1, and we have complete confidence in both sources of evidence, then the rule shows we should multiply the scores (individual masses). On the other hand if C A and C B approach zero, then the rule shows we should add the scores. If we have no confidence in a particular source and complete confidence in the other (C A =1, C B =0, say), then the result is identical to considering source A by itself. Values between 0 and 1 provide "mixtures" of these kinds of behaviour.</p><p>There are potentially problems in applying Dempster-Shafer. First, scores derived from a variety of processes must be transformed into BPAs, and these BPAs should approximate to a probability distribution over the set of documents. Second, with large numbers of documents (n large), the individual masses become very small, and the multiplicative term in the combination is dominated by the additive terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weighted Score-Rank Method</head><p>For a given topic, we assume that we have ranked scores for each relevant sources of evidence. Thus, for a topic with metadata specification genre/any, geography/US, and familiarity/much, we would have the baseline ranking, and a re-ranking of this baseline corresponding to geography/US, and familiarity/much. Depending on the way these rankings were obtained, the range and distributions of scores can be very different, and normalising scores across the rankings becomes an issue. In this approach, we substitute scores based on rank position for the actual scores, which is one way of normalising these scores<ref type="foot" coords="6,188.28,199.87,3.24,5.83" target="#foot_5">5</ref> .</p><p>Let us assume that the baseline ranking contains Tmax documents, then for a document at rank j in a particular ranking, we compute a rank score of (Tmax+1-j). This assigns the maximum score (Tmax) to the top-ranked document, and for a document at rank Tmax, a score of 1. If a document in the baseline is not retrieved for a given metadata source, then it is assigned a score of zero.</p><p>Each source of evidence is then allocated a weight. The score for a given document is computed as the weighted average of the rank-based scores for that document from the relevant sources of evidence.</p><p>For the example above, the baseline, geography/US and familiarity/much rankings would contribute to a score for each document appearing in the baseline for the given topic. Clearly, different topics would combine different sources of evidence.</p><p>The weight for a source of evidence (i.e. ranking) is based on our confidence in the source. These weights could, in principle, be learnt from the training data, but the training data was quite sparse for some meta-pairs. Instead, we chose (guessed) the weights based on intuition. We considered the baseline (topicality-based) ranking to be of most importance to the putative user, and allocated this source a weight of one in all our runs. We weighted the other sources, based on our confidence in the sources, derived through inspection of the re-ranking of the baseline, and on our conjectures (see section 2.2, Manual Query Generation) about the relative importance of the meta-pairs to the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>In all reported experiments, we used the Lemur toolkit to generate the baseline and to perform the rerankings of this baseline.</p><p>The initial baseline was obtained using the Lemur toolkit (LTK), and Okapi BM25 weights. We used the title and description for each topic, and retrieved 1000 document per topic.</p><p>To simplify the re-ranking experiments, we generated a mini-corpus comprising all documents in the baseline, plus the documents in the training data. This mini-corpus enabled us to efficiently run reranking experiments using the LTK ranking over "working set", i.e. over the mini-corpus. We note that, in general, the mini-corpus would have very different statistical properties than the complete corpus, but for most experiments reported here, these differences can be ignored.</p><p>For the relevance feedback experiments, we used the LTK KL method (with feedbackDocCount = 5, feedbackTermCount = 20 and the default parameter settings for this method).</p><p>For the manually generated queries, we used the LTK KL method to re-rank the entire mini-corpus using the generated queries. Thus, we obtained complete rankings of the mini-corpus for the metapairs genre/oped, geography/US and geography/non-US, independent of topic (at this point). We then generated rankings on a per topic basis, using the scores extracted from the mini-corpus ranking, for each meta-pair. Note, that we only generated a ranking for a topic if the meta-pair was specified for that topic.</p><p>In generating topic-specific queries for familiarity based ranking, we generated queries for each topic, depending on the familiarity metadata specification. We constructed a language model over the topranked K (K=10) for each topic, and a language model over the entire HARD track corpus. We then generated either a familiarity/little or familiarity/much query for each topic, depending on the metadata specification. We chose the top L (L=50) terms from the KL ranking (see section 2.3), generated the appropriate KL-representation (resp. KL-discrimination) expressions, and selected the top-ranked M (M=20) terms for the little (resp. much) query. We used the LTK KL method, and re-ranked the minicorpus using the appropriate query on a per topic basis. Thus, we obtained a re-ranking of the baseline for the familarity/little topics, and similarly for the familiarity/much topics.</p><p>The evidence combination experiments were leveraged off the original baseline in all cases. For each topic, the appropriate re-ranked results were combined to obtain a new overall ranking of the baseline. Thus, for a topic with metadata specification genre/any, geography/US, and familiarity/much, we would have the baseline ranking, combined with re-rankings of this baseline based on geography/US, and familiarity/much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>First, we present some results about the properties of the collection, and specifically the HARD training topics and evaluation topics. The statistics on the training topics resulted in us deciding not to use the relevance feedback approach in the official runs we submitted. Then, we present the overall documentlevel results for the various official runs we submitted, and sketch some initial conclusions. We then present a topic-by-topic analysis that suggests that some of the meta-pair sources may be improving hard effectiveness. Finally, we present some preliminary data on the individual performance of each meta-pair source.</p><p>Before presenting the results, we note that all runs are based on re-ranking the originally submitted baseline, and thus the effects of the various approaches are limited to re-ranking of this baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ranking using Metadata</head><p>Relevance Feedback Approach In Table <ref type="table" coords="7,263.33,484.92,3.77,9.02" target="#tab_0">1</ref>, we present summary data for the training topics. It would seem that there are only three meta-pairs, for which the training data might provide a reasonable sample of hard relevant documents, namely: genre/news, geography/US, and familiarity/little. There are a comparatively large number of positive training instances, and a range of topics represented. We believe that the training data for geography/US is unlikely to capture the concept of US-ness. It may be that good models could be derived for 'news' and 'little' through (positive) relevance feedback. However, we did not pursue the idea of using relevance feedback further, at least in the evaluation runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evidence Combination</head><p>Dempster-Shafer In applying the Dempster-Shafer method, in the way described in this paper, we need to normalise the scores for the different sources of evidence. Essentially, we need to transform the raw document scores for each source into a basic probability assignment (BPA). The obvious way to do this is to simply divide each score by the sum of all scores present in the ranking. There are however two problems with this solution. First, the resultant probabilities are extremely small for rerankings contained 1000 documents. As a result, if you combine them using the combination rule, essentially the scores will be added together. Secondly, and more problematically, in the re-rankings derived from the manually generated queries, and to a lesser extent, the familiarity queries, the range and distribution of documents scores is extremely skewed. Consequently, most of the mass in the BPA will be attributed to just a few documents. We believe that solutions to these problems can be found, but given these difficulties, we decided to focus our efforts on the weighted score-rank method.</p><p>Weighted Score-Rank Method We used this method to combine the evidence from the baseline run, and the metadata derived rankings. The details of each run we submitted are summarised in Table <ref type="table" coords="8,486.02,284.76,3.75,9.02" target="#tab_1">2</ref>.</p><p>As indicated earlier, the original baseline is assigned a weight of 1.0, with smaller weights for the arguably less reliable/less important meta-pair rankings.</p><p>In Run 10, we included two sources based on topicality, the original baseline, and a re-ranking of the baseline using the Lemur KL method with pseudo relevance feedback. We weighted these equally. Run 10* is a notional run, which is equivalent in effect to Run 10, and introduced for discussion purposes. It weights the topicality sources at 1.0 in combination, and shows the comparatively lower contributions of the metadata sources.</p><p>Clearly, it is difficult with the runs as submitted, to separate out the effects of the various metadata sources on performance. Later, we present the results on other runs, in which we explore the effect of each metadata source individually. Table <ref type="table" coords="8,115.31,629.82,5.01,9.02" target="#tab_2">3</ref> shows the document-level results for the runs submitted that are based on the weighted scorerank method. The italicised results are those pertaining to the original baseline. For each run, we report average precision, relevant retrieved at rank 10, and R-precision, and we will focus mainly on Rprecision in this discussion. Note, that the hard results are based on hard relevant assessments (on topic documents meeting metadata specification in topic), and the soft results are based on soft relevant assessments (on topic). In general, hard and soft results should not be compared against each other. Given that the data values have large standard deviations, and are not normally distributed, we use two non-parametric tests to test statistical significance. They are the Wilcoxon Signed Ranks test, and the Sign test, both applied at significance level 0.05. Each significance test was performed against the baseline entry in the same column, and is only computed for the hard results. Figures marked with one/two asterisks show figures that are significantly different from the baseline, based on the Wilcoxon and Sign test respectively.</p><p>In respect of Hard R-Precision and Rels@10, all metadata runs had higher average R-precision than the baseline, and especially so for Run 10. However, most of these improvements are not statistically significant. But, the R-Precision result is significant for Run 1 (Sign), and Run 10 (Wilcoxon and Sign). For Run1, there are respectively 21/9/16 topics for which R-Precision is better/worse/tied. For Run10, the corresponding figures are 23/9/14 topics.</p><p>Let us look at the results in more detail. Run10 is clearly best when considering the hard results.</p><p>Potentially, this improvement may be due to at least two factors. It may be due to improvements in soft effectiveness due to the inclusion of the second topicality-based source. And, it may be due to improvements in hard effectiveness due to the influence of the metadata ranking. It is instructive to look at the various metadata runs to try and understand what is going on.</p><p>Any difference between Run6 and Run10 is due to the inclusion of an additional "baseline" run, which is a pseudo relevance feedback run based on the original OKAPI baseline. For the soft results, we observe that for R-precision the difference is 4% points, and for Rels@10 8% points. For the hard results, the improvement in percentage points is respectively 13% and 11%. It would appear that exploiting the metadata may be providing an additional performance boost over that achieved through simply adding the additional "baseline" run.</p><p>Comparing Run1 and Run10 is also instructive. The soft results are effectively the same for these two runs. But, the Run10 (or equivalently Run10*), the hard Rel@10 and R-precision values are 6 and 8 percentage points higher than for Run1. If we examine the weightings given to the metadata sources, they are lower for Run10* (same effective weighting as submitted Run10) than for Run1, and we attribute this is achieving better balance between the topicality-based and metadata-based evidence. These results are very encouraging for three reasons. First, the weightings for the different evidence sources have not been optimised, and were simply best guesses. Second, our treatment the metadata categories genre and geography was extremely simplistic, and in fact, we ignored the 'news' and 'other' genres completely. (Note: this may have been advantageous as effectively genre/news did not appear to be a strong factor in the user assessments.) Third, the initial baseline was rather low, and our familiarity approach assumes that the top-ranked documents in the baseline provide a good sample of relevant documents, c.f. pseudo relevance feedback. Further analysis is required in order to attribute the observed performance improvements in Run1 and Run10 to particular sources of evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Exploratory analysis of individual sources</head><p>We ran a series of experiments in which we combined the baseline source with a single other evidence source, and measured the effect of the second source. We used the weighted score-ranks approach with the baseline weighted at 1.0, and the second source weighted variously as shown in Table <ref type="table" coords="10,450.82,121.50,3.77,9.02" target="#tab_3">4</ref>. In the case of a given meta-pair source, we only explored the effect for those topics with that specified metapair. Table <ref type="table" coords="10,138.34,144.48,5.01,9.02" target="#tab_3">4</ref> summarises the experiments, and reports the performance for R-Precision only. Again, we test statistical significant using the Wilcoxon and Sign tests, comparing the combination against the baseline performance within a particular row of the table.</p><p>Using the familiarity/much source improves the baseline significantly, but using the familiarity/little source does not. It would seem that our new language modelling approach to familiarity ranking is highly effective for high familiarity topics, and this is partly due to the relatively high baseline performance for these topics. Users, who are highly familiar with a topic, generate more effective topic specifications. For the familiarity /much subset, the baseline R-Precision is 0.307 compared with 0.250 over all topics. Consequently, it is likely that better familiarity/much queries will be generated given that we assume the top 10 documents in the baseline are pseudo-relevant.</p><p>Neither of the geographic sources (US or non-US) improves the baseline, and indeed the non-US source harms effectiveness. The reason for the poor non-US source performance is likely due to the original topics. In all cases, the topics with non-US specification included geographic places names in the title/description, which means the baseline already including specific geographic boosting. The relatively high baseline performance (R-Precision, 0.337) attests to this. Our more generic non-US query was then highly likely to damage this already good (hard) baseline performance.</p><p>Combining the original OKAPI BM25-based ranking with a re-ranking based on LEMUR/KL with pseudo-relevance feedback, significantly increased R-precision. It is likely that the very different retrieval mechanisms ranked the documents differently, and that the combination resulted in improved ranking. This phenomenon has been observed generally in fusion/evidence combination.  <ref type="table" coords="10,292.98,652.32,3.76,9.02" target="#tab_3">4</ref>, with any of the evidence combination runs in Table <ref type="table" coords="10,115.23,663.78,3.77,9.02" target="#tab_2">3</ref>, you will note that this simple combination of baseline and re-ranked baseline achieves an R-Precision of 0.326 over all topics, and outperforms the best of the evidence combination runs. That is, a purely topicality based combination, achieves better hard R-Precision, than combinations of techniques specifically devised to improve hard effectiveness. The re-ranking of the baseline is performed using the Lemur KL method that implements a form of pseudo-relevance feedback based on the Kullback-Leibler divergence. And, our familiarity ranking methods are similarly based. We therefore decided to run a further set of experiments comparing the effectiveness of the KLrepresentative queries, the KL-discriminative queries, and the LEMUR KL method.</p><p>We generated KL-discrimination queries for all topics, and re-ranked the baseline resulting in the DQ (discriminating queries) run. Similarly, we generated KL-representation queries for all topics, and reranked the baseline resulting in the RQ (representative queries) run. We then combined the baseline with the DQ run (weighting 1.0 and 0.25), and the baseline with the RQ run (weighting 1.0 and 0.5). The weightings were selected to given the best overall result. Given these combination runs are the equivalent to pseudo-relevance feedback runs, we compared them against the re-ranking of the baseline using the Lemur/KL method, which itself is a pseudo-relevance feedback method. The results of these runs are given in Table <ref type="table" coords="11,184.74,154.86,3.74,9.02">5</ref>. We report R-Precision for the set of all (ALL) topics, and for the familiarity/much (MUCH) and familiarity/little (LITTLE) subsets. Overall, the performance of the pseudo-relevance feedback runs is significantly better than the baseline run, and the KL run is marginally better than the Base/DQ run.</p><p>For the MUCH topic subset, Base/DQ and KL are comparable. Indeed, Base/DQ is significantly better than the baseline, whereas KL is not. Examining the soft effectiveness, it appears that KL achieves improvements in hard effectiveness through corresponding improvements in soft effectiveness.</p><p>Base/DQ achieves comparable hard effectiveness with comparatively lower levels of soft effectiveness. This provides some evidence that the discriminating queries (DQ), when used for the familiarity/much topics, do in fact boost hard effectiveness as measured by R-Precision. Interestingly, the representative queries (RQ) also achieve good levels of performance for this topic subset. As observed earlier, the MUCH topics provide a better starting point for any pseudo-relevance process, given their superior baseline performance.</p><p>Neither the DQ nor RQ queries perform significantly better than the baseline for the familiarity/little topics. Interestingly, the KL run is best for these topics.</p><p>It would seem there is evidence that combining the KL run with the baseline is likely to result in improvements for all topics, as demonstrated in Table <ref type="table" coords="11,308.04,564.96,3.76,9.02" target="#tab_2">3</ref>. The discriminating queries are highly effective when applied to the familiarity/much topics, and it will be interesting to see how to combine the baseline, the KL re-ranking and the DQ/much (DQ for much topics only) sources of evidence to achieve optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In relation to our treatment of the metadata sources of evidence, we conclude that:</p><p>• Our new topic modelling approach to generating discriminating queries, based on the Kullback-Leibler divergence, is highly effective for the familiarity/much topics, and significantly so compared with the baseline. However, comparable levels of performance are achieved for the familiarity/much topics using the standard KL method implemented in the Lemur toolkit. But, there is some evidence that the discriminating queries are boosting hard performance rather than simply boosting soft performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The generic geographic queries were not effective in improving performance, although in the case of the non-US topics, this may be partly attributable to the innate geographic-bias of the topicality parts of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The genre/opinion-editorial manual queries surprisingly improved performance but not significantly so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Although technically not a metadata source, the inclusion of a second topicality sources, based on re-ranking the baseline using a different retrieval mechanism, proved highly effective, and significantly so.</p><p>In relation to our evidence combination approach, and our general approach to ranking based on each source of evidence, and combining the rankings, we conclude that:</p><p>• The weighted score-rank approach proved effective in combining very different sources of evidence, and significantly so in the case of our submitted Run 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Our evidence combination approach enabled us to systematically explore the individual effects of the various metadata sources, which results in more insights concerning the effectiveness of each source.</p><p>In relation to experimental methodology, we offer the following observations:</p><p>• Our approach to ranking sources separately, and subsequent combination, proved useful in understanding the effects of the various metadata sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>By performing a more detailed analysis of the results beyond the overall run averages, we were better able to understand why some of our metadata approaches were effective or otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The paired significance tests, and particularly the use of non-parametric Wilcoxon and Sign Ranks tests, were useful in discovering differences in data, which generally was highly correlated, and had large standard deviations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,117.36,582.90,361.10,177.70"><head>Table 1 :</head><label>1</label><figDesc>Summary data for training topics: s of documents judged hard (positive) and soft (negative) relevant. Number of topics contributing the data is also given.</figDesc><table coords="7,122.40,612.21,346.86,148.38"><row><cell>MetaCat</cell><cell>MetaData</cell><cell>Positives</cell><cell>% Pos</cell><cell>Negatives</cell><cell>% Negs</cell><cell># topics</cell></row><row><cell>Genre</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>news</cell><cell>241</cell><cell>94.9</cell><cell>13</cell><cell>5.1</cell><cell>9</cell></row><row><cell></cell><cell>oped</cell><cell>2</cell><cell>25</cell><cell>6</cell><cell>75</cell><cell>1</cell></row><row><cell></cell><cell>other</cell><cell>18</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>2</cell></row><row><cell></cell><cell>any</cell><cell>76</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>7</cell></row><row><cell>Geography</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>US</cell><cell>103</cell><cell>74.1</cell><cell>36</cell><cell>25.9</cell><cell>8</cell></row><row><cell></cell><cell>non-US</cell><cell>85</cell><cell>89.5</cell><cell>10</cell><cell>10.5</cell><cell>4</cell></row><row><cell></cell><cell>any</cell><cell>149</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell></cell></row><row><cell>Familiarity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>little</cell><cell>288</cell><cell>99.7</cell><cell>1</cell><cell>0.3</cell><cell>13</cell></row><row><cell></cell><cell>much</cell><cell>49</cell><cell>98</cell><cell>1</cell><cell>2</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,90.00,451.74,413.16,163.48"><head>Table 2 :</head><label>2</label><figDesc>Details of evaluation runs showing weighting of difference evidence sources used in applying the weighted score-rank method.</figDesc><table coords="8,101.10,480.93,402.06,134.28"><row><cell>Source</cell><cell></cell><cell></cell><cell>Metadata</cell><cell></cell><cell></cell><cell cols="2">Topicality</cell></row><row><cell>Metadata</cell><cell>Genre</cell><cell cols="2">Geography</cell><cell cols="2">Familiarity</cell><cell>Baseline:</cell><cell>Reranked</cell></row><row><cell>Category</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lemur/Okapi</cell><cell>baseline:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lemur/KL with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pseudo RF</cell></row><row><cell>Metadata</cell><cell>Opinion-</cell><cell>U.S.</cell><cell>Non-</cell><cell>Little</cell><cell>Much</cell><cell></cell><cell></cell></row><row><cell>Value</cell><cell>Editorial</cell><cell></cell><cell>U.S.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>0.4</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.5</cell><cell>1.0</cell><cell>-</cell></row><row><cell>5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>1.0</cell><cell>-</cell></row><row><cell>6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.4</cell><cell>0.6</cell><cell>1.0</cell><cell>-</cell></row><row><cell>10</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.4</cell><cell>0.6</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>10*</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell><cell>0.2</cell><cell>0.3</cell><cell>0.5</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,90.00,379.32,413.21,226.48"><head>Table 3 :</head><label>3</label><figDesc>Document-level evaluation of all submitted runs. Standard deviation in (brackets); percentage change (%) compared with baseline below that. */** indicate statistical significance at level 0.05 using Wilcoxon/Sign test resp.</figDesc><table coords="9,90.00,421.95,404.04,183.84"><row><cell></cell><cell></cell><cell>HARD</cell><cell></cell><cell></cell><cell>SOFT</cell><cell></cell></row><row><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RUN</cell><cell>Avg Prec.</cell><cell>Rels@10</cell><cell>R-Prec.</cell><cell>Avg Prec.</cell><cell>Rels@10</cell><cell>R-Prec.</cell></row><row><cell>Baseline</cell><cell>0.259</cell><cell>3.11</cell><cell>0.250</cell><cell>0.257</cell><cell>3.84</cell><cell>0.275</cell></row><row><cell></cell><cell>(0.251)</cell><cell>(3.23)</cell><cell>(0.246)</cell><cell>(0.229)</cell><cell>(3.32)</cell><cell>(0.213)</cell></row><row><cell>1</cell><cell>0.251</cell><cell>3.24</cell><cell>0.262 -/**</cell><cell>0.241</cell><cell>3.76</cell><cell>0.25</cell></row><row><cell></cell><cell>(0.240)</cell><cell>(3.39)</cell><cell>(0.245)</cell><cell>(0.222)</cell><cell>(3.51)</cell><cell>(0.222)</cell></row><row><cell></cell><cell>-3.02%</cell><cell>4.18%</cell><cell>4.85%</cell><cell>-6.04%</cell><cell>-2.08%</cell><cell>-7.7%</cell></row><row><cell>5</cell><cell>0.2432</cell><cell>3.20</cell><cell>0.261</cell><cell>0.235</cell><cell>3.69</cell><cell>0.248</cell></row><row><cell></cell><cell>(0.242)</cell><cell>(3.33)</cell><cell>(0.249)</cell><cell>(0.224)</cell><cell>(3.50)</cell><cell>(0.223)</cell></row><row><cell></cell><cell>-5.92%</cell><cell>2.89%</cell><cell>4.53%</cell><cell>-8.26%</cell><cell>-3.91%</cell><cell>-9.84%</cell></row><row><cell>6</cell><cell>0.242</cell><cell>3.11</cell><cell>0.251</cell><cell>0.231</cell><cell>3.58</cell><cell>0.246</cell></row><row><cell></cell><cell>(0.238)</cell><cell>(3.35)</cell><cell>(0.243)</cell><cell>(0.220)</cell><cell>(3.49)</cell><cell>(0.221)</cell></row><row><cell></cell><cell>-6.46%</cell><cell>0%</cell><cell>0.44%</cell><cell>-10.21%</cell><cell>-6.77%</cell><cell>-10.64%</cell></row><row><cell>10/10*</cell><cell>0.258 -/**</cell><cell>3.44</cell><cell>0.2801 */**</cell><cell>0.246</cell><cell>3.89</cell><cell>0.2567</cell></row><row><cell></cell><cell>(0.240)</cell><cell>(3.39)</cell><cell>(0.250)</cell><cell>(0.219)</cell><cell>(3.50)</cell><cell>(0.220)</cell></row><row><cell></cell><cell>-0.35%</cell><cell>10.61%</cell><cell>12.17%</cell><cell>-4.09%</cell><cell>1.3%</cell><cell>-6.76%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,90.00,426.48,410.29,234.86"><head>Table 4 :</head><label>4</label><figDesc>Document-level evaluation of baseline combined with a single source using the weighted score-ranks approach. */** indicate statistical significance at level 0.05 using Wilcoxon/Sign test resp. Bold results best for row. All results evaluated using hard relevance.</figDesc><table coords="10,90.00,469.26,355.95,192.08"><row><cell>Second</cell><cell>#</cell><cell cols="5">Base Second source weights and R-Precision</cell><cell></cell></row><row><cell>source</cell><cell cols="2">topics R-Pr wgt</cell><cell>R-Pr</cell><cell>wgt</cell><cell cols="2">R-Pr wgt</cell><cell>R-Pr</cell></row><row><cell>Genre/</cell><cell>7</cell><cell>0.165 0.5</cell><cell cols="5">0.198 0.25 0.251 0.125 0.189</cell></row><row><cell>oped</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Geog/</cell><cell>15</cell><cell>0.190 0.5</cell><cell cols="2">0.176 0.25</cell><cell cols="3">0.176 0.125 0.193</cell></row><row><cell>US</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Geog/</cell><cell>7</cell><cell>0.337 0.5</cell><cell cols="2">0.190 0.25</cell><cell cols="3">0.254 0.125 0.257</cell></row><row><cell>Non-US</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fam/</cell><cell>27</cell><cell>0.208 0.5</cell><cell cols="2">0.210 0.25</cell><cell cols="3">0.200 0.125 0.197</cell></row><row><cell>little</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fam/</cell><cell>19</cell><cell>0.307 0.5</cell><cell>0.395</cell><cell cols="2">0.25 0.393</cell><cell cols="2">0.125 0.357</cell></row><row><cell>much</cell><cell></cell><cell></cell><cell>-/**</cell><cell></cell><cell>*/**</cell><cell></cell><cell>*/**</cell></row><row><cell>Rerank</cell><cell>45</cell><cell>0.250 1.0</cell><cell>0.326</cell><cell>0.75</cell><cell>0.292</cell><cell>0.5</cell><cell>0.268</cell></row><row><cell>baseline</cell><cell></cell><cell></cell><cell>*/**</cell><cell></cell><cell>*/**</cell><cell></cell><cell></cell></row><row><cell cols="4">If you compare the 'Rerank baseline' run in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,90.00,189.36,407.99,153.26"><head>Table 4 :</head><label>4</label><figDesc>Document-level evaluations of pseudo-relevance feedback runs. R-Precision using hard and soft relevance judgments. */** indicate statistical significance compared with baseline at level 0.05 using Wilcoxon/Sign test respectively, for HARD results only.</figDesc><table coords="11,116.22,235.80,350.92,106.82"><row><cell></cell><cell cols="2">Base</cell><cell cols="2">Base + DQ</cell><cell cols="2">Base + RQ</cell><cell cols="2">KL (reranked</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">baseline)</cell></row><row><cell></cell><cell cols="8">HARD SOFT HARD SOFT HARD SOFT HARD SOFT</cell></row><row><cell>ALL</cell><cell>0.249</cell><cell>0.273</cell><cell>0.297</cell><cell>0.295</cell><cell>0.265</cell><cell>0.312</cell><cell>0.311</cell><cell>0.318</cell></row><row><cell></cell><cell></cell><cell></cell><cell>*/**</cell><cell></cell><cell>*/**</cell><cell></cell><cell>*/**</cell><cell></cell></row><row><cell>MUCH</cell><cell>0.307</cell><cell>0.356</cell><cell>0.393</cell><cell>0.375</cell><cell>0.341</cell><cell>0.384</cell><cell>0.394</cell><cell>0.399</cell></row><row><cell></cell><cell></cell><cell></cell><cell>*/**</cell><cell></cell><cell>-/**</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LITTLE</cell><cell>0.206</cell><cell>0.214</cell><cell>0.227</cell><cell>0.236</cell><cell>0.210</cell><cell>0.259</cell><cell>0.250</cell><cell>0.258</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,95.76,749.22,389.98,9.02;1,90.00,760.74,298.94,9.02"><p>Gheorghe Muresan, Rutgers University, was a visiting researcher at the Smart Web Technologies Centre, The Robert Gordon University, and contributed to the RGU TREC</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2004" xml:id="foot_1" coords="1,413.93,760.74,51.36,9.02"><p>experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="2,95.70,749.22,404.21,9.02;2,90.00,760.74,276.99,9.02"><p>We refer to "topic" when referring to the TREC HARD topics, and "topicality" when referring to the use of topic title, description and narrative in retrieval (by topicality).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="5,95.76,737.76,258.84,9.02"><p>http://csep.psyc.memphis.edu/cohmetrix/readabilityresearch.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="5,95.76,749.22,403.79,9.02;5,90.00,760.74,336.89,9.02"><p>In general, Dempster-Shafer enable one to combine evidence for sets of propositions, e.g. for a set of documents. But, for our purposes, it is sufficient to deal with individual documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="6,95.76,737.76,400.83,9.02;6,90.00,749.22,381.53,9.02;6,90.00,760.74,149.85,9.02"><p>Since submitting the TREC runs, Muresan has developed an approach to normalisation based on the use of z-scores, scores based on assuming a normal distribution over scores, and using standard deviation from the mean as the score.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was undertaken within the Smart Web Technologies Centre that was established through a <rs type="funder">Scottish Higher Education Funding Council (SHEFC)</rs> <rs type="grantName">Research Development Grant</rs> (No. <rs type="grantNumber">HR01007</rs>). <rs type="person">Gheorghe Muresan</rs> and <rs type="person">David Harper</rs> were supported through the <rs type="funder">Research Development Initiative of The Robert Gordon University</rs>. We would like to thank our colleagues at <rs type="institution">RGU</rs> and Rutgers for their feedback on this work, and in particular <rs type="person">Nick Belkin</rs>. We would like to thank <rs type="person">Paul Ogilvie</rs> for his invaluable advice on using the Lemur toolkit, and <rs type="person">Diane Kelly</rs> for her insightful feedback on this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RytDS4J">
					<idno type="grant-number">HR01007</idno>
					<orgName type="grant-name">Research Development Grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,90.00,509.04,408.91,9.02;12,90.00,520.50,408.58,9.02;12,90.00,532.02,124.75,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,126.06,509.04,336.29,9.02">HARD track overview in the TREC 2003 High Accuracy Retrieval from Documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,483.31,509.04,15.60,9.02;12,90.00,520.50,140.04,9.02">The Twelfth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="page" from="24" to="37" />
		</imprint>
	</monogr>
	<note>GPO</note>
</biblStruct>

<biblStruct coords="12,90.00,546.54,375.33,9.02;12,90.00,558.00,393.13,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,126.10,546.54,339.23,9.02;12,90.00,558.00,43.27,9.02">HARD track overview in the TREC 2004 (Notebook) High Accuracy Retrieval from Documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,154.17,558.00,144.23,9.02">The 13th Text REtrieval Conference</title>
		<meeting><address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>GPO</note>
</biblStruct>

<biblStruct coords="12,90.00,572.52,404.74,9.02;12,90.00,584.04,326.65,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,245.87,572.52,248.88,9.02;12,90.00,584.04,125.15,9.02">Multiple Evidence Combination in Image Retrieval: Diogenes Searches for People on the Web</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Aslandogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,236.60,584.04,108.99,9.02">Proceedings of SIGIR 2000</title>
		<meeting>SIGIR 2000<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.00,598.50,402.73,9.02;12,90.00,610.02,384.67,9.02;12,90.00,621.54,365.62,9.02;12,90.00,633.00,97.22,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,126.98,610.02,239.61,9.02">HARD and web interactive track experiences at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rutgers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,157.54,621.54,158.11,9.02">The Twelfth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>TREC; Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="page" from="532" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.00,647.52,400.15,9.02;12,90.00,659.04,140.61,9.02" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,232.80,647.52,178.25,9.02">Language modeling for information retrieval</title>
		<editor>Croft, W.B. and Lafferty, J.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.00,673.50,405.12,9.02;12,90.00,685.02,332.86,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,161.73,673.50,240.60,9.02">An integrated approach for multimedia information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,456.48,673.50,38.64,9.02;12,90.00,685.02,155.35,9.02">School of Computing and Mathematical Sciences</title>
		<meeting><address><addrLine>Aberdeen, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Robert Gordon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="12,90.00,699.54,413.51,9.02;12,90.00,711.00,390.15,9.02;12,90.00,722.52,100.55,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,230.07,699.54,269.61,9.02">A retrieval mechanism for semi-structured photographic collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,100.26,711.00,375.68,9.02">Proceedings of the DEXA&apos;97 Conference, number 1308 in Lecture Notes in Computer Science</title>
		<meeting>the DEXA&apos;97 Conference, number 1308 in Lecture Notes in Computer Science</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="276" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,90.00,737.04,407.38,9.02;12,90.00,748.50,258.28,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,230.04,737.04,253.08,9.02">Spatial querying for image retrieval: A user-oriented evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,90.00,748.50,99.54,9.02">Proceedings of SIGIR&apos;98</title>
		<meeting>SIGIR&apos;98<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="232" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,74.34,369.78,9.02;13,90.00,85.86,389.89,9.02;13,90.00,97.32,399.13,9.02;13,90.00,108.84,206.08,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,182.06,74.34,277.72,9.02;13,90.00,85.86,81.98,9.02">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,188.88,85.86,291.01,9.02;13,90.00,97.32,201.31,9.02">Proceedings of the 24th annual international ACM-SIGIR conference on research and development in information retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kraft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 24th annual international ACM-SIGIR conference on research and development in information retrieval<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,90.00,123.36,398.90,9.02;13,90.00,134.82,387.84,9.02;13,90.00,146.34,168.40,9.02" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,296.12,123.36,192.78,9.02;13,90.00,134.82,62.75,9.02">An Adaptive Approach Towards Content-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,169.50,134.82,308.34,9.02;13,90.00,146.34,33.83,9.02">Proceedings of the 3rd International workshop on Content Based Multimedia Indexing</title>
		<meeting>the 3rd International workshop on Content Based Multimedia Indexing<address><addrLine>Rennes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
