<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.64,87.33,312.64,12.64">TREC2004 Robust Track Experiments using PIRCS</title>
				<funder ref="#_xHdY3VJ">
					<orgName type="full">U.S. Govt. DST/ATP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.36,121.37,50.82,10.80"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.75,121.37,54.44,10.80"><forename type="first">L</forename><surname>Grunfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.46,121.37,43.51,10.80"><forename type="first">H</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.27,121.37,38.82,10.80"><forename type="first">P</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.64,87.33,312.64,12.64">TREC2004 Robust Track Experiments using PIRCS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A6331EB50AB047FB5BD9D5AEADE3AE3A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There were two sub-tasks in the TREC2004 Robust track: given a set of topics, a) improve the effectiveness of the lowest performing 25%, and b) predict their ranking according to their average precision.</p><p>For task a), we followed the strategy introduced by us last year to improve ad-hoc retrieval by employing the web as an external thesaurus to supplement a given topic description. A new method of probing the web based on a given topic statement called 'window rotation' was tested.</p><p>For task b) we employed ε-SVR (epsilon support vector regression) to predict performance of test topics based on training with some simple features such as document frequencies, query term frequencies. This allows performance prediction without retrieval. Features were also added from a retrieval list with the hope that they may predict later stage or web-assisted retrieval better. 200 old topics were used for training to predict the ranking of 49 new topics, as well as the whole set of 249.</p><p>Runs were done that made use of title only, description only section of a topic, and titledescription-combination retrieval lists. Ten submissions including runs that were based on initial retrieval only, retrievals with pseudo-relevance feedback, and with web-assistance. Evaluation shows that we have achieved very good performance for most of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Robust Track -Improving Low Performing Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>We introduced a new strategy of improving ad-hoc retrieval based on web-assistance in the Robust Track of TREC2003. In initial retrieval, some queries have low average precision performance (weak or hard queries) while others return good values (strong or easy queries). The objective of this track is to automatically improve the effectiveness of weak topics, and others in general. Strong topics can generally be further improved with pseudo-relevance feedback (PRF), but this does not work for weak topics because for them, an initial retrieval would not bring in much useful material for feedback use. One may try to enrich weak topic wordings via a thesaurus to improve term variety, and thereby enhancing initial retrieval results. However choosing an available and appropriate thesaurus of the right domain without prior knowledge of a topic is quite a challenge. We demonstrated in TREC2003 that employing the WWW as an alldomain word-association resource with appropriate filtering can be successful for this Robust Track objective.</p><p>Additional to normal ad-hoc retrieval on the target collection with the original TREC topics, our method of employing web-assistance consists of four steps, and these are illustrated in Fig. <ref type="figure" coords="1,489.76,680.50,4.32,9.94">1</ref>: 1) for each TREC topic, define associated web queries for a specific search engine;</p><p>2) use the web queries with the search engine to probe the web for relevant or related pages or snippets; 3) from the returned web pages or snippets, define alternate queries (based on proportional word frequencies) for retrieval from the target collection; 4) combine retrieval lists from the original TREC query and the alternate queries to form the final retrieval result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WWW for Ad-Hoc</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig.1: Web-Assisted Ad-Hoc Retrieval</head><p>Reasoning backwards, one sees that this approach works if the alternate queries possess synonyms or lexically different content terms but are semantically related to the original. This implies that the returned web pages/snippets should be answers or topically close to what the TREC topic wants. To achieve this, the web queries defined in Step 1 are of paramount importance, and would determine the success or failure of this strategy.</p><p>The web queries naturally are tied to which search engine one wants to employ. We have focused on using the Google search engine because it is generally effective, offers a convenient API (although we wrote our own interface), and searches an immense collection (over 4G web pages according to its homepage). Google accepts input queries in Boolean AND form: a AND b AND c .. AND d (where a,b, ..d are un-stemmed single keywords or phrases), although its final retrieval is further dependent on page-link analysis. Even though Google's search collection is huge, the Boolean AND operator reduces the output answers rapidly to null when the number of query terms increases (7 or more for example depending on the terms). If a TREC topic is short, like those composed from the 'title' section with 2 to 5 words, one could just use all the content terms as a web query. If one considers longer queries like those obtained from the 'description' section of a TREC topic which is mostly a sentence long, one has to employ some filtering methods to choose salient terms from the 'description' for web retrieval. Selecting salient terms from a sentence or a piece of text is not an easy task and is prone to error. Attempts for salient term selection include <ref type="bibr" coords="2,196.06,621.10,128.64,9.94" target="#b1">(Dorr, Zajic, Schwartz 2003</ref>) for headline generation for example. In TREC2003, we employed word/sentence syntax for this purpose. In addition, data redundancy and fusion was used to improve retrieval effectiveness.</p><p>MINIPAR is a broad-coverage parser available from <ref type="bibr" coords="2,325.76,671.74,42.62,9.94" target="#b4">(Lin 1994</ref>) that, among other data, tags each input word of a sentence according to its POS, as well as indicating the asymmetric binary relationship between word pairs that play the role of governor and dependent. It also recognizes phrases which we call (MINIPAR phrases). Last year, as a general approach we tried selecting salient words based on phrases, and nouns, verbs, adjectives in this order until we obtained a fixed number of words (5 or 6), or in addition choose semantic categories of person, country, organization first, or simply just the first 6 content words of a description.</p><p>Because of brittleness for Boolean retrieval and the uncontrolled nature of web content, we employ multiple (3) web queries so as to have higher chance that at least some of them would have sufficient salient terms. These queries return web pages/snippets that would be used to define alternate queries for target collection retrieval. We also rely on data fusion: past experience has shown that when combining retrieval lists that are reasonably different from each other, the combined final retrieval tends to get a boost in effectiveness above the component lists. This same strategy was followed this year, except that we also tested a method of generating web queries from TREC topics without the need for salient term selection. This is the window rotation method to be discussed in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Nomenclature for Web Queries, Alternate Queries and Their Retrieval Lists</head><p>Different types of web queries can be generated from TREC2004 topics based on sentence syntax that is the result of analysis by MINIPAR, or other analysis. We will employ a systematic nomenclature w-x_y-z to represent them: the first two symbols w-x pertain to properties of a TREC topic, and y-z describes web retrieval properties. Each symbol takes values as follows:</p><p>w = {t, d, ..} denotes the source of a query such as t (from title section of a TREC topic), d (description), etc.;</p><p>x = {n, v, a, p, w, ..} denotes the word syntactic categories or other properties of the source w that are employed to help define web queries. These include phrases (p), nouns (n), verbs (v), adjectives, content words (w), etc. y = {s, r, ..} denotes the mode of web retrieval such as single (s), window rotation (r), etc. z = {s, f, ..} denotes the granularity of retrieved items that can be snippets (s), full page (f), mixtures (sf), etc.;</p><p>When no ambiguity arises, the notation will be used to denote the web queries, the alternate queries derived from web retrieval, and the resultant retrieval lists by these alternate queries from the target collection. As can be seen, numerous types of web queries (and their resulting alternate queries and retrieval lists) can be obtained based on the various parameter settings. The runs we have used in TREC2004 are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Queries from Title Section of TREC Topic</head><p>For retrieval using the title section, the original TREC queries (and their retrieval results) are denoted as t-init and t-prf: for initial and 2 nd stage retrieval based on PRF. The initial queries average to about 3.1 terms (with some 2-word phrases), and none longer than 5. Four Google queries are defined based on various title word and web retrieval properties. These are listed below:</p><p>t-init --initial query from the title section of a topic processed with stop-word removal, Porter stemming, and some 2-word phrases. t-prf(10,90) --PRF query expansion with 10 top documents and 90 top terms from t-init retrieval. is the union of the other two sets. The measures used for evaluation include: MAP (mean average precision), P10 (mean precision at 10 documents retrieved), #0P10 (number of topics without relevant documents at 10 documents retrieved) or its percentage #0P10%, and area measure defined in <ref type="bibr" coords="4,139.90,276.82,70.62,9.94" target="#b8">(Voorhees 2004</ref>) which can be viewed as a weighted sum of the lowest 25% MAP values, with weights favoring the lower ones.</p><p>It can be seen from Table <ref type="table" coords="4,211.96,314.74,5.52,9.94" target="#tab_4">1</ref> that for the weak queries (Hard50), initial results of t-init is much better than t-prf (2 nd stage retrieval) in the 'area' and '#0P10%' measures (although MAP is worse), while the reverse is true for New50. New50 are stronger (easier) queries because results are better than Hard50 for all queries and for all evaluation measures. This suggests that PRF is detrimental to weak queries based on the 'robust' evaluation measures. In all cases, PRF is effective for MAP values, which may be viewed as a measure for strong queries. The All100 set has the Hard50 and New50 combined, and out of the 25 lowest performing t-init queries only 4 belong to the New50 set. Its 'area' measure behaves like Hard50 and drops after PRF.</p><p>The other four alternate queries defined by web-assistance all provide much better area measures for Hard50 as well as for All100 compared to t-init or t-prf. Additional improvements for the Hard50 queries (and other query sets) were explored by combination of the above retrieval lists. We find good combination coefficients for two lists with area measure improvement as the objective by using grid search with steps of 0.1 or 0.05. Lists were combined recursively up to four. Using t-init or t-prf as basis for combination ensures that the resultant retrieval list would not be too far off base. Our search is not exhaustive and the resultant combination is not optimal. On the other hand, we do not want to over-train on the Hard50 or the other sets either. Our 'title' submissions for TREC2004 consists of 4 runs (bolded) as follows: (to simplify reading, the number of snippets or full pages returned from web retrieval and the number of term output for alternate queries are suppressed):</p><p>pircRB04t1 -initial retrieval t-init based on the original TREC2004 titles; pircRB04t2 -2 nd stage retrieval t-prf based on PRF; pircRB04t3 -retrieval based on web-assistance that is a combination of the following retrieval lists with corresponding coefficients: &lt;t-init:0.15, t-w_f-s:0.4, t-w_sf-s:0.35, t-w_s-s:0.1&gt;; pircRB04t4 -retrieval based on web-assistance that is a combination of the following: &lt;t-init:0.2, t-w_sf-s:0.3, t-w_s-s:0.2, t-pw_sf-s:0.3&gt;; pircRB04t5 -un-submitted run that will be used later for 'td' retrieval. It is based on webassistance that is a combination of the following: &lt;t-prf:0.1, t-w_f-s:0.4, t-w_sf-s:0.3, t-pw_sf-s:0.2&gt;.</p><p>The first two submissions based on initial and PRF queries t-init and t-prf without web-assistance will not be competitive for robust evaluation, but they may be good for query ranking prediction (Section 4). pircRB04t3 and pircRB04t4 make use of different combinations of the initial retrieval with other alternate queries. Training with TREC2003 data shows that using initial retrieval as basis is more preferable. The Hard50 results for these combinations are shown in Table <ref type="table" coords="5,117.83,352.66,4.14,9.94" target="#tab_8">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Queries from Description Section of TREC Topic</head><p>This section shows how alternate queries are formed when the longer description section of a TREC topic is used as query. These average out to 7.9 terms (with 2-word phrases) and about 70% of the queries are 6 terms or longer. Here we need term selection from the longer description section as was done last year <ref type="bibr" coords="5,278.06,441.22,82.50,9.94">(Laszlo et.al. 2004</ref>) in order to avoid cases of no web output. Term selection is difficult and could be often erroneous. This year we introduce a simple method of 'window rotation' to avoid salient term selection. We define a window of 5 terms and let it rotate through the description statement. For a description of m terms (m&gt;5), there will be m such queries for web retrieval for each description. Returned web pages are ranked and selected based on their occurrence frequency in these m lists, and with frequency &gt;= 2. When a query has &lt;=5 terms, the 'window rotation method' defaults back to single retrieval. The resultant list of web items is used to define an alternate query. These two types of web retrieval are denoted as s (single) and r (rotation) in the last character of the query nomenclature.</p><p>d-init --initial query from the description section of a topic processed with stop-word removal, Porter stemming, and some 2-word phrases. d-prf(10,90) --PRF query expansion with 10 top documents and 90 top terms from an initial retrieval. d-pn_s-s(100,60) -this method assumes that the most content-bearing terms in a sentence are in phrases, followed by nouns. We consider three types of phrases in the order of: phrases identified by MINIPAR, phases containing nouns only (i.e. n gov n), and those also containing adjectives (adj gov n). We take all single words from the higher-ranking phrases, add to them all the nouns until the query contains six terms. We retrieve the top 100 snippets and select from them the 60 most frequent terms to form alternate query. d-pnv_s-s(100,60) -this is similar to the previous except that verbs are also included with nouns for selection purposes.  After exploring various retrieval list combinations, our 'description' submission for TREC2004 also consists of 4 runs (bolded) as follows (with the Hard50 results for these combinations shown in Table <ref type="table" coords="6,129.24,497.26,4.06,9.94" target="#tab_10">4</ref>):</p><p>pircRB04d1 -an un-submitted run based on initial retrieval using d-init. pircRB04d2 -retrieval based on d-prf, after PRF; pircRB04d3 -retrieval based on web-assistance that is a combination of the following: &lt;d-prf:0.3, d-pn_s-s:0.15, d-w_sf-r:0.25, d-pw_sf-r:0.3&gt;; pircRB04d4 -retrieval based on web-assistance that is a combination of the following: &lt;d-prf:0.35, d-pnv_s-s:0.2, d-w_sf-r:0.2, d-w_s-r:0.25&gt;; pircRB04d5 -same retrieval as pircRB04d3, but with different prediction for topic difficulty ranking;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Combining Title and Description Retrieval</head><p>Because of difficulties of selecting salient words, especially from the much longer narrative section of a TREC topic, we explored combination runs of Section 2.3 (Title queries) and Section 2.4 (Description queries) with the hope to further boost effectiveness. These are:</p><p>pircRB04td2 -retrieval list based on the combination of the following title and description retrieval lists: &lt;pircRB04d3:0.5, pircRB04t5:0.5&gt;; pircRB04td3 -retrieval list based on the combination of the following title and description retrieval lists: &lt;pircRB04d3:0.45, pircRB04t3:0.55&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Results and Discussions 2.6.1 Title Queries</head><p>Table <ref type="table" coords="7,118.08,175.54,5.52,9.94" target="#tab_8">3</ref> shows the 'title' only runs named: pircRB04t1 to pircRB04t4. pircRB04t1 represents the simplest 'initial retrieval' run while pircRB04t2 refers to results of a second retrieval using pseudo-relevance feedback (PRF). The purpose of these runs is to see if we can predict their query difficulty ranking better than other more complicated runs that involve the web and retrieval combination. We know that their MAP and other evaluation measures will not be competitive with web-assisted runs. The web-assisted runs are named pircRB04t3 to pircRB04t5, but the last one was not submitted (indicated with a * in table) because of limits of submissions. Also shown in the table are the 'best' and 'median' results of each measure evaluated over all submitted 'title' runs.</p><p>Table <ref type="table" coords="7,118.68,302.02,5.52,9.94" target="#tab_8">3</ref> displays the set of 249 topics segmented into different sets, and their evaluation. In the following discussion, the results of 't3' (we will suppress the prefix pircRB04 from now on) will be used as an example to understand the evaluation values better. The 'All249' set contains the whole set of topics and differs from the 'Old200' (consisting of topics from TREC-6 to 8 and TREC2003) by the unseen 'New49' set (new for TREC2004). Set 'Hard50' (used in TREC2003) is a subset of the TREC6-8 topics within 'Old200'. The 'New49' query set has the high MAP value of 0.4008, which means this set has fewer low performing topics. If one looks at #0P10 which is the number of topics with zero precision at 10 documents retrieved, 'New49' has only 3, which when added to the 11 of 'Old200' gives a total of 14 for the full topic set 'All249'. The set 'Hard50' has more difficult topics not only because it has low MAP value of 0.1827, but also because its #0P10 value of 6 is 12% of its 50 topics compared to 11 or 5.5% for 'Old200'. The 'area' measure of 'Old200' 0.0333 is close to that of 'All249' 0.0376 since the differing set 'New49' probably has few contribution to the low performing topics. However, its 'area' measure .0333 is more than twice the .0158 value for 'Hard50', which needs some explanation. 'Old200' is a superset of 'Hard50' and it has quite an extra number of low performing topics to interleave into those from 'Hard50'. If we evaluate the area measure on the same number of topics, the value returned from 'Old200' would be smaller than that of 'Hard50'. However, the area definition is to use 25% of the lowest performing topics, which means counting 4 times as many topics in 'Old200' compared to 'Hard50'. This leads to the use of higher performing topics for the area value, and these contribute to a higher area value of .0333 for 'Old200'.</p><p>It is seen from Table <ref type="table" coords="7,186.65,567.70,5.52,9.94" target="#tab_8">3</ref> that between the two web-assisted runs 't3' and 't4', they achieved 12 of the total of 16 best precision values submitted for all 'title' runs. These are bolded. The 4 uncovered values are those of the 'Hard50' set. 't3' by itself has 8 of these 12 and seems to perform better than 't4'. The un-submitted run 't5' itself would have 7 measures (italicized) surpassing TREC2004 'top' values.</p><p>In general, the runs did very well for the 'New49' and the large sets 'Old200' and 'All249', but not as good for the #0P10 and area measures for the 'Hard50' set. For example, the 'Hard50' area for 't3' and 't4' are 0.0158, and 0.185 respectively. These are below the 'best' value of 0.0263. However, they are substantially better than the values of 0.0062 and 0.0036 achieved via initial retrieval 't1' or PRF 't2' respectively without web assistance. In general, our strategy of combining normal ad-hoc and alternate query retrieval lists works well, and training also   generalizes nicely to the "New49' set. For example, run 't3' for 'New49' has all measures improve over 't1' initial and 't2' PRF runs. In particular, the 'best' area value of 0.089 improves over the 0.0209 of 't2' by 325% and over the 't1' value of 0.0259 by 243%. Fig. <ref type="figure" coords="10,447.84,99.70,4.58,9.94">2</ref> provides visual comparison of the two weak query measures for various topic subsets employing titles alone. It shows how poor the initial and PRF title queries (without web-assistance) perform in comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Description Queries</head><p>Table <ref type="table" coords="10,118.26,175.54,5.52,9.94" target="#tab_10">4</ref> tabulates four runs we submitted using only the 'Description' section of a topic to define queries. These are similarly named as before: pircRB04d2 to pircRB04d5. In addition, 'd1' (leaving out the prefix pircRB04) is an un-submitted run that corresponds to results from an initial retrieval only. 'd2' is based on PRF, and 'd3', 'd4' are web-assisted runs. 'd5' retrieval is the same as 'd3' except that ranking of topic difficulty was done different. It is observed that 10 of the 16 best 'description' values were achieved between our 'd3' and 'd4' submissions. Of these two, 'd4' itself accounts for 7 of the 10 'best' values and generally has the better performance than 'd3' except for the 'area' measure of the 'Hard50' set. For these description runs, the area measures did not achieve the 'top' values except for 'New49' set, and covered all the best values for the measure #0P10 except for Hard50. As in title queries, our area measures for Hard50 set (0.0144 for 'd3' and 0.013 for 'd4') are below the best achieved of 0.0205. For the 'New49' query set, 'd4' has all evaluation measures equal or improve over those of 'd1' initial or 'd2' PRF retrievals. In particular, area measure of 0.0739 improves over 0.0404 or 0.0409 by more than 80%.</p><p>For the 'area' measure, web-assisted runs for 'description' are worse than for 'title' runs in all topic sets; but this is not true for the initial or PRF runs. For example for the New49, initial and PRF 'description' queries have area values of approximately .04, better than the corresponding 'title' values of about .02 to .025. However for web-assisted runs, these values for 'description' lie between .0648-.0749, worse than the corresponding 'title' values of .0668-.0890. It seems that the title words form effective web queries that produce effective alternate queries to supplement the original title retrieval, while this is not true starting from 'descriptions'. As before, Fig. <ref type="figure" coords="10,517.40,441.22,4.59,9.94" target="#fig_2">3</ref> shows that initial and PRF retrievals have lower effectiveness in '%0P10' and 'area' measures compared with web-assisted runs. The difference is not as wide as for 'title' queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Title + Description Queries</head><p>Two runs were submitted using a combination of selected 'title' and 'description' retrieval lists of the previous sections and their results are shown in Table <ref type="table" coords="10,373.93,529.78,4.15,9.94" target="#tab_12">5</ref>. pircRB04td2 combines 'd3' (coefficient .5) and 't5' (.5), while td3 combines 'd3' (.45) with 't3' (.55). A third un-submitted run is td4 that combines 'd3' (.5) with 't4' (.5). Results show that 'td2' and 'td3' runs cover 10 of the 16 'best' values, and all measures are above median. The area values are less than the 'best' values except for the 'New49' topic set where 'td3' achieves the best value of 0.0917. Performing this combination between title and description retrieval lists generally brings additional small boost in the effectiveness measures. Fig. <ref type="figure" coords="10,355.07,605.62,4.59,9.94" target="#fig_3">4</ref> compares these 'td' runs with 't3' which is the best title run from Table <ref type="table" coords="10,256.43,618.34,4.15,9.94" target="#tab_5">2</ref>, for various topic subsets. 'td2' and 'td3' perform equal or better than 't3' in all '%0P10' measures, while 'td4' has similar behavior for the 'area' results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">General Observations</head><p>Fig. <ref type="figure" coords="10,108.37,681.58,4.59,9.94">5</ref> summarizes the behavior of different topic lengths and topic sets for the four effectiveness measures: MAP, P10, #0P10% and area based on our submissions. It can be seen that the easiness of the query sets can be depicted as the following order: New49, All249, Old200 and    Hard50 (e.g. New49 plot is higher than others and (not quite so) lower for the 0P10% plot). It is also seen that, except for one or two exceptions, our performance with respect to topic sections is in the following order: 'td', 't', and 'd' (plots slant upwards to the right for all measures except 0P10% which slants downwards).</p><p>TREC2004 Robust track exercise shows that: 1. using the web as an all-domain thesaurus to improve topic representation is effective; 2. data fusion (combination of retrieval lists from web-assisted alternative queries) is effective for improving retrieval, and for low performing topics in particular.</p><p>Compared to all submissions, our results perform very favorably. Table <ref type="table" coords="13,414.83,99.70,5.52,9.94" target="#tab_13">6</ref> shows the comparison using median AP and P10 values. For example, the web-assisted pircRB04t3 average precision has 206 topics better than median, 4 equal and 39 worse. 28 of the 206 have best average precision and none has worst. Tabulated MAP statistics for the increasing method index (e.g. from pircRB04t1 to pircRB04t4) also show the effectiveness of web-assistance for MAP values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Robust Track -Predicting Topic Ranking by MAP</head><p>Over the years, TREC has provided 200 topics that have evaluation results with respect to the TREC-8 collections. From these topics one can form queries of different flavors (like topical content, wordings, specificity, etc.) and sizes (short or verbose, etc.). They span over a whole range of difficulties from MAP values of 0.0 to 1.0. A new task in TREC2004 Robust Track is to see whether the ranking of a given set of new queries can be predicted according to their retrieval difficulties based on training from the Old200 set.</p><p>From the task description, we decided that regression via the Old200 set for training and for parameter setting could be a viable approach. The 200 old topics will provide the features for characterizing the topics, and their MAP evaluation will provide the performance measure of difficulty. The major consideration at hand is to choose a set of reasonable features and to select the type of regression procedure that will do the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Choice of Features</head><p>Each query derived from a TREC topic section is represented by a set of terms. Experience from IR shows that terms with low document frequencies are good discriminators and therefore good for retrieval, and vice versa. We decided to use very simple features: locate the top 3 terms in each query and use their log D k (log document frequency) plus their corresponding occurrence probability in the query (q k /L q ). Here, D k , q k and L q are respectively the document frequency of term k in the collection, the frequency of term k in query, and the length of a query (with minimum threshold set as 15 as used in PIRCS). These 6 basic features can be identified from a query description without any retrieval, and can be used to predict query ranking during initial retrieval, or for that matter, any subsequent retrievals.</p><p>After a retrieval is done, be it initial, PRF, etc., one will have a document list ranked by retrieval status value corresponding to a query under focus. We tried using coordinate matching measure as a feature to indicate how good or how bad a retrieval may be. When a query is short, like from the title section of a TREC topic, document having all the query terms could be a good indication of relevance, and vice versa. We count how many top-ranked documents n x that have x=|q|, |q|-1, and |q|-2 words of the query, and use log(n x ) as additional features. |q| is the number of unique terms in a query. Together with |q|, these give a total of 10 features for our query characterization. These are specific to the initial query and its subsequent representation, and have been shown during n-fold training and validation to be mildly useful.</p><p>For this track, there are only 200 queries for training, which would limit the number of features one could employ. There can be more sophisticated features to use but due to time limitations, we have only considered these simple ones. Since there are different types of retrieval (initial, PRF, web-assisted, etc.), prediction of topic ranking can be done at these different stages.</p><p>Coupled with the different query sizes (title, description, etc.), one can combine to provide a large number of retrievals and predictions. We have provided predictions only for the 10 runs   <ref type="table" coords="14,319.76,292.42,4.15,9.94" target="#tab_14">7</ref>. Prediction for pircRB04d5 made use of linear regression and will be discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Support Vector Regression</head><p>Support vector regression (SVR) is a relatively recent machine learning method for fitting sampled observations to a function f and using it to predict unseen data <ref type="bibr" coords="14,421.19,369.22,100.89,9.94;14,90.00,383.38,23.83,9.94" target="#b6">(Smola and Schölkopf 2004)</ref>. In our task, given the training data {(x 1 ,y 1 ),.. (x 200 ,y 200 )}, where x i is a feature vector describing topic i ε Old200 and y i its average precision, we want to predict the performance of y j given features x j (j ranges over the set New49) via the trained function. Like SVM for classification, it is nonlinear (and may be important for predicting MAP values via our simple features), scalable to large samples of high feature number efficiently (by being dependent on support vectors only and scalar product between data), and has been shown to be effective for various prediction problems (see for example <ref type="bibr" coords="14,292.14,459.58,98.68,9.94">Scholkopf, et.al. 1999)</ref>.</p><p>We employed an implementation of SVR (called LIBSVM) that was developed at National Taiwan University and downloadable <ref type="bibr" coords="14,266.64,497.69,110.43,10.80" target="#b0">(Chang and Lin 2004)</ref>. The simplest version, called epsilon-SVR or ε-SVR <ref type="bibr" coords="14,200.76,511.49,72.40,10.80" target="#b7">(Vapnik 1995)</ref> uses a radial-basis function as kernel although other functions such as polynomial can be defined. Several parameters need to be decided for the algorithm to work such as: epsilon ε (which defines a neighborhood around f where errors are considered tolerable), g (a parameter for the radial-basis function) and C (a cost value). For them, we have relied on the default values of ε = 0.1, g = 1/#feature, and C = 1.0.</p><p>Training was done using the set Old200. Six basic term features were extracted from their queries for pircRB04t1. These are fed into LIBSVM and a model file (Mod200) was produced. For testing, the topic set All249 with their feature vectors together with Mod200 were used and LIBSVM produced predicted MAP values for each topic. Sorting the MAP values produced the ranking needed for submission. This procedure does not involve any retrieval, and would be very useful if it can predict topic difficulty.</p><p>The other runs all involved a set of ranked documents from a retrieval such as PRF or webassistance. Only the top-ranked 200 retrieved documents were considered for feature extraction for a total of 10 features as discussed in Section 3.1. The procedure for prediction is the same as for six features discussed in the previous paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results of SVR Prediction</head><p>The official evaluation measure for this sub-task is Kendall's tau between the observed ranking and the predicted ranking of a topic set. tau = +1/-1 means complete agreement or disagreement in the two ranking results. For the title only experiments (Table <ref type="table" coords="15,410.52,164.02,3.96,9.94" target="#tab_4">1</ref>), the simple 6-feature prediction of pircRB04t1 surprisingly gives a tau = 0.356 for the set All249. There is positive correlation between the lists of predicted and observed rankings. The other title runs all employ 10 features, and their tau values improve: varying between 0.459 (pircRB04t4) to 0.488 (pircRB04t2). When these tau values are converted to test statistics and consulted with the normal curve, they indicate that these rank correlations are all statistically highly significant. These high values are due to the fact that 200 of the 249 topics are used for training. When one considers the New49 set, which are unseen topics, their tau values dropped substantially to between 0.121 (pircRB04t4) to 0.223 (pircRB04t3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t3 (10 features): Observed AP vs Predicted</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig.6: pircRB04t3 prediction of Average Precision for All249 and New49 Topics</head><p>For the description only runs, tau for All249 varies from 0.318 (pircRB04d5) to 0.533 (pircRB04d4); the latter is also the best value among all description submissions. Their New49 values vary from 0.082 (pircRB04d2) to 0.211 (pircRB04d4) and not as good as for titles. pircRB04d5 provides an exception with New49 tau value of 0.330. This prediction is discussed in Section 3.2. The two title + description runs have similar results as for the description runs.</p><p>The observation is that New49 topic prediction is better with the title topics, in particular pircRB04t3, using ε-SVR. The correlation is small. Fig. <ref type="figure" coords="16,340.75,162.94,4.54,9.94">6</ref> shows in greater details how prediction of individual topic precisions behaves for this run. The upper plot is for All249 set and the lower plot for the New49 set. The upper plot seems to show that the trend has been learnt reasonably well, although quite a few failed at the individual level. The lower plot shows that overall trend is not predicted (R 2 negative). Apart from the fact that regression does not predict average precision values well (e.g. lowest observed value is about 0.02 vs. predicted lowest value of 0.11), one can visually see that of the set of five worst ranked topics observed, prediction agrees with two of them. There is only one agreement for the set of five most effective observed. This information is not sufficiently accurate for one to employ specially tailored methods for the weakest or strongest topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Linear Regression</head><p>One of our submissions (pircRB04d5, which has the same retrieval as pircRB04d3 except for the topic difficulty prediction) was used to test linear regression as a prediction tool. For this run average query term weight (av-qtwt) of the PIRCS system was employed as feature because test of initial and PRF queries show that it has a small positive correlation with average precision. The query term weight qtwt involves the inverse collection term frequency and is defined in <ref type="bibr" coords="16,90.00,390.58,59.12,9.94" target="#b3">(Kwok 1995)</ref> as (N w = number of tokens in the collection; F k = collection frequency of term k): qtwt = log [q k /(L q -q k )*(N w -F k )/F k ] As discussed in Section 2.4, each of the topic in Old200 produces one standard TREC initial query and three alternate web-assisted queries, namely: d-init, d-pn_s-s, d-w_sf-r and d-pw_sfr. These generate 8 sets of weights, 4 for initial retrieval and 4 for PRF defined from initial retrieval. Each of these 8 sets defines an average weight (av-qtwt) for each topic and they are employed as 8 features to predict the average precision value of pircRB04d5 using linear regression. This differs from the feature choice of Section 3.1 in that the same attribute from many query types of the same topic are used, rather than several attributes from the same query. This appears quite costly. However, all the weight files are generated as a by-product of the procedure that results in the combination run pircRB04d5 of 4 retrievals. These 4 retrievals can be speeded up substantially via parallel hardware and processing if available.</p><p>As displayed in Table <ref type="table" coords="16,191.26,580.30,4.15,9.94" target="#tab_10">4</ref>, the result of this run is surprising. Its Kendall's tau value for All249 is 0.318 which is not too low compared with our other submissions. However, for the unseen topics New49, its prediction has tau = 0.330, substantially better than other runs. This appears to suggest that the diverse query types of the same topic may contribute clues for difficulty prediction. This has to be studied in greater details with more experimental observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>For a second year in a row, we have demonstrated that our approach of exploiting the web (probing the web to return relevant/related output to define alternate queries for a given query, and combine their retrieval lists) to enhance ad hoc retrieval is viable and effective. This method can improve 'area' measure over 300% and reduce the number of '0P10' queries by 50% compared to initial retrieval using short (title) queries of the unseen New49 set. Similarly, it improves over 180% in 'area' while maintaining the same number of '0P10' queries when medium length (description) queries were employed. These medium 'description' queries do not provide as effective alternate queries as the short 'titles' because they are longer and requires term selection to probe the web. Appropriate term selection is difficult. We introduced a window rotation method that does not rely on term selection -it is stable and effective but time consuming. An important topic of research is how to do term selection and compose web queries from given ad hoc queries when these are medium to long in length.</p><p>We have experimented with support vector regression to predict new query effectiveness from old data using some simple features. Results however are not sufficiently accurate to be used to do individual query tailoring. Apart from the fact that the choice of features might be improved for prediction, the number of old data (200 queries) for training may also be not sufficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,90.00,74.38,422.00,9.94;6,126.00,86.98,160.40,9.94;6,90.00,99.70,426.33,9.94;6,126.00,112.30,99.69,9.94;6,90.00,125.02,424.22,9.94;6,126.00,137.62,367.01,9.94;6,126.00,150.22,159.27,9.94"><head></head><label></label><figDesc>d-w_sf-r(40,90) -content words retrieving 40 full pages and their snippets by window rotation, maximum 90-term alternate queries; d-w_s-r(100,60) -content words retrieving 100 snippets by window rotation; maximum 60 term alternate query output; d-pw_sf-r(40,90) -MINIPAR-identified 2-word phrases (and used as phrases for web retrieval) plus content words retrieving 40 web pages with their snippets by window rotation, maximum 90 term alternate queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,117.24,701.86,377.44,9.94"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: 'Description' Runs for Various Topic Subsets: '%0P10' and 'area' Values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,100.32,700.90,411.36,9.94"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: 'Title + Description' Runs for Various Topic Subsets: '%0P10' and 'area' Values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,694.18,418.24,22.54"><head>t-w_f-s(40,60) -</head><label></label><figDesc>content words from the title to retrieve 40 full web pages that define alternate queries with 60-term output. Page sizes are restricted to &lt;30K. Special file types such as pdf, doc, etc are eliminated(similar to 'qts' of Lazslo et.al. 2003).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,86.98,428.46,35.26"><head>t-w_sf-s(40,90) -</head><label></label><figDesc>content words retrieving 40 full pages and their snippets, maximum 90-term alternate queries output. This differs from the previous mainly in adding snippets to fullpage web output and defines a different size for the alternate queries.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,90.00,125.02,432.13,111.10"><head>t-w_s-s(100,60) -content</head><label></label><figDesc></figDesc><table coords="4,90.00,125.02,432.13,111.10"><row><cell>words retrieving 100 snippets; maximum 60-term alternate query</cell></row><row><cell>output.</cell></row><row><cell>t-pw_sf-s(40,90) -MINIPAR-identified 2-word phrases (and used as phrases for web retrieval)</cell></row><row><cell>plus other content words retrieving 40 web pages with their snippets; maximum 90-term</cell></row><row><cell>alternate queries.</cell></row><row><cell>The above web queries are chosen to provide diverse alternate queries, as well as based on their</cell></row><row><cell>training results. TREC2003 Robust Track topics are used for training, and results are shown in</cell></row><row><cell>Table 1 in three sets: 100 topics (All100), New50 (new during 2003) and Hard50, where All100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,90.00,441.22,432.24,179.98"><head>Title All100 (2003) New50 (2003) Hard50 Queries MAP #0P10% area MAP #0P10% area MAP #0P10% Area t-init</head><label></label><figDesc></figDesc><table coords="4,90.00,441.22,432.24,179.98"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Thus, the web-based alternate</cell></row><row><cell cols="8">representations by themselves are effective in improving the performance of these weak title</cell></row><row><cell cols="8">topics. The MAP values are tabulated for information only and not considered for training</cell></row><row><cell>purposes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>.1972</cell><cell>12</cell><cell>.0122 .2871</cell><cell>8</cell><cell>.0332 .1074</cell><cell>16</cell><cell>.0062</cell></row><row><cell>t-prf</cell><cell>.2414</cell><cell>17</cell><cell>.0108 .3496</cell><cell>8</cell><cell>.0439 .1332</cell><cell>26</cell><cell>.0036</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Web-Assisted</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t-w_f-s</cell><cell>.2734</cell><cell>13</cell><cell>.0257 .3667</cell><cell>8</cell><cell>.0767 .1801</cell><cell>18</cell><cell>.0128</cell></row><row><cell cols="2">t-w_sf-s .2672</cell><cell>11</cell><cell>.0229 .3467</cell><cell>10</cell><cell>.0424 .1877</cell><cell>12</cell><cell>.0153</cell></row><row><cell>t-w_s-s</cell><cell>.2618</cell><cell>11</cell><cell>.0186 .3720</cell><cell>8</cell><cell>.0511 .1516</cell><cell>14</cell><cell>.0112</cell></row><row><cell cols="2">t-pw_sf-s .2662</cell><cell>13</cell><cell>.0193 .3474</cell><cell>12</cell><cell>.0405 .1850</cell><cell>14</cell><cell>.0122</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,141.00,637.30,329.86,9.94"><head>Table 1 : Training Results of Title Queries using TREC2003 Collection</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,90.00,175.54,432.15,255.58"><head>Table 2</head><label>2</label><figDesc>shows the results of training the alternate queries on the TREC2003 data. The initial and 2 nd stage PRF retrieval results of these description queries(d-init, d-prf)  provide better results than the corresponding title queries (t-init, t-prf) except in area measure of All100 for d-init vs. tinit, and #0P10% measure of Hard50 d-prf vs. t-prf. In Hard50, similar observations are true as for titles that area and #0P10% measures are worse for d-prf than for d-init. However, in contrast to title, the alternate queries for 'description' defined from the web are all inferior to d-init or dprf in area measure for Hard50 except for one case (d-w_s-r). These alternate queries are weak because their web queries are not as precise as those derived from the titles. Apparently, humangenerated short queries like the titles can solicit web texts that can form better alternate queries.</figDesc><table coords="6,90.00,302.74,428.40,128.38"><row><cell>Description</cell><cell></cell><cell>All100</cell><cell></cell><cell></cell><cell>New50</cell><cell></cell><cell>Hard50</cell></row><row><cell>Queries</cell><cell cols="3">MAP #0P10% area</cell><cell cols="5">MAP #0P10% area MAP #0P10% area</cell></row><row><cell>d-init</cell><cell>.2342</cell><cell>9</cell><cell cols="2">.0121 .3503</cell><cell>4</cell><cell>.0638 .1182</cell><cell>14</cell><cell>.0063</cell></row><row><cell>d-prf</cell><cell>.2784</cell><cell>17</cell><cell cols="2">.0125 .4044</cell><cell>4</cell><cell>.0839 .1524</cell><cell>30</cell><cell>.0049</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Web-Assisted</cell><cell></cell><cell></cell></row><row><cell>d-pn_s-s</cell><cell>.2575</cell><cell>20</cell><cell cols="2">.0057 .3012</cell><cell>18</cell><cell>.0186 .1380</cell><cell>22</cell><cell>.0031</cell></row><row><cell>d-pnv_s-s</cell><cell>.2485</cell><cell>13</cell><cell cols="2">.0082 .3550</cell><cell>6</cell><cell>.0384 .1421</cell><cell>20</cell><cell>.0028</cell></row><row><cell>d-w_sf-r</cell><cell>.2147</cell><cell>18</cell><cell cols="2">.0070 .2852</cell><cell>16</cell><cell>.0172 .1442</cell><cell>20</cell><cell>.0035</cell></row><row><cell>d-w_s-r</cell><cell>.2497</cell><cell>11</cell><cell cols="2">.0090 .3626</cell><cell>8</cell><cell>.0343 .1368</cell><cell>14</cell><cell>.0055</cell></row><row><cell>d-pw_sf-r</cell><cell>.2543</cell><cell>14</cell><cell cols="2">.0076 .3737</cell><cell>10</cell><cell>.0249 .1373</cell><cell>18</cell><cell>.0044</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,124.92,446.98,362.26,9.94"><head>Table 2 : Training Results of Description Queries using TREC2003 Collection</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,168.21,453.46,282.70,212.07"><head>Table 3 : Results of 'Title' only Runs 'Titile' Runs: '%0P10' &amp; 'area' Values</head><label>3</label><figDesc></figDesc><table coords="8,168.21,493.59,282.70,171.94"><row><cell>0.24</cell><cell></cell><cell></cell><cell></cell><cell>t1:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell cols="3">%0P10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">(lower better)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1 0.12 0.14 0.16 %0P10 &amp; area</cell><cell></cell><cell></cell><cell>t1</cell><cell>t2</cell><cell></cell><cell></cell><cell cols="2">area (higher better)</cell></row><row><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell>t3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell>t4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>old200</cell><cell>new49</cell><cell>hard50</cell><cell cols="2">all249</cell><cell>old200</cell><cell>new49</cell><cell>hard50</cell><cell>all249</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,133.32,493.59,345.20,208.24"><head>init t2: prf t3: web-assist t4: web-assist : top among submissions t3: 344% of initial t3: 50% of initial Fig</head><label></label><figDesc></figDesc><table coords="9,84.48,75.12,445.97,362.12"><row><cell cols="4">DESCRIPTION (TREC2004) web-asist.</cell><cell>web-asist.</cell><cell>web-asist.</cell><cell>PRF</cell><cell>initial</cell></row><row><cell></cell><cell></cell><cell></cell><cell>.3.15.25.3</cell><cell>.35.2.2.25</cell><cell>.3.15.25.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>best</cell><cell cols="6">median pircRB04d5 pircRB04d4 pircRB04d3 pircRB04d2 *pircRB04d1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Old200</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>0.508</cell><cell>0.4535</cell><cell>0.504</cell><cell>0.5075</cell><cell>0.504</cell><cell>0.462</cell><cell>0.437</cell></row><row><cell>map</cell><cell>0.3158</cell><cell>0.2634</cell><cell>0.3139</cell><cell>0.3158</cell><cell>0.3139</cell><cell>0.299</cell><cell>0.2572</cell></row><row><cell>#0P10</cell><cell>15</cell><cell>30</cell><cell>15</cell><cell>17</cell><cell>15</cell><cell>32</cell><cell>20</cell></row><row><cell>area</cell><cell>0.0303</cell><cell>0.0092</cell><cell>0.0245</cell><cell>0.0234</cell><cell>0.0245</cell><cell>0.0112</cell><cell>0.0115</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>New49</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>0.551</cell><cell>0.4633</cell><cell>0.5408</cell><cell>0.5469</cell><cell>0.5408</cell><cell>0.4857</cell><cell>0.4673</cell></row><row><cell>map</cell><cell>0.4074</cell><cell>0.2992</cell><cell>0.4056</cell><cell>0.4074</cell><cell>0.4056</cell><cell>0.3717</cell><cell>0.3166</cell></row><row><cell>#0P10</cell><cell>1</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>1</cell></row><row><cell>area</cell><cell>0.0739</cell><cell>0.0245</cell><cell>0.0648</cell><cell>0.0739</cell><cell>0.0648</cell><cell>0.0404</cell><cell>0.0409</cell></row><row><cell>Kendall's</cell><cell></cell><cell></cell><cell>.330</cell><cell>.211</cell><cell>.175</cell><cell>.082</cell><cell>0.043</cell></row><row><cell>tau</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hard50</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>0.382</cell><cell>0.316</cell><cell>0.372</cell><cell>0.382</cell><cell>0.372</cell><cell>0.322</cell><cell>0.306</cell></row><row><cell>map</cell><cell>0.1635</cell><cell>0.1328</cell><cell>0.1635</cell><cell>0.1622</cell><cell>0.1635</cell><cell>0.1524</cell><cell>0.1182</cell></row><row><cell>#0P10</cell><cell>4</cell><cell>9</cell><cell>5</cell><cell>6</cell><cell>5</cell><cell>15</cell><cell>7</cell></row><row><cell>area</cell><cell>0.0205</cell><cell>0.0071</cell><cell>0.0144</cell><cell>0.013</cell><cell>0.0144</cell><cell>0.0049</cell><cell>0.0063</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>All249</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P10</cell><cell>0.5153</cell><cell>0.4546</cell><cell>0.5112</cell><cell>0.5153</cell><cell>0.5112</cell><cell>0.4667</cell><cell>0.443</cell></row><row><cell>map</cell><cell>0.3338</cell><cell>0.2686</cell><cell>0.3319</cell><cell>0.3338</cell><cell>0.3319</cell><cell>0.3133</cell><cell>0.2689</cell></row><row><cell>#0P10</cell><cell>17</cell><cell>34</cell><cell>17</cell><cell>18</cell><cell>17</cell><cell>37</cell><cell>21</cell></row><row><cell>area</cell><cell>0.0313</cell><cell>0.0105</cell><cell>0.0273</cell><cell>0.0276</cell><cell>0.0273</cell><cell>0.0142</cell><cell>0.0141</cell></row><row><cell>Kendall's</cell><cell>0.533</cell><cell>0.318</cell><cell>0.318</cell><cell>0.533</cell><cell>0.514</cell><cell>0.503</cell><cell>0.52</cell></row><row><cell>tau</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="8,146.98,691.90,331.54,9.94"><p>.2: 'Title' Runs for Various Topic Subsets: '%0P10' and 'area' Values</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,204.72,453.46,202.48,9.94"><head>Table 4 : Results of 'Description' only Runs</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="11,168.77,464.86,274.16,206.90"><head>Table 5 : Results of Combining Title &amp; Description Runs 'Title + Description' Runs: '%0P10' &amp; 'area' Values</head><label>5</label><figDesc></figDesc><table coords="11,168.77,521.53,274.16,150.24"><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t3:</cell><cell cols="2">for comparison</cell></row><row><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell>td2: &lt;d3,t5&gt;</cell><cell></cell></row><row><cell></cell><cell>0.14 0.16</cell><cell></cell><cell></cell><cell>%0P10 (lower better)</cell><cell cols="3">td3: &lt;d3,t3&gt; : top among submissions</cell></row><row><cell>%0P10 &amp; area</cell><cell>0.1 0.12</cell><cell cols="2">as t3 td3: same</cell><cell></cell><cell cols="2">td3: 103% of t3</cell><cell>better) area (higher</cell></row><row><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>old200</cell><cell>new49</cell><cell>hard50 all249</cell><cell>old200</cell><cell>new49</cell><cell>hard50</cell><cell>all249</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="12,174.12,394.32,263.72,206.60"><head>Table 6 : Comparing PIRCS All249 Results with Median</head><label>6</label><figDesc></figDesc><table coords="12,223.20,394.32,155.60,180.20"><row><cell></cell><cell></cell><cell>Median AP</cell><cell></cell></row><row><cell>Run ID</cell><cell>Best</cell><cell cols="2">(&gt;/=/&lt;) Worst</cell></row><row><cell>pircRB04t1</cell><cell>6</cell><cell>108/10/131</cell><cell>0</cell></row><row><cell cols="2">pircRB04t2 21</cell><cell>171/4/74</cell><cell>0</cell></row><row><cell cols="2">pircRB04t3 28</cell><cell>206/4/39</cell><cell>0</cell></row><row><cell cols="2">pircRB04t4 16</cell><cell>208/4/37</cell><cell>0</cell></row><row><cell cols="2">pircRB04d2 21</cell><cell>154/8/87</cell><cell>0</cell></row><row><cell cols="2">pircRB04d3 19</cell><cell>189/18/42</cell><cell>0</cell></row><row><cell cols="2">pircRB04d4 14</cell><cell>199/6/44</cell><cell>0</cell></row><row><cell cols="2">pircRB04td2 5</cell><cell>215/2/32</cell><cell>0</cell></row><row><cell cols="2">pircRB04td3 3</cell><cell>215/2/32</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="14,90.00,266.33,353.51,36.03"><head>Table 7 : Features used for Topic Difficulty Prediction discussed</head><label>7</label><figDesc>in Section 2, and summarized in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by a <rs type="funder">U.S. Govt. DST/ATP</rs> contract <rs type="grantNumber">2003*H532600*000</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xHdY3VJ">
					<idno type="grant-number">2003*H532600*000</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,90.00,340.06,428.97,9.94;17,90.00,352.66,183.34,9.94" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="17,237.70,340.06,216.97,9.94">LIBSVM: a Library for Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">C-C And</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,377.98,432.15,9.94;17,90.00,390.58,431.99,9.94;17,90.00,403.30,49.80,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,294.72,377.98,227.43,9.94;17,90.00,390.58,91.83,9.94">Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D &amp;</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,189.96,390.58,298.39,9.94">Proceedings of HLT-NAACL 2003 Text Summarization Workshop</title>
		<meeting>HLT-NAACL 2003 Text Summarization Workshop</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="3" to="0501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,428.50,431.98,9.94;17,90.00,441.22,432.15,9.94;17,90.00,453.82,432.14,9.94;17,90.00,466.54,219.39,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,354.43,428.50,167.55,9.94;17,90.00,441.22,150.57,9.94">TREC 2003 Robust, HARD and QA Track Experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,270.22,441.22,251.92,9.94;17,90.00,453.82,110.34,9.94">Information Technology: The Twelfth Text REtrieval Conference, TREC 2003</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>US GPO; Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="510" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,491.74,432.17,9.94;17,90.00,504.46,53.51,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,186.74,491.74,270.27,9.94">A network approach to probabilistic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,466.84,491.74,55.33,9.94">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="324" to="353" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,529.78,432.07,9.94;17,90.00,542.38,110.88,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,167.29,529.78,309.73,9.94">PRINCIPAR -an efficient, broad-coverage, principle-based parser</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,486.72,529.78,35.35,9.94;17,90.00,542.38,52.06,9.94">Proc of COLING-94</title>
		<meeting>of COLING-94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="482" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,567.70,432.06,9.94;17,90.00,580.30,161.75,9.94" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m" coord="17,377.52,567.70,144.54,9.94;17,90.00,580.30,102.71,9.94">Advances in Kernel Methods -Support Vector Learing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,606.58,431.98,9.94;17,90.00,619.90,107.88,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,263.99,606.58,178.43,9.94">A tutorial on Support Vector Regression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,462.36,606.58,59.62,9.94;17,90.00,619.90,48.98,9.94">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,645.22,341.97,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="17,173.52,645.22,182.81,9.94">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,90.00,670.54,432.11,9.94;17,90.00,683.14,432.13,9.94;17,90.00,695.74,408.36,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,205.09,670.54,239.60,9.94">Overview of the TREC 2003 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,469.67,670.54,52.43,9.94;17,90.00,683.14,432.13,9.94;17,90.00,695.74,39.98,9.94">Information Technology: The Twelfth Text REtrieval Conference, TREC 2003. E.M. Voorhees &amp; L.P. Buckland</title>
		<meeting><address><addrLine>US GPO; Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
