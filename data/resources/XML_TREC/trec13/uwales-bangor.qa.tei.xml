<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.19,58.51,397.74,16.74">Bangor at TREC 2004: Question Answering Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.13,87.15,77.07,11.70"><forename type="first">Terence</forename><surname>Clifton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Wales</orgName>
								<address>
									<settlement>Bangor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.93,87.15,81.92,11.70"><forename type="first">William</forename><surname>Teahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Wales</orgName>
								<address>
									<settlement>Bangor</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.19,58.51,397.74,16.74">Bangor at TREC 2004: Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CE2B7E61D3E3BF662E1F3ED32959231E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question answering</term>
					<term>knowledgeable agents</term>
					<term>knowledge grid</term>
					<term>regular expressions</term>
					<term>regular expression induction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in the 2004 Text Retrieval Conference. We present additions and modications to the QITEKAT system, initially developed as an entry for the 2003 QA evaluation, including automated regular expression induction, improved question matching, and application of our knowledge framework to the modied question types presented in the 2004 track. Results are presented which show improvements on last year's performance, and we discuss future directions for the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The 2004 evolution of the TREC QA track moved more towards a context based approach to providing the question set. A specied target subject was given, which was then used as the basis for a series of corresponding questions. These questions were made up of three types:</p><p>• Factoid</p><p>• List</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Other</head><p>The TREC QA guidelines <ref type="bibr" coords="1,178.64,499.52,10.51,9.96" target="#b6">[7]</ref> describe the `other' questions as follows:</p><p>The nal question in each series is an explicit `other' question that should be interpreted as tell me other interesting things about this target I didn't know enough to ask directly. This nal question is roughly equivalent to the TREC 2003 QA track's denition question.</p><p>Our 2004 entry was a minor evolution of our 2003 system, and focused mainly on automating the creation of further regular expression patterns, which are the key to the generation of our Knows and KnowsAbout relations. In addition to the automated regular expression production, we have implemented an improved question matching architecture, and improved the application of our logic framework to the `list' and `other' question types.</p><p>We describe these modications, and their eect on system performance in the 2004 QA task, in this paper, which is organised as follows. Firstly we give a general recap of the system architecture which was developed for the TREC 2003 QA task, and provide an overview of the performance in that task in relation to other systems entered (Section 2). We then present the additions and modications made to the QITEKAT system for the 2004 QA evaluation. In Section 4 we present initial results received from NIST, and a brief analysis of how the system performed. Sections 5 describes our plans for the future development of the QITEKAT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>The QITEKAT system is a practical implementation of our logic-based framework for implementing knowledgeable agents that will become the core component for our multi-agent information retrieval systems. A brief overview of the framework and the system architecture is included below, and we refer the reader to <ref type="bibr" coords="1,538.49,412.06,10.51,9.96" target="#b0">[1]</ref> for a more detailed explanation.</p><p>In <ref type="bibr" coords="1,335.01,442.76,9.96,9.96" target="#b4">[5]</ref>, we describe a framework for designing and implementing knowledgeable agents and Knowledge Grids. The framework is based on three types of knowledge relations: Knows, KnowsAbout, and Knowl-edgeableAbout. These are used to dene what an agent knows, what it knows about, and whether an agent has been judged to be knowledgeable by other agents. Essentially, the architecture is based on using knowledgeable agents as a middle layer between the user and the information resources. A key aspect of the design is the use of information extraction coupled with compression-based language modelling technology <ref type="bibr" coords="1,310.99,586.22,10.51,9.96" target="#b5">[6]</ref> and the use of a conversational agent that the user asks questions of and receives answers from the system. In this architecture, there are three types of objects: users, knowledgeable agents and information resources. The users do not interface directly with the information resources. Instead, they must go through a knowledgeable agent who eectively acts as a knowledge broker in determining which of the information resources are likely to contain an answer to the user's questions. Notice that knowledgeable agents may need to go though The knowledge framework proposed by Teahan <ref type="bibr" coords="2,287.74,272.92,9.96,9.96" target="#b4">[5]</ref>, which is used as the basis for the extraction of knowledge relations from suitable source documents essentially relies on a reverse approach to standard Q&amp;A techniques. Rather than using the question text to retrieve a subset of documents from the test collection, which are then analysed to nd an answer, the QITEKAT system was designed to parse the entire collection, forming a number of question/answer relations before any actual questions are posed. The TREC Q&amp;A Track uses the AQUAINT document collection as its source corpus, which consists of over 1 million documents, totalling 375 million words. The system was developed based around three main stages:</p><p>• Documents are normalised;</p><p>• Knowledgeable agents tag and extract Question/Answer pairs;</p><p>• Input questions are analysed, and answers are retrieved and ranked.</p><p>Figure <ref type="figure" coords="2,94.34,545.81,4.98,9.96">2</ref> shows the component make up, and how each of the individual modules interacts with the rest of the system.</p><p>Overall performance of the QITEKAT system on the TREC2003 evaluation was satisfactory -table <ref type="table" coords="2,267.30,598.17,4.98,9.96">1</ref> shows the nal results for the factoid component evaluation <ref type="bibr" coords="2,63.00,622.08,9.96,9.96" target="#b7">[8]</ref>. The initial development timescale meant that the system was designed to only account for two distinct question types (Who and When). Manual examination of the 2003 test questions showed that the system was equipped to deal with a possible 124 questions, of which it correctly answered 107, giving an accuracy score of 86.2% (Figure <ref type="figure" coords="2,127.62,693.82,4.43,9.96" target="#fig_0">1</ref>) <ref type="bibr" coords="2,139.81,693.82,9.96,9.96" target="#b0">[1]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODIFICATIONS AND ADDITIONS</head><p>In this section we describe the additions and modications we made to the QITEKAT system for the 2004 TREC evaluation. Minor bug-xing and optimisation was carried out to improve performance, but the majority of development eort went into the changes detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Regular Expressions</head><p>Regular expressions were developed to pattern match sentence construction for common question types. This approach is similar to that used by Ravichandran and Hovy in <ref type="bibr" coords="2,349.19,496.54,9.96,9.96" target="#b2">[3]</ref>. It was important to make the best use of the previously tagged documents, and to ensure that regular expressions used by the system were not too specic as to require multiple expressions for a single question construct. This led us to develop a dynamic substitution system, whereby a generic regular expression was populated at runtime using the tagged contents of the sentence it was being applied to. We maintained a data store of basic regular expression formats, suitable substitution types, an allowable answer type, and a generic question format for the particular relation.</p><p>By using the named entities already tagged in the document, the system can create a number of actual regular expressions, substituting suitable types into the ANSWER and OBJECT locations. For example, given the sentence: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Grid</head><p>A small number of broad domain types were used (PEOPLE, GEOGRAPHY, BUSINESS, MISC ).</p><p>Our initial regular expressions were hand-crafted, but it became quickly evident that this would not be ecient, either in terms of the time taken, or the required generality of the expressions. Using previous Question-Answering data as a source, we were able to implement an automated system to generate regular expressions, based on a combination of entity type tagging and proximity matching. Given a source document, a question and an answer (that exists in the document), the following procedure is followed:</p><p>1. Extract the `subject' of the question using traditional speech tagging techniques, and PPM compression based language modeling (see <ref type="bibr" coords="4,503.32,599.41,10.51,9.96" target="#b5">[6]</ref> for further details).</p><p>2. Analyse the source document for the proximity of the known answer to the subject.</p><p>(a) Partial matching is applied here to ensure that subjects are recognised in the answer document. 4. This subtext is then parsed and a regular expression generated.</p><p>(a) Stopwords are ignored.</p><p>(b) Named Entity tags are inserted where possible to generalise the regular expression.</p><p>Our experiments with dierent proximity limits (number of characters) on the AQUAINT corpus led us to adopt a proximity level of 150 characters, which offered the best compromise between performance and the quality of expressions. (Larger proximity expressions lose generality, and thus eectiveness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Example</head><p>Given the following information:  We would extract a question subject of `Kenneth Lenihan', and parse the source text to nd a suitable match. Through the use of our partial matching algorithm <ref type="bibr" coords="5,101.29,693.82,10.51,9.96" target="#b0">[1]</ref> we are able to recognise a match between `Kenneth Joseph Lenihan' in the source text and `Kenneth Lenihan' as our subject, resulting in a matching string:</p><p>Kenneth Joseph Lenihan, a New York research sociologist who helped rene the scientic methods used in criminology, died May 25</p><p>From this string we apply our proximity check to determine if the match is within a suitable distance. In this example, the subject is 101 characters from the answer, and thus the match is accepted. We then generalise the string to a suitable regular expression, by removing stopwords and inserting named entity classes where appropriate. Part-of-speech groups in close proximity to the answer, which correlate to the question text are kept to ensure the meaning is retained: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Improved Question Matching</head><p>Upon completion of the 2003 evaluation, it was clear that in a number of cases where incorrect, or no answer was returned by the system, this was not because the answer was not found, but due to an inability to cross-match the entered question with the question in the knowledge base. The original system used a very limited string matching system, coupled with identication of question types (What, When, etc). Unfortunately, even small discrepancies in entered and known questions would result in a NIL match. The 2003 evaluation proved that this was insucient, and that a better system was needed.</p><p>We implemented a vector matching system, whereby entered and known questions were compared based on their non-stopword content. In addition to simple positional matching, we added two extra factors that inuenced match-weight:</p><p>• Subject -Matching the subject explicitly (i.e. not a partial match, see below) boosted a question's match-weight.</p><p>• Word frequency -Using the same dictionary we adopted for tagging, we tested matched words for frequency, boosting weights for less frequent words, as a match was likely to have more signicance.</p><p>To handle the changes brought in to the question format for the 2004 task regarding subject specication, we implemented a simple system to substitute the subject for occurrences of personal pronouns in the question text, and applied our partial matching algorithm <ref type="bibr" coords="6,278.48,305.35,10.51,9.96" target="#b0">[1]</ref> to provide greater generality to the produced questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Handling `Other' and `List' Questions</head><p>The knowledge framework which forms the basis of the QITEKAT system was designed to incorporate the notion of context, and this was practically implemented through the use of domain specication both at the Knows and KnowsAbout level of our architecture. This gave us an excellent starting point when addressing the `list' and `other' question types which formed part of the 2004 TREC QA evaluation. In its original incarnation, the QITEKAT system specied very broad domain constructs for each question it extracted from the source corpus (PEOPLE, GEOG-RAPHY, etc), which were then grouped within each agent to form our knowledge base. These were generated using a two-step approach:</p><p>1. Determine the most likely question/answer subject. This was done using traditional speech tagging techniques, and PPM compression based language modeling (see <ref type="bibr" coords="6,175.41,577.58,10.51,9.96" target="#b5">[6]</ref> for further details) to extract named entities from the question. When required, selection was made based on frequency of occurrence in the parent document, with the assumption that more frequent occurrences were likely to be the focus of information.</p><p>2. Our named entity tagger was then applied to this `focus' object to determine a broad domain classication (PERSON entity type yields PEOPLE domain, etc.)</p><p>The new format, with explicit specication of the subject of the question, correlated well with our method, as we were simply able to remove our second-step, and store the named entity as our domain classication. These specic domains were then grouped into Knows-About relations stored at each agent and generalised where possible using our partial matching algorithm. For example:</p><p>• Bush;</p><p>• George Bush; and</p><p>• George W. Bush occurring in three dierent Knows relations in the same document would be grouped into a single Knows-About relation. In this case, the more explicit domain -George W. Bush -would be used, to allow for improved matching at later stages in the process.</p><p>Once we had our KnowsAbout information, answering `list' queries was a matter of retrieving all corresponding relations for a particular question subject, applying our improved question-matching algorithm, as detailed previously, to nd answers that correspond to those required by the list, and returning the results. In generating answers to the `other' questions we would have ideally liked to reconstruct useful `nuggets' from the question/answer pairs our system extracted, but time didn't permit this element of the system to be completed, and it has been scheduled for a future revision. As a result we were only able to return our known answers, and provide no context from the question, which made the information of little use (in fact, technical issues meant no answers were returned at all -see results section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Example</head><p>Given the following question/answer pairs extracted from a document:</p><p>• When was John Lennon born? October 9, 1940</p><p>• When did Lennon die? December 8, 1980</p><p>• How did John Lennon die? Assassinated</p><p>The system would rst extract the named entities from each of the questions to determine their subjects</p><formula xml:id="formula_1" coords="6,320.94,672.86,66.68,30.91">• John Lennon • Lennon • John Lennon</formula><p>And produce the Knows relations: K1 = Knows(Doc-001-Agent, "Domain: John Lennon", "When was John Lennon born?", "October 9, 1940", 1.0). K2 = Knows(Doc-001-Agent, "Domain: Lennon", "When did Lennon die?", "December 8, 1980", 1.0). K3 = Knows(Doc-001-Agent, "Domain: John Lennon", "How did John Lennon die?", "Assassinated", 1.0).</p><p>Using partial matching, the system would recognise `Lennon' and `John Lennon' as the same subject class, and produce the following KnowsAbout relation (using the more explicit subject as the topic, and the general entity type as the domain):</p><p>KnowsAbout(Doc-001-Agent, "Domain: PEOPLE", "John Lennon", {K 1 ,K 2 ,K 3 }, 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>Performance of the QITEKAT system in the 2003 evaluation was a useful benchmark on which to build, and initial results made available by NIST for the 2004 task show that gains have been made in both `factoid' and `list' categories, as well as in the overall f-score. The 2003 results were hindered by the limited development time, which meant regular expressions were only created for a small subset of question types. This years' performance reects the addition of the automated expression system, and the corresponding increase in the number of regular expressions generated, and thus the question types covered.  <ref type="figure" coords="7,328.47,334.22,4.98,9.96">6</ref> shows a graphical representation of performance over the two years that Bangor has participated in the evaluation.</p><p>These preliminary results show a good performance gain for the QITEKAT system, with a 38% improvement in factoid scores, and an almost three-fold increase in overall performance, which is very promising. Elimination of some technical issues which meant the 2003 evaluation was not a complete run of the system, can account for some of the gains (although the 2004 run suered its own problems which limited the analysis time also), while auto-generating the regular expressions rather than hand-crafting them was far more ecient in terms of time, and meant that a greater proportion of question types could be targeted and answered by the system.</p><p>Although results for other groups entered in the current evaluation will not be made available until after the TREC conference, based on the known min/max/average scores for each of the question types (Factoid, List, Other) we can make a preliminary assessment as to the performance of the QITEKAT system in relation to the other entries. Table <ref type="table" coords="7,498.14,606.99,4.98,9.96" target="#tab_6">4</ref> shows the relative comparison of the QITEKAT performance with the know minimum, maximum and average scores for the 2004 evaluation * . These gures imply that factoid * Overall minimum, maximum and average scores are not provided by NIST and are estimated by assuming that the MIN/MAX/AVERAGE in each category are from the same system run. performance on the 2004 evaluation was very goodwell above average, and the `list' performance was also above the recorded. The technical issues with `other' questions resulted in a zero result, which pulled down the overall score. This was still above average, however, and a good improvement on last year. <ref type="bibr" coords="8,127.37,156.10,49.09,10.60">QITEKAT</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FUTURE DIRECTIONS</head><p>A number of additions and modications have been recognised which could provide improved performance, and are outlined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Improved Named Entity Classication</head><p>Although the Named Entity classier developed as part of the system performs well, for the purposes of Question Answering it is important to broaden the scope of the system, and introduce further NE types in order to allow for more accurate answer matching. Sekine et al. present a system oering a far greater number of Named Entity classications <ref type="bibr" coords="8,187.70,414.58,9.96,9.96" target="#b3">[4]</ref>, which we feel would be a benecial addition to the overall system architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Synonym substitution</head><p>The present system architecture oers no methods for word substitution, which is a limiting factor, both in terms of matching questions with appropriate knowledge relations, and also extracting relations from document texts. The addition of a synonym system, such as WordNet <ref type="bibr" coords="8,118.60,522.19,10.51,9.96" target="#b1">[2]</ref> would enable a greater number of sentence constructs to be identied and extrapolation of questions to form multiple queries, oering a far greater chance of successful responses. As an example, take the question text:</p><p>When did Charles Bronson die?"</p><p>In the present system, this will match only those relations with an equivalent question construct, which may result in no answer being found. With synonym substitution, however, the query would be reformulated as:</p><p>When did Charles Bronson pass away?" which may provide a positive match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Past Participle Determination</head><p>In a similar vein to synonym substitution, it would be useful to develop a feature within the system to automatically generate past participles of verbs, particularly for condence-ranking through search engine corroboration. When querying a search engine, the system passes the main subjects of a question, so for example, given the question:</p><p>When did Charles Bronson die?"</p><p>The system forms a query using Charles Bronson and Die. It is likely however that in any documents retrieved by a search engine the information that we are interested in would be described using the past participle (died), i.e.</p><p>Charles Bronson died on .."</p><p>Substituting the past participle may result in a more useful query string, and ultimately a greater number (or more accurate) results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Automated Regular Expression Production</head><p>Further development of our automated regular expression system is required to handle multiple subject questions, such as:</p><p>How far is it from New York to Los Angeles?"</p><p>Is New Zealand larger in size than Japan or Great Britain?"</p><p>We are investigating the feasibility of parsing input into two or more atomic questions, which can then be addressed and the answers combined to formulate a response to the original query. This would allow us to extract further information from the source corpus, and extend the number of question types the system is capable of answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">`Other' Questions</head><p>We are currently investigating the problems encountered and the conversion our Question/Answer pairs into useful `nuggets' of information about specied question subjects. We hope to be able to leverage the fact that we explicitly store domain information for all question/answer pairs extracted from the corpus in order to target this particular problem successfully.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,63.00,202.94,238.02,10.60;2,63.00,214.94,23.07,9.21"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Results on TREC 2003 (Who and When questions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,348.69,515.87,162.55,10.60"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Generated regular expression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,63.00,72.81,238.03,204.35"><head>Table 2 .</head><label>2</label><figDesc>Eect of proximity level on regular expression generation<ref type="bibr" coords="5,70.20,231.33,3.87,9.96" target="#b2">3</ref>. If the answer falls within a given proximity threshold to the subject (i.e. is within a certain number of characters either side of the subject), we retrieve the surrounding subtext.</figDesc><table coords="5,75.56,72.81,212.85,98.73"><row><cell cols="3">Proximity No of Regexps No of Questions</cell></row><row><cell>10</cell><cell>152</cell><cell>1.4 million</cell></row><row><cell>20</cell><cell>263</cell><cell>2.8 million</cell></row><row><cell>50</cell><cell>393</cell><cell>3.2 million</cell></row><row><cell>100</cell><cell>469</cell><cell>3.6 million</cell></row><row><cell>150</cell><cell>532</cell><cell>3.7 million</cell></row><row><cell>200</cell><cell>566</cell><cell>3.8 million</cell></row><row><cell>500</cell><cell>579</cell><cell>3.9 million</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,63.00,618.99,237.93,21.22"><head>Table 3 .</head><label>3</label><figDesc>Example source information for regular expression induction</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,66.81,156.10,230.31,93.10"><head>Table 4 .</head><label>4</label><figDesc>Relative comparison of 2004 evaluation results.</figDesc><table coords="8,81.48,156.10,201.04,62.01"><row><cell></cell><cell></cell><cell cols="3">MIN MAX AVE</cell></row><row><cell>Factoid</cell><cell>0.643</cell><cell>0.009</cell><cell>0.770</cell><cell>0.170</cell></row><row><cell>List</cell><cell>0.258</cell><cell>0.000</cell><cell>0.622</cell><cell>0.094</cell></row><row><cell>Other</cell><cell>0.000</cell><cell>0.000</cell><cell>0.460</cell><cell>0.184</cell></row><row><cell>Overall</cell><cell>0.386</cell><cell>0.005</cell><cell>0.656</cell><cell>0.155</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,78.49,90.74,222.53,9.96;9,78.49,102.52,222.53,10.18;9,78.49,114.48,222.53,10.18;9,78.49,126.61,103.48,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,257.22,90.74,43.80,9.96;9,78.49,102.69,178.26,9.96">Bangor at TREC 2003: Q&amp;A and Genomics Tracks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Colquhoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Teahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.92,102.52,19.10,10.18;9,78.49,114.48,217.91,10.18">Proceedings of the Twelfth Text Retrieval Conference</title>
		<meeting>the Twelfth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="600" to="611" />
		</imprint>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,78.49,139.54,222.52,9.96;9,78.49,151.33,222.53,10.18;9,78.49,163.45,19.92,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,130.63,139.54,170.38,9.96;9,78.49,151.49,17.74,9.96">WordNet: An On-Line Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,126.56,151.33,170.02,10.18">International Journal of Lexicography</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,78.49,176.39,222.54,9.96;9,78.49,188.18,222.53,10.18;9,78.49,200.13,140.14,10.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,203.48,176.39,97.56,9.96;9,78.49,188.34,179.65,9.96">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.92,188.18,19.10,10.18;9,78.49,200.13,91.89,10.18">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,78.49,213.24,222.52,9.96;9,78.49,225.03,222.52,10.18;9,78.49,236.98,195.27,10.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,259.82,213.24,41.19,9.96;9,78.49,225.19,110.52,9.96">Extended Named Entity Hierarchy</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nobata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,218.67,225.03,82.35,10.18;9,78.49,236.98,101.02,10.18">Proceedings of the LREC-2002 Conference</title>
		<meeting>the LREC-2002 Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1818" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,78.49,250.09,222.54,9.96;9,78.49,262.05,222.53,9.96;9,78.49,273.83,222.52,10.18;9,78.49,285.79,222.52,10.18;9,78.49,297.91,171.37,9.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,160.37,250.09,140.66,9.96;9,78.49,262.05,222.53,9.96;9,78.49,274.00,73.06,9.96">Knowing About Knowledge: Towards a Framework for Knowledgeable Agents and Knowledge Grids</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Teahan</surname></persName>
		</author>
		<idno>AIIA03.2</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Bangor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Informatics, University of Wales</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Articial Intelligence and Intelligent Agents Tech Report</note>
</biblStruct>

<biblStruct coords="9,78.49,310.85,222.52,9.96;9,78.49,322.64,222.53,10.18;9,78.49,334.59,222.53,10.18;9,78.49,346.71,181.25,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,187.68,310.85,113.34,9.96;9,78.49,322.80,178.65,9.96">Using Compression-Based Language Models for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Teahan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.03,322.64,19.99,10.18;9,78.49,334.59,189.79,10.18">Language Modelling for Information Retrieval</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,78.49,359.49,222.53,10.18;9,78.49,371.44,222.52,10.18;9,78.49,383.19,209.17,10.46;9,78.49,395.14,161.94,10.46" xml:id="b6">
	<analytic>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Q&amp;a</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Guidelines</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/act_part/tracks/qa/qa.04.guidelines.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,277.99,359.49,23.04,10.18;9,78.49,371.44,151.74,10.18">Thirteenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>restricted</note>
</biblStruct>

<biblStruct coords="9,78.49,408.46,222.53,9.96;9,78.49,420.24,222.52,10.18;9,78.49,432.20,215.05,10.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,133.92,408.46,167.10,9.96;9,78.49,420.41,73.97,9.96">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,181.58,420.24,119.43,10.18;9,78.49,432.20,110.83,10.18">Proceedings of the Twelfth Text Retrieval Conference</title>
		<meeting>the Twelfth Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
