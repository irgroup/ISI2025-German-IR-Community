<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.50,81.09,339.00,12.91">Question Answering with QED and Wee at TREC-2004</title>
				<funder ref="#_JfGHb4f">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_yGG6QMv">
					<orgName type="full">German Academic Exchange Service (DAAD)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.55,122.75,52.24,10.76"><forename type="first">Kisuh</forename><surname>Ahn</surname></persName>
						</author>
						<author>
							<persName coords="1,221.01,122.75,50.23,10.76"><forename type="first">Johan</forename><surname>Bos</surname></persName>
						</author>
						<author>
							<persName coords="1,279.62,122.75,71.57,10.76"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName coords="1,359.66,122.75,87.81,10.76;1,447.47,120.46,1.49,7.86"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,118.89,136.70,84.20,10.76"><forename type="first">Tiphaine</forename><surname>Dalmas</surname></persName>
						</author>
						<author>
							<persName coords="1,212.01,136.70,89.73,10.76"><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
						</author>
						<author>
							<persName coords="1,310.03,136.70,96.17,10.76"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Smillie</surname></persName>
						</author>
						<author>
							<persName coords="1,413.96,136.70,79.14,10.76"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.50,81.09,339.00,12.91">Question Answering with QED and Wee at TREC-2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E2B6FA7EE94A8AF68EC3A6DC399576C5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the experiments of the University of Edinburgh and the University of Sydney at the TREC-2004 question answering evaluation exercise. Our system combines two approaches: one with deep linguistic analysis using IR on the AQUAINT corpus applied to answer extraction from text passages, and one with a shallow linguistic analysis and shallow inference applied to a large set of snippets retrieved from the web. The results of our experiments support the following claims: (1) Webbased IR is a good alternative to "traditional" IR; and (2) deep linguistic analysis improves quality of exact answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this report we describe the TREC-2004 entry of the Universities of Edinburgh and Sydney for the questionanswering evaluation exercise. This year we experimented with two complementary QA streams: our QED system developed in previous years <ref type="bibr" coords="1,213.22,536.14,81.29,8.97" target="#b6">(Leidner et al., 2004)</ref>, using traditional IR and deep linguistic processing (see Figure <ref type="figure" coords="1,100.18,560.05,3.60,8.97" target="#fig_0">1</ref>), and Wee, a system developed by Tiphaine Dalmas, using Google and shallow linguistic processing. We were interested in comparing the performances of these two streams, as well as finding out whether they could be successfully combined. We therefore aimed to submit three runs:</p><p>• Run A: Wee</p><p>• Run B: hybrid Wee and QED</p><p>• Run C: QED In the remaining of this paper we will first describe the two systems in detail: Section 2 describes QED, and Section 3 is devoted to Wee. Then we will present and discuss our results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The QED System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-processing and Indexing</head><p>The ACQUAINT document collection which forms the basis for TREC-2004 was pre-processed with a set of Perl scripts, one per newspaper collection, to identify and normalize meta-information. This meta-information included the document ID and paragraph number, the title, publication date and story location. The markup for these last three fields was inconsistent, or even absent, in the various collections, and so collection-specific extraction scripts were required.</p><p>The collection was tokenized offline using a combination of the Penn Treebank sed script and Tiphaine Dalmas' Haskell tokenizer. Ratnaparkhi's MXTERMINA-TOR program was used to perform sentence boundary detection <ref type="bibr" coords="1,343.39,492.87,128.33,8.97" target="#b7">(Reynar and Ratnaparkhi, 1997)</ref>. The result was indexed with the Managing Gigabytes (MG 1.3g) search engine <ref type="bibr" coords="1,343.85,516.79,83.49,8.97">(Witten et al., 1999)</ref>. For our TREC-2004 experiments, we used case-sensitive indexing without stopword removal and without stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval and Passage Segmentation</head><p>Using ranked document retrieval, we obtained the best 100 documents from MG, using the query generated from the question. Since our approach involves full parsing to obtain detailed semantic representations in later stages, we need to reduce the amount of text to be processed to a fraction of each document. To this end, we have implemented QTILE, a simple query-based text segmentation and passage ranking tool. This "tiler" uses the words in the query to extract from the set of documents returned by MG, a set of segments ("tiles"). It does this by shifting a sliding window sentence by sentence over the text stream, retaining all window tiles that contain at least one of the words in the query as well as all upper-case query words.</p><p>Each tile gets assigned a score based on the following: the number of non-stopword query word tokens (as opposed to types) found in the tile; capitalization agreement between the appearance of a term in the query and its appearance in the tile; and the occurrence of 2-grams and 3-grams in both question and tile. The score for every tile is multiplied with a window function (currently a simple triangle function) which weights sentences in the centre of a window higher than in the periphery.</p><p>The tiler is implemented in C++, with linear asymptotic time complexity and constant space requirements. For TREC-2004 we used a window size of 3 sentences and pass forward the top-scoring 100 tiles (with duplicates eliminated using a hash signature test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Syntactic and Semantic Analysis</head><p>We used the C&amp;C parser to parse the question and the text segments returned by the tiler and Wee. The C&amp;C parser does POS-tagging <ref type="bibr" coords="2,147.00,617.55,105.59,8.97">(Curran and Clark, 2003a)</ref> and named entity recognition <ref type="bibr" coords="2,144.82,629.51,103.47,8.97">(Curran and Clark, 2003b)</ref>, identifying named entities from the standard MUC-7 data set (locations, organisations, persons, dates, times and monetary amounts) and then returns CCG derivations, which are mapped into semantic representations <ref type="bibr" coords="2,226.21,677.33,68.30,8.97" target="#b2">(Bos et al., 2004)</ref>. This linguistic analysis is applied both to the question under consideration and the text passages that might contain an answer to the question. The semantic analysis forms the basis for query generation, which is basically a list of the lemmas of the content expressions.</p><p>Our semantic formalism is based on Discourse Representation Theory <ref type="bibr" coords="2,395.83,426.28,100.65,8.97" target="#b5">(Kamp and Reyle, 1993</ref>), but we use extended Discourse Representation Structure (DRS), combining semantic information with syntactic and sortal information. DRSs are defined as ordered pairs of a set of discourse referents and a set of DRS-conditions. The following types of basic DRS-conditions are considered: pred(x,S), card(x,S), event(e,S), and argN(e,x), rel(x,y,S), where e, x, y are discourse referents, S a constant, and N a number between 1 and 3. Questions introduce a special DRS-condition of the form answer(x,T) for a question type T. We call this the answer literal; answer literals play an important role in answer extraction.</p><p>Implemented in Prolog, we reached a semantic coverage of around 95%. Each passage or question is translated into one single DRS; hence DRSs can span several sentences. To deal with pronouns in the questions, basic techniques for pronoun resolution are implemented as well. A set of DRS normalisation rules are applied in a post-processing step, thereby dealing with active-passive alternations, question typing, inferred semantic information, and the disambiguating of noun-noun compounds. The resulting DRS is enriched with information about the original surface word-forms and POS-tags (see Figure <ref type="figure" coords="2,529.21,713.20,3.60,8.97" target="#fig_1">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Question Analysis: Evaluation</head><p>The C&amp;C parser used in the system has been trained on the CCG version of the WSJ Penn Treebank Wall Street Journal Corpus. The original parser performs extremely poorly on questions, due to the small number of questions in the Treebank. <ref type="bibr" coords="3,402.14,140.77,123.96,8.97">However, in Clark et al. (2004)</ref> we show how the parser can be rapidly ported to the question domain.</p><p>The novel porting method relies on the separation of the CCG parsing task into two subtasks: supertagging, in which CCG lexical categories are assigned to words, and then a final parsing phase in which the lexical categories are combined together, producing a parse tree. Since lexical categories contain so much syntactic information, supertagging can be thought of as almost parsing, to borrow a phrase from the TAG parsing literature <ref type="bibr" coords="3,478.68,261.51,61.32,8.97;3,313.20,273.46,46.91,8.97" target="#b1">(Bangalore and Joshi, 1994)</ref>. <ref type="bibr" coords="3,323.16,286.60,73.65,8.97">Clark et al. (2004)</ref> show that, by marking up new data at the lexical category level only, and using a newly trained supertagger with the original parsing model, high accuracy can be achieved for parsing questions. The advantage of this method is that marking up lexical category data is easier than marking up full derivation trees.</p><p>In order to adapt the supertagger to questions, we took around 1,500 questions from the TREC competitions for the years 2000-2003. The questions were automatically POS-tagged and then annotated with lexical categories by Clark, who also corrected any errors made by the POS tagger. The creation of the new question corpus took only a few weeks. The supertagger was then retrained on this new question data. The combination of the new supertagger with the original parsing model is sufficient to produce a highly accurate parser of questions. This is shown by the parser's performance on the TREC-2004 questions. Of the 286 factoid and list questions, the parser produced 277 analyses, yielding a semantic coverage of 97%. The number of reasonably correct question-DRSs produced for these analyses was 252 (88% of the total). Incorrect analyses were due to tokenisation problems, POS-tagging errors, CCG-categories that did not appear in the training set, and pronoun resolution. Of the 143 cases of pronouns appearing in the questions, 127 (89%) were correctly resolved. The others were resolved incorrectly due to number disagreement of target and pronoun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Extraction</head><p>The answer extraction component takes as input a DRS for the question, and a set of DRSs for selected passages. It extracts answer candidates from the passages by matching the question-DRS and a passage-DRS, using a relaxed unification method and a scoring mechanism indicating how well the DRSs match each other.</p><p>Matching takes advantage of Prolog unification, using Prolog variables for all discourse referents in the question-DRSs, and Prolog atoms in passage-DRSs. It attempts to unify all terms of the question-DRSs with terms in a passage-DRS, using an A * search algorithm. Each potential answer is associated with a score, which we call the DRS score. High scores are obtained for perfect matches (i.e., standard unification) between terms of the question and passage, low scores for less perfect matches (i.e., obtained by "relaxed" unification). Less perfect matches are granted for different semantic types, predicates with different argument order, or terms with symbols that are semantically familiar according to WordNet <ref type="bibr" coords="4,72.00,230.58,67.38,8.97">(Fellbaum, 1998)</ref>.</p><p>After a successful match, the answer literal is identified with a particular discourse referent in the passage-DRS. This is possible because the DRS-conditions and discourse referents are co-indexed with the surface wordforms of the source passage text (see Figure <ref type="figure" coords="4,252.26,291.02,3.60,8.97" target="#fig_1">2</ref>). This information is used to generate an answer string, simply by collecting the words that belong to DRS-conditions with discourse referents denoting the answer. Finally, all answer candidates are output in an ordered list. Duplicate answers are eliminated, but answer frequency information is added to each answer in this final list. Figure <ref type="figure" coords="4,111.23,375.38,4.98,8.97">3</ref> shows an example output file. The columns designate the question-id, the source, the ranking score, the DRS score, the frequency of the answer, and a list of sequences of surface word-form, lemma, POS-tag and word index. The best answer is selected from this file by calculating a weighted score of the DRS score and frequency. The weights differ per question type, and were determined by running experiments over the TREC-2003 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Wee System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Strategy</head><p>Wee is a web based Question Answering system interfacing an information fusion module, QAAM (Question Answering Answer Model). QAAM is based on the Model-View-Controller design pattern which states that data processing and data rendering should be properly distinguished when engineering a system that deals with both. We apply this pattern to Question Answering: results found on the web are merged into a model and various controllers can access this model and propose a view to the end user. A model may contain several answers at different levels of granularity or aggregation, as well as alternative answers. It may also contain background information, i.e. information that does not correspond to a direct answer but may help for further interpretation.</p><p>A QAAM model is a graph where nodes represent concepts and edges express relationships between them. For instance, for the infamous question Where is the Taj Mahal?, a QAAM model may contain the following nodes: {Agra, India, history, women, Atlantic City, New Jersey, casino, resort}. We discuss below what relations are used and how they are inferred.</p><p>Once a model is generated, a controller can query it and render the provided information. We have a special renderer for TREC, but other renderers can be developed as well: full text renderer, summarizer, multi-media renderer (that grabs pictures on the web related to the information contained by the model), more interesting: a dialog controller that goes back and forth between the user and the model, and can eventually enrich the current model by launching new QA processes.</p><p>The Wee/QAAM architecture consists of three parts: (1) question analysis and web retrieval, (2) model generation and (3) rendering. The final rendering consists in finding a supporting document and collecting answers for one target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Analysis</head><p>Linguistic processing Before being tokenized, each question is reformulated using the target and the question type provided by TREC, by inserting the target as a topicalised expression at the end of the question. This technique allows us to introduce the target into the question without performing pronoun resolution. In the worst case, the question contains redundant information which will in any case be filtered by the query generation module. This was done for both factoid and list questions. For 'other' questions, the reformulated question is simply the target, understood by default as a definition question.</p><p>The question is then tokenized and POS-tagged. We tried out two POS-taggers (C&amp;C and Lingua POS-TAG). It is known that standard POS-taggers do not perform well on questions (for instance, they tend to mistag auxiliary verbs). Therefore, we chose to split the POS-tagging into two steps: using an off-the-shelf tagger followed by a supertagger that we developed ourselves. The supertagger takes as a parameter the tagset of the first tagger and map it onto a smaller set of supertags. This mapping takes into account standard errors from POS taggers and corrects them. Finally, tokens are transformed to lower case except tokens corresponding to named entities, and grouped by their tag family. Also, quotes are preserved and verbs inflected.</p><p>Question typing Wee performs question typing based on five features: the wh-type, the wh-complement, the lexical head of the first NP and lexical head of the auxiliary verb group, the modifier of the first NP (if there is one), and the list of the remaining NPs. This is basically done by a series of look-up processes, which terminate when a question is fully disambiguated. (If this process The question types used in Wee are listed in Figure <ref type="figure" coords="5,278.17,425.01,3.74,8.97">4</ref>.</p><p>We distinguish two types of location and time questions. Any time corresponds to a simple when question which requires some query expansion. Time means there is already a lexical item indicating a time unit, such as in What year was X or When is X's birthday. The same distinction is made for location questions (where versus what country).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web query generation</head><p>The query sent to the web combines the question phrases and expansion keywords selected according to the question type. We used Google as a web search engine, exploiting its special operators, for instance i..j to search on numbers (e.g. 1900..2000 searches for all numbers between 1900 and 2000). We did not use Google's define operator, as it often leads to no results, either because no dictionary has the requested entry or because the word is rare or spelled incorrectly. But we used the number range operator for all the questions expecting a numeric answer.</p><p>The IR process consists in a relaxation loop that starts with a first query that is highly specific and is relaxed if too few answers have been found. The first query is generated by quoting all the NP and verb expressions and combining them with a first series of expansion key-words. This query can then be relaxed by breaking it down into tokens. The second query also uses a different set of expansion keywords (usually fewer).</p><p>Web Filtering and Reranking Once a query has been generated, we simply ask Google for 100 snippets, which are then split into sentences and tokenized. To rerank sentences, we use a scoring based on pattern matching, question words count and the number of different potential answer words.</p><p>A penalty filter is also used to remove web noise, notably "sponsored links" and snippets such as the 1989 World Book Dictionary definition of which indicate a good document but are not contentful as snippets. Snippets coming from certain websites are also penalized, for instance trec.nist.gov and the Answer Bus web sites. Those contain typical QA keywords that add noise to our process (although for our first internal evaluation on TREC 10, they provided many good answers).</p><p>Each sentence receives a score according to the following four criteria:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Minimize the penalty score</head><p>The penalty score is computed on the basis of the penalty filter described above and the number of different potential answer words (i.e. words that are not question or stopwords). If there is no potential answer word, the sentence is highly penalized; otherwise it gets as many points as different answer words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Maximize the question word percentage</head><p>If there are repeated question words, the sentence is penalized; otherwise it gets a score between 0.1 and 1 indicating the percentage of question words that have been found. If there are no question words the score is nonetheless slightly raised (to 0.1) because it is still better than a sentence with too many question words. (We call those "spam snippets".)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Maximize the 'be' score</head><p>A sentence containing the inflected verb be (e.g. was, were, is, are) indicates a potentially useful syntactic structure and therefore a good basis for answer extraction. Sentences from snippets are actually more often phrases or unfinished sentences.</p><p>Therefore this is a good indicator of richer information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Maximize the clue score</head><p>To each question type is associated a list of answer clues defined as regular expressions. Those are patterns for clues, not answers. For instance, currency is a clue to find a currency name in snippet sentences but it is not an answer. So far we have overall 183 patterns for clues that have been gathered manually.</p><p>Sentences for which the penalty score is too high or that do no have any clue are simply removed from the candidate set. 100 web snippets usually generate around 300 sentences. After filtering, we only have around 100 sentences left. Those sentences are then passed on to the modeling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling</head><p>As mentioned above, Wee passes its output to QAAM to generate a model based on a graph structure. Modeling consists of two steps: (1) projection, the process of mapping a set of sentences to a set of nodes; and (2) linking, the process of discovering relationships between the nodes.</p><p>Projection The generation of relevant nodes is done by passing pairs of sentences through the Longest Common Substring (LCS) dynamic programming matrix. Wee is implemented in Haskell and makes use of a lazy algorithm to avoid computing a complete matrix when not necessary. This LCS algorithm was adapted to our needs: instead of comparing the pair of sentences character by character, we compare them token by token using a fuzzy match function based on a lazy version of the edit distance algorithm. For efficiency, each match is cached.</p><p>To see this, consider the subset of sentences produced from a Google query for the question What diseases are prions associated with? as shown in Figure <ref type="figure" coords="6,89.28,547.77,3.74,8.97">5</ref>. First a cache is computed comparing normalized tokens with the edit distance algorithm. The acceptable distance is dynamic, depending on the length of the strings compared. In our example, Encephalopathies, ENCEPHALOPATHIES and Encephalopathy are considered equal. The LCS is then computed for each pair of sentences. For our example we get as matching sublist of tokens: {{Spongiform, Encephalopathies}, {SPONGIFORM ENCEPHALOPATHIES}, {Spongiform, Encephalopathy}}.</p><p>We use the edit distance rather than NLP techniques such as lemmatization or stemming because it allows a match not only between words having the same root but also between words that have been misspelled, which is quite frequent in data coming from the web. Next, substrings are trimmed of stop words and question words if required. For instance, a match such as {diseases, called, Transmissible, Spongiform, En-cephalopathies} is trimmed to {Transmissible, Spongiform, Encephalopathies} because diseases is a question word and called is considered a stop word. Substrings are then checked against answer patterns (regular expressions manually gathered for each question type) and answer stop words, i.e. web stop words or usual stop words. Phrases left after this filtering are considered as good candidates and are selected to be model nodes.</p><p>Linking For each pair of nodes, we select the relations that characterize them. In the current implementation of QAAM we use three relations: two nodes can be EQUIVALENT, one can OCCURS IN the other (and vice-versa with HAS OCCURRENCE) or they can be DIFFERENT. These relations are inferred by computing the intersection of content words between phrases. For instance, we have: This is all based on string matching and thus very shallow. Nevertheless, we end up with graph partitions focusing on specific topics, such as: This technique has the advantage of getting a better frequency count by taking into account co-occurences of words. We also distinguish between families of answers, which prevents the system from producing redundant answers. This selection of graph partitions is actually part of the role of the controller in charge of the final rendering.</p><formula xml:id="formula_0" coords="6,330.14,340.95,192.92,32.88">{Spongiform, Encephalopathy} OCCURS IN {Transmissible, Spongiform, Encephalopathies}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rendering</head><p>Once a model has been generated, one can choose an appropriate controller to provide one or more answers. It is actually possible to select the whole graph, which would correspond to a detailed answer. Alternatively, one can simply select one representative per family (or partition) to build a more compact answer. However, for the TREC exercise we chose to output all the nodes, in order to link the answer back to the AQUAINT corpus, a task for which reformulations of the same type of answer are useful: an answer that is too specific can be difficult to find in the TREC corpus. The controller we used outputs each node of the graph ordered by their partition size. Members of a large family are output first, giving preference to the most specific nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Selecting a supportive TREC document</head><p>Once we have found a web answer with Google, we still need to find a supporting document in the AQUAINT corpus collection in order to meet the requirements for the TREC evaluation exercise. For this we implemented a TREC controller, using the Lucene search engine on the AQUAINT corpus.</p><p>The TREC controller outputs a web answer with two Lucene queries. The first query looks for a co-occurrence of the answer words and question words within a window of 100 tokens (in a TREC document). The second query is the relaxed version of the first: it looks for at least the answer words and if the documents also contains some question words, its ranking is boosted. In both queries, the target is a required element.</p><p>The second query is only used if the first query was not successful. For each document, we take all the sentences where words of the query co-occur. The sentence that maximize the co-occurrence score is output first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Three runs were submitted. Run A (Edin2004A) was solely produced by the Wee system, producing web answers for which a supporting document had been found. Run B (Edin2004B) was produced by the QED system using text extractions produced by Wee. Run C (Edin2004C), finally, was produced by the QED system using traditional IR methods plus text extractions produced by Wee. In both runs B and C, the answer produced by Wee was proposed if QED was not able to find one. We expected Run A to perform fairly well (based on judgements on TREC 2002 questions), and Run B to have more exact answers then Run A (which lacks a sophisticated linguistic analysis). We feared answers found by Run A and B not to be supported by the ACQUAINT corpus, and hoped that Run C would score better on this aspect of evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Factoid questions formed the majority of the questions at the TREC 2004 QA evaluation exercise. Our results over 230 factoid questions are listed in the table below, where W is the number of wrong, U the number of unsupported, X, the number of inexact and R the number of correct answers. As expected, the number of inexact answers was high for runs A and B. (Closer inspection of our inexact answers revealed that the judges were very strict this year in assessing inexact answers.) The number of unsupported answers was also substantial for both runs A and B. Run C was slightly disappointing, we were expecting it to get a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Our 'best' answer was Saloth Sar to question 36.3 Who was its first leader? in the context of target Khmer Rouge. Out of 63 runs we were the only submission that got a correct answer for this question. It is interesting to note that the overlap of our two best runs (A and B) is only 8 correct answers. A better answer selection component could considerably improve our overall system. The table below shows the difference in performance of the three runs distributed over Wee question types, summing over correct, inexact and unsupported answers: 12 4 (33%) 3 (25%) 1 (08%) term 11 1 (09%) 0 (00%) 1 (11%) time 9 1 (11%) 4 (44%) 3 (33%) definition 8 1 (12%) 2 (25%) 1 (12%) famous for 7 1 (14%) 1 (14%) 1 (07%) title 6 2 (33%) 3 (50%) 3 (50%) explanation 6 0 (00%) 0 (00%) 0 (00%) duration 4 0 (00%) 0 (00%) 0 (00%) monetary value 3 2 (66%) 2 (66%) 1 (33%) acronym 3 0 (00%) 0 (00%) 0 (00%) composition 2 1 (50%) 0 (00%) 0 (00%) purpose 2 0 (00%) 0 (00%) 0 (00%) acronym 1 0 (00%) 0 (00%) 0 (00%) frequency 1 0 (00%) 0 (00%) 0 (00%) speed 1 0 (00%) 0 (00%) 0 (00%) Total 230 64 (28%) 63 (27%) 36 (16%)</p><p>We didn't devote much of our research time to list questions, and the bad results clearly underline this. For the 55 list questions we got an average F score of 0.036 for run A, 0.054 for run B, and 0.043 for run C. For our analysis on 'other' questions, a similar story can be told, although the results are not as bad as for the list questions. Over the 64 'other' questions we achieved an average F score of 0.068 for run A, 0.152 for run B, and 0.194 for run C. The latter figure is better higher than the medium score of all the 63 submitted runs. The final scores for our three runs were respectively 0.072 (run A), 0.123 (run B), and 0.105 (run C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Compared to TREC 2003, the two major improvements of the QED system are the use of a more fine-grained question-type ontology, and the utilisation of the CCG parser, accomplishing both higher coverage and precision on both questions and answers. The Wee system, developed by Tiphaine Dalmas, was a completely new component of our TREC-2004 setup.</p><p>In TREC 2004, the overall accuracy of factoid questions of the 63 runs submitted to the QA track ranged between 0.009 and 0.770 (median 0.170). For list questions, the best, median, and worst average F-scores were 0.622, 0.094, and 0.000, respectively. For 'other' questions, the F-scores ranged from 0 to 0.460 (with a median of 0.184).</p><p>The results of the three runs indicate that using the Web for finding answers rather than using standard IR gives better scores for factoids, but not for definition questions. Deep linguistic processing tools gives more exact answers, although the exactness of answers requires considerable improvement in our current system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,329.49,468.00,8.97;2,72.00,341.45,169.03,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The QED system architecture. (Dashed lines represent processing streams for questions, while solid lines represent processing streams for answers.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.00,672.55,226.80,8.97;3,72.00,684.50,226.80,8.97;3,72.00,696.46,226.80,8.97;3,72.00,708.41,226.80,8.97;3,72.00,720.37,68.63,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of an extended DRS for TREC-2004 question 36.3. The words and POS-tags are co-indexed with the discourse referents and DRS-conditions, and the DRS is enriched with information produced by the named entity recogniser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,203.95,147.41,204.10,8.97"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Example output file of answer extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,313.20,381.91,226.80,8.97;6,313.20,393.86,226.80,8.97;6,313.20,405.82,91.86,8.97"><head></head><label></label><figDesc>which gives us a simple notion of entailment, namely Transmissible Spongiform Encephalopathies → Spongiform Encephalopathy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,72.00,73.96,461.92,183.12"><head></head><label></label><figDesc>http://www.portfolio.mvm.ed.ac.uk/ Transmissible Spongiform Encephalopathies . http://kobiljak.msu.edu/CAI/Pathology/ SPONGIFORM ENCEPHALOPATHIES ( PRION DISEA SES ) A. IN-TRODUCTORY CONCEPTS ; CHARACTERISTICS OF PRIONS 1. http://www.bseinfo.org/dsp/dsp locationContent BSEInfo.org The Source For Bovine Spongiform Encephalopathy ... Interesting nodes are usually specific nodes, i.e. those that do not have children generated by OCCUR IN relations. For instance, within the encephalopathy family, {Transmissible, Spongiform, Encephalopathies, or, TSEs} is a good candidate because many nodes in the family have occurences in this node, but it does not occur itself as a whole in another node.</figDesc><table coords="7,133.07,144.34,345.87,8.97"><row><cell>Figure 5: Google snippets for question 10.3 What diseases are prions associated with?</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Dalmas is supported by the <rs type="institution">School of Informatics, University of Edinburgh</rs>. Leidner is supported by the <rs type="funder">German Academic Exchange Service (DAAD)</rs> under scholarship <rs type="grantNumber">D/02/01831</rs> and by <rs type="person">Linguit GmbH</rs> (research contract <rs type="grantNumber">UK-2002/2</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yGG6QMv">
					<idno type="grant-number">D/02/01831</idno>
				</org>
				<org type="funding" xml:id="_JfGHb4f">
					<idno type="grant-number">UK-2002/2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,83.96,72.34,77.71,7.83;3,95.91,84.29,65.76,7.83;3,95.91,96.25,83.69,7.83;3,72.00,120.16,89.67,7.83;3,72.00,132.11,179.32,7.83;3,72.00,156.02,35.87,7.83;3,83.96,179.94,101.62,7.83;3,89.93,191.89,95.64,7.83;3,89.93,203.85,83.69,7.83;3,89.93,215.80,71.73,7.83;3,89.93,227.76,71.73,7.83;3,89.93,239.71,83.69,7.83;3,89.93,251.67,89.67,7.83;3,89.93,263.62,65.76,7.83;3,83.96,287.53,89.67,7.83;3,89.93,299.49,83.69,7.83;3,89.93,311.44,77.71,7.83;3,89.93,323.40,83.69,7.83;3,89.93,335.35,89.67,7.83;3,89.93,347.31,77.71,7.83;3,89.93,359.26,77.71,7.83;3,89.93,371.22,77.71,7.83;3,83.96,395.13,83.69,7.83;3,119.82,407.08,47.82,7.83;3,119.82,419.04,47.82,7.83;3,119.82,430.99,53.80,7.83;3,113.84,442.95,125.53,7.83;3,119.82,454.90,119.56,7.83;3,119.82,466.86,131.51,7.83;3,119.82,478.81,119.56,7.83;3,119.82,490.77,131.51,7.83;3,119.82,502.72,125.53,7.83;3,119.82,514.68,185.30,7.83;3,119.82,526.64,101.62,7.83;3,119.82,538.59,101.62,7.83;3,119.82,550.55,107.60,7.83;3,119.82,562.50,125.53,7.83;3,119.82,574.46,125.53,7.83;3,119.82,586.41,113.58,7.83;3,119.82,598.37,119.56,7.83;3,119.82,610.32,125.53,7.83;3,119.82,622.28,113.58,7.83" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,101.89,84.29,59.78,7.83;3,95.91,96.25,77.71,7.83;3,137.75,179.94,41.84,7.83;3,137.76,287.53,35.87,7.83;3,89.93,299.49,83.69,7.83;3,89.93,311.44,11.96,7.83;3,137.76,323.40,35.87,7.83;3,89.93,335.35,5.98,7.83;3,137.76,335.35,41.84,7.83;3,89.93,347.31,5.98,7.83;3,137.76,359.26,29.89,7.83;3,89.93,371.22,5.98,7.83">TRECTYPE&apos;: &apos;FACTOID&apos;],1)</title>
		<idno>2002:arg2</idno>
	</analytic>
	<monogr>
		<title level="m" coord="3,72.00,120.16,89.67,7.83;3,72.00,132.11,179.32,7.83;3,72.00,156.02,17.93,7.83;3,137.76,203.85,35.87,7.83;3,89.93,215.80,71.73,7.83;3,89.93,227.76,71.73,7.83;3,89.93,239.71,83.69,7.83;3,89.93,251.67,83.69,7.83">Who&apos;), p(2002,was), p(2003,its), p(2004,first), p(2005,leader)</title>
		<imprint>
			<date type="published" when="1001">1001. 2001. 2006. 1001. 2001. 2002. 2003. 2004. 2005. 2006. 2002. 2002</date>
			<biblScope unit="volume">1001</biblScope>
			<biblScope unit="page">p</biblScope>
		</imprint>
	</monogr>
	<note>I-PER&apos;). pred(x0,&apos;Rouge&apos;), 1002:pred(x0,single), 2001:answer(x2,general,person. event(e3,be), 2003:pred(x0,neuter), 2003:pred(x0,single), 2003:rel(x1,x0,of), 2004:pred(x1,first), 2005:pred(x1,leader), 2005:pred(x1,single</note>
</biblStruct>

<biblStruct coords="3,83.96,646.19,5.98,7.83;8,313.20,151.68,55.54,10.76;8,308.22,169.74,231.78,8.07;8,323.16,179.70,216.83,8.07;8,323.16,189.66,216.83,8.07;8,323.16,199.63,171.02,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,376.74,179.70,163.25,8.07;8,323.16,189.66,95.07,8.07">Disambiguation of super parts of speech (or supertags): Almost parsing</title>
		<author>
			<persName coords=""><forename type="first">Joshi</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,434.33,189.66,105.66,8.07;8,323.16,199.63,56.99,8.07">Proceedings of the 15th COL-ING Conference</title>
		<meeting>the 15th COL-ING Conference<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="154" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,308.22,217.56,231.78,8.07;8,323.16,227.52,216.83,8.07;8,323.16,237.48,216.83,8.07;8,323.16,247.45,216.83,8.07;8,323.16,257.41,214.73,8.07;8,308.22,275.34,231.78,8.07;8,323.16,285.30,216.83,8.07;8,323.16,295.27,216.83,8.07;8,323.16,305.23,216.84,8.07;8,323.16,315.19,122.02,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,517.95,227.52,22.05,8.07;8,323.16,237.48,198.96,8.07;8,378.01,285.30,161.99,8.07;8,323.16,295.27,11.01,8.07">Widecoverage semantic representations from a CCG parser</title>
		<author>
			<persName coords=""><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,323.16,247.45,216.83,8.07;8,323.16,257.41,132.56,8.07;8,352.98,295.27,187.02,8.07;8,323.16,305.23,212.33,8.07">Proceedings of the SIGDAT Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;04)</title>
		<meeting>the SIGDAT Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;04)<address><addrLine>Geneva, Switzerland; Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
	<note>Proceedings of the 20th International Conference on Computational Linguistics (COLING &apos;04)</note>
</biblStruct>

<biblStruct coords="8,308.22,333.12,231.78,8.07;8,323.16,343.09,216.83,8.07;8,323.16,353.05,216.84,8.07;8,323.16,363.01,216.83,8.07;8,323.16,372.98,205.73,8.07;8,308.22,390.91,231.78,8.07;8,323.16,400.87,216.83,8.07;8,323.16,410.83,216.83,8.07;8,323.16,420.80,216.84,8.07;8,323.16,430.76,70.97,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,352.70,343.09,187.30,8.07;8,323.16,353.05,45.76,8.07;8,353.31,400.87,186.69,8.07;8,323.16,410.83,42.51,8.07">Investigating GIS and smoothing for maximum entropy taggers</title>
		<author>
			<persName coords=""><forename type="first">Clark</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,385.63,353.05,154.37,8.07;8,323.16,363.01,216.83,8.07;8,323.16,372.98,78.94,8.07;8,385.10,410.83,154.90,8.07;8,323.16,420.80,150.36,8.07">Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)</title>
		<meeting>the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)<address><addrLine>Budapest, Hungary; Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran and Clark</publisher>
			<date type="published" when="2003">2003. 2003. 2003. 2003</date>
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
	<note>Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03)</note>
</biblStruct>

<biblStruct coords="8,370.98,448.69,169.02,8.07;8,323.16,458.65,175.72,8.07" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,506.54,448.69,33.46,8.07;8,323.16,458.65,112.39,8.07">WordNet. An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,308.22,476.59,231.78,8.07;8,323.16,486.55,216.83,8.07;8,323.16,496.51,216.84,8.07;8,323.16,506.47,69.60,8.07" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,323.16,486.55,216.83,8.07;8,323.16,496.51,212.04,8.07">From Discourse to Logic. An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<author>
			<persName coords=""><forename type="first">Reyle</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,308.22,524.41,231.78,8.07;8,323.16,534.37,216.83,8.07;8,323.16,544.33,216.83,8.07;8,323.16,554.30,216.83,8.07;8,323.16,564.26,216.83,8.07;8,323.16,574.22,216.83,8.07;8,323.16,584.18,67.64,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,503.56,544.33,36.44,8.07;8,323.16,554.30,197.90,8.07">The QED open-domain answer retrieval system for TREC 2003</title>
		<author>
			<persName coords=""><surname>Leidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,323.16,564.26,216.83,8.07;8,323.16,574.22,19.30,8.07">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="595" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,308.22,602.12,231.78,8.07;8,323.16,612.08,216.83,8.07;8,323.16,622.04,216.84,8.07;8,323.16,632.00,216.84,8.07;8,323.16,641.97,32.88,8.07;8,308.22,659.90,231.78,8.07;8,323.16,669.86,216.83,8.07;8,323.16,679.82,216.83,8.07;8,323.16,689.79,83.69,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,395.90,612.08,144.10,8.07;8,323.16,622.04,92.42,8.07;8,505.86,632.00,34.14,8.07;8,323.16,641.97,32.88,8.07;8,308.22,659.90,231.78,8.07;8,323.16,669.86,216.83,8.07;8,323.16,679.82,118.42,8.07">Washington, D.C. [Witten et al.1999] Ian A. Witten, Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">Ratnaparkhi</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,433.03,622.04,106.97,8.07;8,323.16,632.00,176.42,8.07">Proceedings of the Fifth Conference on Applied Natural Language Processing</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing<address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
	<note>A maximum entropy approach to identifying sentence boundaries. 2nd edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
