<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,94.50,58.71,421.83,20.11;1,261.75,78.96,88.45,20.11">Experiments with Web QA System and TREC2004 Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,132.00,128.81,90.02,13.28"><forename type="first">Dmitri</forename><surname>Roussinov</surname></persName>
							<email>dmitri.roussinov@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.50,128.81,140.21,13.28"><forename type="first">José</forename><forename type="middle">Antonio</forename><surname>Robles-Flores</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.50,183.56,45.19,13.28"><forename type="first">Yin</forename><surname>Ding</surname></persName>
							<email>yding@asu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Arizona State University Tempe</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Arizona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,94.50,58.71,421.83,20.11;1,261.75,78.96,88.45,20.11">Experiments with Web QA System and TREC2004 Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9CEC7BA7A0AADB628C5F7D9672D40DB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our first participation in TREC. We only competed in the Question Answering (QA) category and limited our runs to factoids. Our approach was to use our open domain QA system that finds the answer among Web pages indexed by a commercial search engine, then project the answer to the TREC test collection. Our Web QA takes advantage of the redundancy of the Web, obtains the candidate answers by pattern matching, then performs probabilistic triangulation of them to assign a final score. Our novel contributions are the following 1) the probabilistic triangulation algorithm, 2) a more powerful pattern language than used in prior research, and 3) use of semantic features of expected answers instead of relying on an elaborate hierarchy of question types. Although, we were able to run only first 91 out of 230 factoid questions before the submission deadline, we find our result encouraging, and if interpolated to the entire questions set, it would have placed us above the median performance on factoid questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Our participation in TREC was essentially a by-product of our involvement in a completely Web based open domain question answering project <ref type="bibr" coords="1,150.00,441.31,141.14,10.79" target="#b7">(Roussinov &amp; Robles, 2004</ref>). We maintain a working demo on the Web (http://129.219.59.31/qademo.html), which has averaged 15 requests a day since the inception in August 2004. Our QA system is entirely web based: it finds the answers among the web pages available through the general purpose search engine indexes (GPSE) such as Google, AltaVista, AOL, MSN etc., without even using the TREC collections. For the TREC QA competition, it then attempts to find the best supporting document within the TREC collection. We extended the approach by <ref type="bibr" coords="1,54.00,499.06,84.98,10.79" target="#b1">(Dumais et al., 2002)</ref> by automatically learning patterns similar to suggested by the other researchers <ref type="bibr" coords="1,462.00,499.06,95.66,10.79;1,54.00,510.31,22.21,10.79" target="#b5">(Ravichandran &amp; Hovy, 2002;</ref><ref type="bibr" coords="1,81.00,510.31,85.21,10.79" target="#b6">Zhang &amp; Lee, 2002)</ref>, and by using a more powerful pattern language. Another distinction from many systems presented earlier at TREC, is that we do not use an extensive hierarchy of question types organized by the expected semantic category of an answer (e.g. person, celebrity, organization, city, state, etc.), but rather we use a small set of independently considered semantic features, presence of which in candidate answers only affects their score rather than excludes entirely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHMS AND IMPLEMENTATIONS</head><p>Our overall Web based QA system is shown on the Figure <ref type="figure" coords="1,293.57,584.56,3.70,10.79">1</ref>. For pattern training and answer matching it uses an underlying general purpose search engine (GPSE). We currently use Google, AltaVista, and AOL. Processing TREC 2004 Format TREC 2004 test questions format was drastically different from the previous competitions and resembled a sequence of questions about an explicitly stated "target", rather than independently formulated questions from earlier competitions. Since we developed and tested our system using the data and input formats from the TREC conferences prior to 2004, we run into certain challenges while interpreting TREC 2004 input and lost 14% of the score simply due to that. Our question reading algorithm (tested on the only two examples provided by NIST before the competition) simply replaced all the pronouns with the targets, if the targets or their stem were not already explicitly mentioned in the question. However, this simple approach resulted in the number of mistakes listed in Table <ref type="table" coords="2,259.17,507.31,3.69,10.79" target="#tab_1">1</ref>. We believe all those mistakes could be avoided by simple heuristics if more examples were given in advance. We hope that the format will not be changed for the next year or number of examples comparable to the number of test questions will be provided by NIST. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mistake type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Language</head><p>Our pattern language allows any words and punctuation marks from the targeted language (English) plus the following special symbols:</p><p>\Q -the "question phrase" (e.g. for the question "Who is the CEO of IBM?" the \Q is "CEO of IBM"). For more complex types we use multiple question parts: \Q1, \Q2, etc.</p><p>* --a wildcard that matches any words, but not the punctuation marks;</p><p>\p --any punctuation mark \A -same matching as wildcard, but the matching text becomes a candidate answer;</p><p>\V -matches the verb from the question only (e.g. for the question "When was radio invented?" \V = invented)</p><p>Table <ref type="table" coords="3,82.51,198.31,4.88,10.79" target="#tab_2">2</ref> shows some examples of patterns. We believe our pattern language has more expressive power than rewrites introduced in <ref type="bibr" coords="3,109.52,210.31,82.84,10.79" target="#b1">(Dumais et al., 2002)</ref>. Having wildcards also allows matching sentences that otherwise would be missed due to the presence of redundant words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Training</head><p>The purpose of training is to assign to each pattern the probability that the matching text contains a correct answer. We used the questions and correct answers from the prior TREC competitions to train our patterns. For 2004 participation, we only trained the following types: what is/was/are, who is/was, when was, where is, verb-type (e.g. Who invented radio? Who manufactures rod hockey games?), when was born, where was born, when died, and when-was-verb type (e.g. When was the radio invented?). These were the most representative questions from past TRECs. The other types would require more examples to train than is currently available.</p><p>During training, for each pair (Question, Answer), the system requests the web pages from the General Purpose Search Engine (GPSE) that have both the question phrase \Q and the answer \A, preferably in proximity. We used Google for training and TREC 2004 tests. In the past, we used Alta Vista proximity operator NEAR to narrow down the set of pages , which was more efficient. Each sentence containing both the \Q and \A is converted into a candidate pattern by replacing the question phrase with \Q and the answer with \A. Once 200 candidate patterns are identified, each pattern is "generalized" to produce more patterns by the following :</p><p>1) replacing words (except \A, \Q, \V) with wildcards, 2) replacing punctuation with \p, 3) forming substrings containing the mentioned \Q , \V and \A.</p><p>Top 500 most frequent patterns after generalization are trained for the probability that the matching text includes a correct answer by modulating the pattern (as detailed below) and looking for the matches in the retrieved documents. After 40 matches from different pages have been identified, the probability is computed by the formula: prob(P) = # matches containing correct answers / # total matches.</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering Steps</head><p>Answering the question "Who is the CEO of IBM?" demonstrates the steps of our algorithm.</p><p>Type Identification. The question itself matches the pattern who is \Q ?, where \Q = "the CEO of IBM" is the question part. Thus, the patterns corresponding to WhatIs type are activated for matching.</p><p>Query modulation converts each answer pattern (e.g. "\A became \Q \p") into a query for a general-purpose search engine (GPSE), e.g. "became the CEO of IBM". The answer pattern "\Q is \A" would be converted into "the CEO of IBM is" etc.</p><p>Answer Matching. The sentence "Samuel Palmisano recently became the CEO of IBM." would result in a match and produce a candidate answer "Samuel Palmisano recently". Each candidate answer is assigned an initial score equal to the accuracy score of the matching pattern trained earlier by the formula (1).</p><p>Answer Detailing produces more candidate answers by forming sub-phrases from the initial candidate answers. Our sub phrases do not exceed 3 words (not counting the words from 251-word "stop word" list) and do not cross punctuation marks. In our example above, the detailed candidate answers would be Samuel, Palmisano, recently, Samuel Palmisano, Palmisano recently.</p><p>The Triangulation process establishes that Samuel Palmisano is the most likely answer as explained in the next section.</p><p>The algorithm stops querying GPSE when a specified number of web pages has been scanned (3000 for TREC 2004) or a specified number of candidate answers found (500 for TREC 2004). If there are fewer candidate answers found, the algorithm resorts to a "fall back" approach: it sends the question to GPSE and creates candidate answers from each sentence of the top returned 200 snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triangulation</head><p>Triangulation, a term widely used in intelligence and journalism, stands for confirming or disconfirming facts using multiple sources ( www.undp.org/eo/ADR/glossary.htm). Our triangulation algorithm can be demonstrated by the following intuitive example. Imagine that we have two candidate answers for the question "What was the purpose of Manhattan Project?" 1) "To develop a nuclear bomb" and 2) "To create a nuclear weapon." Those two answers reinforce (triangulate) each other since they are semantically similar. The advantage of triangulation over simple frequency counting <ref type="bibr" coords="4,498.27,440.56,59.11,10.79;4,54.00,451.81,22.92,10.79" target="#b1">(Dumais et al., 2002)</ref> is stronger for less "factual" questions, those that may allow variation in the correct answers. This includes such frequent types as definitions and "how to" questions.</p><p>The resulting score for each candidate answer s t (a) after triangulation is computed by the formula:</p><formula xml:id="formula_0" coords="4,56.25,496.39,143.57,25.01">∑ ∈ ⋅ = O a i i t i a a sim a s a s ) , ( ) ( ) (</formula><p>, where O is the set of all original (before detailing) answers, s(a) is the original score of candidate answer a, and sim(a1, a2) is the similarity metric between answers a1 and a2. Although we had tested a more finegrained similarity metrics earlier, for the TREC compeition we only used "simplified" tringulation, in which sim(a1, a2) = 1 or 0 depending on wether stemmed string a2 contains the stemmed string a1 or not accordingly. This is still different from frequency count <ref type="bibr" coords="4,127.50,558.31,91.52,10.79" target="#b1">(Dumais et al., 2002)</ref> because the original probablistic score of the candidate answer is taken i nto consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability and Responsiveness</head><p>Since TREC objective was to only test the accuracy, we were not that much concerned with the processing speed. Our Web demo finds an answer within a minute in average. The bottleneck is fetching the contents of the web pages, which can be parallelized on multiple workstations as , for example, has been successfully demonstrated in by <ref type="bibr" coords="4,462.75,661.06,91.39,10.79" target="#b4">Surdeanu et al. (2002)</ref>. Another possible solution would be to have direct access to the GPSE index and cache, which may be, for example possible when the QA system is an integral part of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Filtering</head><p>Contrary to many other groups involved in TREC QA, we do not use an extensive hierarchy of question types organized by the expected semantic category of an answer (e.g. person, celebrity, organization, city, state, etc.), but rather we use a small set of independently considered semantic features of candidate answers which only affect the score of a candidate answer rather than excluding it entirely. Specifically, we used the following boolean (yes/no) semantic features: is number, is date, is year, is location, is person name and is proper name. Depending on the question type we apply several heuristic rules to adjust the score of a candidate answer, for example: The features are checked by regular expressions or dictionary look-ups. We only use the lists of countries, and states to boost the matching answers for "where" questions. We had approximately a dozen of such heuristic rules. Although we set the adjustment weights manually for TREC 2004, we believe it is possible to automatically train them in future similar to training our patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Projection</head><p>We did not have sufficient time to experiment with our answer projection algorithm, which we believe can be significantly improved in future. For each question and its answer, our heuristic algorithm tries to locate the most lexically similar window of text centered on the answer within the TREC collection . It works the following way. First, it retrieves the documents that both 1. Contain the answer.</p><p>2. Contain the target (or only the longest substring of the target if that substring is present in the question instead of the target, e.g. "Nirvana" instead of "group Nirvana").</p><p>If there a re no documents retrieved, the a lgorithm moves to the next candidate answer. Once 20 answers have been unsuccessfully tried, NULL is returned as the answer. Otherwise, each retrieved document is scanned to produce the projection score by looking at 400-byte window around each occurrence of the answer. Every overlap between a word-stem from a question (excluding stopwords) and the word-stem within the window increases the projection score by the following amount: delta = log(N/df(t)) * (200 -distance(t)) / 200, where N is the number of documents on the web (estimated as 3,000,000,000 at the time of tests); df(t) is the number of documents on the web containing the word t (so called document frequency), obtained by querying AltaVista search engine; distance(t) is the byte distance between the occurrence of the word t and the occurrence of the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS AND CONCLUSIONS</head><p>Our official result on the factoid questions was 0.148. We did not attempt "list" and "other" questions. Since our projection algorithm was not optimized and slow as a result, we unexpectedly run out of time and processed only first 92 questions. The system updated the output file in the required TREC format after each question was processed. We stopped the system shortly before the due time and submitted our single run (ASUQA). Table <ref type="table" coords="6,364.64,129.31,4.88,10.79" target="#tab_3">3</ref> shows the breakdown of our errors. The most significant is getting a wrong answer from the web. The second big loss is misunderstanding format of this year TREC question as discussed above. Projecting to TREC is the third significant error component. We estimated the average performance of all the runs submitted to TREC on the first 92 questions was only 5% better than on the rest of the question, thus the first 92 questions did not happen to be significantly more or less difficult. This allowed us to estimate our "would be" performance on all the questions by a simple linear interpolation of the number of correct answers, which we obtained to be .21. We find this encouraging since it would have placed us above the median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>We plan to participate in future and improve our system by possibly: 1) Making our pattern language more flexible, possibly including dependency relations produced by a parser, e.g. Minipar.</p><p>2) Find or create more training examples.</p><p>3) Convert semantic filtering heuristics into trainable rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>) Use combined Web and TREC collections as answer sources.</p><p>5) Add heuristics to perform anaphoric resolution within the questions.</p><p>6) Extend our triangulation mechanisms to numbers.</p><p>We always welcome suggestions from other groups and possible collabotation!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,61.50,63.45,480.70,344.99"><head>Figure 1. The general Web QA approach</head><label></label><figDesc></figDesc><table coords="2,61.50,63.45,480.70,285.05"><row><cell></cell><cell cols="2">What is Calvados? What is Calvados?</cell><cell></cell><cell></cell></row><row><cell>What is … Who was… Where is… … What is … Who was… … Where is…</cell><cell cols="2">Natural Language Questions Natural Language Questions</cell><cell>Using the Using the</cell><cell>Find patterns Find patterns</cell><cell>PATTERN: \Q is \A Where: \Q = ("Calvados") PATTERN: \Q is \A Where: \Q = ("Calvados")</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q&amp;A System Q&amp;A System</cell><cell></cell></row><row><cell></cell><cell cols="2">Answers to original Answers to original</cell><cell></cell><cell></cell></row><row><cell></cell><cell>questions questions</cell><cell></cell><cell></cell><cell></cell><cell>QUERY: QUERY:</cell></row><row><cell></cell><cell cols="2">ANSWER: \Q is \A: ANSWER: \Q is \A:</cell><cell></cell><cell></cell><cell>"Calvados" NEAR "is" "Calvados" NEAR "is"</cell></row><row><cell></cell><cell cols="2">"calvados" is "a dry "calvados" is "a dry</cell><cell></cell><cell></cell></row><row><cell></cell><cell>apple brandy" apple brandy"</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real Questions Real Questions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rewrite Query Rewrite Query</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Training mechanism Training mechanism</cell><cell></cell></row><row><cell>\A is now: "a dry apple brandy" \A is now: "a dry apple brandy"</cell><cell></cell><cell>Triangulation: Triangulation: Finding most Triangulation: Triangulation: Finding most</cell><cell></cell><cell>Using Commercial Search Engines: Mining for answers Using Commercial Search Engines: Mining for answers</cell></row><row><cell></cell><cell></cell><cell>probable answers probable answers</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>...CALVADOS Often used in cooking chicken , pork and veal dishes , Calvados is a dry apple ...CALVADOS Often used in cooking chicken , pork and veal dishes , Calvados is a dry apple</cell><cell>… Multiple possible answers … sometimes conflicting … Multiple possible answers … sometimes conflicting</cell><cell></cell></row><row><cell></cell><cell></cell><cell>brandy made in brandy made in</cell><cell></cell><cell></cell></row><row><cell cols="2">Extracting the answer to the original question Extracting the answer to the original question</cell><cell>Calvados-located in the Normandy region of northern France .... Calvados-located in the Normandy region of northern France ....</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,54.00,548.56,478.48,169.04"><head>Table 1 . Mistakes in interpreting TREC 2004 format.</head><label>1</label><figDesc></figDesc><table coords="2,68.25,548.56,464.23,151.79"><row><cell></cell><cell>Examples</cell><cell>Number of occurrences (out of 91</cell></row><row><cell></cell><cell></cell><cell>questions processed)</cell></row><row><cell></cell><cell></cell><cell>9</cell></row><row><cell>Using both singular and plural</cell><cell>Crips and Crip Gang</cell><cell>1</cell></row><row><cell>forms for the target</cell><cell></cell><cell></cell></row><row><cell>A word from another question</cell><cell>Who plays the role? What year was the</cell><cell>2</cell></row><row><cell>referred via "the" determiner</cell><cell>movie released ?</cell><cell></cell></row><row><cell>"Implied-only" target</cell><cell>How many are there now ?</cell><cell>1</cell></row></table><note coords="2,68.25,577.81,318.01,10.79;2,225.75,589.06,161.08,10.79;2,225.75,601.06,125.83,10.79"><p>Target referred via "the" determiner What does the name mean or come from? Who is the sponsor of the court? Where is the company located?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,54.00,221.56,503.42,263.54"><head>Table 2 . Examples of patterns and the matching sentences for "what is" questions.</head><label>2</label><figDesc>E.g. "Samuel Palmisano recently became the CEO of IBM." matches the pattern \A * became \Q. This distinguishes our pattern language from those introduced earlier (Ravichandran &amp; Hovy, 2002; Zhang &amp; Lee, 2002).</figDesc><table coords="3,54.00,262.81,467.93,204.29"><row><cell>Question</cell><cell></cell><cell>Sentence</cell><cell></cell><cell></cell><cell>Pattern</cell><cell>Candidate Answer</cell></row><row><cell cols="2">What is the California's</cell><cell cols="3">California's state bird is</cell><cell>\s Q is \A \p</cell><cell>the valley quail</cell></row><row><cell>state bird?</cell><cell></cell><cell cols="2">the valley quail.</cell><cell></cell></row><row><cell cols="2">What is the capital of</cell><cell cols="3">Taipei, the capital of</cell><cell>\s \A \p Q \p * \p</cell><cell>Taipei</cell></row><row><cell>Taiwan?</cell><cell></cell><cell cols="3">Taiwan, is an exciting city.</cell></row><row><cell cols="2">That is the abbreviation for</cell><cell cols="3">"OEM" is the abbreviation</cell><cell>\s \A is Q \p</cell><cell>"OEM"</cell></row><row><cell>original</cell><cell>equipment</cell><cell cols="3">for original equipment</cell></row><row><cell>manufacturer?</cell><cell></cell><cell>manufacturer.</cell><cell></cell><cell></cell></row><row><cell>What is anise?</cell><cell></cell><cell cols="3">Aniseed, also known as</cell><cell>\s \A \p also known as Q \p</cell><cell>Aniseed</cell></row><row><cell></cell><cell></cell><cell cols="3">anise, contains several</cell><cell>* \p</cell></row><row><cell></cell><cell></cell><cell cols="2">estrogenic compounds.</cell><cell></cell></row><row><cell cols="3">What is anorexia nervosa? Eating</cell><cell cols="2">disorders</cell><cell>\s \A refers to Q, * \p</cell><cell>Eating</cell><cell>disorders</cell></row><row><cell></cell><cell></cell><cell>commonly</cell><cell>refers</cell><cell>to</cell><cell>commonly</cell></row><row><cell></cell><cell></cell><cell cols="3">anorexia nervosa, bulimia</cell></row><row><cell></cell><cell></cell><cell cols="3">and binge-eating disorder.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,54.00,169.68,270.17,110.68"><head>Table 3 . Breakdown of the errors.</head><label>3</label><figDesc></figDesc><table coords="6,59.25,169.68,264.92,99.39"><row><cell>#of</cell><cell></cell><cell></cell></row><row><cell cols="2">questions %</cell><cell></cell></row><row><cell>Wrong answer from Web QA</cell><cell>53</cell><cell>0.58</cell></row><row><cell>TREC format misunderstood</cell><cell>13</cell><cell>0.14</cell></row><row><cell>Unsupported (projection error)</cell><cell>5</cell><cell>0.05</cell></row><row><cell>Inexact</cell><cell>2</cell><cell>0.02</cell></row><row><cell>Correct (including NULL)</cell><cell>19</cell><cell>0.21</cell></row><row><cell>Total</cell><cell>92</cell><cell>100</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,57.52,512.56,499.61,10.79;6,72.00,524.56,345.65,10.79" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,273.11,512.56,284.02,10.79;6,72.00,524.56,41.13,10.79">Learning Search Engine Specific Query Transformations for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,120.00,524.56,218.15,10.79">Proceedings of the Tenth World Wide Web Conference</title>
		<meeting>the Tenth World Wide Web Conference<address><addrLine>Gaithesburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.52,539.56,498.64,10.79;6,72.00,551.56,92.08,10.79;6,164.25,549.74,5.63,7.47;6,174.00,551.56,382.51,10.79;6,72.00,562.81,115.40,10.79" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,306.95,539.56,217.83,10.79">Web Question Answering: Is More Always Better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,72.00,551.56,92.08,10.79;6,164.25,549.74,5.63,7.47;6,174.00,551.56,382.51,10.79;6,72.00,562.81,35.94,10.79">Proceedings of the 25 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.52,578.56,500.00,10.79;6,72.00,589.81,212.88,10.79" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,302.24,578.56,185.66,10.79">Probabilistic Question Answering on the Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grewal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,497.25,578.56,60.27,10.79;6,72.00,589.81,149.52,10.79">Proceedings of the 11th World Wide Web Conference</title>
		<meeting>the 11th World Wide Web Conference<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.52,605.56,500.18,10.79;6,72.00,616.81,483.24,10.79;6,72.00,628.06,224.17,10.79" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,440.44,605.56,117.26,10.79;6,72.00,616.81,128.97,10.79">Mining the Web for Answers to Natural Language Questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,223.08,616.81,154.23,10.79;6,410.30,616.81,144.94,10.79;6,72.00,628.06,165.48,10.79">Tenth International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
	<note>the Proceedings of the ACM CIKM</note>
</biblStruct>

<biblStruct coords="6,57.52,643.81,499.03,10.79;6,72.00,655.06,322.64,10.79" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,307.13,643.81,249.42,10.79;6,72.00,655.06,24.74,10.79">Performance Analysis of a Distributed Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,108.00,655.06,221.90,10.79">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="579" to="596" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,57.52,670.68,500.36,10.93;6,72.00,681.93,225.56,10.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,232.99,670.81,251.06,10.79">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,502.50,670.68,55.38,10.89;6,72.00,681.93,165.93,10.89">Proceedings of the 40th Annual Meeting of the ACL</title>
		<meeting>the 40th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,57.52,54.31,500.15,10.79;7,72.00,65.56,21.79,10.79" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,215.39,54.31,309.28,10.79">Web Based Pattern Mining and Matching Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,531.75,54.31,25.92,10.79">TREC</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,57.52,81.31,499.44,10.79;7,72.00,92.56,10.13,10.79;7,82.50,90.74,4.88,7.47;7,90.75,92.56,467.06,10.79;7,72.00,104.56,80.16,10.79" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,197.25,81.31,267.11,10.79">Learning Patterns to Answer Open Domain Questions on the Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,483.28,81.31,73.68,10.79;7,72.00,92.56,10.13,10.79;7,82.50,90.74,4.88,7.47;7,90.75,92.56,410.22,10.79">the proceedings of 27 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">July 25 -29, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,57.52,119.56,500.80,10.79;7,72.00,131.56,123.19,10.79" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,234.77,119.56,307.69,10.79">Use of patterns for detection of likely answer strings: A systematic approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,72.00,131.56,97.51,10.79">the Proceeding of TREC</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
