<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.48,90.58,466.90,16.65;1,147.48,111.34,317.00,16.65">Part-of-Speech Sense Matrix Model Experiments in the TREC 2004 Robust Track at ICL, PKU</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">CHINA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.48,90.58,466.90,16.65;1,147.48,111.34,317.00,16.65">Part-of-Speech Sense Matrix Model Experiments in the TREC 2004 Robust Track at ICL, PKU</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8FF24095E9AEAE9A6D5D2E52D1C28883</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Robust Retrieval track is a traditional ad hoc retrieval task with the focus on individual topic effectiveness. This track provides us an opportunity to do experiments on our recently proposed IR model using a word-by-sense matrix document representation, which was called Sense Matrix Model (SMM) <ref type="bibr" coords="1,115.36,309.96,50.82,9.88" target="#b10">[Swen 2003</ref><ref type="bibr" coords="1,174.14,309.96,23.57,9.88" target="#b11">[Swen , 2004]]</ref>. For the first time to extensively test the model, some simpler and easy-toimplement forms of SMM is used for this year's Robust track, where the part-of-speeches of words are treated as the (rough) senses of words. Though the model supports several matrix similarity measures and some advanced data analysis techniques, our initial implementation can only handle sense sets at the scale of a few hundreds of senses. Thus a relatively small part-of-speech tag set is employed and only two different matrix similarity measures used.</p><p>In this paper, we describe our model configuration and methods used in the TREC 2004 Robust track. Implementation issues and the submitted runs are also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SENSE MATRIX MODEL</head><p>The basic idea of the model is to explicitly introduce both words and senses in the document representation, namely, document D ==&gt; term set × sense set (with association term/sense weights)</p><p>A straightforward manner to make use of such combined information in an IR formalism is that we collect the words (feature terms) along with all the senses they actually (or usually) have in the document, and index the document by a term-sense network for retrieval. Such network relationship of words and senses may be further represented by a matrix of weights that represent the association of words and senses, resulting in a matrix representation of documents. A document collection is then represented as a term-sense-document space, in which every sense becomes a term-by-document matrix (and hence the name SMM). Such a matrix-based retrieval model may be regarded as a "sense expansion" of the vector-space model (VSM) <ref type="bibr" coords="1,148.43,640.38,52.86,9.88" target="#b4">[Salton and</ref><ref type="bibr" coords="1,206.52,640.38,106.64,9.88">Lest 1968, Salton 1971</ref>]: VSM's document vector of term weights is "expanded" or "split" (distributed) along the sense direction and thus becomes a matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Measure of Matrix Similarity</head><p>There are several possible methods to evaluate the similarity between document matrices. The first one we may think of is a matrix distance defined by an appropriate matrix norm:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( , ) d A B</head><p>A B = -. The Frobenius-norm ||•|| F and p-(power) norm ||•|| p are two of the commonly used. Secondly, the concept of "angle" between matrices may also be introduced in correspondence to vector angle. Using a "normalized distance", namely, distance between normalized matrices,</p><formula xml:id="formula_0" coords="2,152.22,132.84,123.95,19.41">norm ( , ) 2 A B d AB A B = - ≤ ,</formula><p>we may introduce a correct angle</p><formula xml:id="formula_1" coords="2,153.00,178.73,162.59,22.06">2 norm def norm 1 cos ( , ) 1 ( , ) 4 A B d A B ∠ = - .</formula><p>When the F-norm is used and the document matrices are vectors, this angle is proportional to the standard vector angle:</p><formula xml:id="formula_2" coords="2,153.00,232.98,157.67,19.80">( ) norm VSM 1 cos ( , ) 1 cos ( , ) 2 ∠ = + ∠ q d q d .</formula><p>Thirdly, also note that</p><formula xml:id="formula_3" coords="2,153.06,276.64,93.35,21.69">cos ( , ) AB A B A B ∠ = ⋅</formula><p>is the cosine of the "angle" between any two multipliable matrices A and B, based on the compatibility condition of matrix norms AB A B ≤ ⋅</p><p>. In the case of our document matrix, there are two different possible definitions of matrix angles:</p><formula xml:id="formula_4" coords="2,152.82,347.70,201.96,22.53">T 1 2 1 2 1 2 cos ( , ) D D D D D D ∠ = ⋅ , T 1 2 1 2 1 2 cos ( , ) D D D D D D ′ ∠ = ⋅</formula><p>, where D 1 D 2 T is the term-term correlation via senses, and D 1 T D 2 is the sense-sense correlation via terms. We tend to use the former since IR is traditionally about the retrieval of documents using terms to match related term sets.</p><p>Other possible similarity measures including matrix trace based angles, which also have two possibilities,</p><formula xml:id="formula_5" coords="2,152.88,453.18,215.81,23.01">T 1 2 1 2 1 2 tr cos ( , ) D D D D D D ∠ = ⋅ , T 1 2 1 2 1 2 tr cos ( , ) D D D D D D ∠ = ⋅ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part-of-Speeches as Senses</head><p>Some simpler and straightforward cases of SMM include POS SMM, where the sense dimensions are the part-of-speeches of index terms. It corresponds to splitting the VSM term weights into the weights of a term's part-of-speeches.</p><p>When the input text is POS tagged, there are 2 ways to determine the matrix elements. The simple one it to index each "Word/POS" pair as a VSM term, but record the matrix correspondence (otherwise it would result in a "POS VSM" with more restricted terms). The standard VSM term weightings are directly applicable to these tagged terms.</p><p>The other way is to split the VSM weights with POS distributions, with the document matrix form</p><formula xml:id="formula_6" coords="2,152.64,634.81,119.21,51.03">1,1 1 1, 1 2,1 2 2, 2 ,1 , m m N N N m N p w p w p w p w D p w p w ⎛ ⎞ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟ = ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎟ ⎜ ⎝ ⎠ ,</formula><p>where m is the number of part-of-speeches adopted and N is the term number in the collection. The p i, j parameter may be estimated to be the frequency of the jth POS of word i in document D. The advantage of this method is that for simple applications, the { p i, j } parameters may be set to the POS probability distribution of words in the collection be considered (instead of being computed for each document).</p><p>Since current POS tagging has succeeded considerably, the weighting of POS SMM may be expected to be effective.</p><p>It is easy to prove that POS SMMs with angle similarity measures based on normalized distances (in the F-norm) or sense-sense correlation matrix trace are equivalent to the POS VSM, which can be regarded as a "flattened SMM". Thus we may directly use the POS VSM to test these SMM instances. This greatly facilitates the design of our experimental system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM DESCRIPTION</head><p>We implement the features of SMM as extensions to the SMART-11.0 system <ref type="bibr" coords="3,429.53,272.76,97.98,9.88" target="#b5">(Salton and Lesk 1965</ref><ref type="bibr" coords="3,527.51,272.76,4.95,9.88;3,79.38,285.48,52.91,9.88" target="#b4">, Salton 1971</ref><ref type="bibr" coords="3,132.29,285.48,76.48,9.88">, SMART 1992)</ref>, which in design provides a flexible architecture for adding new IR features and conducting experiments. The space-time efficiency is not yet optimal for our cases. Due to some inherent constraints the experiments are limited to rather small sense sets, and this is the main reason for us to only test the POS SMM cases.</p><p>The system has a new preparser (added to SMART) that checks every input term. If it is a tagged word (a pair of "Word/Tag", with Tag being not limited to POS or any other tags), then the preparser adds the word, the tag and "word/tag" as well to the indexing dictionary, recording a matrix row number for the word and a column number for the tag, and a (row, column) pair for word/tag.</p><p>If the input is a "plain" word, then the preparser first searches a word/sense-list dictionary, with each text line being a form like where the WordNet sense number, the frequency and the document frequency of each sense of the word "bank" is listed (these t.f and d.f. valuses are obtained from a Brown corpus that comes up with the WordNet). If the word is in the sense dictionary, then for each sense a "word/sense" term is constructed for the later indexing process (some simple sparse data handling methods are introduced, with configurable parameters). If the input word is a new word, then a "word\word" term is used for the matrix element indexing (with '\' replacing '/' for such case). In either case, the (row, column) pair is recorded for each constructed index term.</p><p>Matrix computation is straightforward when the matrix elements are recorded this way, ensuring an efficient process of similarity evaluation. On the other hand, SMM with similarity of sense-sense correlation matrix norm or matrix trace is hard to be efficient in the retrieval process, since almost every document matrix shares a large common sense columns with others. Sequential search is implemented in the system, but is not applicable to the TREC data set.</p><p>The compromise made is that we use the short &lt;title&gt; field of a topic as keywords in the search of relevant documents via the inverted index. This reduces the number of documents for similarity computation and more importantly, reduces the complexity of retrieval to make it possible to process the large TREC data set, though relevant documents may be overlooked.</p><p>The POS SMM experiments used a C++ implemented Brill tagger to tag both the documents and the topics, which is a "transformation-based error-driven learning" POS tagger, with 48 part-of-speeches and a precision of 97.2% on the UPenn WSJ corpus. The tagging was quite efficient in terms of time (less than two day on a Linux PC workstation of a 700MHz CPU and 256MB memory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SUBMITTED RUNS</head><p>Using the above implementation of the model, we submitted nine runs for this year's TREC Robust track, described briefly as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submitted Runs icl04pos2t</head><p>Indexing only noun and verb words, using the title field, with normalized matrix distance icl04pos2d</p><p>Indexing only noun and verb words, using the description field, with normalized matrix distance icl04pos2td</p><p>Indexing only noun and verb words, using the title and description fields, with normalized matrix distance icl04pos2f</p><p>Indexing only noun and verb words, using the title, description and narrative fields, with normalized matrix distance icl04pos7td</p><p>Indexing the words of 7 merged POS tags from the 48 original tags, using the title and description fields, with normalized matrix distance icl04pos7f</p><p>Indexing the words of 7 merged POS tags from the 48 original tags, using the title, description and narrative fields, with normalized matrix distance icl04pos7tap</p><p>Indexing the words of 7 merged POS tags from the 48 original tags, using the title field, with sense-sense correlation product matrix norm similarity icl04pos7tat Indexing the words of 7 merged POS tags from the 48 original tags, using the title field, with sense-sense correlation matrix trace similarity icl04pos48f Indexing the words of the 48 original POS tags, using the title, description and narrative fields, with normalized matrix distance</p><p>The submitted runs used the default stopword removal, the 'triestem' stemming (after the POS tagging), and the lnc-ltc weighting. (Other options were also experimented with partial results recorded.)</p><p>The summary measures over 200 old topics, 49 new topics, 50 difficult topics and all topics are listed as follows. Other different weighting schemes were also experimented. An interesting phenomenon found is that for the nnn-nnn weighting, precision increases with the size of POS tags, while for the atc-atc and lncltc weighting, precision goes the other way. On the other hand, stemming seemed to have reduced (or stabilized) the effects of tagging. The performance of the experimented runs came quite close and was of relatively low values. This may be regarded to indicate that smaller tag sets, though may lead to effective tagging, would contribute less to the model. In the future, we plan to have more investigation on this aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TOPIC DIFFICULTY PREDICTION</head><p>For the task of predicting topic difficulty, a topic difficulty model based on word sense ambiguity is proposed. The sense ambiguity of a keyword might be related to the difficulty of document search. Hence the total (or average) ambiguity of the words in a topic may be considered a rough measure of the topic difficulty. Such a measure should be further modified by word collocation, word similarity, word distribution (d.f.) in the collection, and the position (role) of the word in the topic.</p><p>The model is formulated as follows: Where certainty(w) stands for the certainty (being unambiguous) of word w, collocation(w) is the collocation strength of w, similar(w) relates to effects of similar (replaceable) words, doc(w) is the document distribution of w, and weight(position(w)) is the effect of w at a position/role of title, description or positive, negative.</p><p>In the experiments, a simplified version of the model is used. It uses a sense distribution dictionary constructed from WordNet and a sense-tagged Brown corpus to estimate the difficulty of a single word. Then the topic difficulty is measured by the average easiness of words in the topic. Some other variants were also attempted but not included in the submitted runs. The Kendall correlation between predicted and actual difficulty is as follows. Due to time limitation, the word difficulty measure and the topic difficulty prediction had not been used in the retrieval process. We think this is an interesting issue and worth extensive investigation in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In our first TREC experiments, we used a relatively small tag set, namely the part-of-speech tags of the Brill tagger, to evaluate the sense-matrix model. The effectiveness of the POS SMM seems to be less obvious or marginal. We think that this may indicate small, highly merged sense sets such as partof-speeches have insignificant contribution to the model, or other new matrix similarity measures effective for small tag sets need to be investigated. Another practical issue led us to use smaller tag sets is that our current SMART-11.0 based system has inherent constraints on the tag-set size. The inverted index must be within the file size of 2GB (addressable with a signed 32-bit integer), which prevents us from using a more reasonable sense set, such as one that is designed to be tailored specifically according to the TREC data set from the WordNet sense set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,115.38,430.17,17.51,8.10;3,151.37,430.17,358.53,8.10;3,151.38,446.73,358.66,8.10;3,151.38,463.29,282.77,8.10"><head></head><label></label><figDesc>bank 106227059/20/9;106800223/14/6;106739355/2/2;201093881/1/1;106250735/1/1;201599940/0/0; \ 201599852/0/0; 201579642/0/1;201393302/0/0; 200841124/0/0;200464775/0/2;109626760/0/0; \ 109616845/0/0;106800468/0/0;103277560/0/0;102247680/0/0;100109955/0/0</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We have continuing research work on these issues, and plan to participate more TREC evaluations using SMM with better sense sets as well as better similarity measures.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,83.88,608.54,92.18,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.60,631.59,417.99,8.10;6,93.60,642.15,143.08,8.10" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Véronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,93.60,642.15,94.68,8.10">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>Introduction to the Special Issue on Word Sense Disambiguation: The Start of the Art</note>
</biblStruct>

<biblStruct coords="6,93.60,652.71,423.68,8.10;6,93.60,663.27,146.00,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,222.90,652.71,163.10,8.10">Lexical Ambiguity and Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,391.81,652.71,125.47,8.10;6,93.60,663.27,63.71,8.10">ACM Transactions on Information Retrieval Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="141" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.60,673.83,421.98,8.10;6,93.60,684.39,60.22,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,154.35,673.83,137.78,8.10">Wordnet: an On-line Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,298.34,673.83,199.05,8.10">In Special Issue: International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="312" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.60,694.95,418.99,8.10;6,93.60,705.51,80.67,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,154.82,694.95,286.05,8.10">The SMART retrieval system -Experiments in automatic document processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice Hall Inc</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.60,74.25,435.30,8.10;7,93.60,84.81,131.84,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,214.86,74.25,240.73,8.10">The SMART automatic document retrieval system -an illustration</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,461.06,74.25,67.84,8.10;7,93.60,84.81,30.38,8.10">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="398" />
			<date type="published" when="1965-06">1965. June 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.60,95.37,431.06,8.10;7,93.60,105.93,79.38,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,214.86,95.37,189.98,8.10">Computer evaluation of indexing and text processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,410.84,95.37,84.50,8.10">In Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="1968-01">1968. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.60,116.49,289.49,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,191.90,116.49,139.27,8.10">Available via anonymous ftp from ftp</title>
		<author>
			<orgName type="collaboration" coords="7,93.60,116.49,69.68,8.10">SMART version 11</orgName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>cs.cornell.edu</note>
</biblStruct>

<biblStruct coords="7,93.60,127.05,408.49,8.10;7,93.60,137.61,290.40,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,176.17,127.05,228.55,8.10">Word Sense Disambiguation in Information Retrieval Revisited</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stokoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,420.43,127.05,81.66,8.10;7,93.60,137.61,286.42,8.10">The 26th ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;03)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.60,148.17,397.19,8.10;7,93.60,158.73,408.18,8.10;7,93.60,169.29,62.86,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,158.27,148.17,318.51,8.10">Word Sense Disambiguation for Free-Text Indexing Using a Massive Semantic Network</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sussna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,93.60,158.73,369.73,8.10">Proceedings of the 2nd International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 2nd International Conference on Information and Knowledge Management (CIKM)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.60,179.85,426.44,8.10;7,93.60,190.41,238.87,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,204.39,179.85,195.48,8.10">Relative Information and a Sense Matrix Model for IR</title>
		<author>
			<persName coords=""><forename type="first">Bing</forename><forename type="middle">(</forename><surname>Swen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">)</forename><surname>Sun Bin</surname></persName>
		</author>
		<idno>TR-003</idno>
		<ptr target="http://icl.pku.edu.cn/icl_tr/" />
		<imprint>
			<date type="published" when="2003-11">2003. Nov 2003</date>
		</imprint>
		<respStmt>
			<orgName>ICL, Peking Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="7,93.60,200.97,427.16,8.10;7,93.60,211.53,428.91,8.10;7,93.60,222.09,20.28,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,204.39,200.97,186.98,8.10">Sense Matrix Model and Discrete Cosine Transform</title>
		<author>
			<persName coords=""><forename type="first">Bing</forename><forename type="middle">(</forename><surname>Swen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sun Bin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,407.50,200.97,113.26,8.10;7,93.60,211.53,160.15,8.10;7,365.77,211.53,90.49,8.10">Proceedings of AIRS 2004 (the first Asia Information Retrieval Symposium)</title>
		<meeting>AIRS 2004 (the first Asia Information Retrieval Symposium)<address><addrLine>Beijing, CHINA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004-10-18">2004. Oct 18-20. 2004</date>
		</imprint>
	</monogr>
	<note>LNCS AIRS Proceedings</note>
</biblStruct>

<biblStruct coords="7,93.60,232.65,416.04,8.10;7,93.60,243.21,234.49,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,177.87,232.65,230.61,8.10">Using WordNet to Disambiguate Word Sense for Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,424.11,232.65,85.54,8.10;7,93.60,243.21,136.77,8.10">Proceedings of the 16th International ACM SIGIR Conference</title>
		<meeting>the 16th International ACM SIGIR Conference<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
