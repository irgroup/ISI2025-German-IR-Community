<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.68,88.87,369.89,13.50;1,177.60,120.07,235.40,13.50;1,412.98,116.82,4.74,8.53">Improved Feature selection and redundancy computing --THUIR at TREC 2004 Novelty track *</title>
				<funder ref="#_zZZbZ6B">
					<orgName type="full">Chinese National Key Foundation Research &amp; Development Plan (973)</orgName>
				</funder>
				<funder ref="#_cVDZJS8 #_m6zX5BM #_vdcDUq6">
					<orgName type="full">Chinese Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.52,150.75,37.58,9.45"><forename type="first">Liyun</forename><surname>Ru</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,245.70,150.75,33.02,9.45"><forename type="first">Le</forename><surname>Zhao</surname></persName>
							<email>zhaole@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.20,150.75,44.74,9.45"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.48,150.75,56.31,9.45"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CST Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.68,88.87,369.89,13.50;1,177.60,120.07,235.40,13.50;1,412.98,116.82,4.74,8.53">Improved Feature selection and redundancy computing --THUIR at TREC 2004 Novelty track *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D4BE27C58FE0C299EF064EE09C0E29B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the third years that Tsinghua University Information Retrieval Group (THUIR) participates in Novelty task of TREC. Our research on this year's novelty track mainly focused on four aspects:</p><p>(1) text feature selection and reduction; (2) improved sentence classification in finding relevant information; (3)efficient sentence redundancy computing; (4) effective result filtering. All experiments have been performed on TMiner IR system, developed by THU IR group last year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Text feature selection and reduction</head><p>The work of text feature selection and reduction is used as the basis of both relevant and new steps. In novelty/passage retrieval, data sparseness problem is dominant. In a sentence, different terms act as different roles. What terms take most important information for a given user query? What kinds of feature are most useful to identify the core information? This is what we try to find out in our research on text feature selection and reduction. Three kinds of approaches have been carried out:</p><p>(1)Using Named Entity as significant features;</p><p>(2)Using POS-tagging information for feature selection;</p><p>(3)Using PCA transform for feature reduction.</p><p>The former two approaches take NLP information into account. In our experiments, both NEand PCA-based approaches improve system performances, while POS-tagging information does no help. In the following, we'll give some more detailed descriptions of NE-based feature selection and PCA-based feature reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Using Named Entity as significant features</head><p>The basic of NE-based approach is to recognize which phrase is Named Entity and what type of Named Entity it is. In our experiments, 34 types of Named Entities have been recognized 1 , such as organization, person, location, date, country, occupation, animal, area, etc.</p><p>After the annotation of Named Entity, they are used in the following four ways: The experiments show that all NE based approaches we proposed will improve system performances and NE1 achieves best results.</p><formula xml:id="formula_0" coords="1,90.00,603.63,393.09,9.45">Approaches NE1 NE2 NE3 NE4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Using PCA transform for feature reduction</head><p>PCA is a statistical tool for data analysis. It decorrelates second order moments corresponding to low frequency property, and identifies directions of principal variations in the data. There are two advantages brought about by PCA, i.e. dimension reduction and noise reduction.</p><p>PCA is used in task2, task3 and task4 to solve data sparseness problem and reduce noise by finding the most dominant features. In task3 relevant step, query and all corresponding sentences of each topic are used to perform PCA transformation, and then the mean PCA-feature of query and relevant sentences of first 5 documents is taken as new feature of query.</p><p>Suppose PCA-feature of query is Q = [q 1 , …, q m ], PCA-feature of each relevant sentence of first 5 documents is S k = [S k1 , …, S km ], k =1, … , K, where K is the number of relevant sentences of first 5 documents. Then the new PCA-feature of query can be represented as the mean of query and all given relevant sentences:</p><formula xml:id="formula_1" coords="2,205.62,232.12,203.14,24.56">Q '= [q' 1 , …, q' m ], where ) ( 1 1 ' 1 ∑ = + + = K k ki i i s q K q</formula><p>Then cosine similarities between all sentences and new query can be calculated and ranked. For new step of task2, task3 and task4, only relevant sentences are used to PCA transformation. And overlap-based redundancy measurement is performed.</p><p>The experimental results show that this PCA-based feature subspace approach does helpful on finding relevance and sentence redundancy computing, which can be seen in Table <ref type="table" coords="2,453.03,325.95,3.92,9.45">1</ref>.</p><p>Table <ref type="table" coords="2,193.98,341.67,38.72,9.45">1 Effects</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Improved sentence classification</head><p>In TREC2003, we proposed a SVM-based sentence classification approach, which helped finding relevant information in task3 <ref type="bibr" coords="2,247.23,537.69,11.19,9.45" target="#b0">[1]</ref>. In this year, some improvements have been made.</p><p>Positive and negative examples were first selected from relevant sentences provided in first 5 documents. However, results may be not encouraging, if few positive examples are given. To solve this problem, top sentences returned by initial retrieval were added into positive examples set. Weights of positive and negative examples have been balanced, using inverse ratio of the numbers of the two kinds of examples. Then a SVM classifier is learned to find relevant information. Better results have been got in this way. We used a SVM package (version 2.4, by Chih-Wei Hsu, etc. <ref type="bibr" coords="2,172.90,646.89,11.83,9.45" target="#b1">[2]</ref>) to create the classifier.</p><p>It shows that this improved sentence classification does helpful on finding relevance, which can be seen in the following table 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Efficient sentence redundancy computing</head><p>On sentence redundancy computing, both sentence-sentence overlapping and pool-sentence overlapping have been studied in TREC2002 and TREC2003. In this year, we proposed an improved overlap strategy based on selected pool with tightness factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Redundancy elimination based on selected pool</head><p>A selected pool is not a pool that contains all the previously appearing sentences. The previous sentences can be selected to the pool only if they have an overlap of more than some certain threshold to the current sentence. Once the pool is constructed, a comparison of overlap between the current sentence and the pool will be done.</p><p>This selected pool method is the combination of sentence-sentence comparison and pool-sentence overlapping approaches. It overcomes certain drawbacks of the sentence-sentence overlap comparison which certainly excludes the case in which multiple sentences appearing before the current sentence together made the current sentence redundant; and drawbacks of the simple pool-sentence comparison, in which the pool constructed using all the previously appearing sentences will certainly contain too much noise introduced by the sentences not related to the current sentence. The effects of using selected pool are shown in Figure <ref type="figure" coords="3,393.89,331.95,3.94,9.45" target="#fig_1">1</ref>. overlap, X: sentence-sentence overlap approach, X is the redundancy threshold. selpool XsY: selected pool approach, X is the redundant threshold, and Y is the selection threshold for a sentence to be added in the pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A drawback of the overlap based novelty judgment and a remedy -tightness restriction of overlap based methods.</head><p>In overlap-based methods, such as sentence-sentence overlap novelty detection and selected pool overlap, only the number of overlapping terms is taken into account, not concerning term position information. In experiments, however, many sentences with an overlap of nearly 1 are real novel ones. Why? Because a previous sentence that has many terms overlapped a latter short one, with the overlapping terms scattering separately in the previous sentence. When this occasion occurs, the latter sentence (the overlapped one) is usually not redundant as it appears.</p><p>Therefore we consider the window of overlapped terms in the previous sentence (referred as w 0 ) and that in the latter one (referred as w i ). If w 0 &gt; T*w i , the previous sentence is not potent. T is called tightness factor. For selected pool method, tightness can also be used in the process of adding sentences to the pool.</p><p>We call this method a tightness restriction on overlapping (referred as tightness in the following). This tightness method is proved helpful in both novelty 2003 and novelty 2004 top 5 documents . The recall is improved with a comparatively small decrease in precision, and a steady increase in F-measure is obtained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Document and sentence similarities fusion</head><p>The main difference between this year's and last year's novelty task is that each topic of this year will include zero or more irrelevant documents in addition to 25 relevant documents. This year's task more likely happens in real world information retrieval applications. Therefore the role of document and sentence similarities is one of the interesting points in our study.</p><p>Three observations of document and sentence similarities have been carried out.</p><p>(1) Sentence-based filtering: similarities of original documents are ignored. Each sentence is taken as individual document, and those with small similarity scores or at last n percent are filtered out.</p><p>(2) Document-based filtering: Sentences are taken as parts of one document. After initial retrieval on sentence level, top sentences are used to generate a relevant documents list, and then sentences not belonging to relevant documents are discarded.</p><p>(3) Fusion of sentence and document similarity: Searching using sentences and documents respectively, then getting a final score for each sentence by fusing sentence similarity and the corresponding document similarity, and performing result filtering.</p><p>In our experiments, sentence filtering and the fusion of sentence and document are helpful, while document-based filtering does not, which can be seen in the following table 4. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,105.78,347.55,382.44,9.45;3,105.90,504.70,13.58,8.61;3,101.28,478.84,18.20,8.61;3,105.90,452.98,13.58,8.61;3,101.28,427.78,18.20,8.61;3,105.90,401.98,13.58,8.61;3,101.28,376.12,18.20,8.61;3,128.46,516.64,36.74,8.61;3,140.40,527.20,13.52,8.61;3,168.90,516.64,36.74,8.61;3,180.84,527.20,13.52,8.61;3,212.64,516.64,32.12,8.61;3,221.94,527.20,13.52,8.61;3,253.08,516.64,32.12,8.61;3,262.38,527.20,13.52,8.61;3,294.18,516.64,32.12,8.61;3,294.18,527.20,32.12,8.61;3,335.28,516.64,32.12,8.61;3,335.28,527.20,32.12,8.61;3,375.72,516.64,32.12,8.61;3,375.72,527.20,32.12,8.61;3,416.82,516.64,32.12,8.61;3,416.82,527.20,32.12,8.61;3,457.26,516.64,32.12,8.61;3,457.26,527.20,32.12,8.61;3,390.30,413.86,32.12,8.61;3,390.30,425.80,22.82,8.61"><head>Figure 1 .</head><label>1</label><figDesc>Effects of using overlap-based and selected-pool-based redundancy measurement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,114.42,693.69,362.53,73.95"><head>Table 2</head><label>2</label><figDesc>Effects of improved sentence classification using SVM</figDesc><table coords="2,114.42,709.77,362.53,57.87"><row><cell>Approach description</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>tag</cell></row><row><cell>Long query (Baseline)</cell><cell>0.31</cell><cell>0.82</cell><cell cols="2">0.419 THUIRnv0411</cell></row><row><cell>SVM : radial basic function</cell><cell>0.24</cell><cell>0.59</cell><cell cols="2">0.289 un-official run</cell></row><row><cell>Improved SVM: radial basic function</cell><cell>0.40</cell><cell>0.64</cell><cell cols="2">0.438 THUIRnv0434</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,90.00,154.35,408.60,186.51"><head>Table 3</head><label>3</label><figDesc>Effects of using tightness restriction of overlap-based method</figDesc><table coords="4,90.00,170.55,408.60,170.31"><row><cell>Novelty 2004 task2</cell><cell>#ret</cell><cell>Ave P</cell><cell>Ave R</cell><cell cols="2">Ave P*R Ave F</cell><cell>Difference:</cell></row><row><cell>overlap 0.7</cell><cell>6965</cell><cell>0.462</cell><cell>0.950</cell><cell>0.439</cell><cell>0.608</cell><cell>68 in 253</cell></row><row><cell>overlap 0.7 tightness</cell><cell>7218</cell><cell>0.456</cell><cell>0.965</cell><cell>0.441</cell><cell>0.606</cell><cell>are novel 2</cell></row><row><cell>2004 task4, in top5 docs</cell><cell>#ret</cell><cell>Ave P</cell><cell>Ave R</cell><cell cols="2">Ave P*R Ave F</cell><cell></cell></row><row><cell>overlap 0.7</cell><cell>974</cell><cell>0.694</cell><cell>0.964</cell><cell>0.668</cell><cell>0.786</cell><cell>12 in 24 are</cell></row><row><cell>overlap 0.7 tightness</cell><cell>998</cell><cell>0.686</cell><cell>0.981</cell><cell>0.675</cell><cell>0.789</cell><cell>novel</cell></row><row><cell>selpool 0.3s5.0</cell><cell>965</cell><cell>0.694</cell><cell>0.959</cell><cell>0.665</cell><cell>0.784</cell><cell>13 in 25 are</cell></row><row><cell>selpool 0.3s5.0 tightness</cell><cell>990</cell><cell>0.687</cell><cell>0.977</cell><cell>0.672</cell><cell>0.788</cell><cell>novel</cell></row><row><cell>Novelty 2003 task2</cell><cell>#ret</cell><cell>Ave P</cell><cell>Ave R</cell><cell cols="2">Ave P*R Ave F</cell><cell></cell></row><row><cell>overlap 0.7</cell><cell cols="2">13303 0.719</cell><cell>0.972</cell><cell>0.698</cell><cell>0.815</cell><cell>108 in 218</cell></row><row><cell>overlap 0.7 tightness</cell><cell cols="2">13521 0.716</cell><cell>0.979</cell><cell>0.700</cell><cell>0.816</cell><cell>are novel</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,114.42,640.50,356.82,88.84"><head>Table 4</head><label>4</label><figDesc>Effects of result filtering (novelty 2004 task1)</figDesc><table coords="4,114.42,656.85,356.82,72.48"><row><cell>Approach description</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>tag</cell></row><row><cell>NE + Long query +LCE (MI, win=10) (Baseline)</cell><cell>0.26</cell><cell>0.95</cell><cell cols="2">0.381 un-official run</cell></row><row><cell>Sentence Filtering</cell><cell>0.31</cell><cell>0.81</cell><cell cols="2">0.409 THUIRnv0412</cell></row><row><cell>Fusion of sentence and document filtering</cell><cell>0.29</cell><cell>0.84</cell><cell cols="2">0.404 THUIRnv0413</cell></row><row><cell>Document filtering</cell><cell>0.27</cell><cell>0.82</cell><cell cols="2">0.381 THUIRnv0415</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,97.50,751.23,390.31,8.10;4,90.00,761.55,375.61,8.10"><p>Tightness restriction will return more sentences than the overlap method it is based on. This difference is the number of novel sentences in all the sentences tightness method returned more than the basis of overlap.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* Supported by by <rs type="funder">Chinese National Key Foundation Research &amp; Development Plan (973)</rs> (Grant No.<rs type="grantNumber">2004CB318108</rs>) and <rs type="funder">Chinese Natural Science Foundation</rs> (Grants No. <rs type="grantNumber">60223004</rs>, <rs type="grantNumber">60321002</rs>, <rs type="grantNumber">60303005</rs>). 1 The NE recognition toolkit is provided by <rs type="institution">IBM China Research Center</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zZZbZ6B">
					<idno type="grant-number">2004CB318108</idno>
				</org>
				<org type="funding" xml:id="_cVDZJS8">
					<idno type="grant-number">60223004</idno>
				</org>
				<org type="funding" xml:id="_m6zX5BM">
					<idno type="grant-number">60321002</idno>
				</org>
				<org type="funding" xml:id="_vdcDUq6">
					<idno type="grant-number">60303005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submitted official runs</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,102.72,740.91,388.93,8.10" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>At</surname></persName>
		</author>
		<title level="m" coord="5,196.13,740.91,247.55,8.10">TREC 2003: Novelty, Robust and Web, in proceedings of TREC2003</title>
		<imprint>
			<biblScope unit="volume">556</biblScope>
			<biblScope unit="page">567</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,102.72,756.09,339.53,9.45" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chih-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html" />
		<title level="m" coord="5,188.27,756.09,57.71,9.45">SVM package</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
