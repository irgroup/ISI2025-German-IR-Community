<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.16,109.47,344.83,16.26">NLPR at TREC 2004: Robust Experiments</title>
				<funder ref="#_gukpQxQ">
					<orgName type="full">Natural Sciences Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Scientific Research Foundation for Returned Overseas Chinese Scholars, State Education Ministry</orgName>
				</funder>
				<funder ref="#_6ZpRaZq">
					<orgName type="full">Natural Science Foundation of Beijing</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.76,128.30,32.24,10.84"><forename type="first">Jin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,272.29,128.30,48.82,10.84"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.63,128.30,33.80,10.84"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.16,109.47,344.83,16.26">NLPR at TREC 2004: Robust Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AE02CD3A018E7623544C76C37A659B77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview</head><p>It is the second time that the Chinese Information Processing group of NLPR participates in TREC. In the past, we have investigated the use of two key technologies: Window-based weighting method and Semantic Tree Model for query expansion, with success, to tasks in novelty and robust tracks. We focused on the Robust Retrieval Track at this year's conference. Based on the previous IR architecture, our research on this year's robust mainly focused on three aspects:</p><p>(1) two-step retrieval scheme; (2) word sense entropy; (3) several strategies for merging multiple runs.</p><p>Our paper is organized as follows. Section 2 shows the basic architecture of our IR system and the new techniques for improving its performance. Section 2.1 presents the two-step retrieval scheme which mainly attempts to reduce the influence of noise introduced by query expansion. Section 2.2 introduces a new method for query word weighting-word sense entropy which is a measure for the variety of the sense of query word based on WordNet's structured knowledge. Section 2.3 describes several different strategies which we have used for merging the results of multiple runs produced by different retrieval approaches. Section 3 gives the experimental verification of the techniques mentioned in section 2. Section 4 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">New Techniques</head><p>Our IR system is both for English Retrieval and Chinese Retrieval. The basic architecture of the IR system and the fundamental retrieval models have been shown in the [Qianli <ref type="bibr" coords="1,200.50,553.46,47.14,10.84" target="#b1">Jin 2003]</ref>. In this year, we have experimented three new technologies for robust track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Two-step Retrieval Scheme</head><p>As we know, query expansion methods, such as expansion based on pseudo feedback or based on semantic knowledge, usually introduce many query-irrelevant words which are called noise. It would hurt the system performance very much. Noise is one of kernel problems for the application of query expansion. In the following, we presented a two-step retrieval scheme, mainly attempting to reduce the influence of noise.</p><p>There are two characteristics for the TREC's Robust retrieval. (1) A topic of TREC style has three fields: title, description and narrative. It can be found that, "title" field always contains core query words which are mostly nouns and often have discriminative function for this topic's retrieval process, while the "description" field and the "narrative" field are similar to the expansion of the "title" field, because they include more detailed information about this topic. (2) Robust does not require sorting all relevant documents of text corpus, instead, requires the most relevant 1000 documents returned.</p><p>According to these characteristics of Robust track, we adopt a two-step retrieval scheme. The Key Notion is as follow:</p><p>Step 1 is a Boolean retrieval process to get a relevant document pool with "core query words" as input query. With the relevant document pool as the retrieval corpus, Step 2 is a refining retrieval process with "core query words with expansion" as input query.</p><p>In the scheme, the retrieval processing is divided into the following two steps as Figure <ref type="figure" coords="2,170.57,322.64,4.86,10.84" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Two-step Retrieval Scheme</head><p>Step 1, Rough Retrieval: A Boolean retrieval process is conducted based on the query words of the "title" field. This step is called as "rough retrieval". The returned relevant documents, which are called as "relevant document pool", will be the retrieval corpus of the second step.</p><p>Step 2. Refining Retrieval: In this step, we use the "relevant document pool" output from Step 1 as the retrieval corpus, and implement a series of methods to improve the ranking performance. This step is called as "refining retrieval". Specially, for this year's track, we try to use all fields of topic include "title", "description" and "narrative" as retrieval input in this step, and furthermore, we will The two-step retrieval scheme can reduce the influence of noise, because we get the probably relevant documents in step 1, which is the retrieval corpus for step 2.</p><p>Then although there are a lot of noises for retrieval in step 2, it will not cause new irrelevant documents.</p><p>Another advantage is that retrieval cost is reduced, because the amount of documents for refining retrieval is much fewer than that of the whole text corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Word Sense Entropy</head><p>Since selecting the most appropriate sense for an ambiguous word in a sentence is deemed to be of great benefit to Natural Language Processing, many researchers have tried applying Word Sense Disambiguation to information retrieval tasks. However, there are some problems for introducing WSD into IR. First of all, the precisions of the up-to-date WSD technologies are still not high enough. Furthermore, WSD-based IR will introduce extra system cost. Therefore, we attempted an alternative method to implement WSD idea.</p><p>As we know, an important part for various retrieval models is how to estimate the weight of query words, namely how to describe the importance for retrieval of different query words. Then we tried to introduce WSD idea into retrieval model by altering weight of query words. The common measure methods including TF, IDF, BM25 etc, are proved to be efficient in previous practical experiments. However, these methods are all empirical, and the weights of words are not independent on different corpus. The weight for one word might be different for different retrieval tasks, And in many instances, the weight of some word is not reasonable due to the incompletion of the corpus, which is also the common problem for Statistical NLP methods. For example, it is reasonable that the two words: 'polio' and 'bank', should have different weights because 'polio' is more distinctive for IR scoring than 'bank'. But if the two words have both occasionally appeared only once in some corpus, then the IDF weights of the two words will be the same, 1.</p><p>Based on the above analysis, in the paper we proposed a new measure to weight the importance of query items-word sense entropy, which measures the variety of query word senses based on Wordnet's structured knowledge. In the actual retrieval model, this weight is combined with other weight such as TF, IDF to weight the importance of query words.</p><p>As we know, Wordnet is a lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory [Wordnet 2.0]. Wordnet has provided detailed word sense information about English words. Some words have only 1 sense, and the others have several senses. Figure <ref type="figure" coords="3,414.86,688.64,6.49,10.84" target="#fig_0">1</ref> is two examples in Wordnet.</p><p>"polio" has 1 sense:  <ref type="formula" coords="4,135.98,370.88,3.39,7.63">1</ref>) bank, cant, camber --(a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force)10. bank --(a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning); "the plane went into a steep bank")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: An Example from WordNet</head><p>It sounds reasonable that one word with fewer senses should be more important for retrieval than one word with more senses. According to this idea, we introduce word sense entropy to describe the word sense variety of one word. The Key notion is as follow:</p><p>One word with fewer senses should be more important for retrieval and have higher weight than one word with more senses.</p><p>This weight could be used to the formula for scoring the rank of documents. </p><formula xml:id="formula_0" coords="4,110.10,564.97,298.95,55.91">( ) ( )log( ( )) n i i i H W p sense W p sense W = = ∑ ……………………. (1) ( , ) ( ) ( ) i i c sense W p sense W c W = ……………………………… (2)</formula><p>( ) H W : Word sense entropy of word W ; n: The amount of word senses in Wordnet of word W;  ( ) c W : The total frequency of w in tagged texts.</p><p>In the actual retrieval model, this weight is combined with other weight such as TF, IDF to compute the similarity of two documents. For example, for simple TF*IDF retrieval model, the formula introducing word sense entropy is as the below:</p><formula xml:id="formula_1" coords="5,89.64,141.98,413.55,69.62">( ) ( , ) ( )* ( ) j j j Word q d R q d tf Word idf Word ∈ Λ = ∑ ……………………… original TF*IDF formula ( ) ( , ) ( )* ( )* ( ) j j j j Word q d R q d tf Word idf Word H Word ∈ Λ = ∑</formula><p>…………. Amended TF*IDF formula introducing word sense entropy</p><p>Where ( , ) R q d denotes the similarity value between the query q and the document d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Merging Multiple Runs</head><p>Since there are many different retrieval approaches, an old saying "two heads are better than one" could be used in our system to further improve performance. The ideal solution is to make use of multiple IR approaches and create a Meta IR engine whose core is a merging mechanism. In this year's experiment, we have tried several different strategies merging multiple runs produced by different retrieval approaches.</p><p>The problem for merging multiple runs is described as follows: o is an effective ranking, meaning that it tends to rank relevant documents above irrelevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Merging Simply Several Runs</head><p>Suppose that we have 10 run results. First, we select the documents which appear in each of the returned document lists of 10 runs, to become the firstling members of the merging result; then, choose the documents which appear in 9 runs; finally we choose the documents in turn from each of 10 runs which rank is the highest of the remained documents in the specific run. Repeat the final choosing process until the amount of documents of merging result is 1000,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Merging by Score Normalization</head><p>The first merging method is simple but proves useful in improving the precision of retrieval as our experiences; however it requires enough runs produced by different retrieval methods, and the system cost is high. Therefore, we continue to consider how to efficiently merge small amount of runs such as only two runs. The direct idea is to normalize the different scores produced by different retrieval methods to the scores able to be compared with each other. We refer to the normalization methods in statistical theory and experiment Max-Min normalization and Normal normalization. (Although the following experiences aim at merging the results of only two runs, the merging schemes could also be practical for merging the results of beyond 2 runs) For Max-Min normalization formula,</p><formula xml:id="formula_2" coords="6,109.92,208.82,355.78,32.82">' 1 i N i N score score score score score - = - …………………………………………………………<label>(3)</label></formula><p>' i score : Normalized score for the i th document; i score : Score of the i th document in this run; score : Mean score of the 1000 relevant documents in this run; i score : Score of the ith document in this run; σ : Variance of the 1000 relevant documents in this run.</p><p>After normalizing the scores of different runs according to formula (3) and ( <ref type="formula" coords="6,475.93,462.62,4.37,10.84">4</ref>), we can select the top 1000 documents from these two runs according to the ranking sequence of normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Merging by Clustering</head><p>Because each retrieval methods' scoring methods is different, the intuitive explanation is not clear for normalizing retrieval scores. We experiment another merging method, which is based on result documents clustering.</p><p>As we know, the documents could be relevant with each other with great probability, if they are relevant with the specific topic. So if some documents retrieved by different retrieval methods are  similar with each other, then we believe that these documents are more relevant with this topic, ie. the probability, that these documents is relevant with this topic is higher than that of other documents in those runs. The Key notion is demonstrated as right figure (figure <ref type="figure" coords="7,272.22,148.46,4.53,10.84" target="#fig_6">2</ref>):</p><p>According to this idea, we design a merging algorithm (in our experiment the amount of runs is 2):</p><p>Assuming the merged document set is</p><formula xml:id="formula_3" coords="7,108.18,209.55,357.74,29.24">{ }, 1, i D d i N = = , 1000 2000 N ≤ ≤ a)</formula><p>Select the documents which appear in both runs as a part of the new merging result.</p><formula xml:id="formula_4" coords="7,108.18,238.79,138.85,32.34">1 1 1 { } k D d = , 1 1 1, , k M = b)</formula><p>1 D is used as the document pool which is benchmark for comparing the similarity of other documents. c) Compute the similarity scores of the remained documents  </p><formula xml:id="formula_5" coords="7,130.38,304.05,377.25,34.67">{ } i i i k D d = , 1, , i k M = , M is usually 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results</head><p>In this year's track, we create a baseline run "NLPR04OKapi" based on BM25 retrieval method, a baseline run "NLPR04SemLM" which uses windows-based model on word and sense entropy weighting and a baseline run "NLPR04LMts" which uses windows-based model, feedback technique in Lemur toolkit and two-step retrieval scheme. These runs all use three fields of topics: "title", "description" and "narrative".</p><p>Furthermore, for the experiment of merging methods, we use the lemur toolkit which is for language model and information retrieval to create 4 baseline runs <ref type="bibr" coords="7,87.54,596.36,12.53,10.84">[6]</ref>[7]: KL-DIR, KL-DIR-DIVMIN, TWO-STAGE and JM smoothing. (Detailed information about lemur can be seen in http://www-2.cs.cmu.edu/~lemur/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Data</head><p>The followed table 1 is the description for our submitted runs in this year's robust track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID tag Description NLPR04OKapi</head><p>Baseline retrieval system using BM25 NLPR04SemLM</p><p>Baseline retrieval system using windows-based model and Word Sense Entropy weighting NLPR04LMts</p><p>Baseline III) For merging methods by score normalization, they improve the performance of baseline systems; however we could find that the nuance between max-min ("NLPR04okdiv", "NLPR04okall", "NLPR04oktwo") and normal normalization ("NLPR04NcA", "NLPR04LcA") methods' improvements over the baseline system is slight. The reason might be the two score normalization methods are both not clear in intuitive explanation and the two normalization methods have not obvious predominance to each other.</p><p>IV) The run of merging method by clustering ("NLPR04COMB") also perform favorably in comparison with the baseline system of this year, but because we submit only one run, the experiment maybe not sufficient enough to prove the efficiency of the clustering method.</p><p>V) For predicting the hardness of topics, it is a fiasco for our runs. We have misunderstood this sub track. The track has required a strict ordering of all 250 topics in the test set from easiest (1) to most difficult (250). However we have just reversed the ordering: from most difficult (1) to easiest (250). That is why we have got the worst score in Predicting the hardness of topics. Our prediction for hardness is simple, which only use the distribution of score for each run. This process for hardness prediction is proved somewhat simple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Three techniques are introduced in this year's robust track: (1) two-step retrieval scheme; (2) word sense entropy; (3) several strategies for merging multiple runs.</p><p>And the experiments prove that these techniques are efficient to some extent in improving the performance of worst-query. Unfortunately, for predicting the topic hardness, we misunderstand it and get a poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledge</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,157.98,587.83,3.37,5.88"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.82,660.45,1.89,5.94;4,109.92,654.38,224.89,10.18"><head>isense:</head><label></label><figDesc>The ith word sense in Wordnet of word W;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,115.02,671.90,3.89,10.18;4,147.36,671.90,18.65,10.18;4,144.12,678.13,1.88,5.89;4,109.56,671.90,278.72,10.48"><head>:</head><label></label><figDesc>The frequency of w tagged as s i in tagged texts;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,134.16,288.90,3.38,5.90;6,109.92,282.70,98.37,10.15;6,208.32,280.52,5.26,6.34;6,216.42,282.95,94.73,9.49;6,135.12,306.42,4.52,5.90;6,109.92,300.22,351.05,10.15;6,87.54,327.74,176.34,10.84;6,135.48,362.04,1.22,5.90;6,177.66,362.82,1.88,5.90;6,136.80,370.32,1.88,5.90;6,152.94,356.58,63.77,10.21;6,109.92,364.08,25.29,10.21;6,179.88,368.44,7.06,15.15;6,182.88,352.49,6.42,14.33;6,142.74,359.99,6.42,14.33;6,219.72,365.06,239.13,8.90;6,135.48,386.53,1.21,5.88;6,136.80,394.75,1.88,5.88;6,109.92,388.52,208.73,10.18"><head>1 score:</head><label>1</label><figDesc>Score of the 1 st document in this run; N score : Score of the last document (Nth) in this run. N = 1000 in Robust track For Normal normalization formula, score of the ith document;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,369.60,574.34,32.96,7.79;6,369.60,585.32,28.82,7.79;6,284.52,758.93,216.87,8.24;6,363.36,768.35,59.20,8.24"><head></head><label></label><figDesc>document clusterWhere Triangle, Star, Circle present the documents from different runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,320.58,747.99,145.52,9.38"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Merging by clustering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,426.00,296.83,3.37,5.88;7,399.42,290.60,27.75,10.18;7,409.98,286.53,6.40,14.29;7,436.32,290.87,17.07,9.49;7,468.00,296.83,3.37,5.88;7,460.68,290.60,46.92,10.18;7,128.58,308.39,320.45,9.49"><head></head><label></label><figDesc>the documents of top similarity scores as the new merging result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,108.18,344.39,134.39,9.54;7,257.64,350.35,1.88,5.88;7,249.78,344.12,257.77,10.18;7,128.58,359.75,378.94,9.49;7,128.58,381.41,49.86,9.49;7,212.28,395.71,3.37,5.88;7,208.98,372.19,1.88,5.88;7,226.80,387.37,1.88,5.88;7,206.40,395.71,1.88,5.88;7,185.64,381.08,8.43,10.18;7,217.56,381.08,8.43,10.18;7,208.92,393.35,3.70,8.26;7,196.26,375.74,19.54,19.20;7,237.18,381.41,17.07,9.49;7,269.34,387.37,1.88,5.88;7,261.54,381.14,246.10,10.18;7,128.58,408.65,103.39,9.49;7,295.08,417.56,2.45,4.28;7,252.84,414.61,3.37,5.88;7,284.88,414.61,3.37,5.88;7,319.68,414.61,3.37,5.88;7,267.54,408.30,5.62,10.21;7,298.56,408.30,8.57,10.21;7,334.26,408.30,23.03,10.21;7,291.18,417.56,1.36,4.28;7,246.96,414.61,1.88,5.88;7,279.00,414.61,12.18,5.88;7,313.86,414.61,1.88,5.88;7,239.16,408.30,8.46,10.21;7,272.88,408.30,5.86,10.21;7,308.58,408.30,5.20,10.21;7,358.92,408.30,9.75,10.21;7,292.74,415.85,2.69,6.01;7,249.36,412.24,3.71,8.29;7,281.39,412.24,3.71,8.29;7,316.25,412.24,3.71,8.29;7,259.56,404.21,6.42,14.33;7,326.40,404.21,6.42,14.33;7,108.18,425.24,289.87,10.84"><head>d)</head><label></label><figDesc>Define the document set i D in step c as the new document pool of benchmark.. Repeat the process of step c: Compute the similarity scores of the remained documents step c and d until the amount of documents is 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,87.54,121.16,420.00,561.40"><head>Table 2 :</head><label>2</label><figDesc>Description for our submitted runsThe followed table 2 is the evaluation comparison over all topics for our submitted runs in this year's robust track.</figDesc><table coords="8,294.72,121.16,196.78,32.36"><row><cell>retrieval system using windows-based</cell></row><row><cell>model, feedback technique in Lemur toolkit and</cell></row><row><cell>two-step retrieval scheme</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,131.16,704.72,353.14,10.84"><head>Table 3 :</head><label>3</label><figDesc>Evaluation comparison over all topics for our submitted runs</figDesc><table coords="9,87.54,108.68,420.12,168.34"><row><cell>3.2. Experimental Analysis</cell></row><row><cell>According to the above experimental results, we can get the following</cell></row><row><cell>conclusions.</cell></row><row><cell>I) Compared with our last year's submission [Qianli Jin 2003], our baseline</cell></row><row><cell>results ("NLPR04SemLM" and "NLPR04LMts") of this year perform</cell></row><row><cell>favorably. It shows that the Two-Step scheme and Word Sense Entropy</cell></row><row><cell>Weighting techniques are efficient.</cell></row><row><cell>II) Compared with our baseline systems of this year, the merging methods</cell></row><row><cell>prove efficient. The simple merging method's run ("NLPR04clus10") gets</cell></row><row><cell>the highest MAP.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>This work was supported by the <rs type="funder">Natural Sciences Foundation of China</rs> under grant No. <rs type="grantNumber">60372016</rs>, the <rs type="funder">Natural Science Foundation of Beijing</rs> under grant No. <rs type="grantNumber">4052027</rs> and the <rs type="funder">Scientific Research Foundation for Returned Overseas Chinese Scholars, State Education Ministry</rs>.</p><p>Our work have used two web-shared toolkits: Lemur toolkit for language model and information retrieval (URL: http://www-2.cs.cmu.edu/~lemur/), and Wordnet 1.7 (URL: http://www.cogsci.princeton.edu/~wn/).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gukpQxQ">
					<idno type="grant-number">60372016</idno>
				</org>
				<org type="funding" xml:id="_6ZpRaZq">
					<idno type="grant-number">4052027</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,93.21,227.72,80.81,12.63" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Reference</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.06,254.48,402.66,10.84;10,105.06,268.04,348.85,10.84" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,251.22,254.48,251.10,10.84">NLPR at TREC 2003 -Novelty and Robust Track</title>
		<author>
			<persName coords=""><forename type="first">Qianli</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,105.06,268.04,192.53,10.84">Text Retrieval Conference (TREC-12)</title>
		<meeting><address><addrLine>Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.06,293.06,402.53,10.84;10,105.06,306.56,346.27,10.84" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thistlewaite</surname></persName>
		</author>
		<title level="m" coord="10,384.89,293.06,122.70,10.84;10,105.06,306.56,311.31,10.84">Merging Results From Isolated Search Engines. Australasian Database Conference</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.94,331.64,402.66,10.84;10,104.94,345.14,402.66,10.84;10,104.94,358.58,57.65,10.84" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,255.54,331.64,247.13,10.84">Window-based Method for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Qianli</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,144.02,345.14,363.58,10.84;10,104.94,358.58,57.65,10.84">The First International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.94,383.60,402.67,10.84;10,104.94,397.16,348.84,10.84" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,215.88,383.60,286.34,10.84">Overview of the TREC 2003 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,104.94,397.16,192.47,10.84">Text Retrieval Conference (TREC-12)</title>
		<meeting><address><addrLine>NIST, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.13,449.06,402.52,10.84;10,110.88,462.50,396.82,10.84;10,110.88,475.94,289.20,10.84" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,299.94,449.06,207.71,10.84;10,110.88,462.50,77.22,10.84">Model-based feedback in kl divergence retrieval model</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,216.96,462.50,290.74,10.84;10,110.88,475.94,251.01,10.84">Proceedings of the 10th International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 10th International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.94,500.96,402.73,10.84;10,104.94,514.46,180.92,10.84;10,285.84,511.93,6.48,7.21;10,295.62,514.46,93.40,10.84" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,276.48,500.96,231.19,10.84;10,104.94,514.46,101.18,10.84">Risk minimization and language modeling in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,226.88,514.46,58.99,10.84;10,285.84,511.93,6.48,7.21;10,295.62,514.46,58.35,10.84">Proc. Of 24 th ACM SIGIR</title>
		<meeting>Of 24 th ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
