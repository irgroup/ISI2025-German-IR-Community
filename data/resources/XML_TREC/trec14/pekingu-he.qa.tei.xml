<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,102.23,101.43,407.57,12.55">Peking University at the TREC-2005 Question and Answering Track</title>
				<funder ref="#_parVc3d">
					<orgName type="full">PRC Ministry of Education</orgName>
				</funder>
				<funder ref="#_NWmqWHh">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.46,140.65,39.52,10.76"><forename type="first">Jing</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.95,140.65,63.43,10.76"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.35,140.65,61.76,10.76"><forename type="first">Conglei</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.07,140.65,44.41,10.76"><forename type="first">Ping</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.43,140.65,66.10,10.76"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,102.23,101.43,407.57,12.55">Peking University at the TREC-2005 Question and Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D475839076A20439CBA17124E255BD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the architecture and implementation of Tianwang QA system, which can work for the Main task and the Document Ranking task. The system is designed to extract candidate answer snippets from different pipelines, e.g. the high quality search engines' results, the frequently asked question (FAQ) set, and the wellstructured web facts, etc..So the system need to process the Web documents, the FAQ corpus and the knowledge base (KB) from the structural web pages, besides analyzing the query, the TREC document retrieval and the answer merging. The external knowledge we made use of, i.e. FAQ and KB, are proved to be effective for our final results. We classify questions with SVM approaches, construct queries in Boolean way, retrieve and rank the passage with span model and extract answers using named entity technologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Answering human beings' questions in exact words has long been studied from multiple research fields. From 1999, TREC held QA track to prompt it by raising different tasks, releasing the answer results and ranking the runs of each team. The tasks in 2005 includes main task, document ranking and relationship task.. Our group participate TREC QA track this year for the first time. Lack of experience as we are, we make good use of our ability in web crawling to seek for answer verification among external date sources. And we focus on the effect of information retrieval (IR) technologies, used in our Web search engines, on QA research. This paper is organized as follows: In Section 2 we give an overview of our system, including the architecture and the important components. In Section 3 we describe the components in detail and analyze the technology we use. In Section 4 we conclude the system and show the result of this evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overwhelm of TianWang QA system</head><p>As mentioned above, our system, TianWang QA, is set up for the first time in this track.. However, we try many technologies and approaches in our system to deepen our research on the related fields. The system infrastructure are given below. First, the TREC document set is pre-processed and indexed. Second, the question terms are generated from the target and the question type. Third, the question terms are passed to the FAQ index interface, the KB interface and the web crawler. After the candidate answer snippets are returned from different source with ranking order, they are checked in the TREC document set by the answer validation part. And finally, the very matched answers are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main System Components</head><p>Our system is a loose one. Every component is flexible that can be implemented with different approaches or algorithms. In this section, we describe every component in detail and analyze them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Document Module</head><p>The purpose of Document module is to preprocess the collection. Most of the work is based on a generally used package Lucene. We parse the collection, remove some tags and stop words, stem (or not stem for different version) and construct the inverted files for the collection. The size of inverted file is more than 3 Gigabytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Query and Retrieval Module</head><p>This module is the base for the next four steps. In this part, the questions are classified. And the documents are retrieved for entity extraction component and Document Ranking task. We follow the two-layered question taxonomy <ref type="bibr" coords="2,78.46,255.42,10.58,8.97" target="#b6">[7]</ref>, containing 6 coarse grained categories and 50 fine grained categories. We use SVM method with one against all strategy and bag of words feature to perform the classification <ref type="bibr" coords="2,91.62,291.29,15.27,8.97" target="#b10">[11]</ref>. Each question is labeled with only one category with maximum probability. The training dataset is provided by UIUC <ref type="bibr" coords="2,136.07,315.20,10.58,8.97" target="#b6">[7]</ref>, which has 5,500 labeled questions. Each coarse grained category contains an on-overlapping set of fine grained categories. Since finer grained categories can benefit us in locating and verifying the plausible answers, we extend the question taxonomy with some categories to finer categories.</p><p>We construct Boolean queries from questions only and queries are constructed iteratively. The approach is similar to <ref type="bibr" coords="2,82.78,411.82,10.58,8.97" target="#b8">[9]</ref>. However, we use some linguistic knowledge as <ref type="bibr" coords="2,58.50,423.77,10.58,8.97" target="#b3">[4]</ref>. We analyze the POS of the question and extract the entities from it. We start the query with proper nouns and named entities. If the number of documents retrieved is in a predefined scope, the processing terminates. Otherwise, we should loose or contract the query. The strategies for loosing a query include drop some query term, replace the PHRASE query to AND query, add some synonymy or morphological extension and so on. The strategies for contraction include add terms, depending on their idf value. The processing continues until the number document retrieved satisfies a threshold. However, the documents retrieved from the Boolean query have no ranking. So we need to rank the retrieved documents. The most common approach is to calculate the cosine similarity. As <ref type="bibr" coords="2,58.50,591.14,16.60,8.97" target="#b9">[10]</ref> pointed out, span based approach is effective for QA, so we also consider distance between query words and get the documents ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">FAQ Corpus Module</head><p>As many previous work <ref type="bibr" coords="2,172.73,663.83,11.62,8.97" target="#b1">[2]</ref>[3] pointed out, FAQ documents, which consist of a series of frequent asked questions and their authority answers, are important for QA. In order to get a FAQ collection, we tried two steps. First, we select some web sites, such as FAQ Archive, collecting FAQ documents in some domains; Second, we use Google to find some web pages containing some words such as "FAQ""Frequent asked question", etc. They are often the entry pages linked to FAQ web pages. Then we analyzed their anchor texts and link URL and to get the needed FAQ web pages. We finally have about 23,000 qualified web pages. We identify and classify the questions, build the Q-A pair and index the FAQ documents by Lemur.</p><p>If some FAQ documents matches the questions, the related QA pairs are located. We get the candidate answers based on combination score of question category similarity, the document relevance and QA pair relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Knowledge Base Module</head><p>Enlightened by the idea of MIT's Aranea <ref type="bibr" coords="2,501.40,282.11,11.20,8.97" target="#b5">[6]</ref>[1][5], we choose high quality web content with little noises and regular organization as a kind of pre-built knowledge base. Our source data come from CIA factbook, 50states.com and the biography.com. The data extraction is based on pattern built manually, which is the reason why the scale of such kind of knowledge base is limited. However, <ref type="bibr" coords="2,506.54,353.84,11.62,8.97" target="#b7">[8]</ref> has verified that ten selected Web sources provides 47% answers for TREC-2001 QA track.</p><p>We design data model using the tri-tuple ¡object, property, value¿ to describe the entities in the structured or semi-structured web pages. Every entity, no matter how complex originally described, can be simplified into a series of property-value pairs. We expanded the index words of the properties in some scope using synonyms. As we consider the three parts of one tuple: 1) The object words, mainly named entity or proper nouns specifying a certain entity, rarely need expansion; 2) The value part, showing the facts about the entities such as number, date and so on, is not suitable to expand either. 3) The property part, derived from the entities' description or attribution, limited in quantity and diverse in expression, is deserved to be expanded with synonyms or thesaurus.</p><p>Tuples are taken as documents when processing. The key words detected by GATE in the tuple elements are indexed, and the questions are converted into bags of terms. The indexed words come from each parts. In many cases, the question about certain entities should match property or object part. The index format is like: &lt;term, tuple element, tuple id&gt;.The tuple set is processed using Lucence. The result of a query is a set of tuple id-tuple element pair. The candidate answer selection strategy is: If two of the three tuple elements are matched, we extract another element. The extracted element should be in the entity category corresponding to question category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Web Search Module</head><p>We choose Web as an information source for this task. We pick key words from the question and search them in Google. As the snippets are retrieved, we select them according to number of keywords matched threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Answer Extraction and Validation</head><p>The answers may be extracted from document passages, FAQ or Search Engine snippets, but the approaches are similar. The entity types include GATE entities and some predefined entity type. We use WordNet and some Web resources to find list of entities and tag their type. We also write some regular expression to match some type of entities. So the extracted entities are from GATE, list or regular expression matching.</p><p>We assign scores to each entity extracted, and rank entities according to their scores. The score computation is similar to <ref type="bibr" coords="3,100.22,301.95,10.58,8.97" target="#b6">[7]</ref>. One problem is whether or not the entity's type matches the question category. In most cases, the matching is boolean. However, things are not always so simple. If the question category is NUM:date, the "full" dates are ranked above "year" dates. Conversely, the "year" dates are ranked above "full" dates for the question type NUM:date:year. Then entity score is based on the frequency of occurrences of a given entity within the passages. We use the occurrence frequency of an entity as its score. This score is as the second sort key, to impose a ranking on entities that are not distinguished by the first score component. Some entity normalization are needed in counting because same entity such as person name and date may be expressed differently. Then we merge entities from data sources and entities from knowledge base, and the entities from knowledge base are put ahead of all.</p><p>The entities, extracted from many information sources, may not be located in the AQUAINT data source. So we should filter these entities. Then the results filtered are the final results. For list questions, the first 5 entities are as answers; for factoid questions, the first entity is as the answer and for definition questions, the top 7 passage snippets containing any of the top 5 entities are as answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>In this paper, we describe why and how we design each of the components. The scores for factoid, list and other questions are 0.108, 0.053 and 0.025. So the system should be improved in multiple ways. The IR sub-system may be trained by the evaluation data of Document Ranking task this year. Question or Answer patterns may be used in new version. Also we may make use of some natural language processing technologies such as syntax and semantic anal-ysis of sentence. KB and FAQ collection also should be expanded in size and be explored in a quantitative way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,336.86,477.08,199.28,9.22;1,319.50,318.41,254.22,143.91"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Big Picture of Tianwang QA system</figDesc><graphic coords="1,319.50,318.41,254.22,143.91" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work described therein is supported in part by <rs type="funder">PRC Ministry of Education</rs> grant <rs type="grantNumber">20030001076</rs> and by <rs type="funder">NSFC</rs> grant <rs type="grantNumber">60435020</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_parVc3d">
					<idno type="grant-number">20030001076</idno>
				</org>
				<org type="funding" xml:id="_NWmqWHh">
					<idno type="grant-number">60435020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,339.42,210.33,214.07,8.07;3,339.42,221.29,214.07,8.07;3,339.42,232.25,214.07,8.07;3,339.42,243.21,214.07,8.07;3,339.42,254.17,64.50,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,471.35,232.25,82.14,8.07;3,339.42,243.21,194.14,8.07">Integrating web-based and corpus-based techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,339.42,254.17,37.42,8.07">TREC2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,265.13,214.08,8.07;3,339.42,276.09,214.07,8.07;3,339.42,287.05,182.49,8.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="3,439.94,276.09,113.55,8.07;3,339.42,287.05,156.47,8.07">Natural language processing in the faq finder system: Results and prospects</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kulyukin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lytinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tomuro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schoenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,298.01,214.07,8.07;3,339.42,308.96,214.07,8.07;3,339.42,319.92,201.69,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,370.30,308.96,183.19,8.07;3,339.42,319.92,38.17,8.07">Knowledge-based navigation of complex information spaces</title>
		<author>
			<persName coords=""><forename type="first">Robin</forename><forename type="middle">D</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,394.19,319.92,36.97,8.07">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="462" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,330.88,214.07,8.07;3,339.42,341.84,214.07,8.07;3,339.42,352.80,214.07,8.07;3,339.42,363.76,214.08,8.07;3,339.42,374.72,131.98,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="3,514.14,330.88,39.35,8.07;3,339.42,341.84,214.07,8.07;3,339.42,352.80,140.07,8.07">Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,499.17,352.80,54.32,8.07;3,339.42,363.76,214.08,8.07;3,339.42,374.72,46.80,8.07">Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-2000)</title>
		<meeting>the Sixth Applied Natural Language Processing Conference (ANLP-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,385.68,214.07,8.07;3,339.42,396.64,214.07,8.07;3,339.42,407.60,214.07,8.07;3,339.42,418.55,142.67,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="3,429.17,385.68,124.32,8.07;3,339.42,396.64,214.07,8.07;3,339.42,407.60,21.99,8.07">Question answering from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,379.03,407.60,174.46,8.07;3,339.42,418.55,115.91,8.07">the twelfth international conference on Information and knowledge management</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,429.51,214.07,8.07;3,339.42,440.47,214.07,8.07;3,339.42,451.43,214.07,8.07;3,339.42,462.39,66.75,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="3,413.54,440.47,139.95,8.07;3,339.42,451.43,200.55,8.07">Extracting answers from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,339.42,462.39,40.30,8.07">TREC 2002</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,473.35,197.48,8.07" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="3,409.64,473.35,101.71,8.07">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,484.31,214.07,8.07;3,339.42,495.27,214.07,8.07;3,339.42,506.23,195.89,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="3,385.09,484.31,168.40,8.07;3,339.42,495.27,95.41,8.07">The web as a resource for question answering: Perspective and challenges</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,452.61,495.27,100.88,8.07;3,339.42,506.23,169.70,8.07">the third International Conference on Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,517.18,214.08,8.07;3,339.42,528.14,214.07,8.07;3,339.42,539.10,151.90,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="3,488.51,517.18,64.99,8.07;3,339.42,528.14,214.07,8.07;3,339.42,539.10,67.81,8.07">Exploring the performance of boolean retrieval strategies for open domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,424.08,539.10,40.79,8.07">SIGIR 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,550.06,214.07,8.07;3,339.42,561.02,214.07,8.07;3,339.42,571.98,214.08,8.07;3,339.42,582.94,20.17,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="3,418.40,561.02,135.09,8.07;3,339.42,571.98,148.67,8.07">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,507.96,571.98,41.50,8.07">SIGIR 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,339.42,593.90,214.07,8.07;3,339.42,604.86,140.30,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="3,422.57,593.90,130.92,8.07;3,339.42,604.86,56.61,8.07">Question classification using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,412.98,604.86,40.29,8.07">SICIR 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
