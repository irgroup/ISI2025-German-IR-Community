<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.91,118.16,446.17,18.14;1,224.32,143.07,163.36,18.14">Logistic Regression Merging of Amberfish and Lucene Multisearch Results</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-11-15">November 15, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,166.23,182.87,132.93,12.58"><forename type="first">Christopher</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
							<email>fallen@arsc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center Fairbanks</orgName>
								<address>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.17,182.87,114.61,12.58"><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center Fairbanks</orgName>
								<address>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.91,118.16,446.17,18.14;1,224.32,143.07,163.36,18.14">Logistic Regression Merging of Amberfish and Lucene Multisearch Results</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-11-15">November 15, 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">7DCD83737448E3C1030598265F11D5E5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A simple logistic-regression based isolated data fusion algorithm was used to merge results from two free open-source text retrieval tools. The algorithm is described and results from each search tool are compared against the merged results and against each other. Basic performance measures are reported and discussed, and future projects are outlined.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.275" lry="841.889"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>The changing size and structure of the information available on the Internet guarantees that for any given performance measure, there will not be a single optimal method for retrieving information relevant to a specified topic. The availability and sophistication of distributed computing resources will continue to increase and consequently so will the relevance of problems related to distributed information retrieval. Data fusion and collection fusion are two such problems and the former in the context of a grid information retrieval meta-search application is the motivation behind the work described here.</p><p>Two GPL-licensed text retrieval tools, The Apache Software Foundation's Lucene and Etymon systems' Amberfish, were used to retrieve ranked document lists from the gov2corpus document collection for each topic given in the efficiency, ad hoc, and named page tasks in the 2005 TREC Terabyte Track. For the ad hoc and named page tasks, ranked document lists were produced in three official runs: one run each using Lucene and Amberfish with automatically extracted queries, and one run that consisted of merging the results from the previous two runs using normalized linear least square fits to log-transformed relevance scores. For the ad hoc task an additional run consisting of merged Lucene and Amberfish results from manually extracted queries was also submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lucene</head><p>The Apache Software Foundation describes Lucene as "a high-performance, full-featured text search engine library written entirely in Java" <ref type="bibr" coords="2,331.97,112.40,11.70,10.48" target="#b2">[3]</ref>. Simple index and search applications were constructed by modifying the demo classes IndexHTML.java and SearchFiles.java, respectively, included with the 1.4.3 distribution of Apache Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amberfish</head><p>Amberfish is a command-line based general-purpose text retrieval tool developed by Nassib Nassar and is described in greater detail at the Etymon Systems website <ref type="bibr" coords="2,455.61,212.57,11.71,10.48">[7]</ref>. Nassar used Amberfish to submit runs to the 2004 TREC Terabyte track and the results are summarized in his 2004 Terabyte track proceedings paper <ref type="bibr" coords="2,340.96,241.46,11.71,10.48" target="#b6">[8]</ref>. Amberfish 1.4.3 was used without modifications in the 2005 Terabyte track ad hoc task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>For the general fusion problem, Voorhees classifies merging strategies based on only the information available from a ranked document list as isolated and strategies that make use of collection information to merge document lists as integrated <ref type="bibr" coords="2,398.28,351.02,17.17,10.48" target="#b8">[10]</ref>. The logistic regression strategy described in section 4 is isolated and based on the document rank and relevance score information from a TREC-style ranked document list. The work described here is a preliminary step toward a study of the performance of new integrated merging strategies designed for Grid Information Retrieval (GIR) meta-search applications against a canonical set of isolated merging strategies similar to the study by Craswell, et al. <ref type="bibr" coords="2,467.62,423.25,11.71,10.48" target="#b1">[2]</ref>. The GIR architecture and requirements are described by the GIR working group <ref type="bibr" coords="2,431.89,437.69,14.96,10.48" target="#b3">[4]</ref>.</p><p>The logistic regression approach to the search result merging problem is not new nor is it new to TREC. Savoy, Calvé, and Vrajitoru used logistic regression in the TREC-5 experiment as a strategy for both the collection fusion problem where search results from possibly disjoint and independent collections are merged into a single ranked list and the data fusion problem where engines operating on a single data collection interact to provide a single ranked list <ref type="bibr" coords="2,524.36,509.92,11.73,10.48" target="#b7">[9]</ref>. Estimating relevance is perhaps the fundamental challenge of information retrieval; a process to estimate the probability that a given document is relevant to a query from collection or document statistics is described by Gey as logistic inference <ref type="bibr" coords="2,375.41,553.26,14.03,10.48" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficiency Task</head><p>The goal for the efficiency task was to demonstrate basic functionality of a simple JAVA IR tool built with the Lucene API. The index data structures were fed and saved to a remote NFS and a possible network driver bug caused intermittent slow-downs and system unresponsiveness. Consequently, only 66% of the document corpus was indexed before the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency Task Results</head><p>The document text and index data structures were stored remotely on a NFS. The index data structure was built and searched with a commodity dual-Opteron Linux server. Listed below are two topics that threw a parse exception at search time and motivated the addition of simple exception handling to the demo Lucene search application: Topic 35369 ?????????? ?????????? ???? Topic 46131 illinois school public relations ?ssociation</p><p>The median proportion of documents indexed is 100% but the Lucene index application indexed only 66% of the documents by the efficiency task deadline so a meaningful comparison of performance measures can only be made if the document collection is assumed to be homogeneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ad Hoc Task</head><p>The ad hoc task was used to compare the performance of the Lucene and Amberfish IR tools and to evaluate a basic ranked-document list merging algorithm. Runs were submitted using both automatically extracted and manually constructed queries.</p><p>Lucene and Amberfish were used to create separate index data structures of the entire gov2 document corpus over a remote NFS. The corpus files were split into their respective document files before indexing the collection. Malformed HTML caused the Lucene index application to truncate about 1/50th of documents added to the index; the resulting index data structure was about 1/10th the size of the uncompressed document corpus. A corrupted file system prevented Amberfish from indexing one of the 273 document directories; the size of the index was 3/5th the size of the uncompressed document corpus. Both index data structures were partitioned arbitrarily for the purpose of load-balancing at search time, each search application was run sequentially on four dual-Opteron nodes of a commodity Linux cluster. For each run, the queries were automatically constructed from the title fields of the 05.topics query file by joining the terms in each title with boolean-OR.</p><p>The results returned by each node were merged into a single ranked-document list based on document score and reported as runs ctfadhocaf1 and ctfadhocluc2, respectively. A variant of a logistic regression based database merging strategy <ref type="bibr" coords="4,406.15,148.02,12.36,10.48" target="#b0">[1]</ref> was used to merge the ranked-document lists from each search application. The logarithm of the document scores was calculated and linear least squares was used to fit log-normal curves to each of the resulting log score vs. rank data. The final merged document list was ordered by comparing the height of each curve at a given rank and picking the document corresponding to the largest score. This process was repeated for queries constructed manually and specifically for each search tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ad Hoc Task Results</head><p>As measured by the mean average precision, Lucene performed somewhat better than Amberfish with both automatically extracted and manually constructed queries. As measured by binary preference, Lucene performed better than Amberfish with automatically extracted queries and Amberfish performed better than Lucene with manually constructed queries. By both measures the performance of the merged result list was comparable to the performance of Lucene alone. This is consistent with observations made at run time that the merge algorithm nearly always chose a document retrieved by Lucene over a document retrieved by Amberfish at each rank.</p><p>The motivation to fitting the relevance scores from each ranked document list to a lognormal distribution is to weight documents near the top of each list more heavily than documents further down so that presumably irrelevant documents far down the ranked document list will not pollute the presumably relevant results near the top of each list. While it appears from these results that the merge algorithm described above does not reduce the retrieval performance compared to either engine alone, the performance of the merge algorithm needs to be evaluated with respect to additional engines. Since the TREC Terabyte track ranked document lists contain all the information used by the merge algorithm, it is possible to use existing TREC Terabyte track data to further investigate merging performance.</p><p>Measuring by mean average precision and considering the topics where Lucene performed somewhat better than Amberfish using automatically extracted queries, the performance of the merged list was indistinguishable from the performance of Lucene except for topics 757, 789, and 797 where the merged performance was somewhat worse than that of Lucene alone and topic 758 where the merged performance was somewhat better than either engine alone. Amberfish somewhat outperformed Lucene in eight topics and of those topics, the merged performance was indistinguishable from Amberfish alone in topics 779 and 793. The merged performance of the merged list on the remaining six topics was somewhat worse than that of Amberfish alone.</p><p>Topic 758, perhaps by coincidence, is the one topic where the merged results perform better than the results from either engine alone is also a topic where both engines performed well. The poor relative performance on topic 770 is likely because neither Lucene nor Amberfish parse "Kyrgystan-United States" effectively. There is no immediate explanation based on the query text for the inconsistent performance of the merged result list relative to the individual result lists from Amberfish and Lucene. An ideal merge algorithm will combine all relevant documents from several ranked document lists into a single ranked document list. From these results, it is clear that the merge algorithm used here is far from ideal and more work is needed so that the merged performance is not capped at the performance of the best performing engine or to any engine in particular. There may not be a single good method for merging ranked document lists using only rank and relevance score information. The results from the TREC 2005 Terabyte track ad hoc task could possibly be used as a baseline from which to compare more sophisticated merge algorithms that take advantage of additional information like collection term distributions in the case of multiple disjoint document collections or term boost parameters of various engines operating on a single document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Named Page Task</head><p>The goal of the named page task was to evaluate the basic performance of Lucene restricted to the collection of document titles. The index data structure from the ad hoc task was used and SearchFiles.java was modified so that each search was limited to the text contained within the title tags of each document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto query extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual query construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Page Task Results</head><p>The mean reciprocal rank of the named page was better than the median value in 21 out of 252 topics. Of those 21, the following topics scored a reciprocal rank greater than 0.33 with a median value less than 0.1: Topic 632 rules for us federal government lodging and tax exemptions Topic 658 genetics reference glossary Topic 662 c++ allocator object documentation Topic 720 1999 report romania human rights Topic 834 walnut creek fishing conditions Topic 838 denise crawford testimony The StandardAnalyzer class in the Lucene API, used here to parse plain-text at index and search time, uses sophisticated grammar rules <ref type="bibr" coords="8,329.69,465.88,12.36,10.48" target="#b5">[6]</ref> to extract punctuation so it is possible that the strong relative performance for topic 662 is due to leaving the string "c++" intact. Relatively poor performance was observed for several topics that included the strings "us" for U.S. or numeric strings like "1999" to specify a year so there is no obviously plausible explanation for the strong performance on topics 632 and 720. Similarly, poor performance was observed for several topics containing double nouns as in "walnut creek" and "denise crawford."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>Merging results from different systems is a challenging but important contemporary problem. This paper has described an approach with fairly light requirements for system output, and a relatively simple logistic regression-based computational step to merge results. In order to be able to merge results across systems, we need effective means to determine how the individual "hits" in response sets should be ranked relative to each other. This is relevant for Grid-based retrieval, for Web meta-search, and for other situations where a single query might be sent to multiple IR systems.</p><p>We can envision much more complicated approaches to that taken here, and will seek to compare these to the current logistic regression approach, as well as naive approaches (such as simplistic round-robin ranking, or ranking based on self-reported normalized cosine or percentage scores). There are three primary directions for more complicated approaches. First is to look to metadata about collections and response sets. We would like to consider how knowledge about relative collection size, the frequency of query terms in different collections, the number of hits for each query term, and other statistics about the collection and response set can be used to merge. From this, we envision modifications to traditional tf*idf equations that balance contributions from multiple sources, as a basis for re-ranking response sets.</p><p>Second is to look more deeply at response sets themselves. The downside of this approach is that different systems have different capabilities, and different ways of presenting results. In our multisearch experiments, we developed a simple XML envelope for results. More complete standards are under development, such as SRW (see http://www.loc.gov/ z3950/agency/zing/srw), but might make even stronger demands of specific IR systems to present complete results. From a practical standpoint, we see that most systems are able to provide a normalized score for each document, an extracted plain text document title, and basic metadata (such as a URL or word count). Context-sensitive document extracts are sometimes available. We will look at how these factors can contribute to ranking, with an emphasis on simple equations such as logistic regression.</p><p>Third is to actually gather full text (or extracts) from documents linked in response sets. This is the approach taken by <ref type="bibr" coords="9,235.76,408.04,11.70,10.48" target="#b1">[2]</ref>. It has the benefit of using IR systems as "filters" for potentially interesting documents, then ranking them based on these documents' contents. The drawback is the heavier load placed on the contributing systems, to provide full text from each document in a response set.</p><p>Naive approaches will be useful for comparison. For example, would a simple divide-andconquer approach using a single IR system, but with a subdivided collection, be as effective as a unified collection?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The work presented here has demonstrated that merging results across separate IR systems can be effective. We saw some TREC topics in the Terabyte ad hoc task where combined results were better than either system alone, and other results which were less effective than either system component. We consider result set merger a practical necessity for some types of collections, so will continue to investigate different approaches to merging. We will also continue development of a practical Grid-based design for collection management, indexing and query processing across data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,74.35,468.00,199.89"><head>Table 1 :</head><label>1</label><figDesc>Efficiency task performance, Lucene efficiency task deadline. The entire document corpus was indexed for the ad hoc and named page tasks.</figDesc><table coords="3,120.60,74.35,370.80,115.19"><row><cell></cell><cell cols="2">Performance Notes</cell></row><row><cell>Time to index 280GB</cell><cell>80 hours</cell><cell>System slowdown from possible</cell></row><row><cell></cell><cell></cell><cell>network driver bug</cell></row><row><cell>Index size</cell><cell>30GB</cell><cell></cell></row><row><cell>Time to run 50000</cell><cell>4.3 hours</cell><cell>0.33 seconds per query</cell></row><row><cell>queries</cell><cell></cell><cell></cell></row><row><cell cols="2">Average precision after 0.292</cell><cell></cell></row><row><cell>20 docs retrieved</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,77.98,75.34,470.18,210.22"><head>Table 2 :</head><label>2</label><figDesc>Mean average precision for selected topics</figDesc><table coords="5,77.98,75.34,470.18,186.23"><row><cell cols="2">Topic Query text</cell><cell cols="2">Amberfish Lucene Merged TREC median</cell></row><row><cell>751</cell><cell>Scrabble players</cell><cell>0.1009</cell><cell>0.0593 0.0590 0.2254</cell></row><row><cell>753</cell><cell>bullying prevention programs</cell><cell>0.1045</cell><cell>0.0400 0.0400 0.1869</cell></row><row><cell>757</cell><cell>murals</cell><cell>0.0394</cell><cell>0.1688 0.1345 0.2236</cell></row><row><cell>758</cell><cell>Embryonic stem cells</cell><cell>0.2308</cell><cell>0.2779 0.3084 0.6248</cell></row><row><cell>770</cell><cell cols="2">Kyrgyzstan-United States relations 0.0228</cell><cell>0.0018 0.0017 0.2683</cell></row><row><cell>779</cell><cell>Javelinas range and description</cell><cell>0.1019</cell><cell>0.0783 0.1011 0.3945</cell></row><row><cell>784</cell><cell>mersenne primes</cell><cell>0.5045</cell><cell>0.2890 0.3135 0.4560</cell></row><row><cell>786</cell><cell>Yew trees</cell><cell>0.1142</cell><cell>0.0829 0.0829 0.3616</cell></row><row><cell>789</cell><cell>abandoned mine reclamation</cell><cell>0.0242</cell><cell>0.0607 0.0515 0.2136</cell></row><row><cell>793</cell><cell>Bagpipe Bands</cell><cell>0.1019</cell><cell>0.0471 0.1014 0.2235</cell></row><row><cell>797</cell><cell>reintroduction of gray wolves</cell><cell>0.1252</cell><cell>0.4816 0.3125 0.5107</cell></row><row><cell>800</cell><cell>Ovarian Cancer Treatment</cell><cell>0.0583</cell><cell>0.0253 0.0270 0.1929</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,112.54,161.90,385.69,442.55"><head>Table 3 :</head><label>3</label><figDesc>Ad hoc task retrieval summary statistics</figDesc><table coords="6,112.54,161.90,385.69,418.55"><row><cell>Measure</cell><cell cols="6">Amberfish Lucene merged Amberfish Lucene merged</cell></row><row><cell>num q</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>num ret</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>37996</cell><cell>44330</cell><cell>44405</cell></row><row><cell>num rel</cell><cell>10407</cell><cell>10407</cell><cell>10407</cell><cell>9828</cell><cell>10407</cell><cell>10407</cell></row><row><cell cols="2">num rel ret 2205</cell><cell>3803</cell><cell>3825</cell><cell>2851</cell><cell>3584</cell><cell>3610</cell></row><row><cell>map</cell><cell>0.0499</cell><cell cols="2">0.0983 0.0969</cell><cell>0.0673</cell><cell cols="2">0.1099 0.1116</cell></row><row><cell>R-prec</cell><cell>0.1001</cell><cell cols="2">0.1709 0.1703</cell><cell>0.1283</cell><cell cols="2">0.1776 0.1800</cell></row><row><cell>bpref</cell><cell>0.1015</cell><cell cols="2">0.1349 0.1352</cell><cell>0.1371</cell><cell cols="2">0.1334 0.1336</cell></row><row><cell>recip rank</cell><cell>0.4410</cell><cell cols="2">0.4725 0.4599</cell><cell>0.3915</cell><cell cols="2">0.4628 0.4853</cell></row><row><cell cols="2">ircl prn.0.00 0.4785</cell><cell cols="2">0.5514 0.5416</cell><cell>0.4639</cell><cell cols="2">0.5517 0.5713</cell></row><row><cell cols="2">ircl prn.0.10 0.1487</cell><cell cols="2">0.2761 0.2852</cell><cell>0.1982</cell><cell cols="2">0.3083 0.3031</cell></row><row><cell cols="2">ircl prn.0.20 0.0968</cell><cell cols="2">0.1916 0.1937</cell><cell>0.1412</cell><cell cols="2">0.2105 0.2143</cell></row><row><cell cols="2">ircl prn.0.30 0.0601</cell><cell cols="2">0.1236 0.1228</cell><cell>0.0816</cell><cell cols="2">0.1380 0.1402</cell></row><row><cell cols="2">ircl prn.0.40 0.0399</cell><cell cols="2">0.0982 0.0891</cell><cell>0.0454</cell><cell cols="2">0.0814 0.0860</cell></row><row><cell cols="2">ircl prn.0.50 0.0285</cell><cell cols="2">0.0647 0.0588</cell><cell>0.0290</cell><cell cols="2">0.0548 0.0581</cell></row><row><cell cols="2">ircl prn.0.60 0.0059</cell><cell cols="2">0.0343 0.0305</cell><cell>0.0161</cell><cell cols="2">0.0419 0.0461</cell></row><row><cell cols="2">ircl prn.0.70 0.0024</cell><cell cols="2">0.0143 0.0109</cell><cell>0.0082</cell><cell cols="2">0.0330 0.0352</cell></row><row><cell cols="2">ircl prn.0.80 0.0002</cell><cell cols="2">0.0090 0.0062</cell><cell>0.0032</cell><cell cols="2">0.0255 0.0246</cell></row><row><cell cols="2">ircl prn.0.90 0.0002</cell><cell cols="2">0.0005 0.0013</cell><cell>0.0032</cell><cell cols="2">0.0180 0.0173</cell></row><row><cell cols="2">ircl prn.1.00 0.0002</cell><cell cols="2">0.0005 0.0013</cell><cell>0.0032</cell><cell cols="2">0.0038 0.0028</cell></row><row><cell>P5</cell><cell>0.2440</cell><cell cols="2">0.2960 0.2920</cell><cell>0.2920</cell><cell cols="2">0.3280 0.3280</cell></row><row><cell>P10</cell><cell>0.2080</cell><cell cols="2">0.2800 0.2820</cell><cell>0.2700</cell><cell cols="2">0.3200 0.3140</cell></row><row><cell>P15</cell><cell>0.1893</cell><cell cols="2">0.2813 0.2773</cell><cell>0.2533</cell><cell cols="2">0.3360 0.3307</cell></row><row><cell>P20</cell><cell>0.1880</cell><cell cols="2">0.2770 0.2740</cell><cell>0.2380</cell><cell cols="2">0.3300 0.3210</cell></row><row><cell>P30</cell><cell>0.1647</cell><cell cols="2">0.2693 0.2673</cell><cell>0.2193</cell><cell cols="2">0.3180 0.3140</cell></row><row><cell>P100</cell><cell>0.1126</cell><cell cols="2">0.2196 0.2200</cell><cell>0.1738</cell><cell cols="2">0.2540 0.2608</cell></row><row><cell>P200</cell><cell>0.0904</cell><cell cols="2">0.1705 0.1700</cell><cell>0.1298</cell><cell cols="2">0.1818 0.1858</cell></row><row><cell>P500</cell><cell>0.0618</cell><cell cols="2">0.1129 0.1132</cell><cell>0.0822</cell><cell cols="2">0.1108 0.1120</cell></row><row><cell>P1000</cell><cell>0.0441</cell><cell cols="2">0.0761 0.0765</cell><cell>0.0570</cell><cell cols="2">0.0717 0.0722</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,218.17,538.17,175.65,10.48"><head>Table 4 :</head><label>4</label><figDesc>Ad hoc task performance</figDesc><table coords="8,138.84,74.35,334.31,142.49"><row><cell>Measure</cell><cell cols="2">Performance Notes</cell></row><row><cell>Mean reciprocal rank</cell><cell>0.101</cell><cell>252 topics</cell></row><row><cell>Mean of median TREC</cell><cell>0.3786</cell><cell>Stdev:</cell></row><row><cell>reciprocal rank</cell><cell></cell><cell>0.4240</cell></row><row><cell cols="2">Proportion of topics with the named 21%</cell><cell></cell></row><row><cell>page in the top 10%</cell><cell></cell><cell></cell></row><row><cell>Proportion of topics with no named</cell><cell>58%</cell><cell></cell></row><row><cell>page found</cell><cell></cell><cell></cell></row><row><cell cols="2">Proportion of topics with a reciprocal 8%</cell><cell></cell></row><row><cell>rank better than the median</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,216.96,230.34,178.08,10.48"><head>Table 5 :</head><label>5</label><figDesc>Named page task, Lucene</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,96.06,102.07,443.93,10.48;10,96.06,116.52,326.32,10.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,277.73,102.07,262.26,10.48;10,96.06,116.52,18.78,10.48">Database merging strategy based on logistic regression</title>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Calvé</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,124.73,116.52,197.79,10.48">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,140.93,443.94,10.48;10,96.06,155.37,432.49,10.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,387.94,140.93,152.07,10.48;10,96.06,155.37,70.84,10.48">Merging results from isolated search engines</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thistlewaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,191.77,155.37,300.80,10.48">Proceedings of the Tenth Australasian Database Conference</title>
		<meeting>the Tenth Australasian Database Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,179.78,443.94,10.82;10,96.06,194.23,26.66,10.48" xml:id="b2">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/index.html" />
		<title level="m" coord="10,96.06,179.78,148.33,10.48">Apache Software Foundation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,218.64,443.94,10.82;10,96.06,233.08,138.90,10.82" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gamiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Newby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nassar</surname></persName>
		</author>
		<ptr target="http://www.gir-wg.org/" />
		<title level="m" coord="10,298.20,218.64,201.46,10.48">Grid information retreival requirements</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,257.49,443.94,10.48;10,96.06,271.94,443.94,10.48;10,96.06,286.38,26.66,10.48" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,151.07,257.49,367.81,10.48">Inferring probability of relevance using the method of logistic regression</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,96.06,271.94,354.70,10.48">Proceedings of the 17th International Conference of the ACM-SIGIR&apos;94</title>
		<meeting>the 17th International Conference of the ACM-SIGIR&apos;94<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,310.79,433.00,10.48" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetić</surname></persName>
		</author>
		<title level="m" coord="10,286.64,310.79,85.76,10.48">Lucene in Action</title>
		<imprint>
			<publisher>Manning Publications</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,359.61,443.94,10.48;10,96.06,374.05,277.30,10.48" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,182.19,359.61,220.94,10.48">Amberfish at the trec 2004 terabyte track</title>
		<author>
			<persName coords=""><forename type="first">Nassib</forename><surname>Nassar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.72,359.61,106.28,10.48;10,96.06,374.05,241.41,10.48">The Thirteenth Text Retrieval Conference Proceedings (TREC 2004)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.06,398.46,443.94,10.48;10,96.06,412.91,443.94,10.48;10,96.06,427.35,26.66,10.48" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,370.03,398.46,169.98,10.48;10,96.06,412.91,168.11,10.48">Report on the trec-5 experiment: Data fusion and collection fusion</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><forename type="middle">Le</forename><surname>Calvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dana</forename><surname>Vrajitoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,290.03,412.91,243.73,10.48">The Fifth Text REtrieval Conference (TREC-5)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,96.07,451.76,443.93,10.48;10,96.06,466.21,307.63,10.48" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,196.92,451.76,337.55,10.48">Siemens trec-4 report: Further experiments with database merging</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.69,466.21,256.20,10.48">The Fourth Text REtreieval Conference (TREC-4)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
