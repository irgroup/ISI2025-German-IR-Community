<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.18,103.05,311.29,16.20">Simple Language Models for Spam Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,273.89,137.08,64.15,11.96"><forename type="first">Egidio</forename><surname>Terra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Informatics PUC/RS</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.18,103.05,311.29,16.20">Simple Language Models for Spam Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">60CAE7B64E7329B4F651D14707DF4B81</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For this year's Spam track we used classifiers based on language models. These models are used to compute the log-likelihood for each individual message and then classify them as either ham or spam. Different data sets were used to train these language models. Our approach is simple, we initially create simple unigram language models and smooth the probabilities of unseen tokens by means of the expected likelihood estimator with a small discount probability tuned in a training corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The statistical approaches for spam filtering are often Bayesian and assume a multivariate Bernoulli distribution for tokens based on the number of messages they appear <ref type="bibr" coords="1,229.41,501.38,14.64,10.91" target="#b8">[9,</ref><ref type="bibr" coords="1,249.01,501.38,8.48,10.91" target="#b5">6,</ref><ref type="bibr" coords="1,262.44,501.38,8.48,10.91" target="#b0">1,</ref><ref type="bibr" coords="1,275.88,501.38,8.48,10.91" target="#b3">4,</ref><ref type="bibr" coords="1,289.31,501.38,7.67,10.91" target="#b7">8]</ref>.</p><p>Each word in each corpus, ham and spam, is assigned a conditional probability for each given class, i.e. P (w|C = ham) and P (w|C = spam).</p><p>For a new message M , the goal is to compute P (C = ham|M ) and P (C = spam|M ), which will then be used to decide to what class the message belongs. Robinson <ref type="bibr" coords="1,264.19,619.94,11.51,10.91" target="#b7">[8]</ref> proposes the use of the χ 2 distribution confidence intervals to decide if the message is spam or ham. In the most common approach -the naive bayes -every word is considered independent from each other, i.e. for a given class C, P (C|M ) = w∈M P (C|w). To convert P (w|C) into the desired P (C|w), the bayes' formula is used: P (C|w) = [P (w|C)P (C)]/P (w).</p><p>Language Models on the other hand normally take a multinomial approach within a single distribution for all tokens, regardless of how many messages they occur. One benefit of the multinomial approach is the number of available discounting and smoothing methods to handle unseen tokens, whereas some Bayesian approaches often handle this situation heuristically (e.g. unseen tokens are assigned a constant probability regardless of other tokens' probabilities).</p><p>Our submissions to this year's TREC are based on language models. While spam classification can be seen as a text categorization with only two classes, it has some distinct characteristics from normal text, among others: a) e-mail massages have some structure; b) spam is written to look like a legit message; c) the arrival order of messages maybe important; d) spam messages mutate to avoid filters. It is our intention to evaluate the language models in this context since it has yielded good results in other text classification tasks, outperforming state-ofart classifiers such as SVMs in many of them <ref type="bibr" coords="1,525.28,635.46,10.91,10.91" target="#b6">[7]</ref>.</p><p>Section 2 gives a brief introduction to the language models used in our submissions. The following section, 3, describes the training data we used for our models and section 4 presents our 1 results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Simple Language Models</head><p>Language models are generative with some order approximation to the language <ref type="bibr" coords="2,228.44,158.85,15.99,10.91" target="#b9">[10]</ref>. We start described the general form of the model:</p><formula xml:id="formula_0" coords="2,89.61,220.90,211.35,21.35">P (M ) = P (w 1 ) i=2..n P (w i |w 1 ..w i -1)<label>(1)</label></formula><p>where w 1 ..w i -1 is called the history. The effect of history is more important in nearby neighbors, i.e. the outcome of the current word is more influenced by recently occurring ones. This is explored in the high-order approximations and it is equivalent to the Markov assumptions, where older history is not taken into account</p><formula xml:id="formula_1" coords="2,105.90,390.60,195.08,21.35">P (M ) = i=1..n P (w i |w i-k+1 ..w i-1 )<label>(2)</label></formula><p>for a k-order approximation.</p><p>A first-order word approximation (unigram model) is simply</p><formula xml:id="formula_2" coords="2,138.07,488.17,162.92,21.35">P (M ) = i=1..n P (w i )<label>(3)</label></formula><p>where |M | = n and P (w i ) are observed from actual text. Equation 3 is also used to described the zero-order approximation. Shannon makes the distinction between zero and first based only on the estimates of P (w i ). In the zero-order, the estimates are sampled from an uniform distribution.</p><p>The model is called generative since we can, by sampling with replacement, generate sequences that will have similar distribution to the model.</p><p>When used in text classification, a model can be built for each class and the most likely class is chosen <ref type="bibr" coords="2,357.31,74.78,10.91,10.91" target="#b6">[7]</ref>. In the case of a dichotomous test, such as the one found in spam filtering, there are other alternatives. Dunning proposes the use of log-likelihood test to validate the hypothesis that the classes are distinguishable <ref type="bibr" coords="2,461.29,142.52,10.91,10.91" target="#b2">[3]</ref>. For that he explored the fact that a log-likelihood is asymptotically χ 2 distributed and as such we can use confidence intervals from the distribution to decide if a message is spam or not.</p><p>Another important aspect in language models is the sparseness problem. Specially in higherorder approximations, the number of potential combinations is high. For a vocabulary V , containing v = |V | distinct words, a k-order approximation will have v k combinations. Even for very large corpora the number of possible combinations is greater than then number of seen ones, and even for modest values of k because the vocabulary tend to be large. For this problem several approaches exists <ref type="bibr" coords="2,451.53,396.57,11.50,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,466.16,396.57,7.67,10.91" target="#b4">5]</ref>, the so-called smoothing and discounting techniques. The basic idea is to reserve some probability mass for the unseen events in the training data. For simplicity, we have used the expected likelihood estimate:</p><formula xml:id="formula_3" coords="2,375.24,507.74,160.09,25.78">P ele (w i ) = f (w i ) + λ N + Bλ (<label>4</label></formula><formula xml:id="formula_4" coords="2,535.33,515.12,4.64,10.91">)</formula><p>where N is the size of the corpus (tokens) and B is the number of unique words (types). The frequency of a word is given by f (w i ). The value of λ can be trained and if its value is set to 1 then it is equivalent to the Laplace's law <ref type="bibr" coords="2,512.30,604.66,10.90,10.91" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>For each class, ham and spam, we created dis- </p><p>where P s (w) is the probability of generating the word w according to the spam language model and P h (w) the analogous for the ham language model.</p><p>We use a train-on-everything approach, for all incoming messages we update the corresponding language model. In the first run, labeled pucrs0, the token frequencies from the incoming messages are accumulated to those originated from the training data set, in this case the spamassassin corpus. The same approach was used in the second run, labeled pucrs1, but, instead of the spamassassin corpus, we used messages from a spam archive and for ham messages we used documents extracted from the AQUAINT corpus.</p><p>In the third run, pucrs2, the token frequen-   </p><formula xml:id="formula_6" coords="3,350.61,622.95,189.36,92.97">(M ) + (1 -λ s )P sb (M ) λ h P h (M ) + (1 -λ h )P hb (M ) = w∈M log[λ s P s (M ) + (1 -λ s )P sb (M )] -log[λ h P h (M ) + (1 -λ h )P hb (M )]<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>For our run pucrs0, the results are shown in the table 1 for the several corpora used. Table <ref type="table" coords="4,534.32,501.38,5.44,10.91">2</ref> presents the results for pucrs1 and table <ref type="table" coords="4,510.17,518.32,5.44,10.91" target="#tab_1">3</ref>  We have tried Robinson's chi-square Bayesian, adopted in many popular filters such as Bogofilter and Spamassassing, in our framework and the results are not better than the language models. This suggests that the high misclassification rates may be also due to the preprocessing of the messages. Our tokenization process is too simple, we use standard stopword list and all headers are ignored with the exception of the to:, subject: and from: entries. No special handling is made for attachments and multipart messages, all data is considered to be text.</p><p>All words with less than three characters is discarded. Some regular patterns are used to split URL's and e-mail addresses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,104.54,233.70,163.77,10.91"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: ROC for the Full corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,347.51,233.70,155.80,10.91"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: ROC for the sb corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,519.80,518.32,19.97,10.91;4,310.98,535.26,228.81,10.91;4,310.98,552.19,228.79,10.91;4,310.98,569.12,228.85,10.91;4,310.98,586.06,228.81,10.91;4,310.98,603.00,228.79,10.91;4,310.98,619.94,228.80,10.91;4,310.98,636.88,228.78,10.91;4,310.98,653.80,228.81,10.91;4,310.98,670.74,228.81,10.91;4,310.98,687.68,228.79,10.91;4,310.98,704.62,228.81,10.91;5,72.00,74.78,228.84,10.91;5,72.00,91.71,228.79,10.91;5,72.00,108.65,228.78,10.91;5,72.00,125.59,228.81,10.91;5,72.00,142.52,228.78,10.91;5,72.00,159.46,228.78,10.91;5,72.00,176.40,187.28,10.91;5,82.91,200.04,217.90,10.91;5,72.00,216.98,228.80,10.91;5,72.00,233.92,228.83,10.91;5,72.00,250.85,228.82,10.91;5,72.00,267.79,22.44,10.91;5,82.91,291.44,217.85,10.91;5,72.00,308.37,228.81,10.91;5,72.00,325.30,228.82,10.91;5,72.00,342.24,228.78,10.91;5,72.00,359.18,228.80,10.91;5,72.00,376.12,228.83,10.91;5,72.00,393.06,228.82,10.91;5,72.00,409.99,228.80,10.91;5,72.00,426.92,228.81,10.91;5,72.00,443.86,112.75,10.91"><head></head><label></label><figDesc>contain the results for pucrs2. The overall misclassification rates are high and on two corpora, sb and tm, the spam misclassification is very high. These two corpora have number of spam messages small compared to the total number of messages: only around 11% of messages on sb and tm corpora is spam. In these two corpora the classification thresholds are biased to ham scores, i.e., the threshold to classify a message as spam is high. The two other corpora are more balanced, only 18% of messages in mrx corpus are ham and 42% of messages in the full corpus is ham. This poor performance indicates that the current tweaking of the threshold is not robust for corpus with unbalanced number of messages on the two classes. The area under the ROC is a more robust measure to determine the usefulness of the scores assigned by the classifiers. The learning rates are also influenced by the active threshold tuning. Unfortunately, setting the threshold is a quick change but running the filters on the private data is not yet possible if ever. The results for pucrs2 are worse than the other two runs, as shown in the ROC graphs in Figures 1,2,3, and 4. With the exception of the full corpus, pucrs2 performs statistically worse the pucrs0 and pucrs1 in terms of area under the ROC. Only in the full corpus the performance is similar. This suggests that either the mixture with a background model is ineffective or that the initial weight and decaying rate need to be trained for each corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.00,76.08,467.94,639.45"><head>Table 3 :</head><label>3</label><figDesc>Results for run pucrs2</figDesc><table coords="3,72.00,687.68,228.81,27.85"><row><cell>cies from the incoming messages are kept sep-</cell></row><row><cell>arate from those originated from the training</cell></row></table><note coords="3,310.98,359.65,228.81,10.91;3,310.98,376.59,228.84,10.91;3,310.98,393.52,54.27,10.91;3,365.29,397.95,4.88,5.61;3,370.67,393.52,98.34,10.91;3,469.09,397.81,3.91,5.61;3,473.51,393.52,66.42,10.91;3,310.98,410.46,35.02,10.91;3,346.03,414.89,8.50,5.61;3,355.04,410.46,125.86,10.91;3,481.01,414.89,7.54,5.61;3,489.06,410.46,50.87,10.91;3,310.98,427.40,228.80,10.91;3,310.98,444.34,228.85,10.91;3,310.98,461.27,228.76,10.91;3,310.98,478.21,228.78,10.91;3,310.98,495.14,228.80,10.91;3,310.98,512.08,152.12,10.91;3,463.21,516.37,3.91,5.61;3,473.03,512.08,66.91,10.91;3,310.98,529.02,93.28,10.91;3,404.34,533.44,4.88,5.61;3,412.74,529.02,127.13,10.91"><p>data, once again the spamassassin corpus. As result, four distinct models are kept: incoming ham (P h ), incoming spam (P s ), background ham (P hb ) and background spam (P sb ). The two pairs of language models are then linearly interpolated, as depicted in equation 6. The mixture parameters are not fixed and, after a certain number of messages are collected, the weights assigned to background probabilities are reduced linearly. The initial weights λ s for spam language models and λ h for the ham language mod-</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the <rs type="institution">University of Waterloo</rs> for letting us use their computer resources.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,333.39,188.19,206.43,10.91;5,333.40,205.13,206.43,10.91;5,333.40,222.06,206.41,10.91;5,333.40,238.99,206.43,10.91;5,333.40,255.93,206.41,10.91;5,333.40,272.87,206.39,10.91;5,333.40,289.81,203.71,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,384.86,222.06,154.95,10.91;5,333.40,238.99,88.39,10.91">An evaluation of naive bayesian anti-spam filtering</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutsias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">V</forename><surname>Chandrinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Spyropoulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,451.32,238.99,88.51,10.91;5,333.40,255.93,206.41,10.91;5,333.40,272.87,206.39,10.91;5,333.40,289.81,170.32,10.91">Proceedings of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning (ECML 2000)</title>
		<meeting>the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning (ECML 2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,316.60,206.45,10.91;5,333.40,333.54,206.40,10.91;5,333.40,350.48,206.40,10.91;5,333.40,367.41,62.15,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,476.37,316.60,63.48,10.91;5,333.40,333.54,206.40,10.91;5,333.40,350.48,41.18,10.91">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Harvard University</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="5,333.39,394.20,206.40,10.91;5,333.40,411.14,206.41,10.91;5,333.40,428.08,165.86,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,411.71,394.20,128.08,10.91;5,333.40,411.14,171.07,10.91">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,513.62,411.14,26.19,10.91;5,333.40,428.08,101.43,10.91">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,454.87,74.88,10.91;5,436.09,454.87,103.76,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,436.09,454.87,98.18,10.91">A plan for spam</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Graham</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,498.60,206.44,10.91;5,333.40,515.54,206.39,10.91;5,333.40,532.48,187.56,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<title level="m" coord="5,378.36,515.54,161.44,10.91;5,333.40,532.48,95.95,10.91">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,559.27,206.40,10.91;5,333.40,576.21,206.38,10.91;5,333.40,593.14,206.45,10.91;5,333.40,610.07,203.08,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,491.36,559.27,48.43,10.91;5,333.40,576.21,206.38,10.91;5,333.40,593.14,21.82,10.91">Spamcopa spam classification and organization program</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,376.67,593.14,163.18,10.91;5,333.40,610.07,169.89,10.91">Proceedings of AAAI-98 Workshop on Learning for Text Categorization</title>
		<meeting>AAAI-98 Workshop on Learning for Text Categorization</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.39,636.88,206.42,10.91;5,333.40,653.80,206.43,10.91;5,333.40,670.74,206.41,10.91;5,333.40,687.68,206.43,10.91;5,333.40,704.62,206.41,10.91;6,94.43,74.78,206.43,10.91;6,94.43,91.71,202.19,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,397.63,653.80,142.20,10.91;5,333.40,670.74,206.41,10.91;5,333.40,687.68,63.05,10.91">Language and task independent text categorization with simple language models</title>
		<author>
			<persName coords=""><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,424.98,687.68,114.85,10.91;5,333.40,704.62,206.41,10.91;6,94.43,74.78,137.14,10.91">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter</title>
		<title level="s" coord="6,248.39,74.78,52.47,10.91;6,94.43,91.71,169.24,10.91">the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.42,117.62,206.38,10.91;6,94.43,134.55,206.40,10.91;6,94.43,151.49,24.81,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,179.06,117.62,121.74,10.91;6,94.43,134.55,86.85,10.91">A statistical approach to the spam problem</title>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,195.78,134.55,66.83,10.91">Linux Journal</title>
		<imprint>
			<biblScope unit="issue">107</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.42,177.39,206.44,10.91;6,94.43,194.33,206.40,10.91;6,94.43,211.27,206.43,10.91;6,94.43,228.20,206.42,10.91;6,94.43,245.14,142.07,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,247.74,194.33,53.09,10.91;6,94.43,211.27,157.85,10.91">A bayesian approach to filtering junk e-mail</title>
		<author>
			<persName coords=""><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,279.95,211.27,20.91,10.91;6,94.43,228.20,206.42,10.91;6,94.43,245.14,108.88,10.91">Proceedings of AAAI-98 Workshop on Learning for Text Categorization</title>
		<meeting>AAAI-98 Workshop on Learning for Text Categorization</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,94.41,271.04,206.42,10.91;6,94.43,287.98,206.40,10.91;6,94.43,304.91,206.36,10.91;6,94.43,321.85,24.81,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,199.72,271.04,101.10,10.91;6,94.43,287.98,101.24,10.91">A mathematical theory of communication</title>
		<author>
			<persName coords=""><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,205.67,287.98,95.16,10.91;6,94.43,304.91,51.79,10.91">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948-10">July and October 1948</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
