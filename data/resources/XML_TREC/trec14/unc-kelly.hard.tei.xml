<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.56,74.70,338.83,9.88">University of North Carolina&apos;s HARD Track Experiment at TREC 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,253.08,99.96,53.46,9.88"><forename type="first">Diane</forename><surname>Kelly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Carolina 100 Manning Hall</addrLine>
									<postCode>CB#3360, 27599-3360</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.02,99.96,30.86,9.88"><forename type="first">Xin</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Library Science</orgName>
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Carolina 100 Manning Hall</addrLine>
									<postCode>CB#3360, 27599-3360</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.56,74.70,338.83,9.88">University of North Carolina&apos;s HARD Track Experiment at TREC 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2681C9AB8E884382BC6272A2839D5A85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this year's HARD Track, we focused on two aspects related to the elicitation of relevance feedback: the display of document surrogates and features for identifying and selecting terms. We looked at these issues with respect to interactive query expansion (IQE). In typical interactive query expansion scenarios, users mark documents that they find relevant and the system automatically extracts terms from these documents and adds them to users' queries, or suggests potential query terms from these documents and allows users to determine which of these terms are added to their queries. While a large number of studies have been conducted on IQE, results of such studies do not convey a consistent picture of IQE use and effectiveness.</p><p>Empirical, laboratory-based studies have led to the general finding that users of experimental interactive IR systems desire IQE features (c.f., <ref type="bibr" coords="1,380.92,340.26,72.08,9.88" target="#b2">Beaulieu, 1997;</ref><ref type="bibr" coords="1,458.17,340.26,63.81,9.88;1,90.00,352.92,23.68,9.88" target="#b4">Belkin, et al., 2001)</ref>. However, much of the evidence from these studies indicates that relevance feedback features are rarely used and when they are used, they are unlikely to result in retrieval improvements. For instance, some studies have found that users do not select many terms <ref type="bibr" coords="1,90.00,390.84,73.93,9.88" target="#b2">(Beaulieu, 1997;</ref><ref type="bibr" coords="1,167.30,390.84,87.15,9.88" target="#b4">Belkin, et al., 2001)</ref>, while other studies have found that users select terms, but that these terms do not necessarily improve performance <ref type="bibr" coords="1,344.49,403.50,60.08,9.88" target="#b1">(Anick, 2003)</ref>. This has been attributed to problems related to the design of relevance feedback interfaces <ref type="bibr" coords="1,419.21,416.16,73.84,9.88">(Ruthven, 2003)</ref>, task complexity and the user's lack of additional cognitive resources <ref type="bibr" coords="1,386.40,428.82,93.30,9.88" target="#b4">(Belkin, et al., 2001)</ref>, and the amount of extra time required to use such features. Users in a series of studies by <ref type="bibr" coords="1,464.49,441.48,57.55,9.88;1,90.00,454.08,29.31,9.88" target="#b4">Belkin, et al. (2001)</ref> rarely used relevance feedback features and often expressed confusion over suggested terms. In a study of simulated interactive query expansion, <ref type="bibr" coords="1,367.26,466.74,70.66,9.88">Ruthven (2003)</ref> demonstrated that users are less likely than systems to select effective terms for query expansion. <ref type="bibr" coords="1,451.69,479.40,70.36,9.88">Ruthven (2003)</ref> demonstrated some potential benefit of term relevance feedback when the best terms were used in query expansion, but went on to note that users are unlikely to select these terms because of problems with current relevance feedback interfaces. In a Web-based study, <ref type="bibr" coords="1,433.96,517.38,59.36,9.88" target="#b1">Anick (2003)</ref> found that users made use of a term suggestion feature to expand and refine their queries. However, this did not result in improvements in retrieval performance, which suggests that terms users selected were not particularly good. Conversely, in another study of an operational retrieval system, <ref type="bibr" coords="1,90.00,567.96,83.39,9.88" target="#b5">Efthimiadis (2000)</ref> found that users selected about one-third of terms suggested by the system and that, in general, these terms improved retrieval performance. <ref type="bibr" coords="1,369.45,580.62,68.36,9.88" target="#b6">Harman (1988)</ref> also demonstrated that IQE led to retrieval improvements.</p><p>One problem with current relevance feedback interfaces is that terms are often presented in isolation, which might make it difficult for users to fully comprehend relationships between terms and their information needs. Without appropriate term context, it can be difficult for users to understand how terms are used, why terms are suggested, and how such terms might be used to improve retrieval. One purpose of the current study is to investigate if an interface that provides term context helps users make better query expansion decisions. Previous research does not provide a clear idea about how term context will affect user behavior and retrieval. Will users select more terms or fewer terms if term context is provided? Does term context enable users to make better decisions about term selection? In other words, does term context enable users to be more discriminant when selecting terms? Consequently, will selected terms improve or worsen retrieval performance? We hypothesize that users will select more terms when they are presented in context than when they are presented in isolation (H1) and that these additional terms will improve retrieval performance (H2).</p><p>In this study, we are also interested in investigating users' abilities to suggest terms to add to their queries given appropriate stimulation. It has been suggested in the literature that only through interaction with texts can users come to understand and learn about their information needs <ref type="bibr" coords="2,120.77,163.20,65.16,9.88" target="#b3">(Belkin, 1993)</ref>. Furthermore, in our previous work, we found that with appropriate probing, users could articulate additional information about their information needs beyond what they articulated in their initial queries <ref type="bibr" coords="2,262.55,188.46,119.26,9.88" target="#b8">(Kelly, Dollu, &amp; Fu, 2005)</ref>. We propose that interactions with text surrogates can stimulate users' thinking about their information needs and that this stimulation can help users identify additional terms to add to their queries. Specifically, we hypothesize that users will identify more terms using an interface that presents sentence-level document surrogates and elicits free-form text input than an interface that presents these same surrogates with check boxes (H3). We anticipate that sentences will provide users with ideas about terms for query expansion in both a direct fashion (i.e., terms contained within sentences) and an indirect fashion (i.e., via interaction and stimulation, where users think of additional terms not contained within sentences). We further hypothesize that terms suggested by users via the former interface will result in better retrieval performance than those selected via the latter interface (H4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Clarification Forms</head><p>We submitted three clarifications forms (CFs), each demonstrating a different method of displaying document surrogates and eliciting query expansion terms. Table <ref type="table" coords="2,441.02,378.24,5.49,9.88" target="#tab_0">1</ref> summarizes the three methods. The first form displayed a list of twenty terms; users were asked to mark checkboxes next to terms they wanted to add to their queries. The second form displayed a list of the same twenty terms, plus sentences in which these terms appeared; users were asked to mark check-boxes next to terms they wanted to add to their queries. Terms were emphasized in bold within their corresponding sentences. The final form displayed the same sentences from Form 2, but with a text box for input. Users were asked to enter terms they wanted to add to their queries. Users were further instructed that terms could be from sentences or their own terms. Screen shots of the interfaces are displayed below in Figures <ref type="figure" coords="2,420.37,586.62,4.35,9.88">1</ref><ref type="figure" coords="2,424.72,586.62,4.35,9.88">2</ref><ref type="figure" coords="2,429.07,586.62,4.35,9.88">3</ref>. The comparison between Form 1 and Form 2 allowed us to explore hypotheses 1 and 2, while the comparison between Form 1 and Form 3 allowed us to test hypotheses 3 and 4. We designed our forms to look slightly different from one another with respect to style (e.g., font style, background color). Since users would complete all three forms for each of their topics, we hoped to minimize the possibility that users would recognize them as a set and react accordingly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Retrieval System</head><p>We used the Lemur IR toolkit (http://www.lemurproject.org) to conduct our experiments, with its basic defaults for indexing (BuildIndex function), and Okapi BM25 for retrieval. Although we made use of a basic stop word and acronym list, we did not use a stemmer. Our baseline run consisted of the title and description for each topic. Our experimental runs consisted of adding selected or suggested terms to baseline queries.</p><p>To populate our clarification forms, we modified Lemur's basic feature (Reteval) so that for each topic, terms identified by the system to use for pseudo relevance feedback were printed to a file, along with document identification numbers from which these terms were extracted. We set the pseudo relevance feedback parameter to use the top twenty ranking terms from the top ten ranking documents. The technique used for selecting terms is based on Robertson Selection Value (RSV) and described more fully in <ref type="bibr" coords="4,250.97,226.44,246.80,9.88" target="#b9">Robertson, Walker, Jones, &amp; Hancock-Beaulieu (1995)</ref>; this technique is included as part of the Lemur toolkit.</p><p>To identify sentences, we constructed one word queries consisting of terms extracted during pseudo relevance feedback. For each topic, we collected all documents from which terms originated into a directory, parsed documents into sentences so that each sentence was in a unique file, indexed the files, and used the one word queries and corresponding sentence level documents for retrieval. We used the top result for each query to populate Form 2 and Form 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>In this section, we first present the responses that we received from each of the three clarification forms and the statistical tests of H1 and H3. This is followed by a presentation and comparison of retrieval results for each technique to test H2 and H4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Clarification form responses</head><p>Table <ref type="table" coords="4,155.42,416.16,5.49,9.88" target="#tab_1">2</ref> lists the mean number of terms that users marked as relevant on Form 1 and Form 2 as well as the mean number of terms that they entered on Form 3. The figure for Form 3 is calculated based on terms that the retrieval system actually used in the experimental run, after breaking hyphen connected terms into two (e.g., "family-planning" to "family planning") and removing stop words. When raw data is considered, Form 3 elicited an average of 11.12 terms with a standard deviation of 8.559. Overall, Form 3 elicited the most terms from users and Form 2 the least. Paired sample t-tests revealed significant differences between Form 1 and Form 2 [t(49)=3.404, p&lt;0.05] and between Form 2 and Form 3 [t(49)=-2.255, p&lt;0.05], but not between Form 1 and Form 3 [t(49)=-0.535, p=0.595]. Table 2 also lists the mean number of seconds users spent marking or identifying terms with each type of form. Recall that users were limited to 180 seconds (3 minutes) per form. Users spent the least amount of time on Form 1, which is no surprise given that this form contained terms and checkboxes only. The frequency distribution for time reveals that most users (n=48) spent less than 78 seconds completing Form 1. Users spent an average of 141.92 seconds on Form 2, which contained sentences and checkboxes. It is likely that many users did not have time to complete this form. The frequency distribution for time for Form 2 indicates that 13 users stopped at 180 seconds and 13 users stopped at 181, 182 and 183 seconds. Assuming that users were stopped automatically at the end of 180 seconds, it is likely that these 26 users did not evaluate the entire form. This may explain why users identified the fewest terms with Form 2. Finally, users spent an average of 148.50 seconds completing Form 3 which presented sentences and a free-form textbox. Although on average, users spent the most time completing this form, fewer users were stopped by the time limit when completing this form than when completing Form 2. The distribution shows that the time for 12 users was greater than or equal to 180 seconds. It is interesting to note that the maximum time for Form 3 (267 seconds) greatly exceeded that for Form 2 (183 seconds). We are unsure if this represents a processing delay or some other problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Form 1 vs. Form 2 (H1)</head><p>The significant difference between Form 1 and Form 2 does not provide support for hypothesis H1 since it is in the opposite direction than what we hypothesized. In a topic level analysis of the number of selected terms, we noted that users selected equal number of terms from the two forms in nine cases, more terms from Form 1 in 30 cases and more terms from Form 2 in 11 cases. We are mindful that time may have impacted these results.</p><p>After a further examination of the selected terms, we noted that of the 50 topics only five received identical term judgments from both Form 1 and 2. For 16 topics, the selected term set from Form 2 was a subset of the set from Form 1, for 5 topics the selected term set from Form 1 was a subset of Form 2, and for 24 topics there was little overlap between terms. Interestingly, for one of these topics the user marked six terms as relevant on Form 1 and two terms as relevant on Form 2, but these were all different terms. In five cases, users did not mark any terms, once when responding to Form 1 and four times when responding to Form 2. Surprisingly, for these four cases, users selected 2, 4, 11 and 14 relevant terms from the corresponding Form 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Form 2 vs. Form 3 (H3)</head><p>The difference between Forms 2 and 3 supports the hypothesis that the free-form input box on Form 3 would elicit significantly more terms from users than the check boxes on Form 2. A topic level analysis indicated that users entered at least one term on Form 3 in all 50 cases, and for 32 topics, users entered an equal or greater number of terms on Form 3 than they selected from Form 2. Hypothesis H3 is supported by this data. Again, we caution the reader that time may have contributed to these results.</p><p>A comparison of results on a term-by-term basis leads to interesting findings. In no case were terms identical across the two forms for any topic and for six cases, terms selected or entered on Forms 2 and 3 by the same user were exclusive; that is, there was no overlap in these sets of terms. We further examined sources of terms entered on Form 3. In this analysis, all entered terms are considered without applying the stop word list. The average number of terms elicited by Form 3 was 11.12 (std=8.56) terms; 3.26 (std=3.10) of these terms were identified by users when using From 2 while 7.86 (std=6.75) were new. If duplicate terms are removed, the mean drops slightly to 7.38 (std=6.07). Thus, on average, Form 3 elicited seven additional terms from users. Among the 7.86 new terms, 1.22 (std=2.14) were suggested terms from Form 2, 4.56 (std=5.12) were contained within displayed sentences, and 2.08 (std=3.63) were user-generated.</p><p>Interestingly, when considering all terms identified with Form 3, approximately 9 terms came from displayed sentences and only 2.08 were user-generated. Of these 9 terms, about 4.48 were terms the system suggested via term suggestion and used to populate Forms 1 and 2, and 4.56 were terms that the system had access to (i.e., terms contained within the suggested sentences), but did not suggest.</p><p>Users entered at least one new term for almost half (23) of the topics. For five topics, terms on Form 3 were all user-generated. Often times, additional terms were synonyms or antonyms of terms in sentences, or subordinates of some general term that was displayed. For example, in topic 354, "Journalist Risks," the user entered several terms corresponding to specific forms of risk ("harassed," "detained," "killed," "arrested," "injured," "beaten," etc.). Some of these terms were mentioned in the "description" field of the topic ["Identify instances where a journalist has been put at risk (e.g., killed, arrested or taken hostage) in the performance of his work."].</p><p>In general, these results demonstrate the overall success of using sentences as stimulators and providing free-form text input for users to identify additional terms to add to their queries. At the same time, however, we wish to point out that the effectiveness of this interaction technique may vary between users. There were significant individual differences among the six users in this study with respect to the number of user-generated terms. Both ANOVAs investigating user differences with respect to the number of user-generated terms and the ratio of user-generated terms to the total number of terms led to significant results [F(5, 44)=3.36, p&lt;0.05; F(5, 44)=7.68, p&lt;0.01]. In particular, we observed that User D entered a significantly larger number (71.7%) of user-generated terms than any other user (the second highest is only 30.5%). On the contrary, User E only entered one user-generated term for one topic on which he or she worked and none for his or her other topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retrieval results</head><p>Table <ref type="table" coords="6,156.06,314.94,5.49,9.88" target="#tab_2">3</ref> shows the R-precision, mean average precision (MAP), and precision at ten scores (with standard deviations) for our baseline and the three experimental runs. final_1, final_2 and final_3 runs consist of baseline queries plus query expansion using terms obtained from Forms 1, 2, and 3, respectively. We include one pseudo relevance feedback run as an additional baseline. This run is equivalent to adding all terms from Form 1 to baseline queries. Since R-precision is the official measure of the Track, we used this measure to determine if significant differences existed between runs. The paired sample t-tests indicated significant improvements in all three final runs over the baseline [final_1: t(49)=4.784, final_2: t(49)=3.350, final_3: t(49)=3.289, all p&lt;0.05]. Table <ref type="table" coords="6,269.06,548.88,5.49,9.88" target="#tab_3">4</ref> shows the comparison of our final runs to our baseline run at the topic level. All three techniques improved the average precision for around 66% of the topics. These results provide some evidence for the benefit of the relevance feedback techniques used in this study. Recall that in H2 and H4, we hypothesized that providing context for term selection and providing document surrogates and a free-form term input interface would not only result in the elicitation of more terms, but that these additional terms would lead to improved retrieval results. The R-precision scores reported in Table <ref type="table" coords="7,278.26,125.22,5.49,9.88" target="#tab_2">3</ref> do not support H2. Even though the R-precision of final_1 was higher than that of final_2, the paired sample t-test led to a non-significant result <ref type="bibr" coords="7,90.00,150.54,102.29,9.88">[t(49)=1.338, p=0.187]</ref>. The R-precision of final_3 was higher than final_2, but again, no statistically significant difference in means was found [t(49)=-0.529, p=0.599].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross site comparison</head><p>In this subsection, we briefly report a comparison between our results and results of other sites. As Table <ref type="table" coords="7,159.36,213.78,5.49,9.88" target="#tab_5">5</ref> shows, our baseline run falls below the median baseline for most topics, but all three of our experimental runs position above the median for over half of the topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this project, we focused on two aspects related to the elicitation of relevance feedback: the display of document surrogates and features for identifying and selecting terms. We compared three forms for eliciting relevance feedback. The first form displayed a list of twenty terms; users were asked to mark check-boxes next to terms they wanted to add to their queries. The second form displayed a list of the same twenty terms, plus sentences in which these terms appeared; users were asked to mark check-boxes next to terms they wanted to add to their queries. The final form displayed the same sentences from Form 2, but with a text box for input.</p><p>We hypothesized that users would select more terms when they were presented in context (Form 2) than when they were presented in isolation (Form 1) and that these additional terms would improve retrieval performance (H1 and H2, respectively). Statistical tests did not support either hypothesis. In fact, users identified significantly fewer terms with Form 2 than with Form 1, which was contrary to our expectations. It might have been the case that term context allowed users to be more selective and discriminating, which was why fewer terms on average were identified with Form 2. However, the R-precision score for Form 1 was higher than the Rprecision score for Form 2 (although not significantly so), which suggests that the quality of terms identified on Form 1 was not necessarily poor. Thus, a more likely explanation for this is that users simply did not have enough time to complete Form 2. Given more time, users may have selected more terms from Form 2. Our analysis of terms selected by each user with Form 1 and Form 2 indicated no consistent pattern across forms with respect to term selection.</p><p>We also hypothesized that users would identify more terms using an interface that presented sentence-level document surrogates and elicited free-form text input (Form 3) than an interface that presented these same surrogates with check boxes (Form 2) (H3). We further hypothesized that terms suggested by users via the former interface would result in better retrieval performance than those selected via the latter interface (H4). Results demonstrated that users identified significantly more terms with Form 3 than with Form 2, which supported H3. In fact, users identified the most terms with Form 3 despite it being a bit more labor-intensive. However, as with the differences between Forms 1 and 2, the differences between Forms 2 and 3 may be attributed to time, or lack thereof.</p><p>When considering all terms identified with Form 3, approximately 9 terms came from displayed sentences and only 2.08 were user-generated. Of these 9 terms, about 4.48 were system suggested terms from Forms 1 and 2, and 4.56 were terms that the system had access to (i.e., terms contained within the suggested sentences), but did not suggest. These results demonstrate that using sentences as stimulators and providing free-form input leads users to identify significantly more terms than when they are presented with suggested terms, sentences and check-boxes. Performance results were in the general direction of H4, that is, the R-precision score for Form 3 was higher than the R-precision score for Form 2, but this difference was not statistically significant.</p><p>Finally, we found significant differences for many measures according to user. This is not surprising given the experimental setup of the study. Specifically, six users were responsible for assessing fifty topics and responding to a large number of clarification forms, so individual user differences are likely to be present in the data. We are currently conducting a betweensubjects follow-up study with approximately 60 users to further test our three experimental forms and research hypotheses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,239.58,251.28,124.61,9.88;3,170.16,72.00,271.74,176.64"><head>Figure</head><label></label><figDesc>Figure 1. Clarification Form</figDesc><graphic coords="3,170.16,72.00,271.74,176.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,177.84,492.06,230.35,76.55"><head>Table 1 .</head><label>1</label><figDesc>Clarification form design</figDesc><table coords="2,177.84,507.42,230.35,61.19"><row><cell>Form</cell><cell>Display</cell><cell>Elicitation</cell></row><row><cell>1</cell><cell>Terms</cell><cell>Check-boxes</cell></row><row><cell>2</cell><cell>Terms and sentences</cell><cell>Check-boxes</cell></row><row><cell>3</cell><cell>Sentences</cell><cell>Text box</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,92.64,542.64,426.70,84.29"><head>Table 2 .</head><label>2</label><figDesc>Means and standard deviations for the number of terms users selected or entered on CFs and the amount of time in seconds spent accomplishing this</figDesc><table coords="4,179.22,570.00,269.95,56.93"><row><cell></cell><cell>Terms</cell><cell>Time</cell></row><row><cell>Form 1</cell><cell>9.94 (3.835)</cell><cell>45.38 (31.07)</cell></row><row><cell>Form 2</cell><cell>8.06 (4.723)</cell><cell>141.92 (50.57)</cell></row><row><cell>Form 3</cell><cell>10.48 (7.762)</cell><cell>148.50 (47.25)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,97.80,390.84,416.32,102.76"><head>Table 3 .</head><label>3</label><figDesc>Means and standard deviations for R-precision, MAP and Precision at 10 for each run</figDesc><table coords="6,99.78,405.54,407.36,88.06"><row><cell></cell><cell>R-precision</cell><cell>MAP</cell><cell>Precision @ 10</cell></row><row><cell>Baseline</cell><cell>0.218 (0.160)</cell><cell>0.160 (0.162)</cell><cell>0.346 (0.294)</cell></row><row><cell>Pseudo Relevance</cell><cell>0.264 (0.204)</cell><cell>0.224 (0.216)</cell><cell>0.422 (0.361)</cell></row><row><cell>Final_1</cell><cell>0.283 (0.192)</cell><cell>0.235 (0.203)</cell><cell>0.436 (0.361)</cell></row><row><cell>Final_2</cell><cell>0.268 (0.194)</cell><cell>0.219 (0.209)</cell><cell>0.426 (0.343)</cell></row><row><cell>Final_3</cell><cell>0.279 (0.199)</cell><cell>0.228 (0.205)</cell><cell>0.468 (0.353)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,97.68,612.12,416.65,84.28"><head>Table 4 .</head><label>4</label><figDesc>Performance of final runs versus baseline run according to number of topics that were improved, worsened or stayed the same</figDesc><table coords="6,178.44,639.48,253.49,56.92"><row><cell></cell><cell>Better</cell><cell>Same</cell><cell>Worse</cell></row><row><cell>Final_1</cell><cell>36</cell><cell>3</cell><cell>11</cell></row><row><cell>Final_2</cell><cell>29</cell><cell>7</cell><cell>14</cell></row><row><cell>Final_3</cell><cell>29</cell><cell>6</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,107.61,74.70,263.98,9.88"><head>1 Form 1 vs. Form 2 (H2) and Form 2 vs. Form 3 (H4)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,102.24,251.70,402.54,87.16"><head>Table 5 .</head><label>5</label><figDesc>UNC's topic performance versus other sites</figDesc><table coords="7,102.24,266.40,402.54,72.46"><row><cell></cell><cell>Best</cell><cell></cell><cell>Median</cell><cell></cell><cell>Worst</cell></row><row><cell>Baseline</cell><cell>0</cell><cell>17</cell><cell>4</cell><cell>28</cell><cell>1</cell></row><row><cell>Final1</cell><cell>1</cell><cell>26</cell><cell>4</cell><cell>16</cell><cell>3</cell></row><row><cell>Final2</cell><cell>2</cell><cell>25</cell><cell>3</cell><cell>18</cell><cell>2</cell></row><row><cell>Final3</cell><cell>2</cell><cell>25</cell><cell>2</cell><cell>18</cell><cell>3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,94.13,315.00,65.13,9.88" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,340.26,371.18,9.88;8,90.00,352.92,428.04,9.88;8,90.00,365.58,318.35,9.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,210.40,340.26,286.78,9.88;8,90.00,352.92,49.96,9.88">Using terminological feedback for web search refinement: A log based study</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,161.82,352.92,356.22,9.88;8,90.00,365.58,221.27,9.88">Proceedings of the 26th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;03)</title>
		<meeting>the 26th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;03)<address><addrLine>Toronto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,378.24,396.03,9.88;8,90.00,390.84,125.53,9.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,225.97,378.24,238.54,9.88">Experiments on interfaces to support query expansion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,475.91,378.24,46.12,9.88;8,90.00,390.84,65.58,9.88">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,403.50,388.03,9.88;8,90.00,416.16,393.91,9.88;8,90.00,428.82,163.78,9.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,218.90,403.50,295.13,9.88;8,90.00,416.16,36.68,9.88">Interaction with texts: Information retrieval as information-seeking behavior</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,148.98,416.16,281.96,9.88">Information retrieval &apos;93. Von der Modellierung zur Anwendung</title>
		<meeting><address><addrLine>Konstanz</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
		<respStmt>
			<orgName>Universitaetsverlag Konstanz</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,441.48,396.01,9.88;8,90.00,454.08,432.05,9.88;8,90.00,466.74,398.67,9.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,133.92,454.08,388.13,9.88;8,90.00,466.74,139.31,9.88">Iterative exploration, design and evaluation of support for query reformulation in interactive information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Perez-Carballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,238.74,466.74,172.98,9.88">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="404" to="434" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,479.40,395.98,9.88;8,90.00,492.06,432.01,9.88;8,90.00,504.72,136.59,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,243.63,479.40,278.35,9.88;8,90.00,492.06,144.23,9.88">Interactive query expansion: A user-based evaluation in a relevance feedback environment</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,247.92,492.06,274.09,9.88;8,90.00,504.72,49.20,9.88">Journal of the American Society for Information Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,517.38,395.97,9.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,223.00,517.38,164.57,9.88">Towards interactive query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,413.52,517.38,108.45,9.88">Proceedings of the 11th</title>
		<meeting>the 11th</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,530.04,432.01,9.88;8,90.00,542.64,143.64,9.88" xml:id="b7">
	<monogr>
		<title level="m" coord="8,90.00,530.04,432.01,9.88;8,90.00,542.64,50.79,9.88">Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;88)</title>
		<meeting><address><addrLine>Grenoble</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="321" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,555.30,395.93,9.88;8,90.00,567.96,432.05,9.88;8,90.00,580.62,431.93,9.88;8,90.00,593.22,39.46,9.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,312.85,555.30,209.09,9.88;8,90.00,567.96,168.08,9.88">The loquacious user: A document-independent source of terms for query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J</forename><surname>Dollu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,282.12,567.96,239.93,9.88;8,90.00,580.62,350.87,9.88">Proceedings of the 28th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;05)</title>
		<meeting>the 28th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;05)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,126.00,605.88,381.65,9.88;8,90.00,618.54,393.10,9.88;8,90.00,631.20,271.02,9.88;8,126.00,643.86,395.89,9.88;8,90.00,656.52,431.99,9.88;8,90.00,669.11,329.38,9.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,90.00,618.54,74.67,9.88;8,230.60,643.86,291.29,9.88;8,90.00,656.52,42.65,9.88">Re-examining the potential effectiveness of interactive query expansion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,267.12,618.54,215.98,9.88;8,90.00,631.20,48.53,9.88;8,157.32,656.52,364.67,9.88;8,90.00,669.11,221.27,9.88">Proceedings of the 26th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;03)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the 26th Annual ACM International Conference on Research and Development in Information Retrieval (SIGIR &apos;03)<address><addrLine>Washington, D.C.; Toronto, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Government Printing Office. Ruthven, I</publisher>
			<date type="published" when="1995">1995. 2003</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
	<note>TREC-3, Proceedings of the Third Text Retrieval Conference</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
