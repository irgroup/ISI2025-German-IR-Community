<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.08,97.52,363.40,19.82">Insun05QA on QA track of TREC2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.02,147.71,73.66,12.58"><forename type="first">Yuming</forename><surname>Zhao</surname></persName>
							<email>ymzhao@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.63,147.71,66.77,12.58"><forename type="first">Zhiming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.16,147.71,44.06,12.58"><forename type="first">Yi</forename><surname>Guan</surname></persName>
							<email>guanyi@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.48,147.71,44.03,12.58"><forename type="first">Peng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.08,97.52,363.40,19.82">Insun05QA on QA track of TREC2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">87182DF2DA0BE7EFA611544C6C6FBED5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first time that our group takes part in the QA track. At TREC2005, the system we developed, Insun05QA, participated in the Main Task, which submitted answers to three types of questions: factoid questions, list questions and others questions. And we also submitted the document ranking which our answers are generated from.</p><p>A new sentence similarity calculating method is used in our Insun05QA system. It can be considered as an extension of vector space model. And our QA system incorporates several useful tools. These tools include WordNet, developed by Princeton University, Minipar by Dekang Lin, and GATE, developed by University of Sheffield. Moreover, external knowledge such as knowledge from Internet is also widely used in our system.</p><p>Since it is the first time that we take part in QA track and the preparing time is limited, we concentrate on the processing of factoid questions. And the methods we developed to process list and others questions are generated from the method used to process factoid questions.</p><p>The structure of our Insun05QA system will be describe in Section 2, the details of how we process the factoid, list and others questions will be introduced in Section 3, and our results in TREC2005 will be presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture of Insun05QA system</head><p>The architecture of Insun05QA system can be described by Figure .1. It is composed of 6 parts: Preprocess, including questions preprocess and documents preprocess, question analysis, document retrieval, sentence similarity calculation, Web retrieval and answer extraction.</p><p>In our QA system, GATE is used to help us accomplish name entity reorganization in preprocess of questions and documents. Minipar developed by Dekang Lin, is adopted to help making question analysis and it is also used to parse the relevant passage in answer extraction module. We also develop our document retrieval module based on SMART, an information retrieval system, developed by Cornell University in the 1960s.</p><p>In Insun05QA system, we obtain knowledge from the ontology in WordNet which help to classify the answer type and the information from Internet that contribute to answer extractions. And we also use the synsets of WordNet to accomplish question extension and sentence similarity calculation.</p><p>The three kinds of questions, factoid, list and others, almost have the same processing flow, and differences are made inside the modules on the flow towards the three different kinds of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure.1. Flow Chart of Insun05QA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The preprocessing module is developed to accomplish the preprocess task for both documents and questions. The details of process steps are different for documents and questions. The flows of preprocessing for documents and questions are described separately by Figure .2 and Figure <ref type="figure" coords="2,99.59,685.16,4.50,10.80">3</ref>.</p><p>According to documents, the first process step is sentence boundary detect. In this step, we According to questions, there's no need of sentence boundary detect. The first thing we do is to classify the questions according to the type tag of each question in QA2005 test set, and we write the factoid, list and others questions into three files separately. And then, the following process steps, tokenization, part of speech, name entity recognization, and stemming, are applied, just like their being applied on documents. Things should be mentioned here is that when we rewrite the others questions into their own file, we generate the questions in a uniform pattern: "What are the other matters about" + the target.</p><p>The co-reference resolution for questions is different from documents. We accomplish co-reference resolution and rewrite the questions with the help of target information. For the question that target is about person, organization, or something like that, a simple algorithm is used to replace the pronoun and some short form with the question's target. But for the question that target is about events, a co-reference resolution algorithm that is relevantly complicated is applied. These methods make the questions holding enough information so that they will be instructive in the following steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Analysis</head><p>The question analysis module has two functions: keyword generation and answer type prediction. We realize these functions based on two external tools: WordNet and Minipar.</p><p>We use Minipar to analyze the sentence structures of questions, and constituent information are extracted from questions based on this analysis. By using the synsets of WordNet, we build a dictionary of synonymous words, and with the help of this dictionary, we explore the constituents extracted from questions so as to accomplish keyword generation.</p><p>In the answer type prediction step, we adopt a rule-based algorithm to classify the answer type of input questions.</p><p>By studying the TREC questions of former years, we define a classification system which containing fifteen class answer types. These answer types are: person, location, organization, quality, date, ratio, money, ill, address, web address, job, measurement, color, film, and other.</p><p>Based on this fifteen-class answer type classification system, using questions of TREC 8-12 as training set, we get a library of rules by machine learning. These rules describe the relations between answer types of questions and their interrogatives and structures. In the process of learning these rules, constituents information of question sentences obtained in parsing above are used, and the ontology knowledge of WordNet is also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Documents Retrieval</head><p>In document retrieval module, relevant documents according to each question are retrieved from the TREC corpus. We build this module based on SMART, a retrieval system developed by Cornell University.</p><p>To carry out document retrieval, the index and inverse file of the TREC corpus to be dealt with are built at first. Since the file pointer of SMART is 32 bits, the size of the index and inverse file shouldn't exceed 2 GB. And then, for the same reason, documents that SMART could deal with in one time should not exceed 600MB or 800MB. The corpus of TREC2005 is about 3GB, so it's reasonable that we divide the TREC documents into 5 parts. We build index and inverse file for these 5 parts of documents separately, and relevant documents for every TREC2005 question are retrieved from these 5 parts of documents one by one. Then, we obtained the retrieval result by consolidating and re-ranking these 5 parts of retrieval results for every question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence Similarity Calculation</head><p>The sentence similarity calculation module is developed to extract the relevant passages of a question from the corresponding relevant documents obtained in the document retrieval module.</p><p>The core component of sentence similarity calculation module is the system similarity model (SSM), which can be considered as an extension of vector space model (VSM), and we use SSM to calculate the degree of similarity between the question and the sentence in relevant documents. The system similarity theory is regarded as theoretical foundation of SSM. According to the system similarity theory, if there are some shared properties or features, two systems demonstrate some degree of similarity, and we call such two systems similar systems. These shared properties or features are called similar properties.</p><p>In SSM, Given a system µ ≤ ≤ respectively. The system similarity degree is , then: ( , )</p><formula xml:id="formula_0" coords="5,102.66,220.58,257.02,80.66">Q A B 2 1 2 2 2 1 1 1 ( , ) p i i i p m n i i i i i ip x Q A B 2 i x x y µ µ = = = = + = + ∑ ∑ ∑ ∑<label>(1)</label></formula><p>In our sentence similarity calculation module, system denotes a sentence and elements such as 1 2 , , ..., m α α α denote the words in a sentence. In Formula (1), i</p><p>x and means the weights of according words in sentences. They are obtained by the method of variations of standard TF-IDF term weighting scheme. And the element similarity degree i j y µ means the similarity degree between two similar words. We use information content to evaluate semantic similarity between words with the help of WorldNet, and then we obtained a list of words similarity degree which supports us carrying out Formula (1).</p><p>With SSM, we pick out sentences that are more similar to questions. The reason that we take out this step is that we consider that the answer always appears in or near the sentence that is much more similar to the question. But it is evident that the sentence containing answer isn't the most similar sentence to the question, for the answer won't appear in the question and there must be some un-similar elements in the answer sentence. And it is just these answer words that make the similarity degree between question and answer sentence decreased. It is contrary to what we expect, so we improve the SSM to make it adapt the requirement of our system.</p><p>We import a parameter λ to modify Formula (1), the improved is:</p><formula xml:id="formula_1" coords="5,108.66,538.55,335.99,83.02">( , ) Q A B ∑ ∑ ∑ ∑ ∑ ∑ + + = + + = = = = = + + + = n q p i i q p p i i p i i i m i i q i i p i i i y y x x y x B A Q 1 2 1 2 2 1 2 2 1 2 1 2 1 2 ) , ( λ µ λ µ (2)</formula><p>With the information of answer type obtained in answer analysis module, we pick out the words of potential answer according to their name entity tags. We use λ ( 0 ≥ λ ) to simulate the element similarity degree of these potential answer words, and thus make the words of potential answer contribute to the similarity degree between questions and answers. And using the former TREC QA data as training set, we get the proper λ by genetic algorithms.</p><p>We use this improved SSM calculating the similarity degree between questions and all the sentences in according retrieved documents. For each question, we rank the candidate sentences depending on their similarity degree. To avoid losing useful information, we expand each candidate sentence into a candidate passage which includes the sentence just before it and the sentence just after it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Web Retrieval and Answer Extraction</head><p>Based on the results of former steps, the answer extraction module returns the exact answers and according documents numbers with the help of Web retrieval.</p><p>We begin our answer extraction with inspecting the candidate passages which is full of information including POS and NE tags. If the named entity in the candidate passages corresponds to an expected answer type, the entity will be picked out as a candidate answer. The selection of the best candidate answer, when multiple candidate answers exist, is performed with an answer ranking scheme that relies on heuristics method.</p><p>There is a wide, possibly infinite range of text features that can be designed to estimate the relevance of a candidate answer for the purpose of answer ranking. Besides using statistical features such as term frequency, proximity and relative position to the question key words, our methods also include syntactic information derived through parsing, and semantic features like word senses, POS tagging and keyword expansion etc. The following are some associated heuristic rules and relative features:</p><p>•Associated heuristic: if the candidate answer occurs in a passage with a large number of keywords or keywords expansion, then the candidate answer is more likely to be relevant. •Associated heuristic: if the candidate answer occurs in the passage with a large number of different keywords or different keywords expansion, then the candidate answer is more likely to be relevant. •Associated heuristic: if the candidate answer is the subject or object of a sentence, then it is more likely to be relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SO F = (1, if candidate answer is subject or object)</head><p>•Associated heuristic: if the candidate answer is an apposition of a phrase containing many keywords, then it is more likely to be relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APP F = (1, if candidate answer is apposition)</head><p>•Associated heuristic: if the candidate answer is closer to the keywords or keywords expansion, then the answer is more likely to be relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[distance (</head><p>, candidate-answer)] •Associated heuristic: if the candidate answer matches with answer extract pattern, it is more likely to be relevant. The answer extract patterns are built by studying the former TREC QA data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PAT F = (1, if answer extract pattern is matched with)</head><p>Our empirical answer ranking formula gives a score to every candidate answers by linear combination and weight adjusting. Eventually, a ranking of candidates according to the score is obtained.</p><p>To fetch up the information scarcity of documents, we develop the Web retrieval module to use the knowledge from Internet. Based on Google API, we developed a Web crawling tool, WebRept, which can download relevant web pages automatically. And based on some rules, the exact answer and document number are selected from the ranking of candidates, with the help of information returned by WebRept.</p><p>The answer extract method described above is used to deal with the factoid questions. For the list questions, the answer extract method is much similar. The difference is that for list questions, we return a set of answers selected from the ranking of candidates instead of only one answer. For the others questions, the answer extract method is completely different. Based on the ranking of candidate passages obtained in sentence similarity calculation, we get a new ranking of relevant documents. We use our InsunAbs system, an automatic document abstract generating system, on the relevant documents. After wiping off the information having appeared in the answers of the former questions which having the same target, the set of abstracts in the front of the ranking are returned as the answer of the other question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We have submitted only one run, Insun2005QA1, for the main task of TREC2005. The evaluation result is described in Table <ref type="table" coords="8,250.53,76.58,4.50,10.80" target="#tab_2">1</ref>.  <ref type="table" coords="8,155.98,324.98,4.50,10.80" target="#tab_2">1</ref>, we can see that the methods we use to deal with the list questions and others questions are too simple and arbitrary. Further research should be done to improve the performance of our system on dealing with the list and others questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.42,688.29,149.98,9.45;3,301.20,688.29,149.98,9.45;3,343.20,703.77,54.62,9.45;3,114.42,703.77,60.33,9.45"><head>Figure. 2 . 3 .</head><label>23</label><figDesc>Figure.2. Flow Chart of PreprocessFigure.3. Flow Chart of Preprocess for questions for documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,111.66,454.98,9.25,6.25;6,105.06,448.40,7.33,10.80;6,129.90,448.40,377.08,10.80;6,111.66,486.18,13.45,6.25;6,105.06,479.60,7.33,10.80;6,127.92,479.60,418.50,10.80;6,138.90,504.26,37.13,10.80"><head>KNF=</head><label></label><figDesc>Number ({the number of keywords / passage that candidate answer occurs}) KEN F = Number ({the number of keywords expansion / passage that candidate answer occur})</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,115.86,580.98,8.48,6.25;6,109.26,574.40,393.94,10.80;6,115.86,612.18,12.68,6.25;6,109.26,605.60,7.33,10.80;6,133.20,605.60,413.21,10.80;6,137.10,630.26,41.80,10.80"><head>KTF=</head><label></label><figDesc>Number ({number of def-keywords/ passage that candidate answer occurs}) KET F =Number ({number of def-keywords expansion/ passage that candidate answer occurs})</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,93.12,194.48,439.07,317.55"><head></head><label></label><figDesc>boundary and write the text of documents in the form of one sentence a line. And after that, tokenization, part of speech, name entity recognization, stemming, and co-reference resolution, are applied orderly.</figDesc><table coords="2,93.12,194.48,439.07,317.55"><row><cell>detect the sentence</cell><cell></cell><cell></cell></row><row><cell>Questions</cell><cell>Question-Preprocess</cell><cell></cell><cell>Question Analysis</cell></row><row><cell></cell><cell>GATE</cell><cell>Minipar</cell><cell>WordNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SMART</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Queries</cell></row><row><cell>TREC Corpus</cell><cell>Doc-Preprocess</cell><cell></cell><cell>Doc Retrieval</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Search result</cell><cell>Answer</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(full text)</cell><cell>type</cell></row><row><cell>Internet</cell><cell></cell><cell></cell><cell>Relevant Sentence similarity Calculation Answer type</cell></row><row><cell></cell><cell></cell><cell></cell><cell>passage</cell><cell>Key words</cell></row><row><cell></cell><cell cols="2">Web Retrieval</cell><cell>Answer Extraction</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Answers and Doc numbers</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,95.22,96.30,364.19,239.48"><head>Table 1 .</head><label>1</label><figDesc>Performance of Insun2005QA in TREC2005</figDesc><table coords="8,385.44,116.48,73.97,10.80"><row><cell>Insun2005QA1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,83.21,430.16,463.15,10.80;8,83.22,446.18,91.67,10.80" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Wang</forename><surname>Guan-Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang-Qiang</forename><surname>Xiao-Long</surname></persName>
		</author>
		<title level="m" coord="8,311.96,430.16,228.64,10.80">Measurement of System Similarity. JSCL-2005</title>
		<meeting><address><addrLine>Nanjing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">Aug.2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,82.41,462.20,464.05,10.80;8,83.22,478.16,173.92,10.80" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,132.67,462.20,413.79,10.80;8,83.22,478.16,57.34,10.80">Automatic Text Processing: The transformation, analysis, and retrieval of information by computer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,83.52,494.18,462.85,10.80;8,83.22,510.20,153.02,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,220.27,494.18,276.48,10.80">Question answering supported by information extraction</title>
		<author>
			<persName coords=""><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,505.67,494.18,40.69,10.80;8,83.22,510.20,56.56,10.80">TREC-8 Proceedings</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,85.23,526.16,461.20,10.80;8,83.22,542.18,206.36,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,173.84,526.16,264.76,10.80">GATE, a General Architecture for Text Engineering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,449.73,526.16,96.70,10.80;8,83.22,542.18,53.01,10.80">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="223" to="254" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,82.76,558.20,463.60,10.80;8,83.22,574.16,75.30,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,129.66,558.20,336.28,10.80">A Dependency-based Method for Evaluating Broad-Coverage Parsers</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,474.19,558.20,72.18,10.80;8,83.22,574.16,42.94,10.80">Proceedings of IJCAI-95</title>
		<meeting>IJCAI-95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,83.94,590.18,462.29,10.80;8,83.22,606.20,355.68,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,288.18,590.18,258.04,10.80;8,83.22,606.20,37.45,10.80">Using Query Zoning and Correlation with SMART: TREC-5</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitra</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<idno>NIST. 1996</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.89,606.20,231.94,10.80">Proceeding of the 5th Text REtrieval Conference</title>
		<meeting>eeding of the 5th Text REtrieval Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
