<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,206.63,148.91,172.96,15.11;1,151.02,170.83,284.17,15.11">QuALiM at TREC 2005: Web-Question Answering with FrameNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,252.72,203.31,80.77,10.48"><forename type="first">Michael</forename><surname>Kaisser</surname></persName>
							<email>m.kaisser@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University / DFKI GmbH</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,206.63,148.91,172.96,15.11;1,151.02,170.83,284.17,15.11">QuALiM at TREC 2005: Web-Question Answering with FrameNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7C68ADA6C5140FEFCAAD29951D2E66C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper I describe my TREC 2005 participation. The system used was-except from one new module-the same as in TREC 2004. In the following I will describe this new module, which uses the annotated Natural Language data collected in the FrameNet project in order to find paraphrases to answer questions. I will furthermore present and discuss the TREC 2005 results and compare them to those achieved in TREC 2004.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the second time I participated in TREC. The 2005 system is mainly based on the 2004 system as described in <ref type="bibr" coords="1,218.91,400.39,29.54,8.74" target="#b1">[Kai04]</ref>.</p><p>In a nutshell, the TREC 2004 system used two different answer strategies:</p><p>• The pattern based rephrasing algorithm.</p><p>• The Google fallback mechanism based on key words and key phases.</p><p>The rephrasing algorithm accesses patterns that reformulate questions to potential answer sentences. Such a pattern can be seen in figure 1. It accepts for example the question "When was Amtrak founded?" and reformulates it to "Amtrak was founded in AN-SWER(NP)" and "In ANSWER(NP) Amtrak was founded". Google is searched for sentences containing the known parts of potential answer sentence, and if proper results are found, the answer is extracted.</p><p>The fallback mechanism creates rather simple queries based on key words and key phrases from the question. In the snippets returned from the search engine n-grams are mined and the most frequent one is returned as the answer.</p><p>The plan for TREC 2005 was to concentrate on the first algorithm. In 2004, a drawback with it was that the patterns it used to find answers had to be created manually, which of course is a time consuming matter. I thought about different methods to gain more patterns in a automated way. One idea was to use data from lexical resources like FrameNet <ref type="bibr" coords="1,460.98,625.60,30.56,8.74" target="#b0">[FL98]</ref> to create more patterns similar to those used in TREC 2004.</p><p>Unfortunately-because of other commitments-there was not much time to work on this idea. In late July, when the TREC evaluation took place, a first version of the FrameNet algorithm was implemented. It was able to some return some answers, but only for a small fraction of the test questions. As mentioned the 2004 modules were reused in 2005. Because of that and the fact that the FrameNet algorithm caught only for a few questions, the 2005  system did not differ that much from the 2004 system.</p><p>In the following I will give a few brief remarks about FrameNet in general. I will then explain how I used FrameNet to answer questions. Because the algorithm was not finished for TREC 2005 these remarks will be rather short. (Once I am finished with this work, it will be described in more detail elsewhere.) At the end of this paper I will report on the TREC evaluation results QuALiM received in 2005 and compare them to those of 2004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FrameNet</head><p>FrameNet is a lexical database resource based on frame semantics and supported by corpus evidence. It documents the range of semantic and syntactic combinatory possibilities (valences) of target words (so-called lexical units). In order to do this, it contains human-annotated sentences (currently more than 135,000), which exemplify the use of more than 6,100 lexical units organized into 625 semantic frames. As FrameNet is still in ongoing development, not all lexical units contain annotated sentences yet.</p><p>In FrameNet, words with similar semantics receive descriptions with identical role labels. The lexical units for "invent" and "design" for example describe the relation between the roles (frame elements in FrameNet's terminology) Cognizer and Invention. Similarly, the frames for the verbs "buy" and "sell" both list the frame elements Buyer and Seller. The annotated sentences show that the position of these frame elements differ in the the syntactic realizations of the different verbs.</p><p>The purpose of FrameNet is to create a sample selection of how Natural Language works. Many applications using FrameNet try to use the information the annotated sentences provide and apply it to sentences from other sources. Thus, FrameNet can help in getting closer to understanding Natural Language sentences or even texts. In the 2005 version of QuALiM I used FrameNet to 1. understand questions; 2. create a set of exact search engine queries;</p><p>3. analyze the sentences in the snippets returned by the search engine in order to find exact answers to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Answering Questions with FrameNet</head><p>In the following, I will give a short explanation of how the system answers the question "When was the telegraph invented?" First, the incoming questions is parsed using MiniPar <ref type="bibr" coords="3,343.10,417.85,31.91,8.74" target="#b2">[Lin98]</ref>, and the resulting dependency tree simplified to the following structure: head: invented(V) subj: Who whn: Who obj: the telegraph head indicates that the head of the question is the verb invented, subj indicates that the deep subject is who (which whn marks as also being a question word) and obj indicates that the deep object is the telegraph.</p><p>This provides enough information to look up the head verb in the FrameNet dictionary, where two lexical units for invent.v can be found. Parts of the sentences are annotated with frame elements. The system parses all such annotated sentences with MiniPar to find out which semantic roles are assigned to which syntactic roles. It shows that usually the Cognizer role is realized as an NP at subject position, while Invention is an NP at object position.</p><p>As mentioned earlier, the analysis of the question shows that, in a potential answer sentence, the answer should be in subject relation to the verb "invent". Furthermore "the telegraph" needs to be in object relation to the verb. From this it can be concluded that the filler for the Invention frame element is "the telegraph", and that the question asks for a Cognizer.</p><p>The system can now give a pseudo-semantic formula for the question invent_272(Cognizer=X, Invention="the telegraph")</p><p>and replace the frame elements Cognizer and Invention in each annotated sentence with their values from the question. For the above sentence the outcome would be:</p><p>ANSWER(NP) in the USA had invented the telegraph, in the late 1930s...</p><p>The PPs "in the USA" and "in the late 1930s" are recognized as additional information, most likely specific to the topic of the initial annotated sentence, but not transferable to the new domain, so they are-at least in the current version of the system-simply removed:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANSWER(NP) had invented the telegraph</head><p>What we have so far can straightforward be translated into a pattern as used in the TREC 2004 system (see figure <ref type="figure" coords="4,230.47,344.70,3.88,8.74" target="#fig_1">2</ref>). &lt;pattern&gt; &lt;sequence&gt; &lt;word id="1"&gt;Who&lt;/word&gt; &lt;word id="2"&gt;invented&lt;/word&gt; &lt;parse id="3"&gt;NP&lt;/parse&gt; &lt;final&gt;?&lt;/final&gt; &lt;/sequence&gt; &lt;target name="FN_target1"&gt; &lt;answer&gt;NP&lt;/answer&gt; &lt;word&gt;had&lt;/word&gt; &lt;ref&gt;2&lt;/ref&gt; &lt;ref&gt;3&lt;/ref&gt; &lt;/target&gt; &lt;/pattern&gt; From here the strategy is the same as in TREC 2004: 1. The system generates Google queries from the patterns, in this case:</p><p>"had invented the telegraph".</p><p>2. It extract sentences from the Google snippets.</p><p>3. It parse these sentences and checks whether they have the required syntactic structure.</p><p>4. If a sentence has the correct syntax, the potential answer can be extracted, because the system knows from the FrameNet data where in that sentence the answer is located. For example in "By 1832 Samuel FB Morse had invented the telegraph." it must be the NP preceding "had", thus: "Samuel FB Morse".</p><p>For the given example, the system was able to find the correct, exact answer and the open proposition shown above can now be completed: invent_272(Cognizer="Samuel FB Morse", Invention="the telegraph")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* * *</head><p>As mentioned, at the time of TREC 2005, only an alpha version of the described algorithm existed. It was able to answer 35 of the 362 factoid questions asked, 25 of them correct. The main problem was that the mapping from parts of the question to a) a lexical unit and b) frame elements failed in most cases. Whenever this happened the question was simply not processed by the FrameNet module.</p><p>To sum up, in 2005 the following modules were used: The threshold to answer a question with NIL for factoid questions, the number of answers returned when answering a list question, the length of the answers in characters for other questions.)</p><p>Table <ref type="table" coords="5,136.92,518.01,4.98,8.74" target="#tab_2">1</ref> shows the results obtained in TREC 2005 compared to those of 2004. The 2005 results for all type of questions are worse than the 2004 results. This is somewhat surprising as the system was roughly the same as in TREC 2004 and evaluations show that the new FrameNet module improves the system performance rather than deteriorating it.</p><p>One reason for this is that the questions in 2005 seemed harder to answer than those of 2004 and that the assessors were more strict. The median accuracy scores for factoid questions in 2005 were lower than those in 2004 (0.170 compared to 0.152) and the best participant achieved worse results as well (0.770 compared to 0.713, see <ref type="bibr" coords="5,402.07,601.69,31.02,8.74" target="#b4">[Vor05]</ref> and <ref type="bibr" coords="5,454.31,601.69,27.92,8.74" target="#b3">[VD06]</ref>). Note that the scores dropped, although it should be the case that the systems participating in 2005 are better than those in 2004.</p><p>Another reason is that QuALiM could not deal with the event series introduced in 2005. QuALiM needs a complete question to start with. In 2004, where one could rely on the fact that all targets would be NPs, this meant to modify the series' target and insert into the question at the appropriate position. This was not the case in 2005. Targets like "Russian submarine Kursk sinks" or "Plane clips cable wires in Italian resort" are obviously not NPs. Thus, the question construction mechanism failed. The next table shows only the results for those question series that had events as targets.</p><p>The table to the right shows the results for all non-event series. As can be seen, QuALiM performs significantly better for the non-event series.</p><p>The slightly worse results for other questions can be explained by the fact that the targets in 2005 were more complicated than those in 2004. The bad 2005 results for list questions are due to a bug in the module designed to answer them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,120.86,512.02,344.49,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example pattern as used in the 2004 version of the QuALiM system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,169.26,549.02,247.69,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pattern generated by the FrameNet algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,94.68,555.75,396.86,62.93"><head>1</head><label></label><figDesc>One of the entries contains annotated sentences including the following:</figDesc><table coords="3,101.74,597.99,382.74,20.69"><row><cell>Du Pont</cell><cell>in the USA had INVENTED</cell><cell>nylon</cell><cell>in the late 1930s ...</cell></row><row><cell>FE:Cognizer</cell><cell cols="2">lexical unit FE:Invention</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,94.68,281.18,396.86,195.28"><head>Table 1 :</head><label>1</label><figDesc>TREC 2005 results, compared to the 2004 results. The runs differed in parameter settings:</figDesc><table coords="5,94.68,281.18,355.85,161.46"><row><cell cols="3">1. The 2004 rephrasing algorithm</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2. The 2004 fallback mechanism</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">3. The new FrameNet algorithm</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4 TREC Evaluation Results</cell><cell></cell><cell></cell></row><row><cell cols="3">TREC 2005 run 1 run 2</cell><cell cols="3">TREC 2004 run 1 run 2 run 3</cell></row><row><cell>Factoid</cell><cell>0.207</cell><cell>0.235</cell><cell>Factoid</cell><cell cols="2">0.343 0.339 0.343</cell></row><row><cell>List</cell><cell>0.029</cell><cell>0.032</cell><cell>List</cell><cell>0.096</cell><cell>0.111 0.125</cell></row><row><cell>Other</cell><cell cols="2">0.147 0.123</cell><cell>Other</cell><cell>0.145</cell><cell>0.181 0.211</cell></row><row><cell>Combined</cell><cell>0.150</cell><cell>0.158</cell><cell>Combined</cell><cell>0.232</cell><cell>0.242 0.256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,325.34,697.33,166.18,8.74"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="5,362.69,697.33,128.83,8.74"><row><cell>shows QuALiMs 2005 results</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,94.68,182.10,396.86,40.56"><head>Table 2 :</head><label>2</label><figDesc>TREC results with respect to different types of the series' target.(run 2) for different types of series. The leftmost table shows the results for all series.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,109.92,690.81,275.06,6.99"><p>The system does not perform any form of word-sense disambiguation, yet.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,130.68,340.42,360.85,8.74;6,130.68,352.38,208.16,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,385.21,340.42,106.32,8.74;6,130.68,352.38,29.33,8.74">The Berkeley FrameNet project</title>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">F</forename><surname>Baker Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,181.06,352.38,125.69,8.74">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,130.68,372.31,360.84,8.74;6,130.68,384.26,360.85,8.74;6,130.68,396.22,81.00,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,207.58,372.31,283.95,8.74;6,130.68,384.26,35.40,8.74">Question Answering by Searching Large Corpora with Linguistic Methods</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kaisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,187.01,384.26,299.88,8.74">The Proceedings of the 2004 Edition of the Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,130.68,416.14,360.84,8.74;6,130.68,428.10,137.76,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,188.37,416.14,175.58,8.74">Dependency-based evaluation of minipar</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,385.80,416.14,105.73,8.74;6,130.68,428.10,107.07,8.74">Workshop on the Evaluation of Parsing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,130.68,448.02,360.85,8.74;6,130.68,459.98,360.85,8.74;6,130.68,471.93,135.61,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,320.65,448.02,170.89,8.74;6,130.68,459.98,72.73,8.74">Overview of the TREC 2005 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Vorheese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,229.16,459.98,262.38,8.74;6,130.68,471.93,46.41,8.74">The Proceedings of the 2005 Edition of the Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,130.68,491.86,360.85,8.74;6,130.68,503.81,360.85,8.74;6,130.68,515.77,22.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,221.97,491.86,248.43,8.74">Overview of the TREC 2004 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Vorheese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,130.68,503.81,298.53,8.74">The Proceedings of the 2004 Edition of the Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
