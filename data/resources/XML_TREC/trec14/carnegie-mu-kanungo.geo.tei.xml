<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.44,108.00,303.02,16.49;1,135.72,126.48,340.40,16.49">Thresholding Strategies for Text Classifiers: TREC-2005 Biomedical Triage Task Experiments</title>
				<funder ref="#_ySpAHWp">
					<orgName type="full">ARDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.96,157.80,39.42,14.50"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technology Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,315.96,157.80,95.33,14.50"><forename type="first">Tapas</forename><surname>Kanungo</surname></persName>
							<email>kanungo@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="department">IBM Almaden Research Center</orgName>
								<address>
									<addrLine>650 Harry Road</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.44,108.00,303.02,16.49;1,135.72,126.48,340.40,16.49">Thresholding Strategies for Text Classifiers: TREC-2005 Biomedical Triage Task Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A62FC9B914B316A49223F85BB03DD7A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the triage task of biomedical documents in the TREC genomic track. In this paper we describe the methods we developed for the four triage 1 subtasks. Logistic regression and support vector machine algorithms were first trained to generate ranked lists of test documents. Then a subset of the test documents was identified as positive instances by selecting the top-k documents of the ranked lists. Deciding on the ideal value for k requires a good thresholding strategy. In this paper we first describe two thresholding strategies based on i) logistic regression and ii) support vector machines. In addition to these methods, we describe a thresholding method that combines the outputs from logistic regression and support vector machine by applying a joint thresholding strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Task Description</head><p>The goal of Mouse Genome Informatics project <ref type="bibr" coords="1,343.92,472.85,14.07,11.48" target="#b1">[2]</ref> is to provide structured, coded annotation of different topics from biological literature. Human curators spend a large amount of effort on documents of specific topics to generate annotated information. To reduce the amount of effort put in by human curators, the triage process can be utilized to identify relevant documents for specific topics and thus limit the number of documents sent to human curators for detailed analysis. Four triage subtasks were proposed for the 2005 TREC genomic track: find documents that contain information about i) alleles of mutant phenotypes, ii) embryologic gene expression, iii) gene ontology annotation and iv) tumor biology.</p><p>Papers from three journals were used as training and test data for the triage task <ref type="bibr" coords="1,490.80,611.09,12.87,11.48" target="#b1">[2]</ref>. In particular, 5,837 papers published in 2002 and their corresponding ground truth labels (binary variables that indicate whether specific documents are relevant and should be sent for detailed annotations) for the four topics were used for training data; 6,043 papers published in 2003 were used as test data. As the training data and test data were sampled from different publication years, the proportion of relevant documents within training and test data were different.</p><p>The evaluation metric used for triage task was the normalized utility measure, which combines the utility/loss of retrieving a relevant document and retrieving a nonrelevant document, and is defined as <ref type="bibr" coords="2,226.56,265.13,13.12,11.48" target="#b1">[2]</ref>: where u nr is the relative utility/loss of retrieving a nonrelevant document and it is set to -1 for all subtasks; u r is the relative utility of retrieving a relevant document; TP (true positive) denotes the number of retrieved relevant documents; FP (false positive) denotes the number of retrieved nonrelevant documents respectively; and AP (all positive) denotes the total number of relevant documents. The four subtasks have different AP for training and test data and have been associated with different u r as shown in Table <ref type="table" coords="2,486.36,406.85,4.50,11.48">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Algorithm Description</head><p>The triage task can be seen as a text categorization problem. Text categorization algorithms first extract useful features from text data. Then statistical models are built from training data and associated ground truth labels. Finally test documents are classified as relevant or not using the estimated models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>Feature extraction is the first step of building any text categorization system. The provided TREC data includes full text descriptions of each document. As the crosswalk files specified the PubMed ID for each document, we used the PubMed 2 search engine to acquire the MEDLINE records for all the documents. One piece of valuable information within MEDLINE records is the human annotated Medical Subject Headings (MeSH) categories. MeSH ontology is organized into a tree structure with 15 top level categories such as A (anatomy), B (organisms) etc, while each of them is in turn divided into many subcategories. The information within MeSH ontology has been shown to be very helpful for biomedical triage task in TREC 2004 <ref type="bibr" coords="2,289.92,652.13,12.87,11.48" target="#b1">[2]</ref>. In summary, we used the following features in this work: Infections" and its ancestor C04 for "Neoplasms").</p><p>The above features were extracted from the text data in the full text descriptions of the articles in XML format, and MEDLINE records in MEDLINE format. The XML and MEDLINE tags were deleted. Next, text preprocessing was done to remove stopwords, and stemming and case-folding was applied to reduce the number of terms. Finally, the utility BuildIndex within Lemur 3 information retrieval toolkit was used to build an index of terms using the extracted features.</p><p>It is more convenient to represent data as vectors of numeric feature values for building statistical learning models. As TF.IDF (terms frequency times inverse term frequency) has been demonstrated to be effective for other text categorization and information retrieval tasks, it was used to represent the features in this work. Specifically, the weight of each feature is represented as:</p><p>where tf represents the feature frequency within the document, N, which is 5,837, is the number of documents within training set, and idf is the number of training documents that contain the feature in consideration. After the weights have been calculated for the features, they were normalized to make the vector has Euclidean norm as 1.0. This form of TF.IDF representation was also used in previous research <ref type="bibr" coords="3,381.84,490.73,12.87,11.48" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Statistical Learning Methods</head><p>There has been considerable previous research on the application of statistical learning methods to text categorization tasks. In this work, we applied two state-of-the-art methods ---logistic regression and support vector machine ---to the TREC triage task.</p><p>Logistic regression method uses an exponential model to estimate the probability that a document belongs to a specific topic as follows <ref type="bibr" coords="3,320.64,595.49,13.03,11.48" target="#b8">[9]</ref>:</p><p>where {f ij } is the feature representation of ith document; { j ` is the set of model </p><formula xml:id="formula_0" coords="3,199.44,625.74,260.91,41.23">) f exp( 1 ) f exp( ) d , | 1 P(y j ij j j ij j i i + = = Î² (3)</formula><p>parameters; and y i =1 indicates that the ith document is relevant to the topic.</p><p>The model parameters are estimated by maximizing the log-likelihood of the posterior probability of the model parameters. Specifically, a Laplace distribution is used to model the prior distribution of model parameters and the training optimization problem is: where V is the parameter to adjust the weight of the Laplace prior distribution, which is set by cross validation on the training data.</p><p>Support vector machine (SVM) is another statistical learning method for text categorization <ref type="bibr" coords="4,163.32,255.89,13.08,11.48" target="#b2">[3,</ref><ref type="bibr" coords="4,176.40,255.89,8.72,11.48" target="#b7">8]</ref>. The basic idea is to recognize positive and negative data points accurately while maximizing the margin between the two sets of data points. An SVM with a linear kernel can be expressed as a solution to an optimization problem as follows: where b , w * are the SVM model parameters; C controls the trade off between classification accuracy and margin. The output for a test data point is calculated as: Kernelized SVM uses transforms to map feature vectors to a vector space of higher dimension and classifies data points with hyperplanes in the higher dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Thresholding Strategies</head><p>After estimating optimal model parameters of statistical learning models on training data, these models were used to generate output scores of test documents. Furthermore, a subset of the test documents was identified as relevant documents and the other documents were discarded. The last step should be conducted to maximize the utility measure defined in Equation <ref type="formula" coords="4,235.80,584.22,4.49,11.46">1</ref>, which requires a thresholding strategy that can select a subset of documents from the output results of statistical learning methods for optimal utility value.</p><p>Several thresholding strategies have been proposed and studied in text categorization literature. Researchers <ref type="bibr" coords="4,203.04,644.46,14.06,11.46" target="#b6">[7]</ref> have proposed score-based, rank-based and proportion-based thresholding strategies. However, it can be seen from Table <ref type="table" coords="4,400.08,658.26,5.98,11.46">1</ref> that the percentage of relevant documents in the training and test data is not very consistent. One possible explanation is that training data is not very representative of test data as these two sets of documents were published in different years. This observation indicates that ranked-</p><formula xml:id="formula_1" coords="4,164.52,160.02,295.82,293.58">- = V )) d , | log(P(y argmax 1 i i i * (4) 0 0 1 b) w d ( y : to subject C w 2 1 argmax b) , w { i i i i N 1 i i 2 w * â¥ â â¥ + - + â¢ + = = Î¾ (5) b) w d sign( y t t + â¢ = (6)</formula><p>based and proportion-based thresholding strategies may not fit the triage task as the estimated rank threshold or proportion threshold on training data would not be good choices for the test data. Therefore, different types of score-based thresholding strategies have been utilized in the work.</p><p>Logistic regression model generates output scores that are probabilities of relevance for test documents. If we assume that estimated logistic model provides accurate probabilities of relevance, it can be shown <ref type="bibr" coords="5,296.88,163.50,14.06,11.46" target="#b3">[4]</ref> that the optimal score threshold is 1/(u r +1). We call this threshold LR-D-Thre, which stands for the analytically derived threshold of logistic regression. However, the estimation of logistic model generally suffers from many problems such as limited amount of training data and inconsistency between training and test data. This indicates one of the disadvantages of using LR-D-Thre. Therefore, an alternative threshold as LR-CV-Thre was proposed that maximizes the utility value on the hold out set of training data. Different values of LR-CV-Thre were set through cross validation for the four subtasks.</p><p>In order to better reflect the higher utility value of retrieving one relevant document than discarding one nonrelevant document using SVM, researchers have proposed methods <ref type="bibr" coords="5,507.96,293.70,14.06,11.46" target="#b4">[5]</ref> to adjust the weights associated with training errors on positive data points and negative data points by using different values for C in Equation ( <ref type="formula" coords="5,372.98,321.30,4.23,11.46">4</ref>). However, this method still does not explicitly optimize for the goal of utility measures as shown in Equation ( <ref type="formula" coords="5,509.30,335.10,4.23,11.46">1</ref>). Therefore, we calculated score-based thresholds in our SVM-CV-Thre cross-validation method to explicitly optimize the utility measure.</p><p>Can one improve the accuracy of algorithms for triage task by combining the outputs from both logistic regression and support vector machine methods? One approach to address this question is to build a Meta classifier that combines the outputs by logistic regression and SVM methods. This approach was similar to the Stacking method <ref type="bibr" coords="5,482.88,423.90,14.06,11.46" target="#b5">[6]</ref> used in statistics community. However, our attempts at using a Meta classifier based on logistic regression did not yield satisfactory results. One possible reason is that Meta classifier approach could be causing more overfitting than the single-level classifiers. This is a serious problem as first level classifiers have already had many parameters associated with large number of features. We then experimented with a simple approach that is less sensitive to overfitting than the Meta classifier approach. In this method the outputs of the logistic regression method and the SVM method were used as sanity checks for each other. When a set of document were first selected through LR-CV-Thres method, these documents were further filtered by requiring their SVM scores to be higher than a threshold tuned on the outputs of SVM to generate better utility value. This method is called LR-SVM-CV-Thres. On the other side, another method first generated results from SVM-CV-Thres and then verified the results based outputs of logistic regression. The later method is called SVM-LR-Thres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Results</head><p>The Bayesian Binary Regression<ref type="foot" coords="5,254.40,652.32,3.99,7.64" target="#foot_0">4</ref> toolkit and SVM light toolkit <ref type="bibr" coords="5,415.68,655.14,13.94,11.46" target="#b2">[3]</ref> were used in this work to estimate logistic regression model and support vector machine model respectively. For logistic regression, the values for V in Equation (4) for the four tasks were estimated by cross validation. The estimated values were: 60 (Allele), 40 (Expression), 25 (Gene Ontology) and 25 (Tumor). For support vector machine, the polynomial kernel with degree 3 was used. The C values in Equation ( <ref type="formula" coords="6,449.73,325.50,4.65,11.46">5</ref>) for the four tasks were set by across validation: 0.0055 (Allele), 0.0032 (Expression), 0.0125 (Gene Ontology) and 0.006 (Tumor). The threshold values of different thresholding strategies were derived or estimated by cross validation on training data and are shown in Table <ref type="table" coords="6,504.12,366.90,4.49,11.46">2</ref>.</p><p>The results on test data were generated by logistic regression method and support vector machine method with different thresholding strategies. The details are shown in Table <ref type="table" coords="6,513.12,400.50,4.49,11.46" target="#tab_2">3</ref>. It can be seen from the results that the performance of support vector machine (i.e., SVM-CV) is at the same level as that of logistic regression model (i.e., LR-D and LR-CV). This demonstrates that with careful estimation of model parameters and good thresholding strategy, SVM can provide results comparable to that of the logistic regression model. This is a new observation since previous research did not generate comparable results for SVM and logistic regression methods on the triage task of TREC 2004 <ref type="bibr" coords="6,117.00,497.10,12.86,11.46" target="#b1">[2]</ref>.</p><p>For the two thresholding strategies used with the logistic regression model, the LR-CV-Thre method is better than the analytically derived thresholding method, LR-D-Thre, for two subtasks while LR-D-Thre is better than LR-CV-Thre for the one subtask. They used the same thresholding value for the Tumor subtask.</p><p>Moreover, the results in Table <ref type="table" coords="6,245.16,578.10,5.98,11.46" target="#tab_2">3</ref> provide some positive evidence for combining results from logistic regression method and SVM method. Although the improvement is small, both LR-SVM-CV and SVM-LR-CV methods consistently outperform the baseline methods LR-CV and SVM-CV respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper we describe the methods we have developed for the triage task of TREC 2005 genomic task. Statistical methods --logistic regression and support vector machine --were utilized to build classifiers. We studied different thresholding strategies for  <ref type="table" coords="6,104.16,152.99,3.76,9.53">2</ref>. The threshold values for different thresholding strategies. For LR-SVM-CV-Thres (SVM-LR-CV-Thres), the first threshold value is for logistic regression (SVM) and the second is for SVM (logistic regression).</p><p>generating optimal utility on test data. The empirical results show that with good estimation of model parameters and appropriate thresholding strategy, SVM method can achieve results comparable to the logistic regression method. Furthermore, the combination of results from the two methods generates consistent improvement over results from individual methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,86.04,78.43,441.55,641.85"><head>Percentage on Training Data AP/Percentage on Test Data Assigned U r</head><label></label><figDesc>Full Text: Text extracted between (&lt;bdy&gt;) tags in the XML file â¢ MeSH Text Word: Text keywords extracted from MeSH categories. In order to distinguish these words from regular text words, all MeSH keywords were associated with prefix "MH_" (e.g., MH_Diseases). â¢ MeSH Hierarchy Category ID: To associate MeSH category IDs at different levels, each annoated MeSH category, and all their ancestors, were treated as separate features and used to represent the document. The features were the IDs and not the MeSH category text words. (e.g., C04.928 for "Tumor Virus</figDesc><table coords="2,86.04,78.43,441.55,641.85"><row><cell cols="5">Subtasks â¢ Title Text: Text extracted between (&lt;atl&gt;) tags in the XML file â¢ Abstract Text: Text extracted between (&lt;abs&gt;) tags in the XML file AP/Allele 338/5.8% 332/5.5% 17 Expression 81/1.4% 105/1.7% 64 â¢</cell></row><row><cell>Gene Ontology</cell><cell cols="2">462/7.9%</cell><cell>518/8.6%</cell><cell>11</cell></row><row><cell>Tumor</cell><cell cols="2">36/0.6%</cell><cell>20/0.3%</cell><cell>231</cell></row><row><cell cols="5">Table 1. The AP number and the percentage value of relevant documents as well as the assigned U r values for</cell></row><row><cell cols="5">four subtasks as Allele, Expression, Gene Ontology and Tumor. (The total number of training documents is</cell></row><row><cell cols="4">5,837 and the total number of test documents is 6,043)</cell></row><row><cell></cell><cell>norm U =</cell><cell cols="2">r (u *TP)+(u *FP) nr r (u *AP)</cell><cell>(1)</cell></row><row><cell cols="3">2 http://www.ncbi.nlm.nih.gov/entrez/query.fcgi</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,54.60,75.35,511.61,209.81"><head>Table 3 .</head><label>3</label><figDesc>The utility values by different statistical learning method with different thresholding strategies and also the median and best results of all submitted official TREC runs. The best results of our submitted official runs are shown in bold font.</figDesc><table coords="6,57.12,75.35,504.03,179.45"><row><cell>Subtasks</cell><cell></cell><cell>LR-D</cell><cell>LR-CV</cell><cell>SVM-CV</cell><cell>LR-SVM-CV</cell><cell cols="2">LR-SVM-CV</cell></row><row><cell>Allele</cell><cell></cell><cell>0.055</cell><cell>0.03</cell><cell>-0.3</cell><cell>0.03/-0.45</cell><cell></cell><cell>-0.3/0.03</cell></row><row><cell>Expression</cell><cell></cell><cell>0.015</cell><cell>0.006</cell><cell>-0.75</cell><cell>0.006/-1</cell><cell cols="2">-0.75/0.005</cell></row><row><cell cols="2">Gene Ontology</cell><cell>0.083</cell><cell>0.04</cell><cell>-0.8</cell><cell>0.04/-1.1</cell><cell cols="2">-0.8/0.038</cell></row><row><cell>Tumor</cell><cell></cell><cell>0.004</cell><cell>0.004</cell><cell>0.035</cell><cell>0.004/-0.1</cell><cell cols="2">0.035/0.003</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Subtasks</cell><cell>LR-D</cell><cell>LR-CV</cell><cell cols="4">SVM-CV LR-SVM-CV SVM-LR-CV Median Results</cell><cell>Best Results</cell></row><row><cell>Allele</cell><cell>0.849</cell><cell>0.859</cell><cell>0.833</cell><cell>0.865</cell><cell>0.845</cell><cell>0.779</cell><cell>0.871</cell></row><row><cell>Expression</cell><cell>0.849</cell><cell>0.828</cell><cell>0.816</cell><cell>0.829</cell><cell>0.825</cell><cell>0.655</cell><cell>0.871</cell></row><row><cell>Gene Ontology</cell><cell>0.547</cell><cell>0.558</cell><cell>0.544</cell><cell>0.559</cell><cell>0.548</cell><cell>0.458</cell><cell>0.587</cell></row><row><cell>Tumor</cell><cell>0.889</cell><cell>0.889</cell><cell>0.941</cell><cell>0.905</cell><cell>0.947</cell><cell>0.761</cell><cell>0.943</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,95.76,710.75,177.24,9.53"><p>http://www.stat.rutgers.edu/~madigan/BBR/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank <rs type="person">Jamie Callan</rs> for helpful discussions regarding this work. The research presented in this paper is partially supported by an <rs type="funder">ARDA</rs> grant under Phase II of the <rs type="programName">AQUAINT program</rs>. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<div type="funding">
<div><p>* This work was done when <rs type="person">Luo Si</rs> visited <rs type="institution">IBM Almaden Research Lab</rs> in summer, 2005.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ySpAHWp">
					<orgName type="program" subtype="full">AQUAINT program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,94.49,281.10,427.36,11.46;7,111.00,294.90,320.99,11.46" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Dayanik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Fradkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Menkov</surname></persName>
		</author>
		<title level="m" coord="7,209.52,294.90,217.31,11.46">DIMACS at the TREC 2004 Genomics Track</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.49,311.70,427.37,11.46;7,111.00,325.50,392.03,11.46" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">F</forename><surname>Kraemer</surname></persName>
		</author>
		<title level="m" coord="7,111.00,325.50,386.75,11.46">TREC 2004 genomics track overview, The Thirteenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.50,342.30,427.48,11.46;7,111.00,356.10,266.87,11.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,222.72,342.30,225.66,11.46">Making large-Scale SVM Learning Practical</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,459.12,342.30,62.86,11.46;7,111.00,356.10,205.34,11.46">Advances in Kernel Methods -Support Vector Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.50,372.90,427.37,11.46;7,111.00,386.70,248.51,11.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,177.60,372.90,339.16,11.46">Evaluating and optimizing autonomous text classification systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,161.88,386.70,192.33,11.46">Proceedings of ACM SIGIR Conference</title>
		<meeting>ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.49,403.50,427.60,11.46;7,111.00,417.30,379.67,11.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,204.24,403.50,317.86,11.46;7,111.00,417.30,189.20,11.46">Revisiting Again Document Length Hypotheses, TREC 2004 Genomics Track Experiments at Patolis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,307.68,417.30,177.71,11.46">Thirteenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.49,434.10,420.29,11.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,220.56,434.10,107.33,11.46">Stacked generalization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,335.64,434.10,79.56,11.46">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.49,450.90,427.36,11.46;7,111.00,464.70,197.51,11.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,214.32,450.90,289.31,11.46">A Study on Thresholding Strategies for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><forename type="middle">Y M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,111.00,464.70,192.21,11.46">Proceedings of ACM SIGIR Conference</title>
		<meeting>ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.49,481.50,427.49,11.46;7,111.00,495.30,210.47,11.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,271.08,481.50,245.29,11.46">A Re-Examination of Text Categorization Methods</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><forename type="middle">Y M X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,123.84,495.30,192.33,11.46">Proceedings of ACM SIGIR Conference</title>
		<meeting>ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.49,512.10,427.48,11.46;7,111.00,525.90,212.75,11.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,204.12,512.10,317.86,11.46;7,111.00,525.90,39.37,11.46">Text Categorization Based on Regularized Linear Classification Methods</title>
		<author>
			<persName coords=""><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,162.12,525.90,102.64,11.46">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="31" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
