<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.05,83.88,311.61,15.49">Effective Smoothing for a Terabyte of Text</title>
				<funder ref="#_QwfBdWs #_gFVD8PP">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,266.92,116.37,63.92,10.76"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Humanities</orgName>
								<orgName type="laboratory">Archives and Information Studies</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.05,83.88,311.61,15.49">Effective Smoothing for a Terabyte of Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">854B2428FBE6AF00C004E76A5B6F995D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As part of the TREC 2005 Terabyte track, we conducted a range of experiments investigating the effects of larger collections. Our main findings can be summarized as follows. First, we tested whether our retrieval system scales up to terabyte-scale collections. We found that our retrieval system can handle 25 million documents, although in terms of indexing time we are approaching the limits of a non-distributed retrieval system. Second, we hoped to find out whether results from earlier Web Tracks carry over to this task. For known-item search we found that, on the one hand, indegree and URL priors did not promote retrieval effectiveness, but that, on the other hand, the combination of different document representations improved retrieval effectiveness. Third, we investigated the role of smoothing for collections of this size. We found that larger collections require far less smoothing, especially for the adhoc task using very little smoothing leads to substantial gains in retrieval effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As part of the TREC 2005 Terabyte track, we conducted a range of experiments investigating the effects of larger collections. First, we want to test whether our retrieval system scales up to 25 million documents. Second, we hope to find our whether results from earlier Web Tracks carry over to this task. Third, we want to investigate the role of smoothing for collections of this size.</p><p>We submitted runs for two of the Terabyte track's tasks: the adhoc task, and the named page finding task. In addition to the submitted runs, we also discuss post-submission results for the efficiency task. Furthermore, we discuss a range of post-submission experiments that further clarify the role of smoothing, especially for adhoc retrieval on terabytescale collections.. The rest of this paper is organized as follows. In Section 2, we detail the experimental set-up for the three tasks in the Terabyte track. In Section 3, we discuss our results, broken down over the efficiency task ( §3.1); the adhoc task ( §3.2); and the named page finding task ( §3.3). In Section 4, we zoom in on a set of experiments on the amount of smoothing for terabyte-sized collections. Finally, we summarize our findings in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head><p>Our retrieval system is based on the Lucene engine with a number of home-grown extensions <ref type="bibr" coords="1,458.08,310.25,10.79,8.97" target="#b1">[2,</ref><ref type="bibr" coords="1,471.36,310.25,7.19,8.97" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexes</head><p>The Terabyte track uses the GOV2 test collection, containing 25,205,178 documents (426 Gb uncompressed). We created three separate indexes for (1) the full documents, (2) the text in the title tags, (3) the anchor-texts pointing toward the document. For the anchor-texts index, we ignored relative links and only extracted full links. We normalized URLs, and did not index repeated occurrences of the same anchortext. This is similar to our earlier experiments in the TREC Web track <ref type="bibr" coords="1,361.18,452.80,10.79,8.97" target="#b3">[4,</ref><ref type="bibr" coords="1,375.38,452.80,7.19,8.97" target="#b4">5]</ref>. As to tokenization, we removed HTMLtags, punctuation marks, applied case-folding, and mapped marked characters into the unmarked tokens. We used the Snowball stemming algorithm <ref type="bibr" coords="1,439.72,488.67,10.58,8.97" target="#b6">[7]</ref>.</p><p>We created a single, non-distributed index for the collection. The size of our full-text index is 61 Gb. Building the full-text index (including all further processing) took a massive 15 days, 6 hours, and 21 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval models</head><p>For our ranking, we use either a vector-space retrieval model or a language model. Our vector space model is the default similarity measure in Lucene <ref type="bibr" coords="1,441.32,607.31,10.58,8.97" target="#b5">[6]</ref>, i.e., for a collection D, document d and query q:</p><formula xml:id="formula_0" coords="1,344.41,639.61,183.91,39.25">sim(q, d) = ∑ t∈q tf t,q • idf t norm q • tf t,d • idf t norm d • coord q,d • weight t ,</formula><p>where</p><formula xml:id="formula_1" coords="1,389.06,710.51,90.59,9.90">tf t,X = freq(t, X) idf t = 1 + log |D| freq(t, D) norm q = ∑ t∈q tf t,q • idf t 2 norm d = |d| coord q,d = |q ∩ d| |q|</formula><p>Our language model is an extension to Lucene <ref type="bibr" coords="2,240.77,164.99,10.58,8.97" target="#b1">[2]</ref>, i.e., for a collection D, document d and query q:</p><formula xml:id="formula_2" coords="2,75.08,193.81,196.55,23.51">P(d|q) = P(d) • ∏ t∈q ((1 -λ) • P(t|D) + λ • P(t|d)) ,</formula><p>where</p><formula xml:id="formula_3" coords="2,103.85,250.04,137.81,81.25">P(t|d) = tf t,d |d| P(t|D) = doc freq(t, D) ∑ t ∈D doc freq(t , D) P(d) = |d| ∑ d ∈D |d |</formula><p>The standard value for the smoothing parameter λ is 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Official runs</head><p>We submitted six runs in total, using only the short topic statement in the title. For the adhoc task, we submitted the following two runs: UAmsT05aTeVS Vector space model on the full-text index.</p><p>UAmsT05aTeLM Language model (λ = 0.15) on the fulltext index.</p><p>For the named page finding task, we submitted four runs. We submitted a plain language model run:</p><p>UAmsT05nTeLM Language model (λ = 0.70) on the fulltext index.</p><p>We also experimented with web-centric priors <ref type="bibr" coords="2,240.44,567.04,10.58,8.97" target="#b2">[3]</ref>. First, we assumed that pages with more inlinks are more likely to be relevant. Since our implementation of the language model calculates the logs of the probabilities, we took the exponent of the retrieval score, and multiplied it with the root of the indegree. We used the incomplete indegree scores we obtained from the anchor-text index. Second, we assumed that pages with shorter URLs are more likely to be relevant. We calculated the number of components in the domain and file path of the URL, e.g, trec.nist.gov/act_part/act_ part.html has 3 (domain) plus 2 (file path) components. Again, we took the exponent of the retrieval score, and multiplied it with the reciprocal of the length of the URL.</p><p>UAmsT05nTind Language model (λ = 0.70) on the fulltext index, with an indegree prior.</p><p>UAmsT05nTurl Language model (λ = 0.70) on the fulltext index, with a URL prior.</p><p>Finally, we have not yet implemented a proper mixture language model incorporating different document representations. Instead, we combined separate runs made on the different full-text, anchor-text, and titles indexes.</p><p>UAmsT05n3SUM CombSUM of language model (λ = 0.70) runs on the full-text index (relative weight 0.8), anchortext index (relative weight 0.8), and titles index (relative weight 0.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Efficiency task</head><p>We created a run for the efficiency task as a post-submission experiment. Table <ref type="table" coords="2,392.61,305.18,4.98,8.97" target="#tab_0">1</ref> shows the total and average query processing times for the 50,000 efficiency task topics. We used a non-dedicated, dual processor machine running Linux with the retrieval system running as a single Java process with a heap size of 1 Gb. For the efficiency task we used the vectorspace model. Total processing time for the 50,000 queries was 6 hours and 39 minutes. On average, it took 0.480 seconds to produce the top 20 results for a single query.</p><p>For comparison, we also list the system performance for the other Terabyte tasks. In terms of effectiveness, the efficiency task run is identical to ad hoc task run UAmsT05aTeVS discussed below. Its precision at rank 20 is 0.3710.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adhoc task</head><p>There are in total 50 adhoc task topics. The number of relevant documents per topics varies from 4 to 559, with an average of 208 and a median 171. Table <ref type="table" coords="2,491.67,632.05,4.98,8.97">2</ref> shows the results for the adhoc task. We see an interesting compari-UAmsT05 #rel ret map bpref recip rank P@10 P@20 . . . aTeVS 5717 0.1996 0.2596 0.4437 0.3800 0.3710 . . . aTeLM 4180 0.1685 0.2083 0.6023 0.3800 0.3460 Table <ref type="table" coords="2,390.42,710.39,3.88,8.97">2</ref>: Results for the adhoc task. son between the two retrieval models. First, we see that the vector-space model is superior on the overall measures (map and bpref). Second, we see that the language model is superior at early precision (recip rank), but that this waters down quickly (precision is equal at rank 10, and less at rank 20). The outcome deviates from results on the training data-the Terabyte track 2004 adhoc task topics-, where the language model outperformed the vector space model with a map score of 0.1562 versus 0.1413. Below, we will further zoom in on the language model and experiment with the amount of smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Named page finding task</head><p>In total there are 252 named page finding topics (20 topics have been deemed adhoc topics, and have been retracted from the qrels). The minimal number of relevant documents per topic is 1 and the maximum is 4,525. For 187 topics there is a unique relevant page, the few topics with thousands of relevant pages are caused by page-duplicates in the collection. This leads to a skewed distribution with a mean of 47 and a median of 1 relevant page. Table <ref type="table" coords="3,244.34,306.98,4.98,8.97" target="#tab_2">3</ref> shows the results for the named page finding task. We make a number  of observations. First, the indegree prior results in a loss of performance. Second, the URL prior leads to mixed results: a loss of mean reciprocal rank, but a gain in the number of topics with the relevant page in the top 10. Third, the combination run leads to improved performance on all measures. The success of the combination run shows the value of different document representations. On the Web Track data, mixture language models proved far more effective than straightforward run combination <ref type="bibr" coords="3,188.35,523.52,10.79,8.97" target="#b3">[4,</ref><ref type="bibr" coords="3,202.74,523.52,7.19,8.97" target="#b4">5]</ref>. This may also explain, in part, the mixed results for the link and URL priors. Other factors such as the incompleteness of the extracted links may also play an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Smoothing experiments</head><p>In the language modeling framework, smoothing plays an important role: it helps to overcome data-sparseness, it introduces an inverted document frequency effect, and it expresses the relative importance of query terms <ref type="bibr" coords="3,242.69,650.73,10.58,8.97" target="#b7">[8]</ref>. In practice, smoothing is also a handle to tune a run toward recall (much smoothing) or precision (little smoothing). It is known that collection size is a factor influencing precision measures <ref type="bibr" coords="3,94.43,698.55,10.58,8.97" target="#b0">[1]</ref>. Hence, collection size may also be a factor influencing the amount of smoothing needed in the language modeling framework. Here, we focus on linear or Jelinek-Mercer smoothing, and investigate the effect of varying the smoothing parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Named page finding task</head><p>First, we focus on the named page finding task. Since finding a 'unique' page requires precision rather than recall, we choose a relatively high value for the smoothing parameter (i.e., λ = 0.7). Table <ref type="table" coords="3,419.08,164.98,4.98,8.97" target="#tab_4">4</ref> shows the results while varying the smoothing parameter over the interval between 0 and 1. We make a few observations. As expected, we see that  the named page finding topics do not require much smoothing. In fact, as long as we put some weight on the collection model, the less smoothing the better. This is in contrast with results on the Web Track data, where performance actually drops at the highest values of the smoothing parameter.</p><formula xml:id="formula_4" coords="3,346.47,208.11,4.92,11.68">λ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adhoc task</head><p>Next, we focus on the adhoc task. Since adhoc topics require a delicate balance between precision and recall, we choose the standard relatively low value for the smoothing parameter (i.e., λ = 0.15). Table <ref type="table" coords="3,420.96,518.79,4.98,8.97" target="#tab_5">5</ref> shows the results while varying the smoothing parameter over the interval between 0 and 1.</p><p>A few observations present themselves. We see that performance increases if we apply less smoothing. In fact, the gain is substantial; already the improvement for λ = 0.2 is statistically significant (99.9%, one tailed) over λ = 0.15. In sum, the adhoc task evaluated by mean average precision behaves like an early precision task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">On Collection Size and Smoothing</head><p>We conduct an initial, exploratory experiment to study the effect of collection size on smoothing. First, we took the list of documents-ids randomly removed 50% of the documents. By doing this repeatedly, we obtained six samples containing 100%, 50%, 25%, 12.5%, 6.25% and 3.125% of the original collection, where documents are always also contained in  50 Adhoc topics, there is at least one relevant document in all samples. Second, we retrieved the top 10,000 documents for each of the topics, and for eleven values of λ in the range [0, 1]. Then, for each of the samples, we restricted both the run and the qrels by removing document no longer present in the sample. This allows us to evaluate the performance on each of the samples of the total collection. Figure <ref type="figure" coords="4,91.29,483.36,4.06,8.97" target="#fig_0">1</ref>(top) shows precision at 10 scores of the six samples for all values of the smoothing parameter. We see a solid decline in the scores when the samples get smaller. This in line with the expectation that early precision scores increase with collection size <ref type="bibr" coords="4,138.24,531.18,10.58,8.97" target="#b0">[1]</ref>. The scores over different values of the smoothing parameter gets less steep when the sample sizes decrease, but the optimal value of λ remains high. What about mean average precision? Figure <ref type="figure" coords="4,237.21,567.04,4.37,8.97" target="#fig_0">1</ref>(middle) has the MAP scores of the six samples, again over all values of the smoothing parameter. Here, we see almost the same performance over the smoothing parameter for the four samples larger than 10% of the whole collection. For smaller samples, the scores are somewhat higher, but this may be due to a favorable choice of our sampling strategy. As for the smoothing, the optimal values stay markedly in the highest region. As similar picture arises for the binary preference measure, shown in Figure <ref type="figure" coords="4,178.20,674.64,4.23,8.97" target="#fig_0">1</ref>(bottom). Here values seem largely indifferent to the sampling, except for the smallest sample of just over 3% of the whole collection. Again, the optimal value of λ is in the high end of the scale.  The results of our experiment with down-sampling the collection do not show a reversal in the amount of smoothing leading to optimal performance. Just as for the whole collection, smaller samples tend to favor high values of the smoothing parameter. There are two important qualification to make by the sampling strategy employed in this section. Firstly, we used only a single sample, rather than averaged over a large number of samples. Even though we took random samples, this may introduce accidental features especially when samples are small. Secondly, we only sampled a fixed retrieval run, hence the (relative) retrieval scores were determined by the collection as a whole. That is, the statistics from which the scores are estimated, and in particular the data-sparseness that smoothing methods address, do not change by the sampling method we applied in this section. Hence, a more thorough investigation is needed to study the effect of collection size on the amount of smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our participation in the Terabyte track was inspired by a number of aims related to the size of the Terabyte track collection, we now draw some initial conclusions.</p><p>Our retrieval system did scale up to the 25 million documents in the GOV2 collection. Performance at query time is impressive, especially for the optimized implementation of the vector space model. With respect to indexing time, with over two week to build a full-text index we seem to have reached the limits of building a non-distributed index.</p><p>For the adhoc task, we saw that standard IR techniques on a full-text index lead to good performance. We found that, on the 2005 topics, the vector space model outperformed the language model, although the language model can be substantially improved by using less smoothing.</p><p>For the named page finding task, we found that, on the one hand, indegree and URL priors did not promote retrieval effectiveness, but that, on the other hand, the combination of different document representations improved retrieval effectiveness.</p><p>Last, but not least, we zoomed in on the role of smoothing and found that less smoothing leads to the best performance. Whereas this is roughly according to expectation for the named page finding topics, it is unexpected for the adhoc topics. In addition to known relations between retrieval effectiveness and collection size <ref type="bibr" coords="5,186.27,605.87,10.58,8.97" target="#b0">[1]</ref>, there also seems to be a relationship between collection size and the appropriate amount of smoothing in the language modeling framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,316.81,629.72,239.10,8.97;4,316.81,641.68,239.10,8.97;4,316.81,653.63,72.47,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scores over samples of the collection: (top) precision at 10; (middle) mean average precision; and (bottom) binary preference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,339.44,239.10,97.87"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="2,323.60,339.44,225.53,63.26"><row><cell>Task</cell><cell cols="2">#Topics Model</cell><cell></cell><cell>Total</cell><cell>Avg.Q</cell></row><row><cell>Efficiency</cell><cell>50,000</cell><cell>VS</cell><cell cols="3">23,976 (6h39m36s) 0.480</cell></row><row><cell>Adhoc</cell><cell>50</cell><cell>VS</cell><cell>43</cell><cell></cell><cell>(43s) 0.862</cell></row><row><cell>Adhoc</cell><cell>50</cell><cell>LM</cell><cell>798</cell><cell cols="2">(13m18s) 15.962</cell></row><row><cell>Named Page</cell><cell>272</cell><cell>VS</cell><cell>594</cell><cell cols="2">(9m54s) 2.184</cell></row><row><cell>Named Page</cell><cell>272</cell><cell>LM</cell><cell>12,180</cell><cell cols="2">(3h23m) 44.781</cell></row></table><note coords="2,352.01,416.39,203.90,8.97;2,316.81,428.35,163.79,8.97"><p>Performance measurements in seconds for max. 20 results per topic using the full-text index.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,75.63,405.88,195.43,8.97"><head>Table 3 :</head><label>3</label><figDesc>Results for the named page finding task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,316.81,353.75,239.10,20.92"><head>Table 4 :</head><label>4</label><figDesc>Smoothing for the named page finding task using the full-text index.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,53.80,52.81,239.11,308.26"><head>Table 5 :</head><label>5</label><figDesc>Smoothing for the adhoc task using the full-text index. larger samples. Table6shows the resulting sample sizes, in number of documents, as well as the number of relevant documents (for any of the Adhoc topics) remaining. For all</figDesc><table coords="4,86.02,52.81,174.66,308.26"><row><cell>λ</cell><cell>MAP</cell><cell cols="2">B-Pref Prec@10 Prec@20</cell></row><row><cell cols="3">0.0 0.0002 0.0045</cell><cell>0.0020</cell><cell>0.0010</cell></row><row><cell cols="3">0.1 0.1467 0.1847</cell><cell>0.3620</cell><cell>0.3210</cell></row><row><cell cols="3">0.2 0.1856 0.2272</cell><cell>0.4120</cell><cell>0.3650</cell></row><row><cell cols="3">0.3 0.2138 0.2576</cell><cell>0.4600</cell><cell>0.4050</cell></row><row><cell cols="3">0.4 0.2355 0.2783</cell><cell>0.4900</cell><cell>0.4490</cell></row><row><cell cols="3">0.5 0.2540 0.2943</cell><cell>0.5060</cell><cell>0.4830</cell></row><row><cell cols="3">0.6 0.2707 0.3101</cell><cell>0.5380</cell><cell>0.5110</cell></row><row><cell cols="3">0.7 0.2863 0.3243</cell><cell>0.5420</cell><cell>0.5320</cell></row><row><cell cols="3">0.8 0.2998 0.3366</cell><cell>0.5620</cell><cell>0.5420</cell></row><row><cell cols="3">0.9 0.3107 0.3460</cell><cell>0.5680</cell><cell>0.5530</cell></row><row><cell cols="3">1.0 0.2972 0.3437</cell><cell>0.5840</cell><cell>0.5600</cell></row><row><cell></cell><cell cols="3">Percentage Size in Docs Relevant Docs</cell></row><row><cell></cell><cell>100.000%</cell><cell cols="2">25,205,179</cell><cell>45,291</cell></row><row><cell></cell><cell>50.000%</cell><cell cols="2">12,602,589</cell><cell>22,539</cell></row><row><cell></cell><cell>25.000%</cell><cell cols="2">6,301,294</cell><cell>11,356</cell></row><row><cell></cell><cell>12.500%</cell><cell cols="2">3,150,647</cell><cell>5,641</cell></row><row><cell></cell><cell>6.250%</cell><cell cols="2">1,575,323</cell><cell>2,778</cell></row><row><cell></cell><cell>3.125%</cell><cell cols="2">787,661</cell><cell>1,375</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="4,92.59,374.76,161.52,8.97"><head>Table 6 :</head><label>6</label><figDesc>Samples of the GOV2 collection.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">612.066.302</rs> and <rs type="grantNumber">640.001.501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QwfBdWs">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_gFVD8PP">
					<idno type="grant-number">640.001.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,333.41,79.22,222.50,8.97;5,333.41,91.18,222.51,8.97;5,333.41,103.13,22.42,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,464.68,79.22,91.23,8.97;5,333.41,91.18,84.84,8.97">On collection size and retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,424.95,91.18,84.73,8.97">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="99" to="150" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,123.06,222.50,8.97;5,333.41,135.01,216.49,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,360.04,123.06,191.72,8.97">The ILPS extension of the Lucene search engine</title>
		<author>
			<persName coords=""><surname>Ilps</surname></persName>
		</author>
		<ptr target="http://ilps.science.uva.nl/Resources/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,154.94,222.51,8.97;5,333.41,166.89,222.50,8.97;5,333.41,178.85,222.50,8.97;5,333.41,190.80,134.33,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,379.90,154.94,117.78,8.97">Web-centric language models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,519.85,154.94,36.07,8.97;5,333.41,166.89,222.50,8.97;5,333.41,178.85,192.71,8.97">Proceedings of the Fourteenth ACM Conference on Information and Knowledge Management (CIKM 2005)</title>
		<meeting>the Fourteenth ACM Conference on Information and Knowledge Management (CIKM 2005)<address><addrLine>New York NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,210.73,222.50,8.97;5,333.41,222.68,222.50,8.97;5,333.41,234.64,222.51,8.97;5,333.41,246.60,222.50,8.97;5,333.41,258.55,222.50,8.97;5,333.41,270.51,42.34,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,493.08,210.73,62.84,8.97;5,333.41,222.68,126.74,8.97">Language models for searching in Web corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,434.41,234.64,121.51,8.97;5,333.41,246.60,45.56,8.97">The Thirteenth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,290.43,222.51,8.97;5,333.41,302.39,222.50,8.97;5,333.41,314.34,222.51,8.97;5,333.41,326.30,222.50,8.97;5,333.41,338.25,222.50,8.97;5,333.41,350.21,140.30,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,333.41,302.39,168.40,8.97">Approaches to robust and web retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,489.53,314.34,66.39,8.97;5,333.41,326.30,147.62,8.97">The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,370.13,222.51,8.97;5,333.41,383.33,104.11,7.05" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,376.75,370.13,110.07,8.97">The Lucene search engine</title>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,402.01,222.50,8.97;5,333.41,413.97,222.50,8.97;5,333.41,427.17,22.81,7.05" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,378.16,402.01,177.75,8.97;5,333.41,413.97,31.67,8.97">Stemming algorithms for use in information retrieval</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,333.41,445.85,222.50,8.97;5,333.41,457.80,222.50,8.97;5,333.41,469.76,222.50,8.97;5,333.41,481.71,222.50,8.97;5,333.41,493.67,163.17,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,431.71,445.85,124.20,8.97;5,333.41,457.80,222.50,8.97;5,333.41,469.76,24.02,8.97">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,374.60,469.76,181.32,8.97;5,333.41,481.71,174.56,8.97">ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
