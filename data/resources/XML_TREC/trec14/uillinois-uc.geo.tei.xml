<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.36,138.96,317.21,15.49">UIUC/MUSC at TREC 2005 Genomics Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.04,171.47,85.21,10.76"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.03,171.47,57.41,10.76"><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept of Biostatistics, Bioinformatics and Epidemiology Medical</orgName>
								<orgName type="institution">University of South</orgName>
								<address>
									<settlement>Carolina</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.19,171.47,40.28,10.76"><forename type="first">Xu</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.07,171.47,34.88,10.76"><forename type="first">Xin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.67,171.47,75.76,10.76"><forename type="first">Atulya</forename><surname>Velivelli</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.48,185.39,71.67,10.76"><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.91,185.39,44.59,10.76"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.23,185.39,78.38,10.76"><forename type="first">Azadeh</forename><surname>Shakery</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.36,138.96,317.21,15.49">UIUC/MUSC at TREC 2005 Genomics Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">191049314019F09EEBC5A2B603B7E751</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report experiment results from the collaborative participation of UIUC and MUSC in the TREC 2005 Genomics Track. We participated in both the adhoc task and the categorization task, and studied the use of some mixture language models in these tasks. Experiment results show that a structured theme-based language modeling approach is effective in improving retrieval effectiveness for the ad hoc taks and the Latent Dirichlet Allocation method is effective in dimension reduction for the categorization task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Illinois at Urbana-Champaign (UIUC) and Medical University of South Carolina (MUSC) collaborated on both tasks of TREC 2005 Genomics Track with UIUC being more focused on the first ad hoc task while MUSC more on the second categorization task. Our general goal was to explore the effectiveness of several language modeling approaches in these tasks. Specifically, for the ad hoc task, our goal was to study a structured theme-based language model and use it to perform both pseudo feedback and relevance feedback. For the categorization task, our goal was to study the effectiveness of using the Latent Dirichlet Allocation <ref type="bibr" coords="1,258.53,677.32,11.75,8.97" target="#b0">[1]</ref> to perform dimension reduction to alleviate the problem of data sparseness.</p><p>Experiment results show that these approaches are quite promising. The structured theme-based language modeling approach provides a natural way of performing feedback for the structured queries and is shown to improve retrieval accuracy for the ad hoc task. For the categorization task, the Latent Dirichlet Allocation method is shown to be effective in dimension reduction. Compared with other participating groups, our ad hoc runs are mostly above the median and are the best for a few topics, while our categorization runs are mostly below the median due to a deficiency of our basic categorization method (SVM), which prevented our system from achieving a full-spectrum of precision-recall tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad Hoc Retrieval Task</head><p>One interesting characteristic of the ad hoc retrieval task of the TREC 2005 Genomics Track is that the queries are structured; each query is an instantiation of a template with entities such as diseases and genes. It is thus very interesting to study whether such additional information about query structures can be exploited to improve retrieval. In particular, we are interested in studying how to extend the language models developed for unstructured text queries to handle such structured queries. We propose a structured theme-based language model to combine the information from multiple fields of a query and study how we can estimate such a language model with feedback documents. Experiment results show that such a new language model is effective for both pseudo feedback and relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KL-divergence retrieval model</head><p>Our basic retrieval method is the Kullback-Leibler (KL) divergence retrieval model <ref type="bibr" coords="2,188.27,144.16,10.78,8.97" target="#b5">[6,</ref><ref type="bibr" coords="2,204.21,144.16,7.27,8.97" target="#b7">8]</ref>. According to this method, given a query Q and a document D, we estimate a query language model θ Q and a document language model θ D , and then simply score the document using the KL-divergence of θ Q and θ D , defined as</p><formula xml:id="formula_0" coords="2,80.40,214.17,210.96,27.61">s(Q, D) = D(θ Q ||θ D ) = w∈V p(w|θ Q ) log p(w|θ Q ) p(w|θ D )</formula><p>where V is the vocabulary set. In practice, for the sake of efficiency, we truncate our query model θ Q and only use a certain number (100 in our experiments) of highest probability words for scoring.</p><p>Clearly, using such a method, our main tasks are to estimate θ Q and θ D . We estimate θ D using Dirichlet prior smoothing, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(w|θ</head><formula xml:id="formula_1" coords="2,144.24,348.45,106.73,23.52">D ) = c(w, D) + µp(w|C) |D| + µ</formula><p>where p(w|C) is a collection background language model and µ is a smoothing parameter, which we empirically set to 100 based on some tuning with the 10 training topics available to us.</p><p>The main research question we address is how to estimate θ Q for a structured query. When the queries are unstructured, we can use the relative frequency counts of terms in the query to estimate θ Q or use feedback documents to update this model by interpolating it with another topic language model estimated based on the feedback documents <ref type="bibr" coords="2,138.75,505.60,10.60,8.97" target="#b7">[8]</ref>. We propose a theme-based language model, which would allow us to estimate a query language model for structured queries based on either the original structured query or feedback documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Theme-based query language models</head><p>In order to use the KL-divergence method to score a document w.r.t. a structured query, our general idea is to model each query field with a unigram language model and define the overall query language model as a mixture model with each field model as a component. Formally, let Q = (Q 1 , ..., Q k ) be a structured query with k fields. Q i is the i-th query field. For example, query 116 "Provide information about the role of the gene Insulin receptor gene in the disease Cancer" is represented as 3 fields based on the markup in the topic description: Q 1 ="Provide information about the role of the gene"; Q 2 ="insulin receptor gene"; Q 3 ="cancer" <ref type="foot" coords="2,481.92,110.44,3.48,6.28" target="#foot_0">1</ref>Intuitively, each query field characterizes one aspect of the user's information need, thus can be modeled using a theme unigram language model. Thus, we have k unigram language models for query Q, θ 1 , ..., θ k , with θ i modeling field Q i . The overall information need of the user can then be defined as a combination of these field language models:</p><formula xml:id="formula_2" coords="2,373.68,206.89,103.56,30.57">p(w|θ Q ) = k i=1 π i p(w|θ i )</formula><p>where π i is the weight on field model θ i . The remaining questions are how to estimate each theme model θ i and the mixing weights π i 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Estimating a theme-based query model</head><p>The simplest method for estimating a theme-based query model is: (1) Treat each query field as a sample of words drawn from a multinomial distribution (i.e., a theme model) and estimate the underlying theme model using the maximum likelihood estimator, giving us p(w|θ i ) = p(w|Q i ) = c(w,Qi)</p><p>|Qi| . (2) Set π i 's to be uniform, i.e., π i = 1/k. We call this method ThemeOrig, which can be regarded as the baseline theme language model approach.</p><p>Since some fields in the query are biological entities (e.g., genes), one natural way to improve our query model is to automatically perform query expansion using resources such as LocusLink and MeSH. In particular, we used LocusLink to add additional gene synonyms for any given gene name in the query. To avoid introducing too much noise, we only include gene symbols if their official full names match the gene in the query well. This is a conservative, but relatively reliable way of expansion. For MeSH, we look up each query term in the MeSH term list. For each candidate MeSH term matched, we check if the term is entirely contained in our query, and if so, we add its other equivalent terms in the MeSH database to the query.</p><p>With such field expansion, we obtain an expanded structured query</p><formula xml:id="formula_3" coords="2,380.91,600.33,127.46,10.65">Q E = (Q 1 , ..., Q k , E 1 , ..., E k )</formula><p>, where E i is the expanded text for field Q i . For example, if Q i is a gene, E i would be a list of gene symbols found from LocusLink. Naturally, some E i 's are empty. We can now obtain a presumably improved estimate of θ i by combining E i with Q i :</p><formula xml:id="formula_4" coords="2,318.48,683.13,207.36,10.33">p(w|θ i ) = p(Q i |θ i )p(w|Q i ) + p(E i |θ i )p(w|E i ) = p(Q i |θ i ) c(w, Q i ) |Q i | + p(E i |θ i ) c(w, E i ) |E i |</formula><p>where c(w, Q i ) and c(w, E i ) are the counts of word w in Q i and E i , respectively and |Q i | and |E i | are the lengths of Q i and E i , respectively. We call such a method The-meExp. The probabilities p(Q i |θ i ) and p(E i |θ i ) are the weights on the original query field and the expanded field, and clearly, p(Q</p><formula xml:id="formula_5" coords="3,120.36,210.69,85.41,10.33">i |θ i ) + p(E i |θ i ) = 1.</formula><p>In our experiments, we empirically set both p(Q i |θ i ) and p(E i |θ i ) to 0.5, because our focus was on experimenting with improving the estimation through feedback documents. Unfortunately, later we found that the 0.5 setting is far from optimal and has hurt one of our official runs significantly. Indeed, when |E i | &lt;&lt; |Q i |, such a setting would give an expanded term a substantially higher weight than terms in the original query field. How to optimize these weights is an interesting question that we should explore in the future.</p><p>A major research question we address in our TREC experiments is how we can further improve such a structured language model through both pseudo feedback and relevance feedback. We now propose a mixture model approach to use a set of feedback documents to improve our estimation of both the theme language models θ i 's and the mixing weights π i 's. Specifically, suppose we have a set of theme models θ i 's which are estimated based on the original query text and/or expanded query text. We assume that our feedback documents F = {D 1 , ..., D m } are sampled from a mixture model with k component multinomial distributions (corresponding to the k fields of the query) and mixing distribution π i 's. We use θ i to define a Dirichlet prior for the i-th component multinomial distribution, so that each component model will be biased to model the corresponding original query field. We also introduce a special background language model θ 0 to model the general English words and any non-relevant information in F . Our parameters for such a model can thus be represented as</p><formula xml:id="formula_6" coords="3,133.48,556.33,70.14,12.45">Λ = {φ i , π i } k i=0</formula><p>, where φ 0 is the background model; φ 1 , ..., φ k are the k query theme language models; and π i 's are the mixing weights. The parameters can be estimated using the Maximum A Posterior (MAP) estimator:</p><formula xml:id="formula_7" coords="3,121.32,622.65,130.27,10.65">Λ * = argmax Λ p(F |Λ)p(Λ)</formula><p>where p(Λ) ∝ k j=0 w∈V p(w|φ j ) σj p(w|θj ) is a conjugate prior on all the component multinomial distributions and p(F |Λ) is the likelihood of the feedback documents given by</p><formula xml:id="formula_8" coords="3,96.12,694.33,180.24,30.69">p(F |Λ) = m i=1 w∈V [ k j=0 π Di,j p(w|φ j )] c(w,Di)</formula><p>The MAP estimate can be found using the EM algorithm <ref type="bibr" coords="3,311.04,124.00,10.60,8.97" target="#b2">[3]</ref>. The updating formulas are as follows:</p><formula xml:id="formula_9" coords="3,311.04,152.14,248.64,80.96">p (n+1) (zw,i = j) = π (n) D i ,j p (n) (w|φj ) k j =1 π (n) D i ,j p (n) (w|φ j ) π (n+1) D i ,j = w∈V c(w, Di)p (n+1) (zw,i = j) |Di| p (n+1) (w|φj ) = m i=1 c(w, Di)p (n+1) (zw,i = j) + σj p(w|θj ) w ∈V m i=1 c(w , Di)p (n+1) (z w ,i = j) + σj</formula><p>The parameters σ j 's specify our confidence in the prior, which, in effect, control the amount of expansion. A larger σ j would cause a smaller amount of expansion.</p><p>In our experiments, we empirically set σ j = 10m for j = 1, ..., k (for the component models) and σ 0 = 100m (for the background model), where m is the total number of feedback documents. Parameterizing σ j with m allows us to interpret σ j as an "equivalent sample of text" comparable with each document. For example, when σ j = 10m, the strength of our prior is roughly equivalent to 10 words in a feedback document.</p><p>Once we estimate φ i 's and π Di,j 's, we can compute the query model as</p><formula xml:id="formula_10" coords="3,345.36,412.93,159.35,66.81">p(w|θ Q ) = k j=1 p(w|φ j )p(φ j |F ) ∝ 1 k k j=1 p(w|φ j ) m i=1 π Di,j</formula><p>We call such a method ThemeFB</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experiment Results</head><p>We used the Lemur toolkit (http://www.lemurproject.org/) to do all our experiments. Preprocessing of documents is minimum and mainly involves normalization of potential gene names. We noticed the variations in gene name spelling caused by various ways of separating name constituents using white spaces, hyphens, slashes and brackets. To deal with these variations, we use a tokenizer to convert the input text into a sequence of tokens, where each token is either a sequence of lowercase letters or a sequence of numbers. White spaces and all other symbols are treated as token delimiters. For instance, the different synonyms for gene cAMP dependent protein kinase 2, "PKA C2", "Pka C2", and "Pka-C2", would all be normalized to the same token sequence "pka c 2" to allow them to match each other.</p><p>We submitted two official runs -UIUCgAuto and UIUCgInt. For both runs, we first use the baseline KLdivergence method Baseline (i.e., ignoring the structures in the query and treating it as an unstructured text query) to obtain initial retrieval results. For UIUCgAuto, we use the top 5 documents from Baseline to fit the mixture theme language model with a prior defined using a theme language model computed based on the original query fields (i.e., ThemeOrig). It thus represents a completely automatic run without using any biological resources or having any human interaction. For UIUCgInt, we asked two people with some biology background to manually judge the relevance of the top 20 documents from Baseline and then fit the mixture theme language model to only the judged relevant documents with a prior defined using a theme language model computed based on both the original query text and the expanded text (i.e., ThemeExp). This run thus attempts to benefit from both biological resources and human relevance judgments <ref type="foot" coords="4,234.96,325.60,3.48,6.28" target="#foot_1">2</ref> .</p><p>We were expecting UIUCgInt to be much better than UIUCgAuto especially since the documents we used for feedback in the case of UIUCgInt are manually judged. But surprisingly, our manual interactive/manual run UIUCgInt is worse than our automatic run! UIUCgInt has a MAP of 0.2487 while UIUCgAuto is 0.2577. A pertopic comparison shows that UIUCgAuto is better than UIUCgInt for 26 topics and worse for 23 topics out of the 49 topics that have relevant documents in the collection.</p><p>UIUCgInt differs from UIUCgAuto in two aspects: (1) theme language model prior (ThemeOrig for UIUC-gAuto while ThemeExp for UIUCgInt); and (2) feedback documents (top 5 documents for UIUCgAuto while judged relevant documents for UIUCgInt). Since normally, relevance feedback is expected to help significantly, our results suggest that the LocusLink and MeSH expansion is ineffective, i.e., ThemeExp is worse than ThemeOrig, which is confirmed in our post-TREC experiments. It turns out that the problem has to do with the non-optimal weighting of the original text and the expanded text; assigning equal weights (0.5) can be significantly biased toward favoring expanded terms in some cases. This is shown in Table <ref type="table" coords="4,200.26,603.28,3.77,8.97" target="#tab_0">1</ref>, in which we compare ThemeExp, ThemeOrig, and Baseline. We see that The-meExp is clearly worse than ThemeOrig by all measures, indicating that the expanded text using LocusLink and MeSH is either mostly noise, or more likely, not weighted appropriately. Additional experiments need to conducted to further clarify this. We also see that The-meOrig has higher "front-end" precision (i.e., precision at low recall levels) than Baseline, suggesting that constructing the query model by giving equal weights to all the fields can help increase precision, though it also hurts recall and thus the average precision. We now examine the effectiveness of ThemeFB by comparing the feedback runs with the corresponding baseline runs in Table <ref type="table" coords="4,404.18,301.12,3.77,8.97" target="#tab_1">2</ref>. In each run of ThemeFB, the prior model used is given within the parentheses. A The-meFB run can thus be compared with both the Baseline run and the prior run. From Table <ref type="table" coords="4,454.40,337.00,3.77,8.97" target="#tab_1">2</ref>, we see that all the feedback runs (both pseudo feedback and relevance feedback) perform better than the corresponding prior runs as well as the baseline run. Moreover, comparing relevance feedback with pseudo feedback, we see that relevance feedback performs better by all measures, as expected. These results show that the proposed theme-based language models are effective in exploiting feedback documents to improve the estimation of the query language model. Compared with other groups' submissions, both of our runs are above the median of the same type of runs<ref type="foot" coords="4,515.16,649.24,3.48,6.28" target="#foot_2">3</ref> for a majority of topics (shown in Table <ref type="table" coords="4,452.97,662.80,3.63,8.97" target="#tab_2">3</ref>). Moreover, UIUC-gAuto is the best for 2 topics and UIUCgInt is the best for 7 topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Categorization Task</head><p>For the text categorization task, our goal was to study the effectiveness of using the Latent-Dirichlet Allocation (LDA) for dimension reduction. Our basic categorization method is Support Vector Machine (SVM), and we mainly explored two techniques for improving its performance: (1) the use of LDA to generate semantically enriched feature representation of a document; (2) the use of a semi-supervised learning method to augment the training data. Our experiment results show that the semantic feature representation generated using LDA significantly enhances the performance of SVM and semi-supervised learning further improves the recall on the categories with very few training cases. When applied to the test data, our system had good balanced performance on all categories in terms of F values. However, the recall of the SVM was somehow capped, resulting in relatively unsatisfactory utility scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Support Vector Machine</head><p>Support vector machine is a well studied kernel-based classification algorithm, which searches for a linear decision surface that has the largest margin between positive and negative training data. We used the publicly available implementation, SVMlight <ref type="bibr" coords="5,184.56,582.52,10.60,8.97" target="#b4">[5]</ref>. The categorization task was performed by training an one-vs-all classifier for each of the subtasks. We only tuned the cost-factor parameter "-j" of SVMlight, which adjusts the weight of training error on positive training cases relative to that of negative cases. The performance of SVM classification with different cost-factors was evaluated by the built-in leave-oneout (LOO) estimation in SVMlight. In some preliminary experiments, we found that the performance of SVMlight on the training data set tends to reach a plateau as the costfactor is increased, and we decided to set the cost-factor parameter to 20 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vocabulary-based Text Representation</head><p>The full text for task II was processed as follows: (1) SGML tags were stripped; (2) Tokens consisting of letters and numbers were extracted; (3) Words from a stop word list were removed; (4) Tokens were stemmed using a Porter stemmer. We further constructed a vocabulary consisting of 21,000 tokens that were "biologically informative" using a protein-related corpus consisting of MED-LINE abstracts and titles associated with Gene Ontology terms from the Gene Ontology Annotation <ref type="bibr" coords="5,482.07,228.04,11.63,8.97" target="#b1">[2]</ref> The tokens were extracted from the corpus and the mutual information of tokens with respect to GO terms was calculated, sorted and 21,000 tokens with high mutual information were retained.</p><p>For the SVM classification, a document is represented as a vector in a vector space. The element of the vector is further weighted using tf-idf weighting scheme and the length of the vector is normalized to 1 <ref type="bibr" coords="5,467.22,324.64,10.60,8.97" target="#b6">[7]</ref>. Such a vector representation using the original vocabularly was referred to as vocabulary-based text representation ( VocRep).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic Text Representation</head><p>The VocRep of the training text has a key drawback: the text is represented by a sparse vector in a very high dimensional space. Within such a space, data points tend to spread far apart and the SVM decision surface learned in such a space is likely to over-fit the training data, especially when a relatively small number of training cases are available. The ambiguities of natural language, i.e., polysemy and synonym, further complicate the situation because variations of word usages may easily cause two documents with similar semantic contents to be separated far away within the space.</p><p>To alleviate such a problem, we applied a probabilistic topic model, the Dirichlet Allocation Model (LDA) <ref type="bibr" coords="5,518.18,546.64,10.78,8.97" target="#b0">[1,</ref><ref type="bibr" coords="5,531.48,546.64,8.27,8.97" target="#b3">4]</ref> to extract semantic topics from the corpus and represent the text documents within this reduced semantic space. LDA treats a document as a mixture of words from different topics and applies probabilistic inference to extract the semantic topics from a corpus in an unsupervised way. We have applied the model on the combined training and test data sets and extracted 400 semantic topics from the corpus. The semantic topic for each word in the corpus was then inferred, which allowed us to represent a document as a vector within the semantic space defined by these 400 subtopics. Each element of the vector corresponds to the number of words within the document that belongs to the corresponding topic. Such a text representation is referred to as SemRep.</p><p>Semantic representation by the LDA has the following advantages: (1) Text is classified according to the semantic content; (2) Projecting text into a semantic space allows the documents that share few common words yet have similar semantic content to be closer in the vector space, thus increases the sensitivity of the classifier; (3) Dimension reduction increases the generality of the trained classifier; (4) LDA is computationally much less expensive than the classical latent semantic indexing (LSI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semi-supervised learning</head><p>To further enhance the classification performance of the SVM, especially for the categories with few training cases, we augmented the positive training set using a semi-supervised learning approach described in <ref type="bibr" coords="6,265.19,303.52,10.60,8.97" target="#b8">[9]</ref>. This algorithm is based on the graph theory and allows the labels of training examples to be propagated to the unlabeled data. The newly-labeled pseudo-positive cases would then be incorporated into the training data set to enhance training of the SVM classifier. We constructed a weighted undirected graph, in which the vertices are the training documents and the weights of edges connecting the vertices are defined as:</p><formula xml:id="formula_11" coords="6,117.36,419.37,138.23,23.90">w ij = exp( 1 0.03 (1 - v i v j |v i | × |v j | ))</formula><p>where w ij is the weight of the edge connecting vertices i and j; v i v j is the dot product of the vectors for documents i and j. The term |v i | stands for the norm of the vector v i . As negative examples, we have manually selected 500 training documents that do not belong to any of the four categories. These negative cases were used in combination with the provided positive training cases to perform the label-propagation experiments. For each category, we obtained 100 pseudo positive cases to augment the training cases for the category. Then, the SVM models were re-trained using the augmented data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Result Analysis</head><p>We first compare VocRep and SemRep in terms of the training performance of SVM in Table <ref type="table" coords="6,225.56,630.28,5.03,8.97" target="#tab_3">4</ref> We see that Sem-Rep significantly increases the utility and recall of SVMlight based on the leave-one-out evaluation on the training data, though it slightly decreases the F-score in some cases.</p><p>However, the improvement on the two relatively more difficult categories (i.e., E and G) is quite consistent for both F-score and utility, indicating that the LDA model We further examine the effect of semi-supervised learning in Table <ref type="table" coords="6,364.99,312.28,3.77,8.97" target="#tab_4">5</ref>. We see that the semi-supervised learning approach significantly improves recall for the subtasks with very few positive training cases, namely the E and T subtasks, while the benefit on the other two subtasks is less significant. In comparison with other groups' submissions, our official results are very good by F-score, but poor by utility. We believe that the main reason why the utility values of our runs are poor is because the SVM method we used appears to be have some ceiling for recall, probably because of the sparse training data. In the future, we plan to explore how to address this problem by de-regularizing SVM as well as other classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>In summary, our ad hoc retrieval experiments are focused on studying how to optimize retrieval with semistructured queries. The results show that exploiting the query structure is beneficial and the proposed themebased language models are effective in performing both pseudo feedback and relevance feedback for such semistructured queries. Our categorization experiments are focused on studying the effectiveness of using LDA for dimension reduction. The experiment results demonstrate that LDA is capable of capturing meaningful semantic contents of text documents and such features are useful in improving SVM performance. We also found that the SVM approach appears to have some limitation in optimizing strongly biased utility functions and semisupervised learning is beneficial to alleviate this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,311.04,190.60,228.82,69.93"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,311.04,190.60,228.82,69.93"><row><cell cols="5">Comparison of Baseline, ThemeOrig, and The-</cell></row><row><cell>meExp</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">MAP Pr@0.1 pr@10 RelRet</cell></row><row><cell>Baseline</cell><cell>0.2415</cell><cell>0.447</cell><cell>0.382</cell><cell>3340</cell></row><row><cell cols="2">ThemeOrig 0.2366</cell><cell>0.459</cell><cell>0.398</cell><cell>3156</cell></row><row><cell cols="2">ThemeExp 0.2221</cell><cell>0.431</cell><cell>0.351</cell><cell>3101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,311.04,473.32,228.81,146.87"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness of mixture theme language models</figDesc><table coords="4,316.92,485.16,213.53,135.03"><row><cell>Method</cell><cell>MAP</cell><cell cols="3">Pr@0.1 pr@10 RelRet</cell></row><row><cell>Baseline</cell><cell>0.2415</cell><cell>0.447</cell><cell>0.382</cell><cell>3340</cell></row><row><cell>ThemeOrig</cell><cell>0.2366</cell><cell>0.459</cell><cell>0.398</cell><cell>3156</cell></row><row><cell cols="2">Pseudo ThemeFB 0.2577</cell><cell>0.482</cell><cell>0.412</cell><cell>3476</cell></row><row><cell>(+ThemeOrig)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>= UIUCgAuto</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ThemeExp</cell><cell>0.2221</cell><cell>0.431</cell><cell>0.351</cell><cell>3101</cell></row><row><cell>Rel ThemeFB</cell><cell>0.2487</cell><cell>0.485</cell><cell>0.422</cell><cell>3403</cell></row><row><cell>(+ThemeExp)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>= UIUCgInt</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rel ThemeFB</cell><cell>0.2704</cell><cell>0.513</cell><cell>0.431</cell><cell>3500</cell></row><row><cell>(+ThemeOrig)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,102.84,118.84,164.20,82.65"><head>Table 3 :</head><label>3</label><figDesc>Comparison with group median.</figDesc><table coords="5,109.92,130.72,153.26,70.77"><row><cell></cell><cell cols="2">Number of topics</cell></row><row><cell></cell><cell cols="2">UIUCgAuto UIUCgInt</cell></row><row><cell>&gt; median</cell><cell>32</cell><cell>39</cell></row><row><cell>= median</cell><cell>0</cell><cell>3</cell></row><row><cell>&lt; median</cell><cell>17</cell><cell>7</cell></row><row><cell>=best</cell><cell>2</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,311.04,118.84,228.94,178.17"><head>Table 4 :</head><label>4</label><figDesc>Comparison of VocRep and SemRep</figDesc><table coords="6,311.04,130.72,228.94,166.29"><row><cell cols="2">Method</cell><cell>Task</cell><cell></cell></row><row><cell></cell><cell>A</cell><cell>E</cell><cell>G</cell><cell>T</cell></row><row><cell cols="5">F-score VocRep 0.725 0.210 0.281 0.417</cell></row><row><cell></cell><cell cols="4">SemRep 0.843 0.482 0.617 0.611</cell></row><row><cell cols="5">F-score VocRep 0.363 0.152 0.159 0.283</cell></row><row><cell></cell><cell cols="4">SemRep 0.317 0.154 0.160 0.268</cell></row><row><cell>Utility</cell><cell cols="4">VocRep 0.708 0.207 0.239 0.416</cell></row><row><cell></cell><cell cols="4">SemRep 0.793 0.458 0.424 0.607</cell></row><row><cell cols="5">can capture the major directions of the semantic structure</cell></row><row><cell cols="5">of the corpus while maintaining the discriminative power</cell></row><row><cell cols="2">in these cases.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,311.04,391.60,228.82,69.45"><head>Table 5 :</head><label>5</label><figDesc>Recall of before and after semi-supervised learn-</figDesc><table coords="6,311.04,403.48,198.76,57.57"><row><cell>ing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>Task</cell><cell></cell></row><row><cell></cell><cell>A</cell><cell>E</cell><cell>G</cell><cell>T</cell></row><row><cell cols="5">Before 0.840 0.531 0.617 0.639</cell></row><row><cell>After</cell><cell cols="4">0.867 0.691 0.630 0.861</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,325.32,705.92,214.29,7.17;2,311.04,715.40,222.69,7.17"><p>In our experiments, we excluded the background field, as some preliminary experiments showed that excluding it improves performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,86.40,696.44,214.30,7.17;4,72.00,705.92,228.70,7.17;4,72.00,715.40,70.24,7.17"><p><ref type="bibr" coords="4,86.40,696.44,3.95,7.17" target="#b4">5</ref> topics have no relevant documents in the top 20 documents being judged; for these topics, we simply used the baseline results when submitting UIUCgInt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,325.32,705.92,214.34,7.17;4,311.04,715.40,86.54,7.17"><p>UIUCgAuto is compared with all automatic runs, while UIUCgInt all manual/interactive runs.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.55,239.68,212.29,8.97;7,88.56,251.56,212.38,8.97;7,88.56,263.56,75.22,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,235.38,239.68,65.46,8.97;7,88.56,251.56,38.13,8.97">Latent Dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,137.58,251.56,159.00,8.97">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,283.48,212.41,8.97;7,88.56,295.48,212.27,8.97;7,88.56,307.36,212.25,8.97;7,88.56,319.36,206.59,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,147.18,295.48,153.65,8.97;7,88.56,307.36,212.25,8.97;7,88.56,319.36,105.31,8.97">The gene ontology annotation (GOA) database-an integrated resource of go annotations to the uniprot knowledgebase</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Camon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dimmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Apweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,201.69,319.36,54.41,8.97">Silico Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,339.28,212.38,8.97;7,88.56,351.28,212.29,8.97;7,88.56,363.16,212.60,8.97;7,88.56,375.16,22.64,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,276.47,339.28,24.46,8.97;7,88.56,351.28,212.29,8.97;7,88.56,363.16,29.99,8.97">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,129.67,363.16,128.12,8.97">Journal of Royal Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,395.08,212.41,8.97;7,88.56,406.96,212.26,8.97;7,88.56,418.96,87.57,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,213.69,395.08,87.27,8.97;7,88.56,406.96,10.15,8.97">Finding scientific topics</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,105.70,406.96,86.79,8.97">In Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,438.88,212.38,8.97;7,88.56,450.88,212.27,8.97;7,88.56,462.76,212.37,8.97;7,88.56,474.76,64.10,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,143.56,438.88,157.38,8.97;7,88.56,450.88,194.69,8.97">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,88.56,462.76,212.37,8.97;7,88.56,474.76,34.83,8.97">Proceedings of the European Conference on Machine Learning</title>
		<meeting>the European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,494.68,212.37,8.97;7,88.56,506.68,212.50,8.97;7,88.56,518.56,212.38,8.97;7,88.56,530.56,62.78,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,186.71,494.68,114.20,8.97;7,88.56,506.68,212.50,8.97;7,88.56,518.56,31.82,8.97">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,144.31,518.56,101.40,8.97">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">Sept 2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,550.48,212.26,8.97;7,88.56,562.48,212.37,8.97;7,88.56,574.36,212.72,8.97;7,88.56,586.36,22.64,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,246.87,550.48,53.94,8.97;7,88.56,562.48,208.39,8.97">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,88.56,574.36,159.00,8.97">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,606.28,212.26,8.97;7,88.56,618.28,212.50,8.97;7,88.56,630.16,212.37,8.97;7,88.56,642.16,168.36,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,185.15,606.28,115.65,8.97;7,88.56,618.28,119.09,8.97">Model-based feedback in the KL-divergence retrieval model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,224.59,618.28,76.47,8.97;7,88.56,630.16,212.37,8.97;7,88.56,642.16,74.18,8.97">Tenth International Conference on Information and Knowledge Management (CIKM 2001)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,662.08,212.29,8.97;7,88.56,673.96,212.27,8.97;7,88.56,685.96,212.48,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,276.97,662.08,23.86,8.97;7,88.56,673.96,212.27,8.97;7,88.56,685.96,62.65,8.97">Semisupervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,167.42,685.96,104.66,8.97">Proceedings of ICML 2003</title>
		<meeting>ICML 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
