<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.48,81.83,257.36,11.88">York University at TREC 2005: SPAM Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.80,107.70,31.97,8.93"><forename type="first">Wei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.34,107.70,36.50,8.93"><forename type="first">Aijun</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.85,107.70,58.56,8.93"><forename type="first">Xiangji</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<postCode>M3J 1P3</postCode>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.48,81.83,257.36,11.88">York University at TREC 2005: SPAM Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7458B63E86043EFBF698C2D8D81B023B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a variant of the k-nearest neighbor classification method, called instance-weighted k-nearest neighbor method, for adaptive spam filtering. The method assigns two weights, distance weight and correctness weight, to a training instance, and makes use of the two weights when classifying a new email. The correctness weight is also used in the maintenance of the training data to make the training data more adaptive to the changes of spam characteristics. We submitted 4 spam filters to the Spam Track. Two of the filters are purely based on the instance-weighted kNN method. The two other filters combine the kNN method with other spam filtering and classification techniques. We report the official results of our submissions on the Spam Track evaluation data sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As spam emails evolve with time, it is important for a spam filter to dynamically adapt itself to the changes of spam emails. We investigate the use of the k-nearest neighbor (kNN) classification method for adaptively filtering spam emails. The kNN method can incrementally learn from data by updating the training data as new spam emails arrive. For the 2005 TREC Spam Track, we submitted four spam filters. Two of the filters are based on a single kNN algorithm. The other two combine kNN with other spam filtering and classification techniques. The four filters are pre-trained with a set of training data. In this paper, we will first describe our method for data preprocessing and then describe the design of the four filters. The results of the filters on the Spam track test data sets are also given and analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preprocessing</head><p>All the four submitted filters (referred to as submission 1, submission 2, submission 3 and submission 4, respectively) involve a pre-train step. In this step, a filter is trained with a set of training data that include some public data obtained from SpamAssassin <ref type="bibr" coords="1,212.01,441.32,10.92,8.43" target="#b3">[4]</ref> and some private emails of our own. The training set consists of 2214 emails, of which 1659 are spam and 555 are ham. We chose to use a small training set with our kNN methods due to the time limit imposed by the Spam Track on classifying a new email.</p><p>Data preprocessing converts an email into a token stream. It processes each email in a training corpus and converts it into an attribute vector. Before classifying a new email in a test set, it also converts the new email into an attribute vector. In our spam filters, data preprocessing involves the following steps: MIME parsing, tokenization, stop word removal, word stemming, and instance representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MIME parsing and unpacking</head><p>The objective of this step is to parse and decode a MIME (Multipurpose Internet Mail Extensions) formatted email, and then retrieve the useful text part for tokenization. A MIME email may include binary data, such as encoded text parts, audio attachments, video attachments, pictures and PDF files. In the implementation of our MIME parser, we remove all the attachments except the encoded text parts that are decoded into readable text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tokenization</head><p>After MIME parsing and unpacking, the retrieved text part of an email is further converted into a set of tokens. A regular expression is used to segment a piece of text to tokens. The regular expression specifies the characters that can be included in a valid token. The other characters are used as delimiters between tokens. Different regular expressions may be used for different submissions. For example, alphabetical letters, 0-9, '$', '@', '-", '_', '!', '.' '\', and '*' are considered as valid characters in a token for submission 1; but only alphabetical letters, '$' and '@' are considered valid in submissions 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stop word removal</head><p>All of our submitted spam filters remove stop words. But different filters may use different lists of stop words. Submission 4 adopts a standard stop word list from Weka <ref type="bibr" coords="2,312.45,71.00,10.23,8.43" target="#b4">[5,</ref><ref type="bibr" coords="2,322.68,71.00,6.82,8.43" target="#b5">6]</ref>. Submission 3 adds to this list 8 words that appear frequently in the header of emails, such as "message-id" and "reply-to". Submission 1 adds more of such words to the stop word list. Submission 2 uses what is used in SpamAssassin <ref type="bibr" coords="2,322.74,93.56,10.13,8.43" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Stemming</head><p>We use Porter's stemming algorithm <ref type="bibr" coords="2,236.48,130.28,10.79,8.43" target="#b1">[2,</ref><ref type="bibr" coords="2,247.27,130.28,7.19,8.43" target="#b2">3]</ref> to stem the words obtained from the previous steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Feature Selection</head><p>In our spam filters, each email in the training and test sets is represented by a feature vector 〈a 1 , a 2 , …, a m 〉, where a i for i∈ <ref type="bibr" coords="2,101.80,178.16,21.00,8.43">[1,m]</ref> corresponds to a selected feature. Using selected features to describe an instance reduces the dimensionality of feature vectors. There are several methods for selecting features, such as document frequency (DF), information gain (IG), mutual information (MI), χ 2 -test (CHI), and term strength (TS). In <ref type="bibr" coords="2,356.23,200.72,10.13,8.43" target="#b6">[7]</ref>, it is reported that IG and CHI are the most effective methods, DF performs similarly. We use IG to select features based on the set of tokenized training data. Information gain measures the amount of information obtained for classification by knowing the presence or absence of a term in an email. The information gain of term t is defined as:</p><formula xml:id="formula_0" coords="2,116.44,249.59,389.31,17.62">1 1 1 0 0 0 ( ) ( ) log ( ) ( ) ( | ) log ( | ) ( ) ( | ) log ( | ) i i i i i i i i i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G t P c P c P t P c t P c t P t P c t P c t</head><formula xml:id="formula_1" coords="2,130.36,251.08,287.02,16.90">= = = = - + + ∑ ∑ ∑</formula><p>where c 0 and c 1 denote the classes of emails, i.e., ham and spam. The information gain of each unique token in the training corpus is computed. The tokens are then sorted in decreasing order of their information gain. Only the first m unique tokens are selected as features to represent emails. The number m is pre-determined in the pre-train stage. The value of m is set to 400 for submissions 1 and 4, 500 for submission 2 and 200 for submission 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Instance Representation</head><p>Given a set of selected features, 〈a 1 , a 2 , …, a n 〉, an email can be represented by a vector of feature values 〈v 1 , v 2 , …, v n 〉, where v i is the value for feature a i . The value is determined by using term frequency of the feature in the email, which is the number of times the feature (which is a token) occurs in the email.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Instance-Weighted kNN</head><p>All of our submitted spam filters use a variant of the kNN algorithm, which we refer to as instance-weighted kNN. Our submissions 1 and 3 use only the instance-weighted kNN method. Submission 2 combines the instance-weighted kNN with SpamAssassin. Submission 4 is a hybrid spam filter combining instance-weighted kNN, naïve Bayesian and random forest algorithms <ref type="bibr" coords="2,177.13,442.64,10.22,8.43" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Distance Function</head><p>We use a weighted Euclidean distance function to calculate the distance between a training instance and the instance to be classified as follows:</p><formula xml:id="formula_2" coords="2,240.16,508.86,131.45,28.35">∑ = - × = m i i i i y x w Y X d 1 2 ) ( ) , (</formula><p>where X and Y are two vectors representing two instances, m is the number of selected feature, w i is the weight of ith feature, and x i and y i are the values of the ith feature in X and Y respectively. The weight of a feature is the normalized information gain of the feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance Weights</head><p>Two weights are associated with each instance in the training set, distance weight and correctness weight. They measure the goodness of an instance in classifying a new instance and are used in the classification function of the kNN method. The correctness weight is also used in the training set maintenance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Distance Weights</head><p>The distance weight of a training instance is related to the distance between the training instance and the instance to be classified. It measures the similarity between the training instance and the instance to be classified. The closer the training instance is to the instance to be classified, the more influence the training instance should have on classifying the new instance. The distance weight is used when classifying a new instance with k nearest neighbors, which will be described in Section 3.3. We define the distance-weight of a training instance to be the inverse of the distance of the training instance to the new instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Correctness Weights</head><p>The correctness weight of a training instance reflects the historical influence of the training instance on classification of new instances. The more times a training instance gives a correct contribution to historical classifications, the higher correctness weight it obtains. The correctness weight of a training instance is initially set to 1 when the instance is added to the training set, and is updated when the instance is used in classifying a new instance. Also, the correctness weights of the training instances are preserved when re-training is performed.</p><p>The correctness-weight of an instance is updated as follows. When the instance is used as one of the k-nearest neighbors in classifying a new instance and the classification is correct, its correctness weight is updated according to whether it has made a correct contribution in the classification. If it belongs to the same class as the new instance (i.e., it has made the correct contribution), its correctness-weight is incremented by a constant; otherwise, it correctness weight is decremented by a constant. When a misclassification occurs with the training instance as one of the k nearest neighbors, the update of its correctness weight is reversed, that is, the weight is decremented when the two instances belong to the same class and incremented otherwise.</p><p>The correctness weight monitors the behavior of a training instance in classification. It is used for both training set maintenance and new instance classification. If the correctness weight of a training instance is lower than a threshold, the instance will be removed during the re-training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification Function</head><p>To classify a new instance, we first find the set K of its k nearest neighbors with the distance function described in Section 3.1. Then we compute classification scores for the two classes, spam and ham. The classifications score for spam is defined as</p><formula xml:id="formula_3" coords="3,231.28,333.65,150.51,23.02">∑ ∈ × = K spam i i DW i CW spam CS I ) ( ) ( ) (</formula><p>where i is a spam email in set K, and CW(i) and DW(i) are the correctness weight and distance weight of i, respectively. Similarly, the classification score for ham is defined as</p><formula xml:id="formula_4" coords="3,234.64,385.49,144.63,23.02">∑ ∈ × = K ham i i DW i CW ham CS I ) ( ) ( ) (</formula><p>where i is a ham email in set K. The new instance is classified into the class with the higher classification score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incremental Training and Re-training</head><p>Whenever a new email is classified, the correctness weights of the k nearest neighbors are updated and the new email with its correct class label is added into the training set to include as much information about a personalized email stream as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Maintenance of Training Set</head><p>To obtain a reasonable speed for classification with kNN, the size of the training data is maintained at a fixed range. A window size specified at the initialization phase defines the range. When the size of the training data exceeds the window size, the training instances with a correctness weight less than a threshold are removed. If no instance has a correctness weight less than the threshold, the oldest instances are removed to guarantee the training data set falls within the window size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Re-training</head><p>For our kNN method, re-training is to re-select features based on the updated training data set and then represent each instance using the newly selected features. This is to reflect the evolvement of spam and ham characteristics so that the training data will be more adaptive to the test email stream than to the original training data collected by us. Note that the correctness weight of each training instance is kept during the re-training process so that classification history of each case can still be used later. For kNN, there is no need to re-train a model after re-selecting the features. But for other learning methods used in our submission 4, such as naïve Bayesian, a new model is trained after re-selecting the features. Since re-training is expensive, it cannot be performed very frequently due to the limit on classification time. In our submissions, the re-training frequency is once every 800 classifications for submission 1 and once every 1000 classifications for submissions 2, 3 and 4.</p><p>including header and text analysis, heuristic rules, Bayesian filtering, DNS blocklists, and collaborative filtering databases to filter spams. In our submission 2, we first employ SpamAssassin to produce a result and then use the instance-weighted kNN method to generate another result. The two results are combined together to make a final decision. In this integration, the kNN method makes use of the MIME parsing and tokenization results from SpamAssasin instead of using our own MIME parsing and tokenization methods.</p><p>Based on our preliminary experimental results, the instance-weighted kNN has better false negatives than SpamAssassin, but worse false positives. Therefore, we combine the result of SpamAssassin and the instance-weighted kNN as follows. If the classification result from kNN is spam, the score outputted from SpamAssassin is incremented by 1. If the incremented score is greater than 5, the instance is classified into spam; otherwise, the result is ham. If the result from kNN is ham, the result from SpamAssassin is used as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Integrating kNN, Naïve Bayesian and Random Forest Algorithms</head><p>Our submission 4 is a hybrid spam filter that combines decisions from the instance-weighted kNN, naïve Bayesian classifier and the random forest classifier <ref type="bibr" coords="4,243.23,224.48,10.05,8.43" target="#b0">[1]</ref>. The implementation of naïve Bayesian and random forest classifiers is taken from Weka <ref type="bibr" coords="4,152.19,235.76,10.23,8.43" target="#b4">[5,</ref><ref type="bibr" coords="4,162.42,235.76,6.82,8.43" target="#b5">6]</ref>, an open source data mining software package. When combing the results from individual classifiers, each classifier is equally weighted and the majority class label is returned as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>The submissions were evaluated on four data sets: full, mrx, sb, tm. The first one is a public data set and the last three are private data sets. The sizes of the data sets are shown in Table <ref type="table" coords="4,332.33,298.88,3.45,8.43" target="#tab_0">1</ref>. The results of our submissions in terms of the ham misclassification rate (ham%) and the spam misclassification rate (spam%) are shown in Table <ref type="table" coords="4,438.99,310.16,3.52,8.43" target="#tab_1">2</ref> Based on Table <ref type="table" coords="4,157.63,593.12,3.45,8.43" target="#tab_1">2</ref>, the best ham% is achieved by submission 2, which is the combination of instance-weighted kNN and SpamAssassin, and the second best one is submission 3, an instance-weighted kNN. The best spam% is achieved by submission 1, an instance-weighted kNN. The second best one in terms of spam% is submission 3.</p><p>Based on the ROC curves (not shown in this paper due to the space limitation), submission 2 achieves the best results on all the three private data sets. For the public data set (full), submission 2 achieves the best results in the low and medium ham misclassification rate (ham%) regions. In the high region of ham%, submission 4 is the best, which is the hybrid model combining kNN, naïve Bayesian and random forest. For the private data sets, the ROC curves for submissions 1 and 3 are very close. But for the public data set (full), submission 3 is much better than submission 1. Both submissions 1 and 3 use a single instance-weighted kNN algorithm. They differ in tokenization, re-training frequency, and the number of selected features, which indicates that data preprocessing techniques could greatly affect the performance of spam filters on some datasets, but not always. The performance of submission 4 in terms of ROC curves is better than submissions 1 and 3 on two private datasets, but worse than them on the other private set. For the public data set, submission 4 has similar performance with submission 3 and better than submission 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,120.40,310.16,373.48,240.51"><head>Table 1 . Test Data Sets</head><label>1</label><figDesc>.</figDesc><table coords="4,120.40,338.00,373.48,212.67"><row><cell></cell><cell></cell><cell cols="2">Data set Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>full</cell><cell cols="2">92,189 messages</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>mrx</cell><cell cols="3">48,000 private messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>sb</cell><cell cols="3">7,000 private messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>tm</cell><cell cols="3">170,000 private messages</cell><cell></cell><cell></cell><cell></cell></row><row><cell>submissions</cell><cell cols="2">yorSPAM1</cell><cell cols="2">yorSPAM2</cell><cell cols="2">yorSPAM3</cell><cell cols="2">yorSPAM4</cell></row><row><cell>datasets</cell><cell cols="8">Ham% Spam% Ham% Spam% Ham% Spam% Ham% Spam%</cell></row><row><cell>full</cell><cell>2.44</cell><cell>2.43</cell><cell>0.92</cell><cell>1.74</cell><cell>1.29</cell><cell>1.2</cell><cell>2.99</cell><cell>1.36</cell></row><row><cell>mrx</cell><cell>4.96</cell><cell>0.55</cell><cell>0.34</cell><cell>1.03</cell><cell>4.41</cell><cell>0.65</cell><cell>5.20</cell><cell>0.45</cell></row><row><cell>sb</cell><cell>1.19</cell><cell>13.81</cell><cell>0.14</cell><cell>23.64</cell><cell>1.36</cell><cell>15.32</cell><cell>0.77</cell><cell>91.35</cell></row><row><cell>tm</cell><cell>0.82</cell><cell>8.28</cell><cell>0.25</cell><cell>14.90</cell><cell>0.92</cell><cell>8.11</cell><cell>0.63</cell><cell>9.04</cell></row><row><cell>average</cell><cell cols="5">2.3525 6.2675 0.4125 10.3275 1.995</cell><cell>6.32</cell><cell>2.3975</cell><cell>25.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,193.48,567.08,227.29,8.43"><head>Table 2 . Results of our Submissions on the Test Data Sets</head><label>2</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,100.24,694.77,269.18,10.15;3,96.76,715.64,437.49,8.43"><p>Integrating Instance-Weighted kNN with SpamAssassinSpamAssassin is a mature and widely-deployed open source spam filter<ref type="bibr" coords="3,388.64,715.64,10.04,8.43" target="#b3">[4]</ref>. It uses a variety of mechanisms</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Conclusions</head><p>Based on the results, we conclude that integrating a variety of spam filtering techniques (such as in our submission 2) generally achieve better results than using a single method. Also, data preprocessing techniques, such as tokenization techniques, are also important to the performance of spam filtering.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,93.40,186.08,289.15,8.43" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,143.18,186.08,58.05,8.43">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,207.25,186.08,67.22,8.43">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.28,197.36,296.87,8.43" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,142.66,197.36,122.28,8.43">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,270.96,197.36,30.31,8.43">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.28,208.64,359.91,8.43" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://www.tartarus.org/~martin/PorterStemmer/" />
		<title level="m" coord="5,142.66,208.64,120.00,8.43">The Porter Stemming Algorithm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.45,219.92,179.41,8.43" xml:id="b3">
	<monogr>
		<ptr target="http://spamassassin.apache.org/" />
		<title level="m" coord="5,93.45,219.92,52.11,8.43">SpamAssassin</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.40,231.20,414.86,8.43;5,79.96,242.48,123.06,8.43" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,193.07,231.20,279.47,8.43">Data Mining: Practical machine learning tools and techniques, 2nd Edition</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.40,253.76,296.69,8.43" xml:id="b5">
	<monogr>
		<ptr target="http://www.cs.waikato.ac.nz/ml/weka/" />
		<title level="m" coord="5,93.40,253.76,144.31,8.43">Weka 3: Data Mining Software in Java</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.40,265.04,406.75,8.43;5,79.96,276.32,47.76,8.43;5,127.72,274.24,4.73,5.48;5,134.92,276.32,234.08,8.43" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,190.84,265.04,247.05,8.43">A Comparative Study on Feature Selection in Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,444.18,265.04,55.97,8.43;5,79.96,276.32,47.76,8.43;5,127.72,274.24,4.73,5.48;5,134.92,276.32,112.97,8.43">Proceedings of ICML-97 14 th Int Conf on Machine Learning</title>
		<meeting>ICML-97 14 th Int Conf on Machine Learning<address><addrLine>Nashville, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
