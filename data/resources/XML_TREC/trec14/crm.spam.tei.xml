<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,219.90,68.70,172.26,25.64;1,161.90,112.40,288.34,20.64">CRM114 versus Mr. X: CRM114 Notes for the TREC 2005 Spam Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,101.50,165.45,70.18,18.02"><forename type="first">Fidelis</forename><surname>Assis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Empresa Brasileira de Telecomunicações -Embratel</orgName>
								<address>
									<settlement>Rio de Janeiro</settlement>
									<region>RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.80,165.45,97.58,18.02"><forename type="first">William</forename><surname>Yerazunis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.50,165.45,93.08,18.02"><forename type="first">Christian</forename><surname>Siefkes</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">BerlinBrandenburg Graduate School in Distributed Information Systems</orgName>
								<orgName type="laboratory">Database and Information Systems Group</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.70,165.45,105.28,18.02"><forename type="first">Shalendra</forename><surname>Chhabra</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mitsubishi Electric Research Laboratories</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,219.90,68.70,172.26,25.64;1,161.90,112.40,288.34,20.64">CRM114 versus Mr. X: CRM114 Notes for the TREC 2005 Spam Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BF083C460B11ADCEFEBFFF0E72D94347</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper discusses the design decisions underlying the CRM114 Discriminator software, how it can be configured as a spam filter, and what we may glean from the preliminary TREC 2005 results. Unlike most other filters, CRM114 is not a fixedpurpose antispam filter; rather, it's a general purpose language meant to expedite the creation of text filters. The pluggable CRM114 architecture allows rapid prototyping and easy support of multiple classifier engines; rather than testing different cutoff parameters, the CRM114 TREC test set tested different classifier algorithms and learning protocols.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Introduction and History: The CRM114 Discriminator was initially begun as a rough design in 1998 as a hypothetical filter design, meant to categorize incoming mail into a significant number of different "topics" (and with spam as an acknowledged target, but by no means the sole target). One of the design assumptions that CRM114 has held from day 0 has been that single word features are not as important as Nfeature tuples. The initial CRM114 test code worked on Nletter tuples, rather than N word tuples, but the groundwork had been laid. Testing against the very poorly camouflaged spam of 1999, the lettertuple code showed an accuracy in excess of 98%, providing convincing evidence that the concepts of tuple features being substantially superior to singletoken features and that a "no preconceived notions" machinelearning classifier could converge to superior accuracy over any humandesigned heuristic system.</p><p>Work on CRM114 progressed past the initial EMACS macros and shell scripts that proved the concept to a Ccoded multiclass classifier in 2000. With this came the realization that most of the work of filtering and categorizing email was not in the classifier itself it was in massaging the inputs and outputs to conform to the vary large number of individual configurations, mail transfer agents, mail delivery agents, mail user agents, and user desires. Rather than wrap the learning and classification functions inside voluminous shell scripts, an initial look at making a Perl module that performed the N tuple, Nclass classification was made. However, Perl being what it was, the idea of a standalone language made for manipulating the long overlapping strings needed for text classification became much more tenable. The first general public release of CRM114 under the GNU General Public License (GPL) occurred in early 2001, although "friends and family" releases had occurred since the beginning of the project.</p><p>This initial release has been followed by "whenever it's time" releases ever since. Typically CRM114 major releases occur three to six times a year, with bugfix and "secret new feature" releases being done roughly monthly (or whenever a significant advance occurs). The releases are coded with both a date of release, and a nickname; the nickname often indicates the major contributor or motivator for that release. New releases have included multiple classifier engines (the current release has seven different classifiers built in) as well as major improvements such as TRAP/FAULT support, a JIT compiler, a preprocessor, and full 8bitsafe data paths.</p><p>Overview of the CRM114 language: The CRM114 language is described in depth in the manual CRM114 Revealed [Yerazunis 2005], available for free download from the CRM114 web page at http://crm114.sourceforge.net. The language is a command based language with a declensional (rather than positional) syntax, and only one data type -the overlapping string. All CRM114 functions are designed to operate efficiently on strings that may contain other partially overlapping strings; this includes efficient reclamation of unused string space. The language contains only overlapping strings; there are no specific constructs for numbers, pointers, continuations, coroutines, open files, or anything else except overlapping strings.</p><p>The CRM114 language contains specific constructs for stringoriented matching, alteration, iteration, decision, and translation, however in the current context the most important statements are the LEARN and CLASSIFY statements. LEARN and CLASSIFY accept a uniform set of parameters and properly map those parameters onto whichever classifier is requested. Thus, it's possible to change classifiers in a CRM114 program (or even compare different classifiers, such as a Bayesian and a Winnow, or a Markovian and a Hyperspatial) without having to change the rest of the support structure.</p><p>Most of the CRM114 classifiers start with the concept of a word tuple as the basic feature to be considered. The tuple is defined as a series of words, and a word is defined as whatever matches a programmersupplied regex (the default is that a word is one or more printable characters surrounded by whitespace). CRM114 kits include the very efficient TRE approximate regex engine <ref type="bibr" coords="2,506.60,447.31,48.63,14.46;2,56.70,461.81,20.44,14.46">[Laurikari 2001</ref><ref type="bibr" coords="2,77.14,461.81,81.66,14.46">][Laurikari 2004</ref>] allowing a large amount of freedom in defining a word.</p><p>Structure of a CRM114 classifier: CRM114 classifiers are designed to be "pluggable"; a new classifier can be written in as little as a few hours by using a basic classifier code as a skeleton. A classifier is given a pointer to the unknown text, the unknown text's length, and a pointer to an argument parameter block containing any flags or worddefinition regexes supplied; the classifier returns a condition code and optionally a textual representation of the results of the classification. Almost all of the CRM114 classifiers use tuplebased features. That is, as words are tokenized by the regex engine, they are recombined into a series of short phrases of between one and five words long. The recombination sequence varies with the classifier and with the classifier flags; in newer versions of CRM114 any classifier can be configured to use single words as features with the &lt;unigram&gt; keyword; in the case of classifiers using Bayes rule this results in a Grahamstyle Bayesian filter (albeit one with an unlimited "peak window").</p><p>A more detailed description of the CRM114 classifiers may be found in <ref type="bibr" coords="2,414.20,678.81,70.90,14.46">[Chhabra 2004</ref>] and <ref type="bibr" coords="2,515.70,678.81,39.27,14.46;2,56.70,693.21,20.89,14.46">[Siefkes 2004</ref>]; we will give only a basic description here.</p><p>The original CRM114 classifier was the Markovian classifier; this classifier uses a Sparse Binary Polynomial Hashing (SBPH) feature generator that uses all inorder sequences of up to five words to form feature phrases. For example, the sentence "TREC is sponsored by NIST" will generate the following feature phrases: TREC TREC is TREC &lt;skip&gt; sponsored TREC is sponsored TREC &lt;skip&gt; &lt;skip&gt; by TREC is &lt;skip&gt; by TREC is sponsored by TREC &lt;skip&gt; &lt;skip&gt; &lt;skip&gt; NIST TREC is &lt;skip&gt; &lt;skip&gt; NIST TREC &lt;skip&gt; sponsored &lt;skip&gt; NIST TREC is sponsored &lt;skip&gt; NIST TREC &lt;skip&gt; &lt;skip&gt; by NIST TREC is &lt;skip&gt; by NIST TREC is sponsored by NIST All sixteen of these phrases are then used as features; in the Markovian filter they are weighted unequally; longer phrases are given more weight than short ones <ref type="bibr" coords="3,369.30,360.21,76.26,14.46">[Yerazunis 2004</ref>].</p><p>The OSB, OSBF, Winnow, and Hyperspace classifiers all use a simpler feature generator which (paradoxically) simultaneously gives higher performance and higher accuracy. This feature generator is called OSB, for Orthogonal Sparse Bigram; it generates only four features instead of sixteen. Using the same example text "TREC is sponsored by NIST", the features are:</p><p>TREC is TREC &lt;skip&gt; sponsored TREC &lt;skip&gt; &lt;skip&gt; by TREC &lt;skip&gt; &lt;skip&gt; &lt;skip&gt; NIST Note that the unigram feature (single words taken one at a time, in isolation) is not used in the OSB feature set; extensive testing shows that presence of the unigram does not improve results; in fact, it sometimes causes a paradoxical decrease in accuracy.</p><p>After generating the features, a lookup is done into the stored statistics files. In order to accelerate this lookup, CRM114 does not use a generalpurpose database such as MySQL or PostgreSQL. Instead, the feature is converted to a 64bit hash, and that hash value used as the starting index for searching an in memory hash table index. On the average, this lookup takes only six adjacent 32bit reads from memory (assuming that the feature hasn't been seen before and isn't in the CPU L1 cache). Almost all CRM114 classifiers use this hashing scheme to accelerate feature lookups. As a secondary effect, the actual text is not stored; this gives a modicum of security to the statistics files (although individual features would fall quickly to a dictionary attack, the sequences would be more difficult, and of course, it would be impossible to recover any particular learned text with certainty.).</p><p>Once the individual feature lookups are performed, the counts are multiplied by the appropriate feature weight and an appropriate set of conversion and combining rules are used to convert individual feature frequencies into a final judgment. In Markovian, OSB, and Bayesian operation which by default use a Bayes Rulelike combining law, the conversion from frequencyofoccurrence to a priori probability is:</p><formula xml:id="formula_0" coords="4,120.70,144.00,368.13,28.84">P local = 0.5  in_class occurrences -out_of_class occurrences 16 × total occurrences  1</formula><p>Note that this produces a local probability that is strongly biased toward 0.5 (that is, toward uncertainty). These probabilities are weighted and combined via this modified Bayes chain rule to yield the output probabilities:</p><formula xml:id="formula_1" coords="4,189.10,272.91,233.59,37.32">P final_class i = P initial_class i × P local_class i ∑ j P initial_class j × P local_class j</formula><p>These probabilities are then normalized. Additionally, two manipulations are performed -to avoid floatingpoint underflow, the value of the largest probability is renormalized at each step from the sum of the smaller probabilities, and for the purposes of humanreadable output, the result is converted to pR; pR is the base10 log of the probability ratio, and given IEEE floatingpoint inputs, varies from 340 to +340. This allows expression of a very large range of numbers in human readable terms. CRM114 also has some support for a chisquared decision rule, however that support was not used in the current NIST TREC test set.</p><p>Filter Configurations As Tested: The four filter configurations for CRM114 were OSBF, Winnow, OSB Unique, and "plain" OSB. Because CRM114 is a language, not just a filter, changing between these four variants only requires setting a parameter variable to the CLASSIFY and LEARN statements. The version of CRM114 used was CRM11420050518.BlameMercury, (except for the Winnow version, which uses the Java implementation created by Christian Siefkes, available at http://www.inf.fuberlin.de/inst/agdb/software/tie/ because of a suspected minor bug in the CRM114 implementation that causes very slight discrepancies between what should be identical programs).</p><p>Additionally, all four variants enabled some form of "microgrooming", a method of aging out old data from the statistics files to keep the statistics files from growing without bound. Microgrooming is based on the concept that when it is necessary to make room in the database file, and given two seldomseen features, one old and one new, get rid of the older one first as it has a longer history of not being useful. Because CRM114's hash storage structure uses linear intable overflow chaining, this information is indirectly available as the distance each feature is actually found in, compared to its nominal location (i.e. the further down the overflow chain that we actually find a feature compared to where the feature would have been placed if no overflow was present, the newer the feature is.). The Java Winnow system used exact explicit rather than implicit approximate aging; results are similar.</p><p>All four filter configurations use a phrasecreation window of length 5 and OSBstyle features. By using these Nword tuples, OSB features provide a Markov Random Field model rather than a Bayesian model of the texts learned <ref type="bibr" coords="5,231.30,71.21,68.36,14.46">[Chhabra 2004</ref>].</p><p>None of the CRM114 filters use any preprocessing whatsoever. There is no stop word removal, no tagging of items in the headers versus those in the body, and no decoding of any type, not even MIME decoding. All four classifiers operated on the "as seen" texts, without any further interpretation, and the default word defining regex was used <ref type="bibr" coords="5,263.60,143.41,113.50,14.46">(specifically, [[:graph:]</ref>]+ defines a word except in the Java Winnow code which used the X tokenizer from <ref type="bibr" coords="5,290.90,157.91,63.69,14.46">[Siefkes 2004</ref>]; the X tokenizer is XML/HTMLaware and is expressed as [^\p{Z}\p{C}][/!?#]?[\p{L}\p{M}\p{N}]*(?:["'=;]|/?&gt;|:/*)? using Unicode categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptions of individual filters:</head><p>We will now describe the four configurations tested.</p><p>OSBF: OSBF is a variant of OSB that uses an empirically derived confidence factor expressing the class separation power of a feature to influence the local perfeature probability. The local perfeature probability is first estimated as the ratio of the number of documents containing the feature among the total learned in the class. The confidence factor is also combined with an inverseexponential weighting as distance increases in the bigram feature. Beyond that, OSBF uses Bayes rule to combine the local feature probability (a gross abuse of Bayes' assumptions of independence that most filter authors simply live with). Unlike most Bayesian spam filters, there is no "top 10" window of the most extreme values; all values are used, no matter how insignificant. The training was configured so that if a text did not score at least +10 pR units in the correct class, then the text was trained, whether or not it actually was classified correct by some narrow margin. The first 500,000 characters of each text were used.</p><p>Unfortunately, the version of OSBF used in these tests contained a significant bug in the chain rule, hence the values reported below are much worse than the algorithm is capable of<ref type="foot" coords="5,447.90,403.43,3.50,8.44" target="#foot_0">1</ref> .</p><p>WINNOW: WINNOW is an implementation of Nick Littlestone's Winnow algorithm. Winnow is comparable to a backpropagating perceptron individual features start out with a weight of 1.0000 and are multiplicatively promoted or demoted according to whether the feature is found or not found in a particular text being learned. Instead of Bayes rule, WINNOW uses a simple summing of the weights; the class with the highest weight count wins. Our Winnow implementation uses a thick threshold for learning. Documents are trained not only in case of mistakes, but also if the determined score was near the threshold. This makes the classifier more robust when classifying borderline instances. See <ref type="bibr" coords="5,515.50,519.01,39.37,14.46;5,56.70,533.51,22.40,14.46">[Siefkes 2004</ref>] for a detailed description.</p><p>OSB and OSB Unique: OSB and OSB Unique use the basic local probability formula above without confidence factors; like OSBF they use all local probability values (there is no extremum window). Feature counts are weighted in an empirically derived decreasing sequence. There is only one difference between OSB and OSB Unique: whether a repeated feature is allowed to further influence the output or is simply discarded. Among all tested classifiers, only OSB uses all repeated features. OSB Unique discards repeated features, as do OSBF and Winnow. In both OSB and OSB Unique, a singlesided thick threshold for training of +20 pR units was used; any text not scoring at least +20 pR units in the correct class was trained. Only first 4096 characters of each text were used for classification; this is the default for one of the more commonly used CRM114 mail filters deployed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the TREC preconference partial release results:</head><p>The following four tables show how CRM114 fared with respect to the preconference data release. 1ROCAC% and Average LAM% are only prepublished as median values; final LAM% is not prepublished at all (note that unlike some filters, none of the CRM114 configurations contained any preloaded learning. Everything was learned strictly on the fly.) Despite this "no preload", all of CRM114's classifiers beat both the prepublication median 1ROCAC% and LAM%, often by an order of magnitude.</p><p>The design of most CRM114 classifiers assumes a "balanced error" configuration that all misclassifications are equally bad. Although common wisdom is that a false reject is more dangerous than a false accept, at least one author considers this incorrect. Consider the situation of a welldone phishing spam being falsely accepted. Then, consider it being believed by a nontechnical computer (say, a grandmother); the likely resulting financial costs of this one false accept will approach if not exceed the likely financial costs of a false reject of a typical legitimate financial notification. Thus, to a first approximation, we consider that all misclassifications are roughly equally costly and an equal errorrate configuration is a good loss minimization guideline.</p><p>To examine the behavior of filters operated in this equalerror configuration, we consider two "sweet spots" that our experience show are reasonably representative of equalerror operation specifically, what the error rates are for spam when the good error rate is fixed at 1%, and what the error rate for good is when the spam error rate is fixed at 1%. These representative "sweet spots" are shown as the last two columns in the charts. The "sweet spot" column header shows three values -the best value for any filter tested, the median value, and the worst value. A sweet spot entry in BOLD TYPE is used to highlight where CRM114 performance was either "best in class" or "statistically indistinguishable at the 95% confidence level from best in class" over the entire set of 44 filter configurations tested.</p><p>As shown below, for every corpus, and for every "sweet spot", at least one of the four CRM114 configurations was either "best", or statistically indistinguishable from "best". Interestingly, no one configuration of CRM114 was consistently best. This may well be a manifestation of the No Free Lunch theorem in decision and optimization <ref type="bibr" coords="6,272.00,490.21,68.70,14.46" target="#b1">[Wolpert 1997</ref>] </p><formula xml:id="formula_2" coords="6,56.70,519.11,24.73,14.33">MRX</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FULL corpus:</head><p>The FULL corpus is a fairly balanced corpus. It contains 39K good emails and 53K spams (that's 42% good email). In this corpus, the plain OSB (that is, a tuplebased Bayesian filter, using no prefiltering or other "gimmicks", was the best filter tested across all TREC submissions, for both sweet spots. Interestingly, omitting replicated features degraded filter performance to barely statistically significant levels (the error bars still overlap, but the central values are not covered in the opposite configuration's error bars.). Note that in this corpus, OSB beats OSB Unique in everything but 1ROCAC% (0.049 versus 0.042) -keep this in mind as you read the next corpus.  Conclusions: As can be seen, one or more of CRM114's classifiers was either the best or statistically indistinguishable from the best in all eight sweet spots (4 corpora x 2 sweet spots/corpus). However, interesting inversions occurred in things like 1ROCAC% and rate of learning. It seems that the No Free Lunch theorem <ref type="bibr" coords="9,256.00,56.71,68.70,14.46" target="#b1">[Wolpert 1997</ref>] is alive and well with a vengeance in the spam filtering world; statistically significant variation exists with even simple changes like whether to allow repeated features as classifier input or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FULL</head><p>The design decision to make CRM114 a "learning" system versus a "prelearned" system seems to be justified; at least one CRM114 classifier learned fast enough to win every 1% sweet spot in every corpus.</p><p>The design decision to use a hashbased statistics system rather than a database seems to have been justified; because the hashbased system is so fast, CRM114 developers can feasibly test hundreds of variations when other developers can only test two or three different ideas.</p><p>The design decision to make CRM114 a language rather than a singlepurpose spamfilter tool seems to been justified; because the language allows easy creation of multiple coexisting variants that can be tested against each other easily.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,56.70,519.11,498.60,72.26"><head></head><label></label><figDesc>Corpus: The MRX ( Mr. X) corpus is a "single person" corpus sponsored by Prof. Gordon Cormack of Waterloo University. It is biased toward spam -it contains about 9K good emails and 40K spams: (about 18% good emails). Here we see that Winnow, operating with the OSB feature set, was statistically indistinguishable in performance with the best of the 44 filter configurations submitted to TREC. It also had an impressively small 1ROCAC% of just 0.051.</figDesc><table coords="7,64.90,59.51,479.29,226.96"><row><cell>MRX</cell><cell>1ROCAC%</cell><cell>AVG LAM%</cell><cell>Final</cell><cell>Ham 1.0%</cell><cell>Spam 1.0%</cell></row><row><cell>corpus</cell><cell>median= 1.613</cell><cell>median = 2.53</cell><cell>LAM%</cell><cell>0.34 24.24 99.65</cell><cell>0.37 13.35 99.16</cell></row><row><cell>OSBF</cell><cell>0.311</cell><cell>1.46</cell><cell>1.4</cell><cell>3.85</cell><cell>2.85</cell></row><row><cell></cell><cell>(0.244 0.397)</cell><cell>(1.34 1.59)</cell><cell>(1.13 1.62)</cell><cell>(2.82 5.22)</cell><cell>(2.49 3.27)</cell></row><row><cell>OSB Winnow</cell><cell>0.051 (0.035 0.075)</cell><cell>0.60 (0.53 0.68)</cell><cell>0.24 (0.17 0.33)</cell><cell>0.43 (0.31 0.62)</cell><cell>0.51 (0.37 0.69)</cell></row><row><cell>OSB Unique</cell><cell>0.177 (0.128 0.246)</cell><cell>0.98 (0.85 1.13)</cell><cell>0.23 (0.12 0.42)</cell><cell>1.07 (0.86 1.35)</cell><cell>1.07 (0.83 1.39)</cell></row><row><cell>OSB</cell><cell>0.218 (0.157 0.304)</cell><cell>0.79 (0.69 0.90)</cell><cell>0.34 (0.23 0.52)</cell><cell>0.63 (0.50 0.79)</cell><cell>0.56 (0.42 -0.75)</cell></row><row><cell>OSBF (fixed)</cell><cell>0.078 (0.050 0.124)</cell><cell>0.52 (0.45 -0.60)</cell><cell>0.22 (0.15 -0.32)</cell><cell>0.29 (0.20 0.42)</cell><cell>0.51 (0.39 0.67)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,56.70,449.21,498.53,283.26"><head></head><label></label><figDesc>The SB corpus is a smaller corpus with 6K good emails and only 775 spams (88% good); thus it is heavily weighted toward filters with a prejudice to accept borderline emails as good. Both OSB and OSB Unique did very well in this corpus -both are statistically indistinguishable from the best of the 44 filters submitted to TREC. However, OSB Unique seems to have a slight edge in 1 ROCAC%, LAM%, and Final LAM%. Note that this is an inversion from the FULL corpus, where OSB was better than OSB Unique. The TM corpus is also highly biased toward nonspam emails it contains 150K good emails and 19K spams (88% good emails). Here OSB is statistically clearly better than OSB Unique; the error bars don't even overlap. What's also of interest is that Winnow beats OSB in 1ROCAC%, LAM%, and ties it in Final LAM%, yet the 95% confidence interval error bars for OSB and Winnow barely touch, with OSB being "better" in both sweet spots (apparently equal to the best of all 44 filters submitted to TREC.)</figDesc><table coords="7,56.70,449.21,485.59,254.23"><row><cell>corpus OSBF OSB Winnow OSB Unique OSB OSBF (fixed) OSBF OSB Winnow OSB Unique OSB OSBF (fixed) TM Corpus: TM corpus OSBF OSB Winnow OSB Unique OSB SB Corpus: SB corpus OSBF (fixed)</cell><cell>1ROCAC% median= 0.861 0.169 (0.151 0.190) 0.122 (0.102 0.145) 0.042 (0.031 0.056) 0.049 (0.035 0.068) 0.019 (0.015-0.025) 1ROCAC% median= 2.845 2.393 (1.689 3.382) 1.888 (1.315 2.704) 0.231 (0.142 0.377) 0.393 (0.203 0.759) 0.556 (0.316 0.975) 1ROCAC% median= 2.712 0.790 (0.720 0.868) 0.166 (0.138 0.201) 0.195 (0.160 0.238) 0.272 (0.225 0.329) 0.120 (0.104 0.140)</cell><cell>LAM% median = 2.02 1.74 (1.66 1.83) 0.73 (0.68 0.79) 0.63 (0.56 0.70) 0.47 (0.43 0.52) 0.42 (0.38 0.47) LAM% median = 4.79 2.34 (1.87 2.93) 2.14 (1.68 2.73) 1.64 (1.28 2.10) 1.67 (1.32 2.12) 1.65 (1.30 2.11) LAM% median = 2.54 2.44 (2.33 2.56) 0.79 (0.74 0.85) 1.08 (1.01 1.14) 0.83 (0.77 0.89) 0.90 (0.84 0.96)</cell><cell>Final LAM% 3.6 (3.28 3.97) 0.86 (0.72 1.02) 0.43 (0.33 0.56) 0.37 (0.29 0.47) 0.41 (0.32 0.52) Final LAM% 0.91 (0.56 1.47) 1.2 (0.73 1.88) 0.59 (0.34 1.03) 0.44 (0.24 0.80) 0.37 (0.18 0.74) Final LAM% 2.5 (2.28 2.80) 0.46 (0.39 0.54) 0.68 (0.59 0.78) 0.46 (0.39 0.54) 0.56 (0.48 0.66)</cell><cell>Ham 1.0% 0.23 10.35 94.44 3.14 (2.81 3.50) 0.68 (0.59 0.79) 0.35 (0.30 0.42) 0.23 (0.19 0.27) 0.25 (0.20 0.30) Ham 1.0% 3.48 27.23 100.00 8.77 (7.05 10.88) 10.32 (8.32 12.74) 3.48 (2.26 5.32) 3.74 (2.32 5.97) 4.00 (2.75 5.78) Ham 1.0% 1.04 10.28 99.06 10.28 (9.71 10.89) 1.30 (1.15 1.46) 1.58 (1.40 1.78) 1.04 (0.92 1.16) 1.17</cell><cell>Spam 1.0% 0.15 14.20 98.11 3.15 (2.94 3.38) 0.51 (0.42 0.62) 0.21 (0.16 0.27) 0.15 (0.11 0.20) 0.19 (0.15 0.24) Spam 1.0% 3.79 63.76 99.52 97.11 (57.03 99.88) 62.35 (37.84 81.84) 4.01 (2.40 6.63) 5.62 (1.99 14.86) 12.69 (3.47 37.02) Spam 1.0% 1.07 47.70 99.40 16.93 (13.00 21.74) 1.86 (1.34 2.57) 1.71 (1.49 1.97) 1.07 (0.86 1.34)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,66.40,698.66,488.58,12.05;5,56.70,711.26,497.84,12.05;5,56.70,723.26,437.80,12.05"><p>After TREC tests, a fixed version of OSBF was submitted for an extra evaluation with results comparable to the best classified participants. The new results were added as the last row in the tables ahead, for easy comparison. The graphs are available from http://plg.uwaterloo.ca/~gvcormac/spamtestlog/ or http://osbflua.luaforge.net/TREC/results/.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,56.70,302.21,56.70,14.33;9,56.70,330.91,498.50,14.46;9,56.70,345.31,498.30,14.46;9,56.70,359.81,320.10,14.46;9,56.70,388.71,498.33,14.46;9,56.70,403.11,497.50,14.46;9,56.70,417.61,464.00,14.46;9,56.70,446.51,406.24,14.46;9,56.70,460.91,161.34,14.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,482.33,330.91,72.87,14.46;9,56.70,345.31,346.87,14.46;9,61.56,388.71,154.74,14.46;9,231.81,388.71,323.21,14.46;9,56.70,403.11,240.78,14.46">NFAs with Tagged Transitions, their Conversion to Deterministic Automata and Application to Regular Expressions</title>
		<author>
			<persName coords=""><forename type="first">Shalendra</forename><surname>Chhabra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Yerazunis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Siefkes</surname></persName>
		</author>
		<ptr target="http://laurikari.net/tre/index.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,430.40,345.31,124.60,14.46;9,56.70,359.81,285.88,14.46;9,310.90,403.11,243.30,14.46;9,56.70,417.61,112.53,14.46">Proceedings of the Fourth IEEE International Conference on Data Mining (ICDM &apos;04)</title>
		<meeting>the Fourth IEEE International Conference on Data Mining (ICDM &apos;04)<address><addrLine>Chhabra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2004. 2004. September 2000</date>
		</imprint>
	</monogr>
	<note>Symposium on String Processing and Information Retrieval (SPIRE 2000). Laurikari 2004] Laurikari, Ville, TRE approximate regex library -free</note>
</biblStruct>

<biblStruct coords="9,144.20,489.51,410.87,14.46;9,56.70,504.01,497.50,14.46;9,56.70,518.41,496.94,14.33;9,56.70,532.81,54.03,14.33;9,127.60,532.81,48.63,14.33;9,193.00,532.81,9.40,14.33;9,219.20,532.81,50.67,14.33;9,286.70,532.81,43.90,14.33;9,347.40,532.81,51.30,14.46;9,415.60,532.81,27.00,14.46;9,476.30,532.81,50.00,14.46;9,543.10,532.81,12.04,14.46;9,56.70,547.21,249.10,14.46;9,56.70,576.11,498.49,14.46;9,56.70,590.61,498.54,14.46;9,56.70,605.01,24.00,14.46;9,56.70,633.91,498.20,14.46;9,56.70,648.41,498.24,14.46;9,56.70,662.81,233.59,14.46;9,56.70,691.71,498.40,14.46;9,56.70,706.21,498.63,14.46;9,56.70,720.61,204.64,14.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,62.64,504.01,424.52,14.46;9,399.07,576.11,156.13,14.46;9,56.70,590.61,61.46,14.46;9,293.30,691.71,261.80,14.46;9,56.70,706.21,395.63,14.46">Revealed -Or How I learned To Stop Worrying and Trust My Automatic Monitoring Systems , this is the complete CRM114 manual</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Siefkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fidelis</forename><surname>Assis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shalendra</forename><surname>Chhabra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Yerazunis ; David</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Yerazunis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Yerazunis</surname></persName>
		</author>
		<ptr target="http://crm114.sourceforge.net" />
	</analytic>
	<monogr>
		<title level="m" coord="9,506.80,504.01,47.40,14.33;9,56.70,518.41,496.94,14.33;9,56.70,532.81,54.03,14.33;9,127.60,532.81,48.63,14.33;9,193.00,532.81,9.40,14.33;9,219.20,532.81,50.67,14.33;9,286.70,532.81,37.63,14.33;9,250.91,633.91,303.99,14.46;9,56.70,648.41,122.65,14.46;9,198.40,648.41,156.83,14.46">European Conference on Machine Learning (ECML) / European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD)</title>
		<imprint>
			<date type="published" when="1997-04">September 2004. Wolpert 1997. April 1997. Yerazunis 2004. 2004. Yerazunis 2005</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Combining Winnow and Orthogonal Sparse Bigrams for Incremental Spam Filtering</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
