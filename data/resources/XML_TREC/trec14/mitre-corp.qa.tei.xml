<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.80,60.32,206.33,15.84">MITRE&apos;s Qanda at TREC-14</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.36,79.70,67.19,11.96"><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
						</author>
						<author>
							<persName coords="1,326.16,79.70,48.48,11.96"><forename type="first">Sam</forename><surname>Bayer</surname></persName>
						</author>
						<title level="a" type="main" coord="1,202.80,60.32,206.33,15.84">MITRE&apos;s Qanda at TREC-14</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9EE88A5B1117460AA5A515E593CFD4ED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Qanda is MITRE's TREC-style question answering system. In recent years, we have been able to apply only a small effort to the TREC QA activity, approximately two person-months this year. (Accordingly, much of this discussion is strikingly similar to prior system descriptions.) We have made some general improvements in Qanda's processing, including genuine question parsing, generalizing answer selection to better handle variant question types like lists and definition questions, and better integration with a maximum entropy answer scorer, both in training and at run time. We have also attempted to better integrate the results of question processing and document retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">TREC-14 system description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Underlying architecture</head><p>Qanda uses a general computational infrastructure for human language technology called the Annotation Management System (AMS). AMS is a flexible library for pairwise interaction between language processors, based on the Catalyst infrastructure used in previous versions of Qanda <ref type="bibr" coords="1,223.68,449.30,68.30,11.96">(Burger 2004</ref><ref type="bibr" coords="1,291.98,449.30,4.96,11.96;1,54.00,462.02,100.46,11.96" target="#b3">, Burger &amp; Mardis 2002</ref><ref type="bibr" coords="1,154.46,462.02,98.14,11.96" target="#b8">, Nyberg et al. 2004)</ref>. Where Catalyst was specifically designed for fast processing, AMS is designed for compatibility and reuse. Essentially, AMS provides an extensible wrapper between a consistent internal programming model for language processors and the wide range of ways the language processor can be invoked, as well as the wide range of possible annotation formats and storage types. Philosophically, it is similar to IBM's UIMA infrastructure <ref type="bibr" coords="1,119.04,575.78,113.89,11.96" target="#b4">(Ferrucci &amp; Lally 2004)</ref>, without the benefits and drawbacks associated with the strong programming assumptions that UIMA makes. In comparison with Catalyst, AMS allowed us to make rapid changes to our system configuration, and introduce new language processing components with relatively little effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Major system components</head><p>Qanda has a by now shop-worn QA architecture, which proceeds in several phases. Questions are analyzed for expected answer types, as well as keywords to use in forming an IR query. Documents are retrieved using an IR system and are then processed by various taggers to find entities of the expected types in contexts that match the question. For TREC-14, we enhanced the analysis phases for both questions and document passages, including detailed parsing of questions. Below we describe each of the major components in turn.</p><p>• Common question and document processing: This phase consists of several steps: tokenization, sentence boundary detection, part of speech tagging <ref type="bibr" coords="1,380.16,287.54,94.89,11.96" target="#b9">(Ratnaparkhi 1996)</ref>, morphological analysis <ref type="bibr" coords="1,375.36,300.02,99.97,11.96" target="#b7">(Minnen et al. 2001)</ref>, and tagging of named persons, locations and organizations (named entities), as well as temporal expressions, for which Qanda uses Phrag <ref type="bibr" coords="1,466.08,337.94,87.01,11.96">(Burger et al. 2002)</ref>, an HMM-based tagger.</p><p>• Question analysis: After the common initial phase of analysis, questions are chunked and parsed, and salient features of the meaning of the question are extracted. See Section 2 below for more detail.</p><p>• IR wrappers: AMS components have been written for several IR engines, taking the results of the question analysis and formulating an IR query.</p><p>We continue to use the Java-based Lucene engine (Apache 2002). Lucene's query language has a phrase operator, and also allows query components to be given explicit weights. Qanda uses both of these capabilities in constructing queries from the information extracted from the question. For TREC-14, the top 50 documents were retrieved.</p><p>• Passage processing: After the retrieved documents pass through the common analysis phase, Qanda assigns a preliminary score to each sentence by summing the log-IDF (inverse document frequency) of each word that occurs in both the candidate sentence and the question. Those sentences with a low score are not processed by most of the system. This step is performed to reduce the cost of more expensive downstream components.</p><p>• Fixed repertoire taggers: We have a simple facility for constructing AMS taggers from fixed word-and phrase-lists. These were used to re-tag many named locations more specifically as cities, states/provinces, and countries. Qanda also identifies various other (nearly) closed classes such as precious metals, birthstones, several animal categories (e.g., state bird), and so on. Different taggers are applied to the question and to the retrieved passages.</p><p>• Numeric tagging: A fixed repertoire tagger is run on the retrieved passages to identify words and phrases denoting units of measure, and then a simple pattern-based tagger combines these with numeric expressions to identify full-fledged measure phrases, as well as currency, percentages and other numeric phrases.</p><p>• Overlap: The question is compared to each sentence, and a number of overlap features are computed, some in terms of various WordNet relations (see Section 3).</p><p>• Answer collection and ranking: Candidates are identified and merged, a number of features are collected, and a score is computed (Section 3).</p><p>• Answer selection: A final component down-selects the candidates and generates the actual answer strings. For factoid questions, this is simply the highest-scoring phrasal candidate, but definition and list questions require other processing, as detailed in Section 4.</p><p>As described above, all of these components communicate by consuming and producing stand-off annotations. A separate declarative facility is used to indicate which components are interested in consuming which annotations, and AMS arranges for the components to be connected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Question analysis</head><p>In previous TREC evaluations, Qanda performed a limited analysis of the questions. We tagged for partof-speech and named entities, and also applied a simple fixed-repertoire tagger that maps head words to answer types in Qanda's ontology, using a set of approximately 6000 words and phrases, some extracted heuristically from WordNet, some identified by hand. For TREC-14, we added a detailed parsing phase using MITRE's Carafe <ref type="bibr" coords="2,218.16,643.46,78.48,11.96">(Wellner, 2005)</ref> conditional random field chunker and the Pro3Gres dependency parser from the University of Zurich <ref type="bibr" coords="2,54.00,681.38,108.57,11.96" target="#b10">(Schneider et al. 2004)</ref>, and performed a heuristic analysis on the resulting structure to extract various dimensions of the question.</p><p>Because gold-standard data for questions is scarce, many of our corpus-based tools require a repair phase to address some of the more egregious misinterpretations of questions as declarative statements. For instance, it is not uncommon for a part of speech tagger that has been trained on declarative data to attempt to tag questions like Who does John love? as if John love is a noun-noun compound. We found characteristic problems in chunking and parsing as well, which we were able to partially correct using simple heuristics.</p><p>Once these tagging phases are complete, Qanda's question analysis component uses a set of structural heuristics to identify the following aspects of each question:</p><p>• Anchor: the object that the answer refers to. The answer may be the anchor, or it may be a property (e.g., length, color) or name of the anchor. The anchor will have a type and supertype from Qanda's (rather simple) ontology, e.g., PERSON and AGENT. The supertype is used as a backoff for some statistics.</p><p>• Property: the property, if any, of the anchor that is the actual answer, e.g., the height of a mountain. Properties also have a type and supertype in Qanda's ontology.</p><p>• Name: the name, if any, of the anchor that is the actual answer. This case can arise in questions that require descriptive answers, as in Who is Henry Kissinger?</p><p>• Answer restriction: an open-domain phrase from the question that describes the anchor, e.g., first woman in space.</p><p>• Event: the main event in the question, if any; typically the main verb, unless it is simply be.</p><p>• Salient entity: What the question is "about". Typically a named entity, this corresponds roughly to the classical notion of topic, e.g., Matterhorn in What is the height of the Matterhorn?</p><p>• Geographical restriction: Any phrase that seems to restrict the question's geophysical domain, e.g., in America.</p><p>• Temporal restriction: Any phrase that similarly restricts the relevant time period, e.g., in the nineteenth century.</p><p>• Superlative: Relevant adjectives from the question restriction, e.g., first, or fastest.</p><p>These features are emitted as annotations on the question, and are then available for down-stream components to consume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Answer ranking</head><p>Qanda only examines sentences that match the question sufficiently, based on the IDF-weighted overlap described above. It collects candidate answers by gathering phrasal annotations from all of the semantic taggers, and identifies a number of features. These are combined using a conditional maximumentropy model trained from past TREC QA data sets.</p><p>Several TREC participants have used this approach, e.g., <ref type="bibr" coords="3,75.12,221.30,99.38,11.96" target="#b5">Ittycheriah et al (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer candidate features</head><p>Many of the features used in the log-linear model reflect particular kinds of overlap between the question and the context in which the candidate answer is found:</p><p>• Context IDF Overlap: Described above.</p><p>• Context Unigram Overlap: Raw count of words<ref type="foot" coords="3,293.52,333.92,3.48,7.70" target="#foot_0">1</ref> in common with the question.</p><p>• Context Bigram Overlap: Raw count of word bigrams in common with the question.</p><p>• Context Question Restriction Overlap: Raw count of words from the restriction of the question (see Section 2). Most of the question components described engender analogous overlap features.</p><p>• Context Salient Overlap: Raw count of words considered especially salient by question analysis (see Section 2).</p><p>• Context Synonym Overlap: Raw count of words that could be synonymous with questions words.</p><p>The synonym features are computed with respect to WordNet (Fellbaum 1998).</p><p>Several features are computed based on the candidate itself, or its location in the context sentence:</p><p>• Candidate Overlap: Raw count of words in common between the candidate itself and the question, to bias against entities from the question being chosen as answers.</p><p>• Candidate Overlap Distance: Number of characters between the candidate and the closest (content) question word in the context.</p><p>• Candidate Question Restriction Distance: Number of characters between the candidate and a word from the restriction phrase of the question. Analogous distance features are formed for several other question component phrases.</p><p>The only document-level feature currently used is the following:</p><p>• IR Ranking of the source document by the IR system (but see below).</p><p>Candidates with the same textual realizations are merged, with the combined candidate retaining the highest value for each feature. This is the simplest candidate combination possible, but previous work on more robust answer combination across QA systems <ref type="bibr" coords="3,315.12,293.54,143.04,11.96" target="#b2">(Burger &amp; Henderson, 2003)</ref> used Tanimoto set distance to compare answer strings as bags of characters. This year we used several cross-candidate features:</p><p>• Merge Count: (log of) count of identical candidates merged together.</p><p>• Answer similarity: Average character-level similarity between this candidate and all others.</p><p>The latter feature allows textually similar candidates to "vote" for each other. This is especially useful for dates and other types with multiple formats and representations, thus, January, 1964 and Jan 64 can support each other without requiring sophisticated coreference.</p><p>A number of boolean features are also computed that compare the question's expected answer type with the semantic type of the candidate:</p><p>• Type Same: True if the candidate and expected answer types are identical.</p><p>• Type Consistent: True if the candidate's type is "similar" to the expected answer type.</p><p>• Type-Pair: This is a series of features corresponding to selected pairs of consistent types (see below).</p><p>For the most part, candidates are only considered for a question if their types are consistent. For example, Where questions lead to an expected answer type of LOCATION, which is consistent with COUNTRY candidates; How much questions lead to QUANTITY, consistent with PERCENTAGE.</p><p>Ideally, Qanda would consider all candidates for all questions, but, if nothing else, performance considerations justify limiting this. We do not even represent all consistent pairs as explicit features. Instead, a small set of approximately 20 combinations was chosen by hand, as indicated in Figure <ref type="figure" coords="4,254.40,432.98,4.11,11.96">1</ref>. These represent particular biases or preferences that we feel justified in trying to acquire from the training data. In addition, some of these pairwise features represent exceptions to the consistency requirement, e.g., PERSON is not consistent with COUNTRY, but we wish to consider such candidates anyway. Similarly, we wish to consider certain named entity types as candidates, even when question analysis was unsuccessful in divining an expected answer type (unknown).</p><p>After all of the (merged) candidates have been acquired, most of the raw feature values described above are normalized with respect to the maximum across all candidates for a particular question, resulting in values between 0 and 1. We have found features normalized in this way are more commensurate across questions, especially word overlap and related features <ref type="bibr" coords="4,178.32,666.74,78.13,11.96" target="#b6">(Light et al. 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum entropy models</head><p>The normalized features are combined using the weights assigned by a maximum entropy model during training. This year, we trained the model using the question sets from TREC 1999 through 2003, including the 2001 list questions, as well as the 25 AQUAINT definition evaluation questions. Last year's questions <ref type="bibr" coords="4,401.28,105.14,66.00,11.96">(TREC 2004)</ref> were used as a development set. We used Daumé's (2004) MegaM package to train the models.</p><p>We noted some interesting issues in training set conditioning. Because we are using a very small data set, there are arguably too few positive instances to acquire adequate feature weights, especially if we are interested in feature combination. In order to offset this, we experimented with "forcing" Qanda to consider all correct answers (as defined by NIST's judgment sets), even those that it would ordinarily not examine.</p><p>A simple example are those correct answers found only in documents that don't make it past Qanda's top-N IR cutoff (typically 50 documents). When we forced these documents into the pipeline (or alternatively increased the IR cutoff, and downsampled negative candidates), we found that certain features were assigned unintuitive weights by the loglinear model. In particular, the IR rank feature went from a low positive weight to a high negative weight. In retrospect, this makes sense, because high-ranking documents were being over-represented in the positive training instances being presented to the model. In the end, we found it simplest to discard this feature entirely, since it never contributed very much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Definition questions</head><p>Qanda has no real facility for processing definition questions as such. Instead, we attempt to leverage our factoid question processing, which for the most part only considers named and other entities as candidate answers. Of course, very few definition answers correspond directly to named entities, per se, but we have noticed that certain kinds of named entities are involved with some definition answers, as indicated in the example below:</p><p>Who is Gunter Blobel?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is at Rockefeller University 1999 Nobel prize in Medicine was born in 1936 was born in Waltersdorf, Silesia, Germany</head><p>Qanda's question analysis component could already identify the semantic type of the definition target (e.g., PERSON, above). Since definition answers did not need to be exact, we allow Qanda to consider certain entity types as pseudo-answers to definition questions. Accordingly, in recent years Qanda's definition answers are constructed from entire sentences, as described in the next section.</p><p>We used the type-pair features described in Section 3 to license certain combinations of definition target type and candidate type, as shown in Figure <ref type="figure" coords="5,249.60,199.70,4.11,11.96" target="#fig_0">2</ref>.</p><p>Additionally, we inject some non-entity candidates using crude heuristics for identifying short fragments occurring in appositional contexts. Our hope is that the type-pair features, as well as the candidate count feature, would allow the system to find some definition answers. As training data, we used last year's definition questions, as well as 24 questions from TREC 1999 and 2000 that we determined were essentially definition questions, and the AQUAINT definition questions.</p><p>We have had some success with this approach. To illustrate, our best-scoring definition run this year produced the following sentences as a "definition" for Bollywood (question 72.7):</p><p>SWISS-INDIA-FILMS (Lenk, Switzerland) _ Many of the 800 to 900 films produced each year by the Indian movie industry, which is sometimes called Bollywood, feature spectacularly scenic backgrounds that are filmed in faraway locations, typically Switzerland. ``Often I go past a cinema in London and look at the queues for Bollywood films and there are as many white faces as Indians,'' he said in a recent interview. Television production houses, such as Sony Entertainment and Star TV, pay huge sums to buy the rights of Bollywood favorites.</p><p>These sentences were chosen based on the following definitional pseudo-answers, which occurred with the indicated counts in sentences containing the definition term Bollywood:</p><p>• Switzerland: 15 occurrences • Indians: 14 occurrences • Star TV: 4 occurrences • Sony Entertainment: 4 occurrences</p><p>The three-sentence answer above matched both of the vital nuggets for Other question 72.7, as indicated by underlining. This gave a recall of 1.0. However, because we use entire sentences, the answer is rather long, and provided none of the optional nuggets, so its precision was 0.12, leading to an F of 0.52</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Final answer generation</head><p>Except for the pseudo-answers used for definition questions, most of Qanda's processing is independent of the question type. In particular, list questions are treated entirely as factoid questions until the very last stage, actual answer string generation. Here, special processing is required for both definition and list questions.</p><p>In the past, we have simply picked the top N candidate answers, with some fixed cutoff, but in recent years we have attempted something slightly more sophisticated for list and definition questions, picking N dynamically so as to maximize our expected score.</p><p>The basic idea takes advantage of Qanda's candidate evaluation mechanism-since this is probabilistic in nature, we can use it to choose how many answers to generate dynamically, based on the expected value of the score we might receive. Both list and definition questions are scored with variants of F-measure, the weighted harmonic mean of precision and recall:</p><formula xml:id="formula_0" coords="5,350.59,562.55,121.54,85.48">! F = " 2 + 1 ( ) PR " 2 P + R</formula><p>P is precision, the fraction of our generated answers that are correct, while R is recall, the fraction of all possible correct answers that we generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><p>" is a weight used to place more emphasis on either precision or recall. For list questions, P and R were weighted evenly, and so the evaluation simply reduces to the following:</p><p>!  Here, n is the number of answers we choose to generate, c is the number of correct answers we generate, and r is the total number of correct answers possible. The evaluation for definition answers was more complicated. The basis of the evaluation done by NIST is not answer strings, but interesting "nuggets" of information. The evaluators attempt to enumerate all correct nuggets by pooling all the system responses-this gives them a value for r. However, it was decided that n is quite difficult to determine-how many nuggets of information, correct or incorrect, are there in a particular text passage? Instead, precision is approximated with a length allowance-each correct nugget is given 100 characters to be expressed. An additional complication with definition answers is that ! " is set to three. This results in the following expression for evaluating a definition answer set:</p><formula xml:id="formula_1" coords="5,410.00,699.70,52.53,29.38">F list = 2c n + r</formula><formula xml:id="formula_2" coords="6,70.10,289.99,162.15,117.77">! F def = 10 ˆ P R 9 ˆ P + R ˆ P = min(1.0, 100c /l) R = c /r</formula><p>Here, c and r are as before, and l is the total length of the answer set, while the min function caps precision at 1.0.. 2 With these formulae in hand, we can attempt to estimate the score that a particular set of list or definition answers will receive.</p><p>We do not know in advance whether an answer is correct, but we can use probabilistic score for the answer candidates as the basis for an expectation of c. We have no real hope of estimating r, the number of correct answers possible, although David Lewis has suggested using the sum of scores over all answer candidates for a particular question. We have experimented with this, but found the results to worsen slightly, so for the official TREC runs we simply fix r at a magic number of 5. Thus, our algorithm for generating list and definition answers is to add each of the candidates to the answer set in turn, increasing n by one each time. Qanda then calculates the expected score of this answer set using the appropriate F-measure variant above, estimating c as follows:</p><p>2 In fact, this evaluation metric was even more complicated, as the assessors made a distinction between inessential and essential correct nuggets-only the latter counted for recall. We declined to attempt to estimate the essentialness of an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>!</head><formula xml:id="formula_3" coords="6,417.18,55.05,38.45,33.33">c " s i i=1 n #</formula><p>Here, s i is the probabilistic score assigned to candidate i. For list questions, we simply add candidate answers in order of their confidence score. For definition questions, as noted above, we decided to use entire sentences as answer set components. Our pseudoanswer candidates, however, are entities and other short phrases. So, we add the matrix sentence of the pseudo-answer to the definition answer set, as described in Section 4. Borrowing another trick from BBN's definition system last year, if any such sentence has too many words in common with the answer set so far (70% or more), we skip it. We do, however, include these skipped answers in calculating c, because a particular sentence may well contain multiple correct nuggets, as in the James Dean examples above.</p><p>We stop adding candidates to the answer set when the expectation begins to decrease. On last year's list questions, this mechanism performed markedly better than simply generating a single candidate per list question (F = 0.143 vs. 0.060). We have not yet separately evaluated this mechanism on this year's list or definition data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Runs and results</head><p>This year we submitted three variant runs. Run A is from a basic configuration, with the features largely as described in Section 3, but with no feature combinations. Run B is a "bells and whistles" run, with substantial feature combination enabled. Run C is the closest to last year's submission, with fewest features. Results are shown in Figure <ref type="figure" coords="6,495.84,494.90,4.23,11.96" target="#fig_1">3</ref>. All of our development this year centered on factoid questions, but, surprisingly, run C performed best on those questions, while run B performed best on definitions.</p><p>We have yet to explain this adequately.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>As well as the usual description of this year's system architecture, we have discussed Qanda's question analysis and our use of maximum entropy models for answer selection. We also presented our approach to generating definition and list answers using essentially the same system as for factoid questions, as well as the mechanism we use to determine how many of these answers to provide.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,65.04,700.10,220.92,11.96;5,132.48,712.58,86.03,11.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Type-pair features used in evaluating answer candidates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,342.48,711.38,187.81,11.96;6,365.04,724.10,142.69,11.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results for three MITRE runs compared to the 2005 medians</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,57.36,715.35,239.40,10.89;3,54.00,726.87,66.78,10.89"><p>All of the "raw count" features described in this section omit stop words.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,54.00,198.98,197.76,11.96;7,54.00,211.46,241.26,11.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,220.26,198.98,31.50,11.96;7,54.00,211.46,83.06,11.96">Jakarta Lucene-Overview</title>
		<ptr target="http://jakarta.apache.org/lucene/" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Apache Software Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,230.18,210.54,11.96;7,54.00,242.90,227.51,11.96;7,54.00,255.62,216.84,11.96;7,54.00,268.10,163.02,11.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,128.35,242.90,153.16,11.96;7,54.00,255.62,44.45,11.96">Statistical named entity recognizer adaptation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">C</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">T</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,121.44,255.62,149.40,11.96;7,54.00,268.10,121.97,11.96">Proceedings of the Conference on Natural Language Learning</title>
		<meeting>the Conference on Natural Language Learning<address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,286.82,217.80,11.96;7,54.00,299.54,234.12,11.96;7,54.00,312.02,230.16,11.96;7,54.00,324.74,207.00,11.96;7,54.00,337.46,119.82,11.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,225.00,286.82,46.80,11.96;7,54.00,299.54,144.94,11.96">Exploiting diversity for answering questions</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,222.00,299.54,66.12,11.96;7,54.00,312.02,230.16,11.96;7,54.00,324.74,109.80,11.96">Proceedings of the Human Language Technology Conference of the North American Chapter</title>
		<meeting>the Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,355.94,233.52,11.96;7,54.00,368.66,234.60,11.96;7,54.00,381.38,223.74,11.96;7,54.00,400.10,231.12,11.96;7,54.00,412.58,157.98,11.96;7,54.00,425.30,163.80,11.96;7,54.00,444.02,201.96,11.96;7,54.00,456.50,182.70,11.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,224.72,355.94,62.80,11.96;7,54.00,368.66,89.71,11.96;7,158.70,400.10,126.43,11.96;7,54.00,412.58,149.21,11.96;7,197.76,444.02,58.20,11.96;7,54.00,456.50,123.49,11.96">Notes on CG and LM-BFGS optimization of logistic regression</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Mardis</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/~hdaume/megam/ChristianeFellbaum" />
	</analytic>
	<monogr>
		<title level="m" coord="7,166.32,368.66,122.28,11.96;7,54.00,381.38,219.05,11.96">AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">2002. 2004. 1998</date>
		</imprint>
	</monogr>
	<note>WordNet: An Electronic Lexical Database</note>
</biblStruct>

<biblStruct coords="7,54.00,475.22,237.36,11.96;7,54.00,487.94,202.68,11.96;7,54.00,500.42,219.48,11.96;7,54.00,513.14,22.38,11.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,199.72,475.22,91.64,11.96;7,54.00,487.94,202.68,11.96;7,54.00,500.42,113.52,11.96">Building an example application with the Unstructured Information Management Architecture</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,179.28,500.42,94.20,11.96">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,531.86,224.70,11.96;7,54.00,544.58,235.26,11.96;7,54.00,557.06,241.44,11.96;7,54.00,569.78,208.86,11.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,89.30,544.58,190.40,11.96">IBM&apos;s statistical question answering system</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,65.28,557.06,230.16,11.96;7,54.00,569.78,46.28,11.96">Proceedings of the Tenth Text REtrieval Conference (TREC-10)</title>
		<meeting>the Tenth Text REtrieval Conference (TREC-10)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,588.50,240.78,11.96;7,54.00,600.98,216.36,11.96;7,54.00,613.70,199.20,11.96;7,54.00,626.42,78.06,11.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,89.03,600.98,181.33,11.96;7,54.00,613.70,94.81,11.96">Analyses for elucidating current question answering technology</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,172.32,613.70,80.88,11.96;7,54.00,626.42,54.12,11.96">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,54.00,645.14,238.62,11.96;7,54.00,657.62,212.70,11.96;7,54.00,670.34,164.46,11.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,58.99,657.62,198.57,11.96">Applied morphological processing of English</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Guido Minnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darren</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,54.00,670.34,135.95,11.96">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.12,54.50,221.16,11.96;7,315.12,67.22,235.31,11.96;7,315.12,79.70,229.32,11.96;7,315.12,92.42,199.50,11.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,392.29,67.22,158.14,11.96;7,315.12,79.70,83.13,11.96">Software architectures for advanced question answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Mardis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,421.92,79.70,122.52,11.96;7,315.12,92.42,44.77,11.96">New Directions in Question Answering</title>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Maybury</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.12,111.14,220.92,11.96;7,315.12,123.86,197.76,11.96;7,315.12,136.34,231.00,11.96;7,315.12,149.06,53.34,11.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,446.10,111.14,89.94,11.96;7,315.12,123.86,92.28,11.96">A maximum entropy part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,430.56,123.86,82.32,11.96;7,315.12,136.34,231.00,11.96;7,315.12,149.06,48.49,11.96">Proceedings of the Empirical Methods in Natural Language Processing Conference</title>
		<meeting>the Empirical Methods in Natural Language Processing Conference</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,315.12,167.78,218.46,11.96;7,315.12,180.26,220.92,11.96;7,315.12,192.98,192.84,11.96;7,315.12,205.70,213.18,11.96;7,315.12,224.18,197.82,11.96;7,315.12,236.90,167.82,11.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,346.81,180.26,189.23,11.96;7,315.12,192.98,31.03,11.96">Fast, deep-linguistic statistical dependency parsing</title>
		<author>
			<persName coords=""><forename type="first">Gerold</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Dowdall</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/carafe/" />
	</analytic>
	<monogr>
		<title level="m" coord="7,357.84,192.98,150.12,11.96;7,315.12,205.70,146.76,11.96">Workshop on Recent Advances in Dependency Grammar, COLING</title>
		<meeting><address><addrLine>Geneva. Ben Wellner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
