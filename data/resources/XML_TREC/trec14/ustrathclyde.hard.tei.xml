<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,168.48,126.63,258.28,12.91">University of Strathclyde at TREC HARD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,105.87,164.47,49.49,8.97"><forename type="first">Mark</forename><surname>Baillie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.48,164.47,62.75,8.97"><forename type="first">David</forename><surname>Elsweiler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.73,164.47,49.03,8.97"><forename type="first">Emma</forename><surname>Nicol</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.36,164.47,46.81,8.97"><forename type="first">Ian</forename><surname>Ruthven</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.16,164.47,62.04,8.97"><forename type="first">Simon</forename><surname>Sweeney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.48,164.47,50.90,8.97"><forename type="first">Murat</forename><surname>Yakici</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,466.86,164.47,22.56,8.97;1,237.04,176.44,31.73,8.97"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<email>fabioc@cis.strath.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.10,176.44,66.13,8.97"><forename type="first">Monica</forename><surname>Landoni</surname></persName>
							<email>monica@cis.strath.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">-lab group</orgName>
								<orgName type="institution">University of Strathclyde Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,168.48,126.63,258.28,12.91">University of Strathclyde at TREC HARD</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D2A5F114203ED6ACD0A1FE4F50CA7BCF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>The motivation behind the University of Strathclyde's approach to this years HARD track was inspired from previous experiences by other participants, in particular research by <ref type="bibr" coords="1,395.08,321.75,10.58,8.97" target="#b0">[1]</ref>, <ref type="bibr" coords="1,412.90,321.75,11.62,8.97" target="#b2">[3]</ref> and <ref type="bibr" coords="1,446.29,321.75,10.58,8.97" target="#b3">[4]</ref>. A running theme throughout these papers was the underlying hypothesis that a user's familiarity in a topic (i.e. their previous experience searching a subject), will form the basis for what type or style of document they will perceive as relevant. In other words, the user's context with regards to their previous search experience will determine what type of document(s) they wish to retrieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous Research</head><p>Belkin et al. stated that searchers "who are familiar with a topic will want to see documents that are detailed and terminologically specific, and people who are unfamiliar with a topic will want to see general and relatively simple documents" <ref type="bibr" coords="1,222.01,445.52,13.22,8.97" target="#b0">[1]</ref>. Documents in the corpus were assessed by how "readable" they were, using a standard measure called the Flesch readability score <ref type="bibr" coords="1,354.79,457.48,11.71,8.97" target="#b1">[2]</ref>. The Flesch score for a document is derived from the mean number of syllables per word and the number of words per sentence. For each topic, a documents Flesch score was combined with the corresponding Retrieval Status Value (RSV) estimated from the initial document ranking, also known as the baseline. It was discovered that this combination (of the normalised Flesch readability and estimated relevance scores) gave greater weight to readable documents in the ranking, aiding those user's with low topic familiarity.</p><p>In a similar theme, Harper et al. hypothesised that a "user's familiar with a topic will prefer documents in which highly representative terms occur, and user's with a topic will prefer documents in which highly discriminating terms will occur" <ref type="bibr" coords="1,245.41,553.34,12.20,8.97" target="#b2">[3]</ref>. In other words, by identifying terms very specific to a topic, those documents with detailed information (e.g. highly technical documents) were pushed up the original document ranking. Conversely, for those user's not familiar with the subject, expanding the original query with terms very general to the topic will boost those documents that provide an overview. Depending on the user's context, the original baseline ranking can be reordered providing more importance to those documents with a high proportion of either representative or familiar terms, respectively. Analysis of the performance found that Discriminative queries were (on average) effective at improving the original baseline document ranking, particulary when a user had previous knowledge of the topic they were searching.</p><p>Kelly et al. measured user familiarity gained from the meta-data of 2004s topics, against the number of times the person had searched for information about this topic in the past <ref type="bibr" coords="1,389.62,673.12,11.31,8.97" target="#b3">[4]</ref>. Expectedly, they found a degree of agreement between the number of times a user searched previously on this topic and their topic familiarity. In other words, those user's familiar with the TREC topic had on average searched more times previously on this subject than those who stated unfamiliarity. A clarification form was designed to obtain information both on user topic familiarity and also useful information that could be utilised for improving the original baseline ranking. Questions included -what information the user previously knew about the topic, what information they wanted to know, and also a large text box was provided to allow the user to add any further keywords that they felt described the topic. This information was then utilised both to determine a user's topic familiarity, from a newly derived measure derived from the typical the length of answer as well as other responses to the clarification form, and also to expand the original queries. The original queries submitted for each the topic were expanded using different combinations of feedback from the clarification form, with varying degrees of success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Summary</head><p>The common theme running through the above works can be summarised in the following Hypothesis:</p><p>-H1: A user's familiarity in a topic will have an impact on what type of documents they will find relevant.</p><p>A user with very little background knowledge (of a topic) will, potentially, find an overview document more helpful initially than a document with very specific (possibly technical) content. As a consequence, documents that are in someway general to the topic (e.g. little technical detail, simple introduction pieces, etc.) will more likely be judged as relevant by the user. In comparison, a user that has a high degree of familiarity or background knowledge , will (potentially) prefer documents that contain detailed comment on the topic.</p><p>To expand our motivation further, for HARD we examined another aspect of the user's context. The user's in the HARD track are not typical searchers, but TREC assessors. In some regards, these assessors do not own either the topic or the original query submitted to the Information Retrieval system. Potentially, an assessor may have both little knowledge of the topic, and importantly, little interest in researching that topic. We believe that this factor (a user's interest), will also have an impact on the type and style of document they wish to retrieve. We posit that a user with little interest and background knowledge of a subject, would prefer to read documents with little technical "jargon", accessible to read, short in length, and also stylistically pleasing (with a high number of motivating terms and phrases). Surmising our motivation, we assume that a user (in this case a TREC assessor) will often be assigned a task to search that they will have little interest in and/or previous knowledge of. Therefore we formulate the new hypothesis:</p><p>-H2: A user's interest in a topic will have an impact on what type of documents they will find relevant.</p><p>A user with little interesting in reading about a topic will prefer documents with little technical material and are stylistically pleasing to read.</p><p>In order to investigate both hypotheses (H1 and H2), we compared a number of approaches including Pseudo-relevance feedback, Flesch readability scores, Representative and Discriminative queries, and also a new approach that expands the original query with Motivating terms. In the following sections we introduce these techniques, and in particular the concept of expanding the original query with motivating terms. We then report the results and findings of each technique, before concluding our first attempt at both the HARD track, and TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section we introduce the different approaches that we investigated. All algorithms were implemented using the Lemur Information Retrieval framework <ref type="bibr" coords="2,319.45,697.02,13.26,8.97" target="#b5">[6]</ref>. Also, for the HARD track evaluation, the submitted runs from all groups were compared against a baseline run, declared by each participating group. The baseline submission is the initial document retrieval that all other approaches can be compared against, measuring the improvement (or harm) in the new ranking. For example, a typical IR system will submit a query and return the same ranked list for each user, no matter what how differing their context is. However in HARD, the user's context is captured after this initial ranking, allowing for a re-ranking before the final results list is presented to the user. Depending on the user's context, different ranked lists can be presented, tailoring the system towards the user's information need. In order to evaluate how successful each personalised approach is, all techniques are compared against the original baseline ranking. For our baseline, we selected the Okapi BM25, and in particular we used the version implemented in Lemur (with the standard settings) <ref type="bibr" coords="3,241.20,225.26,11.05,8.97" target="#b5">[6]</ref>. For the baseline submission, we also used the topic titles as queries to simulate typically (poor) queries submitted by user's, which may also reflect typical behaviour of user's with little previous knowledge of the topic (or interest).</p><p>In the following sections, we outline how the user's context was captured, through the use of a clarification form, Section 2.1. We then introduce in turn each technique we investigated: a standard Pseudorelevance feedback approach (Section 2.2), combining document Flesch readability scores (Section 2.3), query expansion using Discriminative and Representative terms (Section 2.4), and finally query expansion using Motivational terms in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Capturing user context: Clarification Forms</head><p>We designed a clarification form to collect data from each assessor. Of particular interest was the user's previous topic experience or familiarity, and also their interest in finding out more about the subject. Within the clarification form, details were provided to remind the assessor about the topic, alongside a number of top ranked document summaries from the baseline ranking for the user to assess. However, we found little correlation between these summaries and the answers to the actual questions, therefore we utilised the clarification form as a way of data collection. The answers to each question were then used as a guide for setting the operation parameters in each re-ranking approach, and more importantly for grouping each user by topic familiarity and interest. Overall, we formed four groups based on the answers to the clarification forms (see Table <ref type="table" coords="3,267.50,459.96,3.60,8.97" target="#tab_0">1</ref>).</p><p>For the final analysis, we examined the individual assessors as well. By doing so, we can examine the characteristics of each individual assessor, rather than treat the topics as independent. This was possible because for this years HARD track, the information on which assessor judged what topic was provided. There were six Assessors overall (A-F), who each judged approximately 8-10 topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo-Relevance Feedback</head><p>We compared the performance of each suggested technique against a tried and tested benchmark: Pseudorelevance feedback. We believe this would be an interesting comparison with other approaches based on user familiarity and topic interest. This would allow us to assess whether techniques based on the inclusion of contextual information from the user, improved over an accepted, and proven, automatic technique reranking. In other words, this would be our benchmark technique for comparing all other strategies. We submitted a run re-ranking the baseline using Pseudo-relevance feedback, where the top N documents are assumed to be relevant. These documents were then used to re-rank the baseline. Therefore, the top N ranked documents were used for Pseudo-relevance feedback, re-ranking the top 1000 ranked documents from the baseline. The standard OKAPI Pseudo-relevance feedback algorithm implemented in the Lemur toolkit <ref type="bibr" coords="4,171.76,201.35,11.62,8.97" target="#b5">[6]</ref> was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combing Readability Scores</head><p>The first technique we investigated, was an approach first suggested by Belkin et al. in the previous HARD track <ref type="bibr" coords="4,108.35,262.39,11.43,8.97" target="#b0">[1]</ref>: the Flesch Reading Ease Score <ref type="bibr" coords="4,249.48,262.39,10.58,8.97" target="#b1">[2]</ref>. A documents Flesch score is a reflection of the mean number of syllables per word, and the number of words per sentence in a document. It is assumed that the higher a documents Flesch score, the more readable a document is. We therefore assumed that a document with both a high RSV and a high Flesch score will be more appropriate for user's with low topic familiarity, and/or those user's with little interest in the topic (Groups G3 and G4).</p><p>To utilise a documents readability score, the Flesch value was combined with the RSV using a simple linear combination. To do so, the Flesch score for all documents in the corpus was first computed off-line. Then for each topic, the top 1000 ranked documents, both the RSV and Flesch score were normalised and then combined using simple weighted average (see equation 1). Other evidence combination techniques could be applied such as Dempster-Schaffer <ref type="bibr" coords="4,267.03,369.99,12.78,8.97" target="#b2">[3]</ref>, but we initially wanted to test the hypothesis using a simple approach.</p><p>Hence, the normalised estimated relevance scores, RSV i , for each document i were combined with the normalised document readability score, read i (see <ref type="bibr" coords="4,307.80,405.85,10.45,8.97" target="#b1">[2]</ref>),</p><formula xml:id="formula_0" coords="4,236.49,425.47,265.63,13.98">score(i) = RSV i + α * read i (<label>1</label></formula><formula xml:id="formula_1" coords="4,502.11,429.77,3.87,8.97">)</formula><p>where α is a weighting parameter and,</p><formula xml:id="formula_2" coords="4,233.55,465.89,272.44,24.03">RSV i = RSV i -min(RSV ) max(RSV ) -min(RSV )<label>(2)</label></formula><p>and,</p><formula xml:id="formula_3" coords="4,236.04,509.13,269.95,24.03">read i = read i -min(read) max(read) -min(read)<label>(3)</label></formula><p>Depending on a user's context, the weighting parameter α can be adjusted. For example, a user with both low topic familiarity and interest will have a higher value of α in comparison to a user with high familiarity and interest. By doing so, more weight is placed on the importance on the readability score, thus pushing such documents further up the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Query Expansion using Representative and Discriminative Terms</head><p>The second approach we investigated was the use of Representative and Discriminative terms for query expansion. For user's with little topic familiarity and / or low interest in the topic, it was believed that representative terms for the topic will be able to locate documents that are very general e.g. overview documents. In comparison, discriminative terms can be used to find detailed documents on a topic, for those user's with previous experience of the subject.</p><p>To investigate this approach, we adopted the same strategy first introduced by Harper et al <ref type="bibr" coords="4,468.17,685.07,10.58,8.97" target="#b2">[3]</ref>, ranking terms in a topic model according to their contribution. A topic model being a Language Model (LM) formed from the top N documents in the original baseline ranking. The Kullback-Leibler Divergence measure is then used to determine what each terms contribution to the topic is in the corpus vocabulary. KL is typically used for measuring the difference between two probability distributions <ref type="bibr" coords="5,433.32,141.58,11.25,8.97" target="#b4">[5]</ref>. When applied to the problem of measuring the distance between two term distributions (Language Models), KL estimates the relative entropy between the probability of a term t occurring in the actual collection Θ a (i.e. p(t|Θ a )), and the probability of the term t occurring in the estimated Topic Language Model (LM) Θ e (i.e. p(t|Θ e )).</p><p>KL is defined as,</p><formula xml:id="formula_4" coords="5,216.98,209.56,289.00,28.51">KL(Θ e ||Θ a ) = t∈V p(t|Θ e )log p(t|Θ e ) p(t|Θ a )<label>(4)</label></formula><p>where,</p><formula xml:id="formula_5" coords="5,241.21,253.23,264.78,26.43">p(t|Θ a ) = n(t, Θ a ) t∈Θa n(t, Θ a )<label>(5)</label></formula><p>and,</p><formula xml:id="formula_6" coords="5,224.59,294.30,281.39,27.56">p(t|Θ e ) = d∈Θe n(t, d) + α t ( d∈Θe n(t, d) + α) (6)</formula><p>where n(t, d) is the number of times t occurs in a document d and α is a small non-zero constant (Laplace smoothing). The smaller the KL divergence the closer the topic is to the actual collection, with a zero KL score indicating two identical distributions. To account for the sparsity within the Θ e , Laplace smoothing was applied to alleviate the zero probability problem <ref type="bibr" coords="5,295.85,364.82,12.90,8.97" target="#b6">[7]</ref>.</p><p>Instead of determining the difference between two term distributions (i.e. the collection and topic LM), we are interested in the individual term contribution to the topic LM. A term contribution being the KL score for the term t. The greater the contribution to the topic model the higher the KL score. Therefore, for each term t the contribution was calculated by,</p><formula xml:id="formula_7" coords="5,238.33,431.33,267.66,24.93">KL(t) = p(t|Θ e )log p(t|Θ e ) p(t|Θ a )<label>(7)</label></formula><p>The top C ranked terms in a topic model are then ranked further according to each terms "representative" and "discriminative" properties. To rank a terms discriminative property (e.g. how specific a term is to the topic), the KL discriminative score for term t is calculated by,</p><formula xml:id="formula_8" coords="5,251.50,509.20,254.48,24.93">KL d (t) = log p(t|Θ e ) p(t|Θ a )<label>(8)</label></formula><p>To calculate how general a term is to the topic LM, the KL representative score is used, calculated by,</p><formula xml:id="formula_9" coords="5,259.64,564.46,246.34,11.35">KL r (t) = p(t|Θ e )<label>(9)</label></formula><p>For each topic, either the top K ranked terms corresponding to either the KL-representation or KLdiscrimination (equations 8 and 9 respectively), are then used to expand the query. For those user's with low familiarity and/or topic interest, the top Q ranked representative terms are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Query Expansion using Motivating Terms</head><p>One of the main assumptions stated earlier, is that we believe a user's interest in a topic would have a bearing on the types of documents they will find relevant. For example, a user who is searching a topic they have little interest in, would possibly find documents that are stylistically pleasing better in comparison to very verbose, technically specific documents. A particular stylistic technique for drawing a readers attention is to make use of motivating terms and phrases within the text. We therefore assume that those documents that contain a high number of motivating terms may provide a more suitable entry point into the topic for those user's with little interest in searching on the subject.</p><p>In order to determine what motivating terms to include in the expanded query, a list of typical motivating terms and phrases were collated. A list was manually compiled of words that indicate or portray emotion. This list of motivating terms and phrases was then ranked according to each terms contribution to the topic model, which was formed from the top N ranked documents, for each query. This approach is similar to that outlined in section 2.4, however, only motivating terms are ranked according using the KL score (see equation 7). The top ranked motivating terms for each topic were then used to expand the original query. The approach we adopted is outlined below:</p><p>1. Form a topic LM with the top N ranked documents. 2. Smooth the topic LM with the reference collection using Laplace smoothing. 3. For each term t in the predefined motivating term list, we calculate the KL score (see equation 7). 4. All terms t are then ranked with respect to the KL score (highest to lowest), 5. At this stage two strategies were implemented:</p><p>-Expand the original query with those terms with a positive contribution e.g. KL( t) &gt; 0 -Expand the query with the top Q ranked terms for each topic.</p><p>We now illustrate some examples of an original query being expanded by motivation terms. Below is two topics in this years HARD track. For the first topic (Number 322), the original query submitted to the IR system was "International Art Crime". A topic model was formed from the top 10 ranked documents. The list of motivating terms were then ranked based on their contribution to the topic. Table <ref type="table" coords="6,451.86,378.59,4.98,8.97" target="#tab_1">2</ref> presents the top three ranked motivating terms for this topic. Each term could be considered related to the subject of crime. It is believed by expanding the original query with these terms we will push up those document that may be of more interest to the user, thus providing a higher likelihood being relevant. For a different topic, the title query submitted was "Black Bear Attacks". The top three ranked motivating terms , see Table <ref type="table" coords="6,498.52,426.42,3.74,8.97" target="#tab_1">2</ref>, for topic number 336, were "wild", "stirring" and "dangerous". All three terms could be associated with the description of aggressive animal behaviour or characteristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head><p>In this Section, we discuss the results from the official runs submitted for HARD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Runs</head><p>A summary of the runs submitted for HARD can be found in Table <ref type="table" coords="6,372.05,685.07,3.74,8.97" target="#tab_2">3</ref>. STRA1 was our baseline submission, which was the OKAPI retrieval method <ref type="bibr" coords="6,283.17,697.02,13.20,8.97" target="#b5">[6]</ref>. All other submissions were compared against this baseline. For each submitted run, we fixed the parameters for each attempt to provide a fair comparison  across each different technique. However, after the official results were released, we re-examined a number of new runs over a wider ranger of parameters settings. The results from each run, across a wider range of varying parameters will be released as a technical report once the analysis has been completed.</p><p>For query expansion using Motivating terms, two runs were submitted. In Section 2.1, the assessor for each topic was placed into one of four groups depending on their responses in the clarification form (see Table <ref type="table" coords="7,131.75,360.39,3.60,8.97" target="#tab_0">1</ref>). We would posit that the performance of both runs would be better for those topics placed in groups G3-G4, who stated little interest in reading about the topic, than the other two groups. For forming the topic model prior to ranking the motivating terms, the top ten (N = 10) documents ranked by the baseline run were used. Then for the first submission (STRAxmta), each original query was expanded using the same number of terms (the top Q = 6 ranked terms). For the second run (STRAxmtg), the query was expanded with all top ranked motivating terms that recorded a KL score greater than zero i.e. KL( t) &gt; 0.</p><p>For the Pseudo-relevance feedback submission, the top N documents were also used to re-rank the first 1000 ranked documents (STRAxprfb). For consistency in our comparisons with other approaches, we again fixed N to be 10.</p><p>For comparing the Discriminative and Representative queries (STRAxqedt and STRAxqert respectively), we submitted one run each that expanded the original query with either the top sixth ranked Discriminative or Representative terms. For both runs, we would expect improved performance for groups G1 and G3 (those assessors familiar with the topic) using discriminative queries, and for groups G2 and G4, representative queries. Such a result would indicate that Representative queries rank general overview documents higher for those user's with low familiarity, while Discriminative terms would push up documents very specific to the topic. However, by submitting both sets of queries to all user's, we can also examine the effect of using discriminative queries for those user's with low familiarity, and vice versa. Again, N was set for 10 for ranking the topic terms and the original query was expanded with the top 6 Representative or Discriminative terms.</p><p>We also submitted two runs that combined the document RSV values estimated during the baseline with their Readability score. For submission STRAxreada, the same value was used for the weight α. This would help evaluate the effect of using the readability score across all groups. For the second run, STRAxreadg, we varied α, providing more to those groups with low topic familiarity and interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Table <ref type="table" coords="7,113.40,685.07,4.98,8.97" target="#tab_3">4</ref> provides an overview of the performance of each of the official submissions. In the Table, we also include the proportion of topics where there was an increase over the baseline R-precision for a topic (a success), as well as the proportion of topics where an approach harmed the baseline R-precision (a fail). Baseline -OKAPI STRAxmta Query expansion using the top Q ranked Motivating terms N = 10, Q = 6 STRAxmtg Query expansion using the Motivating terms, with Q differing per topic N = 10, all KL( t) &gt; 0 STRAxprfb Pseudo-relevance feedback N = 10 STRAxqedt Query expansion using Discriminative terms N = 10, Q = 6 STRAxqert Query expansion using Representative terms N = 10, Q = 6 STRAxreada Combine RSV with Readability score, same weight for all groups α = 0.1 STRAxreadg Combine RSV with Readability score, differing weight for all groups α = 0.1, 0.15, 0.15, 0.2</p><p>Examining the results irrespective of groups, it was highlighted that using Pseudo-relevance feedback was the most successful technique (R-precision = 0.263), followed by expanding the query using Representative queries (R-precision = 0.226). On average, Pseudo-relevance feedback increased R-precision by 0.048 over the baseline, while expanding all queries with Representative terms improved the baseline by 0.011. Overall, Pseudo-relevance feedback recorded more successes per topic than any other approach, improving over the baseline 66% of the time, and harming 20% of all topics. Expanding the original query with Representative terms improved 50% of topics, and harmed 38%. The other approaches marginally improved over the baseline, while expanding the original query with Motivating terms in fact harmed the original baseline ranking more often than not, worsening the original R-precision score for approximately 56% of all topics. For HARD, the performance of each technique across the four predefined groups was of more interest (see Table <ref type="table" coords="8,132.56,575.86,3.60,8.97" target="#tab_0">1</ref>). Table <ref type="table" coords="8,170.59,575.86,4.98,8.97" target="#tab_4">5</ref> provides an overview of the results across these groups, with both the R-precision (R-prec) and success rate per topic (suc) presented. Analysing the performance of each group, we discovered that Pseudo-relevance feedback again performs best for three out of the four groups (G1, G3 and G4). For group G1, (user's with a high familiarity and interest), there was some evidence that using Representative terms improved the original baseline ranking. Also, for group G4 (user's with both low familiarity and little interest), there was some evidence that using Representative terms to expand the query improved the baseline ranking (3.4%), with a success rate of 57% per topic.</p><p>No approaches expect Pseudo-relevance feedback improved the baseline for group G3. Also, for group G2, those user's with low topic familiarity but a willingness to read more about the topic, all approaches improved only marginally over the baseline, with the Representative terms performing best. Although all techniques improved on the baseline, the actual baseline R-precision value was very low, at 0.0385. All other runs appeared to be affected with this poor initial baseline ranking. We also examined the performance of each approach across the different TREC assessors, with the results of this analysis presented in Table <ref type="table" coords="9,256.75,312.58,3.74,8.97" target="#tab_5">6</ref>. Again, both the R-precision and success rate for each run is presented along with the percentage of topics that provided an improvement over the baseline ranking. There were 6 TREC assessors overall, who on average judged 8 topics each, with assessor F judging 10.</p><p>From this analysis, we discovered that yet again Pseudo-relevance feedback worked better for five out of the six assessors. However, for assessor D, no approach improved over the baseline. In fact, all approaches harmed the initial baseline ranking for this assessor. For assessors C, E and F, there was some evidence that Representative queries performed well, and for assessor A, Discriminative queries showed improved performance over the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Both the use of Readability scores and Motivating terms resulted in poor performance, when applied in isolation. Expanding the original query with Motivating terms, more often than not resulted in harming the original baseline ranking. The probable reason for this result, was that many of the relevant documents will either share few Motivating terms, or contain none at all. Although, we were encouraged by the ranking of Motivating terms with respect to each TREC topic. Those Motivating terms related to the subject were often ranked highly. This was indication that there was some potential in investigating this approach further. We now plan to investigate the use of Motivating terms further both re-examining dif-ferent weighting schemes for query expansion, and also new approaches that could utilise these terms effectively.</p><p>There was evidence (although marginal) that using both Representative and Discriminative terms for expanding the original query for user's with low and high topic familiarity, respectively, did work. For group G1, Discriminative queries were successful in over 50% of the topics, while for group G4, Representative queries were successful in 57% of topics. Again, further examination of how to successfully use these terms is warranted.</p><p>For G2, the baseline was very poor, which had a negative effect on all other runs. The reason for this could be explained by a number of factors such as poor initial queries, difficult topics, or also the number of relevant topics for the topic (on average 69 partially relevant and 14 highly relevant documents per topic). This group contained four topics (344, 345, 397, 401), each judged by a different assessor. Compared to other topics, there was a similar number of judged relevant documents. Also, comparing the typical performance for these topics for all participants of HARD, the median R-precision for the baseline and final submissions was found to be 0.0873 and 0.078 respectively. This would indicate that the four grouped topics are difficult in someway. Whether this is due to the context of the assessors of some other factor would require further analysis. However, the result does indicate the instability of all submissions evaluated in this work. The majority of techniques are reliant on the original baseline ranking to contain a number of relevant documents in the top N . If this is not the case, then each approach performs poorly as a result.</p><p>Overall, Pseudo-relevance feedback was found to be the best approach both on average, and across the each group and assessor. Only for group G2, and also for assessor D, did Pseudo-relevance feedback not perform well. The other suggested techniques based on a user's context did not record similar improvement over the baseline R-precision in comparison with this approach. It would be of interest in future research to identify new approaches, either based on combining methods such as Discriminative / Representative queries with a tried and tested strategy approach such as Pseudo-relevance feedback. Also, further research into new techniques that are not so reliant on the initial baseline ranking would be beneficial for difficult topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,89.29,126.77,263.49,7.04;7,89.29,148.69,301.13,7.04;7,89.29,159.65,290.38,7.04;7,89.29,170.61,268.87,7.04;7,89.29,192.53,236.60,7.04;7,89.29,214.44,328.02,7.04;7,89.29,225.41,338.77,7.04;7,89.29,236.36,91.41,7.04"><head>&lt;num&gt;</head><label></label><figDesc>Number: 322 &lt;Title&gt; International Art Crime &lt;narr&gt; Narrative: A relevant document is any report that identifies an instance of fraud or embezzlement in the international buying or selling of art objects.... &lt;num&gt; Number: 336 &lt;title&gt; Black Bear Attacks &lt;narr&gt; Narrative: It has been reported that food or cosmetics sometimes attract hungry black bears, causing them to viciously attack humans....</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,184.38,266.76,226.42,8.07"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. A summarised description of TREC topics 322 and 336.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,130.01,540.98,335.26,73.27"><head>Table 1 .</head><label>1</label><figDesc>Group Statistics</figDesc><table coords="3,130.01,561.96,67.52,8.07"><row><cell>Group Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,136.35,482.68,322.47,74.46"><head>Table 2 .</head><label>2</label><figDesc>Top ranked motivating terms for TREC Topic Numbers 322 and 336 respectively</figDesc><table coords="6,223.23,503.65,148.79,53.49"><row><cell>Topic Num: 322</cell><cell cols="2">Topic Num 336</cell></row><row><cell cols="3">Term KL score Term KL score</cell></row><row><cell>dangerous 0.00175</cell><cell>wild</cell><cell>0.0018</cell></row><row><cell cols="3">suspicious 0.0012 stirring 0.0014</cell></row><row><cell cols="3">significant 0.0004 dangerous 0.00068</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,97.88,126.53,387.04,38.58"><head>Table 3 .</head><label>3</label><figDesc>Submitted runs</figDesc><table coords="8,97.88,145.67,91.92,8.07"><row><cell>Run</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,170.83,415.76,251.37,117.51"><head>Table 4 .</head><label>4</label><figDesc>Results for each submitted run -R-precision</figDesc><table coords="8,170.83,436.72,251.37,96.54"><row><cell>Run</cell><cell cols="5">R-precision Ave-precision MAP@10 % Success % Fail</cell></row><row><cell>STRA1</cell><cell>0.215</cell><cell>0.1598</cell><cell>0.338</cell><cell></cell><cell></cell></row><row><cell>STRAxmta</cell><cell>0.174</cell><cell>0.132</cell><cell>0.3</cell><cell>22%</cell><cell>58%</cell></row><row><cell>STRAxmtg</cell><cell>0.176</cell><cell>0.132</cell><cell>0.298</cell><cell>22%</cell><cell>56%</cell></row><row><cell>STRAxprfb</cell><cell>0.263</cell><cell>0.209</cell><cell>0.402</cell><cell>66%</cell><cell>20%</cell></row><row><cell>STRAxqedt</cell><cell>0.219</cell><cell>0.171</cell><cell>0.364</cell><cell>40%</cell><cell>16%</cell></row><row><cell>STRAxqert</cell><cell>0.226</cell><cell>0.185</cell><cell>0.376</cell><cell>50%</cell><cell>38%</cell></row><row><cell>STRAxreada</cell><cell>0.216</cell><cell>0.160</cell><cell>0.339</cell><cell>28%</cell><cell>16%</cell></row><row><cell>STRAxreadg</cell><cell>0.216</cell><cell>0.160</cell><cell>0.339</cell><cell>28%</cell><cell>18%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,164.08,126.46,264.86,146.21"><head>Table 5 .</head><label>5</label><figDesc>R-precision and number of success across Groups</figDesc><table coords="9,164.08,147.43,264.86,125.23"><row><cell></cell><cell></cell><cell>Groups</cell><cell></cell><cell></cell></row><row><cell></cell><cell>G1</cell><cell>G2</cell><cell>G3</cell><cell>G4</cell></row><row><cell>Stra1</cell><cell>0.244</cell><cell>0.0385</cell><cell>0.143</cell><cell>0.245</cell></row><row><cell>Run</cell><cell cols="5">R-prec % suc R-prec % suc R-prec % suc R-prec % suc</cell></row><row><cell cols="5">STRAxmta 0.1968 0.242 0.044 0.5 0.1389 0.1667 0.171</cell><cell>0</cell></row><row><cell cols="5">STRAxmtg 0.201 0.242 0.044 0.5 0.1379 0.1667 0.171</cell><cell>0</cell></row><row><cell cols="6">STRAxprfb 0.300 0.727 0.041 0.25 0.1658 0.5 0.3016 0.7143</cell></row><row><cell cols="6">STRAxqedt 0.2487 0.454 0.044 0.5 0.133 0.1667 0.254 0.285</cell></row><row><cell cols="6">STRAxqert 0.2515 0.515 0.096 0.5 0.1256 0.333 0.2697 0.571</cell></row><row><cell cols="6">STRAxreadA 0.2457 0.303 0.041 0.25 0.141 0.1667 0.242 0.2857</cell></row><row><cell cols="6">STRAxreadg 0.2454 0.303 0.041 0.25 0.141 0.167 0.242 0.286</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,124.28,428.57,344.46,135.23"><head>Table 6 .</head><label>6</label><figDesc>R-precision and number of success across Assessors</figDesc><table coords="9,124.28,449.54,344.46,114.27"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Assessors</cell><cell></cell><cell></cell></row><row><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell></row><row><cell>Stra1</cell><cell>0.208</cell><cell>0.197</cell><cell>0.207</cell><cell>0.190</cell><cell>0.240</cell><cell>0.243</cell></row><row><cell>Run</cell><cell cols="6">R-prec # suc R-prec # suc R-prec # suc R-prec # suc R-prec # suc R-prec # suc</cell></row><row><cell cols="7">STRAxmta 0.180 (1/8) 0.145 (1/8) 0.196 (3/8) 0.133 (1/8) 0.160 (2/8) 0.219 (3/10)</cell></row><row><cell cols="7">STRAxmtg 0.184 (1/8) 0.147 (1/8) 0.196 (3/8) 0.135 (1/8) 0.153 (2/8) 0.230 (3/10)</cell></row><row><cell cols="7">STRAxprfb 0.250 (6/8) 0.236 (4/8) 0.313 (7/8) 0.185 (4/8) 0.281 (5/8) 0.304 (7/10)</cell></row><row><cell cols="7">STRAxqedt 0.236 (3/8) 0.187 (2/8) 0.228 (3/8) 0.178 (4/8) 0.220 (3/8) 0.257 (5/10)</cell></row><row><cell cols="7">STRAxqert 0.183 (2/8) 0.210 (5/8) 0.269 (5/7) 0.144 (3/8) 0.274 (4/8) 0.269 (5/10)</cell></row><row><cell cols="7">STRAxreadA 0.208 (1/8) 0.201 (4/8) 0.209 (2/8) 0.188 (3/8) 0.238 (3/8) 0.246 (4/10)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>We would like to thank <rs type="person">Ellen Voorhees</rs>, <rs type="person">James Allan</rs> and <rs type="person">Ian Soboroff</rs> for solving the mystery as to why our second clarification form failed. A lesson learned for next year.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,92.66,552.34,413.15,8.07;10,101.00,563.31,404.92,8.07;10,101.00,574.26,96.44,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,153.10,563.31,155.59,8.07">Rutgers&apos; hard track experiences at trec 2004</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chaleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-L</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,325.34,563.31,180.58,8.07">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.66,584.55,243.46,8.07;10,101.00,595.52,388.47,8.07" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,144.67,584.55,143.53,8.07">A simplified flesch reading ease formula</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Foulger</surname></persName>
		</author>
		<ptr target="http://www.foulger.info/davis/papers/SimplifiedFleschReadingEaseFormula.htm" />
		<imprint>
			<date type="published" when="1997-08">1997. August 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.66,605.81,413.16,8.07;10,101.00,616.76,404.98,8.07;10,101.00,627.73,20.17,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,396.81,605.81,109.00,8.07;10,101.00,616.76,126.26,8.07">The robert gordon university&apos;s hard track experiments at trec 2004</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wettschereck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wiratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,245.56,616.76,183.16,8.07">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.66,638.02,413.15,8.07;10,101.00,648.98,280.10,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,248.99,638.02,239.56,8.07">University of north carolina&apos;s hard track experiments at trec 2004</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">Deepak</forename><surname>Dollu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,101.00,648.98,181.35,8.07">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.66,659.27,264.21,8.07" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,148.65,659.27,119.46,8.07">Information theoery and statistics</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.66,669.56,391.19,8.07" xml:id="b5">
	<monogr>
		<ptr target="http://www.lemurproject.org/.Web" />
		<title level="m" coord="10,101.00,669.56,125.25,8.07;10,387.31,669.56,43.04,8.07">Lemur Language Modeling Toolkit</title>
		<imprint>
			<date type="published" when="2005-10">2005. October 2005</date>
		</imprint>
	</monogr>
	<note>Last Visited</note>
</biblStruct>

<biblStruct coords="10,92.66,679.85,413.28,8.07;10,101.00,690.81,404.97,8.07;10,101.00,701.78,181.24,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,184.73,679.85,195.94,8.07">Cluster-based language models for distributed retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,397.11,679.85,108.82,8.07;10,101.00,690.81,378.80,8.07">SIGIR &apos;99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
