<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,104.04,142.90,415.66,15.49">Melbourne University 2005: Enterprise and Terabyte Tracks</title>
				<funder>
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">ARC Center for Perceptive and Intelligent Machines in Complex Environments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.00,176.73,42.03,10.76"><forename type="first">Vo</forename><surname>Ngoc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.00,176.73,20.62,10.76;1,269.52,176.73,78.50,10.76"><forename type="first">Anh</forename><forename type="middle">William</forename><surname>Webber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.00,176.73,71.60,10.76"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,104.04,142.90,415.66,15.49">Melbourne University 2005: Enterprise and Terabyte Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">30E6F844A8DF7AAFA51AEC2D069988C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done at The University of Melbourne for the TREC-2005  Enterprise and Terabyte Tracks. In the Enterprise Track, we participated in the Discussion Task. We tried a number of different methods to make use of special features of mailing lists to improve retrieval effectiveness, and found the use of thread context to be promising. In the Terabyte Track, we continued our work with impact-based ranking and sought to reduce indexing as well as query time. A new retrieval system has been developed for this purpose and has shown several improvements over our system from last year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In TREC 2005, The University of Melbourne participated in the Enterprise and Terabyte Tracks.</p><p>In the inaugural Enterprise Track, we took part in the Discussion Task. A baseline run was made on an index from which all quoted text had been stripped. The document scores in this baseline run were then supplemented, first by scores from a parallel index of the quoted text, then by scores of other messages in the same thread, and finally by the frequency with which the message's author posted to the W3C mailing lists. A separate run was made using an impact-based system. In the event, the official results showed the impact-ordered run to be superior to the chosen baseline, and retaining quoted text to be superior to removing it. Enhancing document scores with thread information was a promising technique.</p><p>In the Terabyte Track, we participated in the Ad-hoc and Efficiency Tasks. For both tasks, a retrieval system employing impact-based ranking was used, modified since last year to improve efficiency. For the Ad-hoc Task, runs using anchor text and query term proximity were made. For the Efficiency Task, the system was run both on a single machine and on an eight-machine cluster. Our results indicate a significant improvement in efficiency compared to last year, as well as modest effectiveness gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Enterprise Track</head><p>We participated in the Discussion Task of the Enterprise Track. Two base runs were made, using different ranking metrics and software. One of these base systems was then used as a starting point for three different enhancements. As it turned out, the less effective of the two base systems, as assessed by the official relevance judgments, was the one used for the experimental enhancements. Also, the approach taken of stripping quoted text (usually verbatim parts of the thread of messages being replied to) before indexing was not vindicated by the final effectiveness scores. On the other hand, the experimental technique of enhancing an email's score by reference to the surrounding thread does seem to offer effectiveness gains.</p><p>Subsequent investigation discovered a number of errors in our runs. First, while roughly 90% of the documents in the collection follow the same consistent template for marking up mailing list posts in HTML, another 10% do not. Our preprocessing tool was designed assuming that the said template was used universally, and produced either garbled or empty documents for the 10% of posts that did not follow this format, something we did not discover until after our runs were submitted. Second, our parameters were hand-trained using our own set of document judgments and an older version of the trec eval tool; however, we later discovered that this version of trec eval did not properly handle the slightly different format of qrel file specified for the Discussion Track, and silently produced incorrect evaluation scores. As a result, we mis-trained our parameters. These problems with deviant formats seriously degraded the effectiveness of our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Run Descriptions</head><p>The runs described below allow for a number of different metric tuning parameters. In order to determine suitable values for these parameters, ten questions were selected from the track topics, and around seventy documents for each were judged prior to run submission, to provide sample qrels for evaluation purposes.</p><p>Base (MU05ENd4). Quoted text was stripped from emails before indexing, so that only the content inserted by that author was matched. The scoring metric used was a language model with Dirichlet smoothing, as described by <ref type="bibr" coords="2,316.68,366.21,108.86,9.82" target="#b3">Zhai and Lafferty [2004]</ref>, in the context of the Zettair retrieval system (see www.seg.rmit.edu.au). This run was used as the basis for the following three variants.</p><p>Base+Quote (MU05ENd2). For this run, a parallel corpus was created from all of the quoted text. This corpus was then separately indexed. Each document was scored using a weighted sum of its scores for the base and parallel indexes. The idea was that, although quoted text was not meant to be directly considered when judging whether a document was relevant, it would nevertheless help indicate the subject of the main body of the email. Once the weighting parameter was tuned, the sample evaluations suggested that this was the second most effective technique.</p><p>Base+Threads (MU05ENd1). The idea behind this run was that an email message is more likely to be relevant to a topic if the thread it is part of is relevant. The thread structure of each mailing list was analyzed and separately stored. The score for each document was then enhanced at query time by the scores of the ancestor and descendant emails in its thread, with the contribution decaying the more distant the ancestor or descendant was. Properly tuned, this metric did best in the sample evaluations.</p><p>Base+Author (MU05ENd3). A simplistic attempt at an authority metric, this run was prepared more out of interest than with the expectation that it would perform very well. The number of messages posted by each author (as identified by email address) was calculated as a proportion of the total number documents in the collection. At query evaluation time, each email's score was then increased by this much percent, multiplied by a tunable parameter.</p><p>The sample judgments showed this method as providing only a slight improvement over the base run.</p><p>Impacts (MU05ENd5). This run was made with a different system, based on an impact-ordered index, and the similarity computation used in the Terabyte Track, see the description below of run MU05TBa1. After the judgments were released, and indeed after the TREC conference itself, it became apparent that something was quite wrong with our runs (see the previous subsection), and we performed further runs with a corrected corpus and re-trained parameters. The Okapi BM25 metric was used for all of these runs, as our experiments with the final judgments and corrected corpus and trec eval found it superior to the language model used for the submitted runs. These runs were as follows:</p><p>NoQuoted Message bodies were extracted from the HTML document collection, and all quoted text was stripped off, similar to the submitted Base run.</p><p>Text Message bodies were extracted from the HTML document collection, with the surrounding HTML markup discarded. Similarly, indexing pages in the HTML collection were discarded entirely. Quoted text was neither stripped out nor indexed separately, but left in place.</p><p>HTML The HTML corpus was indexed directly, with no pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HTML+Threads</head><p>The HTML corpus was indexed directly, and message thread information was used as for the submitted Base+Threads run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results</head><p>Table <ref type="table" coords="3,126.48,538.65,5.45,9.82" target="#tab_0">1</ref> gives the official results for the five submitted runs, as well as unofficial results for four further runs that were computed post-judgment using the official qrels. Of the five submitted runs, the best result was achieved by Impacts. The ordering of the others is similar to that predicted by the sample evaluations, except that Base+Author did not do as well as Base alone. Tuning the various metric parameters against the official judgments, rather than the sample ones, can improve these first four scores slightly, but does not change the ordering. The post-submission runs reveal the problems with our submitted runs caused by corpusparsing errors, as can be seen by the improved MAP for NoQuoted over the equivalent submitted Base run. However, they also demonstrate fundamental problems with our approach. The far superior retrieval effectiveness of Text over NoQuoted shows that stripping out quoted text is a very bad idea, even if quoted text is not meant to directly contribute to an entry's relevance (whether this requirement was strictly adhered to in the judging process is another question). Even more surprising, indexing the raw HTML, with all of its repeated and extraneous markup, and even its index pages, is more effective than stripping down just to the email bodies. Part of the reason is that a number of index pages have been incorrectly assessed as "relevant" in the judging process. But in addition, the presence of the email subject, and its repetition in links to follow-up messages, appears to be a significant bonus, whereas the portions of the HTML template that are repeated on every page are correctly down-weighted by the metric scheme. Fortunately, these results do demonstrate that taking thread information into account is still a very useful technique.</p><p>We note in passing that the crawl made of the W3C mailing lists is incomplete. In particular, where a four-month period for any given list had more than 500 messages, only the last 500 were included in the collection. This had the effect, apart from anything else, of decapitating threads on some of the most active lists. It is possible that with a complete collection, and therefore complete thread information, the +Threads enhancement technique would perform even better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Terabyte Track</head><p>In this year's Terabyte Track, we participated in the Ad-hoc and Efficiency Tasks using locallydeveloped software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieval Approach</head><p>In the 2004 Terabyte Track we employed an impact-based ranking technique <ref type="bibr" coords="4,446.28,307.65,77.97,9.82;4,99.24,321.21,30.63,9.82">[Anh and Moffat, 2005b]</ref> as the retrieval mechanism. In the framework of the track, the technique turned out to provide an excellent combination of retrieval efficiency and retrieval effectiveness.</p><p>Impact ranking specifies a method to map each pair (t,x), where t is a term in text x, to an integer impact value that represents the importance of t in x. When x is a document d from a document collection, the resultant impact ω d,t is referred to as the document term impact of t in d. When x is a query q, the value ω t,q is the query term impact of t in q. Document and query term impacts normally correlate with the term frequencies in documents or queries, but might or might not also depend on some collection-wide statistics such as collection frequency f t .</p><p>In contrast to conventional information retrieval systems, where raw term statistics are stored in indexes, and most of the similarity calculation is carried out during query processing, impactbased ranking allows computation of all of the document term impacts when the index is being constructed. The document term impacts are then stored in the index, reducing the computational burden during query processing. Normally, the indexes are impact-sorted.</p><p>This year, we continued our investigation of impact-based ranking, focusing mainly on further improving efficiency, even where that risked possible degradation in effectiveness. A concern with the impact indexing scheme used last year was the need to read the whole input document collection twice in order to build the index. The underlying reason was that document term impacts for a term t depended on the collection-wide IDF factor f t . To facilitate faster indexing, this year we employed an impact scheme that allowed the computation of document term impacts without referring to the IDF factor or to any other collection-wide statistics. This method of assigning impacts is reported in <ref type="bibr" coords="4,202.32,592.17,111.51,9.82">Anh and Moffat [2005b]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>under the name of Local-By-Rank-(TF). With</head><p>Local-By-Rank-(TF) impacts, the indexing can be done by reading the text document collection just once.</p><p>Note that although the indexing does not depend on any collection-wide statistics, the IDF factor is still used in ranking. At query time, the IDF factor and query term frequencies are taken into account when calculating query term impacts. Then, a scoring computation is performed based only on the integral document and query term impacts. No floating-point operations are required, and no document weights are used or even computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Index Structures</head><p>For a document collection, the principal component of its index is the inverted file, where each distinct term of the collection is associated with an inverted list. For this Terabyte Track, two different inverted list structures were employed: impact-sorted: The impact-sorted inverted list for a term t is a list of equal-impact blocks. Each block represents one distinct impact value ω and contains the sequence of document numbers in which t appears and has an impact score of ω. Inside a block, document numbers are arranged in increasing order to facilitate compression of these values. The blocks are arranged in decreasing order of associated impacts to support effective pruning.</p><p>impact-positional: Here, the inverted list for t is similar to that of a conventional positional inverted list, where a document d in the list is also accompanied by a sequence of positions of t in d. The elements of each inverted list are sorted in increasing order of document numbers, and no equal-impact blocking is performed. Unlike conventional positional lists, however, each document d in the inverted list of t is also accompanied by the document impact ω d,t .</p><p>Compression is applied to inverted files. In all of our Terabyte Track experiments a wordaligned compression scheme <ref type="bibr" coords="5,231.24,281.25,112.83,9.82">[Anh and Moffat, 2005a]</ref> was used for inverted list compression. This method provides a good balance between index space and decoding speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hardware Configurations and Efficiency Metrics</head><p>Two hardware configurations were used in our experiments this year:</p><p>Single: A machine with a single 2.8 GHz Intel Pentium-4 processor, 1 GB of memory and 250 GB of local SATA disk, running Debian GNU/Linux.</p><p>Cluster: A Beowulf-style cluster, consisting of a server and eight additional nodes, where each node is a Single. The server is a dual 2.8 GHz Intel Xeon with 2 GB of memory, again running Debian GNU/Linux. In this configuration, the document data are uniformly distributed across the nodes in a cyclic manner. Indexing and querying are done in parallel, without communication between the nodes, in an autonomous document-distributed manner. The server only plays the role of a broker: it receives queries, broadcasts queries to the nodes, receives the answer lists from the nodes, and selects the final document output based on the local similarity scores computed by the nodes.</p><p>The efficiency metrics reported in our experiments include Index Time (elapsed time for indexing, not including any time needed to distribute documents to the nodes), Index Size (total size of the index, including vocabulary and inverted files) and Query Time (average elapsed time to process a query). Over a sequence of queries, the total of Query Time was measured from the moment when the first query arrived, until the last query had been processed, not including the initialization costs associated with loading a range of memory-resident files. Note that in this implementation queries are processed sequentially, with a single query active in the system at any given time. Further throughput gains would be possible by multi-threading the system, so that, for example, CPU idle periods arising from disk operations in one process could be exploited by another process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effectiveness Performance</head><p>The following four runs were submitted for the Ad-hoc Task. All the four runs were conducted using the hardware configuration Single.</p><p>Base (MU05TBa1). This is the baseline, performed with a standard impact-sorted index, using the Local-By-Rank-(TF) computation <ref type="bibr" coords="5,292.08,737.25,107.41,9.82">[Anh and Moffat, 2005b]</ref>. For this run, all the normal content of documents are indexed. No special treatment was applied to any fields (meta, title, heading, and so on). Incoming anchor text is not considered. Base+Anchor (MU05TBa2). This run differs from the baseline by also considering incoming anchor text. The incoming anchor text is simply treated as normal text within the target document. That is, incoming anchor text redefines the within-document frequencies, which in turn result in different document term impacts being assigned.</p><p>Base+Prox (MU05TBa3). This run is similar to the baseline, except that positions of terms in documents are taken into account. The index is document-sorted rather than impact-sorted (that is, it is an impact-positional index). Impact scoring is also applied, but the total score for a document is enhanced by another integer score which is calculated based on the inverse of the least distance in words between the positions of any of the query terms in the document.</p><p>Base2 (MU05TBa4). This run is almost identical to the baseline run. The intention for the run was to make some change in the way impacts were defined. Unfortunately, we subsequently discovered an error that made the intended changes almost silent. For completeness, we report the result for this run, but do not discuss it.</p><p>Table <ref type="table" coords="6,141.48,584.73,5.45,9.82" target="#tab_1">2</ref> shows the performance of these Ad-hoc runs. In terms of effectiveness, all of the runs had similar performance. On the other hand, the baseline run is slightly better than others, suggesting that the methods chosen to integrate incoming anchor text and positional information were ineffective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Efficiency Performance</head><p>In addition to the Ad-hoc runs, the following four runs were submitted for the Efficiency Task:</p><p>BaseSingle (MU05TBy2). This is the baseline run for the Efficiency Task. It has the same setting as MU05TBa1 -the baseline of the Ad-hoc runs. However an oversight in selecting the documents meant that the final list of documents differed slightly from that of MU05TBa1.</p><p>The run was performed using the hardware configuration Single. BaseCluster (MU05TBy1). This run had the same setting as the Base-Single. The only difference is that it was conducted using the hardware configuration Cluster. Retrieval effectiveness changed slightly because in the cluster environment the IDF components are local to each node.</p><p>Cluster+Prune (MU05TBy3). This run had Base-Cluster as a starting point, but used a static index pruning technique, under which the low-impact blocks were excluded from the index at index construction time.</p><p>Cluster+2004 (MU05TBy4). This run employed the software that we used last year in the Terabyte Track, in order to quantify the level of improvement obtained by the software re-write.</p><p>Efficiency performance on a single-computer system is described in Table <ref type="table" coords="7,453.48,398.49,5.45,9.82" target="#tab_2">3</ref> (and also, for the effectiveness runs, in Table <ref type="table" coords="7,240.48,412.05,3.91,9.82" target="#tab_1">2</ref>). Using a single commodity machine, we can index the GOV2 collection in under six hours, at a rate of approximately 70 GB per hour; and can process queries at the rate of approximately five per second. Moreover, the index is small -just 1.4% of the size of the original document collection.</p><p>In terms of efficiency, Table <ref type="table" coords="7,244.80,466.17,5.45,9.82" target="#tab_1">2</ref> reveals the disadvantage of using anchor text and positional index information. Using anchor text more than doubles the indexing time, while using an impactpositional index increases the index size by a factor of six, and the query time by a factor of five.</p><p>To retain a high level of efficiency, innovative methods for handling the anchor text and word positions are required.</p><p>Table <ref type="table" coords="7,140.16,533.97,5.45,9.82" target="#tab_3">4</ref> compares the performance of the three runs which were performed on the cluster. The comparison between BaseSingle (in Table <ref type="table" coords="7,300.60,547.53,4.52,9.82" target="#tab_2">3</ref>) and BaseCluster shows the benefits of using the cluster -indexing time was cut by a factor of almost eight, and query time by a factor of approximately five. Table 4 also shows that static pruning (method Cluster+Prune) helps to reduce the query time significantly, with only a small degradation in effectiveness.</p><p>Finally, Table <ref type="table" coords="7,176.04,601.65,5.45,9.82" target="#tab_3">4</ref> shows a pleasant result for our group when comparing last year's system with this year's system. Lines BaseCluster and Cluster+2004 demonstrate that this year's system is twice as fast as last year's, in terms of both indexing time and querying time, vindicating the considerable effort that went into the software re-write. This year's system also has slightly better effectiveness.</p><p>Figure <ref type="figure" coords="7,144.96,669.45,5.45,9.82" target="#fig_0">1</ref> illustrates the relative performance of our system in the Efficiency Task. The graphs were compiled from data covering the 16 runs submitted by the eight groups with the best scores according to the metric P@20, as reported in Table <ref type="table" coords="7,338.28,696.57,5.45,9.82" target="#tab_3">4</ref> of <ref type="bibr" coords="7,361.44,696.57,117.86,9.82" target="#b2">Clarke and Scholer [2005]</ref>. Two of our runs from these 16 are BaseCluster and Cluster+Prune, which were conducted on the cluster. To complete the picture, the run BaseSingle is added, to show the performance of our the system on the single CPU configuration. Recall that apart from using the single CPU system, the run BaseSingle is similar to BaseCluster in all other aspects. Figure <ref type="figure" coords="7,438.60,750.69,5.45,9.82" target="#fig_0">1</ref> demonstrates that our system is highly competitive in terms of both efficiency and effectiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Directions</head><p>We intend to complete the construction of the new retrieval system and undertake further investigations in connection with anchor text and word positions. In the field of document retrieval from email corpora, we are interested in further exploring the use of thread context and summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,99.24,249.93,425.07,9.82;8,99.24,264.93,424.73,9.82;8,99.24,279.81,424.95,9.82;8,99.24,294.59,424.96,9.95;8,99.24,309.57,215.73,9.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Relative performance in the Efficiency Task of the Terabyte Track (using the GOV2 collection), with effectiveness compared to efficiency (left-hand graph) and to economy (right-hand graph), the latter calculated as throughput normalized by estimated system cost (in terms of purchase price converted to mid-2005 $US). Note that efficiency is measured over 50,000 real-life queries; effectiveness over a subset of 50 queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,126.48,737.13,398.01,36.94"><head>Table 1 :</head><label>1</label><figDesc>The same collection, stripped of quoted text, was used as for Base. Mean average precision for the five submitted Discussion Track runs, and for four subsequent experiments using the relevance judgments.</figDesc><table coords="2,126.48,750.69,397.84,23.38"><row><cell>Because of delays in readying the software, this run was not assessed during the sample</cell></row><row><cell>evaluation process.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,103.08,102.33,417.29,218.51"><head>Table 2 :</head><label>2</label><figDesc>Performance in the Ad-hoc Task. All results relate to the GOV2 collection.</figDesc><table coords="6,103.08,102.33,417.29,218.51"><row><cell></cell><cell></cell><cell>Efficiency</cell><cell></cell><cell></cell><cell>Effectiveness</cell><cell></cell></row><row><cell>RunID</cell><cell>Index size</cell><cell>Index time</cell><cell>Query time</cell><cell cols="2">R.Rank P@10 P@20</cell><cell>MAP</cell><cell>bpref</cell></row><row><cell></cell><cell cols="3">(GB) (minutes) (secs)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Base</cell><cell>5.99</cell><cell>350</cell><cell>0.2</cell><cell cols="4">0.8240 0.6260 0.5760 0.3199 0.3366</cell></row><row><cell>Base+Anchor</cell><cell>6.07</cell><cell>749</cell><cell>0.2</cell><cell cols="4">0.8086 0.6200 0.5730 0.3218 0.3399</cell></row><row><cell>Base+Prox</cell><cell>38.79</cell><cell>438</cell><cell>0.9</cell><cell cols="4">0.8174 0.6360 0.5760 0.3063 0.3264</cell></row><row><cell>Base2</cell><cell>5.99</cell><cell>350</cell><cell>0.2</cell><cell cols="4">0.8228 0.6260 0.5760 0.3092 0.3267</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Efficiency</cell><cell></cell><cell cols="2">Effectiveness</cell></row><row><cell>Run ID</cell><cell></cell><cell>Index size</cell><cell>Index time</cell><cell>Query time</cell><cell cols="2">R.Rank P@10 P@20</cell></row><row><cell></cell><cell></cell><cell cols="2">(GB) (minutes)</cell><cell>(sec)</cell><cell></cell><cell></cell></row><row><cell cols="2">BaseSingle</cell><cell>5.99</cell><cell>350</cell><cell>0.1983</cell><cell cols="2">0.8069 0.6000 0.5480</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,102.36,333.81,416.01,9.82"><head>Table 3 :</head><label>3</label><figDesc>Performance of Efficiency runs in the single computer. All results relate to the GOV2 collection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,120.72,102.33,379.29,114.46"><head>Table 4 :</head><label>4</label><figDesc>Performance of Efficiency runs in the cluster. All results relate to the GOV2 collection.</figDesc><table coords="7,132.00,102.33,359.45,91.67"><row><cell></cell><cell></cell><cell>Efficiency</cell><cell></cell><cell>Effectiveness</cell></row><row><cell>Run ID</cell><cell>Index size</cell><cell>Index time</cell><cell>Query time</cell><cell>R.Rank P@10 P@20</cell></row><row><cell></cell><cell cols="2">(GB) (minutes)</cell><cell>(sec)</cell><cell></cell></row><row><cell>BaseCluster</cell><cell>6.89</cell><cell>44</cell><cell>0.0429</cell><cell>0.8169 0.6360 0.5620</cell></row><row><cell>Cluster+Prune</cell><cell>6.36</cell><cell>44</cell><cell>0.0240</cell><cell>0.8095 0.6280 0.5550</cell></row><row><cell>Cluster+2004</cell><cell>7.18</cell><cell>104</cell><cell>0.0884</cell><cell>0.7646 0.5540 0.5030</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment This work was supported by the <rs type="funder">Australian Research Council</rs> and the <rs type="funder">ARC Center for Perceptive and Intelligent Machines in Complex Environments</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,99.24,509.61,424.95,9.82;8,110.16,523.17,195.57,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,213.84,509.61,263.25,9.82">Inverted index compression using word-aligned binary codes</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,485.64,509.61,38.55,9.82;8,110.16,523.17,57.77,9.82">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="166" />
			<date type="published" when="2005-01">January 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.24,545.73,424.77,9.82;8,110.16,559.29,413.67,9.82;8,110.16,572.73,414.05,9.82;8,110.16,586.29,266.97,9.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,223.80,545.73,204.29,9.82">Simplified similarity scoring using term ranks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Anh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,380.52,559.29,143.31,9.82;8,110.16,572.73,355.73,9.82">Proc. 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</editor>
		<meeting>28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Salvador, Brazil; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.24,608.85,425.19,9.82;8,110.16,622.41,414.03,9.82;8,110.16,635.85,414.93,10.07;8,110.16,649.41,100.65,10.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,252.48,608.85,146.13,9.82">The TREC 2005 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/act_part/t14_notebook/t14.notebook.html" />
	</analytic>
	<monogr>
		<title level="m" coord="8,426.00,608.85,98.43,9.82;8,110.16,622.41,201.93,9.82">The Fourthteenth Text REtrieval Conference (TREC 2005) Notebook</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,99.24,671.97,424.85,9.82;8,110.16,685.53,355.65,9.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,201.72,671.97,322.37,9.82;8,110.16,685.53,34.42,9.82">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,152.52,685.53,185.30,9.82">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
