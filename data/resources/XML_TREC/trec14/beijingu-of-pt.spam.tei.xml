<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.94,109.51,384.38,12.24;1,114.00,125.41,367.18,12.24">PRIS Kidult Anti-SPAM Solution at the TREC 2005 Spam Track: Improving the Performance of Naive Bayes for Spam Detection</title>
				<funder>
					<orgName type="full">Cross -Century Talents Foundation</orgName>
				</funder>
				<funder ref="#_2vA7wcF">
					<orgName type="full">National Natural Science Foundation of China, Beijing, China</orgName>
				</funder>
				<funder ref="#_c3Rt59X">
					<orgName type="full">Key Project of Foundation of Ministry of Education of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,199.86,162.48,41.60,8.77"><forename type="first">Yang</forename><surname>Zhen</surname></persName>
							<email>yangzhen@pris.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.42,162.48,41.15,8.77"><forename type="first">Xu</forename><surname>Weiran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.49,162.48,32.30,8.77"><forename type="first">Chen</forename><surname>Bo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.87,162.48,31.77,8.77"><forename type="first">Hu</forename><surname>Jiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.59,162.48,32.87,8.77"><forename type="first">Guo</forename><surname>Jun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="laboratory">PRIS Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.94,109.51,384.38,12.24;1,114.00,125.41,367.18,12.24">PRIS Kidult Anti-SPAM Solution at the TREC 2005 Spam Track: Improving the Performance of Naive Bayes for Spam Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE83A8771F8EE10AF4D7B13B70C0BDFD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the spam already constituted a serious problem for both e-mail users and Internet Service Providers (ISP). Solutions to the abuse of spam would be both technical and legal regulatory. This paper reports our solution for the TREC 2005 spam track, in which we consider the use of Naive Bayes spam filter for its desirable properties (simplicity, low time and memory requirements, etc.). Then the approaches to modify the Naive Bayes by simply introducing weight and classifier assemble based on dynamic threshold are proposed, which can help to improve the accuracy of a Naive Bayes spam classifier dramatically. Additionally, we discuss some steps that must be adopted naturally thought before, such as stop list, word stemming, feature selection, class prior probabilities.</p><p>The theory analysis implies these steps are not necessarily the best way to extend the Bayesian classifier, and these were also verified empirically. Many of these techniques appear to be counterintuitive but can be explained by the statistical properties of e-mail itself. Experiment results of TREC 2005 spam track demonstrate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the spam already became a serious actual problem, which merely was a latent threat several years ago <ref type="bibr" coords="1,513.94,474.18,11.40,8.77" target="#b7">[8]</ref> [12] <ref type="bibr" coords="1,90.78,488.76,15.00,8.77" target="#b13">[14]</ref>. Though the purpose of e-mail is to make communication more convenient, e-mail does not always provide the increased efficiency desired. World widely, spam is estimated to comprise 69% of global e-mail and the percentage of spam has risen steadily during the past couple of years <ref type="bibr" coords="1,351.42,517.98,15.01,8.77" target="#b13">[14]</ref>. The growing volumes of spam causes huge losses to both e-mail users and ISP due to bandwidth consumption, storage space, mail server processing load, user's efficiency ( time spent for responding, deleting or forwarding etc <ref type="bibr" coords="1,350.14,547.14,15.78,8.77" target="#b11">[12]</ref>  <ref type="bibr" coords="1,368.32,547.14,16.33,8.77" target="#b13">[14]</ref> ).</p><p>Though there is no standardized definition for spam, the TREC 2005 track's definition of spam is "Unsolicited, unwanted email that was sent indiscriminately, directly or indirectly, by a sender having no current relationship <ref type="bibr" coords="1,70.02,590.88,15.37,8.77" target="#b23">[24]</ref>." Generally speaking, the spam detection belongs to problem of the information security domain. People stressed in formal securities, such as the confidentiality, integrality and availability. So in the first time, we doubt whether the spam detection can be treated as a special problem in text categorization or not. But the TREC 2005 spam track let us see the successful application of statistical method based on content. More importantly, we realize that the modern information security should not only be the formal security but also be the content security, which users can enjoy information sharing, in the same time avoid the information abusing in the greatest degree.</p><p>Traditional techniques cope with spam include header analysis and tracking based on sender address or header content, digital signatures, keyword/keyphrase matching and analogous rule-based predicate <ref type="bibr" coords="1,434.03,693.01,14.96,8.77" target="#b13">[14]</ref>. The problem with traditional techniques is that sometimes a valid message may be blocked. Furthermore, in realistic terms, this is really about 1 to 3 truly new spams or spam methods per month. Instead of blocking spam simply, this work aims at deciding whether or not the latent subject matter is consistent with the user's interests.</p><p>Statistical filtering tends to automatically reject e-mail that is classified as spam relying on user's intent. Therefore, the statistical learning techniques are more suitable for spam detection. The state of art include rule learning, Naive Bayes, memory based learning, decision trees, support vector machines or combinations of different learners <ref type="bibr" coords="2,140.76,183.73,45.25,8.77">[3] [6] [15]</ref> [16] <ref type="bibr" coords="2,208.45,183.73,16.33,8.77" target="#b16">[17]</ref> ] <ref type="bibr" coords="2,234.13,183.73,16.33,8.77" target="#b17">[18]</ref>  <ref type="bibr" coords="2,256.57,183.73,14.96,8.77" target="#b18">[19]</ref>. Among these techniques, Bayesian methods <ref type="bibr" coords="2,455.91,183.73,11.40,8.77">[1]</ref> [2] [4] [5] <ref type="bibr" coords="2,514.00,183.73,11.41,8.77" target="#b8">[9]</ref> and their improvements <ref type="bibr" coords="2,169.82,198.31,16.32,8.77" target="#b9">[10]</ref>  <ref type="bibr" coords="2,190.10,198.31,16.33,8.77" target="#b10">[11]</ref> are particularly attractive, because they more formally model the relationship between the content of the spam and the reading favors of the user.</p><p>Naive Bayes uses a simple probabilistic model that makes strong assumptions about the data: it assumes that words in a document are independent. Clearly, this assumption is violated in most natural language text; therefore, some techniques including augmented Naive Bayesian network and augmented Naive Bayes (ANB) <ref type="bibr" coords="2,492.29,256.69,16.32,8.77" target="#b19">[20]</ref> are proposed to relax independent assumption. Nonetheless, Naive Bayes performs quite well in practice even when attributes are not independent <ref type="bibr" coords="2,188.25,285.84,11.40,8.77">[1]</ref> [2], often comparable to more sophisticated learning methods.</p><p>In this report, a spam detection filter framework is proposed based on Naive Bayes for its desirable properties. And then the approaches to modify the Naive Bayes by simply introduce weight and classifier assemble based on dynamic threshold are proposed. Additionally, we discuss some steps that must be adopted naturally thought before, such as stop list, word stemming, feature selection, class prior probabilities. The theory analysis implies these steps are not necessarily the best way to extend the Bayesian classifier, and these were also verified empirically. Many of these techniques appear to be counterintuitive but can be explained by the statistical properties of e-mail itself. Experiment results of TREC 2005 spam track demonstrate the effectiveness of the proposed method.</p><p>The resulting technology has been successfully released in TREC 2005 spam track system 'Kidult Anti-SPAM Solution'. Another rich on-line resources and information about anti-spam include http://plg.uwaterloo.ca/~gvcorma c/spam/, http://www.ceas.cc/, and http://www.spam.com.cn/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Naive Bayes Spam Detection Filter Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Naive Bayes</head><p>Naive Bayes is often used in text classification applications and experiments for its simplicity and effectiveness <ref type="bibr" coords="2,513.96,543.06,11.40,8.77">[1]</ref> [2] [5] <ref type="bibr" coords="2,100.37,557.64,10.33,8.77" target="#b8">[9]</ref>. And spam detection poses a special problem in text categorization. The Naive Bayes classifier is a probability based approach. The basic concept of it is to find whether an e-mail is spam or not by looking at which words are found in the message and which words are absent from it <ref type="bibr" coords="2,336.05,586.80,11.41,8.77" target="#b6">[7]</ref>  <ref type="bibr" coords="2,349.85,586.80,15.00,8.77" target="#b12">[13]</ref>.</p><p>In the literature, the Naive Bayes classifier is defined as follows:</p><formula xml:id="formula_0" coords="2,199.62,608.19,322.21,27.47">arg max ( ) ( | ) NB i k i i L k C PC Pw C ∈ = ∏<label>(1)</label></formula><p>The e-mail composes of k w words, where L is the set of target classes. There are several Naive Bayes models that make different assumptions about how documents are composed from the basic units. The most common models are: multi-variate Bernoulli model, Poisson Naive Bayes model, and the multinomial model <ref type="bibr" coords="2,427.44,670.74,14.97,8.77" target="#b21">[22]</ref>. The most apparent difference between these models is ways of ( | )</p><formula xml:id="formula_1" coords="2,246.18,684.09,94.10,12.41">k i P w C calculation.</formula><p>In this work, ( | ) k i P w C is calculated using multinomial model for its superior performance <ref type="bibr" coords="2,258.27,699.90,15.00,8.77" target="#b21">[22]</ref>.</p><p>For spam detection, there are only two classes ( C + spam/ C -ham)，the score of an input e-mail M calculated as follow, and the logarithm formula of ( <ref type="formula" coords="3,219.32,125.40,3.82,8.77" target="#formula_0">1</ref>) is used:</p><formula xml:id="formula_2" coords="3,109.86,132.73,415.38,27.43">score(M)= log ( ) log ( | ) (log ( ) log ( | )) k k k k P C P w C P C P w C + + - - + - + ∑ ∑<label>(2)</label></formula><p>Therefore，if score(M) &gt; 0, the email will be assigned to C + , and C -otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spam Detection Filter Framework</head><p>Incoming Mail</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spam Detection Filter</head><p>Ham Spam</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold-standard Judgement</head><p>Training Filter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Spam Detection Filter Framework</head><p>In TREC 2005 spam track, spam filtering was defined as a continuously and most practical applications based on online user feedback with fast, incremental and robust learning algorithms. The framework show in Fig. 1 <ref type="bibr" coords="3,506.53,425.22,15.00,8.77" target="#b23">[24]</ref>, which supervised filtering involves the filter and recipient in a closed loop. The recipient regularly examines the ham and spam files and reports misclassifications according to gold-standard back to the filter, which updates its memory accordingly. Obviously, most of mail process terminal software (Microsoft TM Outlook TM etc.) adopts this model, and the only difference is that the performance of the filter is measured by users <ref type="bibr" coords="3,413.77,483.60,14.96,8.77" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Criteria</head><p>In TREC 2005 spam track, Evaluation will be based on these measures: a) HMR: Ham Misclassification Rate, the fraction of ham messages labeled as spam. b) SMR: Spam Misclassification Rate, the fraction of spam messages labeled as ham. c) LAM: Logistic average misclassification. d) 1-ROCA: Area above the ROC curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Representation and Selection</head><p>Feature representation and selection is one of the important links of text categorization. Many complex techniques, such as natural language understanding (NLU) and named entity recognition, can help us to achieve better textual understanding, and enable machine work more intelligently. But if our goal merely is simple level processing of text (e.g. text categorization, spam detection), not involving the higher level application (e.g. natural language's meaning's understanding and the ambiguous natural language's meaning of the computer's understanding). Perhaps the usage of these complex techniques without fully understanding led to performance degradation. In this section, we discuss some steps that must be adopted naturally thought before, such as stop list, word stemming, feature selection, class prior probabilities. The theory analysis implies these steps are not necessarily the best way to extend the Bayesian classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Representation Pretreatment</head><p>For text classifying, the feature is a word. Our filter does not use HTML tags as tokens. Such tags, as well as other information such as images, links and attachment are simply eliminated. In typical text classifying tasks, some measures were often considered:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Word Stemming</head><p>Word stemming is similar to cluster, which can cluster the words with same stem together. Use of word stemming can lowers the size of the feature vector, but it may be the case that certain forms of a word (such as sex and sexy) were important for classification.</p><p>Consider a mail M, described by two attributes A, B. Assume that the two classes, denoted by + and -, are equiprobable P(+) = P(-) =1/2. The optimal classification procedure for test mail is to assign it to class + if</p><formula xml:id="formula_3" coords="4,215.94,359.71,309.31,8.95">P(A|+)P(B|+) -P(A|-)P(B|-) &gt; 0<label>(3)</label></formula><p>and to class -if the inequality has the opposite sign, and to an arbitrary class if the two sides are equal. Applying Bayes' theorem, P(A|+) can be rewritten as P(A)P(+|A)/P(+), and similarly for the other probabilities. Since P(+) = P(-), after canceling like terms this leads to the equivalent expressions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(+|A)P(+|B) -P(-|A)P(-|B) &gt; 0 (4)</head><p>for the optimal decision. Let P(+|A) = p and P(+|B) = q. Then class + should be selected when pq -(1 -p)(1 -q) &gt; 0, which is equivalent to q + p &gt; 1.</p><p>And if A, B have the same stem C, then the mail have only one attribute C after word stemming. The optimal decision function became (using multinomial model):</p><formula xml:id="formula_4" coords="4,241.08,508.26,284.16,8.95">P(+|C) -P(-|C)&gt;0<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(+|C) = P(+)P(C|+)/P(C)=(p+q)/2P(C). And the P(C) = P(A)+P(B)</head><p>, then class + should be selected when p+q &gt; P(C), which is equivalent to q + p &gt; P(A)+P(B). So the classified frontier always moves toward one direction for P(C) &lt; 1. Fortunately, with the increase of words in a special mail, the influence of word stemming gradually became small. For an n-words mail (A, B, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Stop List</head><p>Words like "of," "and," "the," etc., are used to form a stop list. Words on the stop list are not used in forming a feature vector. The rationale for this is that common words are less useful in classification. The argument against using a stop list is that it is not obvious which words, beyond the trivial, should be on the stop list. The choice of words to put on a stop list is probably a function of the classification task and it would be better if learning algorithm itself determined whether a particular word is important or not. For example, financial articles may be distinguished by a prevalence of numeric dollar figures, which may well be discarded wholesale by a preprocessor.</p><p>The most important things are that the Naive Bayes is not sensitive to stop list. Similarly, for an n-words mail (</p><formula xml:id="formula_5" coords="5,74.16,122.23,115.50,14.31">1 { } m i i w = , 1 { } n j j m w = + ), 1 { } m i i</formula><p>w = are the words in the stop list. Let P(+|w i )=k i ,i=3,…,n, then the optimal decision function without using stop list is (logarithm formula of (1) is used):</p><formula xml:id="formula_6" coords="5,159.18,181.33,236.72,37.45">1 1 1 1 1 1 log log log 0 (1 )<label>(1 ) (1 )</label></formula><formula xml:id="formula_7" coords="5,176.34,158.17,348.90,60.63">n m n i i i i i i m n m n i i i i i i m k k k k k k = = = + = = = + = + &gt; - - - ∏ ∏ ∏ ∏ ∏ ∏<label>(6)</label></formula><p>The optimal decision function using stop list is: <ref type="formula" coords="5,275.97,288.55,4.36,10.50" target="#formula_0">1</ref>)</p><formula xml:id="formula_8" coords="5,234.66,271.82,85.81,37.39">1 1 log 0 (</formula><formula xml:id="formula_9" coords="5,252.06,248.66,273.19,60.56">n i i m n i i m k k = + = + &gt; - ∏ ∏ (7)</formula><p>For most application, these word in the stop list appear evenly in ham and spam mail, therefore, the influence of word stemming is small. For a typical mail text, the influence can be omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Selection</head><p>In typical text categorization solutions, we suppose that there have possible advantage of using a finite number of features rather than all. Some mechanisms designed to find the optimum number of features are document frequency ratios, information gain, mutual information, term strength, and 2 χ [16] <ref type="bibr" coords="5,354.40,422.76,14.94,8.77" target="#b18">[19]</ref>.</p><p>One important reason for feature selection is the attributes dependent. Feature subset selection is hoped to improve accuracy on some data sets, but the effection varied from application to application depending on the learning algorithm. Especially, <ref type="bibr" coords="5,158.70,466.51,11.40,8.77">[1]</ref>  <ref type="bibr" coords="5,172.86,466.51,11.40,8.77" target="#b1">[2]</ref> show Naive Bayes is optimal even when attributes are not independent. And the main disadvantage of searching for the best features is that it requires additional time in the training algorithm. It would be far better if the learning machine itself either made the feature selection automatically or used all the features. As implied in <ref type="bibr" coords="5,112.17,510.25,11.40,8.77">[1]</ref> [2], feature selection is not necessarily the best way to extend the Bayesian classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Class Prior Probabilities</head><p>In typical text classification, ignoring prior probabilities altogether (or equivalently, assuming uniform priors) was widely employed. Because in practice, the classification scores are dominated by the word probabilities, and the prior probabilities hardly affect the classification for longer documents. However, in situations where documents are usually very short, especially in our spam detection filter framework (see figure <ref type="figure" coords="5,395.27,612.13,3.53,8.77">1</ref>), because the system comes in with an empty memory and learns what spam is, from the user. In such online feedback cases, the prior probabilities may be oscillatory and skewed and affect the classification obviously. In this work, we found the real class prior probabilities calculated from actual input stream can improve the accuracy of classifier, especially for these samples located around classifying hyperplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>Preprocessing requires language dependent mechanisms like word stemming, stop list, feature selection that may not be readily available for the language now. Based on a review of the literature, in our 'Kidult Anti-SPAM Solution' for TREC 2005 spam track, we use all word as feature without stemming and discarding the words in stop list. And the actual class prior probability calculated from actual input stream was used. But our filter does not use HTML tags as tokens. Such tags, as well as other information such as images, links and attachment are simply eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improving the Performance of Naive Bayes for Spam Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weighted Naive Bayes</head><p>There are two possible approaches that can improve performance of Naive Bayes: (1) modify the data, (2) modify the classifier (or the probabilistic model) <ref type="bibr" coords="6,242.25,316.50,35.52,8.77">[11] [23]</ref>. Many researchers have proposed modifications to the way documents are presented, to better fit the independent assumptions made by Naive Bayes. This includes extracting more complex features, n-Gram, and word clustering. These methods did show some improvement of classification accuracy, but have been largely unsuccessful. And based on the above discuss, Naive Bayes is not sensitive to data and the effection varied from application to application. On the other hand, researchers have tried to improve the performance by using more complex probabilistic model alleviating the effection of independent assumption. This includes TAN (Tree augmented Naive Bayes), TAN assemble, and Bayes network <ref type="bibr" coords="6,401.52,403.99,14.96,8.77" target="#b19">[20]</ref>. These methods were very difficult to optimize even were the NP-hard problem.</p><p>In this section, an approaches to modify the Naive Bayes by simply introducing weight was proposed, which can help to improve the accuracy of a Naive Bayes spam classifier dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Framework</head><p>In text classification, the feature is word, and we always ignore syntax information altogether. Many researchers think this is the key making classifier degraded, and tried to employ more complex techniques to improve this model, such as Nature Language Processing (NLP). But these attempts have been largely unsuccessful, that did not provide a significant benefit on any natural data sets only for complex and invalid.</p><p>In this section, an improvement using simple syntax information is proposed. The basement of our scheme is that some parts of the document may have stronger dependence on the label than other parts. If every input e-mail can be divided into S components, and every component is composed of</p><formula xml:id="formula_10" coords="6,329.16,573.80,196.10,14.35">N d , d=1,…,S, words({ } d k w ,k=1,…, N d , d=1,…,S).</formula><p>The natural extend to the Naive Bayes is to introduce weight to every component, then the (1) can rewrite as:</p><formula xml:id="formula_11" coords="6,170.04,604.10,355.20,30.07">1 1 arg max ( ){ ( | )} d N S d NB i d k i i L d k C PC Pw C α ∈ = = = ⋅ ∏ ∏<label>(8)</label></formula><p>And the weight d α , d=1,…,S is introduced. ( <ref type="formula" coords="6,257.19,640.08,3.80,8.77" target="#formula_12">9</ref>) is the logarithm formula of (8) is used):</p><formula xml:id="formula_12" coords="6,146.64,652.75,378.60,29.90">1 1 arg max{log ( ) (log log ( | ))} d N S d NB i d k i i L d k C PC Pw C α ∈ = = = + + ∑ ∑<label>(9)</label></formula><p>(10) is the normalized formula using N d that can denote the effection of text length:</p><formula xml:id="formula_13" coords="6,161.34,701.29,359.82,30.08">1 1 arg max ( ){ ( | )} d d N S N norm d NB i d k i i L d k C PC Pw C α ∈ = = = ⋅ ∏∏ (<label>10</label></formula><formula xml:id="formula_14" coords="6,521.16,706.80,4.08,8.77">)</formula><p>Where α is chosen as follow, given a training set of m labeled e-mail ( The first natural extense was that dividing the text according the syntax structure (e.g. Title, header, paragraph, accessory). <ref type="bibr" coords="7,139.34,214.81,16.33,8.77" target="#b10">[11]</ref> employ this to USENET posting that can be considered to consist of a subject line and a body component. <ref type="bibr" coords="7,141.76,229.45,16.32,8.77" target="#b10">[11]</ref> achieve good performance, but it is not suitable for e-mail because the header and accessory were ignored and too short to use. b) In this work, we divide the words of e-mail into different components by its probability. Consider a mail, described by n words. For a special word w, if P(w|+)&gt;P(w|-), is to assign it to I + component, and assign to I - component otherwise. This I + were though to be help for a e-mail assigned to spam, and I -are help for a email assigned to ham. So introducing weight were help to improve the performance of Naive Bayes under zero-one loss function <ref type="bibr" coords="7,183.92,316.93,24.67,8.77">[1] [2]</ref>. Weighted Naïve Bayes can modify the classified frontier adaptively according the characteristic of sample, which can improve the classify accuracy for these sample located around classifying hyperplane. And this method was used in our system. The Bagging <ref type="bibr" coords="7,123.93,633.54,11.40,8.77" target="#b2">[3]</ref> predictor is a technique to combine a number of weak learners to form an ensemble. The Bagging predictor is a PAC (probably approximately correct) method to combine a number of weak learners with error rate slightly better than 50% to form an ensemble. In classification task, aggregating can transform predictors into nearly optimal ones <ref type="bibr" coords="7,125.06,677.35,10.38,8.77" target="#b2">[3]</ref>. Spam detection filter framework based on Naive Bayes bagging aggregate (see Fig. <ref type="figure" coords="7,486.86,677.35,4.06,8.77" target="#fig_2">2</ref>) can be naturally induced form the basic model (see Fig. <ref type="figure" coords="7,266.37,691.92,3.53,8.77">1</ref>). This version of Naive Bayes bagging works as following: the filtering involves the filter and recipient in a closed loop. The recipient regularly examines the ham and spam files and reports misclassifications according to gold-standard back to the random filter (Filter #1~ Filter #n), which updates its memory accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classifier Assemble</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Naive Bayes Spam Filter Bagging Based on Dynamic Threshold</head><p>Unfortunately, the empirical results show that simply voting does not have the significant superiority compared to single classifier. This is partly because the number of filters is few and most of them give the same decision for a special e-mail. Though it is widely known that combining multiple classification or regression models typically provides superior results compared to using a single, well-tuned model. However, the ways of combining multiple classifiers still is the important factor affecting the aggregated predictor performance. By using incremental decision tree induction (ITI) [26], the performance can be further improved by generating the assemble predictor from scores made by filter group instead of the binary decisions. Originally, every filter makes binary decision spam/ham for the incoming e-mail M, i.e., if score (M) &gt; 0, the email will be assigned to spam, and ham otherwise. And then the assemble prediction was generated based on these binary decisions. But now the decision tree was generated using the score (M) itself. It means that we were not only to combine a number of weak learners to form an ensemble, but also to adjust the classified frontier of the Naive Bayes automatically by generating dynamic threshold for every filter. The assemble predictor based on dynamic threshold more correctly represent the difference of every classifier. The typical C4.5 <ref type="bibr" coords="8,137.96,356.65,16.28,8.77" target="#b25">[27]</ref> can work well, and ITI algorithm performs incremental decision tree induction on symbolic or numeric variables, and handles noise and missing values. Thus ITI can reduce the computational complexity significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">KidSPAM Filter Results of TREC 200Spam Track</head><p>In this section, we report the test results on eight email datasets provided by TREC 2005 spam track. The basic statistics for all eight datasets are given in Table <ref type="table" coords="8,264.20,467.46,3.66,8.77" target="#tab_0">1</ref>. Further details about these corpora were described in <ref type="bibr" coords="8,487.32,467.46,16.30,8.77" target="#b23">[24]</ref> [25].</p><p>And the performance of Kidult Anti-SPAM Solution is given in Table <ref type="table" coords="8,367.48,482.10,4.87,8.77" target="#tab_1">2</ref> -Table <ref type="table" coords="8,406.47,482.10,3.68,8.77">3</ref>. Any detail results and the comparison of all participant filters were published in <ref type="bibr" coords="8,281.89,496.68,16.32,8.77" target="#b23">[24]</ref> [25]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions and Conclusion</head><p>Spammers continue to devise aggressive and devious techniques, and we are taking a multi-faceted approach to fighting spam. Statistical filter is considered as a possible solution to protecting consumers from spam. Though technology can help to reduce the attacks but it is never complete and by itself cannot meet all the realistic needs.</p><p>Solutions to the abuse of spam would be both technical and legal regulatory <ref type="bibr" coords="9,382.11,110.82,10.38,8.77" target="#b7">[8]</ref>. In this report, we introduce the work of PRIS lab for TREC 2005 spam track. Future work may be directed towards developing better algorithms for our system, including more valid algorithm for weight selection, and more valid assemble algorithm based on incremental decision tree induction for numeric variables. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,251.82,577.08,3.38,6.09;4,225.36,570.64,20.80,10.54;4,245.76,568.80,3.38,6.09;4,237.90,577.08,9.56,6.09;4,230.82,570.64,7.81,10.54;4,247.98,571.92,277.34,11.26;4,70.02,586.50,360.49,8.77;4,457.56,591.66,3.38,6.09;4,431.10,585.22,20.80,10.54;4,451.50,583.38,3.38,6.09;4,443.64,591.66,9.56,6.09;4,436.56,585.22,7.81,10.54;4,453.72,586.50,11.20,11.26"><head></head><label></label><figDesc>= ), A, B have the same stem. The classification scores are dominated by the words probabilities, and the A, B hardly affect the classification for longer documents 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,375.48,107.65,28.45,6.07;7,398.88,115.87,1.87,6.07;7,367.74,109.51,22.78,10.50;7,401.28,110.82,124.00,11.13;7,70.02,125.40,277.84,8.77;7,228.66,167.71,18.43,6.07;7,301.56,167.71,3.37,6.07;7,182.64,153.25,36.50,10.50;7,247.93,153.25,20.41,10.50;7,306.97,153.25,80.36,10.50;7,300.00,146.90,2.45,4.40;7,295.20,143.89,4.50,6.07;7,224.82,144.79,17.83,6.07;7,347.52,151.39,11.35,6.07;7,375.06,151.39,1.87,6.07;7,277.38,159.61,3.37,6.07;7,346.86,159.61,2.99,6.07;7,222.90,167.71,16.39,6.07;7,294.30,167.71,2.99,6.07;7,323.88,153.25,23.50,10.50;7,366.42,153.25,7.78,10.50;7,206.64,161.02,4.25,8.72;7,162.42,148.84,7.34,15.07;7,269.27,148.84,7.34,15.07;7,225.30,165.54,18.75,8.25;7,298.19,165.54,3.70,8.25;7,173.76,149.50,6.39,14.28;7,284.63,149.50,6.39,14.28;7,221.04,146.34,26.42,21.42;7,293.07,146.34,12.46,21.42;7,508.92,143.46,16.33,8.77;7,111.54,190.75,13.99,6.07;7,100.86,184.39,3.88,10.50;7,126.30,184.39,33.65,10.50;7,150.72,190.75,3.37,6.07;7,80.64,179.98,7.34,15.07;7,104.16,179.98,17.23,15.07;7,142.54,179.98,7.34,15.07;7,91.98,180.64,6.39,14.28;7,161.46,185.64,288.84,8.77;7,72.84,200.22,7.57,8.77"><head>.</head><label></label><figDesc>), and a very natural criterion is to choose to maximize the log likelihood of the labeled training data: It remains to specify how e-mail can be divided in different components. a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,148.80,608.78,297.64,7.88"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Spam Detection Filter Framework Based on Naive Bayes Bagging Aggregate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,120.18,520.56,344.15,110.85"><head>Table 1 .</head><label>1</label><figDesc>Corpus Statisitic</figDesc><table coords="8,293.22,538.46,167.29,7.88"><row><cell>Ham</cell><cell>Spam</cell><cell>Total</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,81.78,178.50,433.91,56.54"><head>Table 2 .</head><label>2</label><figDesc>kidSPAM1-kidSPAM2 Results</figDesc><table coords="9,81.78,196.02,433.91,39.02"><row><cell>Corpus</cell><cell cols="2">Ham Misc%</cell><cell cols="2">Spam Misc%</cell><cell cols="2">Logit av Misc%</cell><cell cols="2">(1-ROCA)%</cell></row><row><cell></cell><cell>kidSPAM1</cell><cell>kidSPAM2</cell><cell>kidSPAM1</cell><cell>kidSPAM2</cell><cell>kidSPAM1</cell><cell>kidSPAM2</cell><cell>kidSPAM1</cell><cell>kidSPAM2</cell></row><row><cell>Trec05p</cell><cell>0.91</cell><cell>0.87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(0.81-1.00)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">National Natural Science Foundation of China, Beijing, China</rs> (Grant No. <rs type="grantNumber">60475007</rs>), <rs type="funder">Key Project of Foundation of Ministry of Education of China</rs> (Grant No. <rs type="grantNumber">02029</rs>), and <rs type="funder">Cross -Century Talents Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2vA7wcF">
					<idno type="grant-number">60475007</idno>
				</org>
				<org type="funding" xml:id="_c3Rt59X">
					<idno type="grant-number">02029</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,73.31,597.50,451.96,7.88;9,81.06,607.82,66.51,7.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,195.52,597.50,253.09,7.88">On the optimality of the simple Bayesian classifier under zero-one loss</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,457.93,597.50,63.57,7.88">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="103" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.31,618.08,452.00,7.88;9,81.06,628.34,176.39,7.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,136.21,618.08,245.53,7.88">A unified bias-variance decomposition for zero-one and squared loss</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.02,618.08,124.29,7.88;9,81.06,628.34,104.00,7.88">Proc. of the 17th National Conference on Artificial Intelligence</title>
		<meeting>of the 17th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.31,638.60,449.75,7.88;9,81.06,648.86,271.13,7.88" xml:id="b2">
	<analytic>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno>460</idno>
		<ptr target="ftp://ftp.stat.berkeley.edu/users/breiman/arcall.ps.Z" />
	</analytic>
	<monogr>
		<title level="m" coord="9,127.21,638.60,126.79,7.88">Bias, variance and arcing classifiers</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>University of California at Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,73.31,659.12,451.98,7.88;9,81.06,669.44,91.01,7.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,121.85,659.12,107.57,7.88">The optimality of Naive Bayes</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,249.34,659.12,275.95,7.88;9,81.06,669.44,67.36,7.88">Proc. of the Seventeenth International Florida Artificial Intelligence Research Society Conference</title>
		<meeting>of the Seventeenth International Florida Artificial Intelligence Research Society Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.30,679.70,451.91,7.88;9,81.06,689.96,221.39,7.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,284.39,679.70,158.04,7.88">A Bayesian approach to filtering junk e-mail</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,461.96,679.70,63.25,7.88;9,81.06,689.96,115.09,7.88">AAAI&apos;98 Wkshp. Learning for Text Categorization</title>
		<meeting><address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07-27">July 27, 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,73.31,700.22,451.92,7.88;9,81.06,710.48,200.34,7.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,243.02,700.22,196.10,7.88">Automatic junk e-mail filtering based on latent content</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Bellegarda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,458.74,700.22,66.49,7.88;9,81.06,710.48,176.73,7.88">ASRU &apos;03 Wkshp. Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.31,107.96,451.93,7.88;10,81.06,118.22,136.74,7.88" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,184.04,107.96,280.02,7.88">A study of supervised spam detection applied to eight months of personal email</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<ptr target="http://plg.uwaterloo.ca/~gvcormac/spamcormack.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.31,128.54,272.58,7.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,114.95,128.54,127.02,7.88">Will new standards help curb spam?</title>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geer</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,248.61,128.54,33.12,7.88">Computer</title>
		<imprint>
			<biblScope unit="page" from="14" to="16" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.30,138.80,256.24,7.88" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,125.81,138.80,53.42,7.88">A plan for spam</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://www.paulgraham.com/spam.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.33,149.06,421.46,7.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,128.82,149.06,82.60,7.88">Better Baysian filtering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://spamconference.org/proceedings2003.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,229.42,149.06,90.77,7.88">Proc. of Spam Conference</title>
		<meeting>of Spam Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,159.32,447.95,7.88;10,81.06,169.58,139.09,7.88" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,189.77,159.32,230.38,7.88">Improving the preformance of Naive Bayes for text classification</title>
		<author>
			<persName coords=""><forename type="first">Yirong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/courses/cs224n/2003/fp/yirong99/report.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,179.84,324.61,7.88" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,133.90,179.84,115.01,7.88">Fighting spam on multiple fronts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Jeffrey</surname></persName>
		</author>
		<ptr target="http://www.spam.com.cn/ppt/Yahoo!.ppt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.33,190.10,447.96,7.88;10,81.06,200.42,107.68,7.88" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,335.28,190.10,119.62,7.88">A unified model of spam filtration</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Yerazunis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chhabra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Siefkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Assis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<ptr target="http://crm114.sourceforge.net/UnifiedFilters.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,210.68,447.97,7.88;10,81.06,220.94,122.47,7.88" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,209.00,210.68,209.86,7.88">Legislation: One of the key pillars in the fight against spam</title>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anti-Spam</forename><surname>Coalition</surname></persName>
		</author>
		<ptr target="http://www.hkispa.org.hk/spam/20040113-coalition-paper.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.33,231.20,447.93,7.88;10,81.06,241.46,291.03,7.88" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,221.92,231.20,295.38,7.88">Spam filtering using a Markov random field model with variable weighting schemas</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chhabra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,81.06,241.46,220.25,7.88">Proc. of Fourth IEEE International Conference on Data Mining</title>
		<meeting>of Fourth IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.33,251.72,447.95,7.88;10,81.06,262.04,127.78,7.88" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,252.25,251.72,174.14,7.88">Support vector machines for spam categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,436.14,251.72,89.14,7.88;10,81.06,262.04,19.68,7.88">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1048" to="1054" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,272.30,447.95,7.88;10,81.06,282.56,239.13,7.88" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,135.06,272.30,313.26,7.88">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,457.75,272.30,67.52,7.88;10,81.06,282.56,215.59,7.88">Machine Learning: ECML-98, Tenth European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,292.82,448.06,7.88;10,81.06,303.08,210.22,7.88" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,232.26,292.82,94.93,7.88">Adaptive filtering of SPAM</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Almhana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Choulakian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,339.63,292.82,185.76,7.88;10,81.06,303.08,186.19,7.88">Proc. of the Second Annual Conference on Communication Networks and Services Research (CNSR&apos;04)</title>
		<meeting>of the Second Annual Conference on Communication Networks and Services Research (CNSR&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,313.34,447.91,7.88;10,81.06,323.66,354.93,7.88" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,199.96,313.34,325.27,7.88;10,81.06,323.66,26.35,7.88">An empirical performance comparison of machine learning methods for spam e-mail categorization</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Chi</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,116.22,323.66,296.23,7.88">Proc. of the Fourth International Conference on Hybrid Intelligent Systems (HIS&apos;04)</title>
		<meeting>of the Fourth International Conference on Hybrid Intelligent Systems (HIS&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,333.92,448.01,7.88;10,81.06,344.18,133.95,7.88" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,198.57,333.92,144.36,7.88">Tree-augmented Naive Bayes ensembles</title>
		<author>
			<persName coords=""><forename type="first">Shang-Cai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hong-Bo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,353.93,333.92,171.40,7.88;10,81.06,344.18,110.49,7.88">Proc. of 2004 International Conference on Machine Learning and Cybernetics</title>
		<meeting>of 2004 International Conference on Machine Learning and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,354.44,447.98,7.88;10,81.05,364.70,382.12,7.88" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,416.67,354.44,108.62,7.88;10,81.05,364.70,79.27,7.88">An evaluation of Naive Bayesian anti-Spam filtering</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koutsias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">V</forename><surname>Chandrinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,168.79,364.70,243.86,7.88">Proc. of Workshop on Machine Learning in the New Information Age</title>
		<meeting>of Workshop on Machine Learning in the New Information Age<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,374.96,447.96,7.88;10,81.05,385.28,281.56,7.88" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,218.70,374.96,216.20,7.88">Classification of web documents using a Naive Bayes method</title>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,445.28,374.96,80.00,7.88;10,81.05,385.28,257.93,7.88">Proc. of the 15th IEEE International Conference on Tools with Artificial Intelligence (ICTAI&apos;03)</title>
		<meeting>of the 15th IEEE International Conference on Tools with Artificial Intelligence (ICTAI&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,395.53,447.94,7.88;10,81.05,405.79,444.19,7.88;10,81.05,416.05,19.73,7.88" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,177.77,395.53,289.03,7.88">Techniques for improving the performance of Naive Bayes for text classification</title>
		<author>
			<persName coords=""><forename type="first">Karl-Michael</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,476.19,395.53,49.07,7.88;10,81.05,405.79,344.38,7.88">Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3406</biblScope>
			<biblScope unit="page" from="682" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.31,426.31,448.01,7.88;10,81.05,436.57,84.96,7.88;10,70.01,447.55,455.26,7.88;10,81.06,458.29,50.40,7.88" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Lynam</surname></persName>
		</author>
		<ptr target="http://plg.uwaterloo.ca/~gvcormac/trecspamtrack05/trecspam05appendix.pdf" />
		<title level="m" coord="10,227.12,426.31,115.07,7.88;10,92.64,436.57,73.38,7.88;10,70.01,447.55,225.15,7.88">05paper.pdf 25．TREC 2005 Conference Notebook Appendix -Spam Track</title>
		<imprint/>
	</monogr>
	<note>TREC 2005 spam track overview</note>
</biblStruct>

<biblStruct coords="10,86.51,468.55,438.76,7.88;10,81.05,478.81,66.81,7.88" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,239.19,468.55,220.25,7.88">Decision Tree Induction Based on Efficient Tree Restructuring</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">U</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Jeffery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,468.91,468.55,56.36,7.88;10,81.05,478.81,10.06,7.88">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,77.32,489.13,327.24,7.88" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="10,137.79,489.13,129.45,7.88">C4.5: Programs for machine learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
