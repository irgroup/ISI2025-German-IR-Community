<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.94,74.53,410.12,12.90;1,124.95,90.47,362.09,12.90">Synonym-based Query Expansion and Boosting-based Re-ranking: A Two-phase Approach for Genomic Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.20,114.90,70.84,10.75"><forename type="first">Zhongmin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University Burnaby</orgName>
								<address>
									<postCode>V5A1S6</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.93,114.90,54.81,10.75"><forename type="first">Baohua</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University Burnaby</orgName>
								<address>
									<postCode>V5A1S6</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,288.05,114.90,76.80,10.75"><forename type="first">Fred</forename><surname>Popowich</surname></persName>
							<email>popowich@cs.sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University Burnaby</orgName>
								<address>
									<postCode>V5A1S6</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.07,114.90,72.73,10.75"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University Burnaby</orgName>
								<address>
									<postCode>V5A1S6</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.94,74.53,410.12,12.90;1,124.95,90.47,362.09,12.90">Synonym-based Query Expansion and Boosting-based Re-ranking: A Two-phase Approach for Genomic Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F40C5400D265A947900BE2199ED52E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe in this paper the design and evaluation of the system built at Simon Fraser University for the TREC 2005 adhoc retrieval task in the Genomics track. The main approach taken in our system was to expand synonyms by exploiting a fusion of a set of biomedical and general ontology sources, and apply machine learning and natural language processing techniques to re-rank retrieved documents. In our system, we integrated En-trezGene, HUGO, Eugenes, ARGH, GO, MeSH, UMLSKS and WordNet into a large reference database and then used a conventional Information Retrieval (IR) toolkit, the Lemur toolkit <ref type="bibr" coords="1,210.11,435.48,62.18,9.46" target="#b6">(Lemur, 2005)</ref>, to build an IR system. In the postprocessing phase, we applied a boosting algorithm <ref type="bibr" coords="1,165.64,476.12,111.34,9.46;1,93.82,489.67,20.36,9.46" target="#b5">(Kudo and Matsumoto, 2004</ref>) that captured natural language substructures embedded in texts to re-rank the retrieved documents. Experimental results show that the boosting algorithm worked well in cases where a conventional IR system performs poorly, but this re-ranking approach was not robust enough when applied to broad coverage task typically associated with IR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TREC 2005 Genomics track consists of the adhoc retrieval task and the categorization task. We were participating in the ad-hoc retrieval task only, due to the considerable effort we spent on building the framework of the biomedical IR system. This is the first time that our team is competing in any TREC task.</p><p>The ad-hoc retrieval task aims at the retrieval of MEDLINE records relevant to the official topics. In constrast with the free-form topics of the 2004 task, the 2005 topics are more structured and better defined. A set of 5 generic topic templates (GTTs) was developed following the analysis of the the 2004 topics and the information needs from 25 biologists<ref type="foot" coords="1,532.79,310.77,3.99,6.91" target="#foot_0">1</ref> . Ten topic instances was then derived from each of GTTs. As with last year's ad-hoc retrieval task, the document collection of the 2005 task is a 10-year MEDLINE subset <ref type="bibr" coords="1,394.95,367.01,23.86,9.46">(1994)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(1995)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(1996)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(1997)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(1998)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(1999)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(2000)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(2001)</ref><ref type="bibr" coords="1,418.81,367.01,4.77,9.46">(2002)</ref><ref type="bibr" coords="1,423.58,367.01,23.86,9.46">(2003)</ref>, about 4.6M records and 9.6G bytes in total. The relevance judgement was made by the same pooling method used in the 2004 task, where top ranking documents of every topic from all submitted runs are given to human experts, who then determined each document as definitely relevance (DR), possible relevance (PR) or not relevance (NR) to the topic.</p><p>Three run types were accepted this year: automatic, manual and interactive, which differed depending on how the queries were constructed. Each participant was allowed to submit up to two runs. Our submission was in the manual category, since our queries were manually constructed. One of our goals was to determine how natural language processing (NLP) techniques could be used for reranking in a post-retrieval step. In our current system, we only apply such techniques for re-ranking.</p><p>In the future we plan to apply similar techniques towards query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>In general, the performance of an IR system largely depends on the quality of the query expansion. Most participants of the ad-hoc retrieval task in previous years applied reference database relevance feedback, a technique that finds synonyms and relevant terms from the outside term databases and adds them in the query. Over the past decade, the biomedical databases have evolved dramatically in terms of both the number and the volume, but from the reviews of previous work in this task, most of participants only employed a couple of them to build the reference database. In our system, we collected terms from EntrezGene <ref type="bibr" coords="2,176.27,211.35,83.83,9.46" target="#b2">(EntrezGene, 2005)</ref>, HUGO <ref type="bibr" coords="2,72.00,224.90,65.22,9.46" target="#b4">(HUGO, 2005)</ref>, euGenes <ref type="bibr" coords="2,187.49,224.90,71.55,9.46">(euGenes, 2005)</ref>, ARGH <ref type="bibr" coords="2,72.00,238.45,65.87,9.46" target="#b0">(ARGH, 2005)</ref>, MeSH <ref type="bibr" coords="2,181.92,238.45,63.45,9.46">(MeSH, 2005)</ref>, GO <ref type="bibr" coords="2,276.69,238.45,22.11,9.46;2,72.00,252.00,23.48,9.46" target="#b3">(GO, 2005)</ref>, UMLS <ref type="bibr" coords="2,137.06,252.00,65.34,9.46" target="#b8">(UMLS, 2005)</ref> and WordNet <ref type="bibr" coords="2,267.57,252.00,31.23,9.46;2,72.00,265.55,46.14,9.46">(Word-Net, 2005)</ref>, and integrated them into a large reference database which we then use in our system.</p><p>Traditional NLP techniques have been generally not successful in improving retrieval performance <ref type="bibr" coords="2,72.00,319.82,73.43,9.46" target="#b9">(Voorhees, 1999)</ref>, but there is still interest in examining how the linguistic and domain specific knowledge contained in NLP models and algorithms might be applied to specific IR subtasks to improve performance. In this work, we applied a classification technique: a boosting algorithm to capture substructures embedded in texts <ref type="bibr" coords="2,198.00,401.11,100.80,9.46;2,72.00,414.66,25.45,9.46" target="#b5">(Kudo and Matsumoto, 2004)</ref> in the second phase of our IR system. Different from the typical bag-of-words approach, the algorithm takes each sentence as a labeled ordered tree and classifies it by assigning a relevance score as either relevant (positive) or not (negative). The relevance of each document is then calculated from relevance scores of the sentences in the document.</p><p>Our system consists of two major phases, shown in Figure <ref type="figure" coords="2,117.62,523.13,4.09,9.46" target="#fig_0">1</ref>. In the first phase (left to the dashed line in Fig 1 ), we applied extensive synonym expansion with a conventional IR system, the Lemur toolkit 4.1 <ref type="bibr" coords="2,121.76,563.78,62.58,9.46" target="#b6">(Lemur, 2005)</ref>. The details of our synonym expansion phase and reference database construction are introduced in §3. The second phase is a post-processing step, in which the boosting classification algorithm <ref type="bibr" coords="2,147.56,617.98,126.07,9.46" target="#b5">(Kudo and Matsumoto, 2004</ref>) was used to re-rank the list of retrieved documents from the first phase. §4 describes its implementation details, experiments and evaluations of the boostingbased classification.</p><p>The experiments and evaluations in the second phase was not accomplished when we submitted the runs, but we include them in this report and explic-itly distinguish them from the submitted results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conventional IR Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extensive Synonym Expansion</head><p>Our system involves the manual selection of key words from the official topics (for most topics the key words were already given in the tabular version of topics) according to the given GTTs. The names and symbols related to each key word, for instance, synonyms, acronyms, hyponyms and similar names, were then matched with the public biomedical and generic databases that include synonyms and relevant terms. Specifically, for gene/protein names, we automatically integrated EntrezGene, HUGO, Eugenes and ARGH into a large gene/protein database with 1,620,947 entries, each of which consists of names and symbols that represent the same biomedical substance, and then matched them with each key word in the topics. Similarly, for diseases, organisms and drugs, related names and symbols were automatically matched with entries in MeSH; molecular functions, biological processes and cellular components made use of GO, and general words/phrases were matched (manually so far) in WordNet. In addition, all sets of related names and symbols were further expanded by searching via the UMLS Knowledge Source (UMLSKS) Socket Server. Figure <ref type="figure" coords="2,331.66,449.19,5.45,9.46">2</ref> illustrates the procedure of constructing the reference databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Retrieval</head><p>In this project, we use the Lemur Language Modeling Toolkit 4.1. The Lemur system was designed to facilitate research in language modeling and information retrieval (IR), such as the ad-hoc and distributed retrieval, structured queries, cross-language document retrieval, summarization, filtering, and categorization.</p><p>We use the following three modules provided in Lemur 4.1:</p><p>1. Parsing Query module 2. Building index module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structured Query Retrieval Module</head><p>In the following subsections, we will briefly describe how each module was used in our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Parsing Query</head><p>The Parsing Query module contains two utilities to handle different types of queries, ParseQuery and pareInQueryOp. ParseQuery handles queries written in NIST's Web or TREC formats, while ParseIn-QueryOp is used to parse structured queries written in a structure query language. Both types of queries are then converted into the BasicDocStream format, an document format used inside Lemur. In our experiments, we tried both types of queries and found that the structured queries generally provided better results. Therefore, we used the structured queries in our submitted run.</p><p>The structure query language used in Lemur can be found on its web site 2 . Briefly, it allows a user to define various AND/OR/NOT relations, and it provides for weights of sums (WSUM) among the terms. It even allows a user to consider a sequence of single terms by defining them as a phrase. Hence, the structured query enables more precise query definition.</p><p>A sample structured query looks like: ) );</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Building the Index</head><p>Lemur's BuildIndex module supports constructiion of four types of indices, specifically: InvIndex, InvFPIndex, KeyfileIncIndex, and IndriIndex 3 . We used the KeyfileIncIndex, which includes the position information of a term and can be loaded faster 2 http://www.lemurproject.org/lemur/StructuredQuery.html 3 http://www.lemurproject.org/lemur/indexingfaq.html than InvIndex and InvFPIndex while using less disk space than IndriIndex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Retrieving Structured Query</head><p>The structured queries were passed to the Struct-QueryEval module, which ran retrieval experiments to evaluate the performance of the structured query model using the inquery retrieval method. Note that for structured queries, relevance feedback was implemented as a WSUM of the original query combined with terms selected using the Rocchio implementation of the TFIDF retrieval method <ref type="bibr" coords="4,490.63,224.01,49.36,9.46;4,313.20,237.56,66.51,9.46" target="#b7">(Salton and Buckley, 1988)</ref>. In our official runs, the parameters (feedbackDocCount,feedbackTermCount, feed-backPosCoeff) for relevance feedback are: 100, 100, and 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Among all the official runs submitted to the ad-hoc task of the TREC-2005 Genome Track, 48 are using automatic retrieval methods and 12 including ours are manual ones. Figure <ref type="figure" coords="4,441.72,361.56,5.45,9.46">3</ref> shows MAP (upper), P10 (middle) and P100 (lower) scores of the manual runs. Three runs are shown in the figure: the best, the worst and ours on each topic. To better illustrate the performance of our system among others, we plot each value in the figure as the differenece between the actual score and the median score.</p><p>Although we do not know the evaluation results of every other system, Figure <ref type="figure" coords="4,451.94,470.47,5.45,9.46">3</ref> seems to indicate that our system is above the average. For instance, for the P10 scores of our system on all 49 topics, 36 are above the median and 10 of them are the best; for the MAP scores, 32 are above the median and 2 are the best. The automatic runs perform better than the manual runs on the whole and our system is around the average of the automatic runs. Our future research will involve the invetigation of how our system performs on each topic and each template, looking for insights to further tune our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Post-processing Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Boosting-based Classification</head><p>Traditional NLP techniques, such as word sense disambiguation resolution, chunking and parsing, were examined in the IR community at TREC-5 NLP track, but few of them were shown successful for Figure <ref type="figure" coords="5,102.95,748.04,4.24,9.46">3</ref>: The MAP, P10 and P100 scores of the best, worst manual runs and our system on each topic. Each value is the actual score minus the median. good retrieval performance. The reasons may lie in the broad coverage of the typical retrieval task, the lack of good weighting schemes for compound index terms and the statistical nature of the NLP techniques <ref type="bibr" coords="6,103.21,130.05,72.96,9.46" target="#b9">(Voorhees, 1999)</ref>.</p><p>However, the attempts of applying NLP and machine learning techniques to the IR tasks are still attractive, since a good understanding of the documents could be a breakthrough to the IR tasks. In this project, we adopted Taku Kudo's Boosting Algorithm for Classification of Trees (BACT), a classification method that captures the sub-structures embedded in texts. We use the method and implementation described in <ref type="bibr" coords="6,160.71,252.45,133.39,9.46" target="#b5">(Kudo and Matsumoto, 2004)</ref>. BACT takes a set of all subtrees as the feature set, from which it iteratively calls a weak learner to produce weak hypotheses. The strong hypothesis is finally generated by a linear combination of weak hypotheses.</p><p>We incorporated BACT into the post-processing step, where the list of retrieved documents from Lemur was re-ranked by taking the classification of the documents into account, as shown in the Figure <ref type="figure" coords="6,72.00,388.39,4.09,9.46" target="#fig_2">4</ref>. The documents in the training data were parsed using Charniak's parser <ref type="bibr" coords="6,180.88,401.94,75.83,9.46" target="#b1">(Charniak, 2000)</ref> and then classified by BACT in terms of relevant (positive) or irrelevant (negative). A re-ranking mechanism made the final relevance decision by combining the relevance scores from both Lemur and BACT.</p><p>The major difficulty of applying BACT in this task is that it assigns a classification score (positive or negative) to each sentence rather than assigning a score to a document. This resulted in two issues: 1) the lack of the training data with the label for each sentence; 2) the lack of a mechanism for combining sentence scores into a document score.</p><p>Since we lacked training data of sufficient quality and quantity for the classification task, we were not able to submit the post-processing results to the TREC in time for the initial deadline. After the results of the ad-hoc retrieval task were announced <ref type="bibr" coords="6,72.00,633.17,88.50,9.46">(on Sept. 30, 2005)</ref>, we were able to test the performance of the post-processing, by taking the following steps to prepare the training and test data for BACT:</p><p>1. The retrieved documents in the first two topics in each TTL were taken as the test data and those in the remaining topics as the training data.</p><p>2. The irrelevant documents in the training data were removed due to the unbalance of the training data (irrelevant documents are much more common than relevant ones).</p><p>3. In the training data, sentences were given "approximate" labels by matching them against a disjunction of all terms in the corresponding query as either matched (+1) or unmatched (-1).</p><p>BACT assigned a real number as the classification score to each sentence, with a larger score corresponding to a more relevant the sentence. We took the mean of all sentence scores in each document as the document score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Re-ranking</head><p>The goal of re-ranking is to combine R L and R B , the ranks from Lemur and BACT respectively, such that the rank R maximizes the evaluation scores, for example, MAP, P10 and P100. R L , R B and R are score vectors of retrieved documents. We assumed that such a combination was linear, i.e.:</p><formula xml:id="formula_0" coords="6,385.00,446.15,155.00,10.68">R = R L + i * R B<label>(1)</label></formula><p>We thus looked for i that maximizes the evaluation function E(R ):</p><formula xml:id="formula_1" coords="6,359.77,508.69,133.66,11.86">i = argmax i E(R L + i * R B )</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>As described in §4.1, we extract relevance scores of our retrieved documents (by Lemur) from the evaluation results of 2005 ad-hoc retrieval task. For each TTL, the retrieved documents of the first two topics were taken as the test data, and those of the remaining topics as the training data. Table <ref type="table" coords="6,351.35,631.60,4.09,9.46">1</ref>, 2 and 3 list the MAP, P10 and P100 before (i = 0) and after the re-ranking for the TTL #1, #2 and #3. A linear combination coefficient i was predicted for each TTL following Equation 2. For the TTL #2, i converges at 15 and the linear combination model significantly improves the IR performance: MAP increases from 0.0012 to 0.0024 for Table <ref type="table" coords="7,99.81,418.92,4.24,9.46">1</ref>: Performances of re-ranking on the TTL #1 the topic #110 and from 0.0492 to 0.1602 for the topic #111; Same situations for P10 and P100. However, for the TTL #1 and #3, no linear combination model can improve the IR performance, i.e., i = 0. The scores at i = 10 are also listed in Table <ref type="table" coords="7,274.13,507.33,5.45,9.46">1</ref> and 3 to show that the performance dropped when the linear combination models were applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Our experiments show that BACT as the postprocessing does help when bpref (proportion of judged relevant documents that are retrieved) of the conventional IR system is low, for instance, 0.25 and 0.4356 in the TTL #2. For the TTL #1 and #3 where BACT failed, the average bpref is very high, above 0.8. It seems as if our current use of BACT for reranking cannot scale to the broad coverage of relevant documents in the retrieved document set, especially in the case where bpref is high. This is a Topic # Metrics i = 0 i = 15(i ) common problem of NLP techniques when applied to the IR task. However, employing machine learning and NLP techniques such as BACT as the postprocessing step may help the conventional IR system when the recall is low, by re-ranking the retrieved documents towards a better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,232.35,250.29,147.29,9.46;3,79.20,109.40,453.60,124.48"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The system architecture</figDesc><graphic coords="3,79.20,109.40,453.60,124.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,226.59,254.70,158.81,9.46;7,79.20,72.00,453.61,166.29"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The post-processing phase</figDesc><graphic coords="7,79.20,72.00,453.61,166.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,314.31,335.66,224.57,326.64"><head>Table 2 :</head><label>2</label><figDesc>Performances of re-ranking on the TTL #2</figDesc><table coords="7,341.23,335.66,170.75,326.64"><row><cell></cell><cell>MAP</cell><cell>0.0012</cell><cell>0.0024</cell></row><row><cell>110</cell><cell>bpref P10</cell><cell>0.25 0</cell><cell>0.25 0</cell></row><row><cell></cell><cell>P100</cell><cell>0</cell><cell>0.01</cell></row><row><cell></cell><cell>MAP</cell><cell>0.0492</cell><cell>0.1602</cell></row><row><cell>111</cell><cell>bpref P10</cell><cell>0.4356 0.1</cell><cell>0.4356 0.7</cell></row><row><cell></cell><cell>P100</cell><cell>0.1</cell><cell>0.4</cell></row><row><cell cols="4">Topic # Metrics i = 0(i ) i = 10</cell></row><row><cell></cell><cell>MAP</cell><cell cols="2">0.6113 0.2410</cell></row><row><cell>120</cell><cell>bpref P10</cell><cell cols="2">0.8145 0.8145 1 0.3</cell></row><row><cell></cell><cell>P100</cell><cell>0.88</cell><cell>0.29</cell></row><row><cell></cell><cell>MAP</cell><cell cols="2">0.6697 0.0328</cell></row><row><cell>121</cell><cell>bpref P10</cell><cell cols="2">0.8810 0.8810 0.8 0</cell></row><row><cell></cell><cell>P100</cell><cell>0.34</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,314.31,675.70,224.57,9.46"><head>Table 3 :</head><label>3</label><figDesc>Performances of re-ranking on the TTL #3</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,329.34,714.16,166.64,7.77"><p>http://ir.ohsu.edu/genomics/2005protocol.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Yang Wendy Wang</rs> for the helpful discussions and the constructive comments during the system design and experiments. We greatly acknowledge the organization committee of the TREC 2005 genomics track and those who participated in the topic generation and relevance judgment for their valuable work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,325.15,30.72,8.64;8,122.78,325.15,22.42,8.64;8,165.27,324.97,133.53,8.59;8,82.91,336.11,134.04,8.64" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Argh</surname></persName>
		</author>
		<ptr target="http://invention.swmed.edu/argh/" />
		<title level="m" coord="8,165.27,324.97,129.38,8.59">Biomedical Acronym Resolver</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,356.32,226.80,8.64;8,82.91,367.09,215.90,8.82;8,82.91,378.05,100.16,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,178.84,356.32,119.95,8.64;8,82.91,367.27,22.53,8.64">A maximum-entropy-inspired parser</title>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,125.76,367.09,173.05,8.59;8,82.91,378.05,30.01,8.59">Meeting of the North American Chapter of the ACL</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,398.44,49.52,8.64;8,164.44,398.44,22.42,8.64;8,229.77,398.26,69.02,8.59;8,82.91,409.22,11.07,8.59;8,123.13,409.22,7.75,8.59;8,160.03,409.22,57.31,8.59;8,246.50,409.22,52.30,8.59;8,82.91,420.36,225.99,8.64;8,72.00,440.38,226.80,8.82;8,82.91,451.34,126.19,8.82" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Entrezgene</surname></persName>
		</author>
		<ptr target="http://eugenes.org/" />
		<title level="m" coord="8,229.77,398.26,69.02,8.59;8,82.91,409.22,11.07,8.59;8,123.13,409.22,7.75,8.59;8,160.03,409.22,57.31,8.59;8,246.50,409.22,47.94,8.59;8,147.94,440.38,150.86,8.59;8,82.91,451.34,41.26,8.59">National Center of Biotechnology Ingormation</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Genomic Information for Eukaryotic Organisms</note>
</biblStruct>

<biblStruct coords="8,72.00,471.73,16.88,8.64;8,130.52,471.73,22.42,8.64;8,194.59,471.55,104.21,8.59;8,82.91,482.69,121.68,8.64" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><surname>Go</surname></persName>
		</author>
		<ptr target="http://www.geneontology.org/" />
		<title level="m" coord="8,194.59,471.55,99.82,8.59">The Gene Ontology</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,502.89,31.26,8.64;8,121.41,502.89,22.42,8.64;8,161.96,502.71,136.84,8.59;8,82.91,513.85,166.75,8.64" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><surname>Hugo</surname></persName>
		</author>
		<ptr target="http://www.gene.ucl.ac.uk/nomenclature/" />
		<title level="m" coord="8,161.96,502.71,132.27,8.59">Gene Nomenclature Committee</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,534.06,226.80,8.64;8,82.91,544.84,215.89,8.82;8,82.91,555.80,215.89,8.59;8,82.91,566.76,110.51,8.59" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,231.93,534.06,66.87,8.64;8,82.91,545.02,180.47,8.64">A boosting algorithm for classification of semi-structured text</title>
		<author>
			<persName coords=""><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,280.99,544.84,17.81,8.59;8,82.91,555.80,215.89,8.59;8,82.91,566.76,43.83,8.59">Proceedings of Empirical Methods of Natural Language Processing</title>
		<meeting>Empirical Methods of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,587.14,28.50,8.64;8,118.09,587.14,22.42,8.64;8,158.10,586.96,140.70,8.59;8,82.91,598.10,180.15,8.64;8,72.00,618.31,28.50,8.64;8,127.35,618.31,22.42,8.64;8,176.62,618.13,122.18,8.59;8,82.91,629.27,188.23,8.64" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><surname>Lemur</surname></persName>
		</author>
		<ptr target="http://www.nlm.nih.gov/mesh/meshhome.html" />
		<title level="m" coord="8,158.10,586.96,136.96,8.59;8,176.62,618.13,117.66,8.59">Language Modeling Toolkit 4.1</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note>Medical Subject Headings</note>
</biblStruct>

<biblStruct coords="8,72.00,649.47,226.80,8.64;8,82.91,660.25,215.89,8.82;8,82.91,671.21,103.56,8.59" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,223.94,649.47,74.86,8.64;8,82.91,660.43,139.86,8.64">Term eighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Information Processing and Management</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,691.60,30.17,8.64;8,119.83,691.60,22.42,8.64;8,159.91,691.42,138.89,8.59;8,82.91,702.38,215.89,8.82;8,82.91,713.51,158.60,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,159.91,691.42,138.89,8.59;8,82.91,702.38,12.66,8.59">Unified Medical Language System</title>
		<author>
			<persName coords=""><surname>Umls</surname></persName>
		</author>
		<ptr target="http://www.nlm.nih.gov/research/umls/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Ntional Institute of Health, United States</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,76.47,226.80,8.64;8,324.11,87.25,194.56,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,425.21,76.47,114.78,8.64;8,324.11,87.43,98.08,8.64">Natural language processing and information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,440.08,87.25,18.82,8.59">SCIE</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="32" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,107.18,226.80,8.82;8,324.11,118.00,180.11,8.96" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><surname>Wordnet</surname></persName>
		</author>
		<ptr target="http://www.cogsci.princeton.edu/˜wn/" />
		<title level="m" coord="8,389.68,107.18,150.32,8.59;8,324.11,118.14,22.20,8.59">A lexicon database for English Language</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
