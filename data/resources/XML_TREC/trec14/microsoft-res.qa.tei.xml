<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.48,75.37,379.69,12.64;1,277.36,91.45,79.80,12.64">Factoid Question Answering over Unstructured and Structured Web Content</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.68,116.24,81.55,10.80"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
							<email>silviu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.72,116.24,90.19,10.80"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
							<email>eugeneag@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.48,75.37,379.69,12.64;1,277.36,91.45,79.80,12.64">Factoid Question Answering over Unstructured and Structured Web Content</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A340326F0BA016EAAA97EFD36E4483E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our experience with two new, builtfrom-scratch, web-based question answering systems applied to the TREC 2005 Main Question Answering task, which use complementary models of answering questions over both structured and unstructured content on the Web. Our approaches depart from previous question answering (QA) work in several ways. For unstructured content, we used a web-based system with novel features such as web snippet pattern matching and generic answer type matching using web counts. We also experimented with a new, complementary question answering approach that uses information from the millions of tables and lists that abound on the web. This system attempts to answer factoid questions by guessing relevant rows and fields in matching web tables and integrating the results. We believe a combination of the two approaches holds promise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Previous Work</head><p>The systems described in this paper are entirely webbased and explore two different research directions: one is to employ a web search engine to mine text web pages (unstructured web information henceforth), the other to employ the already structured web information in the form of html tables, which typically summarize various relations of interest to web users.</p><p>There has been a substantial amount of work on using web information and search engines for TREC QA, starting from the premise that a data collection such as the TREC corpus has considerably less answer redundancy than the web and thus, it is easier to match a question to the web data, to extract answers from the matching text, and then project these answers on the restricted data collection (e.g. Brill et al. <ref type="bibr" coords="1,72.04,669.40,10.69,8.96" target="#b1">[2]</ref>, Radev et al. <ref type="bibr" coords="1,138.76,669.40,15.34,8.96" target="#b11">[12]</ref>, Ramakrishnan et al. <ref type="bibr" coords="1,242.44,669.40,15.10,8.96" target="#b12">[13]</ref>).</p><p>Most of the published web-based QA systems focus on one language (English) and employ advanced natural language processing tools and/or extensive hierarchies of answer matching rules and answer types. From a practical perspective (i.e., having a search engine handle natural language questions in all markets in which it is deployed), such assumptions cannot be made. While previous approaches investigated how to scale current paradigms to general QA on the web (e.g. Kwok <ref type="bibr" coords="1,422.68,282.28,15.10,8.96" target="#b10">[11]</ref>), one of our main goals was to determine what performance can be achieved with a moderate annotation effort (in our case, one person-day) by a web-based QA system.</p><p>Because previous research on question answering largely ignored existing html tables and focused either on natural language text from web pages or online databases, another important goal of this work is to investigate a new way of using the existent structured information on the web to retrieve answers to factoid questions. By exploiting the explicit tabular structures created by the web document authors, we can, in principle, get natural language understanding "for free" and hence, advance the applicability and scalability of question answering.</p><p>There have been many efforts to extract structured information from the web. Previous approaches (e.g., Agichtein and Gravano <ref type="bibr" coords="1,420.40,485.80,10.69,8.96" target="#b0">[1]</ref>, Etzioni et al. <ref type="bibr" coords="1,491.32,485.80,11.22,8.96" target="#b5">[6]</ref>) focused on extracting specific relationships (e.g., "is a"), which can then be used to answer the specific questions that these relationships support (e.g., "who is X"). In this work, we attempt to support any question by finding the structured table(s) on the web where this question was already answered. Unfortunately, many of the most useful tables do not contain the text patterns these systems look for. By indexing "all" potentially useful tables we are more likely to achieve high coverage of user's questions.</p><p>In a closely related study, Hildebrandt et al. <ref type="bibr" coords="1,507.04,616.24,11.60,8.96" target="#b8">[9]</ref> used a large number of dictionaries and lists, some of which were constructed dynamically by querying sites such as Amazon. Unlike in our approach, the lists were specific and were constructed in advance for each question type. To the best of our knowledge, our study is the first attempt to integrate, index, and exploit millions of tables for question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Systems Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Common System Architecture</head><p>Figure <ref type="figure" coords="2,100.84,118.60,4.98,8.96">1</ref> outlines the architecture of the two proposed systems. In this section, we describe the preprocessing and post-processing blocks common to both systems. Sections 2.2 and 2.3 present in detail the novel features of these systems.</p><p>In a first step, a question to be answered is passed through a phrase chunker (derived from the parsing system described in <ref type="bibr" coords="2,156.88,203.08,62.48,8.96" target="#b7">Heidorn, 2000)</ref> to extract information about the verbs, pronouns, and noun phrases in the question. The pronoun and noun phrase information is used to resolve the references to the question target, based on a small set of resolution heuristics. The verb information is used further by the WSQA system, as it will be described in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. Overall System Architecture</head><p>Then, the WSQA system attempts to guess the answer type by using rewrite rules, as described in Section 2.2. This information is provided to both WSQA and TQA systems, which attempt to answer the question using unstructured and respectively, structured web content. Optionally, the lists of candidate answers returned by the WSQA and the TQA systems can be combined. We employed a linear mixture strategy that combines the lists using the normalized score associated with the answers by the two systems, the overlap between proposed answers, and the expected accuracy of each of the systems.</p><p>The top candidate answer is then projected onto the AQUAINT corpus to find document support.</p><p>The projection was done by simply retrieving the document that best matched both the query and the candidate answer. Finally, the highest scoring answer and its support document are returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mining Unstructured Web Content (WSQA)</head><p>For unstructured content, we used a web-based QA system inspired from the AskMSR structural design (Brill et al. <ref type="bibr" coords="2,377.08,189.52,10.57,8.96" target="#b1">[2]</ref>), which does not make use of advanced NLP tools. This system employs two novel ideas related to generic answer type matching using web counts and web snippet pattern matching. The former is proposed as an alternative to employing a predefined ontology of answer types, while the latter reduces the number of candidate answers that cooccur frequently with the question words in web search snippets but are not related to the question intent and also eliminates the need of an n-gram assembly stage, as employed in <ref type="bibr" coords="2,449.92,304.48,11.72,8.96" target="#b1">[2]</ref> and <ref type="bibr" coords="2,483.40,304.48,10.60,8.96" target="#b4">[5]</ref>. In the remainder of this section, we will focus mainly on the two novel features of the proposed system. The rules are sorted by question prefix (when, where, what, which, who, how many, how much, how, in/on/by what, and name). For each prefix, they are listed from the most particular to the most general.</p><p>When presented with a question, the system tries each rewriting rule in order until a match is found or all rules are consummated. In the latter situation, a back-off strategy is applied, as described further in this section.</p><p>Each rewriting rule is composed of one Perl-like question matching pattern and one or more rewriting patterns. Each of the rewriting patterns contains a * symbol, which encodes the required position of the answer in the text with respect to the pattern. For example, the rewriting rule </p><formula xml:id="formula_0" coords="2,324.04,665.74,21.60,7.07">When</formula><formula xml:id="formula_1" coords="3,72.04,190.54,216.00,31.43">PERCENTAGE: [0-9\.\,]+\s*(\%|percent)$ PERSON: ^(([A-Z](\.|[a-z]+)\s+)+([a-z\- ]+)?)?(\s*[A-Z][a-z]+)+$</formula><p>In some patterns, the answer type is represented by one of the match constituents in the regular expression instead of one of the standard types, e.g.:</p><formula xml:id="formula_2" coords="3,72.04,267.46,209.76,58.79">What ~V&lt;(is|are|was|were)&gt; the (\S+) (of|for) (.+) { Rewrite: $2 $3 $4 $1 * AnsType: $2 Rewrite: * $1 the $2 $3 $4 AnsType: $2 }</formula><p>According to the rewrite rule above, the question What is the color of the sky? is rewritten as color of the sky is * AnsType: color and * is the color of the sky AnsType: color. Here, color is a generic answer type, obtained automatically.</p><p>When the answer type is obtained in this way and is not mapped to one of the standard types, the system uses a web search engine to validate the matching of each answer candidate (e.g. blue, blue blue, result, grey, usually, being, often, lake, etc.) with the generic answer type (color). For each candidate X with an answer type Y, the following five quoted queries, are sent to the search engine:</p><formula xml:id="formula_3" coords="3,72.04,492.81,101.98,53.19">"Y such as the X" "Y such as X" "X is a|an Y" "X is the Y" "X Y"</formula><p>The score associated with each answer is boosted up based on the number of results returned by the search engine for each of these queries.</p><p>Additional hypernym patterns, for example, similar to those employed by Hearst <ref type="bibr" coords="3,192.76,603.40,10.69,8.96" target="#b6">[7]</ref>, could be used. Unfortunately, for each additional pattern, the system has to perform a number of searches on the order of the number of candidate answers, which means that every additional pattern adds a considerable computational effort. We then count the occurrences of the valid candidates 3 in all snippets retrieved by the rewrite. Note that the longer candidate Lee Harvey Oswald will have a smaller count than its prefix Lee Harvey, which, in turn, will have a smaller count than the prefix Lee. In general, for a rewrite with the * symbol on the rightmost position (henceforth, prefix pattern), multi-word candidates will have smaller counts then their valid candidate prefixes. Similarly, for a rewrite with * on the rightmost position (henceforth, suffix pattern), multi-word candidates will have smaller counts than their valid candidate suffixes. Thus, the raw counts always favor shorter answers. To compensate for this problem, we employ two strategies. The first one is to reduce the count of an affix when the longer candidate's count is greater than a threshold, proportionally to the longer candidate's count (we name these as adjusted counts). The second one is to ensure, as much as possible, that each rewrite rule contains rewrite patterns in which the position of the * symbol varies (prefix, suffix, and infix), and thus, the correct answer receives counts from all types of rewrite patterns, while its prefixes and suffixes receive counts from only one type of patterns. For example, the rewrite R 1 (Q) (i.e. * killed Kennedy) retrieves as candidate answers Oswald, Harvey Oswald, and Lee Harvey Oswald, tains as an affix another candidate for which no web results were found. 2 Note that some patterns may be ungrammatical because the system does not use agreement information. 3 In the provided example, the last three candidates are discarded immediately because they do not match the regular expression for the answer type PERSON.</p><p>but not Lee and Lee Harvey, which were retrieved by the suffix pattern discussed previously. Typically, when the adjusted counts for candidate answers are aggregated from all rewrites, the complete, correct answers are obtained.</p><p>When no rewrite rule matches a question or no web search results for the matching rewrite are found, the system backs off iteratively to search queries that use bags of phrases (if phrase information is available), bags of bigrams, and finally, bags of words (in this case, the search queries do not contain quoted terms). All non-singleton n-grams in the web search snippets returned by the search engine that match the answer pattern are selected as candidate answers. The counts for candidates that are affixes of other candidates are discounted in the same manner as for * matches of rewrite patterns. When the answer type is derived from the question based on a rewriting rule, these counts are further adjusted using the validating strategy described earlier.</p><p>Because each question Q in TREC 2005 has a target T, we run WSQA in parallel on the original question Q and the target-resolved question Q(T). When using the rewrites for the original question, the target T is added as a quoted string to the queries sent to the search engine. The candidate answer lists are combined and the adjusted counts are added up. In this setting, the back-off system described in the preceding paragraph is employed only when the rewrites for both runs fail at retrieving a list of candidate answers. Employing a second run on the rewrites for the original query has two advantages: it may retrieve answers in which the target was referred in the same way it is referred in the question (e.g. pronoun), and it decreases the system's dependence on accurate target resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mining Structured Content on the Web (TQA)</head><p>Traditional question answering systems typically perform complex parsing and entity extraction for both queries and best matching Web pages, and maintain local caches of pages or term weights. Our approach is distinguished from these in that we explicitly target the structured content which presents new challenges and opportunities.</p><p>Consider the example question "When was the telegraph invented?" We often find that someone (perhaps, a history buff) has created a list of all the major inventions and the corresponding years. In our approach, we take this idea to an extreme, and assume that every query or factoid question can be answered by some relationship expressed in a structured content (e.g., an HTML table) on the web.</p><p>Unfortunately, HTML tables on the web rarely have associated metadata or schema. Hence, we use whatever metadata is available (e.g., surrounding text on the page, page title, and the URL of the originating page). More specifically, each structure is associated with a set of keywords that appear in the header, footer, and title of the page, and the first row of the table as column headers.</p><p>Once the structures are indexed, they can be used for answering questions. Our framework for answering factoid questions over this implicitly structured content is outlined in Figure <ref type="figure" coords="4,424.84,680.92,3.77,8.96">2</ref>  </p><formula xml:id="formula_4" coords="4,447.28,68.62,55.59,29.01">------ ------ ------ Table0 ------- ------- ------- Table100 -------- -------- -------</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2.1: The Table Question Answering (TQA) framework</head><p>To answer a question in our framework, we proceed conceptually as outlined in Algorithm 2.1:</p><p>1. Retrieve all matching tables RT from the indexed tables TALL. 2. For each table t in RT select the rows ti in t that match the question target. 3. Extract answer candidates C from the union of all selected rows. 4. Assign a score and re-rank answer candidates Ci in C using the features associated with Ci. Return top K answer candidates.</p><p>Algorithm 2.1: Answering factoid questions over structured web content.</p><p>For our example question, the appropriate semantic relationship is InventionDate(Invention, Date), and a table snippet could be one of many instances of In-ventionDate on the web. The selected rows in R T would be those containing the question target, ``telegraph''. Initially, the question is chunked and automatically annotated with the question target and answer type as described above, and converted to a keyword query. The query is then submitted to the search engine over the metadata stored for our set of tables. The metadata information is indexed as regular text keywords, and the actual table content is stored as "nonindexed" text blob. In this years' TREC QA evaluation, the question target was manually specified which allowed us to filter the candidate rows to include only those that match the target. The candidate answer strings were then filtered to match the answer type. For each surviving candidate answer string we construct a feature vector describing the source quality, the closeness of match, frequency of the candidate and other characteristics of the answer and the source tables from which it was retrieved. The answer candidates are then re-ranked using linear combination of (heuristically assigned) feature weights, and the top-scoring answer is returned as the output of TQA. As a source of tables we used a random sample of 100M documents from the web, obtained from the msnbot (MSN Search) crawler, enhanced with the more focused crawl of likely fact-rich websites such as Wikipedia and FactMonster. Overall, more than 200 million tables were extracted and indexed for this evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Conclusions</head><p>The systems described in this paper were developed from scratch for this year's TREC evaluation and were initially tested on the TREC 2004 factoid QA evaluation. On the development set, WSQA obtained 32.7% U-accuracy, while TQA was reached 14.9%.</p><p>The official performance of these systems in the TREC 2005 evaluation is reported in Table <ref type="table" coords="5,509.92,85.72,3.77,8.96">3</ref>.1. As we can see, WSQA performs well on extracting answers from the web (U+R=21.8%) when the support for these answers in the TREC corpus is ignored. In contrast, TQA performs poorly, achieving only 5.5% exact answer accuracy even with no support required.</p><p>Unfortunately, the WSQA system timed out for almost 10% on the test questions in the official run, and a NULL answer was submitted for those questions, thereby decreasing the highest possible score achievable by our systems.</p><p>Because the proposed systems are web-based, we had to project the hypothesized web answers on the TREC data by retrieving the documents in the collection that " best support" the answers. We used a rather naïve projection strategy, and therefore, our official strict score suffered significantly due to inadequate support for otherwise correct answers. Specifically, the WSQA score dropped from 21.8% to 9.4% and the TQA score dropped from 5.8% to 1.7%. For the next TREC QA evaluation we plan to devote more effort into a more robust and effective answer projection and document-support finding component. Tables 3.2 and 3.3 report the official performance results of WSQA and TQA broken down by question type ("wh-" word) and target type ("event" or "entity"). The accuracy of WSQA for "when" and "where" questions is substantially better than the accuracy on other types of questions.  While the overall performance of TQA is low, we also analyze how this system performs for different question prefixes and target types (Table <ref type="table" coords="6,250.12,267.52,3.69,8.96">3</ref>.3). Not surprisingly TQA performs better on the "when" questions (0.13) than on all other question types. Finally, we analyze the accuracy of different systems for varying number of top K results examined. As shown in Figure <ref type="figure" coords="6,147.16,328.00,3.77,8.96">3</ref>.1, the accuracy numbers of all systems steadily improve. The Combined system (COMB) retrieved the correct answer 40% of the time in the top six and 50% of the time in the top 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Clearly, an important direction for future progress is the improvement of answer ranking for all systems. In summary, we presented two new web-based question answering systems that were evaluated on the TREC 2005 Main Question Answering task. The systems use complementary models of answering questions over unstructured and structured content on the Web, respectively. Despite a rather poor showing in our first TREC evaluation, we believe that a combination of the two approaches holds promise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,324.04,343.00,215.96,8.96;2,324.04,354.40,215.96,8.96;2,324.04,365.92,216.21,8.96;2,324.04,377.44,215.96,8.96;2,324.04,388.96,215.98,8.96;2,324.04,400.48,215.92,8.96;2,324.04,412.00,215.96,8.96;2,324.04,423.40,215.86,8.96;2,324.04,434.92,215.91,8.96;2,324.04,446.44,216.06,8.96;2,324.04,457.96,215.96,8.96;2,324.04,469.60,145.17,8.96"><head></head><label></label><figDesc>The WSQA system employs 198 question matching/rewriting rules, created by one human in one person day, based on the set of TREC-9 questions (<ref type="bibr" coords="2,517.96,365.92,14.86,8.96" target="#b14">[15]</ref>), a subset of the TREC 2004 questions (<ref type="bibr" coords="2,484.11,377.44,14.86,8.96" target="#b13">[14]</ref>), and another custom development set of 150 questions. The only syntactic information encoded in these rules concerns the verb identification. The system submitted to TREC 2005 uses a phrase chunker to derive the needed information, but additional experiments showed that very similar results can be obtained by employing just lexical and verb inflectional morphology information about the language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,72.04,542.52,219.81,8.53"><head>Figure 3 . 1 :</head><label>31</label><figDesc>Figure 3.1: Accuracy at K for TQA, WSQA, and COMB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.04,74.20,468.10,645.44"><head></head><label></label><figDesc>We now describe the usage of the rewrite patterns by using as example the question Q: Who killed Kennedy? According to the first matching rule employed by our system, this question is rewritten as R 1 (Q): * killed Kennedy, R 2 (Q): Kennedy was killed by *, R 3 (Q): Kennedy were killed by *, and R 4 (Q): Kennedy, killed by *, all four rewrites having the answer type PERSON.2   The rewrites are sent as quoted queries to a search engine and the top N search result snippets are extracted (in our experiments, N = 40). Next, the system finds the location of the patterns in the snippets and hypothesizes as possible answers all word n-grams (in our experiments, n = 6) that appear in the position of the * in the pattern. For example, the rewrite R 2 (Q) retrieves from the snippet […] Some people believe that Kennedy was killed by Lee Harvey Oswald , acting as a lone gunman […] candidates such as Lee, Lee Harvey, Lee Harvey Oswald, Lee Harvey Oswald acting, Lee Harvey Oswald acting as, and Lee Harvey Oswald acting as a.</figDesc><table /><note coords="3,129.28,658.81,3.24,5.83;3,72.04,696.97,3.24,5.83;3,79.60,699.16,208.42,8.96;3,72.04,710.68,215.96,8.96"><p><p>1   </p>1 A number of searches for a given pattern can be avoided; for example, when a candidate answer con-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,162.64,65.50,342.51,624.38"><head></head><label></label><figDesc>.1.</figDesc><table coords="4,162.64,65.50,342.51,205.82"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table1</cell></row><row><cell>Question Analysis and Parsing</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Query: "telegraph invented"</cell><cell>Table1</cell><cell></cell></row><row><cell></cell><cell>Born</cell><cell>1867</cell><cell></cell></row><row><cell></cell><cell cols="2">Invents lightbulb 1903</cell><cell></cell></row><row><cell></cell><cell>Died</cell><cell>1923</cell><cell>Table2</cell></row><row><cell></cell><cell></cell><cell cols="3">steam engine 1867</cell></row><row><cell>Candidate Row Selection</cell><cell>Question Focus: telegraph</cell><cell cols="2">Telegraph PC</cell><cell>1844 1981</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Edison</cell><cell>light bulb</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Graham</cell><cell>telephone</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Morse</cell><cell>telegraph</cell></row><row><cell>Candidate Answer Projection</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Candidate Answer Reranking</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answer: "1844"</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,156.76,74.78,344.42,197.38"><head></head><label></label><figDesc>-</figDesc><table coords="4,156.76,74.78,257.99,197.38"><row><cell></cell><cell></cell><cell></cell><cell>Keyword search on</cell></row><row><cell></cell><cell></cell><cell></cell><cell>table metadata</cell></row><row><cell>Candidate Table Retrieval</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Candidate Answers</cell></row><row><cell></cell><cell>1844</cell><cell>0.85 1</cell><cell>1 0.3 ...</cell></row><row><cell>Answer Retrieval</cell><cell cols="2">Morse 0.90 0</cell><cell>1 0.1 ...</cell></row><row><cell></cell><cell cols="2">Ranked Answers</cell></row><row><cell></cell><cell>1844</cell><cell>1</cell></row><row><cell></cell><cell>Morse</cell><cell>0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,322.24,528.52,220.14,162.84"><head>Table 3 . 1 :</head><label>31</label><figDesc>Official results for the three runs submitted to the TREC 2005 QA Task.</figDesc><table coords="5,322.24,558.88,220.14,132.48"><row><cell>(a)</cell><cell cols="6">Total Correct (RU) + Partial (X) Wrong</cell></row><row><cell>"When"</cell><cell cols="3">54 33 61.1%</cell><cell cols="2">2 64.8%</cell><cell>19</cell></row><row><cell>"Where"</cell><cell cols="3">40 12 30.0%</cell><cell cols="2">5 42.5%</cell><cell>23</cell></row><row><cell>"Who"</cell><cell>51</cell><cell cols="2">6 11.8%</cell><cell cols="2">1 13.7%</cell><cell>44</cell></row><row><cell>"What"</cell><cell cols="3">117 14 12.0%</cell><cell cols="2">1 12.8%</cell><cell>102</cell></row><row><cell>"Which"</cell><cell>4</cell><cell>0</cell><cell>0.0%</cell><cell>0</cell><cell>0.0%</cell><cell>4</cell></row><row><cell>"How"</cell><cell>60</cell><cell cols="2">7 11.7%</cell><cell cols="2">0 11.7%</cell><cell>53</cell></row><row><cell>Other</cell><cell>35</cell><cell cols="2">7 19.4%</cell><cell cols="2">1 22.2%</cell><cell>28</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Entity</cell><cell cols="3">266 62 23.3%</cell><cell cols="2">7 25.9%</cell><cell>197</cell></row><row><cell>Event</cell><cell cols="3">96 17 17.7%</cell><cell cols="2">3 20.8%</cell><cell>76</cell></row><row><cell>Overall</cell><cell cols="5">362 79 21.8% 10 24.6%</cell><cell>273</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,324.04,694.24,215.82,20.48"><head>Table 3 . 2 :</head><label>32</label><figDesc>Official results for WSQA broken down by (a) question prefix, and (b) target type.</figDesc><table coords="6,70.24,74.68,220.14,132.48"><row><cell>(a)</cell><cell cols="6">Total Correct (RU) + Partial (X) Wrong</cell></row><row><cell>"When"</cell><cell>54</cell><cell cols="2">7 13.0%</cell><cell cols="2">0 13.0%</cell><cell>47</cell></row><row><cell>"Where"</cell><cell>40</cell><cell>3</cell><cell>7.5%</cell><cell>0</cell><cell>7.5%</cell><cell>37</cell></row><row><cell>"Who"</cell><cell>51</cell><cell>2</cell><cell>3.9%</cell><cell>0</cell><cell>3.9%</cell><cell>49</cell></row><row><cell>"What"</cell><cell>117</cell><cell>3</cell><cell>2.6%</cell><cell>1</cell><cell>3.4%</cell><cell>113</cell></row><row><cell>"Which"</cell><cell>4</cell><cell>0</cell><cell>0.0%</cell><cell>0</cell><cell>0.0%</cell><cell>4</cell></row><row><cell>"How"</cell><cell>60</cell><cell>4</cell><cell>6.7%</cell><cell>0</cell><cell>6.7%</cell><cell>56</cell></row><row><cell>Other</cell><cell>36</cell><cell>1</cell><cell>2.8%</cell><cell>0</cell><cell>2.8%</cell><cell>35</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Entity</cell><cell cols="2">266 14</cell><cell>5.3%</cell><cell>1</cell><cell>5.6%</cell><cell>251</cell></row><row><cell>Event</cell><cell>96</cell><cell>6</cell><cell>6.3%</cell><cell>0</cell><cell>6.3%</cell><cell>90</cell></row><row><cell>Overall</cell><cell cols="2">362 20</cell><cell cols="2">5.5% 10</cell><cell>5.8%</cell><cell>273</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,72.04,210.04,208.14,20.48"><head>Table 3 . 3 :</head><label>33</label><figDesc>Official results for TQA broken down by (a) question prefix, and (b) target type.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="institution">MSN Search</rs> for providing us access to their search engine. We also thank <rs type="person">Eric Brill</rs> for extremely valuable discussions and feedback on this work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,342.04,88.77,198.00,8.10;6,342.04,99.09,192.57,8.10" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,486.52,88.77,53.52,8.10;6,342.04,99.09,188.95,8.10">Snowball: Extracting Relations from Large Plain-Text Collections</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,114.45,197.97,8.10;6,342.04,124.77,198.00,8.10;6,342.04,135.09,71.13,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,365.44,124.77,124.10,8.10">Data-intensive question answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,507.04,124.77,33.00,8.10;6,342.04,135.09,48.60,8.10">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,150.33,197.92,8.10;6,342.04,160.65,198.14,8.10;6,342.04,171.09,96.57,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,445.36,150.33,94.60,8.10;6,342.04,160.65,135.65,8.10">Man vs. Machine: A Case Study in Base Noun Phrase Learning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,495.16,160.65,45.02,8.10;6,342.04,171.09,44.28,8.10">Proceedings of ACL 1999</title>
		<meeting>ACL 1999</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,186.33,198.12,8.10;6,342.04,196.65,198.02,8.10;6,342.04,207.09,161.25,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,496.72,186.33,43.44,8.10;6,342.04,196.65,198.02,8.10;6,342.04,207.09,65.17,8.10">Probe, cluster, and discover: Focused extraction of qa-pagelets from the deep web</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buttler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,423.64,207.09,75.02,8.10">Proceedings of ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,222.33,197.97,8.10;6,342.04,232.65,197.88,8.10;6,342.04,242.97,127.05,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,364.84,232.65,175.08,8.10;6,342.04,242.97,10.11,8.10">Web Question Answering: Is More Always Better</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,367.60,242.97,79.06,8.10">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,258.33,197.97,8.10;6,342.04,268.65,197.97,8.10;6,342.04,278.97,198.06,8.10;6,342.04,289.29,154.77,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,397.84,278.97,142.26,8.10;6,342.04,289.29,37.02,8.10">Web-scale information extraction in KnowItAll</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,395.32,289.29,97.42,8.10">Proceedings of WWW 2004</title>
		<meeting>WWW 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,304.65,198.00,8.10;6,342.04,314.97,198.12,8.10;6,342.04,325.29,124.53,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,397.48,304.65,142.56,8.10;6,342.04,314.97,16.58,8.10">Automated discovery of wordnet relations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,398.20,314.97,141.96,8.10;6,342.04,325.29,15.05,8.10">WordNet: An Electronic Lexical Database</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,340.65,197.98,8.10;6,342.04,350.97,198.00,8.10;6,342.04,361.29,198.24,8.10;6,342.04,371.61,163.05,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,410.80,340.65,107.15,8.10">Intelligent Writing Assistance</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heidorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,534.52,340.65,5.50,8.10;6,342.04,350.97,198.00,8.10;6,342.04,361.29,198.24,8.10;6,342.04,371.61,106.77,8.10">A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. Marcel Dekker</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="181" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,386.97,197.94,8.10;6,342.04,397.29,198.04,8.10;6,342.04,407.61,168.81,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,500.68,386.97,39.30,8.10;6,342.04,397.29,198.04,8.10;6,342.04,407.61,26.96,8.10">Answering Definition Questions with Multiple Knowledge Sources</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,384.88,407.61,103.40,8.10">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,422.97,198.18,8.10;6,342.04,433.29,198.14,8.10;6,342.04,443.61,118.53,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,518.68,422.97,21.54,8.10;6,342.04,433.29,135.63,8.10">Using the web to overcome data sparseness</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ouriopina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,495.16,433.29,45.02,8.10;6,342.04,443.61,57.24,8.10">Proceedings of EMNLP 2002</title>
		<meeting>EMNLP 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,458.97,198.09,8.10;6,342.04,469.29,198.14,8.10;6,342.04,479.61,150.33,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,342.04,469.29,136.69,8.10">Scaling question answering to the web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,495.16,469.29,45.02,8.10;6,342.04,479.61,127.72,8.10">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,494.85,198.09,8.10;6,342.04,505.29,197.94,8.10;6,342.04,515.61,198.12,8.10;6,342.04,525.93,88.05,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,482.20,505.29,57.78,8.10;6,342.04,515.61,162.33,8.10">Miningthe Web for Anwers to Natural Language Questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,523.72,515.61,16.44,8.10;6,342.04,525.93,65.58,8.10">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,541.29,198.21,8.10;6,342.04,551.61,198.00,8.10;6,342.04,561.93,197.94,8.10;6,342.04,572.25,64.05,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,426.64,551.61,113.40,8.10;6,342.04,561.93,46.12,8.10">Is question answering an acquired skill?</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Paranjpe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,402.64,561.93,137.34,8.10;6,342.04,572.25,41.56,8.10">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,587.61,198.18,8.10;6,342.04,597.93,198.18,8.10;6,342.04,608.25,198.09,8.10;6,342.04,618.57,41.61,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,431.80,587.61,93.19,8.10">Overview of TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,342.04,597.93,198.18,8.10;6,342.04,608.25,194.21,8.10">NIST Special Publication 500-261: The Thirteenth Text REtrieval Conference Proceedings (TREC 2004)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,342.04,633.93,198.00,8.10;6,342.04,644.25,198.06,8.10;6,342.04,654.57,198.10,8.10;6,342.04,664.89,182.37,8.10" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
		<title level="m" coord="6,493.84,633.93,46.20,8.10;6,342.04,644.25,177.24,8.10;6,342.04,654.57,198.10,8.10;6,342.04,664.89,134.52,8.10">NIST Special Publication 500-249: The Ninth Text REtrieval Conference (TREC-9) 2000</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>Overview of the Ninth Text REtrieval Conference (TREC-9</note>
</biblStruct>

<biblStruct coords="6,342.04,680.25,197.88,8.10;6,342.04,690.57,198.12,8.10;6,342.04,700.89,151.89,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,447.64,680.25,92.28,8.10;6,342.04,690.57,161.95,8.10">Improving Trigram Language Modeling with the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,523.60,690.57,16.56,8.10;6,342.04,700.89,72.10,8.10">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="592" to="597" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
