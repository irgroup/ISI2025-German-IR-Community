<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.54,86.89,224.47,15.06;1,212.40,104.82,166.73,15.06">Question Answering with Lydia (TREC 2005 QA track)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.34,143.97,61.53,11.46"><forename type="first">Jae</forename><surname>Hong Kil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science State</orgName>
								<orgName type="department" key="dep2">York at Stony Brook Stony Brook</orgName>
								<orgName type="institution">University of New</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.90,143.97,57.18,11.46"><forename type="first">Levon</forename><surname>Lloyd</surname></persName>
							<email>lloyd@cs.sunysb.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science State</orgName>
								<orgName type="department" key="dep2">York at Stony Brook Stony Brook</orgName>
								<orgName type="institution">University of New</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.91,143.97,66.38,11.46"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
							<email>skiena@cs.sunysb.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science State</orgName>
								<orgName type="department" key="dep2">York at Stony Brook Stony Brook</orgName>
								<orgName type="institution">University of New</orgName>
								<address>
									<postCode>11794-4400</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.54,86.89,224.47,15.06;1,212.40,104.82,166.73,15.06">Question Answering with Lydia (TREC 2005 QA track)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7553C5F9DB3FC489A2BEB2082F92F744</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of our participation in TREC 2005 was to determine how effectively our entity recognition/text analysis system, Lydia (http://www.textmap.com) <ref type="bibr" coords="1,356.97,278.76,8.97,11.46" target="#b0">[1]</ref><ref type="bibr" coords="1,365.94,278.76,4.49,11.46" target="#b1">[2]</ref><ref type="bibr" coords="1,370.43,278.76,8.97,11.46" target="#b2">[3]</ref> could be adapted to question answering. Indeed, our entire QA subsystem consists of only about 2000 additional lines of Perl code. Lydia detects every named entity mentioned in the AQUAINT corpus, and keeps a variety of information on named entities and documents in a relational database. We can collect candidate answers by means of information kept in the database. To produce a response for the main task or a ranked list of documents for the document ranking task, we rank the collected candidate answers or documents using syntactic and statistical analyses.</p><p>A significant distinction from other question answering systems <ref type="bibr" coords="1,399.99,373.61,8.97,11.46" target="#b3">[4]</ref><ref type="bibr" coords="1,408.96,373.61,4.49,11.46" target="#b4">[5]</ref><ref type="bibr" coords="1,413.45,373.61,8.97,11.46" target="#b5">[6]</ref> presented earlier at TREC is that we do not use web sources such as Wikipedia and Google to generate candidate answers or answers. Rather, we only use syntactic and statistical features of the test set of questions and corpus provided. Our approach is independent of other sources, and finds answers from the text provided.</p><p>We describe the design of Lydia and associated algorithms in Section 2, and focus on the design and algorithms of the QA system in Section 3. We then analyze the performance of the QA system in Section 4, and conclude this paper with discussion on future directions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Lydia System</head><p>Lydia is designed for high-speed analysis of online text, and it analyzes thousands of curated text feeds daily. Lydia is capable of retrieving a daily newspaper like The New York Times and then analyzing the resulting stream of text in under one minute of computer time.</p><p>These design criteria force us to abandon certain standard techniques from natural language processing as too slow, most notably grammar-based parsing techniques. Instead, we use part of speech tagging <ref type="bibr" coords="1,143.53,606.62,11.52,11.46" target="#b6">[7]</ref> to augment the performance of special-purpose pattern matchers to recognize names, places, and other proper nouns, as well as related patterns of interest.</p><p>A block diagram of the Lydia processing pipeline appears in Figure <ref type="figure" coords="1,414.57,633.71,4.24,11.46" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Named Entity Recognition</head><p>Fundamental to Lydia is a problem referred to as named entity recognition within the natural language processing literature, where one seeks to detect every named entity mentioned in a document. The most important phases of our system for named entity recognition are as follows:  Part of Speech Tagging To extract proper noun phrases from text, we tag each input word with an appropriate part of speech tag (noun, verb, adjective, etc.) on the basis of statistical predication and local rules. We employ Brill's popular part of speech (POS) tagger in our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic Tagging</head><p>In this phase of the pipeline, we use regular-expression patterns implemented in Perl to markup certain classes of important text features such as dates, numbers, and unit-tagged quantities.</p><p>Proper Noun Phrase Classification Each proper noun phrase in a text belongs to some semantic class, such as person, city, or company. We first attempt to classify each entity by looking it up in a series of gazetteers. If that fails, then we employ a Bayesian classifier <ref type="bibr" coords="2,487.18,510.24,11.73,11.46" target="#b7">[8]</ref>.</p><p>Rule-Based Processing Compound entities are difficult to handle correctly. For example, the entity name State University of New York, Stony Brook spans both a comma and an uncapitalized word that is not a proper noun. By comparison, China, Japan, and Korea refers to three separate entities. Our solution is to implement a small set (âˆ¼ 60) of hand-crafted rules to properly handle such exceptions.</p><p>Alias Expansion A single entity is often described by several different proper noun phrases, e.g. President Kennedy, John Kennedy, John F. Kennedy, and JFK, even in the same document. We identify two common classes of aliasing, suffix aliasing and company aliasing, and take appropriate steps to unify such representations into a single set.</p><p>Geographic Normalization Geographic names can be ambiguous. For example, Albany is both the capital of New York State and a similarly-sized city in Georgia. We use a geographic normalization routine that identifies where places are mentioned, resolves any ambiguity using population and locational information, and replaces the name with a normalized, unambiguous representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Juxtaposition Analysis</head><p>A primary goal of Lydia is to measure how entities relate to each other. Given a pair of entities in the database, we seek to assign a score to the juxtapositionness of them and then for any given entity we can find the other entities that scored the highest with it.</p><p>To determine the significance of a juxtaposition, we bound the probability that two entities co-occur in the number of articles that they co-occur in if occurrences where generated by a random process. To estimate this probability we use a Chernoff Bound:</p><formula xml:id="formula_0" coords="3,196.79,264.11,197.47,28.24">P (X &gt; (1 + Î´)E[X]) â‰¤ ( e Î´ (1 + Î´) (1+Î´) ) E[X]</formula><p>where Î´ measures how far above the expected value the random variable is. If we set (1 + Î´)E[X] = F = number of co-occurrences, and consider X as the number of randomized juxtapositions, we can bound the probability that we observe at least F juxtapositions by calculating</p><formula xml:id="formula_1" coords="3,220.53,360.88,120.62,38.96">P (X &gt; F ) â‰¤ ( e F E[X] -1 ( F E[X] ) ( F E[X]</formula><p>)</p><formula xml:id="formula_2" coords="3,347.34,371.12,23.19,13.46">) E[X]</formula><p>where E[X] = nan b N , N = number of sentences in the corpus, n a = number of occurrences of entity a, and n b = number of occurrences of entity b, as the juxtaposition score for a pair of entities. We display -log of this probability for numerical stability and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Answering and Document Ranking</head><p>Our QA system is designed to answer factoid, list, and other questions, and to rank the relevance of documents for each question. The QA system performs question answering and document ranking through three partially overlapped flows by means of information kept in the database of Lydia and tagged AQUAINT corpus which are acquired by running the AQUAINT corpus through the Lydia pipeline.</p><p>A block diagram of the QA system appears in Figure <ref type="figure" coords="3,347.33,581.26,4.24,11.46">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Factoid and List Questions</head><p>To produce an answer for a factoid and answers for a list question, our QA system processes five phases following flowline (1) in Figure <ref type="figure" coords="3,277.22,660.59,4.24,11.46">2</ref>. The five phases are as follows:</p><p>Question Preprocessing To analyze question types, we part of speech tag the test set of questions provided. We employ Brill's part of speech (POS) tagger in the questions. ex) Where is Port Arthur? â‡’ Where/WRB is/VBZ Port/NNP Arthur/NNP ?/. </p><p>(3) Document Ranking</p><p>(1),( <ref type="formula" coords="4,246.88,167.08,2.51,9.26" target="#formula_5">2</ref>)</p><formula xml:id="formula_4" coords="4,180.50,198.18,26.19,55.17">(1)<label>(1)</label></formula><p>(</p><p>(2) Target Extraction The Lydia database keeps track of every named entity recognized. To use information in the database, we extract a target, for example, "Kim Jong Il". When Lydia does not recognize a target, we extract one as follows:</p><formula xml:id="formula_6" coords="4,333.12,146.28,123.67,102.93">Document Scoring (3)<label>(3) (3)</label></formula><p>-Valid Form Extraction -When a target is not in a valid form such as a plural noun, we use WordNet <ref type="bibr" coords="4,135.95,439.72,11.52,11.46" target="#b8">[9]</ref> to obtain a valid form of the target.</p><p>-Partial Extraction -Lydia recognizes proper nouns and various types of numerical nouns as named entities. However, Lydia does not recognize a phrase or a compound word such as "Russian submarine Kursk sinks" as a named entity. In that case, we extract a part of the target, for example, "Kursk" which is recognized by Lydia.</p><p>-Synonym Extraction -If a target and any part of the target are not recognized by Lydia, we acquire synonyms of the target using WordNet, and then choose a synonym which is recognized by Lydia as a target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Answer Collection</head><p>We identify which sentences in the documents contain the target by querying the Lydia database, as well as all the entities which are in these sentences. Since we only identify the sentences where the target is, it takes comparatively less time to collect candidate answers than with whole documents. In addition, it enables us to more specifically identify which part in the text provides an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Answer Elimination and Scoring</head><p>To rank the candidate answers, we use three types of analysis algorithms.</p><p>-Juxtaposition Analysis -We obtain the juxtaposition score between a target and each candidate answer querying the Lydia database. More strongly associated terms are likely to represent answers to commonly asked questions.</p><p>-Question Term Analysis -To analyze question terms in a question, we first eliminate interrogative phrases and stop words <ref type="bibr" coords="5,263.21,411.66,16.01,11.46" target="#b9">[10]</ref>. We then obtain a valid form of each non-trivial question term and its synonyms. We weight each candidate answer identified in a sentence with both the target and a non-trivial question term/its synonym using the following equation:</p><formula xml:id="formula_7" coords="5,262.88,462.11,80.74,27.43">weight = C Ã— n q n t</formula><p>where C is a constant(â‰ˆ 100), n q = number of non-trivial question term or its synonym, and n t = number of total non-trivial question terms.</p><p>-Question Type Analysis -Our methods for eliminating and scoring candidate answers vary with question types. Table <ref type="table" coords="5,219.10,551.48,5.46,11.46" target="#tab_3">1</ref> shows five question types and their example questions.</p><p>â€¢ For question types 1 and 2, the class<ref type="foot" coords="5,290.18,572.69,4.24,8.37" target="#foot_0">1</ref> of an answer is likely to be the same as the noun followed by a verb. For example, "country" is the valid form of "countries" in the example question 1 in Table <ref type="table" coords="5,244.45,601.24,4.24,11.46" target="#tab_3">1</ref>, thus we eliminate candidate answers whose class is not the same with the noun, country. If the noun followed by a verb does not match with any class in Lydia, we obtain hypernyms of the noun using WordNet. For example, the noun, "players" in the example of question type 2 in Table <ref type="table" coords="5,391.29,641.89,5.46,11.46" target="#tab_3">1</ref> does not match with any class. However, a hypernym of "players" is person, and the class of the answer for the question is likely to be person, so we eliminate candidates answers whose class is not person.</p><p>â€¢ The answers for question type 3 are obviously numerical. Therefore, we eliminate candidate answers whose class is not numerical such as company, country, disease or person. â€¢ An interrogative or interrogative phrase does not provide a reliable clue to recognize the class of an answer for question types 4 and 5, thus we use a set of lists, "interrogative, class, and relevance score" for all interrogatives, which are who, whose, whom, when, where, why, what, which, and how. Table <ref type="table" coords="6,303.18,157.39,5.46,11.46" target="#tab_4">2</ref> shows the relevance scores of the nine interrogatives and the selected classes. For example, the interrogative when is on five lists as follows: WHEN CARDINAL where C is a constant(â‰ˆ 2). We then sort all the candidate answers in descending order of score.</p><p>-Answer Selection -An answer for a factoid question is the entity whose score is the highest, and answers for a list question are the entities whose scores are higher than a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Other Question</head><p>To produce answers for an other question, our QA system processes the four phases following flowline (2) in Figure <ref type="figure" coords="6,172.55,533.43,4.24,11.46">2</ref>. Answering an other question does not require the question preprocessing phase since an other question in the test set of questions provided is not in sentence form, but merely a word, "Other". The candidate answer pruning phase plays a role in narrowing down candidate answers as the candidate answer elimination and scoring phase does for a factoid and a list question. The four phases are as follows:</p><p>Target Extraction Basically, the algorithms for an other question are the same with the ones for a factoid and a list question. In addition, we employ a set of first name-nickname pairs <ref type="bibr" coords="6,99.18,659.10,16.98,11.46" target="#b10">[11]</ref> to expand the target using the first name equivalence, since we need to obtain the largest set of sentences which might contain definitions of the target. For example, since Jim is a nickname of James, Jim Inhofe and James Inhofe indicate the same person. Assuming that two consecutive words starting with a capital in a target represent a person name, we search the first word in the set of first name-nickname pairs. If found, we use both the original target and the new target replaced the first word by its pair as targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Answer Collection</head><p>We obtain the documents which contain a target or targets by querying the Lydia database, and collect all the sentences which contain the target or targets.</p><p>Candidate Answer Pruning Though every sentence contains information on a target or targets, we need to prune sentences which are duplicated or not directly related to the target or targets. Our pruning algorithms are as follows:</p><p>-Sentence Preprocessing -At first, we remove meaningless snippets such as newspaper title, publication date and place in each sentence. We then eliminate too short sentences which barely convey significant meanings, and sentences whose subjects are the first-person or the second-person, namely "I", "We" or "You" since they are highly likely to only contain a subjective opinion of the subject. In addition, fully or partially duplicated sentences are eliminated.</p><p>-Sentence Scoring -We assign the same initial score to each sentence, and weight or deweight each sentence. We weight a sentence which fully contains the target more than one with the target in part, and a sentence which contains the possible syntactic pattern of a definition, Target + (is|was|who|which|that) or Target(s|es) + (are|were|who|which|that). On the other hand, we deweight a comparatively long or short sentence (the threshold is determined to be 100 due to the allowance of characters for each correct nugget <ref type="bibr" coords="7,404.73,370.65,16.98,11.46" target="#b11">[12,</ref><ref type="bibr" coords="7,423.52,370.65,13.64,11.46" target="#b12">13]</ref>) as well as those which contain unrelated words such as "say", "ask", "report", "If", "Unless", interrogatives, and subjective pronouns. We also deweight a sentence which contains too many non-trivial words or proper nouns compared to other types of words since the sentence is likely to be just an enumeration of nouns such as names and places.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Production</head><p>We sort all the candidate answers in descending order, and then select sentences whose scores are higher than a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document Ranking</head><p>Since our QA system does not rank documents for the purpose of answering three types of questions, we employ a function of length of a document, number of occurrences of a target and number of occurrences of marked (tagged) terms, or length of a document and number of occurrences of marked (tagged) terms in the document scoring phase.</p><p>The QA system processes four phases following flowline (3) in Figure <ref type="figure" coords="7,415.79,588.80,4.24,11.46">2</ref>. The four phases described in two sections, target extraction, document collection, document scoring, and document ranking are as follows:</p><p>Target Extraction and Document Collection We extract a target with the same algorithms for a factoid and a list question, and then identify all the documents which contain the target or synonyms of the target querying the Lydia database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Scoring and Ranking</head><p>To obtain a ranked list of 1000 documents for each question, we follow the three steps:</p><p>-Documents with the target -We score each document with the function:</p><formula xml:id="formula_8" coords="8,215.76,118.26,176.48,12.65">score = C 1 Ã— l d + C 2 Ã— n t + C 3 Ã— n m</formula><p>where C 1 , C 2 and C 3 are constants, l d = length of a document, n t = number of occurrences of a target, and n m = number of occurrences of marked (tagged) terms. We then sort the documents in descending order.</p><p>-Documents with synonyms of the target -The scoring function is the same with the one for documents with the target. After scoring each document, we sort the documents in descending order, and scale down the scores of the sorted documents to rank them lower than the lowest ranked document which contains the target.</p><p>-Completing the ranking -To ensure we submit 1000 documents for each query, we attempt to identify the top 1000 documents for a "null query", and use these to make up the balance. We score all the documents in the AQUAINT corpus with the function:</p><formula xml:id="formula_9" coords="8,240.41,317.65,127.18,12.65">score = C 1 Ã— l d + C 2 Ã— n m</formula><p>where C 1 and C 2 are constants, l d = length of the document, and n m = number of occurrences of marked (tagged) terms. We then sort the documents in descending order, and generate a list of 1000 documents with the highest score. This procedure is done once since it is independent of questions. Finally, we fill the balance from the 1000 documents if the number of documents which contain the target and synonyms of the target is less than 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Analysis</head><p>We submitted three runs for the main task and the document ranking task of TREC 2005 QA track: SUNYSB05qa1, SUNYSB05qa2 and SUNYSB05qa3. These three runs only differ in settings for answer production. The setting of SUNYSB05qa2 is less conservative than the one of SUNYSB05qa1, and the setting of SUNYSB05qa3 is more conservative than the one of SUNYSB05qa1. According to the final results, our QA system for list questions and other questions performs better when settings are less conservative while for factoid questions it produces better performance when settings are more conservative. Table <ref type="table" coords="8,118.39,562.56,5.46,11.46" target="#tab_6">3</ref> shows performance of our three runs and the median scores of 71 runs submitted to TREC 2005 QA track. The scores of our runs for list questions and other questions are greater than the median scores of 71 runs, but the score for factoid questions is less than the one of 71 runs. The final scores of our runs are almost the same with the median final score of 71 runs. Table <ref type="table" coords="9,118.89,89.80,5.46,11.46" target="#tab_6">3</ref> shows that performance of our QA system for list questions and other questions is better than the one for factoid questions in comparision of performance of other runs submitted to TREC 2005 QA track. This results from the following features of our QA system. It chooses answers using syntactic and statistical features of the test set of questions and corpus provided. The probability of selecting a single correct answer out of candidate answers for a factoid question is very low. On the other hand, the probability of selecting some correct answers for a list question and an other question is relatively high now that it is likely that collected candidate answers include some of the correct answers.</p><p>In general, the accuracy of extracting a proper target fairly affects performance of the QA system. For example, the QA system performs well for a target "Sammy Sosa" since it can properly extract the target and collect candidate answers based on the target. However, it does not perform well for complicated targets like "Plane clips cable wires in Italian resort" or "1998 Baseball World Series." In addition, in case that a correct answer is a non-named entity, performance of the QA system is not satisfactory since Lydia which the QA system essentially depends on mainly detects named entities in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have presented the design of the Lydia question answering, along with analyzing its performance.</p><p>We are continuing to improve Lydia, particularly the entity recognition algorithms, entity classification, and geographic normalization. Since main phases of the QA system such as target extraction, candidate answer collection, candidate answer elimination and scoring are essentially based on Lydia, improvement in Lydia will enhance the performance of the QA system.</p><p>Future directions of work on improving the QA system include exploring the use of a system which automatically extracts relations between entities by means of analyzing syntactic and semantic patterns of verbs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,183.48,318.54,224.55,9.41"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block Diagram of the Lydia Processing Pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,73.39,88.57,457.65,224.33"><head>Table 1 .</head><label>1</label><figDesc>Five Question Types with Example Questions</figDesc><table coords="5,73.39,88.57,457.65,224.33"><row><cell></cell><cell></cell><cell>Type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Question</cell><cell></cell><cell></cell></row><row><cell cols="10">1 (What|Which) + (the|an|a|NN of|) + (NN|JJ + NN) Which countries expressed regret about the loss?</cell></row><row><cell cols="6">2 (List|Name|. . . ) + (the|an|a|NN of|) + (NN|JJ + NN)</cell><cell cols="3">Name players on the French team.</cell><cell></cell></row><row><cell>3</cell><cell></cell><cell cols="2">How + (NN|JJ|RB)</cell><cell></cell><cell></cell><cell cols="3">How many students were wounded?</cell><cell></cell></row><row><cell>4</cell><cell cols="4">(Who|Whose|Whom|When|Where|Why)</cell><cell></cell><cell cols="3">When was Enrico Fermi born?</cell><cell></cell></row><row><cell>5</cell><cell></cell><cell cols="2">(How|What|Which)+ VB</cell><cell></cell><cell></cell><cell cols="3">What is Hong Kong's population?</cell><cell></cell></row><row><cell></cell><cell cols="9">CARDINAL COMPANY COUNTRY DATE DISEASE PERSON TIMEPERIOD VOLUME WEBSITE</cell></row><row><cell>WHO</cell><cell>N/A</cell><cell>2</cell><cell>2</cell><cell>N/A</cell><cell>N/A</cell><cell>10</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WHOSE</cell><cell>N/A</cell><cell>2</cell><cell>2</cell><cell>N/A</cell><cell>N/A</cell><cell>10</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WHOM</cell><cell>N/A</cell><cell>2</cell><cell>2</cell><cell>N/A</cell><cell>N/A</cell><cell>10</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WHEN</cell><cell>10</cell><cell>N/A</cell><cell>N/A</cell><cell>10</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WHERE</cell><cell>N/A</cell><cell>2</cell><cell>10</cell><cell>N/A</cell><cell>1</cell><cell>1</cell><cell>N/A</cell><cell>N/A</cell><cell>1</cell></row><row><cell>WHY</cell><cell>N/A</cell><cell>1</cell><cell>1</cell><cell>N/A</cell><cell>5</cell><cell>1</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WHAT</cell><cell>2</cell><cell>10</cell><cell>2</cell><cell>2</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>10</cell></row><row><cell>WHICH</cell><cell>2</cell><cell>10</cell><cell>2</cell><cell>2</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>10</cell></row><row><cell>HOW</cell><cell>N/A</cell><cell>1</cell><cell>1</cell><cell>N/A</cell><cell>5</cell><cell>1</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,153.44,314.40,284.56,9.41"><head>Table 2 .</head><label>2</label><figDesc>Relevance Scores of Nine Interrogatives and Selected Classes</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,72.00,184.49,447.72,217.57"><head></head><label></label><figDesc>10, WHEN DATE 10, WHEN MONTH 2, WHEN DAY 2, WHEN TIME 2. This means that when the interrogative of a question is when, the class of an answer can be only one of the five classes, cardinal, date, month, day, and time, and the relevance score for each candidate answer whose class is cardinal, date, month, day, or time is 10, 10, 2, 2, or 2, respectively.Answer ProductionTo produce an answer for a factoid question or answers for a list question, we rank candidate answers as follows:</figDesc><table /><note coords="6,77.21,306.60,442.44,11.46;6,88.93,319.99,139.90,11.46;6,213.44,343.52,181.62,11.46;6,88.93,367.06,127.77,11.46;6,165.37,390.60,270.68,11.46;6,436.02,388.59,6.04,8.37"><p>-Candidate Answer Ranking -We score each candidate answer depending on question type, for question types 1, 2 and 3: score = juxtaposition score Ã— weight for question types 4 and 5: score = juxtaposition score Ã— weight Ã— relevance score C</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,102.78,635.85,382.07,81.14"><head>Table 3 .</head><label>3</label><figDesc>Performance of SUNYSB Runs in TREC 2005</figDesc><table coords="8,102.78,635.85,382.07,69.94"><row><cell></cell><cell cols="4">Factoid Question List Question Other Question Final Score</cell></row><row><cell>SUNYSB05qa1</cell><cell>0.102</cell><cell>0.066</cell><cell>0.194</cell><cell>0.120</cell></row><row><cell>SUNYSB05qa2</cell><cell>0.105</cell><cell>0.064</cell><cell>0.196</cell><cell>0.121</cell></row><row><cell>SUNYSB05qa3</cell><cell>0.122</cell><cell>0.059</cell><cell>0.179</cell><cell>0.123</cell></row><row><cell>Median Score</cell><cell>0.152</cell><cell>0.053</cell><cell>0.156</cell><cell>0.123</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,82.91,706.58,436.59,9.41;5,82.91,717.54,436.47,9.41;5,82.91,728.51,346.13,9.41"><p>Currently, Lydia classifies named entities into about 60 semantic classes, e.g. cardinal, country, date and university. We are still developing gazetteer-based algorithm and Bayesian classifier to classify entities more accurately and specifically. Hence, the number of semantic class is steadily increasing.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,80.19,505.70,439.32,9.41;9,89.22,516.66,430.24,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,249.43,505.70,180.93,9.41">Lydia: A system for large-scale news analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kechagias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,450.08,505.70,69.44,9.41;9,89.22,516.66,240.46,9.41">Proc. 12th Symp. of String Processing and Information Retrieval (SPIRE &apos;05)</title>
		<meeting>12th Symp. of String essing and Information Retrieval (SPIRE &apos;05)<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-04">November 2-4 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.19,527.05,439.17,9.41;9,89.22,538.01,20.98,9.41" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,240.35,527.05,209.34,9.41">Identifying synonymous names in large news corpra</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct coords="9,80.19,548.41,436.68,9.41" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,298.54,548.41,126.36,9.41">Spatial analysis of news sources</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct coords="9,80.19,558.80,439.17,9.41;9,89.22,569.76,146.67,9.41" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,496.54,558.80,22.82,9.41;9,89.22,569.76,118.78,9.41">Using wikipedia at the trec qa track</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.19,580.16,439.17,9.41;9,89.22,591.12,63.82,9.41" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,335.87,580.16,183.49,9.41;9,89.22,591.12,35.79,9.41">Experiments with web qa system and trec2004 questions</title>
		<author>
			<persName coords=""><forename type="first">Dmitri</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><surname>Antonio Robles-Flores</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.19,601.51,439.18,9.41;9,89.22,612.48,47.10,9.41" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m" coord="9,428.94,601.51,90.43,9.41;9,89.22,612.48,19.22,9.41">Fduqa on trec2004 qa track</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.19,622.87,439.30,9.41;9,89.22,633.83,172.03,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,130.99,622.87,215.55,9.41">Some advances in rule-based part of speech tagging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,371.72,622.87,147.77,9.41;9,89.22,633.83,144.27,9.41">Proccedings of the Twelfth National Conference on Artificial Intelligence</title>
		<meeting>cedings of the Twelfth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.19,644.22,218.49,9.41" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m" coord="9,140.93,644.22,70.86,9.41">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,80.19,654.62,176.44,9.41" xml:id="b8">
	<monogr>
		<ptr target="http://wordnet.princeton.edu/" />
		<title level="m" coord="9,89.22,654.62,32.72,9.41">Wordnet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,79.85,665.01,252.14,9.41" xml:id="b9">
	<monogr>
		<ptr target="http://dvl.dtic.mil/stoplist.html" />
		<title level="m" coord="9,89.22,665.01,99.90,9.41">Dvl/verity stop word list</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,79.85,675.41,341.82,9.41" xml:id="b10">
	<monogr>
		<ptr target="http://www.veritasinfo.com/page7.html" />
		<title level="m" coord="9,89.22,675.41,160.15,9.41">Nicknames and their possible full names</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,79.85,685.81,439.65,9.41;9,89.22,696.77,171.09,9.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,169.48,685.81,205.13,9.41">Overview of the trec 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,393.97,685.81,125.53,9.41;9,89.22,696.77,142.84,9.41">Proceedings of the Twelfth Text REtreival Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtreival Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,79.85,707.16,439.66,9.41;9,89.22,718.11,191.80,9.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,171.69,707.16,208.40,9.41">Overview of the trec 2004 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.21,707.16,118.30,9.41;9,89.22,718.11,163.54,9.41">Proceedings of the Thirteenth Text REtreival Conference (TREC 2004)</title>
		<meeting>the Thirteenth Text REtreival Conference (TREC 2004)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
