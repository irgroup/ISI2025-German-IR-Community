<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.58,83.76,392.55,15.48">Language Modeling Approaches for Enterprise Tasks</title>
				<funder ref="#_wq2vQHg #_w5b4Kf5 #_8YnDRxY #_DS88Z9Y #_5XNpdRa #_6cVMjGe #_hBNeGbB #_HBwackm #_ay4jr5Q">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_hydGNAG">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.99,116.28,76.70,10.75;1,232.69,114.75,1.41,6.99"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
						</author>
						<author>
							<persName coords="1,259.59,116.28,80.04,10.75"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
						</author>
						<author>
							<persName coords="1,366.53,116.28,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.58,83.76,392.55,15.48">Language Modeling Approaches for Enterprise Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3F222DFA06C164167744D7B8BB6A5633</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC 2005 Enterprise track. We provide a detailed account of the ideas underlying our language modeling approaches to these tasks, report on our results, and give a summary of our findings so far.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our aim for the TREC 2005 Enterprise track was to adapt our existing language modeling framework to the specific needs of each task. A key goal was to incorporate, and make use of, the structure and structured content housed within the W3C data used in the track.</p><p>Using generative language models, we tailored the framework to address the particular needs of the three sub tasks: Email Discussion Search, Known Email Search, and Expert Finding. These tasks were executed on the enterprise collection which contained six different types of web pages created from a crawl of the W3C website. These were lists (email forum), dev, www, esw, other, and people. The former two tasks used only the email forum where structure was a main theme of our research. Here, we examined the link structure generated by replies for the discussion search (Section 2), whilst we considered the internal structure of an email for known item searching (Section 3). For the expert finding task our focus was on associating a document with a candidate expert to build candidate models (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Email Discussion Search</head><p>The goal of this task was to retrieve emails which contained a discussion about the query topic, where highly relevant documents would introduce a new point to the discussion (such as pro or con given the topic). Consequently, for this task only the email forum (lists) documents were considered. This subset contains pages which are not only emails, but pages for the navigation of the forums as well. The collection comprised of 198,275 documents of which approxi-mately 174,413 were emails to the forum lists. <ref type="foot" coords="1,503.44,188.99,3.49,6.05" target="#foot_0">1</ref> Each email page contains links to the other emails that are related to it (i.e., the response(s) an email attracts, the email that was responded to, and next/previous emails in the listing). Of these emails only 75,422 emails had attracted a response. <ref type="foot" coords="1,551.93,236.81,3.49,6.05" target="#foot_1">2</ref>This left 99,991 emails without any replies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discussions</head><p>We assumed a 'discussion thread' consisted of a set of emails, linked by replies, which formed a graph of emails. The number of discussion threads within the 75,422 emails was approximately 19,917, where the average number of emails in a discussion thread was 3.8.</p><p>Within a discussion thread, there were two main quantitative characteristics of interest: breadth, indicating the number of replies an email has directly received, and depth, indicating the number of consecutive replies. We conducted informal interviews with two email forum users, who regularly use technical forums, and asked them about how they used the email forums and about the shape of the discussion threads. The main points ascertained were as follows. They rarely ever searched for discussions, when they did, they would favor the use of navigational methods as opposed to search facilities. When replying to an email, it was important to respond to a point in that email, and that the email should only respond to that point. Further, a separate email should be sent in response to the different points in that email. Hence, an email attracting multiple replies would probably discuss multiple points, whilst consecutive replies would probably discuss one point in detail.</p><p>To follow up this intuition, we examined ten discussion graphs (about 50 emails in total), five of which contained a breadth of at least three and five with a depth of at least three. From this set of graphs, consecutive replies tended to discuss a point in detail, whilst multiple replies would usually present different points, but would sometimes include emails referring to other replies. This suggested that long chains of emails would be indicative of arguments for and against the topic of discussion, and may have been useful for retrieval. We endeavored to encode some of these findings within our retrieval strategy, the language modeling framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Model</head><p>The standard language modeling approach computes the probability of a query q being generated from a document model θ d on behalf of the document d as follows:</p><formula xml:id="formula_0" coords="2,53.80,145.54,209.74,23.58">p(q|θ d ) = t∈q (1 -λ)p(t|d) + λp(t) n(t,q) ,<label>(1)</label></formula><p>where p(t|d) is the maximum likelihood estimate of term t in document d, p(t) is the unconditional probability of t (also determined using the maximum likelihood estimate), n(t, q) is the number of times term t occurs in query q, and λ is the smoothing parameter. If λ is set to β n(d)+β , where n(d) is the size of the document, Bayes Smoothing with a Dirichlet prior of the document model is obtained (instead of Jelinek-Mercer Smoothing) <ref type="bibr" coords="2,176.13,266.68,10.58,8.64" target="#b2">[3]</ref>. Ranking according to the joint probability of a query and document p(q, d), involves the multiplication of the document prior p(d) to both sides of the equation such that p(q, d) = p(q|θ d )p(d). This represents a natural extension to the framework for encoding external evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion Runs</head><p>For our discussion run submissions, we applied two priors; one to filter our non-discussion emails, the other to bias towards larger/longer threads of discussion. For this, the lists collection was indexed using LEMUR. No stemming was applied but standard stop words were removed. We developed a set of five training discussion topics with 54 highly relevant emails and 29 relevant documents, after assessing a total of 169 documents. These topics were used to select our baseline run, to which we applied two different document priors that adjusted the scores according to our intuitions. For our baseline, we examined a host of different parameter settings with both Jelinek Mercer Smoothing and Bayes Smoothing, but Bayes smoothing was found to perform the best when β = 350. The two priors were then applied: Document Type Filter. We removed all messages which were not identified as an email in the collection. This can be considered as a document prior where</p><formula xml:id="formula_1" coords="2,73.72,569.90,219.18,20.91">p(d) = k &gt; 0 if the document is an email, else p(d) = 0.</formula><p>If the document contained the structured fields subject, author and date then it was considered an email.</p><p>Thread Size Prior. The probability of a document, p(d), was proportional to the size of the graph from which that email came. Such that:</p><formula xml:id="formula_2" coords="2,73.72,665.31,159.47,24.72">p(d) = g(d) + α d (g(d ) + α) ,<label>(2)</label></formula><p>where g(d) is the size of the graph given d and α is a smoothing parameter to adjust the influence of the prior (known as Laplace smoothing). This encoded our intuition that discussions are a group of messages, and that an email in a discussion is more likely to be relevant than a email that is not. This prior was applied to the top 1000 documents retrieved documents to re-rank the result set.</p><p>We also considered augmentation of the ranked list, instead of re-ranking. This approach re-structured the results such that if an email appeared in the ranked list, then all related emails in its graph were given the same rank. This was to provide the user with a coherent view of the result list (i.e., grouped by discussion thread). Unfortunately, this run was not successfully submitted due to time constraints. However, given the evaluation scheme used, we would not have expected the results to fare significantly better, because the evaluation is based on a ranked list, and not the representation presented to the user (i.e., a graph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summary of Runs and Results</head><p>The following runs were submitted and the results are displayed in Table <ref type="table" coords="2,383.26,320.45,3.74,8.64" target="#tab_0">1</ref>. All submitted runs were automatic and only the query field of the topic was used. ToNsBs350 Baseline run using Bayes Smoothing (β = 350).</p><p>ToNsBs350F Same as ToNsBs350, but with Document Type Filter.</p><p>ToNsBs350FT Same as ToNsBs350F, but with the Thread Size Prior (α = 1).</p><p>ToNsBs350FT5 Same as ToNsBs350F, but with the Thread Size Prior (α = 5).  <ref type="table" coords="2,361.26,591.27,3.74,8.64" target="#tab_0">1</ref>, the first column displays the run identifier, the second reports the mean average precision (MAP), then the following three columns display the precision at 10, 20 and 100. From these results, we can see that the influence of the filter increased the MAP by about 6% over the baseline run. However, applying the filter resulted in a loss of 21 relevant documents. This was due to either non-emails judged as relevant or documents not being parsed correctly. The application of the thread size prior introduced too much bias and resulted in a massive loss of MAP. Further work is required to examine the influence of the prior on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Email Known-Item Search</head><p>The goal of this task was to find the known (relevant) email given the query topic. The intuition that motivated our research was that users would pose queries for these known emails, based on what they remembered about the known email. We assumed that such query terms would invariably be the most salient features of the email. Our retrieval strategy for the known-item email search used these features and consisted of two components. First, we automatically inferred the structure of a query (with respect to the email's structure) similar to <ref type="bibr" coords="3,139.01,187.14,10.58,8.64" target="#b0">[1]</ref>. Then, we execute the structured query on a fielded language model to utilize this structure in the retrieval process. So, instead of treating each email as a whole document, we broken the email into four structured fields. These were: author, date, subject and body of the email. All other text was disregarded. These fields were chosen because they represented the key fields from which query terms appeared to be generated (or, recalled).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Querying Structuring</head><p>The query was structured by classifying each query term t according to the probability of the field x given t (i.e., p(t|x)). This was evaluated by applying Bayes theorem and then using a generative model, such that</p><formula xml:id="formula_3" coords="3,53.80,373.12,174.73,24.72">p(x|t) = p(t|x)p(x) x p(t|x )p(x ) ,<label>(3)</label></formula><p>where p(t|x) is the probability of t given x, which was estimated with Laplace estimator and proportional to the count of the number of times t occurred in x plus the Laplace constant (α = 0.00001). Query terms were assigned to the field x, if p(x|t) &gt; δ, where δ was a tuning parameter of the system. This was set to δ = 0.1 after training the system on the 25 known-item training topics and selecting δ with respect to the mean reciprocal rank of the fielded language model (Section 3.2). Each automatically structured query consisted of the set of fields, represented by q x , which contained the assigned terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fielded Language Model</head><p>The fielded language model is a simple extension of the standard language modeling approach described in Section 2.2. It treats each field of an email document as an independent source of evidence, from which each of the fields in the query are generated. Formally, this can be represented as</p><formula xml:id="formula_4" coords="3,53.80,642.41,164.18,21.69">p(q|d) = x p(q x |θ x d ),<label>(4)</label></formula><p>where p(q x |θ x d ) is the probability of the query field q x being generated from the model of the document field θ x d . This probability is computed as above for standard documents, but for each of the four fields instead.</p><p>We also considered an alternative approach, where we assumed that the sources of evidence were linearly independent and weighted by p(x), such that by marginalizing over x the query likelihood could be expressed as:</p><formula xml:id="formula_5" coords="3,316.81,116.35,174.24,21.69">p(q|d) = x p(x)p(q x |θ x d ),<label>(5)</label></formula><p>where p(x) denotes the importance of the query field in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Known-Item Runs</head><p>Our experiments examined the hypothesis that automatically inferred queries could be used to improve retrieval performance over unstructured queries (as in <ref type="bibr" coords="3,473.10,238.53,10.46,8.64" target="#b0">[1]</ref>).</p><p>The email fields selected were indexed separately in LEMUR, with Porter stemming applied and standard stop words removed. The 125 known-item queries were processed in a similar fashion. Our first submission was a baseline run, that used the standard language modeling approach on the entire email document using the original unstructured query and then we submitted four further runs using the field language model. Two runs used the query likelihood as shown in Eq. 4 and Eq. 5. However, we were concerned that the differences in length between query fields affected retrieval performance, and thus tried two runs where the normalized query likelihood was used <ref type="bibr" coords="3,457.76,382.51,10.58,8.64" target="#b1">[2]</ref>. This is equivalent to computing the odds ratio of the query being generated by the document versus the query being generated from the collection). For the model defined in Eq. 5, the prior p(x) was set on a query by query basis. p(x) was proportional to the number of query terms that were assigned to that field x, which we assumed would correlate to its importance.</p><p>The 25 training topics were used to tune the free model parameters and compare smoothing methods. We found Jelinek Mercer smoothing tended to give the best performance and subsequently used this form of smoothing for all runs and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary of Runs and Results</head><p>The following runs were submitted: qdFlat Baseline run using language modeling approach with Jelinek Mercer smoothing (λ = 0.1).</p><p>qdC Automatically structured queries (δ = 0.1), using the model in Eq. 4 with Jelinek Mercer smoothing (λ = 0.5 for all fields).</p><p>qdWcEst Same as qdC, but using the model in Eq. 5.</p><p>OddsC Same as qdC, but normalized.</p><p>OddsWcEst Same as qdWcEst, but normalized.</p><p>Run identifier MRR S@10 S@100 F@100 qdFlat 0. The results for the known-item finding subtask are shown in Table <ref type="table" coords="4,90.55,182.24,3.74,8.64" target="#tab_1">2</ref>. The second column gives the mean reciprocal rank (MRR) score. The third and fourth columns report the percentage of topics for which the known-item was found in the top 10 and 100 documents, respectively. The last column reports the percentage of topics where no known-item was found in top 100 documents (F@100).</p><p>From the result table we see that, using the model in Eq. 5 (runs qdWcEst and OddsWcEst), we were able to obtain an improvement over our baseline run, with a sizeable increase in the MRR. Once we obtained the corresponding set of known-items, we tagged the query terms according to the fields in the known emails. We found that the accuracy of our automatic query structuring procedure was just over 50%, whilst if we had simply assumed all terms were from the subject accuracy would have been just under 50%. During the training phase, better classification accuracy led to substantially improvements over the baseline regardless of the type of fielded model. However, here the ambiguous nature of the queries seriously degraded the performance of our retrieval models, as the structure could not be reliably inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Expert Search</head><p>The Expert Search task presents the following scenario: Given the document repositories of the organization, find the experts in a particular topic, field or area. Our approach employs language modeling, information retrieval, and name entity recognition techniques. Our results indicate that the latter is especially important for the task.</p><p>Our approach focuses on building representations of candidate experts from the corpus, and then identifying the set of actual experts for a given topic. To achieve this we apply the language modeling approach to the expert search problem. Each candidate is represented by the documents that are found to be the most relevant given the topic and the candidate is associated with. When a query is issued, we select the subset of the collection, containing documents, found to be the most relevant given the query. To obtain these cut-offs we apply information retrieval techniques over the document set and against the topic as a query. Then, the candidates are ranked according to the probability of the query being generated by the candidate model.</p><p>The main research problem within this work is to find the associations between documents and candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modeling</head><p>Our method is a direct application of standard language modeling techniques, where we infer a candidate model θ ca for each candidate ca, such that the probability of a term given the candidate model is p(t|θ ca ). Using this model, we can then estimate the probability of a query by taking the product across terms in the query.</p><p>p(q|θ ca ) = t∈q p(t|θ ca ) n(t,q) . (6)</p><p>Here, the standard term independence assumption is made. The candidate model is constructed by using a mixture model:</p><formula xml:id="formula_6" coords="4,316.81,250.57,211.92,11.15">p(t|θ ca ) = (1 -λ) d p(t|d)p(d|ca) + λp(t),<label>(7)</label></formula><p>where p(t) is the maximum likelihood estimate of the unconditional probability of the term occurring in the collection. The final estimation of the probability of a query given the candidate model is:</p><formula xml:id="formula_7" coords="4,316.81,328.37,227.49,29.85">p(q|θ ca ) = (8) t∈q (1 -λ)( d∈S p(t|d)p(d|ca)) + λp(t) n(t,q) ,</formula><p>where S is a subset of documents that are found to be the most relevant given the query q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Candidate Document Associations</head><p>As pointed out before, the W3C corpus is a heterogeneous document repository containing a mixture of different document types (technical reports, emails, web pages, etc). A document d in this collection, is assumed to be associated with a candidate ca, if there is a non-zero association a(d, ca) &gt; 0. The forming of these associations is vital to the performance of our methods and overall performance.</p><p>Here we introduce four different methods that we used for associating documents with candidates.</p><p>Extracting candidates is a special named entity recognition task where the list of possible candidates-with names and e-mail addresses-are given.</p><p>Extract Candidates by Name. Identification based on the candidates' name might be the most natural approach. In spite of the apparent simplicity, one has to face different challenges when solving this task. Some cases when a simple exact-matching test on the candidate's name may fail:</p><p>• different name length: middle name(s),</p><p>• accentuated letters,</p><p>• dash in the name,</p><p>• only initials of one or more names.</p><p>We experimented with different name matching methods, each addressing only some of the key issues. The candidate's name and the documents are represented as a sequence of terms, lowercased, accents on letters are replaced, and names with dashes are considered as two different terms (e.g., Hazael-Massieux ⇒ hazael massieux). The following matching methods were considered for our TREC 2005 experiments:</p><p>• M 0 EXACT MATCH: returns true if the name appears in the document exactly as it is written.</p><p>• M 1 NAME MATCH: returns true if the last name and at least the initial of the first name appears in the document.</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runs</head><p>We submitted the following 5 runs: where m = |S| is the number of documents retrieved as most relevant given a topic. In the last two runs we experimented with a linear combination of different candidate-document association methods.</p><formula xml:id="formula_8" coords="5,53.80,553.92,152.38,28.40">uams05run0 m = 500, EXACTMATCH<label>uams05run1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>uams05 #rel map R-prec P@10 P@20 RR1 The results of uams05run0 and uams05run1 show no significant difference. We conclude that using different m values for cut-offs does not really affect overall performance.</p><p>At the same time, the association methods show interesting results; EXACTMATCH (uams05run0, uams05run1) retrieves more relevant hits while the reciprocal rank of the top relevant document (RR1) is much higher for EMAILMATCH (uams05run2). This underlines our expectations that the use of EMAILMATCH results in fewer but stronger associations. Combining different matching methods (uamsrun3, uamsrun4) shows promising results and suggests experimenting with more sophisticated estimation of candidate-document associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we described our participation in the TREC 2005 Enterprise track. We found that structured information was not particularly useful for either email search task. In the case of the discussion search, this was because too much bias was introduced by the thread size prior, whilst in the known-item task the ambiguity of the queries meant that the classification accuracy was mediocre, which influenced the quality of retrieval.</p><p>As to the expert search task, we found that the performance depends crucially on the ability to recognize names of experts. A textual representation of candidates' knowledge has been created according to the documents with which they are associated. In follow-up work we are exploring a second approach which does not create a candidate model directly, but assumes conditional independence between the query and the candidate and builds a so-called aspect model. Initial results from experiments aimed at comparing the two approaches seem to favor the this second, and they confirm, yet again, that the quality of document-candidate associations has great influence on the performance in case of both models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,111.92,573.43,94.26,9.02;5,53.80,592.81,152.38,9.09;5,53.80,612.25,239.11,9.09;5,73.72,624.99,52.31,8.30;5,53.80,643.65,239.11,9.09;5,73.72,655.68,143.93,9.02"><head></head><label></label><figDesc>m = 200, EXACTMATCH uams05run2 m = 200, EMAILMATCH uams05run3 m = 200, 0.5 • EXACTMATCH + 0.5 • EMAILMATCH uams05run4 m = 200, 0.375 • EXACTMATCH + 0.208 • NAMEMATCH + 0.416 • EMAILMATCH,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,322.79,498.12,227.15,101.79"><head>Table 1 :</head><label>1</label><figDesc>Results for Discussion SearchIn Table</figDesc><table coords="2,322.79,498.12,227.15,56.86"><row><cell>identifier</cell><cell>MAP</cell><cell>p@10</cell><cell>p@20 p@100</cell></row><row><cell>ToNsBs350</cell><cell cols="3">0.2907 0.4441 0.4034 0.2047</cell></row><row><cell>ToNsBs350F</cell><cell cols="3">0.3518 0.5407 0.4449 0.2147</cell></row><row><cell cols="4">ToNsBs350FT 0.1947 0.3559 0.2873 0.1442</cell></row><row><cell cols="4">ToNsBs350FT5 0.1988 0.3610 0.2975 0.1480</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,59.78,68.43,224.89,79.19"><head>Table 2 :</head><label>2</label><figDesc>Results for Known-Item Finding</figDesc><table coords="4,59.78,68.43,224.89,56.46"><row><cell></cell><cell>494 75.2% 91.2%</cell><cell>8.8%</cell></row><row><cell>qdC</cell><cell>0.423 56.8% 78.4%</cell><cell>21.6%</cell></row><row><cell>qdWcEst</cell><cell>0.579 79.2% 92.0%</cell><cell>8.0%</cell></row><row><cell>OddsC</cell><cell>0.423 56.8% 78.4%</cell><cell>21.6%</cell></row><row><cell>OddsWcEst</cell><cell>0.547 56.8% 89.6%</cell><cell>10.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,53.80,234.36,239.10,252.14"><head>Table 3 :</head><label>3</label><figDesc>M 2 LAST NAME MATCH: returns true if the last name appears in the document. Note that each method M i (i = 1, 2) keeps, and improves upon, the results achieved by the preceding M i-1 . Results of Name Extraction Methods.</figDesc><table coords="5,53.80,303.47,239.10,160.29"><row><cell cols="5">Extract Candidates by Email Address. This method</cell></row><row><cell cols="5">(EMAIL MATCH) simply extracts all email addresses ap-</cell></row><row><cell cols="5">pearing in a document. Email addresses were identified us-</cell></row><row><cell cols="5">ing a regular expression. According to our experiments this</cell></row><row><cell cols="5">technique is less effective in terms of the number of identi-</cell></row><row><cell cols="5">fied candidates while the associations found by this method</cell></row><row><cell cols="5">look like stronger relationships. See Table 3 for detailed re-</cell></row><row><cell>sults.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell cols="2">#candidates</cell><cell>#assoc</cell><cell>#docs</cell></row><row><cell>EXACT MATCH</cell><cell></cell><cell cols="3">696 324,258 136,627</cell></row><row><cell>NAME MATCH</cell><cell></cell><cell cols="3">757 354,315 139,801</cell></row><row><cell cols="2">LAST NAME MATCH</cell><cell cols="3">924 945,518 212,425</cell></row><row><cell>EMAIL MATCH</cell><cell></cell><cell>456</cell><cell>73,747</cell><cell>59,355</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,316.81,97.55,239.10,165.43"><head>Table 4 :</head><label>4</label><figDesc>Results for the Expert Search task, where #rel is the number of relevant experts retrieved, and RR1 is the reciprocal rank of the first expert found.Table4gives our overall results for the Expert Search task, using the various evaluation measures proposed by the task organizers; the best score per measure is indicated in bold face.</figDesc><table coords="5,321.79,97.55,229.15,56.85"><row><cell>...run0</cell><cell>477 0.1225 0.1802 0.240 0.202 0.3942</cell></row><row><cell>...run1</cell><cell>472 0.1277 0.1811 0.222 0.200 0.4380</cell></row><row><cell>...run2</cell><cell>284 0.0918 0.1288 0.194 0.139 0.4975</cell></row><row><cell>...run3</cell><cell>479 0.1158 0.1489 0.194 0.138 0.4891</cell></row><row><cell>...run4</cell><cell>478 0.1177 0.1444 0.192 0.141 0.5062</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,331.16,692.93,224.75,6.91;1,316.81,702.39,128.38,6.91"><p>This is an estimate of the number of emails in the collection and maybe slightly inaccurate due to parsing errors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,331.16,712.12,158.02,6.91"><p>Note that "maybe replies" were treated as replies.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">640.-001.501</rs>, and <rs type="grantNumber">640.002.501</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hydGNAG">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_wq2vQHg">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_w5b4Kf5">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_8YnDRxY">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_DS88Z9Y">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_5XNpdRa">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_6cVMjGe">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_hBNeGbB">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_HBwackm">
					<idno type="grant-number">640.-001.501</idno>
				</org>
				<org type="funding" xml:id="_ay4jr5Q">
					<idno type="grant-number">640.002.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,70.40,179.67,222.50,8.64;6,70.40,191.62,222.50,8.64;6,70.40,203.58,222.50,8.64;6,70.40,215.36,222.51,8.81;6,70.40,227.32,222.50,8.58;6,70.40,239.44,221.66,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,70.40,203.58,222.50,8.64;6,70.40,215.53,72.42,8.64">The effectiveness of automatically structured queries in digital libraries</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Gonc ¸alves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krowne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Calado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,166.17,215.36,126.74,8.58;6,70.40,227.32,218.84,8.58">JCDL &apos;04: Proceedings of the 4th ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,259.37,222.50,8.64;6,70.40,271.15,222.50,8.81;6,70.40,283.11,143.09,8.81" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,107.84,259.37,185.06,8.64;6,70.40,271.32,51.60,8.64">A maximum likelihood ratio information retrieval model</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,146.47,271.15,146.43,8.58;6,70.40,283.11,113.54,8.58">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,70.40,303.20,222.50,8.64;6,70.40,315.16,222.50,8.64;6,70.40,326.94,222.50,8.81;6,70.40,338.90,222.50,8.81;6,70.40,351.02,163.17,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,168.70,303.20,124.20,8.64;6,70.40,315.16,222.50,8.64;6,70.40,327.11,24.02,8.64">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,111.58,326.94,181.32,8.58;6,70.40,338.90,174.56,8.58">ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
