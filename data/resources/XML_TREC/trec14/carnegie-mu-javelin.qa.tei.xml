<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.64,97.63,294.84,15.67">JAVELIN I and II Systems at TREC 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.80,130.42,59.62,10.99"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.29,130.42,91.88,10.99"><forename type="first">Robert</forename><surname>Frederking</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.35,130.42,87.28,10.99"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
							<email>teruko@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.57,130.42,81.48,10.99"><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
							<email>mbilotti@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,446.53,130.42,70.53,10.99"><forename type="first">Kerry</forename><surname>Hannan</surname></persName>
							<email>khannan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,70.92,144.46,96.20,10.99"><forename type="first">Laurie</forename><surname>Hiyakumoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.15,144.46,65.70,10.99"><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.77,144.46,48.83,10.99"><forename type="first">Frank</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.62,144.46,57.94,10.99"><forename type="first">Lucian</forename><surname>Lita</surname></persName>
							<email>llita@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.36,144.46,60.57,10.99"><forename type="first">Vasco</forename><surname>Pedro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,447.36,144.46,93.48,10.99"><forename type="first">Andrew</forename><surname>Schlaikjer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.64,97.63,294.84,15.67">JAVELIN I and II Systems at TREC 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCD42DC595ED681B730B6C58E1AF5603</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The JAVELIN team at Carnegie Mellon University submitted three question-answering runs for the TREC 2005 evaluation. The JAVELIN I system was used to generate a single submission to the main track, and the JAVELIN II system was used to generate two submissions to the relationship track. In the sections that follow, we separately describe each system and the submission(s) it produced, and conclude with a brief summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">JAVELIN I: Main Track Run</head><p>The JAVELIN I system integrates a set of modules that perform various question-answering tasks, such as question analysis, document and passage retrieval, answer candidate extraction, answer selection, answer merging, and planning <ref type="bibr" coords="1,95.78,410.61,14.60,9.96" target="#b15">[16]</ref>. For the TREC2005 main QA task, our goal was to incorporate a new Answer Merger module, an extended Expert Information Extractor (IX) incorporating Answer Projection, and a reimplemented Java version of the proximity-based extractor (Light IX) originally developed for the multilingual version of JAVELIN <ref type="bibr" coords="1,524.56,434.49,14.60,9.96" target="#b11">[12]</ref>. In addition, several of the existing JAVELIN modules (Question Analyzer, Retrieval Strategist, and Planner) had undergone significant re-engineering since we last participated in TREC, and TREC 2005 provided an opportunity to test them on unseen data. We also retrained the support-vector machine (SVM) and finite-state transducer (FST) extractors in an effort to improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Components used in the TREC evaluation</head><p>1. Question Analyzer (QA). The Question Analyzer's primary functions include answer type classification and keyword selection. Answer types are now classified using the output of the RASP parser <ref type="bibr" coords="1,472.46,543.09,10.00,9.96" target="#b2">[3]</ref>. Using syntactic constituent boundaries from RASP, the QA can determine which constituents are important in selecting the answer type using a set of hand-coded rules. The desired answer type often corresponds to a noun phrase in the parse tree for the question. A mapping from tokens to answer types (using WordNet <ref type="bibr" coords="1,481.93,578.97,10.45,9.96" target="#b5">[6]</ref> as a resource)</p><p>determines the hypothesized answer type for the entire question. The mapping is generated from WordNet hypernym relationships assisted by hand-coded rules.</p><p>Keyword selection depends heavily on the recognition of named entities and significant phrases that can be used to find relevant documents. Not only are possible keywords verified in the WordNet lexicon; part of speech and lemma information assist in the choice of an appropriate grammatical form, and pruning of irrelevant keywords.</p><p>2. Retrieval Strategist (RS) and Retrieval Executor (RE). This year, we have made a significant refinement of JAVELIN's retrieval architecture, factoring our existing Retrieval Strategist into a two-layer retrieval architecture. In the new architecture, the formulation of queries has been decoupled from the actual retrieval process. This separation supports number of retrieval modules, one per collection, fed by a single query formulation module, to enable federated search across distributed resources.</p><p>In place of the original Retrieval Strategist, there are now two modules. The first of these modules is a complete replacement for the query formulation functionality in the original Retrieval Strategist, and continues to bear that name. The other module is the Retrieval Executor, a thin wrapper around the Lemur IR engine<ref type="foot" coords="2,521.76,82.61,3.97,6.97" target="#foot_0">1</ref> that is responsible for executing queries and retrieving documents.</p><p>The query formulation algorithm implemented by the Retrieval Strategist is one of gradual relaxation. Given a Question Analyzer output containing a set of keywords and an expected answer type, the RS formulates an ordered sequence of queries, where the first query is the narrowest or most specific query, and each successive query is broader / less specific than the previous one. At each step in the sequence, the query is relaxed along one or more dimensions, which include the size of the window that contains the keywords and the keyword ordering constraint. Windowing constraints for keywords identified as phrases and proper names are also relaxed, according to a different schedule. The later queries in the sequence are also relaxed by dropping keywords and/or the expected answer type from the query.</p><p>The primary purpose of the Retrieval Executor is to execute a sequence of queries, starting from the narrowest one. The RE concatenates the ranked document lists returned for each query until it has collected the requested number of documents, or until it has exhausted the query sequence. The RE also supports a socket interface for direct querying of the document collection, and responds directly to requests for document or passage text, or for corpus statistics.</p><p>3. Answer Extractors. The system includes a variety of Information Extractor (IX) modules, which implement different extraction algorithms and vary in their utility across different answer types. The simplest extractor is a proximity-based extractor (Light IX) whose task is to compute a non-linear distance function between the keywords and a candidate answer. We have enhanced our support vector machine-based extractor (SVM IX) by re-training it using additional semantic and structural features and using larger datasets of training questions. The SVM IX tries to discriminate between correct answers and incorrect ones based on local semantic and syntactic context. A finite state transducer-based extractor (FST IX) was used to incorporate extraction patterns -part of which were created manually, part of which were generalized automatically -and learn their precision with respect to each answer type. We also integrated a Java-based version of the proximity-based extractor (LIGHT2 IX) originally developed as part of the multilingual JAVELIN system.</p><p>In addition to statistical extractors, we have incorporated an additional extractor (Expert IX) that combines various available resources <ref type="bibr" coords="2,198.24,413.61,15.49,9.96" target="#b9">[10]</ref> into a single high precision, low recall extractor. Currently, the Expert IX is able to find answers in resources such as various gazetteers and WordNet. From gazetteers, we extract very specific information such as structured information about planets, states and countries (e.g. state flag, diameter of Jupiter). The Expert IX also accesses WordNet <ref type="bibr" coords="2,305.16,449.49,15.49,9.96" target="#b13">[14]</ref> to perform limited reasoning based on local semantic information (hypernymy, synonymy, etc.) viewed as a simple semantic graph. The Expert IX handles factoid questions with high precision and can also handle definitional questions by extracting exact definitions and profiles from its resources. Similarly to the BBN system <ref type="bibr" coords="2,325.55,485.37,14.60,9.96" target="#b20">[21]</ref>, answers/profiles are then used to query the local corpus and find similar snippets of text. In future work, we plan to enhance the coverage of the Expert IX while maintaining its high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Answer Generator (AG)</head><p>. The Answer Generator is responsible for producing a ranked list of answers from the set of answer candidates produced by the IX modules. The AG incorporates three steps: answer normalization, answer clustering and answer filtering. Answer normalization canonicalizes the answer candidates into type-specific formats to find redundant or complementary answers. The normalized answers are grouped into clusters, given the assumption that each candidate in the cluster is independent and equally weighted. Answer validation uses gazetteers to filter out invalid answers. When no adequate answer can be found after filtering, the AG the Planner, which may select a different strategy (e.g., application of a different IX module).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Answer Merger (AM).</head><p>The Answer Merger combines the answers from multiple extraction strategies. Answer merging for question answering is comparable to the merging of ranked lists from multiple search engines into a single list, e.g. via the Metasearch algorithms <ref type="bibr" coords="2,320.77,643.89,10.00,9.96" target="#b0">[1]</ref>. We incorporated Metasearch for answer merging in JAVELIN through use of merging strategies such as combSum, combMNZ, linear combination and logistic regression.</p><p>CombSum sums the scores of the input answers and CombMNZ (Multiply-by-number-Non-Zero) multiplies the sum of the scores with the number of non-zero answers. These two methods use the scores from the input without rescaling, and do not require any training. Linear combination is a sum of the weighted scores. To decide the weights, we used each input system's performance as a weight. Logistic regression (a.k.a. maximum entropy) is a statistical regression technique to predict the probability of binary variables. It has been used to combine the documents from multiple search engines <ref type="bibr" coords="3,330.84,95.97,14.60,9.96" target="#b18">[19]</ref>. In question answering, maximum entropy has been used to combine multiple answer selection modules <ref type="bibr" coords="3,329.27,107.97,10.00,9.96" target="#b3">[4]</ref>. As the performance of answer selection modules depends on the answer type of the questions, we trained different logistic regression models for different answer types. As a logistic regression model performs better than the other approaches, logistic regression was used as the answer merging strategy in the TREC evalution.</p><p>6. Answer Projection.</p><p>The system used for the TREC evaluation incorporated answer projection, which is the task of retrieving a document from the collection that supports a given answer <ref type="bibr" coords="3,426.59,176.01,14.60,9.96" target="#b14">[15]</ref>. This process was used to find supporting documents for answers supplied by the Expert IX, which makes use of a variety of ontologies and gazetteers to provide high-precision answers for a subset of the questions. The new Retrieval Strategist treats an answer projection request as a special case of query formulation, one in which the answer is included in every query in the sequence. The relaxation schedule for the windowing constraint on the keywords (including the answer term(s)) is accelerated, broading quickly to the size of the entire document.</p><p>7. Planner. As with our previous TREC system, control of the question-answering process is provided by the JAVELIN Planner. The planning process begins after an initial analysis of the question, which is translated into a planning problem describing the initial information state (features of the current question, including its classification as either a FACTOID, LIST, or OTHER question), and an information goal defined in terms of the expected answer type.</p><p>For TREC 2005, the planning domain model was extended to include the new information extractors, an operator (action) for Answer Projection, and an Answer Merger operator which combines the results from three extractors. Planning parameter estimates for operator success likelihoods were also revised to reflect the current performance of the JAVELIN QA components on a validation subset of TREC 8-12 questions. <ref type="foot" coords="3,525.36,354.53,3.97,6.97" target="#foot_1">2</ref> A list of all operators in the TREC 2005 planning domain is presented in Table <ref type="table" coords="3,402.13,367.77,3.90,9.96" target="#tab_1">3</ref>.</p><p>Because our main focus this year was to evaluate the new Answer Merger, the Planner's SELECT ANSWER operator was implemented to give preference to any answers produced by the AM over the answers from a single IX. The results of a single IX are used only if the AM fails (i.e., fewer than three extractors produced candidates).</p><p>In addition to selecting the source of the final answer, the Planner was also responsible for producing the different answer formats required by the three question categories (FACTOID, LIST, OTHER) comprising the QA task. This was accomplished with a set of very simple heuristics based on the confidence scores of the ranked answers. For all FACTOID questions, the top answer was returned. In the case of LIST and OTHER questions, the Planner displayed all answers greater than or equal to a confidence threshold c = (0.5 * top answer confidence), unless this threshold resulted in just a single answer being returned. In such cases, the threshold was lowered to c = (0.5 * rank 2 answer confidence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TREC Main QA Track Results</head><p>A single TREC QA run was submitted for the QA track main task and the document set retrieved from the main task was submitted for the document ranking task. In the main QA task, the JAVELIN system achieved an average F score of 0.169 for the factoid questions. JAVELIN obtained 0.059 on the list questions and 0.015 on other questions.</p><p>In the document ranking task, the average precision for all relevant documents was 0.1584. However, the system achieves its highest precision with a cutoff of 15 documents (23.87% precision and almost 28% recall).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Analysis</head><p>In the interim between performing the TREC QA evaluation and receiving our official scores, we conducted an internal performance analysis for a subset of the TREC 2005 question set. Since we focused on improving extraction quality for temporal and numeric questions, we did a detailed analysis of those answer types. Project members manually identified correct answers for temporal and numeric questions, along with at least one document containing the answer. We then compared our manually generated answer key with the system's output to determine whether the system returned the correct answer. If not, we identified the point of failure. Table <ref type="table" coords="4,448.59,72.09,4.98,9.96">4</ref> summarizes the results of this analysis. For each failure point, we computed the number of questions that resulted in errorneous output.</p><p>The results show that roughly equal percentages of the failure occurred during document retrieval, answer candidate extraction and answer selection. The performance of JAVELIN was compared with the previous JAVELIN TREC run <ref type="bibr" coords="4,447.01,122.49,58.19,9.96">(TREC 2003)</ref>. As can be seen in Table <ref type="table" coords="4,114.72,134.49,3.90,9.96">5</ref>, this year's system improved performance on factoid questions by 30.8%.</p><p>To evaluate the performance of the Question Analyzer, we generated an answer type classification confusion matrix. Figure <ref type="figure" coords="4,121.21,161.01,4.98,9.96" target="#fig_0">1</ref> shows the answer type classification confusion matrix for the TREC 2005 evaluation. The accuracy of answer type classification for the TREC run we submitted was 0.553. On the training data (TREC8-12), the accuracy of answer type classification was 0.763.</p><p>We also measured the performance of each component for the training questions in TREC8-12. Table <ref type="table" coords="4,523.93,199.53,4.98,9.96">6</ref> shows the performance of five extractors. The outputs from the Question Analyzer and Retrieval Strategist were reused to test multiple IXes. Macro-average assigns an equal weight to each category, regardless of how rare or common a category is. So macro average considers all categories as "equal". Micro-average assigns an equal weight to each question/document (more generic category instance), favoring performance on larger categories <ref type="bibr" coords="4,468.61,247.29,14.60,9.96" target="#b21">[22]</ref>. "Any" provides the percentage of questions with at least one good (matching key) document (in RS) and answer (in IX) in the set.</p><p>These results were used to train the Answer Merger. To decide a merging strategy for the TREC 2005 evaluation, the combination of four extraction strategies (FST, LIGHT, SVM and Expert IX) was tested with different merging techniques. The results show that the logistic regression model outperformed other merging methods and improved the system performance by 13.3% over the best stand-alone answer strategy and by 7.8% over a linear combination model. On the other hand, combSum and combMNZ did not improve the performance at all. 2.4 Post-TREC Extensions 1. Answer Merger (AM). After the TREC evaluation, we did further investigation on the performance of the Answer Merger when using a logistic regression model. As we have five answer extractors, we generated 26 combinations of the extractor outputs and tested the AM with each combination. The results show that the AM produced the best score when combining only four extractors (Expert, FST, Light and Light2 IX). It improved the answer merging performance by 26% over the best stand-alone result (Light IX). However when combining all of the extractors, the performance improved only by 21.1%. Even though it performs relatively better than the other extractors, the SVM IX did not improve the performance of answer merging. This is mostly because the SVM IX does not produce very reliable confidence scores for certain answer types.</p><formula xml:id="formula_0" coords="4,88.21,392.45,437.59,289.97">0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 REX 0 13 7 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 3 ORG 0 0 46 0 2 0 0 0 0 0 1 0 0 0 0 0 0 1 4 PER 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 REL 0 2 4 0 2 0 0 0 0 0 3 0 0 1 0 0 0 0 9 PROP 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 LIST 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 CC 0 0 0 0 0 0 0 62 0 0 0 0 0 0 0 0 0 0 1 TMP 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 LEX 0 0 2 0 0 0 0 0 0 13 0 0 0 0 0 0 0 1 1 BIO 0 1 1 0 0 0 0 0 0 0 47 0 0 1 0 0 0 0 1 LOC 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 4 CA 0 1 0 0 0 0 0 1 0 0 1 0 2 66 0 0 0 1 3 NUM 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 4 TIT 0 2 15 0 0 0 0 0 0 0 14 0 0 0 3 0 0 0 50 LST 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ACT 0 3 0 0 1 0 0 0 0 0 3 0 0 2 0 0 0 21 27 DEF 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 4 16 OBJ R E X O R G P E R R E L PROP LIST C C T M P L E X B I O L O C C A P R O N U M T I T L S T A C T DE F O B J G O L D S T A N D A R D System Classifications</formula><p>To decide whether we should just exclude the SVM in answer merging, we compared the performance of the AM for each answer type. This comparison shows that adding the SVM IX improved the performance of numericexpression and temporal questions, but did not improve the performance of location, person-name and object questions. This information can be useful to a QA system when making strategy decisions, such as selecting the combination of answer lists to maximize performance for the current answer type. For example, when processing numeric-expression and date questions, the Planner calls all five extractors. On the other hand, when processing location and person-name questions, the Planner calls only four extrators. This indicates that multi-strategy answer merging is important in multi-strategy QA systems. For more detailed analysis, see <ref type="bibr" coords="5,544.66,262.77,10.00,9.96" target="#b8">[9]</ref>.</p><p>We plan to do more experiments by incorporating regularization. Si and Callan (2005) have recently shown that regularized logistic regression improves merging for multilingual document lists, and we intend to investigate its application to answer merging in QA. In addition, we will extend the AM to support the multilingual JAVELIN QA system.</p><p>2. Answer Generator. As some of the extractors return too many answer candidates, answer selection has been a challenge, and it is generally difficult to identify the correct answer amongst many incorrect ones. To improve answer selection accuracy, we have combined evidence provided from three semantic resources: excerpts from Web documents, WordNet, and several gazetteers including the CIA World Factbook.</p><p>For gazetteers we assigned the following confidence score for each answer candidate: 1.0 if gazetteers can identify the answer, 0.5 if the answer occurs in the gazetteer within the subcategory of the expected answer type (e.g., if the candidate "Shanghai" is a city, given the question "Which city in China has the largest number of foreign financial companies?"), 0.0 otherwise. The same approach was used for WordNet. A Web score is computed based on the heuristic approach presented by <ref type="bibr" coords="5,331.71,436.05,97.69,9.96">(Magnini et al., 2002)</ref> to analyze the text snippets returned by Google.</p><p>As a preliminary experiment, we tested location and proper-name questions in TREC8-12 with combSum, linear combination, and logistic regression. The results show that linear combination was the best for the location questions. This improved the performance by 15.93%. For proper-name questions, logistic regression produced the best score and improved answer selection performance by 75.93% <ref type="bibr" coords="5,385.67,500.37,9.91,9.96" target="#b8">[9]</ref>. We are adding more resources such as Wikipedia and the EIJIRO dictionary to improved our answer selection performance and to support the multilingual JAVELIN system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Future work</head><p>Time constraints limited our ability to fully exploit the new functionality provided by the new Retrieval Strategist and Answer Merger for TREC. In particular, the RS2 supports a filtering option that can be used to perform successive document retrieval, and the Answer Merger supports N -way merges, not just the 3-way merge used by the planner. Our expanded planning domain includes both a filtering operator and additional operators for N -way merges, and we continue to work on better tuning of planning parameters for all operators. We also continue to test variations of the AM that consider answer type when selecting the weights and merge strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JAVELIN II: Relationship Track Runs</head><p>The JAVELIN II system design is composed of a collection of server modules which perform various high-level tasks in a typical question answering system pipeline. A Question Analysis module is provided input question text along with any contextual material and produces an analysis of the user's information need. This analysis is then used by an Information Retrieval engine, the Retrieval Strategist and Retrieval Executor, to produce a short list of ranked documents from a corpus of trusted material. An Information Extraction module takes the ranked list of documents, along with the initial question analysis output, and extracts from the document set more specific information related to the user's information need. Finally, an Answer Generation module filters the prioritized output from the Information Extraction module and synthesizes a final response to the user's query. More detail on system architecture can be found in <ref type="bibr" coords="6,213.61,119.85,14.60,9.96" target="#b16">[17]</ref>.</p><p>The system architecture exposes precise module interface specifications, allowing various implementation strategies to be adopted on a per-module basis. This flexibility in module implementation also affords straight-forward adaptation of external software packages for use in the system. Of central concern to the JAVELIN II system is a uniform mechanism for the integration of data from multiple sources, which is achieved via the Annotations Database and Text Annotator frameworks<ref type="foot" coords="6,195.00,178.49,3.97,6.97" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">JAVELIN II Components in TREC 2005</head><p>3.1.1 Question Analyzer JAVELIN II's Question Analyzer module has been substantially modified from its initial design. In earlier versions of this module, the goal of question analysis was the identification of question and answer types of various specificity, along with a set of relevant keywords for an information retrieval stage of question processing. In addition to this process, the JAVELIN II question analysis module identifies target semantic predicates expected to be found in relevant answer texts. These target semantic predicates are enriched with entity type information and weighted alternate predicate hypotheses, generating a diverse set of relational matching criteria. These target semantic structures are then employed in later information filtering and ranking steps to arrive at answer candidates.</p><p>To produce a set of target semantic structures, a collection of specialized tools first create various layers of annotation on input question text. These layers of annotation are then merged to form a set of semantic structures, weighted based on the specificity of compositional annotation present in a given structure. Among the tools used to create annotations are Colorado University's ASSERT semantic parser <ref type="bibr" coords="6,366.72,365.97,14.60,9.96" target="#b17">[18]</ref>, which users a statistical model derived from Propbank <ref type="bibr" coords="6,126.51,377.97,15.49,9.96" target="#b10">[11]</ref> data; the BBN IdentiFinder named-entity recognizer <ref type="bibr" coords="6,389.39,377.97,9.91,9.96" target="#b1">[2]</ref>; and ontological resources such as Princeton University's WordNet lexical database <ref type="bibr" coords="6,266.66,389.97,10.57,9.96" target="#b5">[6]</ref> and the CNS Ontology <ref type="bibr" coords="6,380.88,389.97,10.00,9.96" target="#b6">[7]</ref>. All annotations generated from these tools are collected in a single, unified Annotations Database, accessible from all other JAVELIN system modules, affording great flexibility to the weighting and prioritization of annotations during the creation of target semantic structures.</p><p>Overall semantic structure weights, along with more fine-grained weights applied to components of target semantic structures, influence information retrieval query formulation and backoff, as well as information extraction filtering and ranking procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Retrieval Strategist and Retrieval Executor</head><p>These modules were reused from the JAVELIN I system described above. A more advanced set of information retrieval modules has been developed for future use in JAVELIN II, but because of the lack of full annotation coverage of the TREC evaluation corpus, we were unable to use these for our relationship track submissions. The more advanced modules use the semantic structures output by the Question Analysis module to form very specific queries against structural metadata indexed along with the original text of annotated corpora. Furthermore, the more advanced Retrieval Strategist module includes query backoff routines which action the priorities of the components of semantic structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Semantic Information Extractor (SemIX)</head><p>Once a document set has been selected by the Retrieval Strategist, candidate answer sentences are extracted and ranked from these documents by the Semantic Information Extractor (SemIX). This process mirrors that of question analysis, where target semantic structures are created from various layers of annotation on input text. In the SemIX, the text of documents reported by the Retrieval Strategist is annotated, and semantic structures are generated similarly to the process described in section 3.1.1. The annotation of document text is done online, during query processing, only if those documents have not already been annotated previously during offline corpus annotation runs. Significant computational resources are required to fully annotate the TREC corpus with our suite of tools, and not all documents were fully processed by the time the TREC Relationship QA track had begun. Because of the small number of Relationship track questions, selective online annnotation of corpus materials did not pose a significant problem. However, we were not able to apply more advanced information retrieval techniques in our TREC system because of the partially annotated state of the test corpus.</p><p>After completion of document set annotation, structural patterns are generated from the target semantic structures output by the Question Analyzer. These structural patterns are applied to all semantic structures created from the document set in a ranking procedure, where those semantic structures not meeting a minimal match threshold are removed from further consideration. For a single semantic structure in corpus materials, the ranking procedure combines evidence from all target semantic structures. Matching semantic structures are then collated with their containing sentences, and per-structure scores are combined to give sentence-level scores and a global relative ordering of candidate answer sentences. The ranked list of output sentences can be tens or hundreds of sentences long, depending on the amount of source material which matches some portion of the target semantic structures, and duplicate information is likely at this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Answer Generator (AG)</head><p>The Answer Generator module is responsible for producing the set of nuggets that answer each question from the set of answer candidates produced by the SemIX. It does this by eliminating duplicates from the nuggets produced by the SemIX and returning the set of unique nuggets. The cut-off point was set at 15, and in all cases the cut off point was reached (15 nuggets were returned).</p><p>The identification of unique nuggets is done by taking each pair of nuggets and calculating the average between the Levenstein distance and Cosine Similarity. The Levenstein distance is a variation of the Edit distance and the purpose is to measure the difference between string at the character level, while the Cosine Similairity measure will focus on the string difference at the word level by using vector comparison. The current AG is an attempt to filter the nuggets produced by the SemIX and assure a maximum number of unique nuggets.</p><p>The other role of the AG is to format the results to conform to the TREC output specifications. The AG creates a file to which appends the nuggets in the correct form, using the Request Object to retrieve the question ID and the Document ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Performance and Analysis</head><p>The JAVELIN II system was used to submit two runs for the relationship track. The first, fully-automatic run of the system failed to produce answers for six of the relationship questions, due to failure at the Question Analyzer stage. In JAVELIN II, it is essential that the question analysis process produce a semantic structure (key predicate(s) for the input; this is achieved using the ASSERT tool. In some cases, however ASSERT did not produce an output for a question. To address this shortcoming, a team member familiar with ASSERT's output manually labeled the semantic structures in all 25 of the test questions. The second, semi-automatic run submitted used this gold-standard question analysis as input to the rest of the JAVELIN II pipeline. The system was able to produce an answer for each of the questions in the second run.</p><p>Table <ref type="table" coords="7,96.24,533.25,4.98,9.96">7</ref> shows the performance of the two JAVELIN II runs with respect to that of the other track participants. The fully-automatic run, denoted by the run tag CMUJAVSEM, ranked eleventh out of eleven runs in terms of F(3) measure, and also in terms of precision and recall, each considered separately. The semi-automatic run, with the run tag CMUJAVSEMMAN, ranked ninth out of eleven runs in terms of both F(3) measure and precision. In terms of recall, the semi-automatic run placed tenth because the runs RUN-9 and RUN-3 tied for eighth place.</p><p>Given that CMUJAVSEMMAN was only 0.006 behind RUN-9 and RUN-3 in terms of recall, it is likely that the true recall of the three systems is actually quite similar. The difference lies in precision. RUN-9, with RUN-3 not too far behind, was the leader in terms of precision. It was these two runs that returned relatively concise lists of nuggets for the questions, and the two of them outscored by a broad margin all of the other runs. Because these two systems were able to detect and refrain from returning non-relevant nuggets, even though they had performance similar to CMUJAVSEMMAN in terms of recall, they ranked higher than it in terms of F(3) measure.</p><p>RUN-7 is an interesting outlier that ranked 10th in terms of both F(3) measure and recall, right between CMUJAVSEMMAN and CMUJAVSEM. Unlike RUN-9 and RUN-3, RUN-7 was characterized by returning enormous nugget lists for certain questions. One extreme example from the official assessment of RUN-7 is that credit was given on question 17 for matching a nugget on the answer key with their 11,119-th ranked nugget. RUN-7 had excellent recall, scoring second, but on several occasions, matching nuggets were so far down on the ranked list, that very few users would ever be patient enough to find them. The precision score of RUN-7 suffered as a result, yet, interestingly CMUJAVSEM had a lower precision score than RUN-7, even though JAVELIN II returns a maximum of fifteen nuggets per sentence.</p><p>In the official evaluation, assessors were asked to read each system's ranked list of nuggets, and to match them manually against the questions' answer keys. Assessors are asked to use their understanding of natural langage to make positive matches between system responses and answer key nuggets without penalizing for different word usage or syntactic structure, or for paraphrase and rephrasing of the answer key nugget <ref type="bibr" coords="8,412.32,132.45,14.70,9.96" target="#b19">[20]</ref>.</p><p>We recognize that this may be a difficult or tedious task for humans to perform, so we repeated it with an eye toward checking if any mistakes were made unifying the nuggets our system returned with those on the answer key. Although there were cases where we failed to understand how the assessors decided to draw the distinction between vital and okay nuggets, we did not attempt to second-guess these decisions when we repeated the evaluation. See <ref type="bibr" coords="8,547.42,180.81,10.57,9.96" target="#b7">[8]</ref> for some discussion of how the vital/okay distinction can affect evaluation.</p><p>While performing our evaluation, we identified 7 vital and 8 okay nuggets in our CMUJAVSEM run that the assessors did not give credit for. In CMUJAVSEMMAN, we found 3 vital, and 3 okay nuggets that the assessors did not give credit for. We developed a new set of judgments based on these changes, and, though not official, system performance evaluated against this judgment set gives an impression of an upper bound on our scores. With the new judgments, CMUJAVSEM scored 0.058 in precision, 01.62 in recall, and 0.131 in F(3) measure. CMUJAVSEMMAN scored 0.058 in precision, 0.163 in recall and 0.129 in F(3) measure. We noted that, under the new judgments, CMUJAVSEM would have ranked 7th and CMUJAVSEMMAN would have ranked 8th in terms of F(3) measure, followed by RUN-9, RUN-3 and RUN-7. CMUJAVSEM and CMUJAVSEMMAN would have tied for 9th in precision, with 11th place going to RUN-7. CMUJAVSEMMAN would have ranked 8th and CMUJAVSEM would have ranked 9th in recall, followed by RUN-9 and RUN-3 tied for 10th.</p><p>By analyzing the results of the CMUJAVSEMMAN run, we can draw some general conclusions about JAVELIN II's performance. There were several questions where our nuggets were on-topic, but simply not specific enough to match the nuggets in the answer key. Examples include: Q5, where our system returned nuggets relevant to the Chechnya issue, but didn't identify groups that supported the rebels; Q10, where JAVELIN II got nuggets about Ecuador, but not about drug interdiction efforts; Q15, where the system discusses China and Taiwan in a military sense, but fails to address whether pressure from Beijing has affected the sale of armaments to Taipei; and Q17, where JAVELIN II talks about Israel and China with respect to Middle East peace, but fails to discuss arms trading.</p><p>One specific area where JAVELIN II fell short of other systems was in the interpretation of constraints present in the question. Specifically, the higher-scoring systems properly retrieved a list of countries when asked, but this functionality was never explicitly included in JAVELIN II. A prime example where this shortcoming hurt JAVELIN II's performance was Q22, where our system should have returned a list of countries. Q23 is an even better example; not only did the system fail to extract country names, but it also failed to restrict the list of countries to those in South America, as the question asked. In response to a request for countries seeking nuclear capability, JAVELIN II named Syria, Libya, Iran and North Korea, which would have been reasonable answers, except that the system failed to note the constraint that only South American countries should be considered.</p><p>A general observation about performance across the entire track was that there were some questions that were just generally difficult to answer. For questions 2 and 20, only one of the systems got an answer, and for questions 2, 3, 7, 17, 20, 23 and 24, fewer than half of the systems were able to come up with an answer. That said, the two JAVELIN II runs featured the most questions for which no answer key nuggets were matched. Followed by RUN-3 and RUN-9, which each missed more than 10 questions, all other runs were able to get credit for more than fifteen questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating Relationship QA as a Ranked List of Nuggets</head><p>Prior to the release of the official Relationship QA track results, we performed an in-house evaluation. Given that JAVELIN II focused on ranking of relevant nuggets retrieved from the corpus, it seemed natural to evaluate our ranked nugget lists directly. To perform any sort of evaluation on our nuggets, we were going to need relevance judgments.</p><p>Three team members were asked to read the ranked lists from both system runs and to assign binary relevance judgments to each nugget based on their own notion of information need and relevance. No guidelines were used, and no attempt was made to maximize inter-coder agreement. Instead, we wanted to obtain samples of how the nuggets compared with three different information needs. Using these binary judgments, we were able to assign a score to each nugget, in the range from zero to three, corresponding to the number of different information needs  <ref type="table" coords="9,97.92,282.09,4.98,9.96" target="#tab_0">1</ref> shows how this score can be used to analyze the distributions of nuggets in each of the two runs. The semi-automatic run, using the gold-standard question analysis, retrieved almost 5% more Score 3 nuggets and approximately the same number of Score 2 nuggets, when compared with the fully-automatic run.</p><p>In Table <ref type="table" coords="9,109.20,318.09,3.90,9.96">2</ref>, we use a simple MRR metric that averages the reciprocal rank of the first nugget with a score of at least S across all of the questions. There were no nuggets retrieved for each of the six questions <ref type="bibr" coords="9,467.40,329.97,11.65,9.96" target="#b7">(8,</ref><ref type="bibr" coords="9,482.04,329.97,12.85,9.96" target="#b13">14,</ref><ref type="bibr" coords="9,497.77,329.97,12.85,9.96" target="#b18">19,</ref><ref type="bibr" coords="9,513.37,329.97,12.85,9.96" target="#b20">21,</ref><ref type="bibr" coords="9,529.09,329.97,10.02,9.96">24</ref> and 25) that the automatic question analysis was not able to process, so in the second from leftmost column in the table, the average is taken over 19 questions. In the leftmost column, the average is taken over all 25 questions, where only score zero nuggets were assumed to have been retrieved for these six questions. The far right column in the table shows a score calculated for the semi-automatic run over only those 19 questions that the fully-automatic run was able to process. From top to bottom in the table, the minimum score of the nugget (S) increases, so that in the top row, the first nugget with S ≥ 1 is chosen for the purposes of computing the score, and on the bottom row, the first nugget with S ≥ 3 is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Future Work</head><p>Planned extensions to existing JAVELIN II modules address a number of shortcomings of the modules used for this year's relationship track, as well as extensions which address problems in parallel domains, such as scenario-based question answering.</p><p>Because question analysis requires high recall from semantic parsing of input question text to recover even partial predicate structures, efforts will be made to develop a more robust semantic parsing tool, capable of greater coverage and higher throughput than the semantic parsing tool currently used. Not only will higher coverage of question text be beneficial, but application of a more robust semantic parser to corpus materials is also expected to greatly increase system performance.</p><p>More ontological data will also be incorporated into JAVELIN II's semantic structures with the completion of an interface to the Scone ontology <ref type="bibr" coords="9,193.21,568.65,9.91,9.96" target="#b4">[5]</ref>. This resource will provide JAVELIN with greater hierarchical type information on entities found in text, as well as allow semantic structures generated from question text to be enriched with alternate and parallel semantic relations, conditioned on the question domain.</p><p>Anaphora resolution technology will be applied both to input question text as well as corpus materials in order to improve accuracy of semantic structure creation and matching.</p><p>Our information retrieval engine will move from Lemur to Indri <ref type="bibr" coords="9,349.68,628.77,14.60,9.96" target="#b12">[13]</ref>, the next generation search engine from the Lemur project, which will allow more precise structured search of annotated corpus materials. This we expect to filter out many false positives from Retrieval Strategist results, allowing the Information Extractor to find and report higher-scoring semantic structures.</p><p>To refine the bounds on selected text returned by the Information Extractor from a whole sentence, to a specifc phrase or term, the semantic structure will evolve to include terms representing unbound variables. During information extraction, these unbound variables will be bound to specific elements of matching semantic structures, and be reported along with their containing sentences.</p><p>Improvements to semantic structure scoring metrics will also be investigated, utilizing link analysis techniques on corpus semantic structures. Through greater understanding of various corpus statistics for semantic structures, better weighting of the importance of relation and entity types, as well as specific instances of relations and entities present in corpus materials, may be possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>There are a few general observations which can be drawn from the performance of the JAVELIN I and II systems in the TREC 2005 evaluations. First, it seems evident that the performance of the statistical-based extractors and the pattern-based question analysis used in JAVELIN I are weak points for that system. Both of these components show poorer performance on unseen data vs. training data, and the accuracy of document and answer retrieval is impacted accordingly. Even when specialized statistical extractors are developed for different answer types, it seems that much larger amounts of training data would be required to realize an upper bound on extractor performance. Although resource-based extractors (e.g. Expert IX) can provide high-precision answers, in practice there were few TREC questions that were answered by JAVELIN I topic-specific resources, which placed greater emphasis on statistical extractors. Inclusion of a large and diverse number of topic-specific resources is required to realizegains from resource-based extractors in TREC-style evaluations.</p><p>The first run of the JAVELIN II system, with its more semantics-based approach, indicates that more work is required for effective question analysis, in particular to identify key semantic structures which should be sought in the target corpus. Another failure point for the JAVELIN II system was the lack of semantics-based indexing and retrieval, which we have already taken steps to remedy since the TREC evaluation. Even if a corpus has been annotated with semantic information, if that information is not indexed for retrieval at run-time the system must resort to standard keyword-based retrieval followed by semantic analysis of what can be gleaned from the document set. We expect that our TREC 2005 relationship track results will provide a useful baseline for testing the effectiveness of semantic indexing and retrieval, as well as the use of additional general and domain-specific ontological information for answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operator Name Description</head><p>RETRIEVE DOCUMENTS Calls the RS. Applicable when there is an active question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXTRACT CANDIDATES</head><p>Calls one of the four conventional information extractors (SVM, FST, LIGHT, LIGHT2). Applicable when there is an active question and a document set. Outcome likelihoods are generated dynamically, conditioned on the type of extractor and expected answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONSULT KNOWLEDGE BASE</head><p>Calls the EXPERT extractor. Applicable when there is an active question to answer. Outcome likelihoods are generated dynamically, conditioned on the expected answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RANK KB CANDIDATES</head><p>Calls the AG to rank candidates produced by the EXPERT extractor. Applicable when the EXPERT extractor has produced candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RANK IX CANDIDATES</head><p>Calls the AG to rank candidates produced by one of the four conventional extractors. Applicable when such candidates exist. Outcome likelihoods are conditioned on the expected answer type and extractor which generated the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROJECT ANSWER</head><p>Calls the RS to retrieve documents that support the EXPERT candidates. Applicable when the final answer selected comes from the EXPERT IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MERGE ANSWERS3</head><p>Calls the AM to merge candidate sets from three extractors. Applicable whenever at least three extractors have produced candidates (SVM, FST, LIGHT, LIGHT2, EXPERT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SELECT ANSWER</head><p>Chooses an answer to display, giving preference to ranked answer lists produced by the AM over ranked answers from a single source. Applicable when at least one ranked answer list exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISPLAY ANSWER</head><p>Completes the planning session by returning the selected answer to the GUI. Applicable once an answer is selected. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,158.16,710.01,295.95,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Answer type classification confusion matrix (TREC 2005)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,54.00,67.05,432.36,225.00"><head>Table 1 :</head><label>1</label><figDesc>Distribution of Nuggets by Score</figDesc><table coords="9,54.00,88.65,432.36,203.40"><row><cell>Run</cell><cell>Score 0</cell><cell cols="2">Score 1</cell><cell>Score 2</cell><cell>Score 3</cell><cell>Total</cell></row><row><cell cols="7">fully-automatic 193 (67.72%) 46 (16.14%) 33 (11.58%) 13 ( 4.56%) 285</cell></row><row><cell cols="7">semi-automatic 247 (65.87%) 50 (13.33%) 43 (11.47%) 35 ( 9.33%) 375</cell></row><row><cell>overall</cell><cell cols="6">440 (66.67%) 96 (14.55%) 76 (11.52%) 48 ( 7.27%) 660</cell></row><row><cell></cell><cell cols="5">Table 2: MRR Comparison of System Runs</cell></row><row><cell></cell><cell cols="2">Fully-Automatic</cell><cell></cell><cell cols="2">Semi-Automatic</cell></row><row><cell></cell><cell cols="2">0.4457 0.5338</cell><cell cols="2">S ≥ 1 0.7040 0.7509</cell><cell></cell></row><row><cell></cell><cell cols="2">0.3860 0.4553</cell><cell cols="2">S ≥ 2 0.3760 0.4316</cell><cell></cell></row><row><cell></cell><cell cols="2">0.1538 0.2023</cell><cell cols="2">S ≥ 3 0.2733 0.2965</cell><cell></cell></row><row><cell>that judged the nugget relevant.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,177.96,687.45,256.24,9.96"><head>Table 3 :</head><label>3</label><figDesc>TREC 2005 planning domain operators (actions).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,69.24,714.02,130.90,7.97"><p>See: http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,69.24,704.54,488.66,7.97;3,54.00,714.02,211.56,7.97"><p>We implemented additional operators to merge the results from varying numbers of etractors, but due to time constraints, these operators were not enabled during the TREC evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,69.24,704.54,488.62,7.97;6,54.00,714.47,226.92,7.34"><p>Details on these frameworks is outside the scope of this paper, but information and resources are available at http://durazno.lti.cs.cmu.edu/javelin public/releases/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure point</head><p>No. of questions Question Percentage a. system failed to retrieve any documents containing the answer 26 23.0% b. system failed to extract any correct answer candidates 30 26.5% c. system failed to select the correct answer candidate   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,74.52,81.93,408.72,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,243.27,81.93,96.36,9.96">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,360.96,81.93,90.83,9.96">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,100.17,483.25,9.96;12,74.52,112.17,140.29,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,317.19,100.17,191.34,9.96">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,521.03,100.17,36.75,9.96;12,74.52,112.17,36.42,9.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,130.41,483.41,9.96;12,74.52,142.41,374.17,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,193.82,130.41,234.77,9.96">Robust accurate statistical annotation of general text</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,451.19,130.41,106.74,9.96;12,74.52,142.41,281.96,9.96">Proceedings of the Third International Conference on Language Resources and Evaluation</title>
		<meeting>the Third International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,160.65,483.76,9.96;12,74.52,172.65,281.77,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,74.52,172.65,202.76,9.96">Multiple-engine question answering in textmap</title>
		<author>
			<persName coords=""><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,299.04,172.65,24.78,9.96">TREC</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,190.89,197.76,9.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
		<title level="m" coord="12,158.28,190.89,82.69,9.96">Scone User Manual</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,209.13,252.48,9.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,121.69,209.13,173.64,9.96">Wordnet -an electronic lexical database</title>
		<author>
			<persName coords=""><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,227.37,285.73,9.96" xml:id="b6">
	<monogr>
		<title level="m" coord="12,74.52,227.37,254.25,9.96">The Center for Nonproliferation Studies. The cns ontology</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,245.73,483.61,9.96;12,74.52,257.61,229.09,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,307.47,245.73,250.66,9.96;12,74.52,257.61,29.82,9.96">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,125.28,257.61,125.40,9.96">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,275.97,483.47,9.96;12,74.52,287.85,156.97,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,285.75,275.97,255.04,9.96">Exploiting multiple semantic resources for answer selection</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Hiyakumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,74.52,287.85,103.97,9.96">Proceedings of of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,306.21,483.64,9.96;12,74.52,318.09,483.69,9.96;12,74.52,330.09,90.13,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,245.08,318.09,313.13,9.96;12,74.52,330.09,12.07,9.96">Integrating web-based and corpus-based techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,107.40,330.09,24.78,9.96">TREC</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,348.33,482.31,9.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,308.11,348.33,216.94,9.96">Adding semantic annotation to the penn treebank</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,366.57,483.51,9.96;12,74.52,378.57,207.01,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,377.52,366.57,159.36,9.96">Cmu javelin system for ntcir5 clqa1</title>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideki</forename><surname>Shima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,74.52,378.57,175.53,9.96">Proceedings of the 5th NTCIR Workshop</title>
		<meeting>the 5th NTCIR Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,396.81,483.44,9.96;12,74.52,408.81,483.78,9.96;12,74.52,420.69,43.93,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,197.66,396.81,328.30,9.96">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,532.67,396.81,25.29,9.96;12,74.52,408.81,430.74,9.96">Information Processing and Management Special Issue on Bayesian Networks and Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,439.05,362.16,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,134.90,439.05,171.86,9.96">Wordnet: A lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,315.24,439.05,26.65,9.96">CACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.52,457.29,483.42,9.96;12,74.52,469.29,173.29,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,236.92,457.29,173.55,9.96">Query formulation for answer projection</title>
		<author>
			<persName coords=""><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,430.67,457.29,127.28,9.96;12,74.52,469.29,141.99,9.96">27th European Conference on Information Retrieval (ECIR&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,487.53,483.63,9.96;12,74.52,499.53,483.50,9.96;12,74.52,511.41,483.24,9.96;12,74.52,523.41,22.93,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,540.88,499.53,17.15,9.96;12,74.52,511.41,422.16,9.96">The javelin question-answering system at trec 2003: A multi-strategy approach with dynamic planning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Frederking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hiyakumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huttenhower</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Judy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kupsc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Lita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,516.72,511.41,35.19,9.96">TREC12</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,541.65,483.53,9.96;12,74.52,553.65,483.37,9.96;12,74.52,565.53,314.44,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,146.42,553.65,245.47,9.96">Extending the javelin qa system with domain semantics</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Frederking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasco</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Schlaikjer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kerry</forename><surname>Hannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,415.20,553.65,142.69,9.96;12,74.52,565.53,155.96,9.96">Proceedings of the 20th National Conference on Artificial Intelligence</title>
		<meeting>the 20th National Conference on Artificial Intelligence<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,583.89,483.62,9.96;12,74.52,595.77,197.55,9.96" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="12,481.96,583.89,76.19,9.96;12,74.52,595.77,165.59,9.96">Shallow semantic parsing using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Wayne</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kadri</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">H</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,614.13,483.62,9.96;12,74.52,626.01,102.73,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,298.37,614.13,259.78,9.96;12,74.52,626.01,24.50,9.96">Report on the trec-5 experiment: Data fusion and collection fusion</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><forename type="middle">Le</forename><surname>Calv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dana</forename><surname>Vrajitoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,119.88,626.01,24.78,9.96">TREC</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,644.25,476.64,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,161.66,644.25,222.49,9.96">Overview of the trec 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,405.00,644.25,115.18,9.96">Proceedings of TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,662.61,483.68,9.96;12,74.52,674.49,169.09,9.96" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,307.62,662.61,250.59,9.96;12,74.52,674.49,90.76,9.96">Evaluation of an extraction-based approach to answering definitional questions</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Licuanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,186.36,674.49,25.91,9.96">SIGIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,74.53,692.85,483.53,9.96;12,74.52,704.73,381.97,9.96" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,167.07,692.85,209.11,9.96">A re-examination of text categorization methods</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,397.43,692.85,160.62,9.96;12,74.52,704.73,351.43,9.96">Proceedings of SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval</title>
		<meeting>SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
