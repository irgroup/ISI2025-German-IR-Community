<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.98,76.37,308.00,13.97;1,190.02,94.79,231.90,13.97">Structural Term Extraction for Expansion of Template-based Genomic Queries</title>
				<funder ref="#_sm4xxTC">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
				<funder ref="#_QqJdAjg">
					<orgName type="full">Enterprise Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,85.50,131.33,78.31,10.46"><forename type="first">Fabrice</forename><surname>Camous</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.80,131.33,65.73,10.46"><forename type="first">Stephen</forename><surname>Blott</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.49,131.33,66.25,10.46"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.78,131.33,85.67,10.46"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,435.47,131.33,81.01,10.46"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<addrLine>Dublin 9</addrLine>
									<settlement>Glasnevin</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Adaptive Information Cluster</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">IRELAND</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.98,76.37,308.00,13.97;1,190.02,94.79,231.90,13.97">Structural Term Extraction for Expansion of Template-based Genomic Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FC5BCC9576BD18C3B94EC0D23334A25A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our experiments run to address the ad hoc task of the TREC 2005 Genomics track. The task topics were expressed with 5 different structures called Generic Topic Templates (GTTs). We hypothesized the presence of GTTspecific structural terms in the free-text fields of documents relevant to a topic instantiated from that same GTT. Our experiments aimed at extracting and selecting candidate structural terms for each GTT. Selected terms were used to expand initial queries and the quality of the term selection was measured by the impact of the expansion on initial search results. The evaluation used the task training topics and the associated relevance information. This paper describes the two term extraction methods used in the experiments and the resulting two runs sent to NIST for evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper describes our experiments run to address the ad hoc task of the TREC 2005 Genomics track. The collection used for the task included a subset of the MEDLINE database, 50 test topics, and 10 additional sample topics with associated partial relevance information. The topics were expressed with 5 different structures called Generic Topic Templates (GTTs). We hypothesized that two kinds of terms were contained in title and abstract fields of documents relevant to an instance of a GTT: terms showing relevance to the GTT structure and terms showing relevance to the particular instance of the GTT, the topic. Terms relevant to a GTT are expected to express the generic information present in all instances of the GTT, such as interactions and relationships. Terms relevant to the instance of a GTT are expected to express the particular entities specific to the instance, i.e. the topic. We aimed at isolating terms specific to the structure of each GTT. GTTspecific terms were used for query expansion to seek to improve retrieval performance. Both relevance feedback and pseudo-relevance feedback were used to extract GTTspecific terms. Our methods were evaluated with the trec_eval program, using the partial relevance information available for the sample topics. The Físreál <ref type="bibr" coords="1,431.39,559.97,14.01,10.46" target="#b0">[1]</ref> search engine, developed at Dublin City University, was used to generate the rankings. The paper is organized in the following way: Section 2 introduces some background on the collection and relevance/pseudo-relevance feedback methods. Section 3 describes our experimental method and its analysis. Section 4 concludes on future work and experiments.</p><p>♦ Contact author: fcamous@computing.dcu.ie</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background 2.1. Genomics track collection and the GTTs</head><p>The documents contained in the collection are the same as the ones used in the 2004 Genomics track. They consist of a 10-year subset of the MEDLINE biomedical abstract database (approximately 4.5 million documents). Most documents contain textual fields such as a title and an abstract. This year 5 different types of query structures, or Generic Topic Templates (GTTs), were developed. They provided five different models of query expression. Table <ref type="table" coords="2,475.02,180.77,6.00,10.46">1</ref> gives a description of the five GTTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTT queries (entire GTT description) 1</head><p>Find articles describing standard methods or protocols for doing some sort of experiment or procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Find articles describing the role of a gene involved in a given disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Find articles describing the role of a gene in a specific biological process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Find articles describing interactions (e.g., promote, suppress, inhibit, etc.) between two or more genes in the function of an organ or in a disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Find articles describing one or more mutations of a given gene and its biological impact in a given organism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. GTT descriptions given by the task</head><p>The 50 test topics included 10 instances of each GTT and the sample topics 2 instances of each GTT. Table <ref type="table" coords="2,180.03,351.95,6.00,10.46" target="#tab_1">2</ref> gives the full narratives of the 10 sample topics. Partial relevance judgements associated with the sample topics were made available. They contained documents judged "probably relevant" and "definitely relevant" to the topics. They included a total of 596 judged documents with a range of 4-245 judged documents per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTT# Topic # Full narratives of sample topics 90</head><p>Describe the procedure or methods for quality control in microarray experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Query expansion, relevance and pseudo-relevance feedback</head><p>Expanding a query means adding new terms to it. The terms can be extracted from documents judged relevant, in the case of relevance feedback. They can also be extracted from documents that are assumed to be relevant, as in pseudo-relevance feedback. When relevance feedback is used, the number of documents used for term extraction is the number of judged documents available for the topic. However, when pseudo-relevance feedback is used, an arbitrary number of documents located at the top of an initial ranking can be assumed relevant. In our experiment, we assumed the top 5 documents of an initial ranking to be relevant. Terms need to be scored and ranked according to their association with the relevance of the documents. We used Robertson's Offer Weight method <ref type="bibr" coords="3,405.84,133.37,14.00,10.46" target="#b1">[2]</ref> to score the terms contained in title or abstract fields of documents assumed or judged relevant. The term score is given by the following formula:</p><formula xml:id="formula_0" coords="3,91.44,172.06,403.90,38.11">( )( ( )( ) )         + - + - + + - - + = 5 . 0 . 5 . 0 5 . 0 . 5 . 0 log . _ r R r n r R n N r r score term (<label>1</label></formula><formula xml:id="formula_1" coords="3,495.34,187.73,4.67,10.46">)</formula><p>where r is the number of relevant documents containing the term, n the number of documents containing the term, and N= 4,591,008, the total number of documents in the collection. After the terms are scored and ranked, the top n are selected to expand the query. In our experiments, n took the values 5, 10, 15, and 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental method and analysis</head><p>Partial relevance information was only available for the sample topics. As a consequence, our experiment aimed at improving baseline rankings for the sample topics.</p><p>If two distinct topics are instances of a same GTT, relevant documents retrieved in the two distinct rankings are expected to contain title/abstract terms specific to each topic. Our hypothesis is that they will also contain common terms that are specific to the GTT. For example, topic 92 and 93 from table 2 are both instances of GTT 2. This query structure is a basis for formulating an information need relative to the role of a gene in a disease. Topic 92 and 93 concern distinct genes and diseases and relevant documents retrieved for each topic should contain terms that relate to each topic separately. Following our hypothesis, documents relevant to both topics are expected to contain common generic terms that express relationships between genes and diseases. Those terms can be used to expand the initial query and push up the ranks of relevant documents that mention a gene and a disease as well as the GTT relationship. Two methods were used to extract the GTT-specific structural terms. The first method used the relevance information available for the sample topic. It is described in section 3.2. The second method used pseudo-relevance feedback on rankings obtained with Físreál <ref type="bibr" coords="3,472.00,504.71,13.99,10.46" target="#b0">[1]</ref> search engine for the 50 test topics. This method is described in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline sample rankings</head><p>A first objective was to find a baseline ranking for the 10 sample topics by generating queries from the full narratives of table 2. Two options were investigated. First, the full narratives were sent unmodified as queries to Físreál search engine to generate a ranking for each sample topic. Those queries were called "full narratives". Secondly, another set of queries was produced from the full narratives by keeping only terms unique to the individual topic (not present across a GTT). Those queries were called "basic narratives". As an example, the "basic" version of the full narrative "Provide information about the role of the gene DRD4 in the disease Alcoholism" is "DRD4 Alcoholism". No GTT structural information is present in the "basic narratives". Table <ref type="table" coords="3,406.28,665.51,6.00,10.46" target="#tab_3">3</ref> shows a list of all the sample "basic narratives". They were sent to Físreál to generate a ranking for each topic.</p><p>Table <ref type="table" coords="4,122.27,75.17,6.00,10.46" target="#tab_4">4</ref> shows the results for both full and sample basic narratives in terms of Mean Average Precision (MAP) and recall. As the sample basic narratives produced better rankings, we chose them as our baseline rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Using the sample topics relevance information.</head><p>Formula 1 was used to score and rank the terms contained in relevant documents for each sample topic. Table <ref type="table" coords="4,188.44,153.17,6.00,10.46" target="#tab_5">5</ref> shows the amount of documents judged relevant per topic. 10 term rankings, 2 per GTT, were generated. GTT-specific structural terms were assumed to be contained in the two rankings obtained for both instances of the GTT. For each GTT, after score normalization (all scores in each topic ranking are divided by the highest score of the topic ranking), terms in common in the two topic rankings were kept and their scores were added. The common-term list was re-ranked to give the GTT-specific structural term ranking.  The top n terms from each GTT were used to expand the basic narratives of the sample topics related to the GTT. In the expanded query, terms were combined in the following way: Topic-specific terms (from basic narrative) were given a weight of value 3 and GTT-specific terms (from relevance feedback) were given a weight of value 1. This weighting policy gave us good results in preliminary experiments. The results are given in table <ref type="table" coords="4,131.62,685.61,6.00,10.46" target="#tab_6">6</ref> for different values of n. For all n values, there is an improvement in Mean Average Precision. However, recall decreases as n grows. As table 5 shows, the number of relevant documents per topic can vary from 4 to 245. Therefore, high variation of recall for a topic with a high number of relevant documents, such as topic 98, can strongly influence the average result across the 10 topics. Figure <ref type="figure" coords="5,409.72,102.77,6.00,10.46" target="#fig_0">1</ref> shows that a 10-term expansion improves the recall for 5 topics, leaves it unchanged for 2 topics, and decreases it for 3 topics. The 3 topics concerned by the decrease in recall are 95, 98 and 99. They contain 43, 245 and 75 relevant documents, respectively. Therefore the drop of recall for topics 95, 98 and 99 may have strongly contributed to the drop of the average recall for the 10 topics. The best MAP/recall combination is obtained with an expansion of 10 terms.  Basic narratives were produced for the 50 test topics. The basic narratives were sent to the Físreál search engine to generate 50 document rankings. For each document ranking, the top 5 documents were assumed to be relevant. Formula 1 was used to score and rank title/abstract terms contained in the documents assumed relevant.</p><p>To extract each GTT-specific structural term ranking, 10 topic term rankings had to be combined. To obtain a reasonable amount of terms per GTT (more than 20), a term was assumed GTT-specific if it appeared in at least 4 topic term rankings. As before, term scores were normalized in each topic ranking, normalized score were added and GTTspecific terms were re-ranked.</p><p>The GTT-specific terms generated with this method gave poor retrieval results when used to expand basic narratives from the sample topics. However, the terms were used to expand basic narratives from the test topics. Our assumption was that the terms could still work well with more topics and more relevance judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Runs sent to NIST</head><p>Two runs were sent to NIST, dcu1 and dcu2. dcu1 used the GTT-specific terms generated with the method described in section 3.2 to expand basic narratives of the test topics.</p><p>Only the top 10 terms of the GTT-specific term rankings were used. dcu2 used the GTTspecific terms generated with the method described in section 3.3 to expand basic narratives of the test topics. As before, only the top 10 terms were used for expansion.</p><p>Relevance information for the test topics was made available at the end of September 2005. Table <ref type="table" coords="6,155.00,177.77,6.00,10.46" target="#tab_7">7</ref> shows the impact of dcu1 and dcu2 expansion methods on a baseline search with basic narratives of the test topics. dcu1 gave lower MAP and recall values. This can be explained by the few topics per GTT (only 2) and the limited relevance information available. In the future, the relevance information released for the test topics will be used to generate GTT-specific structural terms. dcu2 gave a lower MAP value but maintained the recall level. The drop in MAP can be explained by the noise introduced when relevance is assumed for the top 5 documents from the initial rankings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper we have presented two methods to extract textual structural terms from MEDLINE documents relevant to the structure of template topics. The extraction methods were evaluated by measuring the impact of expanding initial queries with the extracted terms. Although query expansion results were encouraging with the sample topics, the two methods did not impact positively on the test topics results. The negative impact can be explained by the partial relevance information used in the first method, and the noisy pseudo-relevance information used in the second method.</p><p>In future work the relevance information available for the test topics will be used to generated template-specific structural terms. The position of GTT-specific structural terms relative to the topic-specific terms will be integrated in the extraction process.</p><p>Structural information can also be extracted from other fields of MEDLINE documents. The Medical Subject Headings (MeSH) are used to represent the conceptual content of MEDLINE records. This standardized conceptual information will be used to generate structural information relating to relationships present in documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,90.01,488.72,409.89,10.46;5,90.01,508.52,285.65,10.46;5,90.01,525.20,432.17,10.47;5,90.01,539.00,432.09,10.47;5,90.01,552.81,432.09,10.47;5,90.01,566.61,312.57,10.47;5,90.01,583.41,432.06,10.47;5,90.01,597.15,432.06,10.47;5,90.01,610.95,432.01,10.47;5,90.01,624.75,431.95,10.47;5,90.01,638.55,145.28,10.47;5,90.01,655.35,432.39,10.47;5,90.01,669.16,432.07,10.47;5,90.01,682.96,432.11,10.47;5,90.01,696.76,285.06,10.47"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Recall by sample topic and query type (basic, basic + expansion terms)3.3. Pseudo-relevance feedback with test topic rankings.Basic narratives were produced for the 50 test topics. The basic narratives were sent to the Físreál search engine to generate 50 document rankings. For each document ranking, the top 5 documents were assumed to be relevant. Formula 1 was used to score and rank title/abstract terms contained in the documents assumed relevant. To extract each GTT-specific structural term ranking, 10 topic term rankings had to be combined. To obtain a reasonable amount of terms per GTT (more than 20), a term was assumed GTT-specific if it appeared in at least 4 topic term rankings. As before, term scores were normalized in each topic ranking, normalized score were added and GTTspecific terms were re-ranked. The GTT-specific terms generated with this method gave poor retrieval results when used to expand basic narratives from the sample topics. However, the terms were used to expand basic narratives from the test topics. Our assumption was that the terms could still work well with more topics and more relevance judgments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,90.00,614.03,205.69,10.46"><head>Table 2 . Full narratives of sample topics</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,92.99,388.37,411.40,52.46"><head>Table 3 . Basic sample narratives</head><label>3</label><figDesc></figDesc><table coords="4,106.14,408.09,398.25,32.74"><row><cell></cell><cell>Full sample narratives</cell><cell>Basic sample narratives</cell></row><row><cell>Mean Average Precision</cell><cell>0.1144</cell><cell>0.1268</cell></row><row><cell>Recall</cell><cell>0.5772</cell><cell>0.5856</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,90.00,444.65,417.49,148.46"><head>Table 4 . MAP and recall results for basic and full sample narratives GTT# Topic # Number of relevant docs R 90</head><label>4</label><figDesc></figDesc><table coords="4,444.90,476.25,10.05,8.74"><row><cell>49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,90.00,596.93,272.79,10.46"><head>Table 5 . Number of relevant documents, R, per topic.</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,90.00,205.41,427.12,82.00"><head>Table 6 . Results with expansion using relevance feedback</head><label>6</label><figDesc></figDesc><table coords="5,90.24,205.41,426.88,67.72"><row><cell></cell><cell></cell><cell cols="4">Topic Term weight=3, GTT term weight=1</cell></row><row><cell></cell><cell>Basic sample</cell><cell>top 5 terms</cell><cell>top 10 terms</cell><cell>top 15 terms</cell><cell>top 20 terms</cell></row><row><cell></cell><cell>narratives</cell><cell>kept</cell><cell>kept</cell><cell>kept</cell><cell>kept</cell></row><row><cell>Mean Average Precision</cell><cell>0.1268</cell><cell>0.1280</cell><cell>0.1303</cell><cell>0.1309</cell><cell>0.1303</cell></row><row><cell>Recall</cell><cell>0.5856</cell><cell>0.5772</cell><cell>0.5721</cell><cell>0.5621</cell><cell>0.5604</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,90.00,280.41,399.02,47.03"><head>Table 7 . Impact of dcu1 and dcu2 methods on baseline</head><label>7</label><figDesc></figDesc><table coords="6,90.60,280.41,398.42,32.74"><row><cell></cell><cell>Basic narratives</cell><cell>dcu1</cell><cell>dcu2</cell></row><row><cell>Mean Average Precision</cell><cell>0.1947</cell><cell>0.1851</cell><cell>0.1844</cell></row><row><cell>Recall</cell><cell>0.6374</cell><cell>0.6169</cell><cell>0.6405</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is funded by <rs type="funder">Enterprise Ireland</rs> under the <rs type="grantName">Basic Research Grants Scheme</rs>, project number <rs type="grantNumber">SC-2003-0047-Y</rs>, and by <rs type="funder">Science Foundation Ireland</rs> under grant number <rs type="grantNumber">03/IN.3/361</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QqJdAjg">
					<idno type="grant-number">SC-2003-0047-Y</idno>
					<orgName type="grant-name">Basic Research Grants Scheme</orgName>
				</org>
				<org type="funding" xml:id="_sm4xxTC">
					<idno type="grant-number">03/IN.3/361</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,108.00,633.83,374.00,13.28;6,108.00,650.45,86.64,10.46;6,194.64,647.57,6.21,6.96;6,203.86,650.45,291.31,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,241.08,636.65,217.33,10.46">Físreál : A Low Cost Terabyte Search Engine</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,108.00,650.45,86.64,10.46;6,194.64,647.57,6.21,6.96;6,203.86,650.45,222.27,10.46">Proceedings of 27 th European Conference in Information Retrieval</title>
		<meeting>27 th European Conference in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005-03">2005. March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.00,668.21,379.02,10.46;6,108.00,682.01,390.57,10.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,323.29,668.21,163.73,10.46;6,108.00,682.01,39.56,10.46">Simple, proven approaches to text retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Spark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
		<idno>356</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
