<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.36,87.94,461.25,14.42">Rutgers Information Interaction Lab at TREC 2005: Trying HARD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,79.14,108.59,52.33,10.46"><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
							<email>belkin@scils.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,139.46,108.59,37.19,10.46"><forename type="first">M</forename><surname>Cole</surname></persName>
							<email>mcole@scils.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,184.78,108.59,51.19,10.46"><forename type="first">J</forename><surname>Gwizdka</surname></persName>
							<email>jgwizdka@scils.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,244.75,108.59,38.10,10.46"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName coords="1,290.46,108.59,37.10,10.46"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName coords="1,335.48,108.59,54.05,10.46"><forename type="first">G</forename><surname>Muresan</surname></persName>
							<email>muresan@scils.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,398.15,108.59,63.52,10.46"><forename type="first">D</forename><surname>Roussinov</surname></persName>
							<email>dmitri.roussinov@asu.edu</email>
						</author>
						<author>
							<persName coords="1,475.52,108.59,52.01,10.46"><forename type="first">C</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>csmith@scils.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,253.50,122.39,44.67,10.46"><forename type="first">A</forename><surname>Taylor</surname></persName>
							<email>artaylor@eden.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,306.17,122.39,52.33,10.46"><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
							<email>xjyuan]@scils.rutgers.edu</email>
						</author>
						<author>
							<persName coords="1,109.84,163.79,55.67,10.46"><forename type="first">W</forename><forename type="middle">P</forename><surname>Carey</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Communication, Information and Library Studies</orgName>
								<orgName type="institution">Rutgers University New Brunswick</orgName>
								<address>
									<postCode>08901-1071</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287-4606</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.36,87.94,461.25,14.42">Rutgers Information Interaction Lab at TREC 2005: Trying HARD</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">832928EAB16593BC12362EC6AE8873AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Within the structure of the TREC 2005 HARD track guidelines, we investigated the following hypotheses: H1: Query expansion using a "clarity"-based approach will increase effectiveness over baseline queries and baseline queries plus pseudo-relevance feedback; H2: Query expansion based on the Web will increase effectiveness over baseline queries and baseline queries plus pseudo-relevance feedback; H3: Query expansion using terms selected by the searcher from those suggested by clarity modeling and/or the web will increase effectiveness over baseline queries, baseline queries plus pseudo-relevance feedback, and queries expanded by all suggested terms; H4: Query expansion using "problem statements" elicited from the searcher will increase effectiveness over baseline queries and baseline queries plus pseudo-relevance feedback; H5: The effectiveness of query expansion using problem statements will be negatively correlated with query "clarity". H1 and H2 were tested without user intervention; H3 and H4 were tested using two different "clarification forms"; H5 was tested using the results of the H4 clarification form. Baseline queries were generated from the topic titles and descriptions; query expansion was accomplished by adding terms to the baseline queries, with a variety of weights given to the expansion terms, relative to the baseline terms. Preliminary results indicate that H1, H2, H3 and H4 are in part weakly supported, in that performance is increased over baseline, but it is not increased over pseudo-relevance feedback. H5 was not supported. Combining some degree of user interaction (H3) with pseudo-relevance feedback appears to lead to increased performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The 2005 TREC HARD track differed from those in the previous two years in that there was no user "metadata", other than the assessor's degree of familiarity with the topic, all other information about the assessor's context having to be derived from interactive clarification forms (CFs). Since the CFs are understood to be simulations of interactions with a searcher, subsequent to an initial query, we were primarily concerned in this year's investigations with whether, and under what circumstances, such interaction is worthwhile. As has been demonstrated in numerous studies, searchers typically are willing to engage in such interaction only when the payoff, in terms of control and effectiveness, is perceived to be high, and the interaction is clearly relevant to the searcher's goal (e.g. <ref type="bibr" coords="1,278.96,630.95,93.20,10.46" target="#b1">Belkin, et al., 2000)</ref>.</p><p>The primary purposes of the type of interaction represented by CFs have typically been understood to be either more detailed specification of the query/information problem, in particular for disambiguation; or indication of the searcher's context, used to tailor search results in some way or another. The former is typically implemented as some form of query expansion; the latter as re-ranking of the original search results. In both of these cases, the obvious comparison to make with respect to the usefulness of the interaction is with the effectiveness of the original query; and with the effectiveness of the original query, modified by automatic methods which do not require interaction. Query expansion using pseudo-relevance feedback is an obvious candidate for a non-interactive method; so also are query expansion using an external resource, such as the Web, and using language-modeling/clarity rather than traditional relevance feedback methods for expansion term selection. In this year's research, we investigated the use of query expansion based on CFs, and compared performance under those queries with implementations of these two general automatic methods for query expansion, as well as with a baseline query derived only from the title and description of the topic. We did not consider the use of searcher's level of familiarity with the topic for search result modification.</p><p>We organized our investigations in HARD 2005 around several hypotheses, which, in standard scientific manner, we hoped to either disprove, or support. Our initial idea was to test whether searcher selection of terms thought by the system to be useful in query expansion was effective, as some studies have shown (e.g. <ref type="bibr" coords="2,234.02,260.57,145.17,10.46">Koenemann.and Belkin, 1996)</ref>. We also wanted to investigate the effectiveness of query expansion terms under different selection models. We therefore implemented expansion term identification in three different ways: (i) standard pseudo-relevance feedback, as implemented in the Lemur toolkit<ref type="foot" coords="2,296.22,299.09,3.99,6.96" target="#foot_0">1</ref> ; (2) what we call "clarity scoring" of terms based on the database being searched, using Lemur utilities; and (3) use of the Web as an external resource, to identify terms and phrases associated with the query which might otherwise not be identified through database-specific methods. This led us to the first three hypotheses which we tested.</p><p>H1: Query expansion using a clarity approach will increase effectiveness over baseline queries and baseline queries plus standard pseudo-relevance feedback H2: Query expansion based on the Web will increase effectiveness over baseline queries and baseline queries plus pseudo-relevance feedback.</p><p>H3: Query expansion using terms selected by the searcher from those suggested by clarity modeling and/or the web will increase effectiveness over baseline queries, baseline queries plus pseudo-relevance feedback, and queries expanded by all suggested terms.</p><p>Our second area of concern was with elicitation of longer and more complex descriptions of the information problem than a typical keyword query such as that simulated by the content words of the title and description of a TREC topic. This approach is based on the general idea of the ASK hypothesis, that searchers should not be asked to specify that which they don't know <ref type="bibr" coords="2,480.85,546.71,39.01,10.46;2,72.00,560.51,26.11,10.46" target="#b0">(Belkin, 1980)</ref>; on research results that indicate that longer queries in a best-match search system result in improved performance (e.g. <ref type="bibr" coords="2,208.91,574.31,93.46,10.46" target="#b2">Belkin, et al., 2002)</ref>; and on results from the 2004 HARD track which showed performance improvement over baseline when such descriptions were used for query expansion <ref type="bibr" coords="2,154.01,601.91,123.85,10.46">(Kelly, Dollo &amp; Fu, 2005)</ref>. We speculated that elicitation of this type of "problem statement" might be useful for query expansion when an initial query was likely to be ineffective; following <ref type="bibr" coords="2,179.30,629.51,202.39,10.46" target="#b5">Cronen-Townsend, Zhou and Croft (2002)</ref>, we operationalized ineffectiveness as low query clarity. This line of investigation led us to our other two hypotheses. H4: Query expansion using "problem statements" elicited from the searcher will increase effectiveness over baseline queries and baseline queries plus pseudo-relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H5:</head><p>The effectiveness of query expansion using problem statements will be negatively correlated with query "clarity".</p><p>In the next section, we describe in detail how we implemented and used the clarification forms, and other data, for testing each of these hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Clarification Forms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CF1: Internet Mining Terms</head><p>Past experience with TREC topics indicates that, while the query expansion based on pseudorelevance feedback (adding terms from the top returned documents to the query) is the most effective way of improving performance, it is not effective on the worst topics due to a phenomenon commonly known as "query drift:" when the top documents are irrelevant, so are the added terms.</p><p>Since this year's HARD track topics have been selected from the worst prior year topics, we were deliberately looking into different and hopefully safer expansion strategies. Inspired by the success of our <ref type="bibr" coords="3,142.97,284.87,128.67,10.46" target="#b10">(Roussinov &amp; Zhao, 2003)</ref> and other researchers' <ref type="bibr" coords="3,383.48,284.87,93.67,10.46" target="#b9">(Kwok et al., 2004)</ref> work on Internet mining and user-controlled query expansion <ref type="bibr" coords="3,327.30,298.67,141.15,10.46" target="#b8">(Koenemann &amp; Belkin, 1996)</ref>, we developed a Navigation By Expansion (NBE) paradigm and tested it with our submitted runs. The NBE paradigm follows the same intuitive principles of navigation that people employ while, for example, driving or walking in new surroundings. First, the topic surroundings are identified (e.g. as a set of possibly related words or phrases) through the Internet mining (or possibly other resources, such as WordNet). Second, the set of possible moves within those surroundings is identified (e.g. by preserving only those terms that are present in the HARD collection).</p><p>Although the specific methods on which the paradigm builds (expansion, refining) have been studied in the past, we believe that combining them into a single higher level framework is beneficial. This formulation also influenced the particular choice of techniques and models to use in the implementation described below. We also believe that the approach has not been methodologically studied yet in spite of its promise. For example, it is still not known what specific techniques work best for representing the topic surroundings, what models work best for selecting candidate terms for the query expansion, and what are the most effective ways of ranking the documents while using the expanded query.</p><p>In this year's TREC, in order to determine the surrounding concepts, we submitted our query to Google and built what we call an Internet language model for it. Only the concepts with the frequencies of occurrence among the top 200 returned pages (full-text) larger than their background frequencies of occurrence on the Web were considered. We designed and trained a special formula for the probability of being a surrounding concept using logistic regression and different topics from the previous years. Some additional details on the mining approach and its applications beyond ad hoc retrieval can be found in <ref type="bibr" coords="3,326.25,600.41,110.03,10.46">Roussinov et al. (2005)</ref>. Our set of CF1 forms implemented the proposed NBE paradigm.</p><p>The Internet language model (probabilities of occurrences of possibly related terms) for each topic was built and analyzed through the following steps:</p><p>Step 1. The title and description were merged into a single query and sent to Google.</p><p>Step 2. The full text of the top 200 pages returned by Google was downloaded as the mining corpus (the "ore").</p><p>Step 3. For each term (a sequence of up to 3 consecutive words) in the mining corpus, the probability of being "related to the topic" was estimated by approximating the logistic regression on the deviation from randomness when the values of the probability were approaching 1, specifically as following: Pr(t) = 1 -exp (-(s -1) / α), where: s = signal to noise ratio of the term, estimated as: s = (df m / N m ) / (df w / W), where df m was the number of occurrences of the term in the mining corpus, N m was the number of pages in the mining corpus, df w was the number pages on the Web in which the term occurs, obtained by querying Google, W was the total number of pages covered by Google, set at 3,000,000,000 at the time. df m / N m represented "signal", while df w / W represented the "noise." For the non related term, we would expect the proportion of the pages within the mining corpus that have this term to be the same as the proportion of the pages having this term on the entire Web. The ratio of those two proportions represented the deviation from randomness within the mining corpus.</p><p>The adjustment parameter α defined how "steep" the probability curve was relatively to the signal to noise ratio. We set α to 0.5 by visually inspecting the related concepts for the different topics from the preceding years. With this value, a signal to noise ratio of 1.5 would give the probability of 1 -exp (-1) = .63. A signal to noise ratio of 2.5 would result in p = .86, etc. In our case, the results were not very sensitive to the value of parameter α since we only needed to select the top most deviant from the background terms, so we did not rely on the actual probability estimate. Although including the terms that are frequent in the mining corpus as a result of their being frequent in the entire Web would not generally hurt the retrieval results since they would typically have low idf value, it would still put higher cognitive load on the user and require more time to make selections. Since our NBE paradigm requires the expansion terms to represent concept surroundings, the value of Pr(t) served as a good guidance which terms to use in the clarification forms.</p><p>Step 4. Out of the related terms, we preserved only those that occurred at least once in the target collection (Aquaint) and sorted them according to their impact estimate: i = Pr(t) * idf, where idf was the inverse document frequency computed based on the target collection statistic: idf = log(N/df) / log(N), where N was the number of documents in the HARD corpus and df was the number of documents containing the term t. Since idf weight was used by our document ranking function (BM25), it provided a reasonable estimate of each term impact (if selected) relatively to the other selected terms. Using the terms with low idf values in the clarification would not hurt the performance, but again would not be an efficient "move" within our NBE paradigm since their effect on ranking would be negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CF1: Clarity Terms</head><p>The second set of terms that we used was created based on the notion of query clarity <ref type="bibr" coords="5,485.46,89.99,43.30,10.46;5,72.00,103.79,161.85,10.46" target="#b5">(Cronen-Townsend, Zhou and Croft, 2002)</ref>. The clarity of each query, and each of its individual terms was computed by the Lemur toolkit that we used for indexing and ranking (QueryClarity application). We used the following (default) parameters: feedbackDocuments obtained by the BM25 retrieval from the target collection (also with the default parameters), feedbackDocCount = 5, feedbackCoefficient = .5, feedbackTermCount = 100. We sorted the terms according to the clarity values reported by Lemur and selected the top 10. The Lemur QueryClarity application does not support phrases, thus all our clarity terms were single word terms. Since all the selected terms were from the target collection, they all represented valid "moves" within our NBE paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CF1: Merging Terms</head><p>To produce CF1 for presentation to the assessors, we merged the top ten terms from the two different sources, removed duplicates, and presented them to the assessors in alphabetical order, with no reference to the source of the terms. Assessors were asked to choose those terms which they thought would be useful in expanding the original query (see Figure <ref type="figure" coords="5,425.32,298.13,4.33,10.46" target="#fig_0">1</ref>). Each form listed the topic title, description and narrative, followed by up to 20 terms (words or phrases), each preceded by a check box. The instructions to select the term were the following: "The search system has identified several search terms which are thought to be possibly useful for modifying the search query for this topic. Please check all of the search terms which you think would be good to include in a query for this topic." When the user selected the box, the term was used in subsequent query expansion, otherwise it was ignored. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CF2: User Generated Terms</head><p>Following our work in the Interactive Tracks of TREC 2002 <ref type="bibr" coords="6,364.62,89.99,99.26,10.46" target="#b2">(Belkin, et al., 2002)</ref> and 2003 <ref type="bibr" coords="6,72.00,103.79,94.18,10.46" target="#b3">(Belkin et al., 2003)</ref>, our fourth hypothesis was that retrieval performance would be improved if additional terms generated by the user were added to the query. Following the experiment reported by UNC for TREC 2004 <ref type="bibr" coords="6,236.04,131.39,128.30,10.46" target="#b6">(Kelly, Dollu, &amp; Fu, 2004;</ref><ref type="bibr" coords="6,367.33,131.39,28.01,10.46" target="#b7">2005)</ref> we used a second clarification form (CF2), which presented three specific requests for additional terms. In the UNC study, which also presented three open-ended elicitations, the order of the requests was not rotated among subjects. UNC found that the response to their first request, "Describe what you already know about this topic," produced more terms on average than the other two subsequent questions (30.98 vs. 23.11 and 2.47 terms respectively). In addition, when the performance of each of the three elicitations was compared separately to baseline, the first request had the largest positive effect on performance (p &lt; .05) <ref type="bibr" coords="6,266.76,227.93,126.80,10.46" target="#b7">(Kelly, Dollu, &amp; Fu, 2005)</ref>. The UNC papers suggested that the relative superiority of their first open-ended request might be explained as an order effect. One of our objectives was to explore whether information about background knowledge, as requested in the first UNC prompt, was more valuable than other sources of additional contextual terms, as requested in the second and third UNC prompts.</p><p>Our CF2 clarification form was designed to replicate the UNC experiment with control for elicitation order. For this reason, we rotated the order of the three open-ended prompts in our 50 clarification forms. Two of the three UNC elicitations were used in our CF2 form; in addition to the above first elicitation we also used UNC's third elicitation, "Please input any additional keywords that describe your topic." The second UNC elicitation was a question asking the subject "Why do you want to know about this topic?". Because subjects for HARD 2005 were not working on topics they created, which was the case in 2004, this second question did not apply. We replaced this question with one taken from the interview question used in the original ASK study <ref type="bibr" coords="6,128.00,413.33,184.67,10.46">(Belkin, Oddy, &amp; Brooks, 1982, p. 146</ref>), asking the subject, "What sort of information would you like to have as a result of this search?". In the original ASK study, as in the current experiment, the assessment task was undertaken by an agent for the principal information seeker. For this reason we considered our substitute question to be a reasonable replacement for UNC's question related to the expected value of the output of the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Official Runs Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Runs</head><p>Our basic approach was to supplement a query based on the title and description of the topic with terms drawn from two types of sources. The first type depended upon identification of terms in text collections. The other type depends on the assessor's understanding of their task and knowledge of the topic. For both types of information the additional terms were used for query expansion and document retrieval. We used the Lemur IR toolkit to build the queries and retrieval results. With our interest in the effect of combining additional terms from several different types of sources, including user interaction, the structured query evaluation module (StructQueryEval), based on the InQuery retrieval model was employed. The results were evaluated using the standard trec_eval program (http://trec.nist.gov/trec_eval/).</p><p>Our original baseline run, officially submitted to NIST, was deficient in some way that we have not yet identified (MAP of 0.06 seems very unlikely), and so we replaced it in our set of official runs with another baseline run (RUTGBL) which was constructed as originally intended, using the title and description fields of the test topics. RUTGBL is used as the baseline for evaluation of our results. Another submitted run (RUTGBF3) was based on pseudo-relevance feedback. Another run combined all the sets of expansion terms (from the Web, from Lemur, based on clarity, and derived from the answers solicited from assessors in the clarification form) with the topic title and description (RUTGALL).</p><p>The other runs were based on structured queries as a weighted sum of the topic and title with combinations of the various term sets. Three of those runs concerned combinations of the external term sources without assessor interaction (RUTGWS1 -expanded with terms derived from the web, RUTGLS1 -expanded with "clarity" terms, RUTGAS1 -expanded with both term sources). Two runs concerned explicit assessor interactions with the clarification forms (RUTGUS1 -terms selected by the assessor from CF1, RUTGUG1 -terms provided by the assessor in CF2). The details of the query formulation are given in Table <ref type="table" coords="7,423.91,232.97,4.50,10.46" target="#tab_0">1</ref>.</p><p>The RUTGRS1 run was intended to capture a random sample of terms from those presented to the assessor in CF1. With proper re-sampling the performance of user selection can be compared with automatic random selection of the same number of terms. No meaningful conclusions can be drawn, however, from the single run and so there is no further discussion of this run here. Selected from CF1</p><p>Q1 3 Q2 4 Q3 5 RUTGAS1 0.9 0.9 0.1 RUTGWS1 0.9 0.9 0.1 RUTGLS1 0.9 0.9 0.1 RUTGUS1 0.9 0.9 0.1 RUTGUG1 0.9 0.9 0.1 0.1 0.1</p><p>The numbers in each cell indicate the relative weight given to terms from each of the term sources.</p><p>1 Pseudo-relevance feedback with 20 documents and 100 terms, feedback coefficient=0.3 2 The terms in CF1 were the Web and Lemur suggested terms with duplicates removed 3 "</p><p>Please describe what you already know about this topic." 4 "What sort of information would you like to have as a result of this search ?" 5 "Please input any additional keywords that describe this topic."</p><p>There were several problems implementing these intentions in our official runs. In several cases, due to a script bug, the CF1 clarification forms did not provide a few of the unique terms in the web and clarity suggested term sets. In the case of one topic (689) there were no clarity terms available, and for five topics in the web-suggested terms <ref type="bibr" coords="7,345.89,630.65,142.38,10.46">(435, 439, 443, 448, and 622)</ref> the queries were incorrect. We report results here with the deficiencies addressed in the affected web-suggested runs (RUTGALL and RUTGWS1).</p><p>There was also a problem with the intended weighting scheme for our runs, due to our inexperience in using the structured query syntax employed by Lemur. The weights were applied to the term sets taken as a group, rather than to the individual terms. So the official runs were based on the relative contribution of the terms in each group, irrespective of the numbers of terms in each. Our intention was to weight individual terms drawn from the appropriate terms sources in the scheme given in Table <ref type="table" coords="8,252.28,102.77,4.50,10.46" target="#tab_0">1</ref>. In the corrected runs the second weighting scheme has been adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Combination Runs</head><p>Since we were also curious to see how our NBE approach can work in combination with the pseudo-relevance feedback, we created two more runs using the original (non-expanded) baseline, expanded with pseudo-relevance feedback. We used the default parameters in Lemur to create it: feedbackDocCount = 20, feedbackTermCount = 100, feedbackPosCoeff = .3. This created a pseudo-relevance feedback score. Then, we implemented our own expansion module using the available C++ source in Lemur package. The top 1000 documents from the baseline were re-ranked according to the following score: score = pseudo-relevance feedback score + 0.3 * expansion score, where the expansion score was obtained using BM25 ranking with the default parameters for the query consisting only of the user selected CF1 terms. We used an expansion factor of 0.3 instead of 0.1 used with the other runs deliberately to diversify our set of official runs. This run is designated RUTBE in our results. The second combined run, RUTIN, was obtained using structured query in Lemur (StructQueryEval) and all the Web suggested terms added with 0.3 factor. We also specified Lemur to use blind feedback while doing structured query retrieval, with the following parameters: feedbackDocCount = 5, feedbackTermCount = 5, feedbackPosCoeff = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Our runs versus overall median runs.</head><p>We start by comparing our baseline run (RUTGBL) with the overall baseline median results. Rprecision and mean average precision of our baseline run were better than the overall baseline median (Table <ref type="table" coords="8,144.64,465.47,4.34,10.46" target="#tab_1">2</ref>). When compared on individual topics, R-precision of our baseline run was better than the overall baseline median results on 25, worse on 18, and tied on 7 topics. However, these differences were overall not statistically significant. Next we compare our best run (RUTGALL) with the overall final median results (Table <ref type="table" coords="8,497.36,601.73,4.33,10.46" target="#tab_2">3</ref>). R-precision and mean average precision of our RUTGALL run were statistically significantly better than the overall final median (Wilcoxon tests were: Z=-2.54, p=.011 and Z=-2.29; p=.003 respectively). R-precision of our RUTGALL run was better than the overall final median on 29, worse on 19, and tied on 2 topics. RUTGALL p@10 was better, but the difference was not significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of our experimental runs.</head><p>The results of our experimental runs are presented in Table <ref type="table" coords="9,358.58,171.29,6.00,10.46" target="#tab_3">4</ref> and in Figure <ref type="figure" coords="9,434.58,171.29,4.50,10.46">2</ref>.</p><p>R-precision and p@10 were not significantly different for all runs considered together (assessed by Friedman test for k-related samples: χ 2 =11.32, p=.125, χ 2 =9.60, p=.212, for R-precision and p@10 respectively). The mean average precision was significantly different (χ 2 =36.64, p=&lt;.0001). Hence, we cannot establish an absolute ordering of all results in terms of their performance as measured by Rprecision. However, based on pair-wise comparisons (Wilcoxon test) we can state that runs which incorporated all of the suggested terms (both clarity and web) (RUTGAS1), clarity suggested terms (RUTGLS1), web suggested terms (RUTGWS1), user selected terms (RUTGUS1), and user generated terms (RUTGUG1) were all individually significantly better than our baseline run, thus partially confirming hypotheses: H1,H2, H3, H4. Due to high variation of R-precision across topics, we cannot state whether blind feedback (RUTGBF3) and all suggested and user generated terms (RUTGALL) runs were significantly better than the baseline. Comparing RUTGALL to RUTGBL visually (Figure <ref type="figure" coords="9,447.66,453.23,5.00,10.46" target="#fig_1">3</ref>) we can see that on more than half of the topics (27/50) RUGTALL was better than or equal to (2/50) the baseline run.  We also performed pair-wise comparisons of our experimental runs with the blind feedback run (RUTGBF3). Blind feedback performance was not significantly different from other runs, with an exception of mean average precision, which was significantly better than for the baseline run (RUTGBL, Wilcoxon test: Z=-2.23; p=0.026). Thus our hypotheses H1,H2, H3, H4 cannot be confirmed with respect to performance of runs employing blind feedback in addition to baseline queries.</p><p>Table <ref type="table" coords="10,104.35,353.18,4.50,10.80">5</ref>. Summary of significant and non-significant differences in R-precision among our experimental runs. </p><formula xml:id="formula_0" coords="10,91.50,381.67,417.05,75.51">BL AS1 LS1 WS1 US1 UG1 BF3 ALL BL ---- AS1 &gt; ---- LS1 &gt; n/s ---- WS1 &gt; n/s n/s ---- US1 &gt; n/s n/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of CF2-based runs</head><p>While we did not submit separate runs for each of the three elicitations in our second clarification form (CF2) we did produce unofficial runs for each (Q1, Q2, Q3), and for three combinations of the elicitations (Q1Q2, Q1Q3, Q2Q3, ), as well as the official run combining all three (RUTGUG1). We first compared the relative effects of the original query and CF2 terms where one component was included with varied weights, while the other with weight held constant at 1.0. We then focused on the effects of elicitation (Q1Q2, Q1Q3, Q2Q3, and Q1Q2Q3) and compare them with each other for runs with equal weighting of CF2 terms and the original query. We performed the following two sets of runs with varied weights: 1) original query (title + description) varied with weights ranging from 0.0 to 1.0, with the target model (i.e. the user-generated terms from the various questions) held constant with weight=1.0;</p><p>2) original query (title + description) held constant with weight=1.0, while the weight of the target model varied between 0.0 and 1.0. Figures <ref type="figure" coords="11,309.54,88.97,6.00,10.46">4</ref> and<ref type="figure" coords="11,338.88,88.97,6.00,10.46">5</ref> present the results of these runs. Increasing the weight of the original query with respect to the user-generated terms significantly improved performance. R-precision was significantly better for runs with successively increased weights (in all cases χ 2 &gt;100, p&lt;.001; Friedman test) (Figure <ref type="figure" coords="12,363.18,75.17,4.33,10.46">4</ref>). While R-precision also improved with increased weights of CF2 terms, the improvement over the original query was smaller, and only in three cases significant. The improvement was significant for: Q1, Q1&amp;Q3 and Q1,Q2,Q3 (Friedman test: χ 2 =26.73, p=0.003, χ 2 =21.28, p=0.019, and χ 2 =30.833, p=0.001 respectively).</p><p>From figures 4 and 5, it is clear that giving equal weight to the original query terms and to the terms generated by the user in CF2 is an optimum strategy, and that our official run RUTGUG1, in which CF2 terms were combined with the original query in the ratio of 0.1/0.9 is not the best choice. This is confirmed by the data in table 6, in which equal weighting of all CF2 terms with the original query terms leads to performance substantially (although not statistically significantly) better than for RUTGUG1 (R-Precision 0.314 vs 0.286).</p><p>In order to understand more about factors related to these results, we had two objectives. The first was to determine the relative contribution of each of the three elicitations in the performance of our submitted RUTGUG1 run. We also wanted to compare our results, which were controlled for the order in which assessors answered the three elicitations, with UNC's results from TREC HARD 2004, where the elicitations were not controlled for order. These two objectives are discussed below. As can be seen in table 6, when used independently, all three elicitations produced terms that enhanced performance at the top of the list, as measured by p@10 (Q1: Z = -2.415, p &lt; .05; Q2: Z = -2.117, p &lt; .05; Q3: Z = -3.132, p &lt; .01), while only Q3 produced significantly enhanced performance over baseline across the entire list, as measured by MAP (Z = -2.241, p &lt; .05).</p><p>The best performing combination used terms from all three sources (Q1Q2Q3), however, this run was not significantly better than the run including only terms from Q1 and Q3 (see table <ref type="table" coords="12,498.49,621.41,4.33,10.46" target="#tab_6">7</ref>), while it is significantly better than the Q1Q2 run. This finding raises the question of whether the Q2 terms are contributing to performance. The next logical question is the relative value of Q1 and Q3. A comparison of the combined Q1Q2 with the full Q1Q2Q3 run revealed that the addition of Q3 terms produced significantly enhanced performance. Clearly the Q3 terms are valuable, however, when the combined Q1Q2 run was compared with the Q1Q3 run, no significant performance difference was detected. As is revealed in the comparison of Q1 with Q1Q3, the addition of Q3 terms did not produce significantly enhanced performance at the top of the list, but did so across the entire list (as measured by R-precision and MAP The comparison of Q3 and Q1Q3 runs revealed no significant difference in performance. Two conclusions are suggested by the above results. First, the value of the additional Q2 terms may not justify the costs (e.g. cognitive processing) of producing them. This point is related directly to discussion of UNC's results, below. Second, the Q3 elicitation may be producing the most valuable terms. This finding does not, however, indicate that Q3 would be the most productive elicitation in a more natural setting. Assessors completed our CF2 forms while completing forms from other participants. Q3 asked for additional keywords. It is possible, perhaps even likely, that assessors returned terms they remembered from prior forms. Further research requires more controlled experimentation. We have not yet completed the analysis of order effects, so it is not possible to draw conclusions from a comparison with UNC's prior results. One result is worth noting, however. Recall, Q2 was a reformulated question for our TREC HARD 2005; the question was designed to replicate UNC's question related to the expected value of the output of the search <ref type="bibr" coords="13,421.57,452.99,98.00,10.46;13,72.00,466.79,27.34,10.46" target="#b6">(Kelly, Dollu, &amp; Fu, 2004;</ref><ref type="bibr" coords="13,102.33,466.79,25.84,10.46" target="#b7">2005)</ref>. In both the results reported above, and in UNC's prior results, when used independently of the other two elicitations, this question produced the least valuable terms among the three elicitations. Without the order effect analysis, it is not possible to draw conclusions about this; however, it does suggest that the form of an elicitation may affect the retrieval value of the terms produced. This question is deserving of further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Testing H5</head><p>Using the standard Lemur QueryClarity application, we tested to see whether there was any relationship between the effectiveness of query expansion using CF2 and initial query clarity of the topics. Tests were carried out using the following measures of improvement: percentage improvement; absolute magnitude of improvement; and, direction of change. We looked both for correlation of improvement with clarity score, and for any clarity score cut-off value which would identify topics for which CF2 intervention would be valuable. On all measures, and for both cases, we could not identify any significant relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Combining CF1 results with pseudo-relevance feedback</head><p>We investigated one other aspect of the use of our CF1 terms. Given that modifying the original query (RUTGBL) with pseudo-relevance feedback (RUTGBF3) led to somewhat improved performance, we were interested in whether the pseudo-relevance feedback performance could be further improved by additional sources of evidence. We tested this issue with the two "combination" runs described in section 3.2. RUTBE, a run which augmented the original query plus pseudo-relevance feedback with the user-selected terms from CF1 with a relative weight for the last of 0.3, performed significantly better than the baseline, RUTGBL, and also better than RUTGBF3, on all measures. However, the RUTBE used BM25 weighting, whereas RUTGBL and RUTGBF3 used InQuery weighting, so these results can only be indicative of possible performance improvement. By striking contrast, RUTIN, combining, in somewhat different fashion, the original query with pseudo-relevance feedback and only the web-generated terms that were suggested to the user in CF1, and using InQuery weighting, performs markedly worse than either RUTGBL or RUTGBF3, as well as RUTBE. Because the runs are not strictly comparable, we do not report any significance figures for differences in performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>Summarizing our results, we found partial support for our hypotheses H1-H4, in that expanding our original baseline query derived from topic title and description by adding terms, respectively: derived through clarity analysis of query; derived from the web; derived from both sources; selected by the assessor from both sources; and, generated by the assessor in response to elicitations in a clarification form (in this case, when those terms were given equal weight with the original terms), all significantly increased performance over the baseline run. However, there was no significant improvement of any of these methods of query expansion with respect to the baseline run expanded by straightforward pseudo-relevance feedback, nor was any one of these runs significantly different from any other. Thus, it appears that the best we can say with respect to overall results is that our interventions which require some form of effort from the searcher do improve performance, but not significantly more than automatic methods which require no additional searcher interaction.</p><p>The conclusion stated above depends, of course, on the belief that the original query that a searcher will generally provide is more-or-less of the same quality as the query that we generated from the title and description fields of the HARD topics. An alternative view might be that eliciting a query using something like our CF2 in the first instance, would lead to better initial results, which, together with automatic methods, such as pseudo-relevance feedback and our clarity-and web-generated term identification, would lead to more substantial performance improvement. The combination runs lend some support to this idea, but the differences in methods between baseline and combination runs make such a conclusion problematic.</p><p>Given that our pseudo-relevance feedback run expanded the query by 100 terms, and that our best performing experimental run, RUTGALL, expanded the query by all of the terms which appeared in both clarification forms, our results might well be explained as simply due to the well-known phenomena that: longer (reasonable) queries perform better than shorter queries in best-match retrieval; and, combining different sources of evidence leads to increased performance.</p><p>All of our results thus suggest that invoking user interaction in order to perform query clarification is unlikely to be cost-effective. That is, if a system elicited richer information problem descriptions in the first instance, which has been shown to be both possible, beneficial and usable <ref type="bibr" coords="15,125.94,163.97,97.09,10.46" target="#b2">(Belkin, et al., 2002)</ref>, and then enhanced such a description with automatic methods of query expansion such as those that we have described, combining a variety of different sources of evidence, effectiveness of initial information retrieval results could be substantially improved without having to engage the searcher in efforts extraneous to the overall search goal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,118.32,709.10,375.33,10.80;5,109.50,411.36,393.00,294.78"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The clarification form (CF1) for the topic "Human Smuggling."</figDesc><graphic coords="5,109.50,411.36,393.00,294.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,72.00,236.78,457.43,10.80"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Performance (R-precision) of RUTGALL versus RUTGBL on individual topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,72.00,358.94,432.52,10.80;11,72.00,372.74,343.60,10.80"><head></head><label></label><figDesc>Figure 4. R-precision (mean) for the original query (title + description) with variable weights 0.0-1.0, with user-generated terms at constant weight of 1.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,72.00,313.76,300.82,38.84"><head>Table 1 : Query run construction</head><label>1</label><figDesc></figDesc><table coords="7,92.10,330.50,280.72,22.10"><row><cell>run</cell><cell>title descrip-</cell><cell>web clarity Presented in</cell></row><row><cell></cell><cell>tion</cell><cell>CF1 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,520.40,412.43,63.67"><head>Table 2 . Our baseline run compared with the overall baseline median results.</head><label>2</label><figDesc></figDesc><table coords="8,77.16,535.03,407.27,49.05"><row><cell></cell><cell cols="2">R-precision</cell><cell>MAP</cell><cell></cell><cell>p@10</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Overall Baseline median</cell><cell>0.252</cell><cell>0.149</cell><cell>0.190</cell><cell>0.147</cell><cell>0.408</cell><cell>0.28</cell></row><row><cell>RUTGBL (baseline)</cell><cell>0.270</cell><cell>0.167</cell><cell>0.206</cell><cell>0.163</cell><cell>0.408</cell><cell>0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,72.00,74.96,412.44,63.67"><head>Table 3 . Our best run compared with the overall final median results.</head><label>3</label><figDesc></figDesc><table coords="9,77.16,89.59,407.28,49.05"><row><cell></cell><cell cols="2">R-precision</cell><cell>MAP</cell><cell></cell><cell>p@10</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Overall Final median</cell><cell>0.264</cell><cell>0.152</cell><cell>0.207</cell><cell>0.161</cell><cell>0.45</cell><cell>0.30</cell></row><row><cell>RUTGALL</cell><cell>0.299*</cell><cell>0.182</cell><cell>0.253</cell><cell>0.188</cell><cell>0.49**</cell><cell>0.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,72.00,213.39,480.61,460.83"><head>Table 4 . Comparison of our runs against the baseline run (RUTGBL). The runs are sorted by ascending mean. Z-score and p-values were assessed using Wilcoxon signed ranks test.</head><label>4</label><figDesc></figDesc><table coords="9,77.40,213.39,475.21,460.83"><row><cell>0.8000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>374</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rp.ALL</cell><cell>rp.AS1</cell><cell>rp.BF3</cell><cell>rp.BL</cell><cell>rp.LS1</cell><cell cols="2">rp.UG1 rp.US1 rp.WS1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Figure 2. R-precision for all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run name</cell><cell></cell><cell></cell><cell cols="3">R Precision</cell><cell></cell><cell></cell><cell cols="2">Precision at 10</cell><cell></cell><cell cols="2">Mean Average Precision</cell></row><row><cell></cell><cell cols="2">Mean</cell><cell cols="2">SD</cell><cell>Z</cell><cell>p</cell><cell>Mean</cell><cell>SD</cell><cell>Z</cell><cell>p</cell><cell>Mean</cell><cell>SD</cell><cell>Z</cell><cell>p</cell></row><row><cell>RUTGBL</cell><cell cols="2">0.270</cell><cell cols="3">0.167 n/a</cell><cell>n/a</cell><cell cols="3">0.408 0.30 n/a</cell><cell>n/a</cell><cell>0.206</cell><cell>0.16 n/a</cell><cell>n/a</cell></row><row><cell>RUTGAS1</cell><cell cols="2">0.278</cell><cell cols="2">0.168</cell><cell>-3.01</cell><cell>0.003</cell><cell cols="2">0.430 0.31</cell><cell cols="2">-2.21 0.027</cell><cell>0.216</cell><cell>0.17</cell><cell>-4.25</cell><cell>0.000</cell></row><row><cell>RUTGLS1</cell><cell cols="2">0.279</cell><cell cols="2">0.169</cell><cell>-1.99</cell><cell>0.046</cell><cell cols="2">0.424 0.32</cell><cell cols="2">-1.29 0.196</cell><cell>0.218</cell><cell>0.17</cell><cell>-3.36</cell><cell>0.001</cell></row><row><cell cols="3">RUTGWS1 0.281</cell><cell cols="2">0.166</cell><cell>-2.96</cell><cell>0.003</cell><cell cols="2">0.426 0.31</cell><cell cols="2">-1.88 0.060</cell><cell>0.219</cell><cell>0.17</cell><cell>-4.04</cell><cell>0.000</cell></row><row><cell>RUTGUS1</cell><cell cols="2">0.282</cell><cell cols="2">0.166</cell><cell>-2.49</cell><cell>0.013</cell><cell cols="2">0.440 0.31</cell><cell cols="2">-2.43 0.015</cell><cell>0.222</cell><cell>0.17</cell><cell>-4.35</cell><cell>0.000</cell></row><row><cell cols="3">RUTGUG1 0.286</cell><cell cols="2">0.173</cell><cell>-2.93</cell><cell>0.003</cell><cell cols="2">0.458 0.31</cell><cell cols="2">-2.78 0.005</cell><cell>0.228</cell><cell>0.17</cell><cell>-4.96</cell><cell>0.000</cell></row><row><cell>RUTGBF3</cell><cell cols="2">0.287</cell><cell cols="2">0.206</cell><cell>-0.90</cell><cell>0.368</cell><cell cols="2">0.480 0.36</cell><cell cols="2">-1.76 0.078</cell><cell>0.248</cell><cell>0.22</cell><cell>-2.23</cell><cell>0.026</cell></row><row><cell>RUTGALL</cell><cell cols="2">0.299</cell><cell cols="2">0.182</cell><cell>-1.51</cell><cell>0.132</cell><cell cols="2">0.494 0.31</cell><cell cols="2">-2.37 0.018</cell><cell>0.253</cell><cell>0.19</cell><cell>-2.58</cell><cell>0.010</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,72.00,327.56,439.42,203.99"><head>Table 6 . CF2 Comparison with Revised Baseline and Runs with Terms from Three Elicitations and Three Combinations of Terms</head><label>6</label><figDesc>(all runs used equal weighting of original query and user generated terms)</figDesc><table coords="12,72.00,369.85,383.47,161.71"><row><cell>Run name</cell><cell cols="2">R-Precision</cell><cell cols="2">Precision at 10</cell><cell cols="2">Mean Average Precision</cell></row><row><cell></cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>RUTGBL</cell><cell>0.270</cell><cell>0.167</cell><cell>0.408</cell><cell>0.3</cell><cell>0.206</cell><cell>0.16</cell></row><row><cell>Q1</cell><cell>0.290</cell><cell>0.178</cell><cell>0.498*</cell><cell>0.325</cell><cell>0.236</cell><cell>0.183</cell></row><row><cell>Q2</cell><cell>0.274</cell><cell>0.181</cell><cell>0.474*</cell><cell>0.321</cell><cell>0.223</cell><cell>0.181</cell></row><row><cell>Q3</cell><cell>0.295</cell><cell>0.164</cell><cell>0.498**</cell><cell>0.303</cell><cell>0.237**</cell><cell>0.175</cell></row><row><cell>Q1Q2</cell><cell>0.298*</cell><cell>0.182</cell><cell>0.514**</cell><cell>0.326</cell><cell>0.248**</cell><cell>0.190</cell></row><row><cell>Q1Q3</cell><cell>0.313*</cell><cell>0.176</cell><cell>0.538***</cell><cell>0.314</cell><cell>0.263**</cell><cell>0.186</cell></row><row><cell>Q1Q2Q3</cell><cell>0.314**</cell><cell>0.179</cell><cell>0.564***</cell><cell>0.304</cell><cell>0.268**</cell><cell>0.190</cell></row><row><cell cols="5">Q1 -"Please describe what you already know about this topic."</cell><cell></cell><cell></cell></row><row><cell cols="6">Q2 -"What sort of information would you like to have as a result of this search?"</cell><cell></cell></row><row><cell cols="6">Q3 -"Please input any additional keywords that describe this topic."</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,72.00,232.76,437.97,155.17"><head>Table 7 . CF2 Selected Comparison of Combinations of Terms from Three Elicitations (all runs used equal weighting of original query and user generated terms)</head><label>7</label><figDesc></figDesc><table coords="13,84.06,261.19,375.17,126.75"><row><cell>RUNS</cell><cell>r-precision</cell><cell>precision@10</cell><cell>MAP</cell></row><row><cell>Q1 vs. Q1Q3</cell><cell>0.290 vs. 0.313 p &lt; .05</cell><cell>0.498 vs. 0.538 n/s</cell><cell>0.236 vs. 0.263 p &lt; .01</cell></row><row><cell>Q3 vs. Q1Q3</cell><cell>0.295 vs. 0.313 n/s</cell><cell>0.498 vs. 0.538 n/s</cell><cell>0.237 vs. 0.263 n/s</cell></row><row><cell>Q1Q2 vs. Q1Q3</cell><cell>0.298 vs. 0.313 n/s</cell><cell>0.514 vs. 0.538 n/s</cell><cell>0.248 vs. 0.263 n/s</cell></row><row><cell>Q1Q2 vs, Q1Q2Q3</cell><cell>0.298 vs. 0.314 n/s</cell><cell>0.514 vs. 0.564 p &lt; .01</cell><cell>0.248 vs. 0.268 P &lt; .05</cell></row><row><cell>Q1Q3 vs. Q1Q2Q3</cell><cell>0.313 vs. 0.314 n/s</cell><cell>0.538 vs. 0.564 n/s</cell><cell>0.263 vs. 0.268 n/s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="14,72.00,246.56,444.67,93.19"><head>Table 8 . Comparison of Combination Runs with Baseline and Baseline Plus Pseudo-RF</head><label>8</label><figDesc></figDesc><table coords="14,77.40,264.25,390.02,75.51"><row><cell>Run name</cell><cell cols="2">R Precision</cell><cell cols="2">Precision at 10</cell><cell cols="2">Mean Average Precision</cell></row><row><cell></cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>RUTBE</cell><cell>0.334</cell><cell>0.206</cell><cell>0.530</cell><cell>0.367</cell><cell>0.302</cell><cell>0.230</cell></row><row><cell>RUTIN</cell><cell>0.243</cell><cell>0.183</cell><cell>0.400</cell><cell>0.311</cell><cell>0.195</cell><cell>0.185</cell></row><row><cell>RUTGBL</cell><cell>0.270</cell><cell>0.167</cell><cell>0.408</cell><cell>0.30</cell><cell>0.206</cell><cell>0.16</cell></row><row><cell>RUTGBF3</cell><cell>0.287</cell><cell>0.206</cell><cell>0.480</cell><cell>0.36</cell><cell>0.248</cell><cell>0.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.76,711.09,117.71,8.74"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We'd like to thank the other members of the <rs type="institution">Information Interaction Lab</rs> who worked on this project: <rs type="person">Iliana Chaleva</rs>, <rs type="person">Li Hui</rs>, <rs type="person">Narges Kasiri</rs>, <rs type="person">Ying-Hsang Liu</rs>, <rs type="person">Marina Malysheva</rs>, and <rs type="person">Xiangmin Zhang</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="15,93.60,304.01,65.29,12.58;15,72.00,322.73,420.02,10.46;15,72.00,336.20,282.26,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,167.37,322.73,320.35,10.46">Anomalous states of knowledge as a basis for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>References Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,72.00,336.20,198.08,10.80">Canadian Journal of Information Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="133" to="134" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,356.33,378.31,10.46;15,72.00,370.13,466.00,10.46;15,72.00,383.93,463.45,10.46;15,72.00,397.40,405.29,10.80;15,72.00,411.53,119.98,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,276.07,370.13,261.93,10.46;15,72.00,383.93,353.60,10.46">Relevance Feedback versus Local Context Analysis as Term Suggestion Devices: Rutgers&apos; TREC-8 Interactive Track Experience</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Head</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lobash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Savage-Knepshield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,172.02,397.40,183.98,10.80">The Eighth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<publisher>GPO</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="565" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,431.33,428.77,10.46;15,72.00,444.80,456.24,10.80;15,72.00,458.60,124.96,10.80" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<title level="m" coord="15,463.41,431.33,37.36,10.46;15,72.00,444.80,456.24,10.80;15,72.00,458.60,25.80,10.80">Rutgers interactive track at TREC 2002. Proceedings of the Eleventh Text Retrieval Conference (TREC 2002)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,478.73,453.53,10.46;15,72.00,492.20,448.59,10.80;15,72.00,506.00,266.52,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,486.02,478.73,39.51,10.46;15,72.00,492.53,287.45,10.46">Rutgers&apos; HARD and web interactive track experiences at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,367.86,492.20,152.73,10.80;15,72.00,506.00,167.36,10.80">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,526.13,461.04,10.46;15,72.00,539.60,282.30,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,308.28,526.13,224.76,10.46;15,72.00,539.93,66.83,10.46">Ask for information retrieval part ii. Results of a design study</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">N</forename><surname>Oddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,146.64,539.60,124.17,10.80">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="164" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,559.40,431.20,10.80;15,72.00,573.20,451.55,10.80;15,72.00,587.33,107.40,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,343.93,559.73,141.00,10.46">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,72.00,573.20,397.73,10.80">Proceedings of the ACM Conference on Research in Information Retrieval (SIGIR)</title>
		<meeting>the ACM Conference on Research in Information Retrieval (SIGIR)<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">2002. August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,607.13,409.39,10.46;15,72.00,620.60,445.18,10.80;15,72.00,634.40,124.96,10.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,271.66,607.13,209.73,10.46;15,72.00,620.93,128.49,10.46">University of North Carolina&apos;s HARD track experiments at TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">D</forename><surname>Dollu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,208.92,620.60,308.26,10.80;15,72.00,634.40,25.80,10.80">Proceedings of the Thirteenth Text Retrieval Conference (TREC 2004)</title>
		<meeting>the Thirteenth Text Retrieval Conference (TREC 2004)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,654.53,437.16,10.46;15,72.00,668.00,459.00,10.80;15,72.00,681.80,447.66,10.80;15,72.00,695.93,81.06,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,343.29,654.53,165.87,10.46;15,72.00,668.33,232.50,10.46">The loquacious user: A documentindependent source of terms for query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">D</forename><surname>Dollu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,312.60,668.00,218.40,10.80;15,72.00,681.80,443.38,10.80">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;05)</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;05)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08-15">2005. August 15-19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,75.17,413.61,10.46;16,72.00,88.97,427.87,10.46;16,72.00,102.77,351.99,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,271.25,75.17,214.36,10.46;16,72.00,88.97,228.94,10.46">A case for interaction: A study of interactive information retrieval behavior and effectiveness</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koenemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,321.64,88.97,178.24,10.46;16,72.00,102.77,198.95,10.46">Proceedings of the Human Factors in Computing Systems Conference (CHI&apos;96)</title>
		<meeting>the Human Factors in Computing Systems Conference (CHI&apos;96)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,122.57,460.29,10.46;16,72.00,136.04,452.10,10.80;16,72.00,149.84,216.30,10.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,409.60,122.57,122.69,10.46;16,72.00,136.37,122.94,10.46">TREC2004 Robust Track Experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,326.50,136.04,197.60,10.80;16,72.00,149.84,53.02,10.80">Proceedings of the Twelve Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Twelve Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2004. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,169.97,452.28,10.46;16,72.00,183.44,306.60,10.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,248.68,169.97,275.60,10.46;16,72.00,183.77,58.50,10.46">Automatic Discovery of Similarity Relationships through Web Mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,138.96,183.44,122.53,10.80">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="149" to="166" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,203.57,452.79,10.46;16,72.00,217.04,456.68,10.80" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,255.06,203.57,269.73,10.46;16,72.00,217.37,103.56,10.46">Mining Context Specific Similarity Relationships Using The World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,197.99,217.04,325.54,10.80">proceedings of 2005 Conference on Human Language Technologies</title>
		<meeting>2005 Conference on Human Language Technologies</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
