<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,62.18,72.05,485.36,16.59;1,130.00,91.98,349.73,16.59">When Less is More: Relevance Feedback Falls Short and Term Expansion Succeeds at HARD 2005</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_hxzUBmG">
					<orgName type="full">SPAWARSYSCEN-SD</orgName>
				</funder>
				<funder>
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.96,137.55,78.34,11.06"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
							<email>fdiaz@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.89,137.55,64.87,11.06"><forename type="first">James</forename><surname>Allan</surname></persName>
							<email>allan]@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<postCode>01003</postCode>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,62.18,72.05,485.36,16.59;1,130.00,91.98,349.73,16.59">When Less is More: Relevance Feedback Falls Short and Term Expansion Succeeds at HARD 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0687955EA9D8325A0C6704F124884219</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We used clarification forms to study passage term feedback. When compared against pseudo-relevance feedback with an extremely large external corpus, we found that passage feedback resulted in a reduction in performance while term feedback significantly improved recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">OVERVIEW</head><p>UMass tested several new techniques in the HARD track this year. First, we developed a new baseline pseudo-relevance feedback technique based on expanding from a large, external corpus <ref type="bibr" coords="1,98.41,336.38,9.22,7.86" target="#b5">[6]</ref>. Our results indicate that externally expanding a query can result in improvements over passage feedback. Second, we present a new feedback technique which incorporates both relevance and non-relevance information.</p><p>Our results indicate that this regularization-based technique can improve performance when used in conjunction with traditional feedback techniques. Third, we present two successful term feedback techniques. Our most successful technique exploits a structured query language in order to significantly improve recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BASELINE ALGORITHMS</head><p>We used the Indri retrieval engine for retrieval experiments <ref type="bibr" coords="1,81.05,476.94,9.22,7.86" target="#b6">[7]</ref>. We did not submit any runs which did not perform pseudo-relevance feedback. We experimented with a mixture of relevance models for our baseline ranking algorithm (also introduced in our Robust runs this year) <ref type="bibr" coords="1,280.61,508.32,9.22,7.86" target="#b5">[6]</ref>. Readers should consult the refered work for a more thorough description of parameters. Mixture parameters were set to P (aquaint) = 1 for MASSbaseTRM3 and P (bignews) = 1 for MASSbaseTEE3. No dependence models were used in our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CLARIFICATION FORMS</head><p>Our clarification form consisted of several pages of dialog with the searcher. This dialog followed three phases. In the first phase, the searcher was presented with passages from which to judge document relevance. The second phase consisted of term-based feedback; searchers were asked to judge the expected frequency of terms in relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Passage Presentation</head><p>In previous years, we found that passages acted as suitable surrogates for documents when being judged for relevance <ref type="bibr" coords="1,53.80,700.73,9.22,7.86" target="#b0">[1]</ref>. We divided each of the top 5 documents in the MASS-baseEE3 run into 150-word, half-overlapping passages. We then ranked all of the passages according to query likelihood. The top-ranked passage was considered the document surrogate. Searchers were asked to judge documents as "definitely relevant", "probably relevant", "probably not relevant", "definitely not relevant", and "can't tell". An example page from the passage feedback phase is shown in Figure <ref type="figure" coords="1,345.89,275.53,3.59,7.86" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Term Presentation</head><p>We conducted an informal study to determine which structured operators were most helpful during query reformulation. The results of this study indicated that one successful technique was to augment the original query with several concept structures. For example, if our original query were "Iraq-Iran War", the reformulated query would include a node representing concepts such as "cities in Iraq", "cities in Iran", "politicians in Iraq", and "politicians in Iran".</p><p>We presented 15 terminological feedback pages. Each feedback page in our form started by requesting a judgment for some query concept extracted from the query or-if candidates from the query were exhausted-from the initial retrieval. Terms taken from the initial retrieval were filtered to not include named entities, single letters, or numbers. Because we only presented 15 terms, we ranked all candidates according to their Clarity <ref type="bibr" coords="1,425.30,467.74,9.22,7.86" target="#b1">[2]</ref>. These 15 terms represented the core "concepts" of the query.</p><p>In addition to the core terms describing the concept, we were interested in presenting terms for clarifying the concepts themselves. For example, if one of the extracted concepts was "salsa", we would like to have the user disambiguate which sense of "salsa" was intended. One method of presenting alternative senses is to use related terms. We accomplished this by first searching WordNet for synsets containing the concept term <ref type="bibr" coords="1,434.09,561.88,9.22,7.86" target="#b4">[5]</ref>. We selected the first term from each of the synsets. If the concept word was not found in WordNet or if there were fewer than five synsets detected, we issued the concept word as a query and padded out the related term list with the most frequent terms in this retrieval. Figure <ref type="figure" coords="1,353.83,624.65,4.61,7.86" target="#fig_1">2</ref> depicts an example term feedback page. Searchers were asked to indicate the expected frequency of terms in relevant documents. Since our core retrieval algorithm is based on term frequencies, we felt requesting this information would be more helpful than asking searchers to judge the more ambiguous concept of "term relevance". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PASSAGE FEEDBACK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Passage Relevance Model</head><p>Our baseline relevance feedback method builds a relevance model for each set of relevant documents,</p><formula xml:id="formula_0" coords="2,109.26,384.63,183.64,23.59">P (w|θR) = 1 |R| X D∈R P (w|θD)<label>(1)</label></formula><p>After ranking terms in decreasing order of probability, we use the top terms and weights combined with the original query. We then perform retrieval with this expanded model. This run is labeled MASSpassRM3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regularizing the Retrieval Scores</head><p>In previous work, we found that a second pass of regularizing retrieval scores often significantly improves ranking <ref type="bibr" coords="2,53.80,502.39,9.22,7.86" target="#b3">[4]</ref>. In order to use this framework, we first normalize retrieval scores using shift and scale normalization. Then, if any judged documents occur in the second retrieval, we replace the retrieval score with 1 for documents marked relevant and 0 for documents marked non-relevant. This run is labeled MASSpassRM3R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TERMINOLOGICAL FEEDBACK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Regularizing Term Weights</head><p>We adopt the regularization framework used in Section 4.2 to regularize the expanded term weights. In this section, we will sketch the notions of term graphs and term weights.</p><p>A term graph contains one node for each term in the set of expansion terms. In our case, we used the set of terms in the EE3 model. The edges in the graph are meant to represent the relatedness of two terms. We use a language model approach to measure the relatedness. For each term, we construct a language model based on terms frequently co-occurring with the candidate term in some fixed window of terms. Previous work has found bigram-based distributional similarity to be a compelling method for quantifying term relationships <ref type="bibr" coords="2,392.10,366.12,9.22,7.86" target="#b2">[3]</ref>. Our method relaxes the proximity to include words within some window. We accomplished this by retrieving fixed-length passages for each term. We then built a language model out of the weighted combination of maximum likelihood passage models,</p><formula xml:id="formula_1" coords="2,365.15,424.15,186.84,23.60">P (w|θq) = X T ∈R P (w|θT ) P (q|θT ) Z (<label>2</label></formula><formula xml:id="formula_2" coords="2,551.99,429.95,3.93,7.86">)</formula><p>where Z is a normalizer over all retrieved passages, T ∈ R.</p><p>We use the multinomial diffusion kernel to compare language models, in turn defining our edge weights <ref type="bibr" coords="2,481.25,476.85,9.22,7.86" target="#b3">[4]</ref>. Figure <ref type="figure" coords="2,525.64,476.85,4.61,7.86">3</ref> shows an example graph. Once this graph is constructed, we can regularize the term weights used in MASSbaseEE3 following the feedback the method described in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Building Structured Queries</head><p>As mentioned in Section 3.2, we conducted an informal study to determine useful patterns in structured querying. One important technique we found was the use of concepts. Instead of querying with a set of terms, we would, for each concept in the query, expand the term into conceptually related terms. We developed the concept-feedback template presented in Figure <ref type="figure" coords="2,397.94,613.58,3.59,7.86">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">FREE TEXT FEEDBACK</head><p>We also provided the searcher with an opportunity to manually submit additional query terms in a free text box. No searchers managed to reach this phase of the clarification process so we exclude it from our results.  We used the Robust 2004 topics and corpus for training parameters. We only considered the title field for our training experiments. We excluded topics used in Robust 2005 evaluation. We simulated the HARD user study by asking 10 graduate students in our department to complete clarification forms for 10 topics in three minutes. This resulted in a total of 100 topics to use for training feedback algorithm parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">TRAINING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Baseline</head><p>Our baseline pseudo-relevance feedback retrieval used 10 feedback documents to generate the relevance model, 25 terms from the relevance model to interpolate with the original query, and a weight of 0.20 on the original query. When using the external corpus, the retrieval used 25 feedback documents to generate the relevance model, 50 terms from the relevance model to interpolate with the original query, and a weight of 0.10 on the original query. The values of these trained parameters indicate that we should have more confidence in the relevance model built from the external corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Passage Feedback</head><p>We trained our passage Feedback algorithms using document judgments from the Robust 2004 topic set and corpus. This allowed us to train on the complete set of 200 topics (after having excluded the topics used in Robust 2005). Although our clarification forms resulted in passage feedback, we found this training technique successful in previous years <ref type="bibr" coords="4,53.80,700.73,9.22,7.86" target="#b0">[1]</ref>. Our feedback algorithm used only relevant documents to generate the relevance model, 50 terms from the relevance model to interpolate with the original query, and a weight of 0.10 on the original query. Our regularization used a 5nearest neighbor graph, t = 0.70, and α = 0.20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Term Feedback</head><p>We trained both of our term feedback algorithms using the in-house clarification form data. In both cases, only terms labeled "always" and "never" were used for feedback.</p><p>Our term regularization model had two components to train: term language models and term regularization. When build the per-term language models, we used the top 100 50word passage retrieved. Each graph consisted of the top 100 terms from the relevance model. Our regularization used a 75-nearest neighbor graph, t = 0.50, and α = 0.50.</p><p>Our structured term feedback model had two parameters. The original query was given the most weight with λ1 = 0.95. Within the concept node, we found that more weight was placed on the related terms with λ2 = 0.30. We performed a post-processing step where all terms labeled "never" were removed from the structured query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RESULTS</head><p>We submitted two baseline runs and four post-clarification runs. All runs used the title field only. The results are presented in Table <ref type="table" coords="4,382.65,656.83,3.59,7.86" target="#tab_0">1</ref>. We present standard retrieval measures including the official R-Precision measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Baseline Runs</head><p>Our baselines performed similarly to our Robust track runs. Expanding the query using only the Aquaint corpus resulted in an R-Precision much lower than external expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Passage Feedback</head><p>The feedback runs perform better than our pseudo-relevance feedback baseline (MASSbaseRM3). Note that candidate feedback documents were taken from the external corpus. As expected, regularization boosted the relevance feedback performance across almost all measures.</p><p>One result for passage feedback is surprising. Feedback does not outperform external expansion (MASSbaseEE3). This result seems to indicate that true relevance feedback underperforms pseudo-relevance feedback when we gather an external corpus of sufficient size and quality. We are currently conducting experiments to determine the precise situations when external expansion is better than true feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Term Feedback</head><p>Our best runs incorporated term feedback. This is surprising since our previous work indicated that term feedback often did not improve retrieval; only free text reformulation provided performance gains. Because we have adjusted both our term selection and term incorporation scheme from previous years, it is difficult to determine which factor is likely to explain the improvement.</p><p>Several improvements over the MASSbaseEE3 baseline should be pointed out. First, although both term feedback methods improved the official R-Precision metric, only structured feedback improved mean average precision. Second, the only significant improvement over our external expansion baseline was in the number of relevant documents retrieved. There are several reasons for this. Term presentation always occurred after document presentation. Therefore, we tended to have far fewer term judgments. In several cases, searchers did not even get to the term feedback pages. Since additional term information existed for fewer topics, then, it was difficult to measure the significance of this improvement.</p><p>That the number of relevant documents significantly rose is contrary to previous year's results where additional terms only improved high precision measures and sometimes reduced recall. We believed that the searchers were adding terms recognized as discriminative in the top-presented passages on our form or such terms seen on other sites' forms in the study. The result would be to construct a high-precision query for retrieving those documents already included on the clarification form(s). We constrained searchers this year to a small set of terms unlikely to be prone to such over-fitting. Because of it's query-directed nature and the named-entity filtering, the candidate set of terms was much more general. Combined with our structured query, we could carefully expand these general concepts to retrieve a larger mass of relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSIONS</head><p>Our experiments this year provided some compelling results. First, we demonstrated that passage-level feedback sometimes under-performs pseudo-relevance feedback using an external corpus. This is interesting because it means that there are cases where document feedback might be less useful than exploiting some larger or higher quality corpus. Second, we showed that term feedback can be successfully used to improve recall tasks whereas previous results demonstrated only precision gains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,216.94,315.53,175.84,7.89;2,88.86,53.80,431.99,238.30"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Passage Feedback Interface.</figDesc><graphic coords="2,88.86,53.80,431.99,238.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,222.55,649.34,164.62,7.89;3,88.86,113.88,432.00,512.02"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Term Feedback Interface.</figDesc><graphic coords="3,88.86,113.88,432.00,512.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,53.80,200.86,502.11,7.89;4,53.80,211.32,502.11,7.89;4,53.80,221.78,502.11,7.89;4,53.80,232.25,502.12,7.89;4,53.80,242.71,145.53,7.89;4,53.80,262.31,136.51,7.47;4,96.16,272.77,461.31,7.47;4,199.72,283.23,357.75,7.47;4,199.72,293.69,14.12,7.47;4,195.02,304.15,4.71,7.47;4,86.75,314.61,4.71,7.47"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Term Regularization. The figure on the left represents co-occurrence information of terms in the expansion model. Highlighted nodes represent terms presented to the user for feedback. The figure on the right represents the pre-regularization term weights. The weight of highlighted nodes depends on the user feedback (0 for non-relevant terms; 1 for relevant terms). The weight of all other terms in the graph is equal to the expansion model weight. #weight( LAMBDA1 #combine(EE) (1-LAMBDA1) #combine( #wsyn( LAMBDA2 #1(PHRASE1) (1-LAMBDA2) #syn(RELATEDTERM11 RELATEDTERM12...)) #wsyn( LAMBDA2 #1(PHRASE2) (1-LAMBDA2) #syn(RELATEDTERM21 RELATEDTERM22...)) ... ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,53.80,217.48,502.12,336.98"><head>Table 1 :</head><label>1</label><figDesc>Results. Comparisons between official baseline and modified retrieval runs. The first two columns represent local pseudo-relevance feedback (MASSbaseTRM3) and external pseudo-relevance feedback (MASSbaseTEE3). The second two columns represent passage feedback (MASSpsgRM3) and regularized passage feedback (MASSpsgRM3R). The final two columns represent feedback using term regularization (MASStrmR) and structured query construction (MASStrmS). Bold numbers represent the best performance among our systems. Statistical improvements were computed with respect to both baselines. We used a Wilcoxon test and indicate instances where p &lt; 0.05. A superscript * indicates improvement over MASSbaseTRM3 and a superscript † indicates improvement over MASSbaseTEE3.</figDesc><table coords="5,82.97,217.48,443.76,234.52"><row><cell></cell><cell cols="2">MASSbaseTRM3 MASSbaseTEE3</cell><cell cols="2">MASSpsgRM3 MASSpsgRM3R</cell><cell cols="2">MASStrmR MASStrmS</cell></row><row><cell>Retrieved:</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell><cell>50000</cell></row><row><cell>Relevant:</cell><cell>6561</cell><cell>6561</cell><cell>6561</cell><cell>6561</cell><cell>6561</cell><cell>6561</cell></row><row><cell>Relret:</cell><cell>4115</cell><cell>4357</cell><cell>4241</cell><cell>4241</cell><cell>4877  *</cell><cell>4909  *  †</cell></row><row><cell>IntPrec@0.00</cell><cell>0.6302</cell><cell>0.7088</cell><cell>0.7257  *</cell><cell>0.7013</cell><cell>0.7603  *</cell><cell>0.7584  *</cell></row><row><cell>IntPrec@0.10</cell><cell>0.4548</cell><cell>0.5631</cell><cell>0.5049</cell><cell>0.5214</cell><cell>0.5798  *</cell><cell>0.5925  *</cell></row><row><cell>IntPrec@0.20</cell><cell>0.3997</cell><cell>0.4897</cell><cell>0.4336</cell><cell>0.4478</cell><cell>0.4917  *</cell><cell>0.5270  *</cell></row><row><cell>IntPrec@0.30</cell><cell>0.3547</cell><cell>0.4392</cell><cell>0.3791</cell><cell>0.3861</cell><cell>0.4222  *</cell><cell>0.4711  *</cell></row><row><cell>IntPrec@0.40</cell><cell>0.2944</cell><cell>0.3783</cell><cell>0.3221</cell><cell>0.3310</cell><cell>0.3643  *</cell><cell>0.3988  *</cell></row><row><cell>IntPrec@0.50</cell><cell>0.2474</cell><cell>0.3076</cell><cell>0.2718</cell><cell>0.2808</cell><cell>0.3001  *</cell><cell>0.3202  *</cell></row><row><cell>IntPrec@0.60</cell><cell>0.2011</cell><cell>0.2420</cell><cell>0.2146</cell><cell>0.2253</cell><cell>0.2379</cell><cell>0.2605  *</cell></row><row><cell>IntPrec@0.70</cell><cell>0.1625</cell><cell>0.1897</cell><cell>0.1659</cell><cell>0.1734</cell><cell>0.1902</cell><cell>0.2042  *</cell></row><row><cell>IntPrec@0.80</cell><cell>0.1111</cell><cell>0.1256</cell><cell>0.1091</cell><cell>0.1134</cell><cell>0.1250</cell><cell>0.1419  *</cell></row><row><cell>IntPrec@0.90</cell><cell>0.0524</cell><cell>0.0639</cell><cell>0.0497</cell><cell>0.0534</cell><cell>0.0659</cell><cell>0.0662  *</cell></row><row><cell>IntPrec@1.00</cell><cell>0.0068</cell><cell>0.0037</cell><cell>0.0031</cell><cell>0.0029</cell><cell>0.0057</cell><cell>0.0070</cell></row><row><cell>map</cell><cell>0.2445</cell><cell>0.3043</cell><cell>0.2688</cell><cell>0.2766  *</cell><cell>0.3019  *</cell><cell>0.3223  *</cell></row><row><cell>P@5</cell><cell>0.4360</cell><cell>0.5600</cell><cell>0.5160  *</cell><cell>0.5200  *</cell><cell>0.5640  *</cell><cell>0.5600  *</cell></row><row><cell>P@10</cell><cell>0.4300</cell><cell>0.5300</cell><cell>0.4780</cell><cell>0.4880</cell><cell>0.5320  *</cell><cell>0.5600  *</cell></row><row><cell>P@15</cell><cell>0.4013</cell><cell>0.5253</cell><cell>0.4707  *</cell><cell>0.4933  *</cell><cell>0.5093  *</cell><cell>0.5333  *</cell></row><row><cell>P@20</cell><cell>0.3970</cell><cell>0.5050</cell><cell>0.4610  *</cell><cell>0.4740  *</cell><cell>0.4940  *</cell><cell>0.5110  *</cell></row><row><cell>P@30</cell><cell>0.3780</cell><cell>0.4767</cell><cell>0.4347</cell><cell>0.4453  *</cell><cell>0.4647  *</cell><cell>0.4853  *</cell></row><row><cell>P@100</cell><cell>0.2848</cell><cell>0.3580</cell><cell>0.3256  *</cell><cell>0.3340  *</cell><cell>0.3566  *</cell><cell>0.3732  *</cell></row><row><cell>P@200</cell><cell>0.2251</cell><cell>0.2605</cell><cell>0.2454</cell><cell>0.2480  *</cell><cell>0.2731  *</cell><cell>0.2830  *</cell></row><row><cell>P@500</cell><cell>0.1363</cell><cell>0.1498</cell><cell>0.1430</cell><cell>0.1443</cell><cell>0.1614  *</cell><cell>0.1651  *</cell></row><row><cell>P@1000</cell><cell>0.0823</cell><cell>0.0871</cell><cell>0.0848</cell><cell>0.0848</cell><cell>0.0975  *</cell><cell>0.0982  *  †</cell></row><row><cell>rprec</cell><cell>0.2660</cell><cell>0.3291</cell><cell>0.3024  *</cell><cell>0.3082  *</cell><cell>0.3353  *</cell><cell>0.3547  *</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="10.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs>, in part by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs>, and in part by <rs type="funder">SPAWARSYSCEN-SD</rs> grant number <rs type="grantNumber">N66001-02-1-8903</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hxzUBmG">
					<idno type="grant-number">N66001-02-1-8903</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,331.02,204.79,193.41,7.86;6,331.02,215.25,221.31,7.86;6,331.02,225.71,218.10,7.86;6,331.02,236.17,166.15,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,525.87,215.25,26.46,7.86;6,331.02,225.71,123.33,7.86">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,473.14,225.71,75.99,7.86;6,331.02,236.17,137.83,7.86">Online Proceedings of 2004 Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.02,247.63,196.31,7.86;6,331.02,258.09,209.16,7.86;6,331.02,268.55,214.80,7.86;6,331.02,279.01,203.11,7.86;6,331.02,289.47,21.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,331.02,258.09,118.23,7.86">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,468.20,258.09,71.99,7.86;6,331.02,268.55,214.80,7.86;6,331.02,279.01,199.56,7.86">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.02,300.93,200.43,7.86;6,331.02,311.39,185.85,7.86;6,331.02,321.85,121.58,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,466.36,300.93,65.10,7.86;6,331.02,311.39,144.92,7.86">Similarity-based models of cooccurrence probabilities</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,482.74,311.39,34.12,7.86;6,331.02,321.85,33.44,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="43" to="69" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.02,333.30,221.36,7.86;6,331.02,343.76,196.66,7.86;6,331.02,354.23,223.70,7.86;6,331.02,364.69,72.70,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,366.60,333.30,141.98,7.86">Regularizing ad hoc retrieval scores</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,526.92,333.30,25.46,7.86;6,331.02,343.76,196.66,7.86;6,331.02,354.23,218.87,7.86">CIKM 2005: Proceedings of the fourteenth international conference on Information and knowledge management</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.02,376.14,213.11,7.86;6,331.02,386.60,110.88,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,415.14,376.14,128.99,7.86;6,331.02,386.60,34.18,7.86">WordNet: An Electronic Lexical Database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.02,398.06,210.74,7.86;6,331.02,408.52,205.89,7.86;6,331.02,418.98,194.21,7.86;6,331.02,429.44,210.64,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,331.02,408.52,205.89,7.86;6,331.02,418.98,108.81,7.86">Umass at robust 2005: Using mixtures of relevance models for query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,458.66,418.98,66.58,7.86;6,331.02,429.44,182.31,7.86">The Twelth Text REtrieval Conference (TREC 2005) Notebook</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,331.02,440.90,218.19,7.86;6,331.02,451.36,195.49,7.86;6,331.02,461.82,209.10,7.86;6,331.02,472.28,170.04,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,331.02,451.36,195.49,7.86;6,331.02,461.82,62.83,7.86">Indri: A language model-based serach engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,412.25,461.82,127.87,7.86;6,331.02,472.28,141.95,7.86">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
