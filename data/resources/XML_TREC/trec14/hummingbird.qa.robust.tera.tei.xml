<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.37,109.17,433.25,18.08;1,287.17,131.09,37.66,18.08;1,112.38,153.00,234.96,18.08;1,347.35,151.23,22.11,12.55;1,376.41,153.00,123.20,18.08">Enterprise, QA, Robust and Terabyte Experiments with Hummingbird SearchServer TM at TREC 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-02-05">February 5, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,264.32,188.07,83.36,10.46"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<email>stephen.tomlinson@hummingbird.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hummingbird Ottawa</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.37,109.17,433.25,18.08;1,287.17,131.09,37.66,18.08;1,112.38,153.00,234.96,18.08;1,347.35,151.23,22.11,12.55;1,376.41,153.00,123.20,18.08">Enterprise, QA, Robust and Terabyte Experiments with Hummingbird SearchServer TM at TREC 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-02-05">February 5, 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">24FCD4E0CE54B5D85B2A852C37117FD6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird participated in 6 tasks of TREC 2005: the email known-item search task of the Enterprise Track, the document ranking task of the Question Answering Track, the ad hoc topic relevance task of the Robust Retrieval Track, and the adhoc, efficiency and named page finding tasks of the Terabyte Track. In the email known-item task, SearchServer found the desired message in the first 10 rows for more than 80% of the 125 queries. In the document ranking task, SearchServer returned an answering document in the first 10 rows for more than 90% of the 50 questions. In the robustness task, SearchServer found a relevant document in the first 10 rows for 88% of the 50 short (title) topics. In the terabyte adhoc and efficiency tasks, SearchServer found a relevant document in the first 10 rows for more than 90% of the 50 title topics. A new retrieval measure, First Relevant Score, is investigated; it is found to more accurately reflect known-item differences than reciprocal rank and to better reflect robustness across topics than the primary measure of the Robust track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,192.71,491.60,3.97,7.32" target="#foot_0">1</ref> is a toolkit for developing enterprise search and retrieval applications. The SearchServer kernel is also embedded in other Hummingbird products for the enterprise.</p><p>SearchServer works in Unicode internally <ref type="bibr" coords="1,269.94,516.59,10.52,10.46" target="#b3">[4]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval experimentation (TREC <ref type="bibr" coords="1,407.94,528.54,9.96,10.46" target="#b6">[7]</ref>, CLEF <ref type="bibr" coords="1,455.31,528.54,10.52,10.46" target="#b2">[3]</ref> and NTCIR <ref type="bibr" coords="1,525.61,528.54,10.79,10.46" target="#b4">[5]</ref>) have provided judged test collections for objective experimentation with SearchServer in more than a dozen languages.</p><p>This paper describes experimental work with SearchServer (experimental post-6.0 builds) for enterprise search (known-item search of a specific organization's emails), question answering (finding documents which contain the answer to a question), robust retrieval (robustness of ad hoc search across topics) and terabyte retrieval (adhoc search and named page finding on terabyte scales).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Measures</head><p>Traditionally, different retrieval measures have been used for "ad hoc" tasks, which seek relevant items for a topic, than for "known-item" tasks, which seek a particular known document. However, we argue that the known-item measures are not only applicable to ad hoc tasks, but that they are often preferable. For many ad hoc tasks, e.g. finding answer documents for questions, just one relevant item is needed. Also, the traditional ad hoc measures encourage retrieval of duplicate relevants, which does not correspond to user benefit.</p><p>The traditional known-item measures are very coarse, e.g. Success@10 is 1 or 0 for each topic, while reciprocal rank cannot produce a value between 1.0 and 0.5. This year, we've been investigating a new measure, "First Relevant Score" (defined below), which was introduced in <ref type="bibr" coords="2,409.30,145.22,9.96,10.46" target="#b7">[8]</ref>. We consider it our main measure for both ad hoc and known-item tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Primary Recall Measures</head><p>"Primary recall" is retrieval of the first relevant item for a topic. Primary recall measures include the following:</p><p>• First Relevant Score (FRS): For a topic, FRS is 1.08 1-r where r is the rank of the first row for which a desired page is found, or zero if a desired page was not found.</p><p>• Success@n (S@n): For a topic, Success@n is 1 if a desired page is found in the first n rows, 0 otherwise.</p><p>• Reciprocal Rank (RR): For a topic, RR is 1 r where r is the rank of the first row for which a desired page is found, or zero if a desired page was not found. "Mean Reciprocal Rank" (MRR) is the mean of the reciprocal ranks over all the topics. Interpretation of FRS : FRS is an estimate of the percentage of potential result list reading the system saved the user to get to the first relevant item, assuming that users are less and less likely to continue reading as they get deeper into the result list.</p><p>Comparison of First Relevant Score and Reciprocal Rank : Both FRS and RR are 1.0 if a desired page is found at rank 1. At rank 2, FRS is just 7 points lower (0.93), whereas RR is 50 points lower (0.50). At rank 3, FRS is another 7 points lower (0.86), whereas RR is 17 points lower (0.33). At rank 10, FRS is 0.50, whereas RR is 0.10. FRS is greater than RR for ranks 2 to 52 and lower for ranks 53 and beyond.</p><p>Connection of First Relevant Score to Success@10 : The base of 1.08 makes FRS 0.5 at rank 10 which in practice makes FRS a good predictor of Success@10 (e.g. if FRS is 0.8, Success@10 will probably be close to 40/50). This paper lists FRS, S@1, S@10 and MRR for all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Secondary Recall Measures</head><p>"Secondary recall" is retrieval of the additional relevant items for a topic (after the first one). Secondary recall measures place most of their weight on these additional relevant items. They are just considered for ad hoc tasks.</p><p>• Precision@n: For a topic, "precision" is the percentage of retrieved documents which are relevant.</p><p>"Precision@n" is the precision after n documents have been retrieved. This paper lists P20 (Precision@20) for some runs because it was one of the main measures for the Terabyte track.</p><p>• Average Precision (AP): For a topic, AP is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). By convention, AP is based on the first 1000 retrieved documents for the topic. The score ranges from 0.0 (no relevants found) to 1.0 (all relevants found at the top of the list). "Mean Average Precision" (MAP) is the mean of the average precision scores over all of the topics (i.e. all topics are weighted equally).</p><p>• Geometric MAP (GMAP): GMAP was the primary measure for the Robust Track this year (defined in <ref type="bibr" coords="3,109.39,85.45,14.76,10.46" target="#b12">[14]</ref>). It is based on "Log Average Precision" which for a topic is the natural log of the sum of 0.00001 and the average precision. GMAP is -0.00001 plus the exponential of the mean log average precision. (We will argue in the Robust section that FRS is a better measure of robustness than the GMAP measure.)</p><p>-The organizers later made a revision to the GMAP definition (which is believed to be minor); this paper just uses the original definition.</p><p>• GMAP' : We also define a linearized log average precision measure (denoted GMAP') which linearly maps the 'log average precision' values to the [0,1] interval. For statistical significance purposes, GMAP' gives the same results as GMAP, and it has advantages such as that the individual topic differences are in the familiar -1.0 to 1.0 range and are on the same scale as the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">H and J Modifications</head><p>We attach an H prefix to the measure (e.g. HFRS, HS@10, HMAP, etc.) when the measure is just counting "highly relevant" documents as relevant. (The H modifier is just applicable to the ad hoc tasks of the Robust and Terabyte tracks, for which the judgements distinguished highly relevants from ordinary relevants.)</p><p>We attach a J suffix to the measure (e.g. FRSJ, MAPJ) when unjudged documents are omitted rather than being assumed non-relevant (an approach investigated by <ref type="bibr" coords="3,347.43,305.60,10.52,10.46" target="#b0">[1]</ref> for different measures). The J modifier is only applicable for ad hoc tasks, not known-item tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Difference Tables</head><p>For comparison tables such as Table <ref type="table" coords="3,233.47,366.34,3.87,10.46" target="#tab_1">2</ref>, the columns are as follows:</p><p>• "Expt" specifies the experiment (the codes of the runs being compared are in parentheses).</p><p>• "∆" is the difference of the mean scores of the two runs being compared (the column heading says for which retrieval measure).</p><p>• "95% Conf" is an approximate 95% confidence interval for the mean difference (calculated from plus/minus twice the standard error of the mean difference). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact (on average), though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the experimental run scored higher, lower and tied (respectively) compared to the baseline run. These numbers should always add to the number of topics.</p><p>• "3 Extreme Diffs (Topic)" lists 3 of the individual topic differences, each followed by the topic number in brackets. The first difference is the largest one of any topic (based on the absolute value). The third difference is the largest difference in the other direction (so the first and third differences give the range of differences observed in this experiment). The middle difference is the largest of the remaining differences (based on the absolute value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Enterprise Track: Email Known-Item Search Experiments</head><p>The Enterprise Track was new to TREC this year. We participated in its Email Known-Item Search task. The collection to be searched was the 'lists' portion of the "W3C Test Collection". It consisted of the "W3C Public Mailing List Archives" crawled from lists.w3.org in June 2004. Uncompressed, the collection was 1,991,923,793 bytes and consisted of 198,394 documents. The average document size was 10,040 bytes (including HTML markup). For more details, see <ref type="bibr" coords="3,290.27,682.13,9.96,10.46" target="#b1">[2]</ref>.</p><p>For the known-item search queries (e.g. "studies of Web Accessibility for the Disabled"), the goal was to find the particular message the user was trying to retrieve (e.g. "http://lists.w3.org/Archives/Public/w3cwai-ig/2004AprJun/0111.html"). The organizers provided 25 training queries (for which the right answers were also given) and 125 test queries (for which the right answers were not released until after the submitted runs were due in August 2005). The queries and answers were based on contributions from the participants (including 10 which we contributed). The test queries were numbered from 26 to 150. For more details on the task, see <ref type="bibr" coords="4,129.40,145.22,14.61,10.46" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Indexing</head><p>Our indexing approach was the same as we used in the Web Track each of the previous three years (described in detail in <ref type="bibr" coords="4,122.70,205.97,10.79,10.46" target="#b8">[9]</ref>) except that a newer version of the software was used which may have contained an updated English lexicon for stemming.</p><p>Briefly: in addition to full-text indexing (except for a short stopword list and some tags), the custom text reader cTREC populated particular columns such as TITLE (if any), URL, META TITLE, META SUBJECT, META DESCRIPTION, META KEYWORDS and the first heading (e.g. first &lt;H1&gt;, &lt;H2&gt; or &lt;H3&gt; content). (Also, URL TYPE and URL DEPTH fields were populated, but were not used in the Enterprise experiments.) More details are in <ref type="bibr" coords="4,269.21,277.70,9.96,10.46" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Searching</head><p>The techniques used for the 5 submitted runs of August 2005 (plus one other unsubmitted run produced at that time) are described below. The SearchServer '2:3' relevance method was the same as described last year <ref type="bibr" coords="4,93.89,350.40,14.61,10.46" target="#b10">[11]</ref>. Briefly, SearchServer dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="4,145.57,362.36,10.52,10.46" target="#b5">[6]</ref> and dampens the inverse document frequency using an approximation of the logarithm. When doing morphological searching (e.g. inflections), these calculations are based on the stems of the terms (roughly speaking).</p><p>humEK05l: The submitted humEK05l run was a plain content search including linguistic expansion from English inflectional stemming. This run was the analog of the baseline humR05tl run described in the Robust section (including RELEVANCE METHOD '2:3' and RELEVANCE DLEN IMP 250); the enterprise run used the IS ABOUT predicate instead of the CONTAINS predicate (and hence the VECTOR GENERATOR was set to enable inflections instead of the TERM GENERATOR), but the relevance calculation was the same. This run used almost the same approach as the submitted humW04l run of last year <ref type="bibr" coords="4,473.00,458.00,15.50,10.46" target="#b10">[11]</ref> (this year's document length normalization was 250 instead of 500). Below is an example SearchSQL query. Note that the FT TEXT column indexed the content and also all of the non-content fields of the document source (which included the title and meta tags but not the url):</p><p>SELECT RELEVANCE('2:3') AS REL, DOCNO FROM W3CL WHERE (FT_TEXT IS_ABOUT 'studies of Web Accessibility for the Disabled') ORDER BY REL DESC; humEK05tl: The submitted humEK05tl run was the same as humEK05l except that it put an additional 10% weight on matches in the Title column and an additional 10% weight on phrase matches in the Title (via the CONTAINS predicate). Below is an example SearchSQL query. More details on the syntax are in the description of the humNP03pl run of <ref type="bibr" coords="4,253.24,621.39,14.61,10.46" target="#b9">[10]</ref> humEK05pl: The submitted humEK05pl run was the same as humEK05tl except that in place of the TITLE field it searched the ALL PROPS field which was a combination of the title, url, some meta tags and the first heading. This run was the analog of last year's humW04pl run <ref type="bibr" coords="5,385.65,344.57,15.50,10.46" target="#b10">[11]</ref> (except for the lower document length setting this year):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WHERE</head><p>(ALL_PROPS CONTAINS 'studies of Web Accessibility for the Disabled' WEIGHT 1) OR (ALL_PROPS IS_ABOUT 'studies of Web Accessibility for the Disabled' WEIGHT 1) OR (FT_TEXT IS_ABOUT 'studies of Web Accessibility for the Disabled' <ref type="bibr" coords="5,432.90,414.31,52.30,10.46">WEIGHT 10)</ref> humEK05p: The submitted humEK05p run was the same as humEK05pl except that inflections from stemming were disabled with SET VECTOR GENERATOR '' (which disables inflections for the IS ABOUT predicate). Note that inflections were not enabled for the CONTAINS predicate for any Enterprise run.</p><p>humEK05v3l: (This run was not submitted.) The humEK05v3l run was the same as humEK05t3l except that the extra weight on a phrase match in the title was removed:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WHERE</head><p>(TITLE IS_ABOUT 'studies of Web Accessibility for the Disabled' WEIGHT 3) OR (FT_TEXT IS_ABOUT 'studies of Web Accessibility for the Disabled' WEIGHT 10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table <ref type="table" coords="5,100.62,588.63,4.98,10.46">1</ref> lists the mean scores of the 5 submitted runs (plus 1 extra run). The baseline full-text search technique returned the desired message in the first 10 rows for 76% of the 125 test queries (95/125). With additional weight on the Title field, Success@10 increased to 82% (102/125); this increase was statistically significant according to Table <ref type="table" coords="5,207.11,624.49,4.98,10.46" target="#tab_1">2</ref> (i.e. the approximate 95% confidence interval of the "t (tl-l)" line of the ∆S10 section does not contain zero).</p><p>• Title weighting: Table <ref type="table" coords="5,198.64,658.37,4.98,10.46" target="#tab_1">2</ref> shows that the increase in First Relevant Score from the extra 10% weights on the title ("t" experiment) was sometimes dramatic, e.g. topic KI107 increased in score from 0.00 to 1.00. The negative impacts for FRS in the "t" experiment were not large (the biggest was a 21-point decrease on topic KI131 according to Table <ref type="table" coords="6,297.27,429.73,3.87,10.46" target="#tab_1">2</ref>, a fall from rank 4 to 8). The other title-weighting techniques ("p", "v3" and "t3") had larger negative per-topic impacts, but overall were pretty similar. It appears a small extra weight on the title is a reasonable general-purpose technique.</p><p>• Inflections from stemming: Table <ref type="table" coords="6,241.96,473.21,4.98,10.46" target="#tab_1">2</ref> shows that the increase in First Relevant Score from inflections from stemming ("l" experiment) was also sometimes dramatic, though there could also be large decreases on some topics. The mean increase in FRS was statistically significant (the increase did not quite pass the significance test for the other measures in Table <ref type="table" coords="6,307.12,509.08,3.87,10.46" target="#tab_1">2</ref>). Stemming may be a reasonable default behaviour, but the interface should probably give a user a way to disable it for particular terms.</p><p>One can see the advantages of First Relevant Score compared to Reciprocal Rank in Table <ref type="table" coords="6,472.66,542.06,3.87,10.46" target="#tab_1">2</ref>. For example, for the "l" experiment, reciprocal rank picks topic 67 as the largest decrease (83 points), though it is just a fall from rank 1 to 6; FRS considers this just a 32 point decrease. FRS picks topic 131 as the largest decrease (52 points), a bigger fall from rank 3 to 15; RR considers this just a 27-point decrease. The confidence intervals for FRS are also narrower than for MRR.</p><p>One can see in Table <ref type="table" coords="6,181.94,601.83,4.98,10.46">1</ref> that (mean) FRS was a good predictor of (mean) Success@10. Also, in Table <ref type="table" coords="6,532.26,601.83,3.87,10.46" target="#tab_1">2</ref>, the confidence intervals for the mean change in FRS are fairly similar to (but also narrower than) those for S@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Question Answering Track: Document Ranking Experiments</head><p>In the Document Ranking task of the Question Answering Track, the collection to be searched was the "AQUAINT collection" of English newswire articles from the 1998-2000 time period. Uncompressed, the collection was 3,181,313,864 bytes and consisted of 1,033,461 documents. The average document size was 3078 bytes (including SGML markup).</p><p>Each test question included a "target" (e.g. "skier Alberto Tomba") and the question itself (e.g. "What nationality is he?"). The goal was to find all the documents which contained the answer to the question and return them at the top of the list.</p><p>The organizers provided 50 test questions. However, some of them were from the same series (from another Question Answering task) and hence shared the same target. So unlike for most TREC tasks, the 50 queries likely were not independent, making the mean scores less reliable and invalidating the usual approach to statistical significance testing.</p><p>To create a test set with independent questions, we just kept the first judged question for each target, discarding additional questions from the same series. This revised test set contained 38 questions. Our diagnostics are on this latter set.</p><p>Over the original 50 test questions, there were on average 31.5 answer documents per question (low 1, high 285, median 6.5). Over the independent 38 test question subset, there were on average 34 answer documents per question (low 1, high 285, median 7.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Indexing</head><p>The indexing approach was mostly the same as for the Robust Retrieval Track of last year <ref type="bibr" coords="7,472.01,289.65,14.61,10.46" target="#b10">[11]</ref>. We used a SearchServer index which supported both exact matching (after some Unicode-based normalizations, such as decompositions and conversion to upper-case) and matching of inflections based on English lexical stemming (i.e. stemming based on a dictionary or lexicon for the language). For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Some stop words were excluded from indexing (e.g. "the", "by" and "of"). Based on looking at the questions from the previous year's QA Track, we added a few more stopwords ('how', 'many' and 'kind').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Searching</head><p>The techniques used for the 3 submitted runs of July 2005 are described below. The base technique was to use the SearchServer CONTAINS predicate to perform a boolean-OR of the words of the target and question.</p><p>humQ05l: The submitted humQ05l run was a plain content search including linguistic expansion from English inflectional stemming. This run was the same approach as last year's humR04d5 run (including RELEVANCE METHOD '2:3' and RELEVANCE DLEN IMP 500) except that instruction words (such as "find", "relevant" and "document") were not removed. Below is an example SearchSQL query:</p><formula xml:id="formula_0" coords="7,82.46,491.87,387.05,58.28">SELECT RELEVANCE('2:3') AS REL, DOCNO FROM AQ05 WHERE FT_TEXT CONTAINS 'skier'|'Alberto'|'Tomba'|'What'|'nationality'|'is'|'he' ORDER BY REL DESC;</formula><p>humQ05xl: The submitted humQ05xl run was the same as humQ05l except that a small additional weight (20%) was put on matching all of the query words within 200 characters of each other (ignoring stopwords which were not indexed). <ref type="bibr" coords="7,185.76,585.52,26.17,10.46">Below</ref>   humQ05xle: The submitted humQ05xle run was a blind feedback run based 50% on humQ05xl and 25% each on expansion queries from the first 2 rows of humQ05xl. (The expansion queries used a document length normalization of 750.) If the base run contains answer documents at the top of the list, this approach is likely to find more answer documents. From a user perspective, padding the results with more answer documents is unimportant because one answer document presumably should suffice, but the organizers wanted to focus on the recall-oriented 'mean average precision' measure which usually benefits from blind feedback approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Table <ref type="table" coords="8,99.72,509.44,4.98,10.46" target="#tab_3">3</ref> lists the mean scores of the 3 submitted runs. Each run returned an answer document in the first 10 rows for at least 90% of the questions (except for the blind feedback run on the 38-question subset). The proximity technique made little difference on average. The mean differences for blind feedback also did not quite pass the significance test on the 38-question subset (as per Table <ref type="table" coords="8,383.53,545.30,3.87,10.46" target="#tab_4">4</ref>).</p><p>Table <ref type="table" coords="8,113.93,557.25,4.98,10.46" target="#tab_5">5</ref> lists the mean scores of the diagnostic runs. For each approach (boolean-OR and boolean-AND), there was a "baseline" run (SET RELEVANCE METHOD '2:3', SET RELEVANCE DLEN IMP 250, SET TERM GENERATOR 'word!ftelp/inflect'). These settings were the same as for the humQ05l run except that RELEVANCE DLEN IMP was set to 250 instead of 500. (No proximity nor blind feedback was used for these diagnostic runs.) The other runs just had the one listed difference from the baseline run (these differences are explained in detail in Section 2.3 of last year's paper <ref type="bibr" coords="8,370.06,617.03,14.76,10.46" target="#b10">[11]</ref>).</p><p>Table <ref type="table" coords="8,113.26,628.98,4.98,10.46" target="#tab_6">6</ref> isolates the differences from the baseline runs. Document length normalization and stemming both had statistically significant mean benefits for the first relevant item. Switching to the '2:4' method (squaring the importance of inverse document frequency (idf)) did not make a statistically significant difference, but disabling idf completely ('2:5') appeared to be detrimental. Like last year, the hits count method ('2:1') was not competitive for boolean-OR, but was respectable for boolean-AND. Overall, this year's diagnostic results for finding answer documents were pretty similar to last year's diagnostic results for finding relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Robust Retrieval Track: Topic Relevance Experiments</head><p>The Robust Retrieval Track used the same AQUAINT collection described in the Question Answering section. But instead of finding answer documents for questions, the goal was to find relevant documents for topics.</p><p>Each topic contained a "Title" (subject of the topic, e.g. "killer bee attacks"), "Description" (a onesentence specification of the information need, e.g. "Identify instances of attacks on humans by Africanized (killer) bees.") and "Narrative" (more detailed guidelines for what a relevant document should or should not contain, e.g. "Relevant documents must cite a specific instance of a human attacked by killer bees. Documents that note migration patterns or report attacks on other animals are not relevant unless they also cite an attack on a human.").</p><p>The organizers actually re-used 50 of the more "difficult" topics from past TREC ad hoc tracks, but the document set was different, so new relevance assessments were done (and some concern was expressed that the judging pools were shallower than past years).</p><p>The judgements contained on average 131 relevant documents per topic (low 9, high 376, median 113) counting both "relevant" and "highly relevant" as relevant. If just "highly relevants" are counted as relevant, then 5 topics are discarded (for having no highly relevants), and over the remaining 45 topics, there were 62 highly relevants per topic (low 1, high 334, median 50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Indexing</head><p>The same index was used as described in the Question Answering section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Searching</head><p>The techniques used for the 5 submitted runs of July 2005 (and 2 other runs produced at the same time) are described below. The base technique was to use the SearchServer CONTAINS predicate to perform a boolean-OR of the words of the topic field.</p><p>humR05tl: The submitted humR05tl run was a plain content search (including linguistic expansion from English inflectional stemming) on the Title field of the topic. This run used the same approach as the humQ05l run except that we used RELEVANCE DLEN IMP 250 for the title runs. humR05txl: The submitted humR05txl run was the same as humR05tl except that a small additional weight (20%) was put on matching all of the query words within 200 characters of each other (ignoring stopwords which were not indexed). This run used the same approach as the humQ05xl run except that we used RELEVANCE DLEN IMP 250 for the title runs. humR05tx5l: (This run was not submitted.) The humR05tx5l run was the same as humR05txl except the weight was 5-to-1 in favour of the proximity predicate instead of the boolean-OR. (So this run would be likely to rank all documents which matched the proximity predicate ahead of those that did not.) Below is an example WHERE clause of a SearchSQL query: humR05txle: The submitted humR05txle run was a blind feedback run based 50% on humR05txl and 25% each on expansion queries from the first 2 rows of humR05txl. (The expansion queries used a document length normalization of 750.) humR05dl: The submitted humR05dl run used the same approach as humR05tl except that the Description field of the topic was used instead of the Title, instruction words such as "find", "relevant" and "document" were discarded before forming the query (same list as last year), and RELEVANCE DLEN IMP 500 was used for the description runs. This run used the same approach as the humR04d5 run of last year.</p><p>humR05dxl: (This run was not submitted.) The humR05dxl run was to the humR05dl run as humR05txl was to humR05tl.</p><p>humR05dle: The submitted humR05dle run was a blind feedback run based 50% on humR05dl and 25% each on expansion queries from the first 2 rows of humR05dl. (The expansion queries used a document length normalization of 750.)</p><p>For the submitted runs, the participants were asked to append a ranking of the system's confidence of how well it did on the topic. For each run, we just used the relevance value (i.e. the number returned by the SearchServer RELEVANCE() function) of the top-retrieved row as our basis for ranking the topics. A higher relevance value was considered to mean a higher confidence in the relevance of the document. (This was the same approach as we used last year, and Section 2.4.1 of last year's paper <ref type="bibr" coords="11,428.72,638.92,15.50,10.46" target="#b10">[11]</ref> analyzed the results.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>Even though the topics were chosen in part because they had been difficult in past tracks, Table <ref type="table" coords="12,505.86,592.34,4.98,10.46">7</ref> shows that on short title queries, the plain content search technique returned a relevant document in the first 10 rows for 88% of the topics (44/50). When available, a highly relevant was returned in the first 10 rows for 71% of the topics (32/45).</p><p>• 'x' experiment: The modest proximity weighting technique for titles ('x' experiment in Table <ref type="table" coords="12,514.48,650.13,4.43,10.46" target="#tab_8">8</ref>) led to a borderline significant increase in GMAP, though the individual impacts on average precision were small. The biggest increase for GMAP was the increase in average precision from 0.02 to 0.03 for topic 651 ("U.S. ethnic population"), followed by the increase in average precision from 0.32 to 0.50 for topic 648 ("family leave law"); the latter was the largest increase for the MAP measure. Proximity had bigger impacts on First Relevant Score, but was neutral on average.</p><p>• 'X' experiment: On descriptions, the modest proximity weighting ('X' experiment in Table <ref type="table" coords="13,493.48,105.37,4.43,10.46" target="#tab_8">8</ref>) actually led to a borderline significant decrease in FRS. For example, for topic 416 ("What is the status of The Three Gorges Project?"), the first relevant fell from rank 2 to rank 5, apparently because the proximity technique required all non-stop words to be close together to get extra weight, and the word "status" wasn't helpful in this case. For some other descriptions, no documents matched the proximity clause, but the integer relevance scores from the OR-matches were squeezed into a smaller range, and the extra ties apparently caused a minor degrading of the ranking.</p><p>• 'x5' experiment: The heavier weight on proximity weighting for titles ('x5' experiment in Table <ref type="table" coords="13,531.15,197.03,4.43,10.46" target="#tab_8">8</ref>) tended to be detrimental on average (though the mean differences were not statistically significant). There were some large per-topic impacts in each direction.</p><p>• 'e' experiment: The blind feedback technique for titles ('e' experiment in Table <ref type="table" coords="13,443.30,240.87,4.43,10.46" target="#tab_8">8</ref>) produced a statistically significant increase in MAP and GMAP, but FRS was negatively impacted.</p><p>• 'E' experiment: On descriptions, the blind feedback technique ('E' experiment in Table <ref type="table" coords="13,487.21,272.75,4.43,10.46" target="#tab_8">8</ref>) produced statistically significant changes in opposite directions: increases for MAP and GMAP, but a decrease for FRS.</p><p>One can see why we don't hear of the "blind feedback" technique being used in practice. When searching for one item, it is detrimental. When a lot of relevant documents will be reviewed, it's worth your time to supervise the query enhancement process. Either way, the "blind" form of feedback does not make sense in practice.</p><p>Table <ref type="table" coords="13,113.46,366.40,4.98,10.46" target="#tab_8">8</ref> shows that the mean differences for the track's new GMAP measure correlated strongly with those for MAP, even though GMAP picked out different extreme per-topic differences than MAP. In particular, GMAP still significantly favored the non-robust blind feedback technique.</p><p>GMAP was supposed to emphasize poorly performing topics, so why did it fail? On topic 341, for which blind feedback ('E' experiment) caused FRS to fall 42 points (the first relevant fell from rank 5 to 16), average precision actually increased slightly (from 0.0247 to 0.0252), so GMAP also increased slightly. On topic 435, for which blind feedback caused FRS to fall 42 points (again the first relevant fell from rank 5 to 16), average precision fell slightly from 0.0320 to 0.0263, and while GMAP' fell by more (from 0.70 to 0.68 in the linearized version), it still wasn't a substantial drop. A topic GMAP' did emphasize a lot was topic 345, for which average precision increased from 0.0018 to 0.0070, which GMAP' considered a substantial improvement (from 0.45 to 0.57 in the linearized scores), but the first relevant moved up from just (approximately) rank 345 to rank 127, and it doesn't seem that a user would consider this so substantial (FRS considers it just a 0.0001 increase).</p><p>In our poster at the TREC conference, we looked at using blind feedback as a "litmus test" for robustness measures:</p><p>• Blind feedback produced statistically significant increases for MAP, GMAP, R-Precision, Precision@20 and Interpolated Precision at 10% Recall, suggesting that these measures are not suitable as robustness measures.</p><p>• Blind feedback produced statistically significant decreases for FRS, MRR and Success@10, suggesting that these measures are candidates as robustness measures.</p><p>It appears that "primary recall" measures reflect robustness, while "secondary recall" measures do not.</p><p>Putting aside the notion of robustness, it's clear that primary and secondary recall measures can have opposite conclusions about a technique. Unfortunately, most past studies of ad hoc search have only reported secondary measures. Potentially a lot of "established" results for ad hoc search do not apply to retrieval of the first relevant item, particularly those involving blind feedback techniques.</p><p>For the tasks of the Terabyte Track, the collection to be searched was the GOV2 collection, a crawl of most of the .gov domain in early 2004. Once binaries (such as images) were removed, its size was less than half a terabyte. The GOV2 distribution was 457,165,206,582 bytes uncompressed (426 GB) and consisted of 25,205,179 documents. More than 90% of the documents were html, 8% were (extracted text from) pdf, and the rest were extracted text from other formats (plain text, msword, postscript, etc.). The average document size was 18,137 bytes.</p><p>We participated in all 3 tasks of the Terabyte Track: adhoc, efficiency and named page finding. Details on these tasks are in the track guidelines [13].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Indexing</head><p>The indexing approach was the same as described in the Enterprise section.</p><p>In the terabyte adhoc and efficiency tasks, the searches did not make use of the extra columns (e.g. title, meta tags, etc.). The full-text of the document (FT TEXT column), however, in the case of html documents included the title, strings from the meta tags, etc., though not the url.</p><p>In the terabyte named page finding searches, some of the submitted runs used not only the same extra columns as some of the Enterprise runs (title, meta tags, etc.), but also the url type and depth columns. The URL TYPE was set to ROOT, SUBROOT, PATH or FILE, based on the convention which worked well in TREC 2001 for the Twente/TNO group <ref type="bibr" coords="14,252.05,311.48,15.50,10.46" target="#b13">[15]</ref> on the entry page finding task (also known as the home page finding task). The URL DEPTH was set to a term indicating the depth of the page in the site. Examples and the exact rules we used are given in <ref type="bibr" coords="14,250.39,335.39,9.96,10.46" target="#b8">[9]</ref>.</p><p>Unlike for last year's submitted terabyte runs, the entire collection was indexed in one SearchServer table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Adhoc Experiments</head><p>The Adhoc Task of the Terabyte Track was much like the topic relevance task of the Robust Retrieval Track.</p><p>As in the Robust task, there were 50 topics, each with a title, description and narrative field. In the Terabyte adhoc task, the topics were new and meant to be typical (rather than re-using old topics which had been difficult in the past). And of course, the Terabyte task was searching a collection with 24 times as many documents as the Robust task (and the documents were more than 5 times longer on average and were from government web sites rather than news articles).</p><p>The terabyte adhoc judgements contained on average 208 relevant documents per topic (low 4, high 559, median 172) counting both "relevant" and "highly relevant" as relevant. If just "highly relevants" are counted as relevant, then 3 topics are discarded (for having no highly relevants), and over the remaining 47 topics, there were 56 highly relevants per topic (low 1, high 331, median 36).</p><p>The techniques used for the 4 submitted runs of July 2005 are described below. The base technique was to use the SearchServer CONTAINS predicate to perform a boolean-OR of the words of the Title field of the topic.</p><p>humT05l: The submitted humT05l was a plain content search (including linguistic expansion from English inflectional stemming) on the Title field of the topic. This run used the same approach as the humR05tl run described in the Robust section.</p><p>humT05xl: The submitted humT05xl run was the same as humT05l except that a small additional weight (20%) was put on matching all of the query words within 200 characters of each other (ignoring stopwords which were not indexed). This run used the same approach as the humR05txl run described in the Robust section (which has example query syntax).</p><p>humT05x5l: The submitted humR05tx5l run was the same as humT05xl except that the weight was 5-to-1 in favour of the proximity predicate instead of the boolean-OR. (So this run would be likely to rank all documents which matched the proximity predicate ahead of those that did not.) This run used the same approach as the humR05tx5l run described in the Robust section (which has example query syntax). humT05xle: The submitted humT05xle run was a blind feedback run based 50% on humT05xl and 25% each on expansion queries from the first 2 rows of humT05xl. (The expansion queries used a document length normalization of 750.) This run used the same approach as the humR05txle run described in the Robust section.</p><p>Table <ref type="table" coords="15,115.42,649.72,4.98,10.46" target="#tab_9">9</ref> shows that on the short title queries, the plain content search technique returned a relevant document in the first 10 rows for 94% of the topics (47/50). When available, a highly relevant was returned in the first 10 rows for 70% of the topics (33/47).</p><p>Note: even though the submitted runs included 10,000 rows per query, the mean scores in Table <ref type="table" coords="15,517.96,685.58,4.98,10.46" target="#tab_9">9</ref> are • 'x' experiment: The modest proximity weighting technique ('x' experiment in Table <ref type="table" coords="16,461.31,276.45,9.22,10.46">10</ref>) led to a statistically significant increase in MAP (and HMAP), though the individual impacts on average precision were small. It had bigger impacts on First Relevant Score, but was neutral on average. This result is similar to its impact in the Robust task.</p><p>• 'x5' experiment: For the heavier weight on proximity weighting ('x5' experiment in Table <ref type="table" coords="16,504.95,331.61,8.30,10.46">10</ref>), the mean differences were not statistically significant. Compared to the modest proximity weight, there were larger per-topic impacts in each direction. Again, this result is fairly similar to its impact in the Robust task.</p><p>• 'e' experiment: The blind feedback technique ('e' experiment in Table <ref type="table" coords="16,417.52,386.76,9.22,10.46">10</ref>) produced a statistically significant increase in MAP, but mean FRS was negatively impacted. Again, this result is fairly similar to its impact in the Robust task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Efficiency Experiments</head><p>In the efficiency task, the titles of the 50 adhoc topics were seeded into a set of 50,000 web queries. The participants submitted the top-20 results for all 50,000 queries (only the results for the 50 adhoc topics were going to be judged, but it was not announced in advance which 50 queries those were). The efficiency task was held in the first week of July 2005; results were due before the adhoc topics were released.</p><p>Because the efficiency queries were released only one week before results were due, the average query time would have to be under 12 seconds to complete even one run. (We submitted the maximum of 4 runs.) The time constraints discouraged the use of performance-intensive techniques (such as query expansion from blind feedback) and encouraged investigation of the tradeoff of time and retrieval quality.</p><p>Compared to our submitted (adhoc) runs of last year, we were using a faster machine (2.8GHz), the documents were indexed in one table, the searches were conducted locally (not over a network share), and the index was re-organized so that term position information would not need to be read from disk if the search did not include term proximity constraints. Furthermore, in diagnostics on last year's topics we found that retrieval quality for boolean-AND was similar to that for boolean-OR, particularly for early precision measures. Boolean-AND queries tend to be faster because they usually generate fewer internal matches (and hence involve less per-match processing such as relevance value calculations).</p><p>The techniques used for the 4 submitted runs of July 2005 are described below. The base technique was to use the SearchServer CONTAINS predicate to perform a boolean-AND of the words of the Title field of the topic. humTE05i4: The submitted humTE05i4 run used a boolean-AND of the query words. Inflections from stemming were not enabled for this run (SET TERM GENERATOR ''). Document length normalization humTE05i4l: The submitted humTE05i4l run was the same as humTE05i4 except that linguistic expansion from English inflectional stemming was enabled (SET TERM GENERATOR 'word!ftelp/inflect'). This run averaged 1.1 seconds per query (presumably the extra matches from inflections took more time to handle).</p><p>humTE05i4ld: The submitted humTE05i4ld run was the same as humTE05i4l except that document length normalization was enabled (SET RELEVANCE DLEN IMP 250). This run averaged 4.4 seconds per query, in part because in this implementation the 25 million document lengths were re-read from disk for each query.</p><p>humTE05i5: The submitted humTE05i5 run was the same as humTE05i4 except that the '2:5' relevance method was used instead of '2:4' which disabled the use of inverse document frequency. As expected, the average query time was the same as for humTE05i4.</p><p>Table <ref type="table" coords="17,115.63,452.16,9.96,10.46" target="#tab_10">11</ref> lists the mean scores of the submitted efficiency runs. Because at least 2 of the efficiency topics had spelling inconsistencies compared to the corresponding adhoc topics, we also list diagnostic runs performed using the same techniques on the titles of the adhoc topics. (Also note that because just the top-20 rows are included, FRS and MRR might be a little lower than they would be in the adhoc tables, which are evaluated on the top-1000 rows.)</p><p>The efficiency run which included document length normalization produced similar quality scores to the adhoc runs (e.g. Success@10 of 94%). (Diagnostic experiments on the terabyte collection were included in last year's paper <ref type="bibr" coords="17,147.16,535.85,14.76,10.46" target="#b10">[11]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Named Page Finding Experiments</head><p>The Named Page Finding Task of the Terabyte Track was much like the Known-Item task of the Enterprise Track. For each of the 252 queries, the goal was to find the particular page. 187 of the queries just had one right answer; for the others, the extra right answers presumably were duplicates. Of course, the Terabyte task was searching a collection with more than 100 times as many documents as the Enterprise task, and the documents were from government web sites rather than an organization's emails.</p><p>The techniques used for the 4 submitted runs of August 2005 were as follows: humTN05l: The submitted humTN05l run was a plain content search including linguistic expansion from English inflectional stemming. This run used the same approach as the humEK05l run described in the Enterprise section (which has example query syntax). humTN05pl: The submitted humTN05pl run was the same as humTN05l except that it put an additional 10% weight on matches in the ALL PROPS column (which included the title, meta tags, url, etc.) and an additional 10% weight on phrase matches in ALL PROPS. This run used the same approach as the humEK05pl run described in the Enterprise section (which has example query syntax).</p><p>humTN05dpl: The submitted humTN05dpl run was the same as humTN05pl except that it put additional weight on urls of depth 4 or less. This run used the same approach as last year's humW04dpl run (for which example syntax is given in <ref type="bibr" coords="18,189.03,406.54,15.50,10.46" target="#b10">[11]</ref>) except document length normalization (RELEVANCE DLEN IMP) was set to 250 instead of 500.</p><p>humTN05rdpl: The submitted humTN05rdpl run was the same as humTN05dpl except that it put additional weight on the url type. This run used the same approach as last year's humW04rdpl run (for which example syntax is given in <ref type="bibr" coords="18,220.44,454.35,15.50,10.46" target="#b10">[11]</ref>) except document length normalization (RELEVANCE DLEN IMP) was set to 250 instead of 500.</p><p>Table <ref type="table" coords="18,114.79,478.27,9.96,10.46" target="#tab_11">12</ref> lists the mean scores of the 4 submitted runs. The plain content search had a Success@10 of just 41% (103/252), and adding more weight on other columns boosted Success@10 to just 50% (126/252). These success rates are lower than in the named page finding subtask of last year's Web Track (for which the corresponding scores were 65% and 76%). Perhaps terabyte named page finding is harder because the GOV2 collection has 20 times as many pages as the GOV collection of last year's Web Track (a bigger haystack in which to find a needle).</p><p>• 'p' experiment: Table <ref type="table" coords="18,195.03,559.96,9.96,10.46" target="#tab_12">13</ref> shows that the 'p' factor (extra weight on columns such as the Title) led to statistically significant increases in mean FRS, S@1, S@10 and RR. This result is similar to its effect in this year's Enterprise Known-Item task and on last year's Web Named Page queries.</p><p>• 'd' experiment: Table <ref type="table" coords="18,198.54,603.79,9.96,10.46" target="#tab_12">13</ref> shows that the 'd' factor (modest extra weight for less deep urls) was of neutral impact on average, though on some topics it had a substantial impact in each direction. This result is similar to that for last year's Web Named Page queries.</p><p>• 'r' experiment: Table <ref type="table" coords="18,191.99,647.64,9.96,10.46" target="#tab_12">13</ref> shows that the 'r' factor (strong extra weight for urls of root, subroot or path types) led to statistically significant decreases in mean FRS, S@1, S@10 and RR. This result is similar to that for last year's Web Named Page queries.</p><p>The First Relevant Score measure (FRS) was successful at detecting the impact of various retrieval techniques on the first relevant item retrieved.</p><p>• In the known-item tasks, FRS found a statistically significant mean difference for 7 of the 8 experiments, more than for MRR (6/8), S@1 (6/8) and S@10 (4/8). Also, the largest per-topic differences for FRS were not skewed to minor differences in the early ranks, a common shortcoming of reciprocal rank.</p><p>• In the ad hoc tasks, FRS found a statistically significant mean difference for 11 of the 22 experiments, almost as many as for MAP <ref type="bibr" coords="19,223.25,184.98,31.00,10.46">(12/22)</ref>. Unlike for MAP, the differences for FRS are known to apply to the first relevant item retrieved.</p><p>The various retrieval measures moved together for most retrieval techniques. The one case for which we saw retrieval measures move significantly in opposite directions was for the "blind feedback" technique, which boosted secondary recall measures (such as MAP) but was detrimental to primary recall measures (such as FRS).</p><p>(In principle, duplicate filtering should have the opposite effect to blind feedback; i.e. successful duplicate filtering techniques would potentially increase primary recall measures (by filtering out duplicate non-relevants before the first desired item) but may decrease secondary recall measures (which may require duplicate relevants to be retrieved to get a maximum score). However, we did not experiment with duplicate filtering in this paper.)</p><p>We did not find cases where restricting to highly relevants or restricting to judged documents made a substantial difference to the rating of the techniques.</p><p>We did not find cases where secondary recall measures disagreed significantly with each other. In particular, GMAP still moved with MAP, even for the non-robust blind feedback technique. Precision@20 also moved with MAP.</p><p>So if one wants to contrast MAP with another measure for ad hoc tasks, it appears that one should use a primary recall measure, such as FRS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,449.35,458.96,90.65,10.46;10,72.00,470.93,78.51,10.46;10,82.46,492.84,193.52,10.46;10,82.46,504.80,47.06,10.46;10,82.46,516.75,26.15,10.46;10,87.69,528.70,214.44,10.46;10,82.46,540.66,94.14,10.46"><head></head><label></label><figDesc>Below is an example SearchSQL query: SELECT RELEVANCE('2:3') AS REL, DOCNO FROM AQ05 WHERE FT_TEXT CONTAINS 'killer'|'bee'|'attacks' ORDER BY REL DESC;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,82.46,389.85,26.15,10.46;11,87.69,401.81,214.44,10.46;11,87.69,413.76,230.13,10.46;11,103.38,425.72,298.14,10.46"><head></head><label></label><figDesc>WHERE FT_TEXT CONTAINS 'killer'|'bee'|'attacks' OR FT_TEXT CONTAINS PROXIMITY 200 CHARACTERS ('killer' WEIGHT 5 &amp; 'bee' WEIGHT 5 &amp; 'attacks' WEIGHT 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,111.89,80.42,388.21,316.66"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="6,111.89,80.42,388.21,304.70"><row><cell></cell><cell></cell><cell cols="3">: Impact of Enterprise Known-Item Search Techniques</cell></row><row><cell>Expt</cell><cell>∆FRS</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>t3 (t3l-l)</cell><cell>0.056</cell><cell>( 0.018, 0.094)</cell><cell>39-18-68</cell><cell>1.00 (107), 0.96 (117), -0.69 (131)</cell></row><row><cell>t (tl-l)</cell><cell>0.052</cell><cell>( 0.019, 0.086)</cell><cell>33-10-82</cell><cell>1.00 (107), 0.96 (117), -0.21 (131)</cell></row><row><cell>p (pl-l)</cell><cell>0.052</cell><cell>( 0.018, 0.087)</cell><cell>35-10-80</cell><cell>1.00 (107), 0.96 (117), -0.45 (131)</cell></row><row><cell>v3 (v3l-l)</cell><cell>0.051</cell><cell>( 0.015, 0.087)</cell><cell>36-18-71</cell><cell>1.00 (107), 0.96 (117), -0.68 (131)</cell></row><row><cell>l (pl-p)</cell><cell>0.040</cell><cell>( 0.003, 0.078)</cell><cell>21-15-89</cell><cell>1.00 (79), 1.00 (124), -0.52 (131)</cell></row><row><cell></cell><cell>∆S1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t3 (t3l-l)</cell><cell>0.112</cell><cell>( 0.035, 0.189)</cell><cell>19-5-101</cell><cell>1.00 (67), 1.00 (108), -1.00 (56)</cell></row><row><cell>p (pl-l)</cell><cell>0.096</cell><cell>( 0.030, 0.162)</cell><cell>15-3-107</cell><cell>1.00 (79), 1.00 (108), -1.00 (47)</cell></row><row><cell>t (tl-l)</cell><cell>0.088</cell><cell>( 0.023, 0.153)</cell><cell>14-3-108</cell><cell>1.00 (79), 1.00 (67), -1.00 (47)</cell></row><row><cell>v3 (v3l-l)</cell><cell>0.088</cell><cell>( 0.023, 0.153)</cell><cell>14-3-108</cell><cell>1.00 (52), 1.00 (74), -1.00 (47)</cell></row><row><cell>l (pl-p)</cell><cell>0.040</cell><cell>(-0.022, 0.102)</cell><cell>10-5-110</cell><cell>1.00 (79), 1.00 (112), -1.00 (150)</cell></row><row><cell></cell><cell>∆S10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t (tl-l)</cell><cell>0.056</cell><cell>( 0.014, 0.098)</cell><cell>7-0-118</cell><cell>1.00 (71), 1.00 (108), 0.00 (150)</cell></row><row><cell>p (pl-l)</cell><cell>0.048</cell><cell>( 0.003, 0.093)</cell><cell>7-1-117</cell><cell>1.00 (71), 1.00 (108), -1.00 (131)</cell></row><row><cell>l (pl-p)</cell><cell>0.056</cell><cell>(-0.002, 0.114)</cell><cell>10-3-112</cell><cell>1.00 (71), 1.00 (146), -1.00 (116)</cell></row><row><cell>t3 (t3l-l)</cell><cell>0.048</cell><cell>(-0.003, 0.099)</cell><cell>8-2-115</cell><cell>1.00 (71), 1.00 (108), -1.00 (66)</cell></row><row><cell>v3 (v3l-l)</cell><cell>0.048</cell><cell>(-0.003, 0.099)</cell><cell>8-2-115</cell><cell>1.00 (71), 1.00 (108), -1.00 (66)</cell></row><row><cell></cell><cell>∆MRR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t3 (t3l-l)</cell><cell>0.091</cell><cell>( 0.036, 0.146)</cell><cell>39-18-68</cell><cell>1.00 (107), 0.98 (117), -0.89 (148)</cell></row><row><cell>p (pl-l)</cell><cell>0.082</cell><cell>( 0.032, 0.131)</cell><cell>35-10-80</cell><cell>1.00 (107), 0.98 (117), -0.80 (148)</cell></row><row><cell>t (tl-l)</cell><cell>0.076</cell><cell>( 0.028, 0.125)</cell><cell>33-10-82</cell><cell>1.00 (107), 0.98 (117), -0.75 (148)</cell></row><row><cell>v3 (v3l-l)</cell><cell>0.074</cell><cell>( 0.026, 0.121)</cell><cell>36-18-71</cell><cell></cell></row></table><note coords="6,349.00,374.66,151.10,10.46;6,111.89,386.61,30.99,10.46;6,172.27,386.61,22.69,10.46;6,211.63,386.61,66.98,10.46;6,293.87,386.61,36.53,10.46;6,353.99,386.61,141.14,10.46"><p>1.00 (107), 0.98 (117), -0.86 (148) l (pl-p) 0.047 (-0.004, 0.097) 21-15-89 1.00 (79), 0.99 (124), -0.83 (67)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,82.46,585.52,371.35,92.15"><head></head><label></label><figDesc>is an example WHERE clause of a SearchSQL query:</figDesc><table coords="7,82.46,607.43,371.35,58.28"><row><cell>WHERE</cell></row><row><cell>FT_TEXT CONTAINS 'skier' WEIGHT 5|'Alberto' WEIGHT 5|'Tomba' WEIGHT 5|</cell></row><row><cell>'What' WEIGHT 5|'nationality' WEIGHT 5|</cell></row><row><cell>'is' WEIGHT 5|'he' WEIGHT 5</cell></row><row><cell>OR FT_TEXT CONTAINS PROXIMITY 200 CHARACTERS</cell></row></table><note coords="7,92.93,667.21,303.37,10.46"><p>('skier'&amp;'Alberto'&amp;'Tomba'&amp;'What'&amp;'nationality'&amp;'is'&amp;'he')</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,100.99,80.97,409.61,119.15"><head>Table 3 :</head><label>3</label><figDesc>Mean Scores of Submitted Question Answering Runs (50 and 38 Question Sets)</figDesc><table coords="8,100.99,96.21,409.61,103.91"><row><cell>Run</cell><cell>FRS</cell><cell>S@1</cell><cell>S@10</cell><cell>P20</cell><cell>MRR</cell><cell>MAP</cell><cell cols="2">FRSJ MAPJ</cell></row><row><cell>humQ05xl</cell><cell>0.882</cell><cell>33/50</cell><cell>46/50</cell><cell>0.301</cell><cell>0.749</cell><cell>0.416</cell><cell>0.882</cell><cell>0.426</cell></row><row><cell>humQ05l</cell><cell>0.880</cell><cell>34/50</cell><cell>46/50</cell><cell>0.286</cell><cell>0.753</cell><cell>0.413</cell><cell>0.881</cell><cell>0.426</cell></row><row><cell>humQ05xle</cell><cell>0.883</cell><cell>33/50</cell><cell>45/50</cell><cell>0.315</cell><cell>0.750</cell><cell>0.447</cell><cell>0.884</cell><cell>0.459</cell></row><row><cell>[independent 38]</cell><cell>FRS</cell><cell>S@1</cell><cell>S@10</cell><cell>P20</cell><cell>MRR</cell><cell>MAP</cell><cell cols="2">FRSJ MAPJ</cell></row><row><cell>humQ05xl</cell><cell>0.866</cell><cell>23/38</cell><cell>35/38</cell><cell>0.275</cell><cell>0.715</cell><cell>0.389</cell><cell>0.866</cell><cell>0.398</cell></row><row><cell>humQ05l</cell><cell>0.864</cell><cell>25/38</cell><cell>35/38</cell><cell>0.267</cell><cell>0.732</cell><cell>0.391</cell><cell>0.864</cell><cell>0.400</cell></row><row><cell>humQ05xle</cell><cell>0.857</cell><cell>23/38</cell><cell>33/38</cell><cell>0.287</cell><cell>0.710</cell><cell>0.420</cell><cell>0.858</cell><cell>0.431</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,111.06,233.00,389.88,134.49"><head>Table 4 :</head><label>4</label><figDesc>Impact of Question Answering Techniques (38 Question Set)</figDesc><table coords="8,111.06,248.24,389.88,119.25"><row><cell>Expt</cell><cell>∆FRS</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Series)</cell></row><row><cell>x (xl-l)</cell><cell>0.001</cell><cell>(-0.009, 0.011)</cell><cell>3-2-33</cell><cell>0.13 (73), -0.07 (101), -0.07 (127)</cell></row><row><cell>e (xle-xl)</cell><cell>-0.009</cell><cell>(-0.041, 0.024)</cell><cell>4-8-26</cell><cell>0.39 (96), -0.27 (107), -0.33 (136)</cell></row><row><cell></cell><cell>∆MRR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>x (xl-l)</cell><cell>-0.018</cell><cell>(-0.058, 0.023)</cell><cell>3-2-33</cell><cell>-0.50 (127), -0.50 (101), 0.25 (73)</cell></row><row><cell>e (xle-xl)</cell><cell>-0.005</cell><cell>(-0.060, 0.050)</cell><cell>4-8-26</cell><cell>-0.50 (123), -0.50 (89), 0.50 (73)</cell></row><row><cell></cell><cell>∆MAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>x (xl-l)</cell><cell>-0.002</cell><cell>(-0.020, 0.017)</cell><cell>14-17-7</cell><cell>-0.25 (127), -0.10 (78), 0.09 (135)</cell></row><row><cell>e (xle-xl)</cell><cell>0.031</cell><cell>(-0.002, 0.063)</cell><cell>21-14-3</cell><cell>0.38 (73), 0.26 (113), -0.15 (104)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,93.84,80.97,424.32,202.44"><head>Table 5 :</head><label>5</label><figDesc>Mean Scores of Diagnostic Question Answering Runs (38 Question Set)</figDesc><table coords="9,93.84,96.21,424.32,187.20"><row><cell>Run</cell><cell>FRS</cell><cell>S@1</cell><cell>S@10</cell><cell cols="2">P20 MRR MAP FRSJ MAPJ</cell></row><row><cell>OR: "2:3" (normal idf)</cell><cell cols="4">0.872 27/38 34/38 0.262 0.770 0.401 0.872</cell><cell>0.412</cell></row><row><cell>OR: "2:3 with no dlen"</cell><cell cols="4">0.838 22/38 34/38 0.239 0.684 0.344 0.838</cell><cell>0.359</cell></row><row><cell>OR: "2:4" (idf squared)</cell><cell cols="4">0.836 25/38 33/38 0.257 0.731 0.379 0.836</cell><cell>0.390</cell></row><row><cell>OR: "2:3 with no stemming"</cell><cell cols="4">0.835 22/38 34/38 0.249 0.677 0.345 0.836</cell><cell>0.357</cell></row><row><cell>OR: "2:5" (no idf)</cell><cell cols="4">0.813 21/38 33/38 0.234 0.653 0.308 0.821</cell><cell>0.329</cell></row><row><cell>OR: "2:2" (terms count)</cell><cell>0.488</cell><cell cols="3">8/38 17/38 0.147 0.306 0.137 0.523</cell><cell>0.166</cell></row><row><cell>OR: "2:1" (hits count)</cell><cell>0.376</cell><cell cols="3">9/38 14/38 0.079 0.291 0.100 0.516</cell><cell>0.145</cell></row><row><cell>AND: "2:3" (normal idf)</cell><cell cols="4">0.701 24/38 27/38 0.172 0.664 0.213 0.701</cell><cell>0.214</cell></row><row><cell>AND: "2:4" (idf squared)</cell><cell cols="4">0.697 23/38 27/38 0.175 0.647 0.200 0.697</cell><cell>0.202</cell></row><row><cell>AND: "2:5" (no idf)</cell><cell cols="4">0.670 19/38 27/38 0.171 0.568 0.183 0.670</cell><cell>0.186</cell></row><row><cell>AND: "2:3 with no dlen"</cell><cell cols="4">0.659 21/38 26/38 0.168 0.586 0.180 0.664</cell><cell>0.183</cell></row><row><cell>AND: "2:1" (hits count)</cell><cell cols="4">0.626 19/38 25/38 0.149 0.536 0.159 0.634</cell><cell>0.165</cell></row><row><cell cols="5">AND: "2:3 with no stemming" 0.548 16/38 22/38 0.117 0.475 0.123 0.548</cell><cell>0.124</cell></row><row><cell>AND: "2:2" (terms count)</cell><cell>0.445</cell><cell cols="3">8/38 15/38 0.134 0.295 0.094 0.467</cell><cell>0.101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,94.50,315.07,423.00,340.32"><head>Table 6 :</head><label>6</label><figDesc>Impact of Diagnostic Question Answering Techniques (38 Question Set)</figDesc><table coords="9,94.50,330.31,423.00,325.09"><row><cell>Expt</cell><cell>∆FRS</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Series)</cell></row><row><cell>OR: no dlen</cell><cell>-0.034</cell><cell>(-0.066,-0.002)</cell><cell>2-10-26</cell><cell>-0.37 (73), -0.26 (120), 0.19 (136)</cell></row><row><cell>OR: 2:4</cell><cell>-0.036</cell><cell>(-0.092, 0.020)</cell><cell>4-6-28</cell><cell>-0.95 (89), -0.40 (73), 0.13 (91)</cell></row><row><cell>OR: no stem</cell><cell>-0.037</cell><cell>(-0.074, 0.000)</cell><cell>4-10-24</cell><cell>-0.50 (89), -0.32 (125), 0.15 (96)</cell></row><row><cell>OR: 2:5</cell><cell>-0.059</cell><cell>(-0.120, 0.002)</cell><cell>2-13-23</cell><cell>-0.79 (107), -0.63 (136), 0.29 (108)</cell></row><row><cell>OR: 2:2</cell><cell>-0.385</cell><cell>(-0.498,-0.271)</cell><cell>0-29-9</cell><cell>-1.00 (135), -1.00 (89), 0.00 (78)</cell></row><row><cell>OR: 2:1</cell><cell>-0.496</cell><cell>(-0.641,-0.352)</cell><cell>1-28-9</cell><cell>-1.00 (120), -1.00 (127), 0.07 (126)</cell></row><row><cell>AND: 2:4</cell><cell>-0.004</cell><cell>(-0.014, 0.006)</cell><cell>1-2-35</cell><cell>-0.14 (73), -0.07 (125), 0.07 (101)</cell></row><row><cell>AND: 2:5</cell><cell>-0.031</cell><cell>(-0.056,-0.006)</cell><cell>0-7-31</cell><cell>-0.30 (104), -0.26 (79), 0.00 (108)</cell></row><row><cell>AND: no dlen</cell><cell>-0.042</cell><cell>(-0.082,-0.002)</cell><cell>0-5-33</cell><cell>-0.53 (104), -0.42 (73), 0.00 (108)</cell></row><row><cell>AND: 2:1</cell><cell>-0.076</cell><cell>(-0.133,-0.018)</cell><cell>1-8-29</cell><cell>-0.66 (104), -0.54 (73), 0.07 (101)</cell></row><row><cell>AND: no stem</cell><cell>-0.154</cell><cell>(-0.267,-0.040)</cell><cell>1-9-28</cell><cell>-1.00 (131), -1.00 (66), 0.13 (109)</cell></row><row><cell>AND: 2:2</cell><cell>-0.256</cell><cell>(-0.367,-0.145)</cell><cell>0-19-19</cell><cell>-1.00 (135), -0.95 (92), 0.00 (108)</cell></row><row><cell></cell><cell>∆MAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OR: 2:4</cell><cell>-0.021</cell><cell>(-0.066, 0.023)</cell><cell>15-20-3</cell><cell>-0.67 (125), -0.34 (89), 0.26 (97)</cell></row><row><cell>OR: no stem</cell><cell>-0.056</cell><cell>(-0.109,-0.003)</cell><cell>15-20-3</cell><cell>-0.83 (125), -0.35 (84), 0.15 (109)</cell></row><row><cell>OR: no dlen</cell><cell>-0.057</cell><cell>(-0.102,-0.012)</cell><cell>7-28-3</cell><cell>-0.80 (120), -0.23 (89), 0.05 (136)</cell></row><row><cell>OR: 2:5</cell><cell>-0.092</cell><cell>(-0.137,-0.048)</cell><cell>6-30-2</cell><cell>-0.67 (120), -0.38 (75), 0.08 (108)</cell></row><row><cell>OR: 2:2</cell><cell>-0.264</cell><cell>(-0.344,-0.184)</cell><cell>1-36-1</cell><cell>-0.93 (74), -0.92 (125), 0.01 (105)</cell></row><row><cell>OR: 2:1</cell><cell>-0.301</cell><cell>(-0.382,-0.220)</cell><cell>1-35-2</cell><cell>-1.00 (120), -1.00 (74), 0.08 (126)</cell></row><row><cell>AND: 2:4</cell><cell>-0.012</cell><cell>(-0.040, 0.015)</cell><cell>9-8-21</cell><cell>-0.50 (125), -0.11 (73), 0.06 (109)</cell></row><row><cell>AND: 2:5</cell><cell>-0.029</cell><cell>(-0.067, 0.008)</cell><cell>5-13-20</cell><cell>-0.67 (120), -0.16 (98), 0.05 (93)</cell></row><row><cell>AND: no dlen</cell><cell>-0.032</cell><cell>(-0.075, 0.011)</cell><cell>4-15-19</cell><cell>-0.80 (120), -0.15 (73), 0.03 (84)</cell></row><row><cell>AND: 2:1</cell><cell>-0.054</cell><cell>(-0.105,-0.002)</cell><cell>4-16-18</cell><cell>-0.86 (120), -0.42 (127), 0.05 (93)</cell></row><row><cell>AND: no stem</cell><cell>-0.090</cell><cell>(-0.159,-0.021)</cell><cell>2-20-16</cell><cell>-1.00 (120), -0.80 (125), 0.15 (109)</cell></row><row><cell>AND: 2:2</cell><cell>-0.119</cell><cell>(-0.192,-0.047)</cell><cell>3-20-15</cell><cell>-0.92 (125), -0.86 (120), 0.05 (79)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,93.11,80.42,425.78,456.73"><head>Table 8 :</head><label>8</label><figDesc>Impact of Robust Retrieval Techniques</figDesc><table coords="12,93.11,95.10,425.78,442.04"><row><cell>Expt</cell><cell>∆FRS</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>x (txl-tl)</cell><cell>-0.001</cell><cell>(-0.034, 0.031)</cell><cell>7-8-35</cell><cell>0.59 (322), -0.20 (448), -0.39 (383)</cell></row><row><cell>x5 (tx5l-tl)</cell><cell>-0.008</cell><cell>(-0.042, 0.025)</cell><cell>10-15-25</cell><cell>0.46 (426), 0.41 (322), -0.26 (439)</cell></row><row><cell>X (dxl-dl)</cell><cell>-0.012</cell><cell>(-0.024,-0.001)</cell><cell>1-7-42</cell><cell>-0.19 (416), -0.18 (419), 0.01 (448)</cell></row><row><cell>e (txle-txl)</cell><cell>-0.037</cell><cell>(-0.074, 0.001)</cell><cell>5-12-33</cell><cell>-0.83 (322), -0.26 (367), 0.10 (378)</cell></row><row><cell>E (dle-dl)</cell><cell>-0.049</cell><cell>(-0.096,-0.002)</cell><cell>7-14-29</cell><cell>-0.42 (341), -0.42 (435), 0.40 (448)</cell></row><row><cell></cell><cell>∆HFRS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>X (dxl-dl)</cell><cell>-0.005</cell><cell>(-0.016, 0.006)</cell><cell>5-7-33</cell><cell>-0.18 (419), -0.07 (426), 0.10 (650)</cell></row><row><cell>x (txl-tl)</cell><cell>-0.008</cell><cell>(-0.054, 0.038)</cell><cell>8-16-21</cell><cell>0.59 (322), -0.32 (689), -0.39 (383)</cell></row><row><cell>x5 (tx5l-tl)</cell><cell>-0.033</cell><cell>(-0.093, 0.026)</cell><cell>8-22-15</cell><cell>-0.46 (409), -0.46 (658), 0.46 (426)</cell></row><row><cell>e (txle-txl)</cell><cell>-0.038</cell><cell>(-0.083, 0.008)</cell><cell>8-16-21</cell><cell>-0.83 (322), -0.31 (344), 0.22 (307)</cell></row><row><cell>E (dle-dl)</cell><cell>-0.055</cell><cell>(-0.119, 0.009)</cell><cell>11-17-17</cell><cell>-0.52 (372), -0.46 (439), 0.48 (416)</cell></row><row><cell></cell><cell>∆MAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E (dle-dl)</cell><cell>0.044</cell><cell>( 0.025, 0.062)</cell><cell>38-12-0</cell><cell>0.23 (622), 0.20 (625), -0.10 (408)</cell></row><row><cell>e (txle-txl)</cell><cell>0.034</cell><cell>( 0.015, 0.052)</cell><cell>35-15-0</cell><cell>0.22 (325), 0.20 (622), -0.11 (303)</cell></row><row><cell>x (txl-tl)</cell><cell>0.007</cell><cell>(-0.002, 0.016)</cell><cell>26-22-2</cell><cell>0.18 (648), 0.06 (394), -0.04 (325)</cell></row><row><cell>X (dxl-dl)</cell><cell>-0.000</cell><cell>(-0.002, 0.002)</cell><cell>23-23-4</cell><cell>0.03 (393), -0.01 (416), -0.02 (394)</cell></row><row><cell>x5 (tx5l-tl)</cell><cell>-0.006</cell><cell>(-0.022, 0.009)</cell><cell>21-28-1</cell><cell>0.21 (648), 0.08 (336), -0.21 (374)</cell></row><row><cell></cell><cell>∆HMAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E (dle-dl)</cell><cell>0.029</cell><cell>( 0.009, 0.049)</cell><cell>27-17-1</cell><cell>0.24 (622), 0.20 (374), -0.10 (372)</cell></row><row><cell>e (txle-txl)</cell><cell>0.017</cell><cell>( 0.000, 0.033)</cell><cell>21-21-3</cell><cell>0.25 (622), 0.15 (625), -0.08 (303)</cell></row><row><cell>x (txl-tl)</cell><cell>0.001</cell><cell>(-0.007, 0.010)</cell><cell>18-23-4</cell><cell>0.14 (648), -0.05 (374), -0.06 (650)</cell></row><row><cell>X (dxl-dl)</cell><cell>0.001</cell><cell>(-0.001, 0.003)</cell><cell>23-15-7</cell><cell>0.03 (393), 0.01 (650), -0.01 (419)</cell></row><row><cell>x5 (tx5l-tl)</cell><cell>-0.012</cell><cell>(-0.031, 0.006)</cell><cell>13-29-3</cell><cell>-0.30 (374), -0.09 (650), 0.17 (648)</cell></row><row><cell></cell><cell>∆GMAP'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E (dle-dl)</cell><cell>0.019</cell><cell>( 0.007, 0.031)</cell><cell>38-12-0</cell><cell>0.13 (336), 0.12 (345), -0.12 (651)</cell></row><row><cell>e (txle-txl)</cell><cell>0.011</cell><cell>( 0.002, 0.021)</cell><cell>35-15-0</cell><cell>-0.09 (651), 0.08 (372), 0.08 (625)</cell></row><row><cell>x (txl-tl)</cell><cell>0.004</cell><cell>( 0.000, 0.008)</cell><cell>26-22-2</cell><cell>0.04 (651), 0.04 (648), -0.02 (448)</cell></row><row><cell>X (dxl-dl)</cell><cell>0.000</cell><cell>(-0.002, 0.002)</cell><cell>23-23-4</cell><cell>0.03 (393), -0.01 (419), -0.01 (426)</cell></row><row><cell>x5 (tx5l-tl)</cell><cell>-0.002</cell><cell>(-0.009, 0.004)</cell><cell>21-28-1</cell><cell>0.06 (393), 0.04 (648), -0.06 (650)</cell></row><row><cell></cell><cell>∆HGMAP'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E (dle-dl)</cell><cell>0.009</cell><cell>(-0.008, 0.026)</cell><cell>27-17-1</cell><cell>-0.17 (651), -0.12 (372), 0.15 (336)</cell></row><row><cell>e (txle-txl)</cell><cell>0.005</cell><cell>(-0.009, 0.018)</cell><cell>21-21-3</cell><cell>-0.15 (322), -0.10 (651), 0.10 (625)</cell></row><row><cell>x (txl-tl)</cell><cell>0.003</cell><cell>(-0.005, 0.011)</cell><cell>18-23-4</cell><cell>0.11 (322), 0.06 (330), -0.05 (650)</cell></row><row><cell>X (dxl-dl)</cell><cell>0.001</cell><cell>(-0.001, 0.003)</cell><cell>23-15-7</cell><cell>0.03 (393), -0.01 (419), -0.01 (426)</cell></row><row><cell>x5 (tx5l-tl)</cell><cell>-0.008</cell><cell>(-0.019, 0.003)</cell><cell>13-29-3</cell><cell>-0.11 (650), -0.07 (374), 0.07 (330)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="15,97.22,80.42,417.57,488.10"><head>Table 9 :</head><label>9</label><figDesc>Mean Scores of Submitted Terabyte Adhoc Runs</figDesc><table coords="15,97.22,95.10,417.57,473.42"><row><cell>Run</cell><cell>FRS</cell><cell>S@1</cell><cell>S@10</cell><cell>P20</cell><cell>MRR</cell><cell>MAP</cell><cell>FRSJ</cell><cell>MAPJ</cell></row><row><cell>humT05x5l</cell><cell>0.932</cell><cell>36/50</cell><cell>48/50</cell><cell>0.565</cell><cell>0.815</cell><cell>0.332</cell><cell>0.932</cell><cell>0.376</cell></row><row><cell>humT05l</cell><cell>0.930</cell><cell>35/50</cell><cell>47/50</cell><cell>0.580</cell><cell>0.807</cell><cell>0.315</cell><cell>0.930</cell><cell>0.356</cell></row><row><cell>humT05xl</cell><cell>0.929</cell><cell>38/50</cell><cell>48/50</cell><cell>0.596</cell><cell>0.826</cell><cell>0.336</cell><cell>0.929</cell><cell>0.374</cell></row><row><cell>humT05xle</cell><cell>0.903</cell><cell>35/50</cell><cell>46/50</cell><cell>0.623</cell><cell>0.780</cell><cell>0.365</cell><cell>0.904</cell><cell>0.413</cell></row><row><cell>(on highly rels)</cell><cell>HFRS</cell><cell>HS@1</cell><cell cols="6">HS@10 HP20 HMRR HMAP HFRSJ HMAPJ</cell></row><row><cell>humT05xl</cell><cell>0.711</cell><cell>20/47</cell><cell>35/47</cell><cell>0.232</cell><cell>0.526</cell><cell>0.208</cell><cell>0.711</cell><cell>0.218</cell></row><row><cell>humT05l</cell><cell>0.694</cell><cell>19/47</cell><cell>33/47</cell><cell>0.222</cell><cell>0.519</cell><cell>0.195</cell><cell>0.694</cell><cell>0.206</cell></row><row><cell>humT05x5l</cell><cell>0.689</cell><cell>18/47</cell><cell>35/47</cell><cell>0.206</cell><cell>0.492</cell><cell>0.207</cell><cell>0.689</cell><cell>0.218</cell></row><row><cell>humT05xle</cell><cell>0.660</cell><cell>16/47</cell><cell>32/47</cell><cell>0.232</cell><cell>0.470</cell><cell>0.222</cell><cell>0.661</cell><cell>0.236</cell></row><row><cell></cell><cell cols="6">Table 10: Impact of Terabyte Adhoc Techniques</cell><cell></cell><cell></cell></row><row><cell>Expt</cell><cell>∆FRS</cell><cell cols="2">95% Conf</cell><cell>vs.</cell><cell cols="3">3 Extreme Diffs (Topic)</cell><cell></cell></row><row><cell>x5 (x5l-l)</cell><cell>0.002</cell><cell cols="2">(-0.033, 0.037)</cell><cell>6-7-37</cell><cell cols="4">0.56 (762), 0.32 (792), -0.39 (763)</cell></row><row><cell>x (xl-l)</cell><cell>-0.001</cell><cell cols="2">(-0.023, 0.022)</cell><cell>6-5-39</cell><cell cols="4">-0.30 (763), -0.25 (777), 0.25 (792)</cell></row><row><cell>e (xle-xl)</cell><cell>-0.026</cell><cell cols="2">(-0.056, 0.003)</cell><cell>4-11-35</cell><cell cols="4">-0.46 (754), -0.46 (769), 0.20 (795)</cell></row><row><cell cols="2">∆HFRS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x (xl-l)</cell><cell>0.017</cell><cell cols="2">(-0.011, 0.044)</cell><cell>12-9-26</cell><cell cols="4">0.39 (783), 0.25 (798), -0.30 (763)</cell></row><row><cell>x5 (x5l-l)</cell><cell>-0.005</cell><cell cols="2">(-0.072, 0.061)</cell><cell>8-17-22</cell><cell cols="4">0.63 (760), 0.57 (783), -0.58 (777)</cell></row><row><cell>e (xle-xl)</cell><cell>-0.051</cell><cell cols="2">(-0.107, 0.005)</cell><cell>11-18-18</cell><cell cols="4">-0.79 (775), -0.63 (792), 0.27 (777)</cell></row><row><cell cols="2">∆MAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>e (xle-xl)</cell><cell>0.029</cell><cell cols="2">( 0.004, 0.055)</cell><cell>30-20-0</cell><cell cols="4">0.39 (773), 0.25 (772), -0.14 (770)</cell></row><row><cell>x (xl-l)</cell><cell>0.021</cell><cell cols="2">( 0.012, 0.029)</cell><cell>35-13-2</cell><cell cols="4">0.11 (785), 0.08 (772), -0.02 (756)</cell></row><row><cell>x5 (x5l-l)</cell><cell>0.017</cell><cell cols="2">(-0.016, 0.050)</cell><cell>24-25-1</cell><cell cols="4">0.54 (785), 0.21 (755), -0.18 (777)</cell></row><row><cell cols="2">∆HMAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>e (xle-xl)</cell><cell>0.014</cell><cell cols="2">(-0.010, 0.039)</cell><cell>23-23-1</cell><cell cols="4">0.36 (772), 0.29 (768), -0.15 (779)</cell></row><row><cell>x (xl-l)</cell><cell>0.013</cell><cell cols="2">( 0.005, 0.022)</cell><cell>30-11-6</cell><cell cols="4">0.13 (783), 0.07 (772), -0.04 (786)</cell></row><row><cell>x5 (x5l-l)</cell><cell>0.012</cell><cell cols="2">(-0.011, 0.035)</cell><cell>23-21-3</cell><cell cols="4">0.38 (783), 0.17 (790), -0.13 (765)</cell></row><row><cell></cell><cell>∆P20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>e (xle-xl)</cell><cell>0.027</cell><cell cols="2">(-0.009, 0.063)</cell><cell>24-17-9</cell><cell cols="4">0.40 (768), 0.25 (800), -0.30 (792)</cell></row><row><cell>x (xl-l)</cell><cell>0.016</cell><cell cols="2">(-0.008, 0.040)</cell><cell>20-11-19</cell><cell cols="4">-0.25 (777), -0.15 (799), 0.20 (782)</cell></row><row><cell>x5 (x5l-l)</cell><cell>-0.015</cell><cell cols="2">(-0.063, 0.033)</cell><cell>18-19-13</cell><cell cols="4">0.40 (782), -0.30 (776), -0.35 (799)</cell></row><row><cell cols="2">∆HP20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x (xl-l)</cell><cell>0.010</cell><cell cols="2">(-0.003, 0.022)</cell><cell>12-3-32</cell><cell cols="4">-0.15 (777), 0.10 (798), 0.10 (763)</cell></row><row><cell>e (xle-xl)</cell><cell>0.000</cell><cell cols="2">(-0.028, 0.028)</cell><cell>10-14-23</cell><cell cols="4">0.30 (768), 0.20 (799), -0.25 (770)</cell></row><row><cell>x5 (x5l-l)</cell><cell>-0.016</cell><cell cols="2">(-0.043, 0.011)</cell><cell>11-14-22</cell><cell cols="4">-0.35 (799), -0.25 (777), 0.20 (793)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="16,72.00,80.97,416.23,185.62"><head>Table 11 :</head><label>11</label><figDesc>Mean Scores of Submitted Terabyte Efficiency Runs (20 Rows Retrieved)</figDesc><table coords="16,72.00,96.21,415.26,170.38"><row><cell>Run</cell><cell>FRS</cell><cell>S@1</cell><cell>S@5</cell><cell>S@10</cell><cell>S@20</cell><cell>P20</cell><cell>MRR</cell></row><row><cell>humTE05i4ld</cell><cell>0.895</cell><cell>35/50</cell><cell>45/50</cell><cell>46/50</cell><cell>47/50</cell><cell>0.549</cell><cell>0.788</cell></row><row><cell>humTE05i5</cell><cell>0.807</cell><cell>28/50</cell><cell>38/50</cell><cell>41/50</cell><cell>46/50</cell><cell>0.446</cell><cell>0.650</cell></row><row><cell>humTE05i4</cell><cell>0.796</cell><cell>24/50</cell><cell>37/50</cell><cell>43/50</cell><cell>45/50</cell><cell>0.439</cell><cell>0.613</cell></row><row><cell>humTE05i4l</cell><cell>0.792</cell><cell>27/50</cell><cell>39/50</cell><cell>40/50</cell><cell>44/50</cell><cell>0.451</cell><cell>0.644</cell></row><row><cell>on adhoc titles</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(humTE05i4ld)</cell><cell>0.915</cell><cell>36/50</cell><cell>46/50</cell><cell>47/50</cell><cell>48/50</cell><cell>0.565</cell><cell>0.808</cell></row><row><cell>(humTE05i5)</cell><cell>0.813</cell><cell>29/50</cell><cell>38/50</cell><cell>41/50</cell><cell>46/50</cell><cell>0.454</cell><cell>0.666</cell></row><row><cell>(humTE05i4)</cell><cell>0.798</cell><cell>25/50</cell><cell>37/50</cell><cell>43/50</cell><cell>45/50</cell><cell>0.447</cell><cell>0.623</cell></row><row><cell>(humTE05i4l)</cell><cell>0.794</cell><cell>28/50</cell><cell>39/50</cell><cell>40/50</cell><cell>44/50</cell><cell>0.460</cell><cell>0.654</cell></row><row><cell cols="2">based on just the first 1000 rows.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="17,72.00,80.42,468.02,228.78"><head>Table 12 :</head><label>12</label><figDesc>Mean Scores of Submitted Terabyte Named Page Finding Runs An example SearchSQL query is below. This run averaged 0.8 seconds per query, including fetching of the top-20 rows. (Note that the times are averaged over the 50,000 web queries which may have been longer on average than the typical adhoc query.)</figDesc><table coords="17,72.00,95.10,468.00,214.10"><row><cell>Run</cell><cell>FRS</cell><cell>S@1</cell><cell>S@5</cell><cell>S@10</cell><cell>S@1000</cell><cell>MRR</cell></row><row><cell>humTN05rdpl</cell><cell>0.457</cell><cell>67/252</cell><cell>109/252</cell><cell>117/252</cell><cell>200/252</cell><cell>0.335</cell></row><row><cell>humTN05dpl</cell><cell>0.488</cell><cell>76/252</cell><cell>115/252</cell><cell>125/252</cell><cell>201/252</cell><cell>0.371</cell></row><row><cell>humTN05pl</cell><cell>0.487</cell><cell>79/252</cell><cell>115/252</cell><cell>126/252</cell><cell>202/252</cell><cell>0.378</cell></row><row><cell>humTN05l</cell><cell>0.406</cell><cell>53/252</cell><cell>93/252</cell><cell>103/252</cell><cell>195/252</cell><cell>0.279</cell></row><row><cell cols="7">was not enabled (SET RELEVANCE DLEN IMP 0). The '2:4' relevance method was used (which squared</cell></row><row><cell cols="7">the importance of inverse document frequency) because it gave higher reciprocal rank scores than '2:3' last</cell></row><row><cell cols="3">year (though not significantly so). SELECT RELEVANCE('2:4') AS REL, DOCNO</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FROM GOV2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">WHERE FT_TEXT CONTAINS 'Puerto'&amp;'Rico'&amp;'state'</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ORDER BY REL DESC;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="18,93.61,80.42,424.79,221.01"><head>Table 13 :</head><label>13</label><figDesc>Impact of Terabyte Named Page Finding Techniques</figDesc><table coords="18,93.61,95.10,424.79,206.33"><row><cell>Expt</cell><cell>∆FRS</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>p (pl-l)</cell><cell>0.082</cell><cell>( 0.054, 0.109)</cell><cell>106-27-119</cell><cell>1.00 (658), 1.00 (686), -0.40 (621)</cell></row><row><cell>d (dpl-pl)</cell><cell>0.001</cell><cell>(-0.010, 0.011)</cell><cell>37-52-163</cell><cell>-1.00 (658), 0.29 (820), 0.34 (672)</cell></row><row><cell>r (rdpl-dpl)</cell><cell>-0.031</cell><cell>(-0.047,-0.015)</cell><cell>16-83-153</cell><cell>-0.79 (686), -0.71 (667), 0.63 (658)</cell></row><row><cell></cell><cell>∆S1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>p (pl-l)</cell><cell>0.103</cell><cell>( 0.063, 0.144)</cell><cell>27-1-224</cell><cell>1.00 (849), 1.00 (602), -1.00 (695)</cell></row><row><cell>d (dpl-pl)</cell><cell>-0.012</cell><cell>(-0.030, 0.006)</cell><cell>1-4-247</cell><cell>-1.00 (632), -1.00 (658), 1.00 (657)</cell></row><row><cell>r (rdpl-dpl)</cell><cell>-0.036</cell><cell>(-0.067,-0.005)</cell><cell>3-12-237</cell><cell>-1.00 (675), -1.00 (615), 1.00 (632)</cell></row><row><cell></cell><cell>∆S10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>p (pl-l)</cell><cell>0.091</cell><cell>( 0.054, 0.128)</cell><cell>23-0-229</cell><cell>1.00 (839), 1.00 (838), 0.00 (872)</cell></row><row><cell>d (dpl-pl)</cell><cell>-0.004</cell><cell>(-0.018, 0.010)</cell><cell>1-2-249</cell><cell>-1.00 (814), -1.00 (658), 1.00 (641)</cell></row><row><cell>r (rdpl-dpl)</cell><cell>-0.032</cell><cell>(-0.057,-0.006)</cell><cell>1-9-242</cell><cell>-1.00 (641), -1.00 (817), 1.00 (658)</cell></row><row><cell></cell><cell>∆MRR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>p (pl-l)</cell><cell>0.099</cell><cell>( 0.067, 0.131)</cell><cell>106-27-119</cell><cell>1.00 (686), 1.00 (658), -0.50 (695)</cell></row><row><cell>d (dpl-pl)</cell><cell>-0.008</cell><cell>(-0.020, 0.005)</cell><cell>37-52-163</cell><cell>-1.00 (658), -0.75 (686), 0.50 (657)</cell></row><row><cell>r (rdpl-dpl)</cell><cell>-0.035</cell><cell>(-0.057,-0.014)</cell><cell>16-83-153</cell><cell>-0.94 (667), -0.91 (849), 0.67 (676)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,677.44,452.82,9.50;1,72.00,688.03,266.38,8.37"><p>SearchServer TM , SearchSQL TM and Intuitive Searching TM are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="19,86.70,446.82,414.14,9.41" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="19,243.70,446.82,200.06,9.41">Retrieval Evaluation with Incomplete Information</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,465.75,405.07,9.41" xml:id="b1">
	<monogr>
		<ptr target="http://research.microsoft.com/users/nickcr/w3c-summary.html" />
		<title level="m" coord="19,86.70,465.75,144.12,9.41">Nick Craswell. W3C Test Collection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,484.68,307.16,9.41" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><surname>Cross-Language</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="19,153.48,484.68,107.58,9.41">Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,503.61,453.11,9.41;19,82.51,514.57,20.98,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,160.00,503.61,196.90,9.41">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,363.50,503.61,172.04,9.41">Sixteenth International Unicode Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,533.50,397.57,9.41" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="19,124.28,533.50,154.77,9.41">NII-Test Collection for IR) Home Page</title>
		<author>
			<persName coords=""><surname>Ntcir</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/∼ntcadm/index-en.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,552.43,453.10,9.41;19,82.51,563.39,60.64,9.41" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="19,400.81,552.43,139.00,9.41;19,82.51,563.39,31.36,9.41">Okapi at TREC-3. Proceedings of TREC-3</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,582.32,280.97,9.41" xml:id="b6">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="19,86.70,582.32,190.80,9.41">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,601.24,379.18,9.41;19,466.03,599.96,11.73,6.28;19,480.87,601.24,59.09,9.41;19,82.51,612.21,185.04,9.41" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<title level="m" coord="19,168.51,601.24,297.36,9.41;19,466.03,599.96,11.73,6.28;19,480.87,601.24,59.09,9.41;19,82.51,612.21,180.29,9.41">European Ad Hoc Retrieval Experiments with Hummingbird SearchServer TM at CLEF 2005. Working Notes for the CLEF 2005 Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,86.70,631.13,453.10,9.41;19,82.51,642.09,51.51,9.41;19,134.04,640.82,11.73,6.28;19,149.33,642.09,176.76,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,179.18,631.13,360.62,9.41;19,82.51,642.09,51.51,9.41;19,134.04,640.82,11.73,6.28;19,149.33,642.09,57.47,9.41">Experiments in Named Page Finding and Arabic Retrieval with Hummingbird SearchServer TM at TREC 2002</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,214.07,642.09,87.96,9.41">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,91.30,661.02,370.06,9.41;19,461.52,659.74,11.73,6.28;19,477.31,661.02,62.66,9.41;19,82.51,671.98,112.02,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,210.10,661.02,251.27,9.41;19,461.52,659.74,11.73,6.28;19,477.31,661.02,58.46,9.41">Web and Genomic Retrieval with Hummingbird SearchServer TM at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,82.51,671.98,107.82,9.41">Proceedings of TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.30,74.34,370.12,9.41;20,461.57,73.07,11.73,6.28;20,477.34,74.34,62.63,9.41;20,82.51,85.31,112.02,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,210.04,74.34,251.38,9.41;20,461.57,73.07,11.73,6.28;20,477.34,74.34,58.44,9.41">Web and Terabyte Retrieval with Hummingbird SearchServer TM at TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,82.51,85.31,87.97,9.41">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.30,104.23,443.76,9.41;20,72.00,123.16,367.77,9.41" xml:id="b11">
	<monogr>
		<ptr target="http://plg.uwaterloo.ca/∼claclark/TB05.html" />
		<title level="m" coord="20,91.30,104.23,161.86,9.41;20,91.30,123.16,156.03,9.41">TREC-2005 Enterprise Track Guidelines</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>TREC 2005 Terabyte Track Guidelines</note>
</biblStruct>

<biblStruct coords="20,91.30,142.09,409.18,9.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,170.84,142.09,210.33,9.41">Overview of the TREC 2004 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,388.46,142.09,87.97,9.41">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,91.30,161.02,448.50,9.41;20,82.51,171.98,150.47,9.41" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,315.58,161.02,224.21,9.41;20,82.51,171.98,30.96,9.41">Retrieving Web Pages using Content, Links, URLs and Anchors</title>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,120.96,171.98,87.97,9.41">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
