<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.00,72.02,328.13,17.98">A TREC along the Spam Track with SpamBayes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,265.70,108.59,80.73,11.24"><forename type="first">Tony</forename><forename type="middle">Andrew</forename><surname>Meyer</surname></persName>
							<email>tony.meyer@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SpamBayes Development Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.00,72.02,328.13,17.98">A TREC along the Spam Track with SpamBayes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0ABD96D17ACA3CB238CCAE0ED0ACA5E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the SpamBayes submissions made to the Spam Track of the 2005 Text Retrieval Conference (TREC). SpamBayes is briefly introduced, but the paper focuses more on how the submissions differ from the standard installation. Unlike in the majority of earlier publications evaluating the effectiveness of SpamBayes, the fundamental 'unsure' range is discussed, and the method of removing the range is outlined. Finally, an analysis of the results of the running the four submissions through the Spam Track 'jig' with the three private corpora and one public corpus is made.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">SpamBayes</head><p>SpamBayes <ref type="bibr" coords="1,119.60,336.49,10.10,11.24" target="#b0">[1]</ref> was born on August 19th 2002, soon after publication of A Plan for Spam <ref type="bibr" coords="1,434.40,336.49,9.21,11.24" target="#b1">[2]</ref>; Tim Peters and others involved with the Python development community developed code based on Graham's ideas, with the initial aim of filtering python.org mailing-list traffic, although this quickly progressed to also filtering personal email streams (today, although most python.org mailing lists do use SpamBayes, the overwhelmingly most common use of SpamBayes is for personal email filtering). Although the project initially started with Graham's original combining scheme, it currently uses the chi-squared combining scheme developed by Robinson.</p><p>The SpamBayes distribution includes a plug-in for Microsoft™ Outlook™, a generic POP3 proxy and IMAP4 filter, various command-line scripts, and a suite of tools for testing modifications of the tokenization/classification systems.</p><p>The software is free <ref type="foot" coords="1,150.10,463.01,2.71,6.52" target="#foot_0">1</ref> , released under the Python Software Foundation Licence. A basic introduction to the distribution can be found in <ref type="bibr" coords="1,137.90,476.99,9.28,11.24" target="#b2">[3]</ref>. The testing outlined in this paper used version 1.1a1; the only modifications were adjustments of the client/server architecture to improve the speed of the TREC testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TREC 2005 Spam Track Corpora</head><p>This paper covers results from testing SpamBayes with the three 2005 TREC spam track private corpora, and the 2005 TREC spam track public corpus (the full corpus, and four subset variants). As the author had access to the TM corpus prior to the 2005 TREC spam track submission, no consideration will be made of the results of the runs against this corpus in this paper, although these runs were performed (results were generally similar to those of the Public and MrX corpora). Table <ref type="table" coords="1,139.50,592.19,4.68,11.24" target="#tab_0">1</ref> outlines the vital statistics for each of the copora; more details about the composition of the eight corpora, including a detailed explanation of the creation of the Public corpus and a breakdown of the SB corpus into both ham and spam genre can be found in the overview of the spam track for TREC 2005 <ref type="bibr" coords="1,424.60,620.29,9.21,11.24" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Handling the 'unsure' range</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>The unsure range in practice SpamBayes uses Robinson's chi-squared probabilities combining scheme <ref type="bibr" coords="2,378.70,102.89,9.21,11.24" target="#b4">[5]</ref>. A chi-squared test calculates the probability that a particular distribution matches a hypothesis (in this case that the message is spam and, separately, that the message is ham). The results of these two chi-squared tests are then combined and scaled to give an overall message spam score in the range 0 to 1. The 'unsure' middle ground is defined as any message with a final score falling between an upper and lower bound. In the SpamBayes distribution, the default unsure range is a message with a final combined spam score between 0.20 and 0.90; this lack of symmetry reflects an aversion to false positives. Messages tend to score at the extremes of the range, apart from difficult to classify messages (those that strongly resemble both ham and spam, and those that do not resemble either) which fall near 0.5. A remarkable property of chi-combining is that people have generally been sympathetic to its 'unsure' ratings: people usually agree that messages classed unsure really are hard to categorize. For example, commercial HTML email from a company you do business with is quite likely to score as unsure the first time the classifier sees such a message from a particular company. Spam and commercial email both use the language and devices of advertising heavily, so it is hard to tell them apart. Training quickly teaches the system how to identify desired commercial email by picking up clues, ranging from which company sent it and how they addressed you, to the kinds of products and services it offers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>SpamBayes users typically experience no false positives; this is not from an inherent strength of SpamBayes over similar statistical (or other) filters, but as a result of the unsure range. Essentially, the messages that would otherwise have been false positives are classified as unsure. The advantage of this system is that the volume of mail that the user must scan to find errors (both false positives and false negatives) is greatly reduced; typically between one and five percent of messages are classified as unsure, which is generally much lower than the percentage of mail that is spam.</p><p>As a result, users are more likely to take the time to scan the unsure folder than they would be to scan the entire spam folder, more able to identify the correct classification (rather than missing a false positive in a crowded spam folder) and more likely to appropriately train messages therein. The disadvantage of this system is that the percentage of messages that are classified as unsure is typically higher than the combined percentage of false negative and false positive messages obtained when using a classifier that does not include an unsure range. In simple terms, more messages must be manually corrected, but fewer messages must be manually examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Treatment of the unsure range for TREC 2005 Although any spam filter that generates a score (rather than a simple binary decision) for messages could be adapted to include an unsure range, the range is fundamental to the classifier that SpamBayes currently uses; there are three distinct clusters of messages scores (near 0.0, near 0.5, and near 1.0). Note that the classification is not a true ternary classification, however, such as systems like POPFile <ref type="bibr" coords="3,281.20,86.09,10.10,11.24" target="#b5">[6]</ref> and CRM114 <ref type="bibr" coords="3,349.00,86.09,10.10,11.24" target="#b6">[7]</ref> may provide, merely a method of distributing the message scores across the potential 0.0 -1.0 range.</p><p>The difficulty arises when attempting to utilise SpamBayes as a simpler binary decision engine, requiring a "ham" or "spam" output. The options are to either change the combining scheme that is used, or to set the ham-unsure and unsure-spam thresholds to the same value; while using a different combining scheme is likely to be more effective, this then considerably changes the method that SpamBayes is using, distancing any test results from results that could be expected from 'real life' usage. As such, for the purposes of the TREC spam track, the single-threshold technique was used -the resulting question is then to decide what value to assign to this threshold. While 0.5 is the most obvious choice, in practice, the unsure range tends to include more spam than ham; this is most likely the result of both the growing ham::spam imbalance in the average mail stream and the greater homogeneity (particularly over time) of ham as compared to spam. With this consideration, a threshold value around 0.4 would likely produce the best results. However, the lower the threshold is, the greater the chance of incurring additional false positives; since a false positive is generally considered many times worse than a false negative, the threshold for all four variants of the 2005 TREC spam track runs was set to 0.6 (incurring additional false negatives in order to avoid as many false positives as possible).</p><p>The figure of 0.6 was chosen fairly arbitrarily; little analysis was done to determine an optimal value. The primary reason for this is that it is the score output (see ROC analysis below) that is of more interest to the author than the simple false negative/false positives results; in addition, it appears that the balance of ham and spam in the unsure range is highly sensitive to the balance of ham and spam in the entire mail stream, which was an unknown factor in the tests. Automatic adjustment of the thresholds during the run was considered unnecessary, due both to the greater interest in the ROC analysis and the desire to keep the simulation as similar to 'real use' of SpamBayes as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Run Variants</head><p>Four variants of the SpamBayes distribution were submitted to the 2005 TREC spam track; all four variants included the same files, with selection of the variant completed by a simple script that selected the training script and SpamBayes configuration file to use. All four variants are possible using the standard SpamBayes distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">tam1 -train-on-everything/defaults</head><p>The first submission, tam1, uses all the default settings in the 1.1a1 SpamBayes distribution, other than the 0.6 value for the ham and spam thresholds (as above); this is also identical to the SpamBayes sample script provided in the initial 2005 TREC spam track 'jig', other than the threshold values, which were there set to 0.9. A brief outline of the tokenization methods used can be found in <ref type="bibr" coords="3,191.70,566.59,9.21,11.24" target="#b2">[3]</ref>. The training regime used in tam is train-on-everything. Every message is trained with the correct classification immediately after classification, and before any further message is classified.</p><p>This technique provides the classifier with the most information, but not necessarily the highest quality information; previous testing <ref type="bibr" coords="3,140.30,622.79,9.58,11.24" target="#b7">[8,</ref><ref type="bibr" coords="3,154.60,622.79,7.40,11.24" target="#b8">9]</ref> has shown that more minimalist training regimes are not only faster and result in smaller databases, but deliver superior results. The expectation was that tam1 would be the least effective of the four submissions, and was primarily intended as a base with which to compare the other results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>tam2 -fpfnunsure/bigrams While early testing <ref type="bibr" coords="3,145.90,692.99,14.80,11.24" target="#b9">[10]</ref> showed that using either character or word n-grams was less effective than simple split-on-whitespace unigrams, in late 2002 Robinson and Robert Woodhead independently came up with an idea for using both unigrams and bigrams, with a twist to avoid generating highly correlated clues. This idea was cleaned up and implemented by Peters, although it was only added to the SpamBayes code in late 2003; the technique is further outlined in <ref type="bibr" coords="4,81.90,100.09,9.21,11.24" target="#b2">[3]</ref>.</p><p>The tam2 submission differs from tam1 in only two respects: it enables the use_bigrams scheme outlined above (retaining default values for all non-threshold options as in tam1), and it uses the fpfnunsure training regime (train on all false positives, false negatives, and unsure messages<ref type="foot" coords="4,263.10,156.31,2.71,6.52" target="#foot_1">2</ref> ) rather than train-on-everything. This is the training regime that SpamBayes recommends users employ. Previous testing has indicated that the tiled unigram/bigram scheme delivers superior results in almost (but not all) corpora <ref type="bibr" coords="4,255.90,184.39,9.21,11.24" target="#b2">[3]</ref>; as above, testing has also indicated that a fpfnunsure training regime is always superior to train-on-everything.</p><p>As a result, the expectation was that tam2 would considerably outperform tam1; one complication is that the tiled unigram/bigram scheme does often increase the number of messages in the 'unsure' range, which is treated unusually (for SpamBayes) in the TREC submission. In addition, some previous testing <ref type="bibr" coords="4,383.10,254.69,10.10,11.24" target="#b2">[3]</ref> has shown the tiled unigram/bigram scheme to decrease accuracy when used in an 'incremental' testing setup (e.g. the incremental setup in <ref type="bibr" coords="4,474.00,268.69,9.21,11.24" target="#b2">[3]</ref>, and the 2005 TREC spam track jig), rather than in cross-validation testing; this is at odds with the anecdotal evidence of use of the scheme in practice, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">tam3 -train-to-exhaustion/bigrams</head><p>The third SpamBayes submission, tam3, uses identical tokenisation and classification options to tam2, but uses the trainto-exhaustion training regime. 'Training to exhaustion' <ref type="bibr" coords="4,289.60,352.99,14.80,11.24" target="#b10">[11]</ref> is a regime developed by Gary Robinson that resembles the perceptron algorithm <ref type="bibr" coords="4,160.10,367.09,13.58,11.24" target="#b11">[12]</ref>. In this regime, a collection of ham and spam are repeatedly trained (using any training method, although typically not train-on-everything; here fpfnunsure is used) until all messages in the collection are able to be successfully classified, or an iteration limit is reached. The regime aims to find the smallest subset of messages that, when trained on, will correctly identify the largest subset of messages.</p><p>Although previous testing <ref type="bibr" coords="4,182.60,437.29,10.10,11.24" target="#b7">[8]</ref> has indicated that train-to-exhaustion delivers results that are superior to train-oneverything, non-edge training, fpfnunsure, and fpfn, the training regime is much more resource intensive than any of the other regimes. In each iteration, all messages in the collection must be reclassified (whereas in the other regimes, no reclassification is done), and multiple messages may be trained (whereas in the other regimes, a maximum of one message is trained). In addition, more than one iteration is likely to be required (for tam3 and tam4 a maximum of ten iterations was permitted). The regime is therefore more suitable to batch training than training after every message.</p><p>The initial concept for using train-to-exhaustion with tam3 and tam4 was to include the entire known corpus in the training collection (i.e. after classifying 5,000 messages, there would be 5,000 messages in the training collection). This proved to be far too slow with large corpora, however, and so the final submission used the most recent 1,000 ham and 1,000 spam (for the first 1000 ham and 1,000 spam, all available messages were used). Although a batch training mode could have been used (e.g. only training every hundredth message, or at the end of every 'day'), the submissions used the simpler method of executing a full, 2,000 message, 10-iteration-maximum, train-to-exhaustion cycle for every incorrectly classified message. This technique proved far too slow in practice, far exceeding the two-second-per-message intended limit for the spam track jig. As a result, the larger corpora (the public corpus, in all but one variation, and the TM corpus) did not complete the tam3 and tam4 runs. If the train-to-exhaustion regime is used in future 'incremental' style simulations, such as the 2005 TREC spam track jig, then implementation of a batch version of this regime would be necessary. A potential method would be to batch-train once per simulation day, including only messages received during that 'day' (rather than the most recent 1,000 ham and 1,000 spam), and to add to the existing database, rather than replace it.</p><p>The train-to-exhaustion regime includes two, potentially significant, improvements over fpfnunsure, in addition to the cyclical, backtracking, nature of the regime. The first is that if a limit is placed on the number of ham and spam included in the training collection, and the database is recreated with each train-to-exhaustion cycle, then messages are automatically expired from the database. With the tam3 and tam4 submissions, this expiry was age-based -only the most recent 1,000 ham and 1,000 spam could be used for training; other selection (and therefore expiry) methods are also possible.</p><p>In addition to the automatic expiry, the train-to-exhaustion regime automatically balances the database. Each train-toexhaustion iteration processes the ham and spam collection independently, classifying and potentially training one ham message, then one spam message (and so on). Any messages left after one collection is exhausted are ignored for that training cycle (although for the tam3 and tam4 submissions an equal number of ham and spam were provided, so this would never be the case). Like other statistical filters, SpamBayes appears to be fairly sensitive to large imbalances between the number of ham and spam trained (see also below), and so it is expected that results would improve as a result of this automatic balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">tam4 -train-to-exhaustion/all options</head><p>The fourth, final, submission, tam4, uses the train-to-exhaustion training regime, like tam3 (above); as a result, it too failed to complete the runs of the larger corpora. The difference between tam3 and tam4 is that tam4 enables nearly all of the optional tokenizer techniques that SpamBayes 1.1a1 includes. The intent behind this submission was to get a general idea of whether the 'turn everything on' attitude that some users have would be beneficial or not; ideally each of these options should be considered individually (as was the case before they were added to the distribution), but this was outside the scope of the TREC evaluation. The hope was that tam4 would outperform tam3; even if one of the additional options was detrimental, it ought to be countered by benefits from other options. tam4 used the same thresholds as the other three submissions, and used the tiling unigram/bigram scheme as tam2 and tam3. A brief outline of the other various options appeared in the track notebook paper for the submission; the reader is referred to the SpamBayes website <ref type="foot" coords="5,147.40,493.51,2.71,6.52" target="#foot_2">3</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1</head><p>False positive and false negative rates Tables 2 through 5 outline simple false positive and false negative results for the two 4 private corpora and (full) public corpus, along with (for reference) the total size and ham::spam ratio for each corpus. At first glance, it appears that SpamBayes performs somewhat better than (the author) expected, given the 'unsure' range difficulty, but that all submissions perform extremely poorly on the SB corpus. The MrX and Public results are reasonably similar across the four submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Explaining the poor SB corpus results</head><p>It is difficult to determine why performance was so poor for the SB corpus; the two obvious differences are that the corpus is relatively small and is weighted much more towards ham than spam <ref type="foot" coords="6,371.60,100.11,2.71,6.52" target="#foot_3">5</ref> . One possibility is that the ham and spam are unevenly temporally distributed, so that a large number of ham are classified before any spam are seen; this particularly fits with the decrease in false negatives between tam1 and tam2 -since tam2 only trains when a mistake is made, this imbalance would have a much smaller effect. This could also explain the shift from excessive false negatives to high (but not as high) false positives and false negatives when comparing tam1 and tam2 with tam3 and tam4. Since the train-to-exhaustion regime reorganises the training order of messages, alternating between ham and spam, and also forces a 1::1 ham::spam ratio, this imbalance would have a significantly different effect.</p><p>The SB corpus is largely composed of mailing-list messages (48% of ham) and three frequent correspondents (12% of ham) <ref type="bibr" coords="6,95.40,226.59,9.21,11.24" target="#b3">[4]</ref>. "List" spam was particularly likely to be classified as ham (40, 8, and 4 messages for tam1, tam2, and tam3 respectively); this is not entirely unexpected, as mailing list messages tend to have a large number of tokens that are present in every message. Since around 3,000 mailing-list messages were trained as ham, these tokens end up as very strong ham clues; the best solution to this problem is to install a good spam filter in the mailing-list delivery process (as this filter has access to the raw message, prior to the mailing-list specific tokens being added to it). A similar explanation probably applies to "newsletter" spam (14, 10, and 5 messages for tam1, tam2, and tam3, respectively). "Sex" spam was also particularly likely to be classified as ham (147, 48, and 17 messages for tam1, tam2, tam3 respectively); this is much more difficult to explain. One possible explanation is that the ham messages included many messages about sex, neutralising those tokens; another explanation is that these messages were primarily image-based. SpamBayes does not yet do any processing of images, so image-based messages generally have very few tokens and end up in the unsure range (which, for TREC, would mean ham). These three spam genre account for 94%, 92%, and 93% of the false negatives for tam1, tam2, and tam3 respectively. Genre classification of false positives offers fewer clues as to the poor performance. "List" and "newsletter" false positives have the greatest increase across submissions, from 3 (tam1) to 32 (tam2) to 197 (tam3); as above, the most likely explanation for these errors is the temporal distribution of the messages, along with the ham::spam imbalance. If the spam "list" and "newsletter" messages arrived much earlier than the ham messages, then the large number of tokens would start out as strongly spam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3</head><p>ROCA and LAM% A difficulty of comparing filters is that the choice of threshold value includes a subjective judgement about the cost of false positives versus false negatives. To compare filters across all threshold values, Receiver Operating Characteristic (ROC) curves <ref type="bibr" coords="6,129.10,549.69,14.80,11.24" target="#b12">[13]</ref> were calculated for each run, and the area under the ROC curve was calculated. The area under the ROC curve is a cumulative measure of the effectiveness of the filter over all possible threshold values; for consistency with false negative and false positive values, the track uses the area above the ROC curve, as a percentage ((1 -ROCA)%). Note that the 'unsure' range does not fit particularly well in ROCA analysis, as the effectiveness is a measurement of a binary classification, and is not able to factor in the 'unsure' range.</p><p>Another single-figure result provided in the track is the logistic average misclassification percentage (LAM%) <ref type="bibr" coords="6,501.10,633.99,9.21,11.24" target="#b3">[4]</ref>, which is the geometric mean of the odds of ham and spam misclassification, converted back to a proportion. Note that neither of these measures impose any relative importance on ham or spam misclassification, and reward equally a fixed-factor improvement in the odds of either. While a single-figure statistic is convenient for comparison purposes, and is certainly preferred by the media and general public, it is somewhat misleading, given that there is a recognised difference in cost between false positives and false negatives (with the former considered much more expensive). However, until valid data quantifying the relative costs is available, determining a single-figure statistic remains difficult.</p><p>The tam1 submission had a ROCA value of 0.172 (the range of all submissions<ref type="foot" coords="7,388.90,114.21,2.71,6.52" target="#foot_4">6</ref> was 0.051 to 33.079; almost all were under 5.0), and the tam2 submission had a ROCA value of 0.209. These are fairly satisfactory results, and more indicative of actual performance than the raw false positive and false negative results; for example, tam1 was the 10 th most successful filter in terms of ROCA, but 3 rd in terms of false positives, and 24 th in terms of false negatives. This suggests that the 0.6 threshold was not ideal, and should have been lower in order to minimize the total number of errors, although considering the perceived (see above) cost of false positives, this trade-off could be considered worthwhile.</p><p>The tam1 submission had a LAM percentage of 1.07 (the range of all submissions was 0.62 to 35.88; most were under 2.0), and the tam2 submission had a LAM percentage of 1.10. Most interestingly, these are very similar results (more similar than the ROCA and, particularly, false positive/negative measures), indicating that the primary difference that the use_bigrams option and fpfnunsure regime made was to shift errors to/from false negatives to false positives. Comparatively, both tam1 and tam2 did not do as well with this measure, with tam2 ranked at 18 th ; selecting a threshold that minimized total errors rather than false positives would probably have improved this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4</head><p>Comparing submissions Comparing tam1 and tam2, tam2 does not perform as well as expected. The tam2 submission added the tiled unigram/bigrams technique and changed to a mistake-based training regime; both of these have given superior results in the majority of previous testing. The percentage of false negatives did decrease between tam1 and tam2 (vastly in the case of the unusual SB corpus), but this decrease came at the cost of an increase in false positives in all three corpora; whether this trade-off is acceptable would depend on the relative cost of false positives versus false negatives. The author's hypothesis is that the change to the fpfnunsure training regime improved results (particularly reducing the number of false negatives), but that the unigram/bigram tiling technique may have increased the number of false positives (particularly those that would normally fall in the unsure range). This hypothesis is based on previous research, where the tiling technique has increased the number of messages in the unsure range and has not always given superior results in an 'incremental' simulation, rather than on data provided from the TREC simulations themselves.</p><p>The shift from tam2 to tam3, which only changed the training regime from fpfnunsure to train-to-exhaustion, was expected to (at the cost of vastly increasing the simulation runtime) significantly improve the classification results. The actual results are unclear; with MrX there is a slight improvement in the false positive rate at the cost of a much smaller increase in the false negative rate, with the Public corpus there is a larger improvement in the false positive rate but at the cost of an even larger increase in the false negative rate, and with the SB corpus the change appears to mostly be a shifting of the error rate from mostly false negatives to both false negatives and false positives.</p><p>The failure of the train-to-exhaustion technique to deliver improved results (particularly in the comparison of tam2 and tam3, where nothing else was changed) is puzzling. It is possible that the restriction of the training collections to the most recent 1,000 ham and 1,000 spam had a detrimental effect, but the simple enforced balance of the training data was expected to improve results. Comparing tam3 to tam4, where many additional tokenization options were enabled, the MrX corpus had improved results (a small increase in the false positive rate, but a near halving of the false negative rate), while the unusual SB corpus essentially swapped the false positive and false negative rates (overall, a success, since a false positive is considerably worse that a false negative). It is odd that the tam4 changes had opposite effects in these two corpora, decreasing false negatives with MrX, but decreasing false positives with SB; unfortunately the size of the public corpus, combined with the lethargy of the tam3 and tam4 submissions, meant that results for tam4 were unable to be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5</head><p>Training data ham::spam balance SpamBayes does not perform well when the ratio of trained ham to trained spam is very large or very small (some users have reported ratios as high as 1::300 and 1::500!). The expectation, therefore, is that the percentages of false positives and false negatives would be lowest where the ham::spam ratio is closest to 1, and as the ratio approached zero and infinity, the false positive and false negative rates would increase. The data is not conclusive (particularly since there are only a small number of corpora represented), but, as shown in Figure <ref type="figure" coords="8,343.60,566.09,3.44,11.24" target="#fig_0">1</ref>, there is some evidence of this trend. The shape of the false positive data points does form a very shallow parabola, with the low centre point around the 1::1 point. The false negative data is less clear -there are some fairly high rates near 1::1, and fairly low rates towards 0.1, but many of the data points would fit into a curve of a similar shape to that of the false positive data points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>For the majority of the corpora tested as part of the TREC 2005 spam track (including the variants of the public corpus, not discussed here), SpamBayes performed better than the author expected, given that a somewhat arbitrary value was chosen for the ham and spam thresholds, in order to remove the unsure range. Anecdotal reports suggest that a false negative rate around 1%, a false positive rate of around 0%, and an unsure rate of around 1% to 5% is common with practical SpamBayes use. Dividing the 1%-5% unsure range (as above) into the false negative and false positive ranges results in an expectation of around a 2% to 4% false negative range, and a 0% to 2% false positive range. The results obtained match this fairly closely, with typical false positive values around 1% and typical false negative values around 2%-4% (with the exception of the SB corpus, where results are abysmal). These results strengthen those obtained in the research of the SpamBayes development team, with their own cross-validation and incremental testing simulations <ref type="bibr" coords="9,520.50,481.69,9.58,11.24" target="#b2">[3,</ref><ref type="bibr" coords="9,532.90,481.69,6.88,11.24" target="#b7">8,</ref><ref type="bibr" coords="9,72.00,495.79,6.39,11.24" target="#b8">9]</ref>.</p><p>However, the comparison of the four submissions is extremely unclear. Those techniques that have, both in simulation and in practice, performed well in the past, have failed to deliver improved results in the TREC simulations. The reasons for this failure are not apparent; it is possible that there will not be enough data available from the TREC simulations for the reasons to be conclusively determined. From limited analysis, it does appear that the expected performance decrease as the trained ham::trained spam ratio diverges from 1::1 is observed in the TREC simulations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,72.00,324.39,308.94,11.24;9,380.90,324.41,2.71,6.52;9,383.60,324.39,2.18,11.24"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: False positive and false negative rates across varying ham::spam ratios 7 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,71.50,229.89,469.00,138.94"><head>Table 1 :</head><label>1</label><figDesc>Breakdown of testing corpora.</figDesc><table coords="2,297.00,229.89,243.50,11.24"><row><cell># Ham</cell><cell># Spam</cell><cell>Ham::Spam Ratio</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,71.50,72.49,467.50,138.44"><head>Table 2 :</head><label>2</label><figDesc>Simple results summary for tam1.</figDesc><table coords="8,71.50,72.49,467.50,138.44"><row><cell></cell><cell>FP %</cell><cell cols="2">FN % Total Messages</cell><cell>Ham::Spam Ratio</cell></row><row><cell>MrX</cell><cell>0.2774%</cell><cell>2.614%</cell><cell>49086</cell><cell>0.23::1</cell></row><row><cell>SB</cell><cell>0.1446%</cell><cell>37.90%</cell><cell>7006</cell><cell>8.04::1</cell></row><row><cell>Public (Full)</cell><cell>0.2621%</cell><cell>4.270%</cell><cell>92189</cell><cell>0.75::1</cell></row><row><cell>Corpus</cell><cell>FP %</cell><cell cols="2">FN % Total Messages</cell><cell>Ham::Spam Ratio</cell></row><row><cell>MrX</cell><cell>1.448%</cell><cell>1.784%</cell><cell>49086</cell><cell>0.23::1</cell></row><row><cell>SB</cell><cell>1.054%</cell><cell>10.24%</cell><cell>7006</cell><cell>8.04::1</cell></row><row><cell>Public (Full)</cell><cell>0.8550%</cell><cell>1.467%</cell><cell>92189</cell><cell>0.75::1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,71.50,213.59,467.50,82.24"><head>Table 3 :</head><label>3</label><figDesc>Simple results summary for tam2.</figDesc><table coords="8,71.50,242.19,467.50,53.64"><row><cell>Corpus</cell><cell>FP %</cell><cell cols="2">FN % Total Messages</cell><cell>Ham::Spam Ratio</cell></row><row><cell>MrX</cell><cell>0.8239%</cell><cell>1.883%</cell><cell>49086</cell><cell>0.23::1</cell></row><row><cell>SB</cell><cell>6.708%</cell><cell>3.774%</cell><cell>7006</cell><cell>8.04::1</cell></row><row><cell>Public (Full)</cell><cell>0.2239%</cell><cell>4.667%</cell><cell>92189</cell><cell>0.75::1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,71.50,298.49,467.50,68.24"><head>Table 4 :</head><label>4</label><figDesc>Simple results summary for tam3.</figDesc><table coords="8,71.50,327.09,467.50,39.64"><row><cell>Corpus</cell><cell>FP %</cell><cell cols="2">FN % Total Messages</cell><cell>Ham::Spam Ratio</cell></row><row><cell>MrX</cell><cell>0.9720%</cell><cell>0.8944</cell><cell>49086</cell><cell>0.23::1</cell></row><row><cell>SB</cell><cell>4.075%</cell><cell>6.602%</cell><cell>7006</cell><cell>8.04::1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,72.00,369.39,164.98,11.24"><head>Table 5 :</head><label>5</label><figDesc>Simple results summary for tam4.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,77.20,709.65,58.96,10.12"><p>Beer and speech.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,77.50,684.35,462.42,10.12;4,72.00,697.25,467.93,10.12;4,72.00,709.85,330.01,10.12"><p>With typical SpamBayes use, where the unsure range is present, this regime differs from fpfn (false positives, false negatives, but not unsure messages); both, however, are commonly referred to as train-on-error and other similar names. For the 2005 TREC spam track, these two regimes are, in fact, identical, of course, since the unsure range was eliminated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,77.20,695.55,76.52,10.12"><p>http://spambayes.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,77.20,708.79,405.02,11.24"><p>However, the TM corpus had an even higher ham::spam ratio, and did not experience these poor results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="7,77.40,696.95,462.49,10.12;7,72.00,709.85,275.86,10.12"><p>Both ROCA and LAM% refer to aggregate scores over all corpora (including the SB corpus, for which SpamBayes results were very poor). tam3 and tam4 are not included, as they did not complete on all corpora.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="9,77.70,696.95,462.40,10.12;9,72.00,709.85,325.06,10.12"><p>This figure includes data for the two private corpora (tam1, tam2, tam3, tam4), and the public corpus (tam1, tam2) including the four additional variations. One data point (for the SB corpus) is outside of the range of the graph.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>As an open-source project, SpamBayes is developed by a team of volunteers, headed by <rs type="person">Tim Peters</rs>, and all credit for SpamBayes should be directed to them; however, the author assumes responsibility for any errors in this paper. The 2005 TREC spam track public corpus SpamBayes runs were executed on <rs type="institution">Massey University</rs>'s helix cluster [14]; many thanks to the <rs type="institution">Institute of Information and Mathematical Sciences</rs> for providing this resource. Finally, many thanks to <rs type="person">Gordon Cormack</rs> (and his team) for their assistance in massaging the SpamBayes submission so that it would complete the private corpora runs sufficiently fast, and for patiently waiting for the tam3 and tam4 runs to complete.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,75.44,156.29,3.44,11.24;10,112.50,156.29,427.50,11.24;10,108.00,167.59,222.68,11.24" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,287.60,156.29,158.94,11.24">Bayesian anti-spam classifier written in Python</title>
		<author>
			<persName coords=""><surname>Spambayes-Development-Team</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Spambayes</surname></persName>
		</author>
		<ptr target="http://spambayes.org" />
		<imprint>
			<date type="published" when="2003">2003. 2003 10 October</date>
		</imprint>
	</monogr>
	<note>cited</note>
</biblStruct>

<biblStruct coords="10,75.44,184.79,3.44,11.24;10,115.20,184.79,424.78,11.24;10,108.00,196.09,162.68,11.24" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Plan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Spam</surname></persName>
		</author>
		<ptr target="http://www.paulgraham.com/spam.html" />
		<imprint>
			<date type="published" when="2002-01">2002 January 2003. 2004 April 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.44,213.29,3.44,11.24;10,111.90,213.29,427.21,11.24;10,108.00,224.59,202.18,11.24" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,231.00,213.29,256.15,11.24">SpamBayes: Effective open-source, Bayesian based, email classification system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Whateley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,504.10,213.29,35.01,11.24;10,108.00,224.59,66.66,11.24">Conference on Email and Spam</title>
		<meeting><address><addrLine>Mountain View, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.44,241.79,3.44,11.24;10,111.40,241.79,418.22,11.24" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<title level="m" coord="10,222.90,241.79,223.60,11.24">TREC 2005 Spam Track Overview. in Text Retrieval Conference</title>
		<meeting><address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.44,259.09,3.44,11.24;10,111.40,259.09,309.88,11.24" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,168.40,259.09,149.75,11.24">A Statistical Approach to the Spam Problem</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,324.30,259.09,51.68,11.24">Linux Journal</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,75.44,276.29,3.44,11.24;10,119.00,276.29,420.98,11.24;10,108.00,287.59,181.98,11.24" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graham-Cumming</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Popfile</surname></persName>
		</author>
		<ptr target="http://popfile.sourceforge.net/cgi-bin/wiki.pl" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>cited 2006 25th January</note>
</biblStruct>

<biblStruct coords="10,75.44,304.79,3.44,11.24;10,111.40,304.79,373.52,11.24" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,169.30,304.79,68.22,11.24">CRM114 versus Mr</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Assis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,266.50,304.79,85.91,11.24">Text Retrieval Conference</title>
		<meeting><address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Spam Track</note>
</biblStruct>

<biblStruct coords="10,75.44,322.09,3.44,11.24;10,112.50,322.09,427.06,11.24;10,108.00,333.29,318.98,11.24" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,163.30,322.09,80.05,11.24">SpamBayes Exhaustion</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
		<ptr target="http://www.massey.ac.nz/~tameyer/research/spambayes/exhaustion.html" />
		<imprint>
			<date type="published" when="2004">2004 10th Feburary 2004</date>
		</imprint>
	</monogr>
	<note>cited 2005 20th October</note>
</biblStruct>

<biblStruct coords="10,75.44,350.59,3.44,11.24;10,112.60,350.59,426.96,11.24;10,108.00,361.79,222.28,11.24" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,155.90,350.59,84.82,11.24">Alex&apos;s spambayes testing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popiel</surname></persName>
		</author>
		<ptr target="http://www.wolfskeep.com/~popiel/spambayes/" />
		<imprint>
			<date type="published" when="2003">2003 25th December 2003</date>
		</imprint>
	</monogr>
	<note>cited 2006 25th January</note>
</biblStruct>

<biblStruct coords="10,79.72,379.09,3.86,11.24;10,111.20,379.09,428.76,11.24;10,108.00,390.29,328.18,11.24" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,150.70,379.09,152.11,11.24">The first trustworthy &lt;wink&gt; GBayes results</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Peters</surname></persName>
		</author>
		<ptr target="http://mail.python.org/pipermail/python-dev/2002-September/028513.html" />
		<imprint>
			<date type="published" when="2004-04-12">2002 2 September [cited 2004 April 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,79.72,407.59,3.86,11.24;10,113.10,407.59,426.78,11.24;10,108.00,418.79,302.28,11.24" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,172.00,407.59,138.85,11.24">Instructions for Training to Exhaustion</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Robinson</surname></persName>
		</author>
		<ptr target="http://garyrob.blogs.com/garys_longer_rants/2004/02/instructions_fo.html" />
		<imprint>
			<date type="published" when="2004-04-12">2004. 2004 April 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,79.72,436.09,3.86,11.24;10,112.70,436.09,426.10,11.24;10,108.00,447.29,432.08,11.24;10,108.00,458.59,127.98,11.24" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,267.20,436.09,271.60,11.24;10,108.00,447.29,47.49,11.24">Feature Selection, Perceptron Learning, and a Usability Case Study for Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,172.40,447.29,340.50,11.24">20th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,79.72,475.79,3.86,11.24;10,113.00,475.79,426.98,11.24" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,233.90,475.79,302.03,11.24">A Study of Supervised Spam Detection Applied to Eight Months of Personal Email</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,79.72,515.59,3.86,11.24;10,111.60,515.59,427.38,11.24;10,108.00,526.79,387.58,11.24" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,301.60,515.59,237.38,11.24;10,108.00,526.79,73.75,11.24">Performance Characteristics of a Cost-Effective Medium-Sized Beowulf Cluster Supercomputer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L C</forename><surname>Barczak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Messom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,187.90,526.79,279.60,11.24">Research Letters in the Information and Mathematical Sciences (RLIMS)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
