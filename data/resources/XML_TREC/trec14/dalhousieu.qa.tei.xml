<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,86.96,164.85,451.98,15.11;1,234.01,186.77,157.88,15.11">DalTREC 2005 QA System Jellyfish: Mark-and-Match Approach to Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-02-05">February 5, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,126.38,219.25,100.28,10.48"><forename type="first">Tony</forename><surname>Abou-Assaleh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.13,219.25,67.95,10.48"><forename type="first">Nick</forename><surname>Cercone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.57,219.25,51.37,10.48"><forename type="first">Jon</forename><surname>Doyle</surname></persName>
							<email>doyle@cs.dal.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,434.42,219.25,65.08,10.48"><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.06,233.20,77.78,10.48"><forename type="first">Chris</forename><surname>Whidden</surname></persName>
							<email>whidden@cs.dal.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">Dalhousie University</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,86.96,164.85,451.98,15.11;1,234.01,186.77,157.88,15.11">DalTREC 2005 QA System Jellyfish: Mark-and-Match Approach to Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-02-05">February 5, 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">41FA85BEBBAA8DDC10C68A50712BC9CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the second year that Dalhousie University actively participated in TREC. Three runs were submitted for the Question Answering track. Our results are below the median; however, they're signifantly larger than minimal, the lesson learnt will guide our future development of the system. Our approach was based on a mark-and-match methodology with regular expression rewriting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>2005 is the second year that Dalhousie University participated actively in TREC. The project DalTREC 1 was initiated in 2003 and serves as an umbrella for different TREC-related activities at Dalhousie. The tracks of interest were Question Answering, Spam, HARD, Genomics, and Enterprise. We submitted runs for the Question Answering (QA) track and the Spam track. This paper discusses the QA track.</p><p>In our previous work, we explored the application of several approaches to question answering in the overlapping area of unification-based and stochastic NLP (Natural Language Processing) techniques <ref type="bibr" coords="1,516.28,498.75,10.52,8.74">[5,</ref><ref type="bibr" coords="1,529.20,498.75,7.01,8.74">1]</ref>. Two novel methods that were explored relied on the notions of modularity and just-in-time sub-grammar extraction.</p><p>One of the learned lessons of the previous experiments was that the regular expression (RegExp) substitutions are a very succinct, efficient, maintainable, and scalable method to model many NL subtasks of the QA task. This was also observed in the context of lexical source-code transformations of arbitrary programming languages <ref type="bibr" coords="1,194.08,570.48,9.96,8.74">[2]</ref>, where it is an alternative to manipulations of the abstract syntax tree. In this context, RegExp transformations are more robust in the face of missing header files, errors, usage of macros, templates, and other embedded programming language constructs. This is the bottom-up part of our system. The top-down part consists of unification-based parsing and matching and, while being developed, it has not been yet deployed in the system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Regular Expression Rewriting</head><p>The basic method used at various components of the QA system is RegExp rewriting. The open angle bracket (&lt;) is used as a special escape character, hence we make sure that it does not appear in the source text, which is either a question or a passage. The basic text substrings, such as the target or named entities, are recognized using regular expressions and replaced with an angle-bracket-delimited expression. For example, the target is marked as &lt;TARGET&gt;. More commonly, a named entity e of type t is replaced with &lt;t_e s &gt;, where e s is the named entity e encoded as a string of printable characters that do not include &lt;. The RegExp rewriting can be seen as a bottom-up deterministic parsing technique. For example, the rewriting in which "&lt;NP_x&gt; &lt;VP_y&gt;" is replaced with "&lt;S_z&gt;" corresponds to the contextfree rule S → NP VP. The value z is obtained by decoding x and y, concatenating them, and encoding the result again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Architecture</head><p>An overview of the system architecture is shown in figure <ref type="figure" coords="2,349.50,507.73,4.98,8.74" target="#fig_0">1</ref> and it is described in more detail in the remaining of this section.</p><p>The current system could be described as a sequence of the following phases:</p><p>1. Question Processing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Processing</head><p>The Question Processing component takes the original TREC questions as the input and produces an annotated list of questions. The process includes question parsing and generating complete questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Question Parsing</head><p>Questions are parsed using RegExp substitutions. During the parsing stage, based on RegExp matching, the system identifies the question category. In addition, the system may identify some category and question specific metadata. For instance, if the category is SIZE, the system may also identify the measurement units. Some metadata extracted during parsing is analyzed using WordNet <ref type="bibr" coords="3,393.54,310.19,10.52,8.74">[3]</ref> to identify additional metadata and relationships between the target and what the question is asking about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Generating Complete Questions</head><p>The TREC 2005 questions, like the TREC 2004 questions, are grouped by targets. A set of questions around a target may include just an anaphoric reference to the target and not the full target name; e.g., 'it' or 'he'. These are incomplete questions in the sense that they do not provide sufficient information for the answer to be retrieved, but frequently require the target information. We find it useful to generate a complete version of the question for several reasons: (1) we can rely on a continuation of the strategy used on the previous TREC's, and (2) we can approach the QA problem with a general question-processing methodology instead of requiring that our system handles context specified in certain way, even though it may mean simply an addition of a target.</p><p>The original TREC 2004 questions are grouped by targets. A set of questions around a target may include just an anaphoric reference to the target and not the full target name (e.g., 'it', 'he'). In order to be able to independently use each question in passage extraction, they are rewritten so that they include the target name. We call this form a "complete question" or "full question". Additionally, as the result of parsing the questions, we obtain question category (i.e., the expected answer type), and some other optional information, such as type of the relation between the target and the answer. Question parsing and generating full questions is based on regular expression rewriting rules.</p><p>Generating the full question was done in the following way: We start with the original question. If target appears in the question, it is not changed. For a question of type "other," we generate the question "What is &lt;TARGET&gt; ?". Otherwise, we attempt the following substitutions: We evaluated this approach on TREC-2004 questions, and came up with the following results about the anaphora rewritings:</p><formula xml:id="formula_0" coords="3,86.17,590.31,287.67,20.25">elsif ($Qtype eq 'OTHER') { $_ = " What is &lt;TARGET&gt; " } elsif (/ it</formula><p>• 1 difficult question (almost independent of the target)</p><p>• 143 correctly rewritten questions • 2 acceptable but not considered strictly correct, namely How did Heaven's Gate commit suicide ? Why did Heaven's Gate commit suicide ?</p><p>• 40 incorrect but acceptable; hard to fix; e.g., "Gordon Gekko, what year was the movie released?"</p><p>and "Gordon Gekko, who plays the role?"</p><p>• 15 incorrect byt acceptable; could be relatively easily fixed</p><p>• 76 correctly decided not apply a rewriting,</p><p>• 65 "other" questions</p><p>• 5 unnecessary target addition due to a</p><p>• 4 unneccessary target addition due to singular-plural variations: 3 Crips vs. Crip, and 1 Rhodes scholars vs. Rhodes scolar,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieval</head><p>The passage retrieval from the AQUAINT data set is performed by an external search engine using the full questions generated in the question processing phase. We used two different sets of passages. The first set was the passages provided by NIST using the PRISE search engine. The second set was generate using our MySQL-based search engine that employed MySQL's full-text indexing functions. In both cases, the PRISE and MySQL, the results are treated in the same way-as passages relevant to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Target Marking</head><p>The string in the question that constitutes that target is identified during the question processing step (section 3.1). Using simple RegExp rewriting rules, the target is identified in the passages and replaced with the &lt;TARGET&gt; tag. In effect, this phase annotates sentences in the passages that may contain an answer. Currently, our system does not handle intra-sentence references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Question Category Marking</head><p>During this step, the system scans all the relevant passages and uses RegExp rewriting to mark entities that belong to the question category. The type of RegExp used depends on the question category and may be a simple keyword-based RegExp or a sophisticated multi-RegExp expression. For example, if the question category is COUNTRY, then a regular expression that contains a predefined list of country names is fetched, and all RegExp rewriting is applied to matches. These keyword-list RegExps are compiled manually from various sources. Matching the DATE category, on the other hand, requires a combination of keyword-list RegExps for month names and abbreviations and RegExps to recognize number, which are combined in a larger RegExp to match various date formats.</p><p>We use Perl's autoloading feature to load the appropriate RegExps at runtime. Thus, the concept of sub-grammar extraction is applied to RegExp rules where only the RegExp rules that are useful for the current question category are loaded into memory and applied to the passages.</p><p>Question category marking acts as a just-in-time annotation phase where only the passages that may be relevant to the current question are annotated, and the annotation data is customized based on the question category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Matching</head><p>At this point, the system has two sets of information: question metadata and annotated passages. These two sets of information are combined using special RegExp rules to generate candidate matches. The rules are applied sequentially. Every time a match is found, it is added to the list of matches. Currently, there is no separate ranking step for the extracted matches (although support for this step is built into the system); instead, the matches are ranked implicitly by the order in which the RegExp rules are applied. Hence, more specific RegExp rules are applied before the more general ones.</p><p>We were faced with two challenges when creating the RegExp rules. The first was related to determining how general should the most general rule be. Experimenting with questions from the TREC 2004 QA Track, we found that some of the general RegExp rules that we had in place captured erroneous matches for the questions that do not have an answer in the dataset. The second challenge was determining the order in which the RegExp rules should be applied. There were generic and question category specific RegExp rules. We were faced with the difficult tasks of determining the relative placement of generic and specific rules, as well as the placement of rules within a particular question category. For instance, our most specific generic rule is</p><formula xml:id="formula_1" coords="5,86.17,604.85,203.98,8.30">/&lt;_TARGET_&gt;.*?\b$Qrel &lt;${Qcat}([^&gt;]*)&gt;/</formula><p>where $Qrel is typically a verb that appears in the question, and $Qcat is the question category. However, this rule will only work for the questions for which the metadata $Qrel has been identified. When $Qrel is not available, we found that the RegExp rule /&lt;TARGET&gt;[^.]*?&lt;${Qcat}_([^&gt;]*)&gt;/ is too general for most question categories and should not be used. However, it is the best matching rule if the question category is one of AGE, CITY, COUNTRY, DATE, NATIONALITY, PROFESSION, PROVINCE, or YEAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Pre-run to Run filtering</head><p>The result of matching is a list of answers for each question-several answers may appear for any question, or we may have no answers at all. The task of the pre-run-to-run filtering component is to prepare the final run using the following rules:</p><p>• only the first answer is passed for any factoid question (TREC requirement),</p><p>• a NIL answer is introduced for questions with no generated answers,</p><p>• not more than seven answers are allowed per list question (rule of thumb),</p><p>• all answers to a list or 'other' question have to be unique,</p><p>• additionally, answers to an 'other' question may not appear as an answer to any previous question about the same target, and</p><p>• any answer longer than 100 bytes is truncated.</p><p>Although not used in our runs for TREC 2005, our system can handle web-reinforced question answering. In the case where external resources are used to located candidate answers, this phase is used to locate the relevant documents to support the answers in the AQUAINT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Document Ranking</head><p>The ID of documents that support the answers are identified when the matches are found during the Answer Matching phase (section 3.5). The document ranking phase simply checks that the final output conforms to the output specifications of TREC 2005, and corrects any potential errors. For example, if external resources are used to find an answer, but no supporting document is found in the given dataset, then a default document is used, since it is not allowed to leave the document field blank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We had two version of our system that differed in the way annotations are encoded within the text. In the first version, the tags are encoded into sequences of characters that can be matched in Perl using the RegExp /\S+/<ref type="foot" coords="6,149.96,565.83,3.97,6.12" target="#foot_0">2</ref> ; while in the second version, the encoded annotations match the RegExp /\w+/. <ref type="foot" coords="6,507.88,565.83,3.97,6.12" target="#foot_1">3</ref>Originally, our RegExp patterns were designed to work with \w+ encoding. Later, we felt that the \S+ encoding was more robust than the \w+ encoding. During development, we evaluated our system using TREC 2004 questions. We found that the two tag encodings gave comparable overall accuracy for factoid questions, but the sets of correctly answered questions sometimes differed.</p><p>We used two sets of passages in the submitted runs. One set was retrieved using out MySQL-based search engine, and the other was provided by NIST from the PRISE search engine. Since we were limited to 3 runs, we chose the following combinations of encodings and search engines based on their performance on the TREC 2004 questions (see table <ref type="table" coords="7,260.68,139.92,3.88,8.74" target="#tab_4">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TREC 2005 Evaluation Summary</head><p>Results of the three runs, Dal05m, Dal05p, and Dal05s, as returned from the NIST assessors, are summarized in table <ref type="table" coords="7,160.37,257.94,3.88,8.74" target="#tab_3">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on TREC 2004 Questions</head><p>During the development of Jellyfish, the system was constantly evaluated using the TREC 2004 questions.</p><p>As a result, the TREC 2004 questions became the training set for our system, while the TREC 2005 question were the testing set. The results of evaluating Jellyfish on TREC 2004 questions are listed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>The results from our system in TREC 2005 (table <ref type="table" coords="7,316.02,642.03,4.43,8.74" target="#tab_3">1</ref>) are within our expectations. It is evident from comparing table 1 and table 2 that our MySQL-based search engine has limited coverage. Although there is a noticeable reduction in accuracy on TREC 2005 questions due to the increase of the total number of questions from TREC 2004, the total number of correctly answered questions is comparable to that of the training data.</p><p>There is no significant difference in the accuracy when using either of the tag encoding schemes. We believe that using \S+ tag encoding will facilitate improving the RegExprules, modularizing, and layering them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,253.46,293.88,118.97,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,264.56,139.92,7.75,8.74;7,86.17,160.80,305.76,9.05;7,86.17,180.33,270.98,9.05;7,86.17,199.85,269.13,9.05"><head></head><label></label><figDesc>): Dal05m: MySQL-based search engine passage and \w+ tag encoding. Dal05p: PRISE search engine passage and \w+ tag encoding. Dal05s: PRISE search engine passage and \S+ tag encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,86.17,602.26,235.37,68.08"><head></head><label></label><figDesc>If none of them succeeds, we prepend "&lt;TARGET&gt;, " to the original question.</figDesc><table coords="3,86.17,602.26,235.37,68.08"><row><cell>elsif (/ they /)</cell><cell>{ s/ they / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/ their /)</cell><cell>{ s/ their / &lt;TARGET&gt;'s / }</cell></row><row><cell cols="2">elsif (/ theirs /) { s/ theirs / &lt;TARGET&gt;'s / }</cell></row><row><cell>elsif (/^It /)</cell><cell>{ s/ It / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/^Its /)</cell><cell>{ s/ Its / &lt;TARGET&gt;'s / }</cell></row><row><cell>elsif (/^He /)</cell><cell>{ s/ He / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/^His /)</cell><cell>{ s/ His / &lt;TARGET&gt;'s / }</cell></row><row><cell>elsif (/^She /)</cell><cell>{ s/ She / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/^Her /)</cell><cell>{ s/ Her / &lt;TARGET&gt;'s / }</cell></row><row><cell>elsif (/^They /)</cell><cell>{ s/ They / &lt;TARGET&gt; / }</cell></row><row><cell cols="2">elsif (/^Their /) { s/ Their / &lt;TARGET&gt;'s / }</cell></row><row><cell cols="2">elsif (/^Theirs /) { s/ Theirs / &lt;TARGET&gt;'s / }</cell></row><row><cell>/)</cell><cell>{ s/ it / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/ its /)</cell><cell>{ s/ its / &lt;TARGET&gt;'s / }</cell></row><row><cell>elsif (/ he /)</cell><cell>{ s/ he / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/ his /)</cell><cell>{ s/ his / &lt;TARGET&gt;'s / }</cell></row><row><cell>elsif (/ she /)</cell><cell>{ s/ she / &lt;TARGET&gt; / }</cell></row><row><cell>elsif (/ her /)</cell><cell>{ s/ her / &lt;TARGET&gt;'s / }</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,133.63,257.94,358.64,123.45"><head>Table 1 :</head><label>1</label><figDesc>. Jellyfish results from TREC 2005 QA Track</figDesc><table coords="7,133.63,278.27,358.64,81.27"><row><cell>Run</cell><cell cols="6">Correct Factual Unsupported Inexact List Other Session</cell></row><row><cell>Dal05m</cell><cell>27</cell><cell>0.075</cell><cell>4</cell><cell>10</cell><cell>0.017 0.056</cell><cell>0.056</cell></row><row><cell>Dal05p</cell><cell>40</cell><cell>0.110</cell><cell>4</cell><cell>5</cell><cell>0.033 0.088</cell><cell>0.086</cell></row><row><cell>Dal05s</cell><cell>41</cell><cell>0.113</cell><cell>4</cell><cell>5</cell><cell>0.033 0.088</cell><cell>0.087</cell></row><row><cell>MAX</cell><cell></cell><cell>0.713</cell><cell></cell><cell></cell><cell>0.468 0.248</cell><cell>0.534</cell></row><row><cell>MEDIAN</cell><cell></cell><cell>0.152</cell><cell></cell><cell></cell><cell>0.053 0.156</cell><cell>0.123</cell></row><row><cell>MIN</cell><cell></cell><cell>0.014</cell><cell></cell><cell></cell><cell>0.000 0.000</cell><cell>0.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,86.17,468.12,453.55,103.13"><head>table 2 ,</head><label>2</label><figDesc>where the suffix to Dal04 is interpreted as follows: m means MySQL-based search engine is used, p means the PRISE search engine is used, w means that \w+ tag encoding is used, and s mean that \S+ tag encoding is used.</figDesc><table coords="7,202.94,514.29,220.01,56.96"><row><cell>Run</cell><cell cols="4">Correct Factual List Combined</cell></row><row><cell>Dal04mw</cell><cell>40</cell><cell>0.174</cell><cell>0.093</cell><cell>0.147</cell></row><row><cell>Dal04pw</cell><cell>45</cell><cell>0.196</cell><cell>0.098</cell><cell>0.163</cell></row><row><cell>Dal04ms</cell><cell>41</cell><cell>0.178</cell><cell>0.083</cell><cell>0.146</cell></row><row><cell>Dal04ps</cell><cell>46</cell><cell>0.200</cell><cell>0.092</cell><cell>0.164</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,187.96,584.37,249.97,8.74"><head>Table 2 :</head><label>2</label><figDesc>Jellyfish evaluation using TREC 2004 Questions</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,101.42,657.80,164.15,6.99"><p>Matches a sequence of non-space characters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,101.42,667.30,273.55,6.99"><p>Matches a sequence of alphanumeric characters, including the underscore.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We present a relatively simple QA framework based on regular expression rewriting. We apply the concepts of modular grammar and just-in-time annotation to RegExprewrite rules. Although our system is still in the early stages of development, we have demonstrated that the mark-and-match approach with RegExprewriting is a robust method that can reduce the computation requirements of deeper analysis. We learned several lessons that will direct our further development of Jellyfish.</p><p>Future work includes fine-grained modularization of RegExprules, answer ranking, and better usage of syntactic and semantic information during the marking and the matching phases. In particular, we are working on incorporating shallow semantic parsing of the candidate answers in order to rank them.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,102.23,221.90,277.57,8.74" xml:id="b0">
	<monogr>
		<title level="m" coord="8,111.08,221.90,264.29,8.74">RegExp rewriting is a simple and powerful parsing technique</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,102.23,241.46,437.48,8.74;8,111.08,253.41,83.30,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,111.08,241.46,428.63,8.74;8,111.08,253.41,78.00,8.74">We have effectively used coarse-grained modularization of RegExp, and combined it with dynamic loading of RegExp</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,102.23,272.97,437.49,8.74;8,111.08,284.93,23.33,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,111.08,272.97,428.64,8.74;8,111.08,284.93,19.44,8.74">Fine grained modularization of RegExp is possible, and would simplify the task of creating RegExp rules</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,102.23,304.49,408.44,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,111.08,304.49,395.70,8.74">Using a macro system, such as Starfish, can greatly reduce the complexity of RegExp rules</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,102.23,324.05,437.50,8.74;8,86.17,353.80,8.08,12.62;8,110.39,353.80,210.96,12.62;8,86.17,492.09,75.53,12.62" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,111.08,324.05,428.64,8.74;8,86.17,353.80,8.08,12.62;8,110.39,353.80,210.96,12.62;8,86.17,492.09,75.53,12.62">Just-in-time RegExp-based annotation can lower the computation requirements of deeper analysis. 5 Conclusions and Future Work References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,101.68,516.95,438.04,8.74;8,101.67,528.90,413.48,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,516.85,516.95,22.87,8.74;8,101.67,528.90,197.25,8.74">From computational intelligence to web intelligence</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Cercone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lijun</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aijun</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kanlaya</forename><surname>Naruedomkul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,307.28,528.90,68.27,8.74">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="72" to="76" />
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,101.68,548.46,438.04,8.74;8,101.67,560.42,408.13,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,372.39,548.46,149.86,8.74">Lexical source-code transformation</title>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Abou-Assaleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,101.67,560.42,246.37,8.74">Proceedings of the STS&apos;04 Workshop at GPCE/OOPSLA</title>
		<meeting>the STS&apos;04 Workshop at GPCE/OOPSLA<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,101.68,579.98,356.26,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,193.70,579.98,180.20,8.74">WordNet: An Electronic Lexical Database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,101.68,599.54,438.04,8.74;8,101.67,611.49,438.05,8.74;8,101.67,623.45,320.42,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,170.23,599.54,234.33,8.74">Question answering using unification-based grammar</title>
		<author>
			<persName coords=""><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,176.40,611.49,146.25,8.74">Advances in Artificial Intelligence</title>
		<title level="s" coord="8,468.22,611.49,71.49,8.74;8,101.67,623.45,77.21,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Eleni</forename><surname>Stroulia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</editor>
		<meeting><address><addrLine>AI; Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001-06">2001. June 2001</date>
			<biblScope unit="volume">LNAI</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,101.68,643.00,438.04,8.74;8,101.67,654.96,403.82,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,169.71,643.00,224.24,8.74">Modular stochastic HPSGs for question answering</title>
		<author>
			<persName coords=""><forename type="first">Vlado</forename><surname>Kešelj</surname></persName>
		</author>
		<idno>CS-2002-28</idno>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<pubPlace>Waterloo, Ontario, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
