<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.15,96.38,420.96,15.15;1,97.75,118.29,415.76,15.15;1,271.24,140.21,68.77,15.15">TALP-UPC at TREC 2005: Experiments Using a Voting Scheme Among Three Heterogeneous QA Systems</title>
				<funder ref="#_cFfc95B">
					<orgName type="full">Spanish Research Department (ALIADO</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Universities, Research and Information Society (DURSI) of the Catalan Government</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund</orgName>
				</funder>
				<funder ref="#_n2CtRCR">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,122.95,174.08,66.87,8.77;1,189.82,172.53,1.36,6.12"><forename type="first">Daniel</forename><surname>Ferrés</surname></persName>
							<email>dferres@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.49,174.08,71.62,8.77;1,272.11,172.53,1.36,6.12"><forename type="first">Samir</forename><surname>Kanaan</surname></persName>
							<email>skanaan@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.78,174.08,108.43,8.77;1,391.21,172.53,1.83,6.12"><forename type="first">David</forename><surname>Dominguez-Sal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMA-UPC Software Department Department of Computer Architecture Universitat Politècnica de Catalunya Universitat Politècnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.45,174.08,79.02,8.77;1,480.47,172.53,1.36,6.12"><forename type="first">Edgar</forename><surname>González</surname></persName>
							<email>egonzalez@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,85.51,188.03,64.42,8.77;1,149.94,186.48,1.36,6.12"><forename type="first">Alicia</forename><surname>Ageno</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.61,188.03,71.88,8.77;1,232.48,186.48,1.36,6.12"><forename type="first">Maria</forename><surname>Fuentes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.15,188.03,94.15,8.77;1,337.30,186.48,1.36,6.12"><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.97,188.03,80.04,8.77;1,428.01,186.48,1.36,6.12"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,458.05,188.03,63.11,8.77;1,521.16,186.48,1.36,6.12"><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">TALP Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.15,96.38,420.96,15.15;1,97.75,118.29,415.76,15.15;1,271.24,140.21,68.77,15.15">TALP-UPC at TREC 2005: Experiments Using a Voting Scheme Among Three Heterogeneous QA Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E3B4151AA0E4E5D9751E9A3862E5A56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the experiments of the TALP-UPC group for factoid and 'other' (definitional) questions at TREC 2005 Main Question Answering (QA) task. Our current approach for factoid questions is based on a voting scheme among three QA systems: TALP-QA (our previous QA system), Sibyl (a new QA system developed at DAMA-UPC and TALP-UPC), and Aranea (a web-based data-driven approach). For defitional questions, we used two different systems: the TALP-QA Definitional system and LCSUM (a Summarization-based system).</p><p>Our results for factoid questions indicate that the voting strategy improves the accuracy from 7.5% to 17.1%. While these numbers are low (due to technical problems in the Answer Extraction phase of TALP-QA system) they indicate that voting is a succesful approach for performance boosting of QA systems. The answer to definitional questions is produced by selecting phrases using set of patterns associated with definitions. Its results are 17.2% of F-score in the best configuration of TALP-QA Definitional system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the experiments of the TALP-UPC group for factoid and 'other' questions at TREC 2005 Main Question Answering (QA) task.</p><p>The current approach for factoid questions is based on a voting scheme among three QA systems: TALP-QA, Sibyl and Aranea. TALP-QA is a multilingual open-domain Question Answering system under development at UPC for the past three years (see <ref type="bibr" coords="1,290.13,698.19,10.51,8.74" target="#b5">[6]</ref> and <ref type="bibr" coords="1,331.26,288.97,10.29,8.74" target="#b4">[5]</ref>). The approach is based on the use of indepth NLP tools and resources to create semantic information representation. Sibyl is a new QA system developed during the last year at DAMA-UPC and TALP-UPC. This system uses a set of robust NLP tools to exploit inherent discourse properties. Aranea<ref type="foot" coords="1,344.51,359.12,3.97,6.12" target="#foot_0">1</ref> is a Web-based factoid QA system that uses a combination of data redundancy and database techniques <ref type="bibr" coords="1,341.38,384.61,14.61,8.74" target="#b10">[11]</ref>.</p><p>For definitional questions we used two different approaches: the TALP-QA Definitional system and LCSUM. TALP-QA Definitional system is a threestage process: passage retrieval, pattern scanning over the previous set of passages, and finally a filtering phase where redundant fragments are detected and excluded from the final output. LCSUM is a summarizer based on Lexical Chains (see <ref type="bibr" coords="1,500.35,480.25,10.29,8.74" target="#b6">[7]</ref>). We used this summarizer to extract relevant information about the targets.</p><p>We have not designed any system for list questions. List questions are processed as factoid questions, but selecting answers among the ranked candidates that have a score higher than a certain threshold.</p><p>Finally, we outline below the organization of the paper. In Section 2, we present the overall architecture of the different factoid QA systems used and the voting scheme used for this kind of questions. Then, the definitional systems used at TREC 2005 are presented in Section 3. In section 4 and 5, we present the experiments and results obtained by our official runs at TREC 2005. Finally, in Section 6 and 7 we describe our evaluation and conclusions about the systems and the experiments.</p><p>In this section we describe our two factoid QA systems (TALP-QA and Sibyl), the Aranea QA System, and the voting scheme used. But first, we present the target substitution process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Target Substitution</head><p>The original questions of the TREC 2005 QA track are guided by a target. Because our current QA system does not process questions within context, we designed a component to substitute all the references to the target in the original question with the target. A set of heuristics, implemented by means of regular expression patterns, has been applied to solve some forms of coreference. If the substitution is not possible, then the target is added at the end of the question; following this pattern: Question + "in the" + &lt;TARGET&gt; ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TALP-QA System</head><p>TALP-QA is a multilingual open-domain Question Answering (QA) system under development at UPC for the past three years (see <ref type="bibr" coords="2,202.99,368.60,10.52,8.74" target="#b5">[6]</ref> and <ref type="bibr" coords="2,238.18,368.60,10.29,8.74" target="#b4">[5]</ref>). The system architecture has three phases that are performed sequentially without feedback: Question Processing (QP), Passage Retrieval (PR) and Answer Extraction (AE).</p><p>The TALP-QA approach is based on in-depth NLP processing and semantic information representation. A set of semantic constraints are extracted for each question. The answer extraction algorithm extracts and ranks sentences that satisfy the semantic constraints of the question. If matches are not possible the algorithm relaxes the semantic constraints structurally (removing constraints or making them optional) and/or hierarchically (abstracting the constraints using a taxonomy).</p><p>The version used in TREC 2005 is almost identical to the version used in TREC 2004 <ref type="bibr" coords="2,245.88,560.55,9.97,8.74" target="#b5">[6]</ref>, with the exception of the following modules: Question Classification, Document Indexing, and Answer Selection.</p><p>The main subsystems are described below, but first we will describe the processing tasks over the document collection and the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Collection Pre-processing</head><p>We have used the Lucene<ref type="foot" coords="2,179.25,665.76,3.97,6.12" target="#foot_1">2</ref> Information Retrieval (IR) engine to perform the PR task. We indexed the whole AQUAINT collection (i.e. about 1 million documents) and we computed the idf weight at document level for the whole collection.</p><p>We pre-processed the whole collection with linguistic tools (described in sub-section 2.2.2) to mark the part-of-speech (POS) tags, lemmas and Named Entities (NE) of the text. This information was used to build an index with two fields per document: i) the lemmatized text with POS tags, and the recognized Named Entities with its class, ii) the original text (forms) with Named Entity Recognition. The first field is used in a search by lemma, and the information of both fields is retrieved when a query succeeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Question Processing</head><p>The main goal of this subsystem is to detect the expected answer type and to generate the information needed for the other subsystems. For PR, the information needed is basically lexical (POS and lemmas) and syntactic, and for AE, lexical, syntactic and semantic.</p><p>For TREC 2005 we used a set of general purpose tools produced by the UPC NLP group and another set of public NLP tools. The same tools are used for the processing of both the questions and the retrieved passages. The following components were used:</p><p>• Morphological components, an statistical POS tagger (TnT) <ref type="bibr" coords="2,420.38,418.14,10.51,8.74" target="#b1">[2]</ref> and the WordNet lemmatizer (version 2.0) are used to obtain POS tags and lemmas. We used the TnT pre-defined model trained on the Wall Street Journal corpus.</p><p>• A modified version of the Collins parser, which performs full parsing and robust detection of verbal predicate arguments (see <ref type="bibr" coords="2,483.48,500.11,10.30,8.74" target="#b3">[4]</ref>).</p><p>• ABIONET, a Named Entity Recognizer and Classifier that identifies and classifies NEs in basic categories (person, place, organization and other). See <ref type="bibr" coords="2,382.03,558.16,9.97,8.74" target="#b2">[3]</ref>.</p><p>• Alembic, a Named Entity Recognizer and Classifier that identifies and classifies NEs with MUC classes (person, place, organization, date, time, percent and money). See <ref type="bibr" coords="2,442.63,616.22,9.97,8.74" target="#b0">[1]</ref>.</p><p>• EuroWordNet, used to obtain the following semantic information: a list of synsets (with no attempt to Word Sense Disambiguation), a list of hypernyms of each synset (up to the top of each hypernymy chain), and the EWN's Top Concept Ontology (TCO) class <ref type="bibr" coords="2,430.27,698.19,14.61,8.74" target="#b13">[14]</ref>.</p><p>• Gazetteers: location-nationality relations (e.g. Spain-Spanish) and actor-action relations (e.g. write-writer).</p><p>The application of these linguistic resources and tools, obviously language dependent, to the text of the question is represented in two structures:</p><p>• Sent, which provides lexical information for each word: form, lemma, POS, semantic class of NE, list of EWN synsets and, finally, whenever possible, the verbs associated to the actor and the relations between locations and their nationality.</p><p>• Sint, composed by two lists, one recording the syntactic constituent structure of the question (including the specification of the head of each constituent) and the other collecting the information about relations among constituents (subject, object and indirect object relations).</p><p>Once this information is obtained we can find the information relevant to the following tasks:</p><p>• Environment. The semantic process starts with the extraction of the semantic relations that hold between the different components identified in the question text. These relations are organized into an ontology of about 100 semantic classes and 25 relations (mostly binary) between them. Both classes and relations are related by taxonomic links. The ontology tries to reflect what is needed for an appropriate representation of the semantic environment of the question (and the expected answer).</p><p>The environment of the question is obtained from Sint, the semantic information included in Sent and EuroWordNet. A set of about 150 rules was built to perform this task.</p><p>• Question Classification. The most important information we need to extract from the question text is the Question Type (QT), which is needed by the system when searching the answer. The QT focuses the type of expected answer and provides additional constraints. Currently we are working with about 26 QTs.</p><p>The Question Classification module is composed of 72 hand made rules. These rules use a set of introducers (e.g. 'where'), and the predicates extracted from the environment (e.g. location, state, action,...) to classify the questions.</p><p>• Semantic Constraints. The Semantic Constraints Set (SCS) is the set of semantic relations that are supposed to be found in the sentences containing the answer. The SCS of a question is built basically from its environment. The environment tries to represent the whole semantic content of the question while the SCS should represent a part of the semantic content of the sentence containing the answer. Mapping from the environment into the SCS is not straightforward. Some of the relations belonging to the environment are placed directly in the SCS, some are removed and some are modified (usually to become more general) and, finally, some new relations are added (e.g. type of location, type of temporal unit,..., frequently derived from the question focus words). Relations of SCS are classified into two classes: Mandatory Constraints (MC) and Optional Constraints (OC). MC have to be satisfied in the answer extraction phase, OC are not obligatory, their satisfaction simply increases the score of the answer.</p><p>In order to build the semantic constraints for each question a set of rules (typically 1 or 2 for each type of question) has been manually built.</p><p>The environment is basically a first order formula with variables denoted by natural numbers (corresponding to the tokens in the question). Several auxiliary predicates over this kind of formulas are provided and can be used in these rules. Usually these predicates allow the inclusion of filters, the possibility of recursive application and other generalization issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Passage Retrieval</head><p>The main function of the passage retrieval component is to extract small text passages that are likely to contain the correct answer. Document retrieval is performed using the Lucene Information Retrieval system. For practical purposes we currently limit the number of documents retrieved for each query to 1000. The passage retrieval algorithm uses a datadriven query relaxation technique: if too few passages are retrieved, the query is relaxed first by increasing the accepted keyword proximity and then by discarding the keywords with the lowest priority. The reverse happens when too many passages are extracted. Each keyword is assigned a priority using a series of heuristics fairly similar to <ref type="bibr" coords="3,397.54,662.33,14.61,8.74" target="#b12">[13]</ref>. For example, a proper noun is assigned a priority higher than a common noun, the question focus word (e.g. "state" in the question "What state has the most Indians?") is assigned the lowest priority, and stop words are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Factoid Answer Extraction</head><p>After PR, for factoid AE, two tasks are performed in sequence: Candidate Extraction (CE) and Answer Selection (AS). In the first component, all the candidate answers are extracted from the highest scoring sentences of the selected passages. In the second component the best answer is chosen.</p><p>• Candidate Extraction. The process is carried out on the set of passages obtained from the previous subsystem. First, these passages are segmented into sentences and each sentence is scored according to its semantic content using the tf * idf weighting of the terms from the question and taxonomically related terms occurring in the sentence <ref type="bibr" coords="4,181.97,291.51,14.62,8.74" target="#b11">[12]</ref>. The linguistic process of extraction is similar to the process carried out on questions and leads to the construction of the environment of each candidate sentence.</p><p>Once the set of sentence candidates has been preprocessed the application of the extraction rules follows an iterative approach. In the first iteration all the Mandatory Constraints have to be satisfied by at least one of the candidate sentences. If the size of the set of candidate sentences satisfying the MC is smaller than a predefined threshold a relaxation process is performed and a new iteration follows otherwise the extraction process is carried out.</p><p>The relaxation process of the set of semantic constraint is performed by means of structural or semantic relaxation rules, using the semantic ontology. Two kinds of relaxation are considered: i) moving some constraint from MC to OC and ii) relaxing some constraint in MC substituting it for another more general in the taxonomy. Once the SCS is relaxed the score assigned to the sentences satisfying it is decreased accordingly.</p><p>The extraction process consists on the application of a set of extraction rules on the set of sentences that have satisfied the MC. The Knowledge Source used for this process is a set of extraction rules owning a credibility score. Each QT has its own subset of extraction rules that leads to the selection of the answer. If no answer is extracted from any of the candidates a new relaxation step is carried out followed by a new iteration step. If no sentence has satisfied the MC or if no extraction rule succeeds when all possible relaxations have been performed the question is assumed to have no answer.</p><p>• Answer Selection After all candidates have been extracted, a single one must be selected as the answer. For this process we have used Support Vector Machines ( <ref type="bibr" coords="4,448.10,141.39,14.87,8.74" target="#b15">[16]</ref>).</p><p>Specifically, we have used the framework for ranking defined in <ref type="bibr" coords="4,410.56,171.36,10.52,8.74" target="#b7">[8]</ref> and implemented in SVM-Light <ref type="foot" coords="4,358.74,181.74,3.97,6.12" target="#foot_2">3</ref> . For each candidate we extract the following attributes: the relaxation level in which the candidate has been extracted, the rule which allowed the extraction of the candidate, the rule score, the semantic score, and the passage score.</p><p>The ranking uses a linear kernel. The best ranked candidate is given as the answer.</p><p>The SVM was trained with the corpora of questions from TREC8 to TREC12, a total of 2392 questions. These questions were processed with a version of our system and the obtained candidates were checked against the official answers provided in the TREC website. Of the 2392 questions, candidates were found for 1592, and only 222 questions had the right answer among their candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sibyl: Robust Harnessing of Discourse Properties</head><p>The Sibyl QA system implements a divergent approach from the TALP-QA system introduced in the previous section. While the first system described in this paper uses complex resources, e.g. full parsing and semantic dictionaries, to achieve an in-depth understanding of the answer and question texts, the second system we developed uses only robust NLP tools to exploit inherent discourse properties, such as locality and density of question keywords in the proximity of candidate answers. This system follows the same framework previously introduced, i.e. Question Processing (QP) -Passage Retrieval (PR) -Answer Extraction (AE), but most of the components and the NLP resources employed are completely different. We chose not only to implement a radically different approach to QA but also to use different NLP tools, e.g. NERC, in order to maximize the differences between the individual QA systems, a key feature to successful voting. We describe the relevant components and resources used in the new system next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Question Processing</head><p>The QP component implements two tasks: (i) it detects the type of the expected answer, and (ii) it converts the NLP question into a list of prioritized keywords to be used for document/passage retrieval.</p><p>The first task is implemented using a question classification framework largely inspired by <ref type="bibr" coords="5,244.08,158.04,9.97,8.74" target="#b8">[9]</ref>. Similarly to <ref type="bibr" coords="5,84.33,170.00,9.97,8.74" target="#b8">[9]</ref>, we extract from each question to be classified a rich of set features as follows: (i) we generate unigrams and bigrams for all the question words; (ii) we generate unigrams and bigrams for the head words of all basic syntactic phrases detected in the question; (iii) in all the n-gram features created we identify the first word of the question and the question focus word, both of each give strong hints of the question type; and (iv) all lexicalized n-grams constructs are expanded using the semantic classes provided by <ref type="bibr" coords="5,290.13,277.59,10.51,8.74" target="#b8">[9]</ref> and the proximity-based thesaurus supplied by <ref type="bibr" coords="5,282.38,289.55,14.61,8.74" target="#b9">[10]</ref>. Unlike <ref type="bibr" coords="5,103.23,301.50,9.97,8.74" target="#b8">[9]</ref>, we did not implement a hierarchy of classifiers, but rather opted for a single, flat classifier using Maximum Entropy. The classifier was trained using the training set of questions provided by <ref type="bibr" coords="5,287.36,337.37,9.96,8.74" target="#b8">[9]</ref>, which includes 50 question classes. On the same testing data as <ref type="bibr" coords="5,125.06,361.28,9.97,8.74" target="#b8">[9]</ref>, our classifier obtains an accuracy of 88.4%. Once a question is classified, the expected answer type is set using a mapping generated off-line from the 50 question classes to one of the 9 NE classes recognized by our NERC.</p><p>The selection of question keywords for passage retrieval is implemented using the heuristics for keyword priority reported by <ref type="bibr" coords="5,187.99,448.59,14.62,8.74" target="#b12">[13]</ref>. Essentially, we favor proper names over nouns, which in turn have a higher priority than verbs, etc. The lowest priority is assigned to the question focus word, which is unlikely to appear in candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Passage Retrieval</head><p>The passage retrieval algorithm used by Sibyl is similar to the one used by TALP-QA. In a nutshell, we use an incremental query relaxation technique that adjusts both the keyword proximity and the number of keywords included in the query, until a certain number of documents and passages is retrieved. Because our answer ranking algorithm works better when more question keywords are found in the candidate answer contexts, we first relax keyword proximity and only if the desired number of documents/passages is not obtained with the largest acceptable proximity we discard lower priority keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Answer Extraction</head><p>The answer extraction component ranks candidate answers based on the properties of the context where they appear in the retrieved passages. We consider as candidate answers all named entities of the same type as the answer type detected by the question processing component. Candidate answers are ranked using a set of six heuristics, inspired by <ref type="bibr" coords="5,459.82,159.62,14.62,8.74" target="#b12">[13]</ref>:</p><p>• Same word sequence -computes the number of words that are recognized in the same order in the answer context;</p><p>• Punctuation flag -true when the candidate answer is followed by a punctuation sign;</p><p>• Comma words -computes the number of question keywords that follow the candidate answer, when the later is succeeded by comma. The last two heuristics are a very basic detection mechanism for appositive constructs, a common form to answer a question;</p><p>• Same sentence -the number of question words in the same sentence as the candidate answer.</p><p>• Matched keywords -the number of question words found in the answer context.</p><p>• Distance -the largest distance (in words) between two question keywords in the given context. The last three heuristics quantify the proximity and density of the question words in the answer context, which are two intuitive measures of answer quality.</p><p>All these heuristics can be implemented without the need for any NLP resources outside of a basic tokenizer.</p><p>For each candidate answer, these six values are then converted into an answer score using the formula proposed by <ref type="bibr" coords="5,403.42,522.58,14.62,8.74" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">NLP Resources</head><p>The two NLP tools required by this system are: recognition of basic syntactic phrases, i.e. chunking, for QP, and named entity recognition and classification (NERC) for AE. The chunker used was trained using a series of oneversus-all classifiers for each syntactic category. Each classifier was implemented using Support Vector Machines (SVM) with a polynomial kernel of degree 2. We trained the chunker on the corpora provided by the 2000 CoNLL shared task <ref type="bibr" coords="5,446.41,674.28,14.62,8.74" target="#b14">[15]</ref>. On the testing data from the same evaluation exercise, our chunker obtains an F1 measure of 95.21.</p><p>The NERC employed by this QA system recognizes the following 9 semantic categories: location, person, organization, and other miscellaneous names; times and dates; monetary values; percents and numbers. The first four categories (all names) are recognized with a system very similar with the previouslyintroduced chunker: one-versus-all SVM classifiers trained on the CoNLL shared task data <ref type="bibr" coords="6,259.80,153.19,14.61,8.74" target="#b14">[15]</ref>. On the CoNLL testing data, this system obtains an F1 measure of 87.50. Temporal entities and percents are recognized with the Alembic system <ref type="bibr" coords="6,234.65,189.05,9.96,8.74" target="#b0">[1]</ref>. Finally, all other numbers are identified with an in-house system based on regular expression grammars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Aranea</head><p>The QA system Aranea took part in TREC 2002, TREC 2003 and TREC 2004 evaluations, and is described in <ref type="bibr" coords="6,117.14,282.78,14.61,8.74" target="#b10">[11]</ref>. Aranea uses two different techniques:</p><p>• Knowledge Annotation. For some very frequent fixed-pattern questions such as What is the population of X?, What is the atomic symbol of X? or Who was the second president of the United States?, structured databases available in the web, such as the CIA World Factbook or biography.com are queried. The question is transformed into a query using simple patterns.</p><p>• Knowledge Mining. For the rest of the questions, the keywords of the question are detected and a web search engine (Google, Teoma) is used to retrieve passages. As the web is extremely redundant, the answer is expected to be found expressed in terms similar to the question, and to be extractable using simple pattern matching techniques.</p><p>We executed the open source version of Aranea using Google and Teoma as search engines to get extra evidence for list and factoid question candidates. The answers provided by Aranea are not directly selectable, as they do not come from the AQUAINT corpus, yet can boost candidates coming from the other two QA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Voting Scheme</head><p>Our voting algorithm selects the final answer to each factoid question from the lists of the best 20 candidates extracted by each one of our QA systems: TALP-QA and Sibyl. This is done in two separated stages: considering that the answer for a question is the pair &lt; c, d &gt; of both the best candidate c answering the question and a document d in which it occurs, the first stage selects c. Given that this text can occur in a set of documents, the second stage selects document d as the most plausible one from the set.</p><p>In order to select the best candidate c, a score s 1 (c i ) is computed as follows for each different candidate c i occurring in the lists of top 20 answers:</p><formula xml:id="formula_0" coords="6,361.87,160.60,124.91,27.27">s 1 (c i ) = o∈Occ(ci) 1 ranking(o)</formula><p>where Occ(c i ) is the set of occurrences o of candidate c i in a) the list of top 20 candidates achieved by TALP-QA, b) the list of top 20 ones achieved by Sibyl, and c) the list of top 20 ones achieved by Aranea (used to take into account evidences of the answer in the web). Function ranking(o) is the ranking in which occurrence o is located in these lists. So, this score promotes those candidates located highest in the lists of top 20. Taking into account these scores, the candidate with highest score is selected to be c.</p><p>Finally, in order to select the most plausible document d for the answer, a score s 2 (d i ) is computed for each document d i in which the selected candidate c occurs. This scores is as follows:</p><formula xml:id="formula_1" coords="6,358.18,370.86,132.30,27.27">s 2 (d i ) = o∈Occ(di,c) 1 ranking(o)</formula><p>where Occ(d i , c) is the set of occurrences o of candidate c in document d i . The document with highest score is selected to be d.</p><p>The same scores are used for list questions. However, for list questions a list of answers is provided. Firstly, the list of those candidates c i extracted by systems TALP-QA and Sibyl achieving a score s 1 (c i ) higher than a threshold (80% was used) is selected. Then for each of these candidates, the best document d is selected, as explained for factoid questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definitional QA Systems</head><p>We describe below our two approaches for definitional QA: TALP-QA Definitional and LCSUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TALP-QA Definitional System</head><p>The TALP-QA Definitional system has three steps: first, the 50 most relevant documents with respect to the target are retrieved, from which the passages referring to the target are retrieved; second, sentences referring to the target are extracted from the previous set of documents, and last, redundant senteces are removed from the final output of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Document and Passage Retrieval</head><p>An index of documents has been created using Lucene that searches using lemmas instead of words. The search index has two fields: one with the lemmas of all non-stop words in the documents, and another with the lemmas of all the words of the documents that begin with a capital letter. The target to define is lemmatized, stopwords are removed and the remaining lemmas are used to search into the index of documents. Moreover, the words of the target that begin with a capital letter are lemmatized; the final query sent to Lucene is a complex one, composed of one subquery using document lemmas and another query containing only the lemmas of the words that begin with a capital letter. This second query is intended to search correctly the targets that, although being proper names, are composed or contain common words. For example, if the target is 'Liberty Bell 7', documents containing the words 'liberty' or 'bell' as common names are not of interest; the occurrence of these words is only of interest if they are proper names, and as a simplification this is substituted by the case the words begin with a capital letter. The score of a document is the score given by Lucene. Once selected a number of documents (50 in the current configuration), the passages (blocks of 200 words) that refer to the target are selected for the next phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Sentence Extraction</head><p>The objective of the second phase is to obtain a set of candidate sentences that might contain interesting information about the target. As definitions usually have a certain structure, as appositions or copulative sentences, a set of patterns has been manually developed in order to detect these and other expressions usually associated with definitions (for example, '&lt;phrase&gt; , &lt;target&gt;', or '&lt;phrase&gt; be &lt;target&gt;'). The sentences that match any of these patterns are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Sentence Selection</head><p>In order to improve precision in the system's response, redundant sentences are removed from the set of extracted sentences from the previous step. The redundance detection first creates a set with the first sentence (a sentence from the best scored document) and then adds to the set all the sentences whose word coincidence with the sentences in the set does not exceed a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LCSUM System</head><p>LCSUM is a summarizer based on Lexical Chains (see <ref type="bibr" coords="7,331.80,99.85,10.30,8.74" target="#b6">[7]</ref>). We used the English version of this system to extract relevant information about the targets. The summarization system receives as input the passages extracted by the Passage Retrieval module of the TALP-QA system. Firstly, for each target, the summarizer uses all the passages extracted in all the questions related to the target. Then, the lexical chains are computed for each passage related to the target. Finally, the first sentence with some word in a lexical chain is selected as a summary of the passage, trying not to exceed 300 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We designed a set of experiments for factoid, list, and 'other' questions (see Table <ref type="table" coords="7,435.29,285.37,3.88,8.74" target="#tab_0">1</ref>). Concretely, we submitted 3 runs: run1 (talpupc05a), run2 (talpupc05b) and run3 (talpupc05c).</p><p>The first run (run1) uses the TALP-QA system for factoid and list questions, and the TALP-QA Definitional system for 'other' questions. The second run (run2) consists of a voting scheme among TALP-QA and Sibyl for factoid and list QA, and another configuration for the TALP-QA Definitional system for 'other' questions. Finally, the third run (run3) uses a voting scheme among TALP-QA, Sibyl and Aranea for factoid and list QA, and the LCSUM summarizer for 'other' questions.</p><p>The Document Ranking experiments were the following: run1 uses only documents from the TALP-QA Factoid and Definitional systems. The second run (run2) uses documents from both TALP-QA and Sibyl for factoid and list, and TALP-QA Definitional for others. The third run (run3) uses documents from both TALP-QA and Sibyl for factoid and list (because Aranea do not provide documents from AQUAINT) and TALP-QA for 'other' (because LC-SUM uses passages retrieved by TALP-QA system). Finally, due to the fact that the task was not required to retrieve documents from 'other' questions, run2 and run3 were identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Factoid </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>This section presents the evaluation of the TALP-QA system for factoid Questions and the global results at TREC 2005.</p><p>• Question Processing. This subsystem has been manually evaluated for factoid questions (see Table <ref type="table" coords="8,138.00,159.65,4.43,8.74" target="#tab_1">2</ref>) and the following components: target substitution in the original question, basic NLP tools (POS, NER and NEC), semantic preprocessing (Environment, MC and OC construction) and finally, Question Classification (QC).</p><p>In the following components the errors are cumulative: basic NLP tools (NER is influenced by POS-tagging errors and NEC is influenced by NER and POS-tagging errors), semantic preprocessing (the construction of the environment depends on the errors in the basic NLP tools and the syntactic analysis, the MC and OC errors are influenced by the errors in the environment), and QC (is influenced by the errors in the basic NLP tools and the syntactic analysis). • Passage Retrieval. The evaluation of this subsystem was performed using the set of correct answers given by the TREC organization (see Table <ref type="table" coords="8,119.32,558.75,3.88,8.74" target="#tab_3">3</ref>).  This section summarizes the evaluation of our participation in the TREC 2005 Main QA and Document Ranking tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsystem</head><p>• Question Answering Task. Our system obtained a final score of 0.088 in run1, 0.125 in run2, and 0.116 in run3 (see Table <ref type="table" coords="9,248.11,161.46,3.88,8.74" target="#tab_5">5</ref>). We conclude with a summary of the system behaviour for each question class:</p><p>-Factoid Questions. The accuracy over factoid questions is 7% in run1, 14.6% in run2, and 17.1% in run3 (see 5). The results of the TALP-QA system (run1) are low due to errors in the Candidates Extraction module. Otherwise, the voting scheme is useful as seen in runs 2 and 3.</p><p>The TALP-QA system (run1) has been evaluated in its three phases: i) Question Processing. The Question Classification subsystem has an accuracy of 76.79%. We improved slightly the results of this component with respect to the TREC 2004. In the previous evaluation we obtained an accuracy of 74.34%. These are good results if we take into account that in TREC 2005 has increased the average length of both questions and targets. ii) Passage Retrieval. We evaluated that 62.60% of questions have a correct answer in their passages. The evaluation taking into account the document identifiers shows that 46.37% of the questions are definitively supported. The accuracy of our PR subsystem has decreased in comparison with the TREC 2004 evaluation (72.41% and 58.62% of accuracy for the previous measures respectively). This drop may be due to the increase of the average question length at TREC 2005.</p><p>iii) Answer Extraction. The accuracy of the AE module for factoid questions for which the answer occurred in our selected passages is 5.79%. This poor accuracy is due to a technical error in the AE module. Otherwise, we expect to improve these results by reducing the error rate in the construction of the environment, MC and OC.</p><p>-Other questions. The results for the questions in the 'other' category were 17.20%, 16.40%, and 7.9% F-score in run1, run2 and run3 respectively. The two runs with the TALP-QA Definitional system, had both similar results (17.2% and 16.4% of f-score), and they differ in the threshold applied in the sentence selection phase (70% and 60% respectively) in order to exclude redundant fragments from the final output of the system. LCSUM obtained a F-score of 7.9%, mainly because this summarizer has not its own Passage Retrieval system and used the passages retrieved by TALP-QA for factoid questions.</p><p>-List Questions. The F-score over list questions is clearly poor : 2.4% in run1, 2.6% in run2, and 2.8% in run3.</p><p>• Document Ranking Task. The results of the Document Ranking task are presented in Table <ref type="table" coords="9,330.53,291.40,3.88,8.74" target="#tab_6">6</ref>. Our system obtained an Average Precission of 0.1191 (run1) and 0.1468 (run2 and run3), a R-Precission of 0.1287 (run1) and 0.1685 (run2 and run3). The Document Raking Median of over all runs of TREC 2005 was 0.1574. We obtained an Average Precission Difference over all runs of -32.15% (run1) and -7.22% (run2 and run3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We combined the results of three heterogeneous factoid QA Systems: TALP-QA (a precission-oriented QA system), Sibyl (a recall-oriented QA system) and ARANEA (a recall-oriented and Web-based QA system). The resulting voting scheme has been successful, improving the accuracy over run1 with 108% in run2 and with 144% in run3.</p><p>The results in factoid questions were 7% of accuracy in the run without voting, and 14.6% and 17.1% in the runs with voting. While these numbers are low (due to technical problems in the Answer Extraction phase of TALP-QA system) they indicate that voting is a succesful approach for performance boosting of QA systems. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,310.61,603.22,228.64,81.61"><head>Table 1 :</head><label>1</label><figDesc>Experiments at TREC 2005 QA Main Task.</figDesc><table coords="7,320.46,603.22,208.94,59.75"><row><cell></cell><cell>QA</cell><cell>Other QA</cell></row><row><cell>run1</cell><cell>TALP-QA</cell><cell>TALP-QA Def. (1)</cell></row><row><cell cols="3">run2 TALP-QA &amp; Sibyl TALP-QA Def. (2)</cell></row><row><cell cols="2">run3 TALP-QA &amp; Sibyl</cell><cell>LCSUM</cell></row><row><cell></cell><cell>&amp;Aranea</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,353.38,228.65,141.78"><head>Table 2 :</head><label>2</label><figDesc>Results of Question Processing evaluation for the TALP-QA system.</figDesc><table coords="8,94.10,353.38,184.45,107.97"><row><cell></cell><cell>Accuracy</cell></row><row><cell>Target Substitution</cell><cell>89.83% (309/344)</cell></row><row><cell>POS-tagging</cell><cell>98.87% (3149/3185)</cell></row><row><cell>NE Recognition</cell><cell>93.53% (434/464)</cell></row><row><cell>NE Classification</cell><cell>82.11% (381/464)</cell></row><row><cell>Environment</cell><cell>49.45% (179/362)</cell></row><row><cell>MC</cell><cell>31.77% (115/362)</cell></row><row><cell>OC</cell><cell>58.01% (210/362)</cell></row><row><cell>Q. Classification</cell><cell>76.79% (278/362)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,86.83,69.50,452.42,637.43"><head>Table 3 :</head><label>3</label><figDesc>TALP-QA Passage Retrieval results.We designed two different measures to evaluate the Passage Retrieval for Factoid questions: the first one (called answer) is the accuracy taking into account the questions that have a correct answer in its set of passages. The second one (called answer+docID) is the accuracy taking into account the questions that have a minimum of one passage with a correct answer and a correct document identifier in its set of passages.</figDesc><table coords="8,320.57,136.69,218.69,129.33"><row><cell cols="2">• Answer Extraction. We evaluated the Candi-</cell></row><row><cell cols="2">dates Extraction (CE) module, the Answer Se-</cell></row><row><cell cols="2">lection (AS) module and finally we performed</cell></row><row><cell cols="2">an evaluation of the AE subsystem's global ac-</cell></row><row><cell cols="2">curacy for factoid questions in which the answer</cell></row><row><cell cols="2">appears in our selected passages.</cell></row><row><cell>Subsystem</cell><cell>Accuracy (answer)</cell></row><row><cell>Candidates Extraction</cell><cell>8.11% (28/345)</cell></row><row><cell>Answer Selection</cell><cell>71.42% (20/28)</cell></row><row><cell>Answer Extraction</cell><cell>5.79% (20/345)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,320.57,279.15,218.68,226.94"><head>Table 4 :</head><label>4</label><figDesc>TALP-QA Answer Extraction results.• Global Results. The overall results of our participation in the TREC 2005 Main QA Task are listed in Table5. The results of Document Ranking Evaluation Task are listed in Table6.</figDesc><table coords="8,322.45,374.21,204.96,131.88"><row><cell>Measure</cell><cell cols="3">run1 run2 run3</cell></row><row><cell>Factoid Total</cell><cell>362</cell><cell>362</cell><cell>362</cell></row><row><cell>Factoid Right</cell><cell>27</cell><cell>53</cell><cell>62</cell></row><row><cell>Factoid Wrong</cell><cell>330</cell><cell>288</cell><cell>279</cell></row><row><cell>Factoid IneXact/Uns.</cell><cell cols="3">4/1 17/4 17/4</cell></row><row><cell>Factoid Precision NIL</cell><cell cols="3">7/172 5/76 5/77</cell></row><row><cell>Factoid Recall NIL</cell><cell cols="3">7/17 5/17 5/17</cell></row><row><cell>Accuracy over Factoid</cell><cell cols="3">0.075 0.146 0.171</cell></row><row><cell>Average F-score List</cell><cell cols="3">0.024 0.026 0.028</cell></row><row><cell cols="4">Average F-score Other 0.172 0.164 0.079</cell></row><row><cell>Final score</cell><cell cols="3">0.088 0.125 0.116</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,319.02,519.21,211.82,142.52"><head>Table 5 :</head><label>5</label><figDesc>Results of TALP's runs at TREC 2005.</figDesc><table coords="8,331.21,565.73,187.44,96.01"><row><cell>Run</cell><cell>run1</cell><cell>run2</cell></row><row><cell>AvgP.</cell><cell>0.1191</cell><cell>0.1468</cell></row><row><cell>R-Prec.</cell><cell>0.1287</cell><cell>0.1685</cell></row><row><cell>Docs. Retrieved</cell><cell>781</cell><cell>1619</cell></row><row><cell>Recall (%)</cell><cell>11.68%</cell><cell>20%</cell></row><row><cell>Recall</cell><cell cols="2">184/1575 375/1575</cell></row><row><cell>∆ AvgP. Diff.(%)</cell><cell>-32.15%</cell><cell>-7.22%</cell></row><row><cell>over all runs AvgP.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,322.09,674.86,205.68,8.74"><head>Table 6 :</head><label>6</label><figDesc>TREC 2005 Document Ranking Task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,72.00,69.50,228.65,104.38"><head></head><label></label><figDesc>David Domínguez is granted by Catalan Government (2005FI 00437). Daniel Ferrés is supported by a UPC-Recerca grant from Universitat Politècnica de Catalunya (UPC). Mihai Surdeanu is a research fellow within the Ramón y Cajal program of the Spanish Ministry of Education and Science. Our research group, TALP Research Center, is recognized as a Quality Research Group (2001 SGR 00254) by DURSI.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,325.85,690.09,164.90,6.99;1,310.61,700.13,194.77,6.64"><p>Aranea is a QA system released under GPL. http://www.umiacs.umd.edu/~jimmylin/downloads/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,87.24,700.13,135.49,6.64"><p>http://jakarta.apache.org/lucene</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,325.85,700.13,118.55,6.64"><p>http://svmlight.joachims.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">European Commission</rs> (CHIL, <rs type="grantNumber">IST-2004-506909</rs>), the <rs type="funder">Spanish Research Department (ALIADO</rs>, <rs type="grantNumber">TIC2002-04447-C02</rs>), the <rs type="funder">Ministry of Universities, Research and Information Society (DURSI) of the Catalan Government</rs>, and the <rs type="funder">European Social Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_n2CtRCR">
					<idno type="grant-number">IST-2004-506909</idno>
				</org>
				<org type="funding" xml:id="_cFfc95B">
					<idno type="grant-number">TIC2002-04447-C02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,92.48,219.11,208.16,8.74;10,92.48,231.07,208.16,8.74;10,92.48,243.02,208.16,8.74;10,92.48,254.98,208.16,8.74;10,92.48,266.93,208.16,8.74;10,92.48,278.89,70.88,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,223.95,231.07,76.69,8.74;10,92.48,243.02,208.16,8.74;10,92.48,254.98,3.88,8.74">MITRE: Description of the ALEMBIC System Used for MUC-6</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aberdeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vilain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,131.17,254.98,169.47,8.74;10,92.48,266.93,86.43,8.74">Proceedings of the 6th Message Understanding Conference</title>
		<meeting>the 6th Message Understanding Conference<address><addrLine>Columbia, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="141" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,296.97,208.17,8.74;10,92.48,308.92,208.16,8.74;10,92.48,320.88,208.17,8.74;10,92.48,332.83,55.40,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,147.99,296.97,152.66,8.74;10,92.48,308.92,25.64,8.74">TnT -a statistical part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,142.28,308.92,158.36,8.74;10,92.48,320.88,105.52,8.74">Proceedings of the 6th Applied NLP Conference, ANLP-2000</title>
		<meeting>the 6th Applied NLP Conference, ANLP-2000<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,350.91,208.16,8.74;10,92.48,362.87,208.16,8.74;10,92.48,374.82,208.16,8.74;10,92.48,386.78,134.50,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,133.29,362.87,167.35,8.74;10,92.48,374.82,28.16,8.74">Named Entity Extraction using Ad-aBoost</title>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.43,374.82,120.37,8.74">Proceedings of CoNLL-2002</title>
		<meeting>CoNLL-2002<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,404.85,208.16,8.74;10,92.48,416.81,208.16,8.74;10,92.48,428.76,115.93,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,148.18,404.85,152.46,8.74;10,92.48,416.81,112.33,8.74">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,92.48,446.84,208.16,8.74;10,92.48,458.80,208.16,8.74;10,92.48,470.75,208.16,8.74;10,92.48,482.71,208.17,8.74;10,92.48,494.66,208.16,8.74;10,92.48,506.62,208.17,8.74;10,92.48,518.57,208.17,8.74;10,92.48,530.53,208.17,8.74;10,92.48,542.48,208.16,8.74;10,92.48,554.44,106.32,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,211.51,470.75,89.13,8.74;10,92.48,482.71,208.17,8.74;10,92.48,494.66,208.16,8.74;10,92.48,506.62,31.37,8.74">The TALP-QA System for Spanish at CLEF 2004: Structural and Hierarchical Relaxing of Semantic Constraints</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Ferrés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samir</forename><surname>Kanaan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alicia</forename><surname>Ageno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,106.05,542.48,160.28,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="557" to="568" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.48,572.52,208.17,8.74;10,92.48,584.47,208.16,8.74;10,92.48,596.43,208.17,8.74;10,92.48,608.38,208.16,8.74;10,92.48,620.34,208.16,8.74;10,92.48,632.29,208.17,8.74;10,92.48,644.25,22.70,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,207.74,596.43,92.91,8.74;10,92.48,608.38,208.16,8.74;10,92.48,620.34,139.13,8.74">TALP-QA System at TREC 2004: Structural and Hierarchical Relaxation Over Semantic Constraints</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Ferrés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samir</forename><surname>Kanaan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alicia</forename><surname>Ageno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,250.99,620.34,49.66,8.74;10,92.48,632.29,141.37,8.74">Proceedings of the Text Retrieval Conference</title>
		<meeting>the Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>TREC-2004</note>
</biblStruct>

<biblStruct coords="10,92.48,662.33,208.16,8.74;10,92.48,674.28,208.16,8.74;10,92.48,686.24,208.17,8.74;10,92.48,698.19,22.70,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,275.96,662.33,24.68,8.74;10,92.48,674.28,208.16,8.74;10,92.48,686.24,44.81,8.74">Using cohesive properties of text for automatic summarization</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,160.54,686.24,134.85,8.74">Processings of the JOTRI-2002</title>
		<meeting>essings of the JOTRI-2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.08,69.50,208.17,8.74;10,331.08,81.46,208.16,8.74;10,331.08,93.41,208.17,8.74;10,331.08,105.37,59.61,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,395.87,69.50,143.38,8.74;10,331.08,81.46,75.70,8.74">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,431.91,81.46,107.34,8.74;10,331.08,93.41,208.17,8.74;10,331.08,105.37,28.80,8.74">Proceedings of the ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.08,125.29,208.17,8.74;10,331.08,137.25,208.17,8.74;10,331.08,149.20,172.40,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,431.93,125.29,107.32,8.74;10,331.08,137.25,166.71,8.74">Learning question classifiers: The role of semantic information</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,506.77,137.25,32.49,8.74;10,331.08,149.20,94.11,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,169.13,59.30,8.74;10,414.00,169.13,125.24,8.74;10,331.08,181.08,218.03,8.74;10,331.08,193.04,22.70,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,414.00,169.13,120.80,8.74">Proximity-based thesaurus</title>
		<author>
			<persName coords=""><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.cs.ualberta.ca/˜lindek/downloads.htm" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,212.96,208.16,8.74;10,331.08,224.92,208.17,8.74;10,331.08,236.87,208.17,8.74;10,331.08,248.83,208.17,8.74;10,331.08,260.78,208.17,8.74;10,331.08,272.74,208.17,8.74;10,331.08,284.69,78.60,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,463.01,212.96,76.24,8.74;10,331.08,224.92,208.17,8.74;10,331.08,236.87,150.92,8.74">Question answering from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,511.69,236.87,27.57,8.74;10,331.08,248.83,208.17,8.74;10,331.08,260.78,208.17,8.74;10,331.08,272.74,19.53,8.74">CIKM &apos;03: Proceedings of the twelfth international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="116" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,304.62,208.16,8.74;10,331.08,316.57,208.17,8.74;10,331.08,328.53,208.17,8.74;10,331.08,340.48,157.28,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,371.31,316.57,162.37,8.74">QA UdG-UPC System at TREC-12</title>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Massot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Ferrés</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,343.81,328.53,195.44,8.74;10,331.08,340.48,57.87,8.74">Proceedings of the Text Retrieval Conference (TREC-2003)</title>
		<meeting>the Text Retrieval Conference (TREC-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,360.41,208.16,8.74;10,331.08,372.36,208.17,8.74;10,331.08,384.32,208.16,8.74;10,331.08,396.28,208.17,8.74;10,331.08,408.23,37.95,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,503.96,372.36,35.29,8.74;10,331.08,384.32,138.75,8.74">LASSO: A tool for surfing the answer net</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gîrju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,489.59,384.32,49.66,8.74;10,331.08,396.28,208.17,8.74;10,331.08,408.23,7.96,8.74">Proceedings of the Eighth Text Retrieval Conference (TREC-8)</title>
		<meeting>the Eighth Text Retrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,428.16,208.16,8.74;10,331.08,440.11,208.17,8.74;10,331.08,452.07,208.17,8.74;10,331.08,464.02,208.17,8.74;10,331.08,475.98,208.16,8.74;10,331.08,487.93,208.16,8.74;10,331.08,499.89,73.89,8.74" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,416.14,452.07,123.11,8.74;10,331.08,464.02,208.17,8.74;10,331.08,475.98,151.29,8.74">The Top-Down Strategy for Bulding EuroWordNet: Vocabulary Coverage, Base Conceps and Top Ontology</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Climent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloksma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alonge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bertanga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roventini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,519.81,208.16,8.74;10,331.08,531.77,208.17,8.74;10,331.08,543.72,208.17,8.74;10,331.08,555.68,71.46,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,331.08,531.77,208.17,8.74;10,331.08,543.72,39.85,8.74">Introduction to the CoNLL-2000 Shared Task: Chunking</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,396.40,543.72,142.85,8.74;10,331.08,555.68,40.40,8.74">Proceedings of CoNLL-2000 and LLL-2000</title>
		<meeting>CoNLL-2000 and LLL-2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.09,575.60,208.16,8.74;10,331.08,587.56,208.17,8.74;10,331.08,599.51,119.59,8.74" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,431.20,575.60,108.05,8.74;10,331.08,587.56,62.94,8.74">The nature of statistical learning theory</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
