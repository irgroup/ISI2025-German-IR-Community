<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.35,154.89,318.55,15.49">Overview of the TREC-2005 Enterprise Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,117.40,187.37,68.73,10.76"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<email>nickcr@microsoft.com</email>
						</author>
						<author>
							<persName coords="1,425.67,187.37,60.46,10.76"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">MSR Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Arjen P. de Vries CWI</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">NIST</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.35,154.89,318.55,15.49">Overview of the TREC-2005 Enterprise Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">142B766CE7CF33010378CDE163896C6D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the enterprise track is to conduct experiments with enterprise data -intranet pages, email archives, document repositories -that reflect the experiences of users in real organisations, such that for example, an email ranking technique that is effective here would be a good choice for deployment in a real multi-user email search application. This involves both understanding user needs in enterprise search and development of appropriate IR techniques.</p><p>The enterprise track began this year as the successor to the web track, and this is reflected in the tasks and measures. While the track takes much of its inspiration from the web track, the foci are on search at the enterprise scale, incorporating non-web data and discovering relationships between entities in the organisation.</p><p>Obviously, it's hard to imagine that any organisation would be willing to open its intranet to public distribution, even for research, so for the initial document collection we looked to an organisation that conducts most if not all of its day-to-day business on the public web: the World Wide Web Consortium (W3C). The collection is a crawl of the public W3C (*.w3.org) sites in June 2004. It is not a comprehensive crawl, but rather represents a significant proportion of the public W3C documents. It comprises 331,037 documents, retrieved via multithreaded breadthfirst crawling. Some details of the corpus are in Table <ref type="table" coords="1,287.75,609.69,3.74,8.97" target="#tab_0">1</ref>.</p><p>The majority of the documents in this collection are email, and thus the tasks this year focus on email. Note that the documents are not in native formats, but are rendered into HTML.</p><p>There are two tasks with a total of three experiments:</p><p>â€¢ Email search task: Using pages from lists.w3.</p><p>org.</p><p>-Known item experiment: 125 queries. The user is searching for a particular message, enters a query and will be satisfied if the message is retrieved at or near rank one. There were an additional 25 queries for use in training.</p><p>-Discussion search experiment: 59 queries. The user is searching to see how pros and cons of an argument/discussion were recorded in email. Their query describes the topic, and they care both whether the results are relevant and whether they contain a pro/con. There were no training queries, and indeed no judgements prior to submission. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Email search task</head><p>This task focuses on searching the 198,394 pages crawled from lists.w3.org. These are html-ised archives of mailing lists, so participants can treat it as a web/text search, or they can recover the email structure (threads, dates, authors, lists) and incorporate this information in the ranking. Some participants made their extracted information available to the group.</p><p>In the known item search experiment, participants developed (query, docno) pairs that represent a user who enters a query in order to find a specific message (item). Of the 150 pairs developed, 25 were provided for training and 125 were used for the evaluation reported here. Results are in Table <ref type="table" coords="2,122.11,402.52,3.74,8.97" target="#tab_1">2</ref>. The measures for this task were the mean reciprocal rank (MRR) of the correct answer, and the fraction of topics with the correct answer somewhere in the top 10 ("Success at 10" or S@10). Also reported is the fraction of topics that found the correct answer anywhere in the ranking (S@inf). In recent Web Track homepage finding experiments, it was possible to find the correct homepage with MRR &gt; 0.7 and S @10 0.9. Known item email search results are quite good for a first year, being about 0.1 lower on both metrics.</p><p>Nearly every group took a different approach at integrating the email text with email metadata and the larger thread structure. To give some examples, University of Glasgow (uog) combined priors for web-specific features -anchor text, titles of pages -with email-specific priors -threads and dates in messages and topics <ref type="bibr" coords="2,267.31,582.26,10.58,8.97" target="#b6">[7]</ref>. Microsoft Cambridge (MSRC) used their fielded BM25 with message fields, text, and thread features <ref type="bibr" coords="2,233.38,606.17,10.58,8.97" target="#b3">[4]</ref>. CMU (CMU) mixed language models for individual messages, message subjects, threads, and subthreads, and used thread-depth priors <ref type="bibr" coords="2,98.60,642.03,10.58,8.97" target="#b7">[8]</ref>. While the initial results are encouraging, it's clear that with this many types of data to balance, more work remains to be done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>MRR S@10 S@inf uogEDates2  In the discussion search experiment, participants developed topic descriptions and performed relevance judgements as described in Section 4. There are three types of answers: irrelevant, relevant without pro/con statement (also called "partially relevant") and relevant with pro/con statement. Table <ref type="table" coords="3,178.60,421.94,4.98,8.97" target="#tab_2">3</ref> shows discussion search results where any document that is not judged irrelevant is relevant (conflating the two positive judging levels). Interestingly, the top two runs are significantly better than the rest on our main measure mean average precision (MAP). For TITLETRANS, this is primarily due to the influence of a single topic <ref type="bibr" coords="3,140.32,493.67,10.58,8.97" target="#b5">[6]</ref>. The table also reports several other measures: R-precision (precision at rank R, where R is the number of relevant documents for that topic), bpref <ref type="bibr" coords="3,286.53,517.58,10.58,8.97" target="#b1">[2]</ref>, precision at ranks (5, 10, 20, 30, 100, 1000), and reciprocal rank of the first relevant document retrieved.</p><p>Table <ref type="table" coords="3,107.76,555.90,4.98,8.97" target="#tab_3">4</ref> shows similar results if we now conflate the lower two judging levels, giving a 'strict' evaluation that only counts documents that include a pro/con statement as relevant. The overall rankings of systems are nearly identical, with a Kendall's tau of 0.893. Figure <ref type="figure" coords="3,265.45,603.72,3.74,8.97" target="#fig_0">1</ref>, shows a scatter plot, with the two types of MAP being strongly correlated.</p><p>The common focus of most groups in the discussion search subtask was how to effectively exploit thread structure and quoted material. University of Maryland (TITLETRANS in table 3 and 4) explored expanding documents using threads and the trade-off between reinforcing quoted passages and removing them altogether, with mixed results <ref type="bibr" coords="3,368.68,163.83,10.58,8.97" target="#b5">[6]</ref>. University of Amsterdam (ToNsBs) applied a straightforward language model with a filter to eliminate non-email documents <ref type="bibr" coords="3,437.60,187.74,10.58,8.97" target="#b0">[1]</ref>. Microsoft Research's (MSRC) best-performing run used only textual fields from the messages and no static features (year of message, number of parents in the thread) <ref type="bibr" coords="3,450.22,223.60,10.58,8.97" target="#b3">[4]</ref>. So it seems that these results represent mostly topic-relevance retrieval effectiveness, and we have not yet found definitive solutions to discussion search.</p><p>An important point raised by the University of Maryland team is that some of the topics did not necessarily lend themselves to pro/con arguments on the subject. Additionally, while the relevance judgements do indicate whether a pro/con argument is present in the message, we did not collect whether the argument was for or against the subject. They also found that some topics were not only more amenable to pro/con discussions, but also exhibited greater agreement between assessors. For the 2006 track, we plan to focus more closely on the topic creation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Expert search task</head><p>In the expert search task, participants could use all 331,037 documents in order to rank a list of 1092 candidate experts. This could involve creating a document for each candidate and applying simple IR techniques, or could involve natural language processing and information extraction technologies targeted at different document types such as email. Results are presented in Table 5.</p><p>For this year's pilot of this task, the search topics were so-called "working groups" of the W3C, and the experts were members of these groups. These ground-truth lists were not part of the collection but were located after the crawl was performed. This enabled us to dry-run this task with minimal effort in creating relevance judgments.</p><p>Top-scoring runs used quite advanced techniques:</p><p>THUENT0505 This run makes use of all w3c web part information and Email lists (the list part) together with inlink anchor text of these files. Text content are Run MAP r-prec bpref P@5 P@10 p@20 p@30 P@100 P@1000 RR1 reconstructed and formed description files for each candidate person. Structure information inside web pages was also used to improve performance. Words from important pages are emphasised in this run. Bigram retrieval was also applied <ref type="bibr" coords="5,218.32,175.78,10.58,8.97" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSRA054</head><p>The basic model plus cluster-based reranking. (The basic model, 1) a two-stage model of combining relevance and co-occurrence 2) the co-occurrence model consists of body-body, titleauthor, and title-tree submodels 3) a back-off query term matching method which prefers exact match, then partial match, and finally word-level match.) <ref type="bibr" coords="5,289.02,267.41,11.62,8.97" target="#b2">[3]</ref> This suggests that there were gains in effectiveness to be had via leveraging the heterogeneity of the dataset and the 'information extraction' flavor of the task. On the other hand, some groups (including THU and others) did notice that the search topics were W3C working groups, and took advantage of this fact by mining working group membership knowledge out of the collection. Thus, these results should be considered preliminary pending a more realistic expert search data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Judging</head><p>Since each known item topic is developed with a particular message in mind, that message is by definition the only answer needed, so no further relevance judging is required. However, in a corpus with significant duplication, it may be necessary to examine the pool for duplicates or near-duplicates of the item, as in the Web and Terabyte tracks. This year, because we do not believe that duplication is such a problem in lists.w3.org, we decided to expend effort in duplicate identification, so each query has exactly one answer.</p><p>Similarly, there was no judging required for the expert search task. This is because we used working group membership as our ground truth, as described in Section 3.</p><p>For the discussion search task, the judging was more involved. Because it is an ad hoc search task, it needs true relevance judgments, but the technical nature of the collection meant that NIST assessors would not be ideal topic creators or relevance judges. Instead, track participants both created the topics and judged the pools to determine the final relevance judgments.</p><p>In response to a call for participation in April, thirteen groups submitted candidate topics for the discussion search and known item tasks. For the known item search task, the topics included the query/name for the page and the target docno. For discussion search, the topic included a "query" field (equivalent to the traditional "title" field) and a "narrative" field to delineate the relevance boundary of the topic. In all, 63 topics were submitted, and NIST selected 60 topics for the final set.</p><p>Judging was done over the internet using an assessment system at CWI. Each topic was assigned to two groups, the group who authored the topic (the primary assessor) and another group (the secondary assessor). Secondary assessment assignments were made so as to balance authors across judging groups and to somewhat limit overall judging load. The topics and judging groups are shown in table <ref type="table" coords="5,333.11,319.24,3.74,8.97" target="#tab_5">6</ref>. One group created three topics (24, 27, and 46) but did not submit any runs or respond to requests to help judge; their topics were reassigned to groups A, B, and C respectively as primary judges. Groups M and N did not contribute topics but did submit runs and agreed to help judge as secondary assessors. The pools were intentionally kept small to reduce the judging burden on sites. Three runs from each group were pooled to a depth of 50, and the final pools contained between 249 and 865 documents (mean 529).</p><p>Judging began in August and ran through early October, and was extremely successful, with all but three topics fully judged by their primary assessor, and 52 by the secondary assessor. The official qrels set consists of the primary judgments for 56 topics, and the secondary judgments for the remaining topics (26, 53, and 57). No relevant documents were found by the primary assessor for topic 4, and so we have left this topic out. This qrels set contains 31,258 judgments: 27,813 irrelevant, 1,441 relevant non-pro/con (R1) and 2,004 relevant pro/con (R2) messages. Median per topic was 14 for R1 and 20 for R2.</p><p>At the time of this writing, we have done some examination of the affects of assessor disagreement, by comparing the ranking of systems according to the primary and secondary judgments. For this experiment, we considered the 48 topics for which judgments exist from both assessors (and again dropping topic 4). Comparing the rankings of systems using each set of judgments yields a Kendall's tau of 0.763, which is less than the level of 0.9 taken to indicate "essentially identical", but still signifi-Run MAP r-prec bpref P@5 P@10 P@20 P@30 P@100 P@1000 RR1 ). We intend to look more closely at this data to see if particular topics or assessors cause more variation in the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This year participants made heavy use of email structure and combination of evidence techniques in email search and expert search with some success, but there remains much to learn. In future enterprise search experiments it would be nice to further our exploration of novel data types such as email archives, and of novel tasks such as expert search. This might include incorporation of a greater amount of real user data (perhaps query and click logs) to enhance our focus on enterprise user tasks.</p><p>For discussion search, we plan to approach topic creation with more care. Specifically, next year's topics will more closely target pro/con discussions, and we may ask assessors to label messages as either pro, con, both, or can't tell. This year's foray into community-developed topics and relevance judgments marked a significant change for TREC, although such is the practise in other forums such as INEX. It has been a very successful experience, and we intend to continue collection development this way next year.</p><p>Task details for this year are maintained on the track wiki, at http://www.ins.cwi.nl/projects/ trec-ent/wiki/index.php/Main Page.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,301.31,228.64,8.97;3,72.00,313.27,228.64,8.97;3,72.00,325.22,113.06,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MAP for the 57 discussion search runs, calculated by conflating the top two (MAP) or bottom two (Strict MAP) judging levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,310.61,494.36,228.64,174.45"><head>Table 1 :</head><label>1</label><figDesc>Details of the W3C corpus. Scope is the name of the subcollection and also the hostname where the pages were found, for example lists.w3.org. The exception is the subcollection 'other' which contains several small hosts.</figDesc><table coords="1,320.25,554.20,206.88,114.61"><row><cell></cell><cell></cell><cell>Size</cell><cell></cell><cell>avdocsize</cell></row><row><cell>Type</cell><cell cols="2">Scope (GB)</cell><cell>Docs</cell><cell>(KB)</cell></row><row><cell>Email</cell><cell>lists</cell><cell cols="2">1.855 198,394</cell><cell>9.8</cell></row><row><cell>Code</cell><cell>dev</cell><cell>2.578</cell><cell>62,509</cell><cell>43.2</cell></row><row><cell>Web</cell><cell>www</cell><cell>1.043</cell><cell>45,975</cell><cell>23.8</cell></row><row><cell cols="2">Wiki web esw</cell><cell>0.181</cell><cell>19,605</cell><cell>9.7</cell></row><row><cell>Misc</cell><cell>other</cell><cell>0.047</cell><cell>3,538</cell><cell>14.1</cell></row><row><cell>Web</cell><cell cols="2">people 0.003</cell><cell>1,016</cell><cell>3.6</cell></row><row><cell></cell><cell>all</cell><cell>5.7</cell><cell>331,037</cell><cell>18.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,310.61,273.53,228.64,271.92"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="2,341.51,273.53,166.84,212.20"><row><cell></cell><cell>0.621 0.784</cell><cell>0.920</cell></row><row><cell>MSRCKI5</cell><cell>0.613 0.816</cell><cell>0.952</cell></row><row><cell>covKIRun3</cell><cell>0.605 0.792</cell><cell>0.896</cell></row><row><cell>humEK05t3l</cell><cell>0.604 0.808</cell><cell>0.912</cell></row><row><cell>CMUnoPS</cell><cell>0.601 0.816</cell><cell>0.912</cell></row><row><cell>CMUnoprior</cell><cell>0.598 0.824</cell><cell>0.912</cell></row><row><cell>qdWcEst</cell><cell>0.579 0.792</cell><cell>0.920</cell></row><row><cell>priski4</cell><cell>0.551 0.728</cell><cell>0.896</cell></row><row><cell>KITRANS</cell><cell>0.536 0.728</cell><cell>0.880</cell></row><row><cell>WIMent01</cell><cell>0.533 0.784</cell><cell>0.912</cell></row><row><cell>csiroanuki5</cell><cell>0.522 0.776</cell><cell>0.888</cell></row><row><cell>UWATEntKI</cell><cell>0.519 0.712</cell><cell>0.888</cell></row><row><cell>csusm2</cell><cell>0.510 0.712</cell><cell>0.792</cell></row><row><cell>qmirkidtu</cell><cell>0.367 0.600</cell><cell>0.768</cell></row><row><cell>LPC5</cell><cell>0.343 0.480</cell><cell>0.504</cell></row><row><cell cols="2">PITTKIA1W8 0.335 0.496</cell><cell>0.808</cell></row><row><cell>LMplaintext</cell><cell>0.326 0.544</cell><cell>0.704</cell></row><row><cell>DrexelKI05b</cell><cell>0.195 0.376</cell><cell>0.624</cell></row></table><note coords="2,346.36,500.62,192.88,8.97;2,310.61,512.57,228.64,8.97;2,310.61,524.53,228.64,8.97;2,310.61,536.48,122.94,8.97"><p>Known item results, the run from each of the 17 groups with the best MRR, sorted by MRR. The best in each column is highlighted. (An extra line was added to show the run with best S@10.)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,167.23,480.03,436.36"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table coords="4,77.98,167.23,474.06,436.36"><row><cell>TITLETRANS</cell><cell cols="6">0.3782 0.4051 0.3781 0.5831 0.5000 0.4246 0.3712 0.2427</cell><cell cols="2">0.0469 0.7637</cell></row><row><cell>ToNsBs350F</cell><cell cols="6">0.3518 0.3769 0.3588 0.5729 0.5407 0.4449 0.3768 0.2147</cell><cell cols="2">0.0439 0.7880</cell></row><row><cell>UwatEntDSq</cell><cell cols="6">0.3187 0.3514 0.3266 0.5153 0.4831 0.4034 0.3610 0.2244</cell><cell cols="2">0.0415 0.6860</cell></row><row><cell>csiroanuds1</cell><cell cols="6">0.3148 0.3597 0.3310 0.5593 0.5102 0.4051 0.3469 0.2037</cell><cell cols="2">0.0416 0.7292</cell></row><row><cell>MSRCDS2</cell><cell cols="6">0.3139 0.3583 0.3315 0.5864 0.5169 0.4127 0.3475 0.1966</cell><cell cols="2">0.0428 0.7423</cell></row><row><cell>irmdLTF</cell><cell cols="6">0.3138 0.3461 0.3318 0.5254 0.4797 0.4169 0.3729 0.2183</cell><cell cols="2">0.0409 0.7249</cell></row><row><cell>prisds1</cell><cell cols="6">0.3077 0.3393 0.3294 0.5797 0.4966 0.3881 0.3277 0.1815</cell><cell cols="2">0.0381 0.6617</cell></row><row><cell>du05quotstrg</cell><cell cols="6">0.2978 0.3431 0.3163 0.5288 0.4712 0.3881 0.3362 0.2047</cell><cell cols="2">0.0417 0.6793</cell></row><row><cell>qmirdju</cell><cell cols="6">0.2860 0.3202 0.3017 0.5119 0.4695 0.3788 0.3226 0.1976</cell><cell cols="2">0.0421 0.7026</cell></row><row><cell>LMlam08Thr</cell><cell cols="6">0.2721 0.3062 0.2884 0.3932 0.3746 0.3263 0.2887 0.1819</cell><cell cols="2">0.0412 0.5678</cell></row><row><cell cols="7">PITTDTA2SML1 0.2184 0.2494 0.2333 0.3864 0.3271 0.2712 0.2288 0.1339</cell><cell cols="2">0.0290 0.4759</cell></row><row><cell>MU05ENd5</cell><cell cols="6">0.2182 0.2655 0.2530 0.4407 0.3831 0.3136 0.2893 0.1819</cell><cell cols="2">0.0381 0.6121</cell></row><row><cell>NON</cell><cell cols="6">0.0843 0.1305 0.1082 0.2576 0.2237 0.1771 0.1508 0.0869</cell><cell cols="2">0.0087 0.4123</cell></row><row><cell>LPC1</cell><cell cols="6">0.0808 0.0981 0.0907 0.2237 0.1746 0.1305 0.1062 0.0544</cell><cell cols="2">0.0072 0.3670</cell></row><row><cell>Run</cell><cell>MAP</cell><cell>r-prec</cell><cell>bpref</cell><cell>P@5 P@10</cell><cell>p@20</cell><cell cols="2">p@30 P@100 P@1000</cell><cell>RR1</cell></row><row><cell>TITLETRANS</cell><cell cols="6">0.2958 0.3064 0.3381 0.3661 0.3356 0.2797 0.2429 0.1531</cell><cell cols="2">0.0279 0.5710</cell></row><row><cell>ToNsBs350F</cell><cell cols="6">0.2936 0.3065 0.3286 0.4068 0.3763 0.2907 0.2407 0.1292</cell><cell cols="2">0.0256 0.6247</cell></row><row><cell>MSRCDS2</cell><cell cols="6">0.2742 0.2892 0.3043 0.4339 0.3661 0.2864 0.2282 0.1200</cell><cell cols="2">0.0253 0.6376</cell></row><row><cell>UwatEntDSq</cell><cell cols="6">0.2735 0.2990 0.3086 0.3593 0.3220 0.2669 0.2373 0.1388</cell><cell cols="2">0.0250 0.5612</cell></row><row><cell>prisds1</cell><cell cols="6">0.2626 0.2803 0.2977 0.4000 0.3407 0.2695 0.2232 0.1136</cell><cell cols="2">0.0237 0.5234</cell></row><row><cell>du05quotstrg</cell><cell cols="6">0.2600 0.2837 0.2883 0.3864 0.3356 0.2576 0.2226 0.1246</cell><cell cols="2">0.0246 0.5436</cell></row><row><cell>irmdLTF</cell><cell cols="6">0.2592 0.2712 0.2852 0.3966 0.3407 0.2881 0.2514 0.1464</cell><cell cols="2">0.0247 0.5890</cell></row><row><cell>csiroanuds1</cell><cell cols="6">0.2583 0.2854 0.3000 0.3864 0.3492 0.2712 0.2243 0.1253</cell><cell cols="2">0.0253 0.5791</cell></row><row><cell>qmirdju</cell><cell cols="6">0.2446 0.2750 0.2841 0.3492 0.3153 0.2568 0.2085 0.1236</cell><cell cols="2">0.0248 0.5673</cell></row><row><cell>LMlam08Thr</cell><cell cols="6">0.2153 0.2442 0.2409 0.2576 0.2390 0.2068 0.1836 0.1149</cell><cell cols="2">0.0254 0.4369</cell></row><row><cell cols="7">PITTDTA2SML1 0.1978 0.2072 0.2165 0.2949 0.2508 0.1907 0.1565 0.0868</cell><cell cols="2">0.0176 0.4110</cell></row><row><cell>MU05ENd5</cell><cell cols="6">0.1847 0.2262 0.2309 0.3322 0.2627 0.2136 0.1989 0.1214</cell><cell cols="2">0.0230 0.5518</cell></row><row><cell>NON</cell><cell cols="6">0.0842 0.1285 0.1099 0.1864 0.1678 0.1280 0.1040 0.0568</cell><cell cols="2">0.0057 0.3061</cell></row><row><cell>LPC1</cell><cell cols="6">0.0724 0.0872 0.0811 0.1661 0.1220 0.0873 0.0723 0.0369</cell><cell cols="2">0.0050 0.3012</cell></row></table><note coords="4,107.15,346.50,432.10,8.97;4,72.00,358.46,312.41,8.97"><p>Discussion search: Evaluation where judging levels 1 and 2 are 'relevant'. Lists the run with best MAP from each of the 14 groups, sorted by MAP. The best in each column is highlighted.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,72.00,618.48,467.24,32.88"><head>Table 4 :</head><label>4</label><figDesc>Discussion search: Strict evaluation, where only judging level (includes a pro/con statement) is considered relevant. Lists the run with best MAP from each of the 14 groups, sorted by MAP. The best in each column is highlighted.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,72.00,182.17,468.77,418.43"><head>Table 5 :</head><label>5</label><figDesc>Expert search results, the run from each of the 9 groups with the best MAP, sorted by MAP. The best in each column is highlighted. (An extra line was added to show the run with best P@100.)</figDesc><table coords="6,77.98,182.17,462.80,418.43"><row><cell></cell><cell cols="5">0.2749 0.3330 0.4880 0.4880 0.4520 0.3390 0.2800 0.1142</cell><cell>0.0114 0.7268</cell></row><row><cell>MSRA054</cell><cell cols="5">0.2688 0.3192 0.5685 0.4080 0.3700 0.3190 0.2753 0.1306</cell><cell>0.0131 0.6244</cell></row><row><cell>MSRA055</cell><cell cols="5">0.2600 0.3089 0.5655 0.3920 0.3580 0.3150 0.2733 0.1308</cell><cell>0.0131 0.5832</cell></row><row><cell>CNDS04LC</cell><cell cols="5">0.2174 0.2631 0.4299 0.4120 0.3460 0.2820 0.2240 0.0942</cell><cell>0.0094 0.6068</cell></row><row><cell cols="6">uogES05CbiH 0.1851 0.2397 0.4662 0.3800 0.3160 0.2600 0.2133 0.1130</cell><cell>0.0113 0.5519</cell></row><row><cell>PRISEX3</cell><cell cols="5">0.1833 0.2269 0.4182 0.3440 0.3080 0.2530 0.2087 0.1026</cell><cell>0.0103 0.5614</cell></row><row><cell>uams05run1</cell><cell cols="5">0.1277 0.1811 0.3925 0.2720 0.2220 0.2000 0.1753 0.0944</cell><cell>0.0094 0.4380</cell></row><row><cell>DREXEXP1</cell><cell cols="5">0.1262 0.1743 0.3409 0.3120 0.2500 0.1760 0.1467 0.0720</cell><cell>0.0072 0.4635</cell></row><row><cell>LLEXemails</cell><cell cols="5">0.0960 0.1357 0.2985 0.2000 0.1860 0.1530 0.1213 0.0628</cell><cell>0.0063 0.4054</cell></row><row><cell>qmirex4</cell><cell cols="5">0.0959 0.1511 0.2730 0.2360 0.1880 0.1390 0.1233 0.0534</cell><cell>0.0053 0.4189</cell></row><row><cell cols="2">Group</cell><cell></cell><cell>Authored topics</cell><cell>Assigned topics</cell><cell>Total</cell></row><row><cell cols="2">A</cell><cell>7</cell><cell>8 33 41 52 24</cell><cell>12 25 48 60</cell><cell>10</cell></row><row><cell cols="2">B</cell><cell cols="2">4 37 43 51 60 27</cell><cell>13 26 49</cell><cell>9</cell></row><row><cell cols="2">C</cell><cell cols="2">6 11 20 34 48 46</cell><cell>14 37 50</cell><cell>9</cell></row><row><cell cols="2">D</cell><cell cols="2">9 19 58</cell><cell>1 15 27 38 51</cell><cell>8</cell></row><row><cell cols="2">E</cell><cell cols="2">3 15 23 31 35</cell><cell>2 16 28 39 52</cell><cell>10</cell></row><row><cell cols="2">F</cell><cell cols="2">5 10 14 16 36</cell><cell>3 17 29 40 53</cell><cell>10</cell></row><row><cell cols="2">G</cell><cell>1</cell><cell>2 25 26 53</cell><cell>4 18 41 54</cell><cell>9</cell></row><row><cell cols="2">H</cell><cell cols="2">39 40 50 56</cell><cell>5 19 30 42 55</cell><cell>9</cell></row><row><cell>I</cell><cell></cell><cell cols="2">18 30 45</cell><cell>6 31 36 43 56</cell><cell>8</cell></row><row><cell>J</cell><cell></cell><cell cols="2">12 32 47 55 57</cell><cell>7 20 44 46</cell><cell>9</cell></row><row><cell cols="2">K</cell><cell cols="2">22 29 38 42 49</cell><cell>8 21 32 45</cell><cell>9</cell></row><row><cell cols="2">L</cell><cell cols="2">13 17 21 28 44 54 59</cell><cell>9 22 33 57</cell><cell>11</cell></row><row><cell cols="2">M</cell><cell></cell><cell></cell><cell>10 23 34 47 58</cell><cell>5</cell></row><row><cell cols="2">N</cell><cell></cell><cell></cell><cell>11 24 35 59</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,72.00,615.49,467.24,20.92"><head>Table 6 :</head><label>6</label><figDesc>Topic assignments for relevance assessment. "Authored topics" were created by that group. "Assigned topics" were assigned to that group by NIST for judging. cantly correlated (p &lt; 2.2 â€¢ 10 16</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We are grateful to the <rs type="institution">World Wide Web Consortium</rs> for allowing us to make a snapshot of their site available as a research tool. We also thank the <rs type="institution">University of Glasgow</rs> for hosting a Terrier-based search interface to the W3C collection for topic development. Lastly, we thank the participants of the 2005 enterprise track for helping to create the test collection.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,88.60,653.99,212.04,8.97;7,88.60,665.94,212.04,8.97;7,327.20,127.96,147.83,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,111.77,665.94,188.87,8.97;7,327.20,127.96,18.68,8.97">Language modeling approaches for enterprise tasks</title>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,364.00,127.96,94.44,8.97">Voorhees and Buckland</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.20,147.89,212.04,8.97;7,327.20,159.84,212.05,8.97;7,327.20,171.80,212.04,8.97;7,327.20,183.75,212.04,8.97;7,327.20,195.71,212.05,8.97;7,327.20,207.66,92.32,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,481.65,147.89,57.60,8.97;7,327.20,159.84,141.61,8.97">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,489.89,159.84,49.36,8.97;7,327.20,171.80,212.04,8.97;7,327.20,183.75,212.04,8.97;7,327.20,195.71,91.49,8.97">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004)</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004)<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.20,227.59,212.04,8.97;7,327.20,239.54,212.04,8.97;7,327.20,251.50,175.79,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,344.75,239.54,194.50,8.97;7,327.20,251.50,45.89,8.97">Research on expert search at enterprise track of TREC 2005</title>
		<author>
			<persName coords=""><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shenghua</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,391.95,251.50,94.44,8.97">Voorhees and Buckland</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.20,271.42,212.04,8.97;7,327.20,283.38,212.04,8.97;7,327.20,295.33,147.82,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,350.38,283.38,188.87,8.97;7,327.20,295.33,18.67,8.97">Microsoft Cambridge at TREC-14: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,363.99,295.33,94.44,8.97">Voorhees and Buckland</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.20,315.26,212.04,8.97;7,327.20,327.21,212.04,8.97;7,327.20,339.17,147.82,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,402.69,327.21,136.55,8.97;7,327.20,339.17,18.67,8.97">THUIR at TREC 2005: Enterprise track</title>
		<author>
			<persName coords=""><forename type="first">Yupeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yize</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,363.99,339.17,94.44,8.97">Voorhees and Buckland</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.20,359.09,212.04,8.97;7,327.20,371.05,212.04,8.97;7,327.20,383.00,212.04,8.97;7,327.20,394.96,212.04,8.97;7,327.20,406.91,33.75,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,532.05,371.05,7.19,8.97;7,327.20,383.00,212.04,8.97;7,327.20,394.96,73.78,8.97">A menagerie of tracks at maryland: HARD, enteprise, QA, and genomics</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eileen</forename><surname>Abels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejun</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>oh my! In Voorhees and Buckland [9</note>
</biblStruct>

<biblStruct coords="7,327.20,426.84,212.04,8.97;7,327.20,438.79,212.04,8.97;7,327.20,450.75,212.04,8.97;7,327.20,462.70,151.70,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,441.08,426.84,98.17,8.97;7,327.20,438.79,212.04,8.97;7,327.20,450.75,212.04,8.97;7,327.20,462.70,23.00,8.97">University of Glasgow at TREC 2005: Experiments in terabyte and enterprise tracks with terrier</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,367.86,462.70,94.44,8.97">Voorhees and Buckland</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>Vassilis Plachouras, and Iadh Ounis</note>
</biblStruct>

<biblStruct coords="7,327.20,482.63,212.04,8.97;7,327.20,494.59,212.04,8.97;7,327.20,506.54,149.48,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,451.59,482.63,87.65,8.97;7,327.20,494.59,212.04,8.97;7,327.20,506.54,20.06,8.97">Experiments with language models for known-item finding of e-mail messages</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,365.65,506.54,94.44,8.97">Voorhees and Buckland</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,327.20,526.47,212.05,8.97;7,327.20,538.42,212.04,8.97;7,327.20,550.38,202.69,8.97" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</author>
		<title level="m" coord="7,521.44,526.47,17.81,8.97;7,327.20,538.42,212.04,8.97;7,327.20,550.38,51.61,8.97">Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text REtrieval Conference (TREC 2005)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
