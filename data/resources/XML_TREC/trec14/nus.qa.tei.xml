<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,85.26,72.23,424.78,12.58">Using Syntactic and Semantic Relation Analysis in Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.78,99.48,51.66,9.88"><forename type="first">Renxu</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName coords="1,146.54,99.48,48.46,9.88"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName coords="1,217.03,99.48,60.52,9.88"><forename type="first">Yee</forename><forename type="middle">Fan</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName coords="1,299.58,99.48,45.54,9.88"><forename type="first">Hang</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName coords="1,364.34,99.48,71.22,9.88"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
						</author>
						<author>
							<persName coords="1,457.60,99.48,65.02,9.88"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
						</author>
						<title level="a" type="main" coord="1,85.26,72.23,424.78,12.58">Using Syntactic and Semantic Relation Analysis in Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62D316E608EA18C0D5FAA43E9D77ACB4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our participation at TREC this year focuses on integrating dependency and semantic relation analysis of external resources into our existing QA system. In TREC-13, we have proposed the use of dependency relation matching to perform answer extraction for factoid and list questions. The results showed that the technique is effective in answer extraction within the corpus. However, we have also identified some problems and limitations with this technique. First, dependency relation matching does not perform well on short questions, which have only very few key terms. Therefore we need to integrate query expansion and make use of external resources to provide additional contextual information to these short questions. Second, the technique cannot be directly applied to extract answer nuggets from external web pages. As web pages contain much more noise as compared to the corpus, statistical based dependency relation matching tends to make a lot of errors based on our previously trained model on the corpus. Moreover, we do not have sufficient training data to retrain a model on the web. Therefore we propose to use semantic relation analysis to supplement dependency relation analysis to extract answer nuggets for factoid and list questions on the web. Finally, we adopt a soft pattern matching model <ref type="bibr" coords="1,56.70,550.44,76.40,9.88" target="#b0">(Cui et al., 2005)</ref> for definition sentence retrieval in the definitional QA task.</p><p>We focus on the following three features in this year's TREC:</p><p>(1) We incorporate dependency relation analysis into query expansion based on external web resources, and we perform query expansion for both document and passage retrieval to provide more contextual information for short questions.</p><p>(2) We propose the use of semantic relation</p><p>analysis to extract answer nuggets for factoid and list questions from external web resources. These external answer nuggets can either be directly projected back to the corpus (for list questions) or be used as additional supporting evidence for corpus-based answer extraction (for factoid questions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Department of Computer Science</head><p>School of Computing National University of Singapore {sunrenxu,jingjian,tanyeefa,cuihang,chuats,kanmy}@comp.nus.edu.sg We employ the bigram model to identify good definition sentence candidates. This paper is organized as follows: In the next section, we present the overall architecture of our system. In Sections 3, 4 and 5, we respectively give the details of the above three features. In Section 6, we conclude the paper with discussion of future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>In Figure <ref type="figure" coords="1,364.53,373.61,4.12,9.57">1</ref>, we illustrate the architecture of our QA system. We have leveraged our prior work in question analysis, document retrieval, passage retrieval to build the system. Our major modification lies in query expansion, semantic answer nugget extraction from web resources and the bigram model for definition sentence selection. In our comprehensive pre-processing step, we store a named entity profile and the full parsing of each article in the TREC corpus. The offline processing greatly accelerates answer extraction.</p><p>Our framework functions as follows: Target analysis and document retrieval: First, the user submits a topic, e.g., "Aaron Copland", to the system. Lucene 1 is used to index the documents. In handling topics with qualifiers, for instance, "skier Alberto Tomba", we rely on the Web to separate the qualifiers from the main topic words, e.g., "Alberto Tomba" in the above example. Specifically, we calculate the pointwise mutual information (PMI) 2 between each pair of topic terms based on the hits returned by Google when using the topic terms as query. Terms with PMI values beyond a predefined threshold are grouped together. To construct a suitable Lucene query, we first use logical "AND" to connect terms in the same group, and employ logical "OR" to connect different groups. To handle errors or infrequent expressions 1 http://jakarta.apache.org/lucene/docs/index.html</p><formula xml:id="formula_0" coords="1,318.18,756.57,76.33,25.59">2 ) ( ) , ( X P Y X P PMI = Figure 1</formula><p>. The illustration of the TREC QA system architecture in the given topics, we replace our original query by any query suggestion from Google<ref type="foot" coords="2,217.14,461.17,3.51,6.12" target="#foot_0">3</ref> . For instance, our system automatically changes "Harlem Globe Trotters" to "Harlem GlobeTrotters" according to Google's result. From the document retrieval on the NE pre-tagged corpus, we get a set of NE tagged relevant documents related to the given topic.</p><p>• Factoid/List Question Analysis: We first extract the expected answer NE type for each question. We then parse each question using Minipar <ref type="bibr" coords="2,95.14,588.81,50.31,9.62" target="#b1">(Lin, 1998)</ref> and store the dependency parse tree, which will be used for dependency based answer extraction. Finally, we parse the question using shallow semantic parser ASSERT <ref type="bibr" coords="2,236.45,626.49,50.59,9.93;2,56.70,639.15,43.72,9.93" target="#b1">(Pradhan et al., 2004)</ref> and also store the parse tree, which will be used for semantic answer nugget extraction from web resources. As some of the questions cannot be parsed by ASSERT (i.e empty output), we only perform semantic parsing on the subset of questions which have non-empty output. For the rest of the questions, we only perform dependency based answer extraction.</p><p>• Query expansion and passage retrieval for factoid and list questions: We incorporate dependency relation analysis into query expansion, which will be introduced in Section 3. The method picks expansion terms from Google snippets according to the terms' relation with the question terms in the snippets. For document ranking task this year, we select the top k expanded terms together with the non-trivial question terms to form the query. Our passage retrieval module also takes in expanded queries as input, and performs densitybased lexical matching to rank passages, which consist of a window of three sentences.</p><p>• Answer extraction: We perform answer extraction on corpus documents as well as answer nuggets extraction on external web documents and select the final answer by answer projection (for list questions) and verification (for factoid questions). We use dependency based answer extraction to extract the answer string from the corpus. However, our preliminary experiment shows that this approach does not work well on web pages as they contain much more noisy data as compared to corpus. Therefore we propose the use of semantic based answer nugget extraction which is less sensitive to noise and we give the technical details in Section 4.</p><p>• Definition generation: The relevant document set for the given topic is the basis for generating the definition for that topic. The definition generation module first extracts definition sentences from the document set. It identifies definition sentences using centroid-based weighting and then applies the soft-pattern model for matching these definition sentences. It also leverages existing definitions from external resources. We will discuss definition sentence extraction in Section 5. After redundancy removal, the module produces the definition for the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Expansion using Dependency Relations</head><p>In TREC-13, we have proposed to use dependency relation matching to perform answer selection. However, our experiments showed that dependency relation matching does not perform well on short questions with very few (less than four) key words. Therefore we need to introduce additional contextual information for these short questions through query expansion. However, most query expansion methods only introduce new terms and cannot be directly applied to relation matching. Thus we propose a query expansion algorithm, which can expand new terms as well as relation paths based dependency relation analysis.</p><p>To perform query expansion, we first send the queries to Google and use the top 50 returned snippets as a basis for query expansion. We parse the snippets using MiniPar and rank each non-stop token in the parsing tree of the snippet by its relation path to the tokens in the parsing tree of the question using the trained scores of individual relation. Finally, we select the top k relation paths in the snippets to combine with relation paths derived from the original questions to perform answer selection.</p><p>We will next introduce our query expansion algorithm follow by details on how we train individual relation scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Expansion</head><p>Most query expansion techniques rank expansion terms using their co-occurrence with the query term by performing local context analysis <ref type="bibr" coords="3,252.73,684.57,34.28,9.93;3,56.70,697.23,38.24,9.93" target="#b1">(Xu et al.,1996)</ref>. However, we observed that due to the noise on the web, the same technique cannot be applied to the Web. This is because some irrelevant terms, such as commercial related terms co-occur very frequently with the query terms in some snippets. This will mislead the query expansion algorithm to select them as relevant terms. Therefore we propose to use dependency relation between query terms and expanded terms as additional evidence to infer the relevance of the expanded term. Our general framework is similar to the local context analysis method, but with two major differences: (1) We perform query expansion using web resources rather than the top N passages retrieved within the corpus. (2) We score the expanded terms using their relation paths to query terms rather than statistical co-occurrence.</p><p>Below are the steps we use to incorporate dependency relation analysis to expand a query Q based on web resource D w .</p><p>1) We input each question as a query to Google and collect the top 50 returned snippets as a basis for query expansion. We combine the 50 snippets as a whole document denoted as D w and perform sentence splitting and dependency parsing using Minipar. Each sentence S i in D w becomes a dependency tree T i after parsing. A dependency tree depicts the dependency relations between tokens of a sentence. For any two tokens (a token may either be a single word, a noun phrase or a verb phrase) in a sentence, there exists a path between them. The path consists of a series of intermediate nodes linked by labeled edges called relations. So we can define relation path in the form of (Start_Token, Rel 1 … Rel k …Rel m , End_Token). 2) After step 1, we have N dependency parsing trees that corresponds to N sentences in D w . The non-stop word tokens denoted as Tk in D w are ranked according to the formula: Besides the tokens, we also rank the path associated with each token Tk and select the top ranked path with start token Tk to the expanded path of Tk. The selection formula is shown as </p><formula xml:id="formula_1" coords="3,326.28,476.11,238.56,108.11">∏ ∑ ∈ = × + = Q t idf Tk n s i i i t N idf s t Tk score path Q Tk Score ) log ) ) , , ( _ ( log ( ) , ( 10 1 10 δ (3.1.1) Where ) (Re ) , , ( _ ) , ( Re ∏ ∈ ∧ ∈ ∧ ∈ = t Tk path l s t s Tk i i l score s t Tk score path (3.1.2) N N N idf Tk Tk 10 10 log / ) / ( log = (3.1.3) N N N idf i i t t</formula><formula xml:id="formula_2" coords="4,103.92,131.79,172.07,48.13">D s Q t ∈ ∈ = = (3.1.5)</formula><p>(3) We add top k tokens denoted as to the original query. We set the weight of original query terms to be 1.0 and the weight of ith expanded token to be (1-0.9*i/k). We use the expanded set to perform document and passage retrieval. We add</p><formula xml:id="formula_3" coords="4,163.44,196.50,122.50,81.69">} ... { 1 k Tk Tk } 1 | ) ( _ { k i Tk ex path i ≤ ≤</formula><p>to the set of paths derived from the original question to perform dependency matching for answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Individual Relation Weights</head><p>As explained in the previous section, the relevance of the expanded token Tk is judged by its relation paths linking to the tokens in the query. And each relation path is a sequence of individual relations. Under the assumption that each relation appears independent of the other relations in the same path we have We use TREC 8 and TREC 9, QA sentence pairs to perform training. We denote each QA pair as (Q ) (Re i l score i ,A i ). We then collect the top 50 snippets returned by Google for each question and perform sentence splitting and dependency parsing and select the relevant paths from the set of parsing trees of the snippets. A path p in the snippets of Qi denoted as (Start_Token,</p><formula xml:id="formula_4" coords="4,168.06,428.89,55.21,31.04">∏ ∈ ∧ ∈ ∧ ∈ = ) ,<label>(</label></formula><formula xml:id="formula_5" coords="4,56.70,610.05,232.35,99.76">Rel 1… Rel k… Rel m, End_Token) is relevant if i A Token Start ∈ _ and . i Q Token End ∈ _ )} 1 {log( max / ) 1 log( ) (Re _ Re 1 _ Re + + = ∈ ≤ ≤ ∈ path relevant l N i path relevant l i i i C C l score (3.2.2) Where path relevant l i C _ Re ∈</formula><p>is the number of in relevant paths i l Re N is the total number of relation types According to the formula, the score of the relation is proportional to the probability that it is in a relevant path. In other words the more often a "good" expansion term is inferred by the relation, the higher the score it will get. We normalize the score to be between 0 and 1 by dividing the score by the maximum score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Results and Discussions</head><p>We perform document and passage retrieval using the query expansion technique described above. Figure <ref type="figure" coords="4,372.70,147.51,5.51,9.62">2</ref> shows the precision-recall graph of the document ranking task. To facilitate the answering of topic-related factoid and list questions we use web resources as supporting documents to perform answer nugget extraction. We use http://www.answers.com/ as a search engine to get topic related web documents. And we perform semantic based answer nugget extraction on these documents. Finally, we project the answer nuggets back to corpus and verify the correctness of the answer nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Collecting and Pre-processing of Web Pages</head><p>For each topic, we input the topic as the query string to http://www.answers.com. The engine will return a resource page if the topic is in its database. Otherwise it will return the search result by Google. We use the resource page as our external web resource. We perform web page segmentation and classification on the resource page. For each resource page we classify the page segments into three classes, namely lists, descriptive data segments in natural language and commercial advertisements. We will remove all commercial segments as we will only use non-commercial segments to perform answer nugget extraction.</p><p>Since the semantic answer selection works only on natural language sentences, we have to assign the surrounding texts as descriptive titles for tables and lists and use these titles to perform semantic matching with the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Answer Extraction</head><p>Our goal is to match the semantic structure contained in the question with sentences from the web resources containing answer nuggets. After ranking the sentences we use the top k matched NEs (if the question has an NE typed answer target) or arguments (if the question has no NE typed answer target) as our candidate answer nuggets. There are two major challenges for this semantic matching. One is the different syntactic representations of the same semantic structure. The other is the use of different lexical terms to refer to the same concept. To tackle these problems, we use a shallow semantic parser to unify different syntactic representations into the same semantic representation, and we use WordNet and eXtended WordNet to find synonyms and semantically related verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Shallow Semantic Parsing</head><p>To capture the semantic structures contained in a sentence, we need to identify verbs and their arguments. We also need to label the arguments with their semantic roles. This goal is achieved by performing shallow semantic parsing. The shallow semantic parser we use is the ASSERT parser, which is trained on the PropBank <ref type="bibr" coords="5,208.68,390.51,78.31,9.93;5,56.70,403.47,25.69,9.62" target="#b1">(Kingsbury et al., 2002)</ref> corpus and uses support vector machine classifiers.</p><p>PropBank was manually annotated with verbargument structures. Following the annotation rules in PropBank, the ASSERT parser tags the arguments of a verb with labels from ARG0 up to ARG5. Although such semantic roles are verbspecific, some labels such as ARG0 and ARG1 tend to be general to all verb classes. For example, for any transitive verb, ARG0 is always the subject, and ARG1 is always the direct object. Besides these core arguments, ASSERT also tags adjunctive arguments. Examples are ARGM-LOC for locatives and ARGM-TMP for temporal. Figure <ref type="figure" coords="5,239.37,567.93,5.51,9.62" target="#fig_0">3</ref> shows a sample output of ASSERT containing the parsing result of the question and several answer candidate sentences.</p><p>To represent sentences in terms of the semantic structures, we define a semantic frame (or frame for short) as a verb-argument structure obtained from a sentence by the ASSERT parser. A frame consists of a verb, which we call the predicate, and a set of arguments. The arguments include both core arguments and adjunctive arguments. Each argument is associated with a label such as ARG0 and ARG1 to indicate the semantic role of the argument. Therefore, a frame F can be represented as F=(v,A), where v is the predicate and A is the set of arguments. Each element a in A is a pair consisting the argument label and the argument text, represented by a=(l, T), where l is the label and T is the set of terms that the argument contains. Because a sentence may contain more than one semantic structure, a sentence is represented by a set of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Verb Expansion using WordNet</head><p>Before we show our similarity scoring function, we first look at verb similarity scores. An answer passage can express the same semantic structure of the question with a different verb that is either of the same meaning as the verb in the question or semantically related to the verb in the question. Therefore, when matching semantic frames, we need to consider the semantic similarity between two verbs. We use WordNet and eXtended WordNet to measure this verb similarity.</p><p>Our verb similarity function is very similar to the weighting function in <ref type="bibr" coords="5,422.06,422.49,116.55,9.93">Moldovan et al., 2002.</ref> Suppose we want to measure the similarity between two verbs v1 and v2. We start from one of the verbs, say, v1. This original verb is assigned a score of 1. We select the synset that corresponds to the first sense of v1 in WordNet. All words in this synset get the same score as the original word. From this synset, we follow the links to other synsets with relations such as hyponyms and entailment. We also follow the gloss links and reverse gloss links provided by the eXtended WordNet. A gloss link from synset S1 to synset S2 means S2 appears in the gloss of S2, and a reverse gloss link from synset S1 to synset S2 means S1 appears in the gloss of S2.</p><p>If from v1 we follow the relation R to a synset which contains the word w, then the score for the word w is</p><formula xml:id="formula_6" coords="5,363.12,622.17,28.36,16.16">R W × 1</formula><p>, where is the weight for relation R. If we follow the link further from w to another synset which contains the word u by relation S, then the score of the word u is</p><formula xml:id="formula_7" coords="5,316.08,625.95,134.50,67.56">R W s R W W × × 1</formula><p>. Such expansion continues until we reach a certain depth. In our experiments, we set the depth to 2 because our preliminary experiments show that a deeper expansion does not improve the performance. The weights of the relations are the same as that used in <ref type="bibr" coords="5,399.44,748.17,100.49,9.93">Moldovan et al., 2002.</ref> We also penalize synsets that are more commonly used. Each synset gets a generality score G which is defined as where C is a constant, and is the number of glosses in which the member of synset S appears. We set C=500. After taking into account this penalizing weight, the score of the word w is therefore , where is the score of the previous word v, is the relation connecting v and w, and is the synset containing w.</p><formula xml:id="formula_8" coords="6,101.16,95.24,117.84,108.13">gloss r N - w w v v Gs R s × × , v s w v R , w s</formula><p>After we expand the verb v1, we check if v2 appears in the expanded set. If it does, then it is assigned the score as explained above. If not, v2 is assigned a score of 0. We denote this similarity score between v1 and v2 as .</p><p>) , (</p><formula xml:id="formula_9" coords="6,180.90,259.26,53.02,12.86">2 1 v v Sim V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Semantic Matching</head><p>First, we define the similarity scores between two frames. Let F1= (v1, A1), F2 = (v2, A2). We divide the similarity score into two components, one indicating the similarity between the verbs, and the other indicating the similarity between the arguments.</p><formula xml:id="formula_10" coords="6,59.46,370.41,230.77,31.46">) , ( ) 1 ( ) , ( ) , ( 2 1 2 1 2 1 A A Sim v v Sim F F Sim A V × - + × = α α (4.2.3.1)</formula><p>where denotes the similarity score between two argument sets, and</p><formula xml:id="formula_11" coords="6,89.94,405.75,60.07,12.38">) , ( 2 1 A A Sim A</formula><p>α is a weighting parameter that can be tuned. Our experimental results show that α does not affect the performance much within a certain range. Therefore we fix α to be 0.5.</p><p>The similarity between the two sets of arguments is measured at the lexical level. We do not use WordNet to expand the terms in the arguments because many of the arguments are named entities such as persons and organizations, for which finding similar terms is not so meaningful.</p><p>To precisely match the argument sets of two frames, we should do pairwise matching of the arguments, that is, matching ARG0 in the first frame with ARG0 in the second frame, and ARG1 in the first frame with ARG1 in the second frame, etc. However, we choose to do a fuzzy matching by considering all arguments in a frame together as a bag of independent terms. There are two reasons for doing fuzzy matching: (1) ASSERT often makes mistakes and therefore does not tag the arguments consistently, especially for adjunctive arguments, and (2) since we consider semantically related verbs, the semantic roles of the arguments may be different in different frames. Our preliminary experimental results also show that considering all arguments together is better than considering them separately.</p><p>We use Jaccard coefficient to measure the similarity between two sets of arguments. Suppose we are to compute , where A1 and A2 are two argument sets:</p><formula xml:id="formula_12" coords="6,313.44,85.68,234.30,80.07">) , ( 2 1 A A Sim A )} , ( ),..., , ( ), , {( , 1 , 1 2 , 1 2 , 1 1 , 1 1 , 1 1 m m T l T l T l A = (4.3.2.2) )} , ( ),..., , ( ), , {( , 1 , 1 2 , 2 2 , 2 1 , 2 1 , 2 2 n n T l T l T l A = (4.2.3.3) j i</formula><p>l , is the argument label of the jth argument of Ai, and is the set of terms in the jth argument of Ai.</p><formula xml:id="formula_13" coords="6,308.28,172.01,205.50,61.19">Let j i T , U m i i T T 1 , 1 1 = = and (4.2.3.4) U n i i T T 1 , 2 2 = =</formula><p>We then remove the stop words from T1 and T2. Let the sets of terms after stop word removal be T1' and T2'. We then define the similarity between A1 and A2 as ' '</p><formula xml:id="formula_14" coords="6,348.00,287.11,162.66,45.23">' ' ) , ( 2 1 2 1 2 1 U I T T T T A A Sim A = (4.2.3.5)</formula><p>Both the question and the answer passages may contain more than one semantic frame. We compute pairwise frame similarity scores between the question and an answer passage, and pick the maximum score as the semantic similarity between the question and the answer passage.</p><p>Finally, we use the semantic similarity scores to rank passages. For a question, we first use a density based passage retrieval method to retrieve the top 100 passages. We than rank these 100 passages based on their semantic similarities to the question, as defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3</head><p>Answer Projection and Verification After we obtain the answer nuggets from the web, we need to project the answer nuggets back to the corpus. For list questions, we focus more on the recall so we only perform answer projection on the document level. For factoid questions after the projection step we use dependency relation based answer ranking to verify if the answer nugget from the web is correct in the local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Results</head><p>Table . 1 shows the evaluation result for TREC-14 factoid and list questions based on run NUSCHUA1. We find that our performance for factoid questions is improved over our result last year. The main reason is that after query expansion our system is more capable of handling short questions. However, our semantic answer nugget selection does not perform very well on list questions. The major problem is due to the recall of the semantic parser and the strict matching criteria we imposed on the frame matching, which will reduce the recall of finding answers for list question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Definition Generation for Topics</head><p>We consider it important to identify precise and complete definition sentence for topics because it facilitates the answering of factoid and list questions, and more importantly, it helps to answer the Other questions. In TREC-13, we applied a soft pattern model to boost the recall of definition sentence retrieval. This year, we use the improved bigram soft pattern model <ref type="bibr" coords="7,181.77,279.69,79.81,9.93" target="#b0">(Cui et al., 2005)</ref>, and combine it together with external knowledge, to identify precise definition sentences. Note that for external knowledge, we choose to utilize specific websites rather than general search engines, so that we get more precise results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Statistical Ranking of Definition Sentences with External Knowledge</head><p>To ensure recall, for each topic, we construct two data sets as the basis for selecting definition sentences: one based on the TREC corpus and the other from external knowledge. The TREC set is constructed by relevant documents determined by the document retrieval module using the topic as the query. We retrieve up to 800 documents for each topic. These documents are split into sentences. To construct the external knowledge set, we accumulate existing definitions for the topics from http://www.answers.com/. The definitions are downloaded through pre-written wrappers for the website.</p><p>We first perform statistical weighting of sentences on both of the data sets to find the sentences relevant to the given topics. When ranking sentences with corpus word statistics, we employ the centroid-based ranking method, which has been used in other definitional QA systems (e.g., <ref type="bibr" coords="7,56.70,633.21,70.02,9.93" target="#b1">Xu et al., 2003)</ref>. We select a set of centroid words (excluding stop words) which co-occur frequently with the search target in the input sentences. To select centroid words, we use mutual information to measure the centroid weight of a word w as follows: (5.1.1) where Co(w, sch_term) denotes the number of sentences where w co-occurs with the search term sch_term, and sf(w) gives the number of sentences containing the word w. We also use the inverse document frequency of w, idf(w)<ref type="foot" coords="7,456.84,71.88,3.52,6.14" target="#foot_1">4</ref> , as a measure of the global importance of the word. Words whose centroid weights exceed the average plus a standard deviation are selected as centroid words.</p><formula xml:id="formula_15" coords="7,114.30,709.67,180.36,22.05">) ( ) 1 ) _ ( log( ) 1 ) ( log( ) 1 ) _ , (<label>log</label></formula><p>The weighting of centroid words can be improved by using external knowledge. We augment the weight of the centroid words which also appear in the definitions from the external knowledge data set. We form centroid words into a centroid vector, which is then used to rank input sentences by their cosine similarity with the vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generic Soft Pattern Model</head><p>After performing the statistical ranking step above, we have a list of ranked sentences with definition sentences ranked highly. However, not all sentences that are ranked highly are definition sentences, although all of these sentences are related to the topic.</p><p>In most TREC QA systems, definition patterns are manually constructed in a labour intensive manner, and are usually in the form of regular expressions. Such patterns require exact matching and hence we call them hard patterns. We observe that definition sentences such as "… the weed kudzu, a vine planted for soil stabilization that has grown like wild ..." follows certain patterns, but often with minor variations for a variety in writing styles. Hard patterns usually fails in matching such linguistic variations in vocabulary and syntax, and learned hard patterns cannot match definition sentences that are not seen in the training data. Therefore we propose and employ a soft pattern model discussed in <ref type="bibr" coords="7,400.15,488.73,78.81,9.93" target="#b0">(Cui et al., 2004)</ref>, and further improved it in <ref type="bibr" coords="7,384.25,501.39,86.35,9.93" target="#b0">(Cui et al., 2005)</ref> to produce a theoretically sound generic soft pattern model. In <ref type="bibr" coords="7,308.28,526.65,73.96,9.93" target="#b0">(Cui et al., 2005)</ref>, a bigram soft pattern model and a profile HMM soft pattern model are proposed, and we apply the bigram soft pattern model here because the profile HMM model requires more training instances to converge.</p><p>For a definition pattern containing the search target, we consider the tokens on the left of the search target, left_seq, separately from those on the right of the search target, right_seq, and compute their respective scores separately. We combine the two scores using linear interpolation: In the original bigram soft pattern model used in TREC-13, the probability of a sequence is computed simply as a product of probabilities of bigrams. For our improved bigram soft pattern model, we apply linear interpolation of unigrams and bigrams to represent the probability of bigrams to smooth the distribution to generate more accurate statistics for unseen data, as well as to incorporate the conditional probability of individual tokens appearing in specific slots. For a given bigram model µ with slots S 1 to S L , a sequence of pattern tokens t 1 to t L is modeled as follows:</p><formula xml:id="formula_16" coords="8,60.30,209.50,222.59,21.38">( ) ( ) ( ) ( ) ( ) ( 1 1 1 1 2 1 score , , | log | log | 1 | L L i i i t t P t S P t t P t S L µ λ - = ⎛ ⎞ = + + - ⎜ ⎟ ⎝ ∑ K ) i i λ ⎠</formula><p>(5.2.2) where P(t i | S i ) is the conditional probability of token t i appearing in slot S i . The unigram and bigram probabilities are estimated using maximum likelihood estimation, and Laplacian smoothing is applied to these probabilities. For more details on our bigram soft pattern model, please refer to <ref type="bibr" coords="8,267.39,310.23,19.58,9.62;8,56.70,322.59,51.24,9.93" target="#b0">(Cui et al., 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Manually Constructed Patterns</head><p>On top of centroid-based weighting and soft pattern matching, we also optionally use a set of manually constructed patterns for matching definition sentences. This set of patterns includes the subset of patterns we used for TREC-12 that are used in TREC-13. The set consists mainly consisting of appositives and copulas patterns, which are high-precision patterns represented in regular expressions, such as "&lt;SEARCH_TERM&gt; is DT$ NNP". For this year's TREC QA, we used additional high-precision patterns used to match numeric data, such as years and distance units.</p><p>Such hard matching patterns are used on top of the soft patterns because a small number of good definition sentences are dropped due to the imposed cut-off of ranking scores by centroid-based weighting and soft pattern matching, and we want to capture these sentences. Also, sentences containing numerical data are presented in a large number of formats and are not given very high scores by the soft pattern models.</p><p>Hence, the system works by first ranking all the sentences using centroid-based ranking and soft pattern matching, and then taking the top ranked sentences as candidate definition sentences. Optionally, for boosting the recall of the definition sentences, it then examines those lower ranked sentences which are not included in the candidate definition sentences, and adds those sentences matched by any of the manually constructed patterns into the list of definition sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Redundancy Removal</head><p>Like in TREC-13, this year's TREC QA guidelines requires that systems remove nuggets that are already covered in the topic-related factoid and list questions from their list of definition nuggets. Our system performs a two-stage redundancy check when selecting definition sentences into the final answer. First, we define the list of sentences used to answer the factoid and list questions for the same topic as factoid sentences. For selecting N sentences for the final answer, we apply the following selection process on our ranked list of sentences: 1. Add the first sentence in the ranked list of sentences into the list of answer sentences. Here, we measure the similarity between two sentences using simple cosine similarity with each term weighed by its inverse document frequency (IDF). Since the answers to factoid or list questions tend to account for very small fraction of the sentences, we apply a stricter similarity threshold on these sentences.</p><p>For this year, we choose to return only full sentences as our definition nuggets, without attempting to extract the relevant substrings of the sentences. This is unlike TREC-13, where heuristic rules were used to extract only the relevant parts, such as the appositive part. The reason is that the context of the sentence is often lost when such extraction is done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation Results</head><p>This year, we submitted three runs for the Other questions. The first run produces 14 definition sentences using only soft pattern matching. The second run produces 12 definition sentences using soft pattern matching and 2 using the hard pattern matching rules as used in TREC-13, but not including the numeric ones. The third run produces 12 definition sentences using soft pattern matching and 6 using the hard pattern matching rules, including the numeric ones. The average F 3 -scores are shown in Table <ref type="table" coords="8,395.07,760.65,4.13,9.62" target="#tab_6">2</ref>. From the scores, we see that our bigram soft pattern model performs as well as the manual nonnumeric hard pattern matching rules, which already has a performance that is much better than the median average F 3 -score of 0.156 across all the 71 runs. However, since number matching is a hard problem in general, adding some hard numeric matching rules to augment the soft pattern model in our third run actually boosts the results, even though the returned answers are much longer now.</p><p>This year, the topics are extra challenging for answering the Other questions, reflected by the across-the-board low scores, because a large number of the topics are events. A reason is because events can span across a long period of time and involve a large number of entities. At the same time, having to exclude all definition nuggets that have been covered by the topic-related factoid and list questions continue to make answering the Other questions difficult. We continue to observe that most of the important aspects of a topic have already been asked in the factoid and list questions, leaving little else for answering the Other question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have reviewed the newly-adopted techniques in our QA system. They include using dependency relation analysis for query expansion, using semantic relation analysis for answer nugget extraction from the web and using bigram soft pattern model for definition sentence selection. While these techniques have improved our previous QA system, we note that more improvements may be pursued in future work. First, the recall of semantic parsing is not high enough to cover most of the questions. Therefore to increase the recall of semantic parsing will be one of our future works in applying semantic relation analysis to QA. Secondly, more experiments should be conducted to figure out the effect of query expansion on the specific type of questions, in particular for questions with different lengths. Third, further work needs to be done for answering Other questions for events. Also, there is a need to find ways to integrate numberic matching into the soft pattern models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,308.28,203.81,230.37,9.57;1,326.28,216.47,212.25,9.57;1,326.28,229.13,86.67,9.57"><head>( 3 )</head><label>3</label><figDesc>We train a bigram soft pattern statistical model to capture the patterns inherent in a set of training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,318.36,325.20,210.17,9.88;4,332.40,337.86,164.00,9.88;4,308.28,362.52,5.49,9.88;4,329.88,362.52,208.78,9.88;4,329.88,375.18,21.97,9.88"><head></head><label></label><figDesc>Figure. 2 Precision-recall graph of document ranking task (RUN NUSCHUAR1)4 Semantic Answer Nugget Extraction on the Web</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,64.80,799.26,208.96,9.88"><head>Figure</head><label></label><figDesc>Figure.3 Sample Output By ASSERT Parser</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,333.12,662.83,3.11,16.21;7,406.68,662.83,3.11,16.21;7,447.48,660.63,3.11,19.40;7,500.64,660.63,3.11,19.40;7,420.60,682.27,3.11,16.21;7,442.68,682.27,3.11,16.21;7,467.64,680.07,3.11,19.40;7,531.36,680.07,3.11,19.40;7,312.24,668.73,20.21,8.82;7,366.42,668.73,2.53,8.82;7,426.54,668.73,20.21,8.82;7,482.34,668.73,2.02,8.82;7,423.54,688.23,5.06,8.82;7,446.70,688.23,20.21,8.82;7,509.22,688.23,2.02,8.82;7,491.76,674.01,7.09,5.26;7,518.76,693.45,10.81,5.26;7,336.78,668.45,69.38,9.10;7,451.09,668.45,29.84,9.10;7,471.54,687.95,36.19,9.10;7,418.56,664.62,6.37,13.08;7,486.17,664.62,5.82,13.08;7,435.06,684.12,6.37,13.08;7,513.05,684.12,5.82,13.08;7,411.90,665.20,5.54,12.38;7,414.24,684.70,20.29,12.38;7,509.16,704.85,29.39,9.62;7,308.28,717.21,38.48,9.93;7,346.74,717.21,42.68,10.56;7,389.34,717.51,149.23,10.26;7,308.28,730.11,229.18,9.62"><head></head><label></label><figDesc>2.1)where µ left and µ right are the bigram soft pattern models for the left and right sequences respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,38.70,56.70,545.28,351.36"><head></head><label></label><figDesc></figDesc><graphic coords="2,38.70,56.70,545.28,351.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,308.28,677.70,237.68,84.82"><head>Table 1 . Performance for factoid and list questions</head><label>1</label><figDesc></figDesc><table coords="6,311.28,690.17,210.93,72.36"><row><cell></cell><cell>NUSCHUA1</cell><cell>TREC Highest</cell><cell>TREC Medium</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>for factoid</cell><cell>0.666</cell><cell>0.713</cell><cell>0.152</cell></row><row><cell>questions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>for list</cell><cell>0.331</cell><cell>0.468</cell><cell>0.053</cell></row><row><cell>questions</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,61.68,59.40,227.12,34.52"><head>Table 2 . Performance for Other questions</head><label>2</label><figDesc></figDesc><table coords="9,61.68,72.45,227.12,21.47"><row><cell></cell><cell cols="3">NUSCHUA1 NUSCHUA2 NUSCHUA3</cell></row><row><cell>Avg F 3 -score</cell><cell>0.195</cell><cell>0.193</cell><cell>0.211</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,74.52,763.77,212.56,8.74;2,56.70,775.29,26.15,8.74"><p>Defined as when Google returns : "Did you mean: XXX"</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="7,324.90,764.07,213.68,8.74;7,308.28,775.59,230.35,8.74;7,308.28,787.05,146.49,8.74"><p>We use the statistics from the Web Term Document Frequency and Rank site to approximate words' IDF (http://elib.cs.berkeley.edu/docfreq/)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,61.58,714.33,225.55,9.93;9,56.70,726.93,230.35,9.93;9,56.70,739.59,230.28,9.93;9,56.70,752.55,230.25,9.62;9,56.70,765.21,62.95,9.62;9,308.28,59.31,230.38,9.93;9,308.28,71.97,230.27,9.93;9,308.28,84.63,230.23,9.93;9,308.28,97.53,230.27,9.62;9,308.28,110.19,230.31,9.62;9,308.28,122.85,149.50,9.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,129.06,726.93,157.99,9.93;9,56.70,739.59,225.56,9.93;9,338.70,71.97,199.85,9.93;9,308.28,84.63,84.27,9.93">National University of Singapore at the TREC-13 Question Answering Main Task</title>
		<author>
			<persName coords=""><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,56.70,752.55,230.25,9.62;9,400.26,84.93,138.25,9.62;9,308.28,97.53,230.27,9.62;9,308.28,110.19,181.16,9.62">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2005. Aug 15-19</date>
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
	<note>Proceedings of the 13th Text Retrieval Conference</note>
</biblStruct>

<biblStruct coords="9,308.28,147.87,230.33,9.93;9,308.28,160.53,230.31,9.93;9,308.28,173.43,201.69,9.62;9,308.28,198.45,230.34,9.93;9,308.28,211.11,230.38,9.93;9,308.28,223.77,230.22,9.93;9,308.28,236.67,230.31,9.62;9,308.28,249.33,24.78,9.62;9,308.28,274.35,230.25,9.93;9,308.28,287.07,230.32,9.93;9,308.28,299.61,230.32,9.93;9,308.28,312.57,230.34,9.62;9,308.28,325.23,230.25,9.62;9,308.28,337.89,230.37,9.62;9,308.28,350.55,230.33,9.62;9,308.28,363.21,22.98,9.62;9,308.28,388.17,230.35,9.93;9,308.28,400.83,230.40,9.93;9,308.28,413.49,230.30,9.93;9,308.28,426.45,230.36,9.62;9,308.28,439.05,74.92,9.62;9,308.28,464.07,230.31,9.93;9,308.28,476.73,230.29,9.93;9,308.28,489.69,189.19,9.62;9,308.28,514.65,233.01,9.93;9,308.28,527.37,230.46,9.93;9,308.28,539.97,230.31,9.93;9,308.28,552.93,184.26,9.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,402.18,147.87,136.43,9.93;9,308.28,160.53,55.02,9.93;9,503.15,211.11,35.50,9.93;9,308.28,223.77,225.16,9.93;9,511.08,274.35,27.45,9.93;9,308.28,287.07,230.32,9.93;9,308.28,299.61,34.51,9.93;9,382.32,400.83,156.36,9.93;9,308.28,413.49,74.65,9.93">Query expansion using local and global document analysis</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lin ; Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,389.39,160.83,149.19,9.62;9,308.28,173.43,70.25,9.62;9,308.28,236.67,157.98,9.62;9,432.33,312.57,106.29,9.62;9,308.28,325.23,230.25,9.62;9,308.28,337.89,230.37,9.62;9,308.28,350.55,92.55,9.62;9,399.54,413.79,139.04,9.62;9,308.28,426.45,161.89,9.62;9,355.14,476.73,183.43,9.93;9,308.28,489.69,105.72,9.62">Information Retrieval Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<editor>
			<persName><surname>Moldovan</surname></persName>
		</editor>
		<meeting><address><addrLine>Granada, Spain; Boston, MA; Zurich, Switzerland; San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1998. May, 1998. 2004. 2004. 1996. 1996. 2002. 2002. 2002. 2003</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
	<note>Lexical Chains for Question Answering, Proceeding of COLING. Xu et al., 2003] J. Xu, A. Licuanan, R. Weischedel, TREC 2003 QA at BBN: Answering Definitional Questions, The Twelfth Text REtrieval Conference (TREC 2003) Notebook</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
