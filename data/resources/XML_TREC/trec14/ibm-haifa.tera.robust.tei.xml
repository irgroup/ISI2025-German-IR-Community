<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.83,101.33,350.34,15.15;1,154.05,121.26,303.91,15.15">Juru at TREC 2005: Query Prediction in the Terabyte and the Robust tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.59,181.84,68.85,8.77"><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
							<email>yomtov@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.33,181.84,67.50,8.77"><forename type="first">David</forename><surname>Carmel</surname></persName>
							<email>carmel@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.32,181.84,67.50,8.77"><forename type="first">Adam</forename><surname>Darlow</surname></persName>
							<email>darlow@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.19,192.80,53.07,8.77"><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
							<email>dpelleg@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.84,192.80,95.44,8.77"><forename type="first">Shai</forename><surname>Errera-Yaakov</surname></persName>
							<email>shaie@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.50,192.80,47.31,8.77"><forename type="first">Shai</forename><surname>Fine</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Haifa Research Lab Haifa 31905</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.83,101.33,350.34,15.15;1,154.05,121.26,303.91,15.15">Juru at TREC 2005: Query Prediction in the Terabyte and the Robust tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">827723959B1CB007AE8CDA7726E4E5D0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our experiments focus this year on the ad-hock tasks of the Terabyte and the Robust tracks. In both tracks we experimented with the query prediction technology we developed recently. In the Terabyte track, we investigated how query prediction can be used to improve federation of search results extracted from several indices. We show that federated search based on query prediction can achieve comparable results to single-index search. In the Robust track we trained a predictor over one collection (TREC-8) for predicting query difficulty over another collection (AQUAINT). The experimental results show that difficult topics on the TREC-8 collection were not found to be consistently difficult on the AQUAINT collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our experiments focused this year on the ad-hock tasks of the Terabyte and the Robust tracks. In both tracks we experimented with the query prediction technology we developed recently <ref type="bibr" coords="1,243.85,488.83,10.52,8.74" target="#b7">[7,</ref><ref type="bibr" coords="1,256.03,488.83,7.01,8.74" target="#b6">6]</ref>. In the Terabyte track, we investigated how query prediction can be used to improve federation of search results extracted from several indices. A predictor is learned for each index, based on last year's topics for training. When a (new) query is executed, a ranked list of documents is returned from each index and the prediction of query difficulty is computed for each result set. The predicted difficulty is used for weighting the scores of results and the final ranking is built by merging the lists using these weighted scores.</p><p>In addition, we experimented with two issues related to distributed search. The first is whether federated search over distributed index (a set of indices, each represents a sub-collection), can be comparable in terms of effectiveness to searching over one single index representing the whole collection. The second is whether the partitioning policy of the data to the distributed sub-collections affects the effectiveness of federated search. For the first question, we compared query-prediction based federated search over distributed index with search over one single equivalent index. For the second question, we experimented with several partition schemes, testing how they affect search results.</p><p>In the Robust track, one of the tasks is to predict the queries average precision. We trained our predictor on the TREC-8 collection using the old 249 TREC topics, and used the predictor over the AQUAINT collection for which no training data was given. Our hypothesis was that the TREC based predictor can predict the query difficulty on different collection than the collection it was trained for. In addition, following the good results obtained by several groups last year using web expansion, we upgraded our system to benefit web expansion based on the answers.com web search engine <ref type="foot" coords="2,183.73,150.30,3.97,6.12" target="#foot_0">1</ref> . answers.com is a free search service, providing instant answers over many topics. Our expansion procedure worked by first submitting the topic title to answers.com, and then using the result page for query expansion.</p><p>The rest of the paper is organized as follows: Section 2 describes the distributed search over .gov2 collection, and compares federated search with single-index search. Section 3 describes our experiments in the Robust track. Finally, Section 4 concludes.</p><p>2 Terabyte track: Federated versus single-index search</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributed search over the gov2 collection</head><p>Our experiments with federation required a baseline run. This baseline run should optimally have been a single search index representing the entire collection. However, our search engine Juru, at the time of experimentation, was not able to index the entire collection into one single index in reasonable time. Therefore, we used a distributed search framework in order to simulate a single search index. The distribution was done over a partition of the gov2 collection into ten equal disjoint parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The indexing process</head><p>Each document in the collection was indexed according to its textual content and according to its anchor data which includes all page's in-link anchors. A special anchor database was created that maps every page in the collection to its anchor data. We used the following process to collecting anchors:</p><p>1. Scan the .gov2 collection page by page and collect all anchor tuples (url, anchor, link-type) e.g. (www.nasa.gov, "Nasa Home page", different-hosttype).</p><p>2. Distribute the anchor tuples into 53 disjoint files according to the url's first characters. Filter out all urls linking outside the collection.</p><p>3. Sort each of the files according to the urls.</p><p>4. Collect all anchors related to a specific url and add them to the Anchor database.</p><p>The anchor database includes 19 million records, i.e. 76% of the pages have at least one anchor. The average number of anchors per page is 10, however there are few pages with more than 1,000 anchors, and even one page with 680,000 anchors. The database was used through the indexing process to extract the page's anchors to be concatenated with its textual content.</p><p>The page's tokens were marked according to their type (regular, headline, title, different-host-anchor, same-host-anchor). These types were associated with different boosts during query execution time. A title token contributes three times more than a regular token to the document's score. In addition, every page was associated with a static score reflecting its authority. The page's static score was computed based on square root of the number of in-links pointing to the page. At query time, the textual score of the page is linearly combined with the page's static score to yield its final score.</p><p>The indexing process took 96 hours on a single machine (1 CPU, 2.4GH, 2GB physical memory). As already mentioned, following our system limitations, we partitioned the data into ten disjoint indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The search process</head><p>The distributed search algorithm has a two phase query process. In the first phase, the query retrieves all relevant term statistics, e.g. terms' document frequency, from each of the indices. These statistics are combined in order to create global statistics. These global statistics are sent back to the indices for a second phase which consists of the actual ranking of documents. The retrieved results from all of the indices can then be merged easily because all of their scores were calculated based on the same global statistics. In this approach, distributed search is equivalent to a single-index search.</p><p>Average query time was still relatively high with the distribution algorithm. Therefore we experimented with a pruning algorithm. Pruning was done using the WAND algorithm <ref type="bibr" coords="3,171.82,327.98,9.97,8.74" target="#b0">[1]</ref>. WAND is a document-at-a-time algorithm based on a two level approach: at the first level, it iterates in parallel over query term postings and identifies candidate documents using an approximate evaluation taking into account only partial information on term occurrences and no query independent factors; at the second level, promising candidates are fully evaluated and their exact scores are computed. The efficiency of the evaluation process is improved significantly using dynamic pruning techniques with very little cost in effectiveness. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation.</p><p>In our experiment we tested the effect of the two stage evaluation process by comparing two runs. One run did pruning as described above and the other did no pruning at all. Interestingly, the results shown in Table <ref type="table" coords="3,381.35,454.50,4.98,8.74" target="#tab_0">1</ref> demonstrate that the run which did pruning achieved slightly better results than the run which did no pruning. This indicates that not only did the pruning reduce execution time by a factor of three, it was actually able to filter out some noisy documents which the normal ranking algorithm ranked too highly. A possible explanation stems from the fact that the much simpler scoring model used in the first phase does not take into account many document features such as occurrence counts, document length and link analysis. Therefore, it is not sensitive to documents that are noisy in relation to such features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Federated search using query prediction</head><p>Using federated information retrieval on a large collection such as .gov2 requires two additional processing stages during indexing and retrieval. Before indexing, it is first required to partition the collection into smaller sub-collections, each of which are indexed separately. During retrieval, each sub-collection is queried separately and so it is necessary to federate the results into a coherent result list.</p><p>We experimented mainly with different partition schemes, which we detail below. Federation was based on query prediction methods we have previously developed <ref type="bibr" coords="4,126.00,178.26,10.52,8.74" target="#b7">[7,</ref><ref type="bibr" coords="4,138.17,178.26,7.01,8.74" target="#b6">6]</ref>, although these were further developed as we describe in the remaining paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partitioning schemes</head><p>Our initial hypothesis was that the precise scheme used to partition the documents into indices might have an effect on retrieval performance. We identified several straightforward partitioning schemes, described in detail below. Due to the volume of the .gov2 collection, we decided to apply and evaluate the different schemes on the smaller .gov collection. The different schemes and identifiers follow.</p><p>random This scheme places each document in a random partition ("bin").</p><p>domain The document's URL is stripped to its two top-level domains. Then, a hash function maps each stripped URL to one of the bins. The hash functions guarantees that two identical strings are always mapped to the same bin. But it does not guarantee a unique bin to each string (and this constraint is impossible to meet, since the number of documents is several orders of magnitudes greater than the number of bins). domain* One bin is dedicated to the nasa.gov documents (150K documents), and another to noaa.gov documents (102K documents). The other documents are placed in the remaining bins as in domain. rand-ts Documents with no meaningful title string are placed into their own bin.</p><p>Such documents have typically empty (or all-whitespace) titles, or titles such as: "untitled document", "image", "ppt", "you are now leaving the ...", etc. Additionally, another bin is dedicated to documents from large groups which all share the same title. Such are the entire domains gcmd2.gsfc.nasa.gov and www.fleets.doe.gov, as well as portions of students.gov and clinicaltrials.gov. All other documents are placed into the remaining N -2 bins randomly. titlehash A hash function is applied to the document title. If there is no title, the URL is used. ttl-stop Two special bins are used for the documents with meaningless and repeating titles, as in rand-ts. Then, a hash function maps the title (or URL, if it is missing) to one of the remaining N -2 bins. sizelink Each bin is used for documents with a similar in-degree. To assure equallysized bins, documents with in-degree one were split into four bins according to their size, and documents with in-degree of four or more were aggregated. ttl-stem Document titles (or URLs) are tokenized, stemmed, and stripped of stop words. The resulting bags-of-words are then mapped into a bin by a hash function cluster Clusters of documents are created with respect to the distance measure between bags of words. Each cluster corresponds to a bin.</p><p>sequential This partition simply mimics the directory structure of the distributed GOV2 files, generating partitions of roughly the same size. This is not a random partition, and seems to be based on a BFS order of the documents, starting from the site roots.</p><p>In our experiments, we normally used N = 10 bins. For evaluation, we trained the resulting federated search system on the 50 topic distillation queries of the TREC 2003 web track and tested it on the 50 topic distillation queries of the TREC 2002 web track. Comparison was based on both the mean average precision (MAP) and the precision-at-10 (P@10). In general, the partition methods that performed best were based on the document title, with random coming in at a close second. However, the overall differences in scores were minor. Consequently, it is our impression that the precise partitioning scheme is immaterial. Having said that, we note that some methods (such as cluster, and one based on cliques found in the hyperlink graph), result in consistently poor results.</p><p>Another issue which we briefly touched is the effect of partition size on retrieval results. This has implications when choosing the number of partitions and also when projecting the results to the much bigger .gov2 collection. To this end, we used partition sizes of N = 15, 20, 30 with the methods ttl-stem, random, ttl-stop, and sizelink. The results were slightly lower than for the N = 10 case. Due to the large amount of processing involved, we were not able to determine if this difference is significant or if the trend continues for larger N .</p><p>The TREC results were obtained using the sequential and random partitions, which needed the least amount of processing and therefore were ready first. As a sanity check, we later repeated the run with the ttl-stem partition. The results are summarized in Table <ref type="table" coords="5,220.45,361.19,3.88,8.74" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Federation using query prediction</head><p>In a previous paper <ref type="bibr" coords="5,218.07,403.86,10.52,8.74" target="#b6">[6]</ref> we described the use of a query prediction algorithm for federated retrieval. The gist of the method we developed in that article is to train a query predictor for each sub-collection. When a query is executed, the ranked list of documents is returned from each sub-collection and the prediction of query difficulty is computed for each. The predicted difficulty is used for weighting the document scores (DSs) of the results and the final ranking is built by merging the lists using these weighted DSs. Thus the method we proposed sets a query-by-query weight for each search engine and document collection pair.</p><p>Training a predictor requires several training queries with known accuracies. In the context of the Terabyte track this year, only the 50 queries from TREC 2004 could be used.</p><p>The input features used for the query predictor are:</p><p>1. The overlap between the results set of the full query and those of the subqueries (query terms containing keywords and lexical affinities).</p><p>2. The document frequency (DF) of the sub-queries.</p><p>In <ref type="bibr" coords="5,139.62,604.40,10.51,8.74" target="#b6">[6]</ref> these pairs of overlaps and document frequencies are then mapped into a histogram. Each cell in the histogram is considered a feature, and from these features a mapping is learned to the actual query difficulty. Unfortunately, due to the limited number of training queries and the size of the histogram (which is dependent on the span of the keyword document frequencies) the training set has more features than examples. This can cause over-fitting of the predictor to the training set <ref type="bibr" coords="5,467.55,659.20,9.97,8.74" target="#b5">[5]</ref>.</p><p>Therefore, we developed a method for representing the histogram using an equivalent histogram with fewer independent cells. This feature reduction algorithm generates a new histogram starting with a histogram in which each cell is considered independent (and thus is later converted into a feature). Since a training query is represented by a list of overlap and DF pairs which map every sub-query into a histogram cell, as well as a difficulty (e.g., mean average precision (MAP)), the algorithm merges adjacent cells if one of the following conditions is met:</p><p>1. They contain no training data.</p><p>2. The points inside them are from queries with similar difficulty.</p><p>3. Each of the queries that have a point in the cells and a dissimilar difficulty have at least one sub-query which does not appear in other queries which appear in these cells. Thus, there is at least some evidence which can explain the dissimilar difficulty.</p><p>The algorithm randomly selects the cells to be merged and repeats merging until no merges have been performed for a given number of merge attempts. Our experiments showed that a histogram of 506 cells (11 overlaps and 46 DF values) can be efficiently represented using approximately 20 values. An example of such a mapping is shown in figure <ref type="figure" coords="6,163.75,308.78,4.98,8.74" target="#fig_0">1</ref> in which each color represents cells which map into the same feature. This aggregation results in a lower dimensional feature vector, which greatly reduces the possibility of overfit, even when the number of training queries is small. Data for the federation algorithm was obtained by running the WAND algorithm on each of the 10 partitions and collecting a full (10,000 document) results set from each. Finally, instead of the linear learning algorithm suggested in <ref type="bibr" coords="6,438.57,615.36,10.52,8.74" target="#b6">[6]</ref> we used a Support-Vector Machine (SVM) operating in regression mode as the learning algorithm. The SVM implementation we used was SVMlight <ref type="bibr" coords="6,405.44,637.28,9.97,8.74" target="#b4">[4]</ref>, with a radialbasis kernel using the default parameters supplied by SVMlight. We used the SVM algorithm due to its improved accuracy, especially with limited training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of federation using query prediction</head><p>Table <ref type="table" coords="7,154.33,94.29,4.98,8.74" target="#tab_1">2</ref> shows the results of the different partition schemes compared to that of distributed search and the best and median TREC results. This table shows that the different partitioning schemes result in minor retrieval differences, which are also not significant. Additionally, the results of the federated search are very similar to those of the distributed search, which is equivalent to single-index search, thus exhibiting that prediction-based federation can be used as a viable alternative to single-index search. Such federated search has the additional benefits of lower computational cost and better scaling properties. 3 Query Prediction in the Robust track</p><p>The Robust track addressed a new open question this year -does a query considered "difficult" by most systems on one collection will still be considered difficult when executed on a different collection? Another question to experiment with was whether the special techniques developed for the Robust tracks proved to be useful on one collection are still useful when invoked on another collection. For that, systems were tested on 50 old "difficult" TREC-8 topics. The systems' task was to run those topics against the AQUAINT collection, for which no Qrels exist.</p><p>Following this direction of research, we experimented with the question whether a query difficulty predictor, trained by the 249 old TREC-8 topics and their given TREC-8 Qrels, can be used to predict query difficulty on the AQUAINT collection.</p><p>For that, we trained two predictors using TREC-8 collection and topics, one for description based queries and one for title based queries, and used those predictors to predict the queries difficulty as required by one of the track's tasks.</p><p>In addition, following the good results obtained by several groups last year using web expansion, we upgraded our system to benefit web expansion using answers.com web search engine. Answers.com is a free search service, providing instant answers over many topics. As opposed to standard search engines that serve up a list of links to follow, it displays quick, snapshot answers with concise, reliable information. If it fails to answer the query it returns the first result returned by Google for that query.</p><p>Our expansion procedure worked by first submitting the topic title to answer.com, and then using the result page for query expansion. Each query term is expanded by its lexical affinities as found in the expanding Web page <ref type="bibr" coords="7,387.13,620.34,9.97,8.74" target="#b1">[2]</ref>.</p><p>One of the issues related to query expansion is how to optimally weight the expanded terms. Rocchio formula and other expansion approaches set an ad-hock fixed weight for the expanded terms, however, it seems that optimal weights should be set dynamically depending on the original query terms and the quality of the expanded terms.</p><p>We experimented with using the query predictor to set optimal weights for the expanded query terms. Given an expanded query, we searched over the weight vector space for an optimal weight vector. By exhaustive search we find an "optimal" weight vector that maximizes the predicted average precision for the given expanded query. For each weight vector we used the query predictor to predict the average precision of the expanded query, when the query terms are weighted according to the given weight vector. Since the weight space is infinite we discretized the weight values to 6 possible discrete values, reducing the search space size to |q| 6 , where |q| is the number of terms in the expanded query.</p><p>Note that our main hypothesis behind this run is that optimal weights for a query, computed using the TREC-8 collection, are also optimal weights for AQUAINT. Note also that the query's average precision was computed using TREC-8 Qrels, therefore this run actually tunes the system to optimally handles the queries given the Qrels, hence is considered as manual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Robust track results</head><p>Table <ref type="table" coords="8,153.73,296.12,4.98,8.74" target="#tab_2">3</ref> shows the result of applying query expansion to the Robust topics of this year. The table shows that the Web expansion improves over simple retrieval. These differences are all significant (paired sign test, p &lt; 0.05), except for the difference between P@10 of the run with no expansion and the manual expansion run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results of query prediction</head><p>Table <ref type="table" coords="8,153.47,576.51,4.98,8.74" target="#tab_3">4</ref> shows the results of the query prediction algorithm tested on the TREC-8 and the AQUAINT collections. This table shows three types of comparisons: R 2 , which is commonly used to estimate the quality of regression algorithms; Kendall'sτ , which is the measure used in <ref type="bibr" coords="8,268.08,609.38,9.97,8.74" target="#b6">[6]</ref>; and the area between the MAP of the queries when sorted by actual difficulty and when sorted by predicted difficulty, which is the measure used this year.</p><p>The table shows that even for the TREC-8, the query prediction algorithm did not do well compared to the results cited in <ref type="bibr" coords="8,303.23,659.20,9.97,8.74" target="#b6">[6]</ref>, especially for description-part queries. This is explained by the fact that the predictor learned to predict using 249 topics spanning a wide distribution of query difficulties, but tested on only 50 difficult topics. In such cases it may be prudent to use trasductive algorithms <ref type="bibr" coords="9,423.45,97.08,10.52,8.74" target="#b2">[3]</ref> rather than inductive algorithms. The former assume that the test topics are known, and thus focuses the learning on the area of the distribution where these queries are located. This, however, will results in a predictor which is not universally applicable.</p><p>A further conclusion from this table is that although there is some degradation caused by learning a predictor on one collection and using it on a different collection, the predictor is still useful, at least for the short title queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>The results we obtained from this year's experiments demonstrate how prediction of query difficulty can successfully be used. In the Terabyte track the query predictor was used to effectively federate search results -federated search based on query prediction can achieve comparable results to single-index search. In the Robust track, a predictor learned over one collection could predict query difficulty over another collection, with reasonable results, at least for the title-based queries.</p><p>The experiments also provide some negative results. Different partition schemes of the data do not affect precision significantly; random partition achieved comparable results to sophisticated partition schemes. However, our experiments are limited in the sense that they split the data to only few bins -this conclusion should be explored more deeply when the data is partitioned into many more bins.</p><p>In the Robust track, most of the difficult TREC topics obtained high precision when executed against the AQUAINT collection. The difficulty characteristic of those topics was not preserved against AQUAINT. It is difficult to confidently judge the reasons for the improved precision, but it seems to lend support to our claim that query difficulty mostly depends on the collection features rather than on query independent features. Finally, this is the first time that we were able to index and search over collection of (half) terabyte of data in a reasonable time, and to achieve decent results -an important milestone for our search system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,126.00,544.22,360.00,7.89;6,126.00,554.21,183.21,7.86;6,198.00,368.59,215.99,161.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A typical aggregated histogram. Areas of the same color represent cells which map into the same feature for the query predictor.</figDesc><graphic coords="6,198.00,368.59,215.99,161.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,126.00,573.67,360.00,59.92"><head>Table 1 .</head><label>1</label><figDesc>Distributed search results. Average query execution time is measured over the 50,000 queries provided for the Terabyte track's efficiency task.</figDesc><table coords="3,340.07,573.67,75.43,7.86"><row><cell>2004</cell><cell>2005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,126.00,203.76,360.00,104.38"><head>Table 2 .</head><label>2</label><figDesc>Performance of different partitions on the .gov2 collection, compared to the average best and median results of all participants.</figDesc><table coords="7,232.18,203.76,147.64,78.82"><row><cell>Type</cell><cell cols="3">Partition MAP P@10</cell></row><row><cell cols="2">Federated random</cell><cell cols="2">0.275 0.530</cell></row><row><cell>search</cell><cell cols="3">sequential 0.269 0.538</cell></row><row><cell></cell><cell>ttl-stem</cell><cell cols="2">0.264 0.522</cell></row><row><cell cols="2">Distributed DF1</cell><cell cols="2">0.285 0.536</cell></row><row><cell>search</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TREC</cell><cell>Best</cell><cell>0.51</cell><cell>0.9</cell></row><row><cell>results</cell><cell>Median</cell><cell cols="2">0.28 0.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,186.31,360.70,239.39,149.21"><head>Table 3 .</head><label>3</label><figDesc>Query expansion results</figDesc><table coords="8,186.31,360.70,239.39,138.60"><row><cell>Topic part</cell><cell cols="2">Run description GAP MAP P@10</cell></row><row><cell>Title</cell><cell>Web expansion</cell><cell>0.157 0.239 0.496</cell></row><row><cell></cell><cell cols="2">Manual expansion 0.059 0.173 0.340</cell></row><row><cell></cell><cell>No expansion</cell><cell>0.099 0.189 0.376</cell></row><row><cell></cell><cell>TREC best</cell><cell>0.233 0.332 0.592</cell></row><row><cell></cell><cell>TREC median</cell><cell>0.129 0.224 0.434</cell></row><row><cell>Description</cell><cell>Web expansion</cell><cell>0.128 0.230 0.472</cell></row><row><cell></cell><cell>No expansion</cell><cell>0.064 0.148 0.332</cell></row><row><cell></cell><cell>TREC best</cell><cell>0.178 0.289 0.536</cell></row><row><cell></cell><cell>TREC median</cell><cell>0.103 0.184 0.386</cell></row><row><cell cols="2">Title + Description Web expansion</cell><cell>0.141 0.242 0.484</cell></row><row><cell></cell><cell>No expansion</cell><cell>0.070 0.155 0.346</cell></row><row><cell></cell><cell>TREC best</cell><cell>0.234 0.332 0.628</cell></row><row><cell></cell><cell>TREC median</cell><cell>0.126 0.218 0.432</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,175.94,202.32,260.12,50.24"><head>Table 4 .</head><label>4</label><figDesc>TREC-8AQUAINT Topic part R 2 Kendall's-τ Area R 2 Kendall's-τ Area Query prediction results</figDesc><table coords="9,175.94,223.73,259.41,18.22"><row><cell>Title</cell><cell>0.295</cell><cell>0.362</cell><cell>3.638 0.0951</cell><cell>0.262</cell><cell>7.431</cell></row><row><cell cols="2">Description 0.095</cell><cell>0.053</cell><cell>5.676 0.0010</cell><cell>0.030</cell><cell>7.857</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,135.96,659.88,101.50,7.86"><p>http://www.answers.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,129.59,605.24,356.42,7.86;9,138.15,615.20,347.85,7.86;9,138.15,625.16,347.85,7.86;9,138.15,635.12,200.19,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,468.98,605.24,17.03,7.86;9,138.15,615.20,225.23,7.86">Efficient query evaluation using a two-level retrieval process</title>
		<author>
			<persName coords=""><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Herscovici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,382.57,615.20,103.43,7.86;9,138.15,625.16,318.01,7.86">CIKM &apos;03: Proceedings of the twelfth international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,129.59,649.91,356.42,7.86;9,138.15,659.88,347.85,7.86;10,138.15,75.84,347.84,7.86;10,138.15,85.80,193.34,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,389.97,649.91,96.03,7.86;9,138.15,659.88,236.32,7.86">Automatic query refinement using lexical affinities with maximal information gain</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eitan</forename><surname>Farchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yael</forename><surname>Petruschka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aya</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,393.76,659.88,92.24,7.86;10,138.15,75.84,347.84,7.86;10,138.15,85.80,51.00,7.86">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,99.75,356.41,7.86;10,138.15,109.71,347.86,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,316.93,99.75,101.54,7.86">Learning by transduction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.12,99.75,45.88,7.86;10,138.15,109.71,280.51,7.86">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.15,119.67,102.93,7.86" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,133.62,356.42,7.86;10,138.15,143.59,347.84,7.86;10,138.15,153.55,87.23,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,229.15,133.62,174.28,7.86">Making large-scale svm learning practical</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.77,143.59,211.22,7.86;10,138.15,153.55,11.10,7.86">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT-Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,167.50,356.42,7.86;10,138.15,177.46,347.86,7.86;10,138.15,187.42,60.46,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,202.86,167.50,161.45,7.86">An introduction to pattern classification</title>
		<author>
			<persName coords=""><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,262.43,177.46,165.92,7.86">Advanced Lectures on Machine Learning</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3176</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,201.37,356.41,7.86;10,138.15,211.33,347.85,7.86;10,138.15,221.29,347.85,7.86;10,138.15,231.26,347.86,7.86;10,138.15,241.22,21.00,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,379.52,201.37,106.47,7.86;10,138.15,211.33,347.85,7.86;10,138.15,221.29,51.07,7.86">Learning to estimate query difficulty including applications to missing content detection and distributed information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Darlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.63,221.29,277.36,7.86;10,138.15,231.26,213.29,7.86">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,129.59,255.17,356.42,7.86;10,138.15,265.13,347.85,7.86;10,138.15,275.09,209.46,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,444.77,255.17,41.23,7.86;10,138.15,265.13,257.21,7.86">Improving Document Retrieval According to Prediction of Query Difficulty</title>
		<author>
			<persName coords=""><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Einat</forename><surname>Amitay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,414.24,265.13,71.75,7.86;10,138.15,275.09,127.06,7.86">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>TREC2004</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
