<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,75.73,131.20,443.82,18.14;1,109.55,156.11,376.19,18.14">Spam Filtering using Character-level Markov Models: Experiments for the TREC 2005 Spam Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,182.84,195.91,88.77,12.58"><forename type="first">Andrej</forename><surname>Bratko</surname></persName>
							<email>andrej.bratko@ijs.si</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Intelligent Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>informacijske tehnologije, Jozef Stefan Institute Stegne 21c</addrLine>
									<postCode>SI-1000, SI-1000</postCode>
									<settlement>Klika, Ljubljana, Jamova 39, Ljubljana</settlement>
									<country>Slovenia, Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.83,195.91,90.12,12.58"><forename type="first">Bogdan</forename><surname>Filipič</surname></persName>
							<email>bogdan.filipic@ijs.si</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Intelligent Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,75.73,131.20,443.82,18.14;1,109.55,156.11,376.19,18.14">Spam Filtering using Character-level Markov Models: Experiments for the TREC 2005 Spam Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47B9F10767E7B113D9FDA3AEEE7190C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes our participation in the TREC 2005 spam track, in which we consider the use of adaptive statistical data compression models for the spam filtering task. The nature of these models allows them to be employed as Bayesian text classifiers based on character sequences. We experimented with two different compression algorithms under varying model parameters. All four filters that we submitted exhibited strong performance in the official evaluation, indicating that data compression models are well suited to the spam filtering problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Text REtrieval Conference (TREC) is an annual event devised to encourage and support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. In 2005, the set of tracks included in TREC was expanded with the addition of a new track on spam filtering. The goal of the spam track is to provide a standard evaluation of current and proposed spam filtering approaches. To this end, a methodology for filter evaluation and a software toolkit that implements this methodology were developed. As a testbed for the comparison, a collection of evaluation corpora, both public and private, was also compiled by the organizers of the track.</p><p>This paper describes the system submitted to the spam track by the Department of Intelligent Systems at the Jozef Stefan Institute in Slovenia ("Institut Jožef Stefan" -IJS). The primary goal of our participation was to evaluate a character-based approach to spam filtering, as opposed to common word-based methods. Specifically, we consider the use of adaptive statistical data compression models for the spam filtering task. The nature of these models allows them to be employed as Bayesian text classifiers based on character sequences <ref type="bibr" coords="1,355.30,678.65,89.09,9.57" target="#b4">(Frank et al., 2000)</ref>. We compare the filtering performance of the Prediction by Partial Matching (PPM) <ref type="bibr" coords="1,459.66,692.20,35.49,9.57;2,100.13,89.54,86.78,9.57" target="#b1">(Cleary and Witten, 1984)</ref> and Context Tree Weighting (CTW) <ref type="bibr" coords="2,366.67,89.54,102.21,9.57" target="#b10">(Willems et al., 1995)</ref> compression algorithms, and study the effect of varying the order of the compression model.</p><p>Our approach is substantially different from the methods adopted by typical spam filtering systems. In most spam filtering work, text is modeled with the widely used bag-of-words (BOW) representation, even though it is widely accepted that tokenization is a vulnerability of keyword-based spam filters. Some filters, such as the Chung-Kwei system <ref type="bibr" coords="2,228.91,184.39,136.96,9.57" target="#b6">(Rigoutsos and Huynh, 2004)</ref>, use patterns of characters instead of word features. However, their techniques are different to the statistical data compression models that are evaluated here. In particular, while other systems use character-based features in combination with some supervised learning algorithm, compression models were designed from the ground up for the specific purpose of modeling character sequences. Furthermore, the search for useful character patterns is expensive, and does not lend itself well to the incremental learning setting that was used for the TREC evaluation. To our knowledge, our system was the only character-based system that was evaluated in the spam track at TREC 2005.</p><p>The remainder of this paper is structured as follows. Section 2 outlines the approach to spam filtering that was adopted in our system with a brief review of statistical data compression models and the relation to Bayesian text classification. We then describe the filter evaluation procedure that was used for TREC 2005, as well as the four system configurations that were submitted for the official runs by our group (Section 3). In Section 4, we summarize the relevant results from the official evaluation. The major conclusions that can be drawn from the evaluation are presented in Section 5, which also outlines our plans for the future development of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The "IJS" system uses statistical data compression models for email classification. Such models can be used to estimate the probability of an observed sequence of characters. By building one model from the training data of each class, compression models can be employed as Bayesian text classifiers <ref type="bibr" coords="2,374.43,531.39,91.51,9.57" target="#b4">(Frank et al., 2000)</ref>. The classification outcome is determined by the model that deems the sequence of characters contained in the target document most likely. The following subsections describe our methods in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Statistical Data Compression Models</head><p>Probability plays a central role in data compression: Knowing the exact probability distribution governing an information source allows us to construct optimal or nearoptimal codes for messages produced by the source. Statistical data compression algorithms exploit this relationship by building a statistical model of the information source, which can be used to estimate the probability of each possible message that can be emitted by the source. Each message d produced by such a source is typically represented as a sequence s n 1 = s 1 . . . s n ∈ Σ * of symbols over the source alphabet Σ.</p><p>For text generating sources, a symbol is usually interpreted as a single character. To make the inference problem tractable, each symbol in a message d ∈ Σ * is assumed to be independent of all but the preceding k symbols (the symbol's context):</p><formula xml:id="formula_0" coords="3,208.19,140.17,286.96,32.96">P (d) = |d| i=1 P (s i |s i-1 1 ) ≈ |d| i=1 P (s i |s i-1 i-k ) (1)</formula><p>The number of context symbols k is also referred to as the order of the model. To achieve accurate prediction from limited training data, compression algorithms typically blend models of different orders, much like smoothing methods used for modeling natural language (n-gram language models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Prediction by Partial Matching</head><p>The Prediction by Partial Matching (PPM) algorithm <ref type="bibr" coords="3,366.53,271.13,128.62,9.57" target="#b1">(Cleary and Witten, 1984)</ref> uses a special escape mechanism for smoothing. An order-k PPM model works as follows: When predicting the next symbol in a sequence, the longest context found in the training text is used (up to length k). If the target symbol has appeared in this context in the training text, its relative frequency within the context is used as an estimate of the symbol's probability. These probabilities are discounted to reserve some probability mass for the escape mechanism. The accumulated escape probability is effectively distributed among symbols not seen in the current context, according to a lower-order model. The procedure is applied recursively, ultimately terminating in a default model of order -1, which always predicts a uniform distribution among all possible symbols. Many versions of the PPM algorithm exist, differing mainly in the way the escape probability is estimated. In our implementation, we used escape method D <ref type="bibr" coords="3,254.28,433.72,73.95,9.57" target="#b5">(Howard, 1993)</ref> in combination with the exclusion principle <ref type="bibr" coords="3,145.01,447.27,69.40,9.57" target="#b7">(Sayood, 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Tree Weighting</head><p>The Context Tree Weighting (CTW) algorithm <ref type="bibr" coords="3,324.08,501.06,100.64,9.57" target="#b10">(Willems et al., 1995)</ref> uses a mixture of all tree source models up to a certain order to predict the next symbol in a sequence. Each such model can be interpreted as a set of contexts with one set of parameters (i.e. next-symbol probabilities) for each context in the model. These contexts form a tree. The CTW algorithm blends all models that are subtrees of the full order-k context tree (see Figure <ref type="figure" coords="3,303.65,568.81,4.24,9.57">1</ref>). When predicting the next symbol, the models are weighted according to their Bayesian posterior, which is estimated from the training data. Although the mixture includes more than 2 |Σ| k-1 different models, the algorithm's complexity is linear in the length of the training text. This is achieved with a clever decomposition of the posteriors, exploiting the property that many of these models share the same parameters. The CTW algorithm has favorable theoretical properties, i.e. a theoretical upper bound on non-asymptotic redundancy can be proven for this algorithm. However, the algorithm is not widely used in practice, possibly due to its complex implementation, and the observation that it yields only minor gains in compression compared to PPM (or no gains at all). The original CTW algorithm was designed for compression of binary sources.</p><p>We used a version of the algorithm that is adapted for multi-alphabet sources, using a PPM-style escape method for estimating zero-frequency symbols that is described in <ref type="bibr" coords="4,112.86,116.64,103.39,9.57" target="#b9">(Tjalkens et al., 1993)</ref>.</p><formula xml:id="formula_1" coords="4,82.33,144.63,268.56,74.04">0 1 1 0 1 0 1 0 1 p(0|11)</formula><p>Input sequence: 010110 ...</p><formula xml:id="formula_2" coords="4,191.39,143.55,319.95,74.87">0 1 0 1 p(0|1) 0 1 p(0|1) 0 p(0) p(0|11)</formula><p>Figure <ref type="figure" coords="4,107.19,257.01,4.55,10.48">1</ref>: All tree models considered by an order-2 CTW algorithm with a binary alphabet. An example input sequence is given to show the context that each of the models use for prediction of the last character in this sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Using Compression Models for Text Classification</head><p>To classify a target document d, Bayesian classifiers select the class c that is most probable with respect to the document text using Bayes rule:</p><formula xml:id="formula_3" coords="4,225.25,402.85,269.90,36.88">c(d) = arg max c∈C [P (c|d)] (2) = arg max c∈C [P (c)P (d|c)]<label>(3)</label></formula><p>A statistical compression model M c , built from the training data for class c, can be used to estimate the conditional probability of the document given the class P (d|c):</p><formula xml:id="formula_4" coords="4,204.46,505.56,290.68,17.65">c(d) = arg max M ∈{M ham ,Mspam} [P (c)P M (d)]<label>(4)</label></formula><p>For the binary spam filtering problem, compression models M spam and M ham are trained from examples of spam and legitimate email. In our implementation, we ignored the class priors, which is equivalent to setting P (c) in Equation 4 to 0.5 for both spam and ham.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adapting the Model During Classification</head><p>Each sequence of characters in an email message (such as, for example, a word), can be considered a piece of evidence on which the final classification is based.</p><p>The simplest such sequence is a single character, which is always evaluated in the context of its preceding k characters. Each sequence favors classification into one particular class, albeit to a varying degree. While the first occurrence of a sequence undoubtedly provides fresh evidence to be used for classification, repetitions of a sequence are to some extent redundant, since they convey little new information.</p><p>However, when evaluating the (conditional) probability of a document according to Equation 1, repeated occurrences of the same sequence contribute an equal weight on the classification outcome.</p><p>To discount the influence of repeated subsequences, we chose to allow the model to adapt during the evaluation of the target document, as would typically be the case in adaptive data compression. To this end, the statistical counters contained in model M are incremented after the evaluation of each character when estimating the conditional document probability P M (d) according to Equation 1. These statistics are removed from both models M ham and M spam after classification. Updating the models at each character improved performance considerably in our preliminary experiments, as well as in follow-up experiments on the public corpus compiled for TREC. A further theoretical analysis of this modification and empirical evaluation of its effect on performance are given in <ref type="bibr" coords="5,293.61,252.13,119.82,9.57" target="#b0">Bratko and Filipič (2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Calculating the Spamminess Score</head><p>Probability estimates for P (d|spam) and P (d|ham) are computed as a product over all character occurrences in the document text (Equation <ref type="formula" coords="5,401.64,319.48,4.24,9.57">1</ref>). With increasing document length, these estimates converge towards zero. To convert probability estimates into "spamminess scores" that are comparable across documents, the following transformation was used:</p><formula xml:id="formula_5" coords="5,183.27,382.65,227.04,26.71">spam score(d) = p(d|spam) 1/|d| p(d|spam) 1/|d| + p(d|ham) 1/|d|</formula><p>(5)</p><p>In the above equation, |d| denotes the length of the document text, i.e. the total number of all character occurrences. The spamminess score can be loosely interpreted as the geometric mean of the probability that the target document is spam, calculated over all character occurrences. Note that this transformation does not affect the classification outcome for any target document, but helps produce scores that are relatively independent of document length. This is crucial when thresholding is used to reach a desirable tradeoff between ham misclassification and spam misclassification, which is also the basis of the Receiver Operating Characteristic (ROC) curve analysis that was the primary measure of classifier performance for TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Spam Filtering Task at TREC 2005</head><p>For the spam filtering task at TREC, a number of email corpora containing legitimate (ham) and spam email were constructed automatically or labeled by hand. To make the evaluation realistic, messages were ordered by time of arrival. Evaluation is performed by presenting each message in sequence to the classifier, which has to label the message as spam or ham (i.e. legitimate mail), as well as provide some measure of confidence in the prediction -the "spamminess score". After each classification, the filter is informed of the true label of the message, which allows the filter to update its model accordingly. This behavior simulates a typical setting in personal email filtering, which is usually based on user feedback, with the additional assumption that the user promptly corrects the classifier after every misclassification. Each participating group was allowed to submit up to four different filters for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Filters Submitted by the IJS Group</head><p>The filters that were submitted by our group at the Jozef Stefan Institute are listed in Table <ref type="table" coords="6,144.66,197.53,4.24,9.57" target="#tab_0">1</ref>. Three of the filters were based the PPM compression algorithm, the fourth was based on the CTW algorithm, since, in our initial tests, it was found that the PPM algorithm generally outperforms CTW. The difference between the three PPM-based filters was in a different order of the model, since we primarily wanted to study the effect of varying the PPM model order. All of the four submitted filters adaptive the classification models during evaluation of the target document, as described in Section 2.4. This decision was based on the results of initial experiments, in which this approach generally outperformed a static model. In hindsight, we probably should have omitted the CTW-based method altogether and submitted at least one filter in which the model would be kept static during classification. We would very much wish to see the difference in performance confirmed in the results of experiments on the private corpora. Our system was designed as a client-server application. The server maintains the compression models required for classification in main memory and updates the models incrementally with each new training example. The system performs very limited message preprocessing, which primarily includes mime-decoding and stripping messages of all non-textual message parts. All sequences of whitespace characters (tabs, spaces and newline characters) were replaced by a single space. The reason for this step was to undo line breaks added to messages by email clients. The newline characters that delimit header fields and separate the header from the body part were preserved. No other preprocessing was done.</p><p>Since the data collections used for TREC were large, we limited memory usage to around 500MB. When this limit was reached, the server discarded one half of the training data and rebuilt the classification models from the remaining examples. In general, the newest examples were kept for the pruned model, but measures were also taken to keep the dataset balanced. Specifically, if the number of all training messages was N when the memory limit was hit, the server would discard all but the newest N/4 spam and N/4 ham messages.</p><p>In this section, we summarize the results of the TREC evaluation that are most relevant to our system. In particular, we compare the performance of different compression models and evaluate the effect of varying the order of the compression model. Finally, we compare the performance of our system to the results achieved by other participants. The full official results of the evaluation are reported in the spam track results section of the TREC 2005 proceedings<ref type="foot" coords="7,374.77,179.69,4.23,6.99" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Four email datasets were used for the official TREC evaluation. The TREC 2005 public dataset<ref type="foot" coords="7,167.99,247.03,4.23,6.99" target="#foot_1">2</ref> (t05p) contains almost 100,000 messages received by employees of the Enron corporation. The dataset was prepared specifically for the evaluation in the TREC 2005 spam track. The messages were labeled semi-automatically with an iterative procedure described by <ref type="bibr" coords="7,272.65,289.63,132.83,9.57" target="#b3">Cormack and Lynam (2005)</ref>. The mrx, sb and tm datasets contain hand-labeled personal email. As these datasets are not publicly available for privacy reasons, the evaluation of filters was done by the organizers of the TREC 2005 spam track. The basic statistics for all four datasets are given in Table <ref type="table" coords="7,130.13,343.83,4.24,9.57" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Criteria</head><p>A number of evaluation criteria proposed by <ref type="bibr" coords="7,312.77,524.06,133.14,9.57" target="#b2">Cormack and Lynam (2004)</ref> were used for the official evaluation. In this paper, we only report on a selection of these measures:</p><p>• HMR: Ham Misclassification Rate, the fraction of ham messages labeled as spam.</p><p>• SMR: Spam Misclassification Rate, the fraction of spam messages labeled as ham.</p><p>• 1-ROCA: Area above the Receiver Operating Characteristic (ROC) curve.</p><p>We believe the 1-ROCA statistic is the most comprehensive measure, since it captures the behavior of the filter at different filtering thresholds. Increasing the filtering threshold will reduce the HMR at the expense of increasing the SMR, and vice versa. The desired tradeoff will generally depend on the user and the type of mail they receive. The HMR and SMR statistics are interesting for practical purposes, since they are easy to interpret and give a good idea of the kind of performance one can expect from a spam filter. They are, however, less suitable for comparing filters, since one of these measures can always be improved at the expense of the other. For this reason, we focus on the 1-ROCA statistic when comparing different filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Comparison Between Compression Algorithms</head><p>A comparison between the performance of the PPM and CTW algorithms is given in Figure <ref type="figure" coords="8,149.36,251.12,4.24,9.57" target="#fig_0">2</ref>. In all comparisons, an order-8 compression model is used, since we only submitted an order-8 CTW-based filter. The reason for this is that the CTW algorithm weights models of different orders according to their utility, so we expect performance to improve as the order of the model is increased.  Although the performance of the algorithms is similar, the PPM algorithm usually outperforms CTW in the 1-ROCA statistic, with the exception of the mrx dataset. We observed a similar pattern in our preliminary experiments <ref type="bibr" coords="8,437.54,531.75,57.61,9.57;8,100.13,545.30,60.41,9.57" target="#b0">(Bratko and Filipič, 2005)</ref>. The CTW algorithm is more complex and slower than PPM, without yielding any noticeable improvement in PPM's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Varying the Model Order</head><p>The effect of varying the order of the PPM compression model on classification performance is given in Figure <ref type="figure" coords="8,250.69,625.59,4.24,9.57" target="#fig_1">3</ref>. No particular model is uniformly best, however, it can be seen that using an order-6 model usually produces good results. In data compression, it is known that increasing the order of PPM to beyond 5 characters or so usually results in a gradual decrease of compression performance <ref type="bibr" coords="8,453.03,666.23,42.12,9.57;8,100.13,679.78,24.25,9.57" target="#b8">(Teahan, 2000)</ref>, so the results are not unexpected. Since the difference in performance is much greater between different datasets than the order of the PPM model, we may conclude that the filter is rather robust to the choice of this parameter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison to Other Filters</head><p>A summary of the results achieved with our order-6 PPM model (ijsSPAM2pm6) on the official datasets is listed in the first row of Table <ref type="table" coords="9,371.93,316.82,4.24,9.57" target="#tab_4">3</ref>. Overall, ijsSPAM2pm6 was our best performing filter configuration. The results achieved by the best performing filters of six other groups are also listed for comparison. The IJS system outperformed other systems on most of the evaluation corpora in the 1-ROCA criterion. However, the tradeoff between ham misclassification and spam misclassification varies considerably for different datasets. A comparison with the statistics in Table 2 reveals that our system disproportionately favored classification into the class that contains more training examples. Our system did not vary the filtering threshold dynamically, but rather kept it fixed at a spamminess score above 0.5. The results in Table <ref type="table" coords="9,204.94,438.76,5.45,9.57" target="#tab_4">3</ref> indicate that adjusting the threshold with respect to the number of ham and spam training examples or, alternatively, with respect to previous performance statistics, would be beneficial. This modification would have no effect on the ROC curve analysis. ROC curves for the same set of filters on the aggregate result set of per-message classification outcomes on all four datasets are plotted in Figure <ref type="figure" coords="9,428.18,693.33,4.24,9.57" target="#fig_2">4</ref>. The ROC learning curves on the same aggregate results for the seven best-performing groups is depicted in Figure <ref type="figure" coords="10,208.68,89.54,4.24,9.57" target="#fig_2">4</ref>. The graph shows how the area above the ROC curve changes with an increasing number of examples. Note that the 1-ROCA statistic is plotted in log-scale. All filters learn fast at the start, but performance levels off at around 50,000 messages. This behavior may be attributed to the implicit bias in the learning algorithms, data or concept drift, or even errors in the gold stand. It remains to be seen whether a more refined model pruning strategy would help the filter to continue improving beyond this mark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our system performed beyond our expectations in the TREC evaluation, especially since some competing systems were large open source and commercial (IBM) solutions with a long development history. The TREC results confirmed our intuition that compression models offer a number of advantages over word-based spam filtering methods. By using character-based models, tokenization, stemming and other tedious and error-prone preprocessing steps that are often exploited by spammers are omitted altogether. Also, characteristic sequences of punctuation and other special characters, which are generally thought to be useful in spam filtering, are naturally included in the model.</p><p>We find that Prediction by Partial Matching usually outperforms the Context Tree Weighting algorithm, although the margin is typically small. Somewhat surprisingly, varying the order of the PPM compression model does not have a major effect on performance. An order-6 PPM model performed best in most experiments. This filter configuration was also the best ranking filter overall in the 1-ROCA statistic for most of the datasets in the official evaluation.</p><p>Despite the encouraging results at TREC, we believe there is much room for improvement in our system. In future work, we intend to adapt the system to use a disk-based a model, rather than keeping the data structures in main memory. This would enable us to overcome the need for pruning the model, which was typically required at around 15,000 examples. We also wish to employ a suitable mechanism for dynamically adapting the filtering threshold. Finally, an interesting avenue for future research would be to devise a strategy which would automatically determine the order of the PPM model that optimizes classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,124.40,478.81,346.49,10.48"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of CTW and PPM algorithms for spam filtering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,134.59,248.51,326.10,10.48"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of order-4, order-6 and order-8 PPM models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,70.87,366.96,453.54,10.48;10,70.87,381.22,222.46,9.57"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ROC curves (left) and learning curves (right) for seven best performing filters from different groups on the aggregated raw results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,105.77,380.19,383.74,88.15"><head>Table 1 :</head><label>1</label><figDesc>Filters submitted by IJS for evaluation in the TREC 2005 spam track.</figDesc><table coords="6,190.63,399.67,212.73,68.67"><row><cell>Label</cell><cell cols="2">Comp. algorithm Order</cell></row><row><cell>ijsSPAM1pm8</cell><cell>PPM</cell><cell>8</cell></row><row><cell>ijsSPAM2pm6</cell><cell>PPM</cell><cell>6</cell></row><row><cell>ijsSPAM3pm4</cell><cell>PPM</cell><cell>4</cell></row><row><cell>ijsSPAM4cw8</cell><cell>CTW</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,172.15,375.33,250.99,86.03"><head>Table 2 :</head><label>2</label><figDesc>Basic statistics for the evaluation datasets.</figDesc><table coords="7,203.62,392.69,188.04,68.67"><row><cell cols="3">Dataset Messages Ham</cell><cell>Spam</cell></row><row><cell>t05p</cell><cell>92189</cell><cell cols="2">39399 52790</cell></row><row><cell>mrx</cell><cell>49086</cell><cell cols="2">9038 40048</cell></row><row><cell>sb</cell><cell>7006</cell><cell>6231</cell><cell>775</cell></row><row><cell>tm</cell><cell>170201</cell><cell cols="2">150685 19516</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,155.22,86.11,274.81,146.54"><head></head><label></label><figDesc>Comparison of order-4, order-6 and order-8 PPM models. The best results in the 1-AUC statistic are in bold.</figDesc><table coords="9,155.22,86.48,274.81,146.17"><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell>Order</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Dataset SpamAssassin 0.4</cell><cell>4 0.188</cell><cell>6 0.148</cell><cell>8 0.132</cell><cell></cell></row><row><cell>t05p</cell><cell>0.35</cell><cell>0.022</cell><cell>0.019</cell><cell>0.021</cell><cell></cell></row><row><cell>mrx sb tm 1-ROCA (%)</cell><cell>0.2 0.25 0.3</cell><cell>0.05 0.475 0.181</cell><cell>0.069 0.285 0.135</cell><cell>0.069 0.372 0.155</cell><cell>t05p mrx sb tm</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell></cell><cell>6</cell><cell>8</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PPM model order</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,84.54,512.27,426.19,147.15"><head>Table 3 :</head><label>3</label><figDesc>Comparison of seven best performing filters from different groups.</figDesc><table coords="9,84.54,533.12,426.19,126.31"><row><cell></cell><cell></cell><cell>t05p</cell><cell></cell><cell></cell><cell>mrx</cell><cell></cell><cell></cell><cell>sb</cell><cell></cell><cell></cell><cell>tm</cell><cell></cell></row><row><cell>Filter</cell><cell>1-roca</cell><cell>hm%</cell><cell>sm%</cell><cell cols="3">1-roca hm% sm%</cell><cell cols="2">1-roca hm%</cell><cell>sm%</cell><cell>1-roca</cell><cell>hm%</cell><cell>sm%</cell></row><row><cell>ijsSPAM2</cell><cell>0.019</cell><cell>0.23</cell><cell>0.95</cell><cell>0.069</cell><cell>1.52</cell><cell>0.34</cell><cell>0.285</cell><cell>0.16</cell><cell>11.74</cell><cell>0.135</cell><cell>0.36</cell><cell>3.43</cell></row><row><cell>lbSPAM2</cell><cell>0.037</cell><cell>0.51</cell><cell>0.93</cell><cell>0.083</cell><cell>1.63</cell><cell>0.23</cell><cell>0.835</cell><cell>0.03</cell><cell>33.16</cell><cell>0.411</cell><cell>0.29</cell><cell>11.63</cell></row><row><cell>621SPAM1</cell><cell>0.044</cell><cell>2.38</cell><cell>0.20</cell><cell>2.616</cell><cell>2.31</cell><cell>2.77</cell><cell>2.389</cell><cell>1.57</cell><cell>5.29</cell><cell>0.161</cell><cell>2.17</cell><cell>0.74</cell></row><row><cell>crmSPAM2</cell><cell>0.122</cell><cell>0.62</cell><cell>0.87</cell><cell>0.051</cell><cell>1.50</cell><cell>0.24</cell><cell>1.888</cell><cell>0.30</cell><cell>13.55</cell><cell>0.166</cell><cell>0.21</cell><cell>2.91</cell></row><row><cell>tamSPAM1</cell><cell>0.164</cell><cell>0.26</cell><cell>4.10</cell><cell>0.138</cell><cell>0.28</cell><cell>2.55</cell><cell>1.892</cell><cell>0.14</cell><cell>27.48</cell><cell>0.294</cell><cell>0.25</cell><cell>8.25</cell></row><row><cell>yorSPAM2</cell><cell>0.457</cell><cell>0.92</cell><cell>1.74</cell><cell>0.051</cell><cell>0.34</cell><cell>1.03</cell><cell>0.983</cell><cell>0.14</cell><cell>23.64</cell><cell>0.619</cell><cell>0.25</cell><cell>14.90</cell></row><row><cell>kidSPAM1</cell><cell>1.463</cell><cell>0.91</cell><cell>9.40</cell><cell>1.274</cell><cell>4.02</cell><cell>9.10</cell><cell>3.553</cell><cell>3.37</cell><cell>13.57</cell><cell>0.530</cell><cell>0.65</cell><cell>5.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,88.80,694.51,156.91,8.30"><p>http://trec.nist.gov/pubs.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,88.80,706.47,156.91,8.30"><p>http://trec.nist.gov/data.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,100.13,192.90,395.01,9.57;11,111.84,206.45,383.31,9.57;11,111.84,220.00,94.39,9.57" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,254.61,192.90,187.46,9.57">Spam filtering using compression models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipič</surname></persName>
		</author>
		<idno>IJS-DP-9227</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Jožef Stefan Institute</publisher>
			<pubPlace>Ljubljana, Slovenia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Intelligent Systems</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,100.13,243.51,395.01,9.57;11,111.84,257.06,383.31,9.57;11,111.84,270.61,74.45,9.57" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,314.86,243.51,180.28,9.57;11,111.84,257.06,147.72,9.57">Data compression using adaptive coding and partial string matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,269.91,257.06,225.24,9.57">IEEE Transactions on Communications COM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="1984-04">1984. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,294.12,395.01,9.57;11,111.84,307.67,383.31,9.57;11,111.84,321.22,111.34,9.57" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,276.24,294.12,218.91,9.57;11,111.84,307.67,156.11,9.57">A study of supervised spam detection applied to eight months of personal email</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,100.13,344.73,395.02,9.57;11,111.84,358.28,383.31,9.57;11,111.84,371.83,71.37,9.57" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,276.40,344.73,141.07,9.57">Spam corpus creation for trec</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,440.77,344.73,54.38,9.57;11,111.84,358.28,279.72,9.57">Proceedings of Second Conference on Email and Anti-Spam CEAS 2005</title>
		<meeting>Second Conference on Email and Anti-Spam CEAS 2005<address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,395.34,395.01,9.57;11,111.84,408.89,383.31,9.57;11,111.84,422.44,373.82,9.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,326.02,395.34,169.12,9.57;11,111.84,408.89,54.53,9.57">Text categorization using compression models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,194.30,408.89,295.79,9.57">Proceedings of DCC-00, IEEE Data Compression Conference</title>
		<meeting>DCC-00, IEEE Data Compression Conference<address><addrLine>Snowbird, US; Los Alamitos, US</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,445.95,395.01,9.57;11,111.84,459.50,329.00,9.57" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,191.80,445.95,303.35,9.57;11,111.84,459.50,35.83,9.57">The Design and Analysis of Efficient Lossless Data Compression Systems</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Providence, Rhode Island</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct coords="11,100.13,483.01,395.01,9.57;11,111.84,496.56,383.31,9.57;11,111.84,510.11,294.98,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,306.39,483.01,188.76,9.57;11,111.84,496.56,363.72,9.57">Chung-kwei: A pattern-discovery-based system for the automatic identification of unsolicited e-mail messages (spam)</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rigoutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,111.84,510.11,289.63,9.57">Proceedings of the First Conference on Email and Anti-Spam</title>
		<meeting>the First Conference on Email and Anti-Spam</meeting>
		<imprint>
			<date type="published" when="2004-07">2004, July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,533.62,323.31,9.57" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,194.36,533.62,160.69,9.57">Introduction to Data Compression</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,557.14,395.01,9.57;11,111.84,570.68,383.30,9.57;11,111.84,584.23,244.25,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,212.06,557.14,283.09,9.57;11,111.84,570.68,33.44,9.57">Text classification and segmentation using minimum crossentropy</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Teahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,174.90,570.68,262.13,9.57">Proceeding of RIAO-00, 6th International Conference</title>
		<meeting>eeding of RIAO-00, 6th International Conference<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>FR</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,607.75,395.01,9.57;11,111.84,621.29,383.31,9.57;11,111.84,634.84,105.00,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,393.14,607.75,102.01,9.57;11,111.84,621.29,129.84,9.57">Context tree weighting: Multi-alphabet sources</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tjalkens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shtarkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M J</forename><surname>Willems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,266.79,621.29,228.35,9.57;11,111.84,634.84,36.20,9.57">Proceedings of the 14th Symp. on Info. Theory, Benelux</title>
		<meeting>the 14th Symp. on Info. Theory, Benelux</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,100.13,658.36,395.00,9.57;11,111.84,671.91,380.00,9.57" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,414.78,658.36,80.36,9.57;11,111.84,671.91,166.93,9.57">The context-tree weighting method: Basic properties</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M J</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">M</forename><surname>Shtarkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Tjalkens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,288.22,671.91,124.57,9.57">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="664" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
