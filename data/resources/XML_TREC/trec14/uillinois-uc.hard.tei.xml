<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.00,155.88,360.03,15.49;1,132.00,177.84,330.26,15.49">Interactive Construction of Query Language Models -UIUC TREC 2005 HARD Track Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.96,210.23,37.88,10.76"><forename type="first">Bin</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.55,210.23,75.76,10.76"><forename type="first">Atulya</forename><surname>Velivelli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.83,210.23,44.71,10.76"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.27,210.23,85.21,10.76"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.00,155.88,360.03,15.49;1,132.00,177.84,330.26,15.49">Interactive Construction of Query Language Models -UIUC TREC 2005 HARD Track Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B32B5E6D3A56079EAB2203215A98CCAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the language modeling approach, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents <ref type="bibr" coords="1,72.00,398.08,10.78,8.97" target="#b2">[3,</ref><ref type="bibr" coords="1,86.38,398.08,7.27,8.97" target="#b0">1]</ref>. This is in line with the traditional way of doing relevance feedback -presenting the user with documents or passages for relevance judgment and then extracting terms from the judged documents or passages to improve a query model. Such an indirect way of obtaining help from a user to construct a query model has the drawback that irrelevant terms that occur with relevant ones in the judged content may be erroneously used for query model modification.</p><p>A more direct way to involve a user in improving the query model is to present some candidate terms to a user and directly ask the user to judge the relevance of each term or specify the probability of each term. This strategy has been discussed in <ref type="bibr" coords="1,183.34,556.48,10.60,8.97" target="#b1">[2]</ref>, but has not been seriously studied in any existing work. In participating in the TREC 2005 HARD Track task, we explored how to exploit term-based feedback to better involve a user in constructing an improved query model for information retrieval with language models.</p><p>There are several research problems related to the "term feedback" approach in general. First, since the user is only likely to judge the relevance of a modest number of terms, it is important to study which terms to present to the user for judgment. If the query is ambiguous, i.e., there are multiple clusters of the retrieved documents and the user is only interested in one of them, it is necessary to diversify the presented terms so that each cluster gets adequate representation. It would be unwise to devote the presentation completely to terms from a dominant cluster, because if the user happens to be searching for documents in a small cluster, then he/she gets no chance to indicate his/her interests as the small cluster is under-represented.</p><p>Second, we must design a weighting scheme to designate how good a term is for relevance feedback. It is usually not sufficient to only assign weights to terms presented for judgment, since the number of these terms are not many. If an unpresented term is similar to a presented term, then there is good reason to propagate weight of the presented term to the unpresented one. Also, it is tricky how to treat negative term feedback. A presented term may be left unselected because it is irrelevant, or because the user does not have enough knowledge to determine its relevance.</p><p>Third, a serious drawback of term feedback is that, unlike document or passage feedback, the presented terms have almost no context, which makes it hard for a user to judge their relevance. If a term is only meaningful when combined with other terms or if the term is part of a name or terminology which the user has not heard of, then it is very likely to be missed by the user.</p><p>In our exploration, we address the first two problems of term relevance feedback. Specifically, we use clustering to nominate representative terms and construct the query language model based on both judged terms and their attached clusters. Experiment results show that our approach is effective for involving a user to interactively construct an accurate query language model based on term judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Method</head><p>We use the KL-divergence retrieve method, in which the relevance score of a document with respect to a query is given by the KL-divergence value between the document language model θ D and the query language model θ Q . If a feedback language model θ F can be estimated from either pseudo-feedback or relevance feedback, a modified query model</p><formula xml:id="formula_0" coords="2,72.00,223.53,220.16,22.21">θ Q = λθ Q + (1 -λ)θ F can be used in place of θ Q [3]</formula><p>. We use Lemur's implementation of the KL-divergence retrieval method with Dirichlet prior smoothing (parameter set to 2000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods for Query Model Construction</head><p>For each query topic, the collection of 50 top-ranked result documents returned by the KL-divergence retrieval method is taken as the topic context from which to choose feedback terms. We use the mixture multinomial distribution model method proposed in <ref type="bibr" coords="2,268.57,386.44,11.63,8.97" target="#b3">[4]</ref> to discover K theme clusters within this small queryspecific collection. Each cluster represents a possible latent theme in the collection and is simply a multinomial word distribution (also called unigram language model).</p><p>We select L distinct terms with highest probabilities from each cluster to form a pool of K × L terms. If a term is suggested by different clusters, then it is assigned to the one in which it has highest probability. Terms that occur in the title of the topic are filtered. We then present these terms in a clarification form to the user for evaluation of their relevance to the topic.</p><p>Before discussing the details our methods, we introduce some notations:</p><p>• θ Q : The query language model as mentioned in the previous section.</p><p>• θ F : The feedback language model as mentioned in the previous section.</p><formula xml:id="formula_1" coords="2,81.96,645.21,77.25,10.33">• θ i (i = 1 . . . K):</formula><p>The unigram language model of cluster i.</p><p>• p(w|θ i ): Probability of term w in θ i .</p><p>• p(θ i ): Prior probability that a document is sampled from θ i .</p><p>• T = {t i,j } (i = 1 . . . K, j = 1 . . . L) The set of terms presented to the user in the clarification form. t i,j is the j-th term chosen from cluster i.</p><p>• δ w : Indicator variable which is 1 if term w is in T and judged relevant by the user and 0 otherwise.</p><p>Given the evaluation results δ i,j , the task is to compute a feedback language model θ F which reflects the user's judgment that all terms for which δ i,j = 1 are relevant to the topic. Hopefully, the feedback model θ F (possibly interpolated with the query model θ Q ) will improve the retrieval accuracy.</p><p>We now describe several different methods for computing θ F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">BLPFB (Baseline Pseudo-Feedback):</head><p>This method does not use relevance judgments.</p><formula xml:id="formula_2" coords="2,358.20,348.45,127.92,20.89">p(w|θ F ) = i=1...K p(w|θ i )p(θ i )</formula><p>2. TFB (Term Feedback): We give non-zero weights to those terms that are judged relevant by the user. We interpolate it with the original query model and set Dirichlet prior to be proportional to query length |Q| so that longer queries receive more weight. Cluster information (i.e., which cluster a judged term comes from) is not used.</p><formula xml:id="formula_3" coords="2,342.96,476.85,158.49,25.45">p(w|θ F ) = δ w + µp(w|θ Q ) w ∈T δ w + µ , µ = k|Q|</formula><p>For this method we do not need to interpolate θ F with θ Q , as θ Q is already used in the computation of θ F .</p><p>3. CFB (Cluster Feedback): Cluster language models are interpolated with weights determined by the number of presented terms that are judged relevant in each cluster. We do not distinguish which terms are judged relevant in a cluster; only the count matters.</p><p>p(w|θ F ) = As was in the case of TFB, there is no need to interpolate θ F with θ Q .</p><p>This method is proposed as we believe TFB and CFB have strengths and drawbacks in different aspects. TFB assigns weights to the presented terms but completely ignores unpresented terms. CFB remedies this by treating terms in a cluster equally, such that unpresented terms receive weights when presented terms in that cluster are selected, but it does not differentiate which terms in the cluster are selected. By combining the two methods we believe we can get the greatest benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Clarification Forms</head><p>As described in the previous section, each clarification form (CF) contains terms from different clusters for relevance feedback. The evaluators were asked to select all terms that she/he deemed relevant to the topic but did not know that the terms form clusters. We generated three sets of clarification forms: 1 × 48, 3 × 16 and 6 × 8. The 1 × 48 CFs contain 48 terms from a single large cluster. The 3 × 16 CFs have 3 clusters and 16 terms from each. The 6 × 8 CFs have 6 clusters and 8 terms from each. Figure <ref type="figure" coords="3,174.73,666.28,5.03,8.97" target="#fig_0">1</ref> shows an example of filled 3 × 16 clarification form. Table <ref type="table" coords="3,207.45,678.28,5.03,8.97" target="#tab_0">1</ref> shows some statistics about the number of selected terms in each set of clarification forms. The trend that the average number of selected terms increases with the number of clusters seems to support our hypothesis that a larger number of theme clusters tend to diversify the presented terms so that the user gets a better chance to find terms that match the topic. However, when we evaluated the CFs generated for the HARD 2004 topics, the average numbers of selected terms were 18.4 for the 1 × 48 CF and 16.8 for the 6 × 8 CF, which seemed to contradict the hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Results</head><p>Our baseline run (BLNFB) uses the KL-divergence language model based retrieval method with pseudo feedback (feedback documents = 5). For other runs we use the parameters that worked best when we evaluated the HARD 2004 topics. More specifically, we set µ to 4|Q| in TFB, ν to 0.000001 in CFB and λ to 0.8 in TCFB.</p><p>The statistics of our runs are displayed in Table <ref type="table" coords="3,495.27,690.88,3.77,8.97" target="#tab_1">2</ref>. The runs are tagged with the method and the number of clusters used. For example, TCFB6C denotes the run with the TCFB method and 6 × 8 CFs. We find that the 3 × 16 runs yield the best performance, although the difference between the runs using different CFs is small. <ref type="foot" coords="4,180.24,581.20,3.48,6.28" target="#foot_0">1</ref> This is again contrary to our hope that a larger cluster size (6 × 8) would facilitate better term feedback by suggesting more diversified terms. A close look of the results, however, reveals that the 3 × 16 and 6 × 8 runs excel at different topics, which means neither of them performs better than the other consistently. For example, the average precision of TCFB3C is higher than TCFB6C by more than 0.2 for as many as 13 topics, while the average precision of For TFB3C, we vary the Dirichlet prior µ = k|Q| with k from 0 to 10. The results are shown in Table <ref type="table" coords="4,514.57,447.64,3.77,8.97">3</ref>. The best retrieval performance is achieved when k = 5.</p><p>We also vary the interpolation factor λ in the TCFB3C calculation, the result of which is given in Table 4. The best value for λ is 0.4, which differs from the one we got on the HARD 2004 data.</p><p>The performance of TFB and CFB is comparable, and combining them with TCFB gives best performance for all three CFs. We look at how their performance differs across topics. Table <ref type="table" coords="4,399.37,573.88,5.03,8.97" target="#tab_3">5</ref> sums up the number of topics on which one methods is significantly better and worse than another method:</p><p>We find that TFB is more stable than CFB when compared to baseline, as it performs worse than the baseline for only 2 topics rather than 8 in the case of CFB. TFB and CFB perform better than each other in 11 topics respectively, which confirms our hypothesis that they have relative strengths. When combining the two methods with TCFB, we are able to get the best results by reducing the number of topics in which TCFB performs worse to any of them to 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this study we proposed several methods for utilizing term relevance feedback information to construct an improved query language model. We compare these methods and show how they can be combined to achieve best retrieval performance. There is still much to be explored in this direction of research. We should study better ways to incorporate both term and cluster feedback information. We should make best use of both positive and negative term feedback. It is also important to study how to present the terms in context to facilitate the user's relevance judgment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,202.92,309.40,188.45,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Filled clarification form for topic 658</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,80.43,136.24,422.81,145.48"><head></head><label></label><figDesc></figDesc><graphic coords="3,80.43,136.24,422.81,145.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,302.16,367.96,220.09,67.89"><head>Table 1 :</head><label>1</label><figDesc>Number of feedback terms in clarification</figDesc><table coords="3,302.16,379.96,168.38,55.89"><row><cell>forms</cell><cell></cell><cell></cell></row><row><cell>CF</cell><cell cols="2">Average Std. Dev.</cell></row><row><cell>1×48</cell><cell>11.8</cell><cell>7.40</cell></row><row><cell>3×16</cell><cell>14.0</cell><cell>9.68</cell></row><row><cell>6×8</cell><cell>15.6</cell><cell>9.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,157.96,220.08,373.41"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="4,72.00,157.96,220.08,373.41"><row><cell cols="3">: Mean average precision (MAP) and precision</cell></row><row><cell cols="3">at 10 (Prec@10) for different runs. The ones marked by</cell></row><row><cell cols="2">* are submitted official runs.</cell><cell></cell></row><row><cell>Run</cell><cell cols="2">MAP Prec@10</cell></row><row><cell cols="2">BLNFB* 0.2244</cell><cell>0.460</cell></row><row><cell>TFB1C*</cell><cell>0.2890</cell><cell>0.546</cell></row><row><cell>TFB3C*</cell><cell>0.2929</cell><cell>0.556</cell></row><row><cell>TFB6C*</cell><cell>0.2816</cell><cell>0.542</cell></row><row><cell>CFB1C</cell><cell>0.2391</cell><cell>0.446</cell></row><row><cell>CFB3C</cell><cell>0.2966</cell><cell>0.548</cell></row><row><cell>CFB6C</cell><cell>0.2859</cell><cell>0.514</cell></row><row><cell cols="2">TCFB1C* 0.2898</cell><cell>0.540</cell></row><row><cell cols="2">TCFB3C* 0.3018</cell><cell>0.568</cell></row><row><cell cols="2">TCFB6C* 0.2917</cell><cell>0.530</cell></row><row><cell cols="3">Table 3: Performance of TFB3C under different settings</cell></row><row><cell>of µ = k|Q|.</cell><cell></cell><cell></cell></row><row><cell>k</cell><cell>MAP</cell><cell></cell></row><row><cell cols="2">0 0.2411</cell><cell></cell></row><row><cell cols="2">1 0.2687</cell><cell></cell></row><row><cell cols="2">2 0.2823</cell><cell></cell></row><row><cell cols="2">3 0.2890</cell><cell></cell></row><row><cell cols="2">4 0.2929</cell><cell></cell></row><row><cell cols="2">5 0.2954</cell><cell></cell></row><row><cell cols="2">6 0.2951</cell><cell></cell></row><row><cell cols="2">7 0.2944</cell><cell></cell></row><row><cell cols="2">8 0.2923</cell><cell></cell></row><row><cell cols="2">9 0.2914</cell><cell></cell></row><row><cell cols="2">10 0.2894</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,302.16,135.81,220.11,287.67"><head>Table 4 :</head><label>4</label><figDesc>Performance of TCFB3C with µ = 5|Q| and different settings of λ. When λ = 0, it equals TFB3C.</figDesc><table coords="4,302.16,159.69,220.08,263.79"><row><cell cols="2">When λ = 1, it equals CFB3C.</cell></row><row><cell>λ</cell><cell>MAP</cell></row><row><cell>0</cell><cell>0.2954</cell></row><row><cell cols="2">0.1 0.3036</cell></row><row><cell cols="2">0.2 0.3081</cell></row><row><cell cols="2">0.3 0.3108</cell></row><row><cell cols="2">0.4 0.3116</cell></row><row><cell cols="2">0.5 0.3108</cell></row><row><cell cols="2">0.6 0.3093</cell></row><row><cell cols="2">0.7 0.3065</cell></row><row><cell cols="2">0.8 0.3030</cell></row><row><cell cols="2">0.9 0.2985</cell></row><row><cell>1</cell><cell>0.2966</cell></row><row><cell cols="2">TCFB6C is higher by more than 0.2 for an equal num-</cell></row><row><cell cols="2">ber of other topics. We speculate that this may have</cell></row><row><cell cols="2">something to do with how the true number of clusters</cell></row><row><cell cols="2">matches the predefined number of clusters (either 3 or</cell></row><row><cell>6).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.00,135.76,449.81,104.97"><head>Table 5 :</head><label>5</label><figDesc>Number of topics on which one method is better and worse than another method by more than 0.05 in average precision. AP 1 is the average precision of Method 1. AP 2 is the average precision of Method 2.Method 1 Method 2 AP 1 -AP 2 &gt; 0.05 AP 2 -AP 1 &gt; 0.05</figDesc><table coords="5,161.88,172.00,236.56,68.73"><row><cell>TFB3C</cell><cell>BLNFB</cell><cell>22</cell><cell>2</cell></row><row><cell>CFB3C</cell><cell>BLNFB</cell><cell>24</cell><cell>8</cell></row><row><cell>TCFB3C</cell><cell>BLNFB</cell><cell>26</cell><cell>6</cell></row><row><cell>TFB3C</cell><cell>CFB3C</cell><cell>11</cell><cell>11</cell></row><row><cell>TFB3C</cell><cell>TCFB3C</cell><cell>4</cell><cell>12</cell></row><row><cell>CFB3C</cell><cell>TCFB3C</cell><cell>4</cell><cell>8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,86.40,697.16,205.32,7.17;4,72.00,706.64,219.70,7.17;4,72.00,716.12,26.60,7.17"><p>CFB1C is an exception. Because it has only one cluster, it cannot incorporate term relevance feedback information by adjusting cluster weights.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,88.55,473.32,203.51,8.97;5,88.56,485.20,203.49,8.97;5,88.56,497.20,82.67,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,205.65,473.32,86.41,8.97;5,88.56,485.20,53.63,8.97">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,162.74,485.20,99.36,8.97">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">Sept 2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.55,517.12,203.52,8.97;5,88.56,529.12,203.43,8.97;5,88.56,541.00,114.45,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,131.46,517.12,160.61,8.97;5,88.56,529.12,78.81,8.97">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Massachusetts at Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="5,88.55,560.92,203.39,8.97;5,88.56,572.92,203.54,8.97;5,88.56,584.92,203.39,8.97;5,88.56,596.80,203.73,8.97;5,88.56,608.80,22.64,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,189.83,560.92,102.11,8.97;5,88.56,572.92,139.96,8.97">Model-based feedback in the KL-divergence retrieval model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,254.46,572.92,37.64,8.97;5,88.56,584.92,203.39,8.97;5,88.56,596.80,131.59,8.97">Tenth International Conference on Information and Knowledge Management (CIKM 2001)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.55,628.72,203.63,8.97;5,88.56,640.72,203.40,8.97;5,88.56,652.60,96.23,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,219.31,628.72,72.88,8.97;5,88.56,640.72,168.17,8.97">A cross-collection mixture model for comparative text mining</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Velivelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,274.10,640.72,17.86,8.97;5,88.56,652.60,65.20,8.97">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
