<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,214.56,83.88,180.58,15.49;1,124.82,105.80,360.07,15.49">Towards a Multi-Stream Question Answering-As-XML-Retrieval Strategy</title>
				<funder ref="#_C9h8k9j">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_mUjDPzK #_F3SAQNT #_kaWNp4x #_wrea7Ms #_SR7spYX #_PHRfKR4 #_jwJHzrr #_NeUbdy4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,188.89,138.29,55.17,10.76"><forename type="first">David</forename><surname>Ahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.02,138.29,68.10,10.76"><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.07,138.29,84.75,10.76"><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.13,152.17,170.89,10.83"><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Üller Maarten De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.97,152.23,56.80,10.76"><forename type="first">Erik</forename><surname>Tjong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.76,152.23,50.82,10.76"><forename type="first">Kim</forename><surname>Sang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,214.56,83.88,180.58,15.49;1,124.82,105.80,360.07,15.49">Towards a Multi-Stream Question Answering-As-XML-Retrieval Strategy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D238F383361A09EBEBC16C2CBAAC52AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the TREC 2005 Question Answering track; our main focus this year was on improving our multi-stream approach to question answering and on making a first step towards a question answering-as-XML retrieval strategy. We provide a detailed account of the ideas underlying our approaches to the QA task, report on our results, and give a summary of our findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the TREC 2005 Question Answering track, we made two major adaptations to the multi-stream system with which we participated in previous years <ref type="bibr" coords="1,173.81,408.75,10.79,8.97" target="#b7">[8,</ref><ref type="bibr" coords="1,187.15,408.75,12.04,8.97" target="#b9">10]</ref>: first by enabling a socalled table stream to process additional question types and generate more candidate answers, and second, and by encoding the document collection and its linguistic annotation in XML thus enabling a QA-as-XML-retrieval strategy. We took part in the main task of the question answering track as well as in the relationship finding task. We describe the results of our participation in the main task in Section 2, and the results of the relationship finding task in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Main Task</head><p>We built on the multi-stream QA architecture that we have been developing over the past few years as part of our work for the TREC and CLEF QA tasks. The architecture has several streams running in parallel: each is based on a different approach to QA and is a self-contained QA system in itself. No new streams were added this year, leaving us with a total of seven streams: a table stream (detailed in §2.1 below), pattern matching and ngram mining (both against the collection and against the web), a Wikipedia stream (which gets answers out of Wikipedia), and Tequesta (which was updated to XQuesta this year, see §2.2 below). For a more detailed description of our multi-stream approach we refer to <ref type="bibr" coords="1,64.04,710.51,10.79,8.97" target="#b0">[1,</ref><ref type="bibr" coords="1,77.32,710.51,7.47,8.97" target="#b1">2,</ref><ref type="bibr" coords="1,87.28,710.51,7.47,8.97" target="#b4">5,</ref><ref type="bibr" coords="1,97.25,710.51,7.47,8.97" target="#b6">7,</ref><ref type="bibr" coords="1,107.21,710.51,7.19,8.97" target="#b8">9]</ref>.</p><p>Our QA efforts for the main task were concentrated in two areas. First, we enabled the table module to handle more question types and generate more candidate answers (Section 2.1). Second, we upgraded the Tequesta stream by encoding the document collection as well as the different linguistic annotations in XML, thus enabling a "QA-as-XMLretrieval" strategy (2.2). Additionally minor changes were made in the question processing part (2.3) and the named entity recognizer (2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Table Stream Modifications</head><p>An important part of our QA system is a table stream which relies on question-specific tables with answers which were extracted offline rather than during question processing <ref type="bibr" coords="1,541.81,398.32,10.58,8.97" target="#b5">[6]</ref>. The tables contain, among others, information on abbreviation expansions, birthdays, country leaders, and event dates. Evaluation of this stream on the 2004 factoid questions showed that while the accuracy of the returned answers was low (28% lenient evaluation), its coverage was even worse (7%). The work described in this section was intended to improve these scores.</p><p>The first step we took was to add extra tables. The topics of the tables were chosen based on TREC 2004 factoid questions which the system had been unable to answer: birthplaces of people, definitions, groups and their members, nicknames, and organizations, their founders and founding dates. The tables were filled by applying to the document collection hand-crafted extraction rules which utilized available syntactic and named entity annotation of the documents. Most of the tables were small (about 20k entries or less) with the group table (100k) and the definition tables (5M) as exceptions. The latter grew this large because the extraction rules included rules that derived definitions for arbitrary noun phrases. The new tables enabled the stream to handle previously unanswered questions like What kind of animal is an agouti? and their positive effects were larger than the side effect of pattern overgeneration.</p><p>Next, the question analysis and the table processing modules were updated. This work included defining new question topics, creating new question templates and making new links between question topics and tables. We also added a filter to the output of the table stream to make sure that when named entity answers from a certain category (person, location or organization) were requested by the question, ill-typed answers were removed. The filtering step could not be as precise as we would have liked it to be since the categories returned by our named entity analysis are more coarse-grained than the categories of the question analysis.</p><p>The adaptations of the table stream had a positive effect on its performance on the TREC 2004 factoid questions. The recall of the top answers went up from 7% (lenient evaluation) to 21% and even the precision of the answers went up (from 28% to 35%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Structured Information Retrieval</head><p>The system used in our previous participations in TREC-QA <ref type="bibr" coords="2,70.85,261.26,10.79,8.97" target="#b0">[1,</ref><ref type="bibr" coords="2,84.37,261.26,8.30,8.97" target="#b8">9]</ref> contained a text retrieval stream named Tequesta. This year we changed the task of the stream to XML element retrieval. The document collection was enriched automatically with token boundaries, syntactic and named entity annotation. Both the annotation and the original documents were stored in stand-off XML format. Our new stream XQuesta is able to query the collection with XPath and to retrieve elements that satisfy lexical, syntactic and named entity constraints. For this purpose the document collection was divided in non-overlapping sequences of paragraphs containing at least 400 characters. We found that having access to the corpus annotation improved the quality of the text snippets and allowed more elaborate answer filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Question Processing</head><p>Question processing is the first stage in our system architecture which is common to all the streams. Each of the questions is tagged, and a question class is assigned based on our question classification module. The question types correspond to WordNet words and their senses. For instance, the question type of (1) is COACH%1. To determine the expected answer types, we also use the WordNet hierarchy. In our example, the question type COACH%1 is mapped to the expected answer type PERSON.</p><p>(1) Q69.6: Who was the coach of the French team?</p><p>Our question analysis was extended by exploiting the syntactic structure of the questions. Thus, we parse the questions using Charniak's parser <ref type="bibr" coords="2,150.42,625.96,10.58,8.97" target="#b2">[3]</ref>. The parses provide information about the NP/PP-chunks which are used to determine the focus of the sentence, here French team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Named Entity Recognition</head><p>For factoid questions, we highly depend on the correct output of our named entity recognizer. We discovered some problems when assigning the correct named entities to a sentence. One improvement was to post-process the output of the recognizer by correcting obvious inconsistencies of named entity sequences. The following two examples exemplify the sort of errors corrected in the output. The named entity recognizer often failed to assign the correct tag to names which are included in the name of an organization such as in (2).</p><p>( In such cases, the named entity tags are changed to the most common one. Moreover, film titles and quotes in quotation marks are hard to detect for the named entity recognizer such as in <ref type="bibr" coords="2,382.06,242.38,10.58,8.97" target="#b2">(3)</ref>. They are often misclassified as ORG or PER instead of MISC.</p><p>(3)</p><formula xml:id="formula_1" coords="2,350.02,277.66,205.90,20.92">you/O 're/O in/O "/O The/I-ORG Sixth/I-ORG Sense/E-ORG ./O "/O</formula><p>These errors have also been corrected with a postprocessing filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Handling List and "Other" Questions</head><p>The system changes described in the previous sections dealt with factoid questions. For list questions we only made a small modification: we return the same number of answers for each question (eight, when available) because with that number we obtained the best results for the TREC-2004 questions.</p><p>The basic strategy for answering the "other" questions has not changed significantly from last year. The method uses IR and NLP techniques to locate documents containing information about the topic, and extract nuggets from the retrieved documents. The nuggets are assigned an initial score, i.e., the retrieval score of the document from which the nugget was extracted. Duplicate or near duplicate nuggets are removed by using a word overlap similarity.</p><p>Two approaches are adopted for ranking the nuggets. The first approach makes use of a reference corpus, an encyclopedia, in order to rank the nuggets extracted from the target corpus in case the topic is found in the encyclopedia. Especifically, the enyclopedia entry for the topic is extracted and its content is split into sentences. The word overlap score is computed between each nugget and the sentences of the encyclopedia. The nugget is assigned the score for the most similar encyclopedia sentence. Finally, the nuggets are sorted by their respective scores and the top N nuggets are returned.</p><p>The second approach is applied to topics which do not have an entry in the encyclopedia. This approach uses centroid-based summarization technique in order to determine the importance of a nugget. This involves computing centroids, a set of statistically significant words which describe the list of nuggets extracted from the documents. The nuggets are then ranked based on their distance from the centroid <ref type="bibr" coords="3,75.11,92.82,16.60,8.97" target="#b14">[15]</ref> and the top N nuggets are returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Runs</head><p>We submitted three runs for TREC-QA 2005. We were interested in two research questions. First, would the system perform better with all six 1 streams or with a subset of these streams? This question was important since our work this year has focused on two streams (table and XQuesta) while other streams were not changed. In order to determine the best combination, we evaluated different stream combinations on the TREC 2004 questions. We found that the combination of table, XQuesta, Wikipedia and web ngrams was the best for factoid questions while XQuesta, ngrams from the web and ngrams from the collection performed best for list questions.</p><p>Using ngrams from the web for generating factoid answers has as a disadvantage that answers will be generated for almost all questions. This means that few NIL answers will be produced. We considered the presence of NIL answers as an interesting difference between the run with all streams (uams05all) and the run with a subset (uams05be3) and therefore we excluded the web ngrams stream from the factoid questions run. This means that the stream subset run used combinations of three streams: table, XQuesta and Wikipedia for factoid questions and XQuesta, ngrams from the web and ngrams from the collection for list questions. All runs contained the same answers for other questions.</p><p>The second question in which we were interested was: Will answer reranking based on web frequencies improve the quality of the top answers? In order to test this, we created a run (uams05rnk) in which the answers of the complete system had been reranked based on their frequency. We replicated the search engine corroboration method of the Bangor entry of TREC 2003 <ref type="bibr" coords="3,141.98,502.77,10.58,8.97" target="#b3">[4]</ref>, in which answers are ranked according to their frequency of occurrence in the summaries of the top 1000 hits returned by a search engine for a query based on the question. We depart from the Bangor method in two respects: first, we use Yahoo rather than Google, because of the more convenient API, and second, we use a different method to construct queries on the basis of questions. Instead of extracting all NPs and VPs from a question to use as a single query, we submit two queries for each question and use the top 500 hits from each. One query is simply the question itself as a set of keywords, i.e., not constrained to be a phrase. The other query consists of question selectors, words from the question that are highly likely to occur in a correct answer snippet <ref type="bibr" coords="3,144.96,658.19,15.27,8.97" target="#b15">[16]</ref>. Question selectors are extracted from a question using a C4.5 decision tree trained on pairs of questions and correct answer snippets from previous edi- 1 The web pattern match stream which was employed in the last two editions was not included this year because of technical difficulties. tions of the TREC QA track; we followed <ref type="bibr" coords="3,481.53,56.96,16.60,8.97" target="#b15">[16]</ref> in our training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Results</head><p>Table <ref type="table" coords="3,341.93,124.01,4.98,8.97" target="#tab_0">1</ref> gives the combined results for the 3 QA tasks (accuracy for factoids, F score for list and other questions) and the overall scores of our three runs uams05all, uams05be3 and uams05rnk. The column factoid accuracy contains three numbers: exact answers, unsupported answers and inexact answers. The scores for "other" questions are excellent (ranked eighth overall) but the factoid scores (median score of the 71 participating runs was 0.152) are disappointing, especially given the fact that a large part of the work on our system this year was aimed at improving the performance on factoid questions. The answers produced by the limited version of our system (uams05be3) proved to be better than the answers of the complete system (uams05all). Re-ranking based on web-frequencies (uams05rnk) does not improve performance-in fact, it produces substantially fewer correct answers. The primary reason for this decline is that reranking tends to prefer answers that are shorter and more common on the web (irrespective of the question). An analysis of the 287 factoid questions for which the uams05rnk run yielded a different answer than the uams05all run reveals that in 241 cases, the answer chosen by re-ranking is more common than the answer chosen without re-ranking (according to Yahoo). <ref type="foot" coords="3,374.91,499.82,3.69,6.63" target="#foot_0">2</ref> This analysis suggests that the first step to improving re-ranking is to use a more sophisticated scoring mechanism that normalizes with respect to the overall frequency of candidate answers; see <ref type="bibr" coords="3,453.93,537.54,15.77,8.97" target="#b13">[14,</ref><ref type="bibr" coords="3,472.65,537.54,13.28,8.97" target="#b16">17]</ref> for discussion of using web statistics in QA.</p><p>A potential cause for the low factoid scores could be a ranking problem: correct answers might not be ranked as number one. In order to check this, we estimated the accuracy of the system on factoid questions while looking at the top-n answers rather than only examining the top answer. This evaluation was performed automatically and therefore inexact and unsupported answers have also been counted as correct unlike in the official TREC-QA evaluation where only supported exact answers are correct. As Table <ref type="table" coords="3,521.45,658.16,4.98,8.97" target="#tab_2">2</ref> shows, our system potentially could have answered close to 60% of the factoid questions correctly (corresponding to an estimated 32% exact score) with a perfect ranking scheme.  A close look at the top 1 factoid answers generated for the first five targets by our best run (uams05be3) revealed that the errors made by the system had causes in different modules. Of the 27 factoid questions in this group, 22 were answered incorrectly. Incorrect handling of the target topic or the questions caused errors in twelve of the latter questions. We use Wikipedia for finding the most common version of names in the topic but unfortunately this process mapped the topic France wins World Cup in soccer to FIFA Beach Soccer World Cup which made finding correct answers for the related six questions hard. In other cases it was just difficult to determine the question topic or to find the right focus words. Question analysis is over-represented in the error cause list but to its defense it should be noted that where input processing failed, problems in other modules usually did not have a chance to surface.</p><p>The most important other problem was the justification of Wikipedia answers. In five cases the presented justification document was irrelevant for the question topic. Incorrect named entity labeling also caused problems for five questions although in two of these a solution would require annotation at a micro level which is beyond our current automatic annotation efforts (stadium and placeInItaly rather than location). Another system task which we need to review carefully is answer tiling (i.e., the combination of several partial answers to produce the final answer delivered as output). Four questions displayed tiling problems, often because correct answers were lost after they were combined with incorrect ones.</p><p>The three streams involved in this evaluation caused fewer errors than the previously mentioned parts. The Wikipedia and the XQuesta stream each produced three incorrect answers while the table stream generated one of these. The internal ranking of the Wikipedia stream should be improved as should answer filtering within XQuesta. An extra correct answer was missed because the word competitor was not linked to its synonym contestant. A more careful future use of WordNet could be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relationship Finding Task</head><p>In the Relationship Finding task, systems were given topics, i.e., relatively verbose descriptions of user information needs, and had to return collection nuggets answering these needs. In most cases, a topic set a context and asked an explicit question about relationship between two or more entities. E.g., <ref type="bibr" coords="4,316.81,157.26,11.62,8.97" target="#b3">(4)</ref> The This year we took part in this task with a system based on passage retrieval, word similarity, and named entity matching. The system first retrieved passages relevant to a topic, then extracted sentences from the retrieved passages, and reranked the sentences based on similarity to the topic. We describe the process in some detail (sections 3.1-3.4) and present the results (3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Passage Retrieval</head><p>The collection documents were split into passages of 400 characters (extended to the end of a paragraph). As in the main QA task, we used Lucene <ref type="bibr" coords="4,465.36,377.76,16.59,8.97" target="#b12">[13]</ref> with the standard Lnu.ltc model for passage retrieval. We used original full topics as retrieval queries, after (automatically) removing phrases and words likely to be irrelevant for user information needs, such as "The analyst is interested in information regarding" in example 4. The top 10 retrieved passages were split into sentences and processed further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Processing</head><p>For a topic T , our system took its last sentence t (most often, the question expressing the user's information need) for subsequent processing. We extracted named entities from t using our NE tagger; in case t contained fewer than two named entities, we expanded t with preceding sentences, until it contained at least two NEs. For the example 4, t is "Who are Trimble and Hume, and what was their relationship?" and two NEs "Trimble" and "Hume" are extracted. The text t and the list of extracted named entities were used to rerank sentences obtained in the passage retrieval step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-based Sentence Score</head><p>Each retrieved sentence s was assigned a score based on directed word similarity between s and t. In essence, we summed similarities between each word in t and its most similar word in s, according to a specific word similarity measure <ref type="bibr" coords="4,353.50,698.55,15.27,8.97" target="#b10">[11]</ref>. More details on the word-based calculation of similarity can be found in <ref type="bibr" coords="4,432.47,710.51,15.27,8.97" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NE-based Sentence Score</head><p>We combined the word similarity-based score with the score based on the number of shared named entities between s and t. To detect whether two sentences contain common named entities (persons, organizations, locations, miscellaneous entities), we used a dictionary of NE variants created from lists of location-adjective correspondences (e.g., Europe and European) and redirecting links in Wikipedia (e.g., William Jefferson Blythe IV is also known as Bill Clinton, and Burma is an alternative name for Myanmar).</p><p>Collection sentences were ranked using the sum of wordbased and NE-based scores, duplicates and near duplicates were removed using a simple string distance measure, and the best 5 sentences for each topic were returned as answer nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Runs and Results</head><p>We submitted two fully automatic runs for the 25 official test topics. The run uams05l was created as described above and the run uams05s was identical, expect for the fact that the nuggets were shortened by removing all definite and indefinite articles, adjectives and adverbs (other than first, last, etc.). With the second run, we tried to create answers that were as short as possible (evaluation included a length penalty) without removing important material.</p><p>Both runs obtained the F-score of 0.12, with the median over all submitted fully automatic runs being 0.12, the best 0.228 and the worst 0.06.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Document Ranking Task</head><p>Our multi-stream QA architecture does not rely on an ordered set of documents returned from a preprocessing phase. In order to create the obligatory entry for the document ranking task, we returned the justification documents for each answer set in the same order as the final ranking of the answers. Note that our system associates exactly one justification document with each answer.  <ref type="table" coords="5,88.49,638.77,4.98,8.97" target="#tab_4">3</ref> lists the results obtained by the three runs submitted to the document ranking task with the scores for average precision, precision at 10 answers and precision at R answers, where R is the number of correct answers found by the human assessors. Since our main interest in the QA task lies with QA and not with IR, we have not taken any separate actions to optimize these scores. However, this change in combination with adaptations of the named entity annotation, question analysis and the table stream have not lead to an improvement over our 2004 scores. We have identified a number of potential causes on which we will work in the coming year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We described our participation in the TREC 2005 Question Answering track. This year, our work for the Question Answering track was largely motivated by the wish to port one of the streams to a "pure" QA-as-XML-retrieval setting, where the target collection is automatically annotated with linguistic information at indexing time, incoming questions are converted to semistructured queries, and evaluation of these queries gives a ranked list of candidate answers.</p><p>Neither this work nor the other recent modifications of our system have brought us the score improvements that we were looking for. Still, we believe that the direction we have taken this year is both promising with respect to future system performance as well as scientifically interesting. Frequent modification of the system in combination with continuous evaluation must lead to better scores in the TREC evaluation. And representing QA as semi-structured XML retrieval makes our work interesting for both the XML and the IR community. We expect that the feedback of these communities will have a positive effect on our QA work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,324.56,161.62,3.87,8.97;2,350.02,161.62,205.90,8.97;2,350.02,173.58,205.89,8.97;2,350.02,185.53,124.87,8.97"><head></head><label></label><figDesc>) the/O director/O of /O the/O Rose/E-PER Institute/I-ORG of /I-ORG State/I-ORG and/I-ORG Local/I-ORG Government/I-ORG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,333.38,205.77,205.97,74.95"><head>Table 1 :</head><label>1</label><figDesc>Results for the main QA task.</figDesc><table coords="3,333.38,205.77,205.97,52.30"><row><cell></cell><cell>factoid accuracy</cell><cell></cell></row><row><cell>run</cell><cell>(exact,unsup.,inex.)</cell><cell cols="2">list F other F overall</cell></row><row><cell cols="3">be3 0.119 , 0.052 , 0.050 0.064</cell><cell>0.201</cell><cell>0.127</cell></row><row><cell cols="3">all 0.105 , 0.058 , 0.086 0.050</cell><cell>0.200</cell><cell>0.113</cell></row><row><cell cols="3">rnk 0.066 , 0.025 , 0.039 0.029</cell><cell>0.201</cell><cell>0.090</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,64.63,200.79,217.44,8.97"><head>Table 2 :</head><label>2</label><figDesc>Potential improvements of QA factoid scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,62.77,557.62,203.87,90.12"><head>Table 3 :</head><label>3</label><figDesc>Results for the document ranking taskTable</figDesc><table coords="5,84.72,557.62,177.25,41.34"><row><cell>run</cell><cell cols="3">avg. prec. prec. at 10 R-prec.</cell></row><row><cell>uams05all</cell><cell>0.108</cell><cell>0.170</cell><cell>0.129</cell></row><row><cell>uams05rnk</cell><cell>0.094</cell><cell>0.156</cell><cell>0.106</cell></row><row><cell>uams05be3</cell><cell>0.071</cell><cell>0.132</cell><cell>0.082</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,331.16,692.94,224.75,7.17;3,316.81,702.40,239.10,7.17;3,316.81,711.87,161.02,7.17"><p>Of the 46 times in which the answer chosen by re-ranking is less common, that answer is correct (or inexact) 9 times, while the answer chosen without re-ranking is correct (or inexact) 10 times.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">017.001.190</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">264-70-050</rs>, <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, <rs type="grantNumber">612.069.006 640.-001.501</rs>, and <rs type="grantNumber">640.002.501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C9h8k9j">
					<idno type="grant-number">017.001.190</idno>
				</org>
				<org type="funding" xml:id="_mUjDPzK">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_F3SAQNT">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_kaWNp4x">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_wrea7Ms">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_SR7spYX">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_PHRfKR4">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_jwJHzrr">
					<idno type="grant-number">612.069.006 640.-001.501</idno>
				</org>
				<org type="funding" xml:id="_NeUbdy4">
					<idno type="grant-number">640.002.501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,338.39,544.01,217.52,8.97;5,338.39,555.97,217.52,8.97;5,338.39,567.92,217.52,8.97;5,338.39,579.88,217.52,8.97;5,338.39,591.83,50.64,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,432.59,555.97,123.32,8.97;5,338.39,567.92,40.30,8.97">Using Wikipedia in the TREC QA Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,406.88,567.92,149.04,8.97;5,338.39,579.88,186.87,8.97">E. Voorhees and L. Buckland, editors, The Thirteenth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct coords="5,338.39,614.86,217.52,8.97;5,338.39,626.82,217.52,8.97;5,338.39,638.77,217.52,8.97;5,338.39,650.73,217.52,8.97;5,338.39,662.68,217.52,8.97;5,338.39,674.64,217.53,8.97;5,338.39,686.60,217.52,8.97;5,338.39,698.55,217.52,8.97;5,338.39,710.51,22.42,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,473.74,626.82,82.17,8.97;5,338.39,638.77,217.52,8.97;5,338.39,650.73,104.41,8.97">Making stone soup: Evaluating a recall-oriented multi-stream question answering stream for Dutch</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kluck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,361.03,674.64,194.89,8.97;5,338.39,686.60,217.52,8.97;5,338.39,698.55,39.12,8.97">Multilingual Information Access for Text, Speech and Images: Results of the Fifth CLEF Evaluation Campaign</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="423" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,56.96,217.52,8.97;6,75.38,68.91,133.20,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,128.95,56.96,148.24,8.97">A Maximum-Entropy-Inspired Parser</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,75.38,68.91,103.16,8.97">Proceedings of NAACL-00</title>
		<meeting>NAACL-00</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,90.29,217.52,8.97;6,75.38,102.24,217.52,8.97;6,75.38,114.20,217.52,8.97;6,75.38,126.15,173.26,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,253.10,90.29,39.80,8.97;6,75.38,102.24,154.18,8.97">Bangor at TREC 2003: Q&amp;a and genomics tracks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Colquhoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Teahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,246.64,102.24,46.26,8.97;6,75.38,114.20,217.52,8.97;6,75.38,126.15,143.92,8.97">The Twelfth Text Retrieval Conference (TREC 2003). National Institute for Standards and Technology</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,147.53,217.52,8.97;6,75.38,159.48,217.52,8.97;6,75.38,171.44,217.53,8.97;6,75.38,183.39,217.52,8.97;6,75.38,195.35,217.53,8.97;6,75.38,207.30,81.24,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,208.75,147.53,84.15,8.97;6,75.38,159.48,217.52,8.97;6,75.38,171.44,13.08,8.97">Answer selection in a multi-stream open domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,256.84,171.44,36.07,8.97;6,75.38,183.39,217.52,8.97;6,75.38,195.35,69.52,8.97">Proceedings 26th European Conference on Information Retrieval (ECIR&apos;04)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<meeting>26th European Conference on Information Retrieval (ECIR&apos;04)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2997</biblScope>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,228.68,217.52,8.97;6,75.38,240.63,217.53,8.97;6,75.38,252.59,194.65,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,237.02,228.68,55.88,8.97;6,75.38,240.63,149.67,8.97">Preprocessing documents to answer Dutch questions</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,243.55,240.63,49.36,8.97;6,75.38,252.59,49.61,8.97">Proceedings of BNAIC&apos;03</title>
		<meeting>BNAIC&apos;03<address><addrLine>Nijmegen, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,273.96,217.52,8.97;6,75.38,285.92,217.52,8.97;6,75.38,297.87,111.41,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,249.70,273.96,43.20,8.97;6,75.38,285.92,80.26,8.97">How frogs built the Berlin Wall</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,175.35,285.92,93.69,8.97">Proceedings CLEF2003</title>
		<meeting>CLEF2003</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,319.25,217.52,8.97;6,75.38,331.20,217.53,8.97;6,75.38,343.16,217.52,8.97;6,75.38,355.11,217.52,8.97;6,75.38,367.07,87.44,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,182.89,331.20,110.01,8.97;6,75.38,343.16,100.99,8.97">The University of Amsterdam at QA@CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,108.22,355.11,180.01,8.97">Working Notes for the CLEF 2004 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,388.44,217.52,8.97;6,75.38,400.40,217.52,8.97;6,75.38,412.35,217.52,8.97;6,75.38,424.31,201.84,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,194.22,400.40,98.68,8.97;6,75.38,412.35,213.29,8.97">The University of Amsterdam at the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,86.17,424.31,96.63,8.97">Proceedings TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="586" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,445.68,217.52,8.97;6,75.38,457.64,217.52,8.97;6,75.38,469.59,217.52,8.97;6,75.38,481.55,217.53,8.97;6,75.38,493.50,217.52,8.97;6,75.38,505.46,217.52,8.97;6,75.38,517.41,184.21,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,194.22,457.64,98.68,8.97;6,75.38,469.59,213.79,8.97">The University of Amsterdam at the TREC 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,277.96,481.55,14.94,8.97;6,75.38,493.50,188.38,8.97">The Twelfth Text REtrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,538.79,217.52,8.97;6,75.38,550.74,217.52,8.97;6,75.38,562.70,101.13,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,113.16,538.79,179.74,8.97;6,75.38,550.74,19.61,8.97">An information-theoretic definition of similarity</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,115.24,550.74,177.66,8.97;6,75.38,562.70,71.89,8.97">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,584.07,217.52,8.97;6,75.38,596.03,217.52,8.97;6,75.38,607.98,92.85,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,109.50,584.07,183.40,8.97;6,75.38,596.03,60.89,8.97">Recognizing textual entailment: Is word similarity enough?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,156.49,596.03,136.40,8.97;6,75.38,607.98,21.35,8.97">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="6,75.38,629.36,217.53,8.97;6,75.38,642.56,134.59,7.05" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,114.16,629.36,105.81,8.97">The Lucene search engine</title>
		<author>
			<persName coords=""><surname>Lucene</surname></persName>
		</author>
		<ptr target="http://jakarta.apache.org/lucene/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,75.38,662.68,217.52,8.97;6,75.38,674.64,217.52,8.97;6,75.38,686.60,217.53,8.97;6,75.38,698.55,217.53,8.97;6,75.38,710.51,135.07,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="6,277.21,662.68,15.69,8.97;6,75.38,674.64,217.52,8.97;6,75.38,686.60,59.82,8.97">Is it the right answer? Exploiting web redundancy for answer validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,154.47,686.60,138.44,8.97;6,75.38,698.55,91.41,8.97">Proceedings of ACL 40th Anniversary Meeting (ACL-02)</title>
		<meeting>ACL 40th Anniversary Meeting (ACL-02)<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,338.39,56.96,217.52,8.97;6,338.39,68.91,217.53,8.97;6,338.39,80.87,213.54,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,517.73,56.96,38.19,8.97;6,338.39,68.91,174.14,8.97">Centroidbased summarization of multiple documents</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,520.50,68.91,35.42,8.97;6,338.39,80.87,131.11,8.97">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="919" to="938" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,338.39,100.79,217.52,8.97;6,338.39,112.75,217.52,8.97;6,338.39,124.70,217.52,8.97;6,338.39,136.66,136.98,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,413.56,112.75,142.35,8.97;6,338.39,124.70,21.59,8.97">Is question answering an acquired skill?</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Paranjpe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,381.47,124.70,174.44,8.97;6,338.39,136.66,107.25,8.97">Proceedings of the 13th international conference on World Wide Web</title>
		<meeting>the 13th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,338.39,156.58,217.52,8.97;6,338.39,168.54,217.52,8.97;6,338.39,180.49,209.42,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,338.39,168.54,217.52,8.97;6,338.39,180.49,29.78,8.97">Data-driven type checking in open domain question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,376.01,180.49,97.87,8.97">Journal of Applied Logic</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
