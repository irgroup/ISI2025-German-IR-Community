<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,58.08,57.18,478.67,12.91">TREC2005 Question Answering Experiments at Tokyo Institute of Technology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,168.36,96.73,87.75,10.76"><forename type="first">Edward</forename><surname>Whittaker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.27,96.73,68.69,10.76"><forename type="first">Pierre</forename><surname>Chatain</surname></persName>
							<email>pierre@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.24,96.73,67.37,10.76"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
							<email>furui¡@furui.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<addrLine>2-12-1, Meguro-ku</addrLine>
									<postCode>152-8552</postCode>
									<settlement>Ookayama, Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.76,186.37,79.72,10.76"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
							<email>dietrich.klakow@lsv.uni-saarland.de</email>
							<affiliation key="aff1">
								<orgName type="department">Lehrstuhl für Sprachsignalverarbeitung Saarland University</orgName>
								<address>
									<postCode>D-66041</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,58.08,57.18,478.67,12.91">TREC2005 Question Answering Experiments at Tokyo Institute of Technology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">268439E2518D3A6E5DD11C07A757CA31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe Tokyo Institute of Technology's speech group's first attempt at the TREC2005 question answering track which placed us eleventh overall among the best systems of the 30 participants in the track. All our evaluation systems were based on novel, non-linguistic, datadriven approaches to question answering. Our main focus was on the factoid task and we describe in detail one of the new models used in this year's evaluation runs. The list task was treated as a simple extension of the factoid task while the other question task was treated as an automatic summarization problem by important sentence selection. Our best system on the factoid task gave 21.3% correct in first place; our best result on the list task was an average F-score of 0.069 and on the other question task a best average F-score of 0.138.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe the application of a new, general, data-driven and non-linguistic framework for the factoid task of TREC2005 that was presented previously in <ref type="bibr" coords="1,61.91,594.30,15.33,8.97" target="#b17">[18]</ref>. We believe our approach is substantially different to conventional approaches though it shares elements of other statistical, data-driven approaches to factoid question answering in the literature <ref type="bibr" coords="1,156.85,630.18,10.78,8.97" target="#b0">[1,</ref><ref type="bibr" coords="1,170.03,630.18,7.54,8.97" target="#b1">2,</ref><ref type="bibr" coords="1,179.97,630.18,7.54,8.97" target="#b3">4,</ref><ref type="bibr" coords="1,189.91,630.18,7.54,8.97" target="#b6">7,</ref><ref type="bibr" coords="1,199.85,630.18,12.45,8.97" target="#b14">15,</ref><ref type="bibr" coords="1,214.82,630.18,12.45,8.97" target="#b15">16,</ref><ref type="bibr" coords="1,229.68,630.18,11.86,8.97" target="#b16">17]</ref>.</p><p>The availability of large amounts of data, both for system training and answer extraction logically leads to exam-ining statistical approaches to QA. In <ref type="bibr" coords="1,463.27,299.58,11.63,8.97" target="#b0">[1]</ref> a number of statistical methods is investigated for what was termed bridging the lexical gap between questions and answers. In <ref type="bibr" coords="1,533.25,323.46,11.63,8.97" target="#b6">[7]</ref> a maximum-entropy based classifier using several different features was used to determine answer correctness and in <ref type="bibr" coords="1,319.91,359.34,16.66,8.97" target="#b15">[16]</ref> performance was compared against classifying the actual answer. A statistical noisy-channel model was used in <ref type="bibr" coords="1,318.83,383.22,11.63,8.97" target="#b3">[4]</ref> in which the distance computation between query and candidate answer sentences is performed in the space of parse trees. In <ref type="bibr" coords="1,371.57,407.10,16.66,8.97" target="#b16">[17]</ref> the lexical gap is bridged using a statistical translation model. Of these, our approach is probably most similar to <ref type="bibr" coords="1,396.16,431.10,16.66,8.97" target="#b16">[17]</ref> and the re-ranker in <ref type="bibr" coords="1,502.60,431.10,15.33,8.97" target="#b15">[16]</ref>. Statistical approaches still under-perform the best TREC systems e.g. <ref type="bibr" coords="1,349.38,454.98,16.66,8.97" target="#b10">[11]</ref> but have a number of potential advantages over highly tuned linguistic methods including robustness to noisy data, and rapid development for new languages and domains.</p><p>The system we developed for the factoid QA task in TREC2005 involves a statistical, noisy-channel approach where we treat QA as a classification problem. We use a new mathematical model that can include all kinds of dependencies in a consistent manner and is fully trainable requiring minimal human intervention once sufficient data is collected. In doing so we largely remove the need for ad-hoc weights and parameters that are a feature of many TREC systems. Our motivation is the rapid development of data-driven QA systems in new languages and to remove the need for linguistic modules that require a lot of effort to create.</p><p>There are several major differences between our ap-proach and most contemporary approaches to QA: for example, we only use capitalised word tokens in our system and do not use WordNet <ref type="bibr" coords="2,149.40,49.14,10.90,8.97" target="#b5">[6,</ref><ref type="bibr" coords="2,162.82,49.14,12.45,8.97" target="#b10">11,</ref><ref type="bibr" coords="2,177.79,49.14,12.45,8.97" target="#b11">12,</ref><ref type="bibr" coords="2,192.77,49.14,11.86,8.97" target="#b13">14]</ref>, named-entity (NE) extraction, or any other linguistic information e.g. from semantic analysis <ref type="bibr" coords="2,114.53,73.14,11.75,8.97" target="#b5">[6]</ref> or from question parsing <ref type="bibr" coords="2,230.19,73.14,10.78,8.97" target="#b5">[6,</ref><ref type="bibr" coords="2,243.61,73.14,7.42,8.97" target="#b6">7,</ref><ref type="bibr" coords="2,253.67,73.14,11.86,8.97" target="#b10">11]</ref>. We also rely heavily on the web and a conventional web search engine as a source of data for answering questions 1 . We also want to make clear that our approach is also very different to other purely web-based approaches such as askMSR <ref type="bibr" coords="2,258.10,120.90,11.63,8.97" target="#b1">[2]</ref> and Aranea <ref type="bibr" coords="2,81.68,132.90,15.33,8.97" target="#b9">[10]</ref>. For example, we use entire documents rather than the snippets of text returned by web search engines; we do not use structured document sources or databases and we do not transform the query in any way either by term re-ordering or by modifying the tense of verbs.</p><p>Three runs were submitted (asked05a,b,c) for evaluation, all of which were based on variations of this new statistical approach. For the list task, an extension to the system used in the factoid task is used. For the other question task a variation on a system used for speech summarization <ref type="bibr" coords="2,83.00,253.38,11.63,8.97" target="#b7">[8]</ref> is employed.</p><p>The rest of the paper is organized as follows: we first present a summary in Section 2 of the mathematical framework for factoid QA as a classification task that was presented in <ref type="bibr" coords="2,89.59,302.10,15.24,8.97" target="#b17">[18]</ref>. We then describe the extension of our factoid QA approach to answering list questions in Section 3 and the automatic summarization approach applied to other questions in Section 4. We then describe our experimental setup and the performance on all 3 tasks of TREC2005 in Section 7. A discussion and conclusion are given in Sections 8 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Factoid question task</head><p>It is clear that the answer to a question depends primarily on the question itself but also on many other factors such as the person asking the question, the location of the person, what questions the person has asked before, and so on. Although such factors are clearly relevant in a real-world scenario they are difficult to model and also to test in an offline mode, for example, in the context of the TREC evaluations. We therefore choose to consider only the dependence of an answer on the question ¡ , where each is considered to be a string of ¢ ¤£ words ¦¥ ¨ § © § and ¢ "! words ¡ #¥ %$ © &amp; '$ ( ) , respectively. In particular, we hypothesize that the answer depends on two sets of features 0 ¥ 21 43 5¡ 76 and 8 9¥ A@ B3 C¡ D6 as follows:</p><formula xml:id="formula_0" coords="2,113.90,599.75,172.65,13.71">E 3 C %F G¡ 76 H¥ E 3 I %F 0 P8 Q6 (<label>(1)</label></formula><p>1 For interest's sake, however, in this year's TREC we also performed one run that used no web data at all. where 0 ¥ SR T© &amp; PR U WV can be thought of as a set of ¢ IX features describing the "question-type" part of ¡ such as when, why, how, etc. and 8 Y¥ a`© ( ` cb is a set of ¢ "d features comprising the "information-bearing" part of ¡ i.e. what the question is actually about and what it refers to. For example, in the questions, Where was Tom Cruise married? and When was Tom Cruise married? the informationbearing component is identical in both cases whereas the question-type component is different.</p><p>Finding the best answer e involves a search over all for the one which maximizes the probability of the above model:</p><formula xml:id="formula_1" coords="2,370.60,176.05,174.67,18.20">e A¥ 2f Gg 'h Hi pf &amp;q £ E 3 I rF 0 P8 Q6 (<label>(2)</label></formula><p>This is guaranteed to give us the optimal answer in a maximum likelihood sense if the probability distribution is the correct one. We don't know this and it's still difficult to model so we make various modeling assumptions to simplify things. Using Bayes' rule this can be rearranged as</p><formula xml:id="formula_2" coords="2,364.70,274.45,180.57,26.60">f sg h ti pf Gq £ E 3 0 P8 uF &amp; T6 tv E 3 I w6 E 3 0 P8 x6<label>(3)</label></formula><p>The denominator can be ignored since it is common to all possible answer sequences and does not change. Further, to facilitate modeling we make the assumption that 8 is conditionally independent of 0 given to obtain:</p><formula xml:id="formula_3" coords="2,348.10,371.05,197.17,18.20">f Gg 'h Hi pf &amp;q £ E 3 I8 yF G T6 v E 3 0 F &amp; T6 v E 3 I w6<label>(4)</label></formula><p>Using Bayes rule, making further conditional independence assumptions and assuming uniform prior probabilities, which therefore do not affect the optimisation criterion, we obtain the final optimisation criterion:</p><formula xml:id="formula_4" coords="2,363.20,461.05,182.07,33.60">f Gg 'h ti pf &amp;q £ E 3 I rF 8 Q6 ¤ ' W ( ' v E 3 0 F &amp; T6 ( I<label>(5)</label></formula><p>The E 3 C dF e8 Q6 model is essentially a language model which models the probability of an answer sequence given a set of information-bearing features 8 , similar to the work of <ref type="bibr" coords="2,358.74,546.66,15.24,8.97" target="#b12">[13]</ref>. It models the proximity of to features in 8 . We call this model the retrieval model and examine it further in Section 2.1.</p><p>The</p><formula xml:id="formula_5" coords="2,340.00,577.75,43.28,13.10">E 3 0 F T6</formula><p>model matches an answer with features in the question-type set 0 . Roughly speaking this model relates ways of asking a question with classes of valid answers. For example, it associates dates, or days of the week with when-type questions. In general, there are many valid and equiprobable for a given 0 so this component can only re-rank candidate answers retrieved by the retrieval model. If the filter model were perfect and the retrieval model were to assign the correct answer a higher probability than any other answers of the same type the correct answer should always be ranked first. Conversely, if an incorrect answer, in the same class of answers as the correct answer, is assigned a higher probability by the retrieval model we cannot recover from this error. Consequently, we call it the filter model and examine it further in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval model</head><p>The retrieval model essentially models the proximity of to features in 8 . Since</p><formula xml:id="formula_6" coords="3,179.60,175.25,45.10,6.20">¥ S § © &amp; §</formula><p>we are actually modeling the distribution of multi-word sequences. This should be borne in mind in the following discussion whenever is used. As mentioned above, we currently use a deterministic information-feature mapping function 8 ¥ #@ B3 C¡ D6 . This mapping only generates word ¡ -tuples ( ¡ ¥ ¢ ¤£ ) from single words in ¡ that are not present in a stop-list of around 50 high-frequency words. In principle the function could of course extract deeper linguistic features but we leave this for future work.</p><p>We first assume that a corpus of text data ¥ is available for searching for answers comprising F ¥ F sentences ¥ © ¤¥ §¦ ©¦ and F pF documents and a vocabulary of F F unique words. We use the notation 8 to define an active set of the features</p><formula xml:id="formula_7" coords="3,55.90,340.86,234.50,20.85">`© &amp; ` b such that 8 ¥ `© v 3 © 6 v 3 6 P` b v 3 cb 6 where</formula><p>3 Pv 6 is a discrete indicator function which equals 1 if its argument evaluates true (i.e. its argument(s) are equal, is not an empty set, or is a positive number) and 0 if false (i.e. its argument(s) are not equal, is an empty set, is 0 or is a negative number) and</p><formula xml:id="formula_8" coords="3,58.60,406.10,211.51,17.45">¥ ! © " b §# is the solution 2 to $ ¥ &amp;% b ' "( © £ ' 0) © ' .</formula><p>The probability </p><p>where 8 6 is a zerogram distribution, and E 3 C rF 8 6 is the conditional probability of given the feature set 8 and is computed as the maximum likelihood estimate from the corpus ¥ :</p><formula xml:id="formula_10" coords="3,78.50,499.70,144.90,21.70">@ d B ¥ ¢ ED £ b for all $ , E 3 I SF 8</formula><formula xml:id="formula_11" coords="3,117.80,565.45,168.75,27.05">E 3 I rF 8 6 H¥ GF 3 C 7 8 6 F 3 I8 6 (7)</formula><p>2 Note that the value of H is simply the base10 number that represents the binary encoding of the active features in I QP . 3 A linear interpolation of models, which borrows directly from statistical language modeling techniques for speech recognition, was found to give retrieval performance approximately twice that of a naive-Bayes or log-linear formulation.</p><p>where</p><formula xml:id="formula_12" coords="3,326.70,49.30,218.57,33.05">F 3 C 7 P8 6 ¥ ¦ ©¦ 7 ' "( © 3 I8 CR @ B3 S¥ ' 6 P6 tv 3 I R ¥ ' 6 (<label>(8)</label></formula><formula xml:id="formula_13" coords="3,338.70,83.50,206.57,26.30">F 3 I8 6 ¥ 7 UT 6V F 3 XW P8 6<label>(9)</label></formula><p>We modify Equation ( <ref type="formula" coords="3,413.62,124.50,3.92,8.97" target="#formula_12">8</ref>) to include contributions from adjacent sentences weighted by @ ( ' which typically has a value Y ¢ :</p><formula xml:id="formula_14" coords="3,310.80,175.50,238.66,61.64">F 3 I 7 8 6 H¥ ¦ ©¦ 7 ' "( © 3 I8 §R @ 3 ¥ ' 6 6 v i f Gq 3 I R ¥ ' 6 ( @ ( ' v 3 I R ¥ ' 0) © 6 ( @ ( ' v 3 C R ¥ ' "`© 6 ¡<label>(10)</label></formula><p>It turns out that smoothing the maximum likelihood estimates from each component distribution has little effect on performance so none is performed. This is partly because of the inherent smoothing effect achieved by interpolating all the distributions together and partly since there is no need to smooth for non-occurring events since such zerotons are never likely to be selected as answers.</p><p>One clear deficiency, however, is the use of equal-valued interpolation weights for all distributions. One might expect a dependence on the number of active features or on F 3 ¤8 6 , however, no such reliable relationship has so far been determined although investigations continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filter model</head><p>The question-type mapping function 1 43 5¡ 76 extracts atuples (a ¥ ¢ ¤£ ) of question-type features from the question ¡ , such as How, How many and When were. A set of F b F ¥ c£ d e£ £ single-word features is extracted based on frequency of occurrence in questions in previous TREC question sets. Some examples include: when, where, who, whose, how, many, high, deep, long etc.</p><p>Modeling the complex relationship between 0 and directly is non-trivial. We therefore introduce an intermediate variable representing classes of example questions-andanswers (q-and-a) f for g D¥ ¢ F h pi TF drawn from the set h pi , and to facilitate modeling we say that 0 is conditionally independent of f given as follows:</p><formula xml:id="formula_15" coords="3,336.90,596.20,208.36,32.85">E 3 0 F &amp; w6 ¥ ¦ q sr t¦ 7 ( © E 3 0 "f F G w6<label>(11)</label></formula><p>¥ ¦ q r ¦ 7</p><formula xml:id="formula_16" coords="3,411.50,641.85,133.76,23.40">( © E 3 0 F Ef 6 v E 3 f F s T6 (<label>(12)</label></formula><p>Given a set of example q-and-a ¡ ' for ¢ a¥</p><formula xml:id="formula_17" coords="4,50.16,31.70,238.74,86.70">¢ F F where ¡ ' ¥ y3 C$ ¤ 6 is then obtained by f ¥ ' £ "! ( 1 43 ¡ ' 6 £ ( © § '</formula><p>, so that: (13)   Assuming conditional independence of the answer words in class f given , and making the modeling assumption that the ¢ th answer word § ' in the example class f is dependent only on the ¢ th answer word in we obtain:</p><formula xml:id="formula_18" coords="4,60.40,134.25,193.10,50.50">E 3 0 F &amp; w6 H¥ ¦ q r ¦ 7 ( © E 3 0 F R © R V 6 v E 3 C § © § F &amp; w6</formula><formula xml:id="formula_19" coords="4,67.90,267.40,218.64,33.65">E 3 0 F &amp; T6 e¥ ¦ q r ¦ 7 ( © E 3 0 F Ef 6 v ¤ # ' "( © E 3 C § ' F G § ' 6 (<label>(14)</label></formula><p>Since our set of example q-and-a cannot be expected to cover all the possible answers to questions that may be asked we perform a similar operation to that above to give us the following:</p><formula xml:id="formula_20" coords="4,50.50,385.80,237.20,45.14">E 3 0 F &amp; T6 H¥ ¦ q r ¦ 7 ( © E 3 0 F Ef 6 # ' "( © ¦ q ¦ 7 ( © E 3 I § ' F Ef 6 E 3 f F &amp; § ' 6 (<label>15</label></formula><formula xml:id="formula_21" coords="4,282.34,421.98,4.19,8.97">)</formula><p>where f is a concrete class in the set of F h £ F answer classes h £ . The independence assumption leads to underestimating the probabilities of multi-word answers so we take the geometric mean of the length of the answer (not shown in Equation ( <ref type="formula" coords="4,92.54,494.10,8.05,8.97" target="#formula_20">15</ref>)) and normalize</p><formula xml:id="formula_22" coords="4,170.30,489.35,51.83,13.10">E 3 0</formula><p>F &amp; w6 accordingly. The system using the above formulation of filter model given by Equation ( <ref type="formula" coords="4,131.54,518.46,8.39,8.97" target="#formula_20">15</ref>) is referred to as model ONE. Systems using the model given by Equation ( <ref type="formula" coords="4,215.43,530.34,8.39,8.97">13</ref>) are referred to as model TWO. The training of Model ONE has been described in detail in <ref type="bibr" coords="4,126.16,554.22,15.33,8.97" target="#b17">[18]</ref>. The details of Model TWO will be described in a future publication. The approach to QA that has been presented is similar in essence to that of approaches to automatic speech recognition (ASR) where there are separate acoustic and language models. In ASR, it is necessary to include a language model weight, $ , which raises the probabilities given by the lan- guage model to the power $ , otherwise performance is very poor:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reconciling</head><formula xml:id="formula_23" coords="4,343.20,72.95,163.70,35.35">e ¥ f Gg 'h ti f Gq £ E 3 C rF 8 Q6 "% v E 3 0 F &amp; T6 % £ '&amp; E 3 I )( F 8 Q6 % v E 3 0 F &amp; 0( W6</formula><p>Several, possibly related, explanations have been given for this requirement including compensation for the independence assumption. In any case, the dynamic range of the models is typically very different and needs compensating somehow. $ can be optimised easily once the individual models have been optimised separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">List question task</head><p>For the list task we essentially use identical systems to those used in the factoid task. Our factoid QA systems always output a list of all the possible answers they encounter in the data, ranked by their probabilities. The issue for the list task is therefore to determine how many of the top answers to output so as to maximise the F-score. We investigated different methods during the development phase for selecting output thresholds. These are discussed for each of the three different runs we submitted in Section 7.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Other question task</head><p>We treat the answering of other questions as a summarization task and employ a variation on a method used for speech summarization <ref type="bibr" coords="4,418.31,406.38,11.75,8.97" target="#b7">[8]</ref> for this purpose. The data from which the nuggets are to be extracted (either web or AQUAINT) is first cleaned to remove words that are unlikely to be required in a nugget but which occur frequently in the data. Duplicate sentences are also removed along with sentences shorter than 40 bytes and longer than 220 bytes. We then select up to 500 sentences which contain as many of the topic words associated with the question as possible, assigning a score to each topic word based on an idf value obtained from the AQUAINT corpus. This results in a single document which is then summarized by selecting up to 175 important sentences according to a combination of a linguistic score (using a 3-gram language model) and a significance score (measured by a tf/idf score), according to the following: bytes are compacted so that all nuggets have a length between 40 and 140 bytes, using a similar summarization process. Finally, upto F ¡ nuggets are selected according to their final summarization score, making sure that the byte-wise Levenstein distance between two nuggets is less than ¢ % of the bytes in any previously selected sentence.</p><formula xml:id="formula_24" coords="4,350.90,586.70,61.90,40.60">¥ 3 0 6 H¥ ¢ F 1</formula><p>Once the set of nuggets had been determined no attempt was made to suppress nuggets that contained answers already given for factoid or list questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">System combination</head><p>For one run this year, for all 3 tasks we combined the output from 3 different systems and submitted this as a separate run. For the factoid and list tasks this combination is performed by summing the inverse rank of an answer § from each component system £ to generate a new score for the answer as follows:</p><formula xml:id="formula_25" coords="5,122.40,369.90,164.14,25.80">£ Uf ¥¤ §¦ Eg 3 I § 6 e¥ 7 ¨¢ ¦ ¨3 C § 6<label>(17)</label></formula><p>where ¦ ¨3 I § 6 is the rank of answer § in system £ . If § is not output by system £ we define ¦ ¨3 C § 6 ¥ © . The answers, sorted by their new score, then form the ranked output of the combined system.</p><p>For the other question task, system combination was performed simply by concatenating nuggets from two systems upto a maximum number F U ¥ of nuggets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Support generation</head><p>The Aranea system <ref type="bibr" coords="5,142.23,534.78,16.54,8.97" target="#b9">[10]</ref> was fortuitously released a few months prior to the TREC2005 evaluation and we took the code for the ProjectAnswer module and made a few simple changes to suit the kind of answers we needed to search for (e.g. all upper-cased answers in all upper-cased text). In all cases, only the (upto) 1000 documents retrieved by the PRISE search engine and provided by NIST were used for searching for support information for each question (i.e. not the documents retrieved by our system for the documentranking task). The same tool was used for determining support for answers in all 3 tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental work</head><p>Three different systems (asked05a,b,c) were submitted for evaluation with characteristics given in Table <ref type="table" coords="5,537.36,178.74,3.77,8.97" target="#tab_0">1</ref>. System asked05a uses model ONE and only AQUAINT data. System asked05b uses model TWO and only Web data. System asked05d uses model ONE and only Web data. System asked05c is a combination of the outputs from systems asked05a, asked05b and asked05d combined according to the method presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Question pre-processing</head><p>Conversion from the XML format provided by NIST to that required by our system was elementary. For each question set the target is extracted and each component question extracted. All target and question strings are then mapped to upper-case. All punctuation except for "'S" is removed both from target and question strings (for some reason commas were not removed but this did not cause any problems). Then, if the target for a question does not appear characterfor-character in that question string it is simply appended to the end of the question string. In general, we feel our approach is quite robust to errors in pre-processing so we do not worry too much about it.</p><p>In addition, although the questions in each set are supposed to be part of a dialogue in which subsequent questions can reference prior questions and answers in the same set, we do not attempt to exploit this. Consequently, each question is treated independently of all other questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Target document preparation</head><p>Our system was designed with web-based question answering in mind. However, for the sake of interest we also performed one run (asked05a) which only used the (upto) 1000 documents from the AQUAINT corpus retrieved by the PRISE search engine and supplied by NIST. The other source of documents we used was obtained by passing each pre-processed, upper-cased question as-is to a web search engine; the top 500 text or HTML documents returned were then downloaded and kept separate for each question. <ref type="bibr" coords="5,528.43,642.30,5.44,8.97">(</ref>  <ref type="table" coords="6,89.99,109.27,4.14,9.22">2</ref>. Performance on all 3 tasks of the 3 submitted runs and an estimated performance score for the factoid task of run asked05d which was not submitted for evaluation.</p><p>the query.) In contrast to other experiments using web data in the literature <ref type="bibr" coords="6,115.14,183.54,11.63,8.97" target="#b2">[3]</ref> none of our experiments has yet found a point at which performance deteriorates after a certain number of documents. We therefore settled on 500 documents for reasons of expediency rather than optimality. Subsequent text processing of the downloaded documents proceeds in essentially the same way as for question preprocessing except that HTML markup is also removed and sentence boundaries are inserted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Factoid question task</head><p>For system development we optimise performance on the TREC2002,3 and 4 evaluation questions using a rotating form of cross-validation but with an emphasis on the performance on the TREC2004 questions. For training the filter model we use 288812 example q-and-a from the Knowledge Master KM data <ref type="bibr" coords="6,118.62,378.54,11.63,8.97" target="#b4">[5]</ref> plus 2408 q-and-a from the TREC-8,9 and TREC2001 questions, and also the TREC2002,3,4 evaluation q-and-a in a rotating manner so as not to include test questions as examples during development.</p><p>The most frequent F q HF ¥ £ £ ¡ £¢ £¢ ¤¢ words from the AQUAINT corpus were used to obtain h £ for F h £ F ¥ d ¥¢ 2d ¥¢ £¢ ¤d ¤¢ ¤¢ £¢ clusters as described in <ref type="bibr" coords="6,201.61,450.54,15.33,8.97" target="#b17">[18]</ref>. The vocabulary q covers approximately 90% of the answers in . The maximum number of features used in the retrieval model was set to ¢ ¤d ¥ ¢ d for reasons of speed and memory efficiency.</p><p>Answer accuracy for the TREC2002,3 and 4 test sets is computed automatically and is based on an exact character match between the answers provided by our system and the capitalized answers in the judgment files provided by NIST. For development we do not worry about support information assuming that this can be constructed reliably later on. Also, the current system never outputs NIL when an answer cannot be found so we automatically get all such answers wrong in both development and evaluation.</p><p>Although in principle we could maximise the likelihood of each correct answer to optimise the system our final objective is the number of correct answers. Consequently we use this as our optimisation criterion on the set of 1341 questions from the TREC2002,3 and 4 QA tasks. The optimised parameters were found to be: ¡ ¥ §¦ , a ¥ ¨¦ , @ ' ¥ ©¢ ¦ , F pF ¥ d ¥¢ £¢ , and $ ¥ £ ¢ . The best set of h £ classes of those investigated was F h £ F s¥ cd ¥¢ ¤¢ £¢ classes <ref type="foot" coords="6,538.56,205.86,3.48,6.28" target="#foot_0">4</ref> .</p><p>For our evaluation system we use an identical setup to the best system determined during development except that we included the TREC2002,3 and 4 q-and-a permanently in</p><formula xml:id="formula_26" coords="6,309.30,251.80,118.73,15.10">h i (F h i F ¥ F F ¥ £ ¤ ¢ £ £ ¥¢ ).</formula><p>The results for all 3 runs on all 3 tasks are shown in Table <ref type="table" coords="6,430.79,269.70,5.03,8.97">2</ref> together with an estimated performance for run asked05d which was not submitted for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">List question task</head><p>System development proceeded in a manner essentially identical to that for the factoid question task described in the previous section, except that the list q-and-a from TREC2002,3 and 4 were also used and added to the set of example q-and-a using the rotating method of crossvalidation.</p><p>For the evaluation system the best system determined during development was selected with the following parameter settings: ¡ ¥ ¦ , a ¥ ©¦ , @ ' ¥ ¢ ¦ , F F ¥ d ¤¢ ¤¢ , $ ¥ ¢ d and F h £ F ¥ d ¥¢ ¤¢ £¢ classes. In addition the list q- and-a from TREC2002,3 and 4 were permanently added to h i .</p><p>The number of questions to output was different for each of the 3 runs we submitted and was determined during development under conditions expected to be similar to the evaluation conditions in each case. For run asked05a we selected 5 answers and then performed answer filtering which typically resulted in fewer than 5 answers per question. For runs asked05b,d we performed answer filtering first and then selected the top 5 answers. For run asked05c (using system combination) we simply used the set of answers from runs asked05a,b,d which resulted in between 11 and 15 answers per question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Other question task</head><p>System development for the other question task for all 3 runs was performed using only the TREC2004 other questions and evaluated using POURPRE-1.0c <ref type="bibr" coords="7,219.52,73.26,11.63,8.97" target="#b8">[9]</ref> with the metric based on simple term counts. During development we determined the optimal number of nuggets to output for runs asked05a,b,c as</p><formula xml:id="formula_27" coords="7,134.30,102.30,50.70,22.30">F ¡ ¥ ¢ ¡</formula><p>¢ and ¢ ¡¢ , respectively, the length of nuggets produced by our system to be between 40 and 140 bytes and set</p><formula xml:id="formula_28" coords="7,151.00,126.20,41.46,16.00">¢ ¥ ¢ ¦ %.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and analysis</head><p>Our best run, run asked05c, ranked 11th among the best systems from each of the 30 participants on the factoid question task this year. While the performance of run asked05b was almost as good as and contributed most to the performance of run asked05c, the performance of run asked05a was quite low, as expected from our development experiments. For the analysis of our system performance in this section, we therefore choose to concentrate on the factoid task of run asked05b since, while the performance of run asked05c was the best overall, it is a combination of outputs from several systems which makes it is less clear where errors originated and is therefore more difficult to analyse.</p><p>In Table <ref type="table" coords="7,98.84,340.50,5.03,8.97" target="#tab_2">3</ref> we give the percentage of errors (i.e. wrong and inexact answers as judged by NIST) for run asked05b on questions in the evaluation set that can be attributed to the retrieval, filter or a combination of retrieval and filter models. For this analysis we call an error anything that was marked wrong or inexact, of which there were 271 such errors.</p><p>Percentage of errors in each model combination NOT R F R&amp;F NK ERR. 41.3% 28.0% 24.4% 5.6% 0.7% It is clear that, even given this subjective evaluation of the errors, the retrieval model is mostly to blame. This is hardly surprising given the simplicity of our retrieval model. For example, we got almost all questions wrong that contained a "did..." construction such as "When did X die?" since the verb is almost always in a form different to that in the text where answers are likely to occur e.g. "X died in 1974". Such questions accounted for almost 20% of the total set of factoid questions this year.</p><p>A large deficiency of model TWO used in the asked05b run is that numbers are not always assigned an equal probability by the filter model. Actually this applies equally to any ostensibly similar class of answers but the differences are most apparent for numbers. Approximately 19% of errors in the filter model could be attributed to this. The percentage is high partly because the number of questions this year which could be answered correctly with only a number was also very high-approximately 29% of questions, with 36% having a number somewhere in the correct answer.</p><p>Our system never output NIL as an answer. We preferred instead to output an answer whether or not support could be found for the answer. This year about 5% of questions required a NIL answer to be marked correct so we got them all wrong for all runs.</p><p>Despite the time difference between data in the AQUAINT corpus and the web data we were using very few errors were caused by this difference-only about 2% of errors.</p><p>There were also 2 answers in our output that were classified as wrong although we believe the document supports the answer which would actually make them either right or inexact rather than wrong. For question 100.1 "Sammy Sosa": "Where was Sammy Sosa born?" we gave the answer "SAN PEDRO DE MACORIS" in document NYT19980927.0104 from:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAN PEDRO de MACORIS, Dominican Republic -As</head><p>Sammy Sosa came to bat for the final time Sunday, the crowd of about 150 men in what may have been the only place here, in his hometown with a... For question 121.2 "Rachel Carson": "Where was her home?" we gave the answer "MAINE" in document NYT19991230.0073 from:</p><p>...1962. Maine biologist Rachel Carson..."</p><p>A breakdown of the inexact answers showed that 9 errors were in location questions where a state was given but no town (or vice-versa); 6 errors were in time questions where only a year was given but a day and month was also required; and 4 errors were in names of people where a surname but no first name was given. From our system's point-of-view these were not errors since the q-and-a examples used in training (including those from previous TREC evaluations) also contained equally inexact answers but had been classified as correct. In future we will endeavour to remove potentially inexact training examples or replace them with more exact equivalent answers.</p><p>For the list question task it turned out to be somewhat naive to take the top-scoring answers from our factoid question answering system since many irrelevant and inappropriate answers were output as a result. Consequently it probably would have been better to output more answers rather than the 1 to 15 answers that were output for list questions; for example, run asked05c performed best and also had the largest average number of answers per question. A substantial cause of the poor list question performance was that there were far fewer list training q-and-a examples than those available for factoid training resulting in worse question-matching and therefore worse answermatching performance. This matching was further muddied by the inclusion of factoid questions in the q-and-a set since the factoid question types are substantially different to list question types cf. use of singular vs. plurals in list questions.</p><p>In future experiments on list questions we will restrict ourselves to using only list q-and-a as examples.</p><p>For the other question task, run asked05c (F-measure of 0.131) was quite similar to run asked05a (F-measure of 0.138), since it was made by taking the best answers from the asked05a run, and completing with nuggets from run asked05b up to a maximum of 18 nuggets. Run asked05a,c performed better than run asked05b (F-measure of 0.091), mostly because the web data that was used contained many garbage tokens that had not been cleaned correctly.</p><p>Projecting answers obtained from web data back on to the AQUAINT corpus documents turned out to be far from trivial. Indeed we lost around 20% of our correct, exact answers for runs asked05b,c because they were unsupported by the document we provided. Had the support been correct our best score would have been 26.5% for run asked05c on the factoid task. For the answers obtained only using the AQUAINT corpus the projection operation worked better and the loss was only around 13%.</p><p>Finally for run asked05c our system combination method was found to be surprisingly effective and robust despite being very simple. An absolute improvement in accuracy of 1.4% (or 7.0% relative) over our best individual run (asked05b) was obtained on the factoid task and 21.1% relative F-score improvement on the list task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have described our novel, data-driven and nonlinguistic approach to question answering and presented the official results obtained in the TREC2005 evaluation. We have shown that our method, despite being very different to contemporary approaches achieves performance on the factoid task that is better than the majority of other systems. However, such performance is still substantially worse than the best participating systems.</p><p>We aim to extend our data-driven approach by includ-ing minimal linguistic transformations of the question such as verb-tense modification and term re-ordering such as performed by Aranea <ref type="bibr" coords="8,402.84,49.14,16.66,8.97" target="#b9">[10]</ref> and other systems. We will also demonstrate that our approach achieves similar performance on other languages when sufficient and suitable training data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Online demonstration</head><p>A demonstration of the system using model ONE supporting questions in English, Japanese, Chinese, Russian and Swedish can be found online at http://asked.jp/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Acknowledgments</head><p>This research was supported by JSPS and the Japanese government 21st century COE programme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,308.88,596.30,239.31,66.96"><head>Table 1 . Descriptions of systems developed for TREC2005.</head><label>1</label><figDesc></figDesc><table coords="4,308.88,596.30,239.31,66.96"><row><cell>7 ( © 32</cell><cell>3 IR 6 54 6$ v 87 3 ¤R 6 ¡</cell><cell>(16)</cell></row><row><cell cols="3">where 2 3 IR 6 and 7 3 IR 6 are the linguistic score and the signifi-F is the number of words in the sentence 0 , and</cell></row><row><cell cols="3">cance score of word R , respectively. Sentences over 140</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,308.88,642.30,236.12,20.97"><head>Table</head><label></label><figDesc>We relied on the web search engine to strip out stop words from</figDesc><table coords="6,124.92,24.06,343.20,72.33"><row><cell></cell><cell></cell><cell>Factoid task</cell><cell></cell><cell>List</cell><cell>Other</cell><cell>Avg. per-</cell></row><row><cell>System</cell><cell>Right</cell><cell>Unsupp.</cell><cell>ineXact</cell><cell>task</cell><cell>task</cell><cell>series score</cell></row><row><cell cols="6">asked05a 45 (12.4%) 7 (1.9%) 21 (5.8%) 0.044 0.138</cell><cell>0.108</cell></row><row><cell cols="6">asked05b 72 (19.9%) 19 (5.2%) 21 (5.8%) 0.057 0.091</cell><cell>0.136</cell></row><row><cell cols="6">asked05c 77 (21.3%) 19 (5.2%) 22 (6.1%) 0.069 0.131</cell><cell>0.157</cell></row><row><cell>asked05d</cell><cell cols="2">64 (17.7%)</cell><cell>10 (2.8%)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,62.04,479.35,212.44,45.10"><head>Table 3 . Percentage of errors of total 271 in Retrieval and Filter models, Not Known errors, and NOT actually ERRors for run asked05b on the TREC2005 factoid task.</head><label>3</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="6,323.16,655.66,221.61,7.17"><p>There may be a more optimal number or combination of such classes.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,330.46,295.94,214.53,8.07;8,330.48,306.86,214.43,8.07;8,330.48,317.78,214.44,8.07;8,330.48,328.82,214.29,8.07;8,330.48,339.74,168.37,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,346.19,306.86,198.72,8.07;8,330.48,317.78,56.74,8.07">Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.03,317.78,141.89,8.07;8,330.48,328.82,214.29,8.07;8,330.48,339.74,83.09,8.07">Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 23rd annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.46,351.74,214.45,8.07;8,330.48,362.66,214.44,8.07;8,330.48,373.70,214.45,8.07;8,330.48,384.62,125.44,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,473.31,351.74,71.60,8.07;8,330.48,362.66,137.18,8.07">An Analysis of the AskMSR Question-answering System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,490.11,362.66,54.81,8.07;8,330.48,373.70,214.45,8.07;8,330.48,384.62,63.20,8.07">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.46,396.62,214.43,8.07;8,330.48,407.54,214.42,8.07;8,330.48,418.58,214.43,8.07;8,330.48,429.50,214.42,8.07;8,330.48,440.42,51.81,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,505.56,396.62,39.33,8.07;8,330.48,407.54,143.78,8.07">Web Question Answering: is more always better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,490.93,407.54,53.96,8.07;8,330.48,418.58,214.43,8.07;8,330.48,429.50,174.83,8.07">Proceedings of the 25th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on research and development in information retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.46,452.42,214.41,8.07;8,330.48,463.46,214.44,8.07;8,330.48,474.38,94.63,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,433.83,452.42,111.05,8.07;8,330.48,463.46,73.32,8.07">A Noisy-Channel Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,427.02,463.46,117.90,8.07;8,330.48,474.38,67.91,8.07">Proceedings of the 41st Annual Meeting of the ACL</title>
		<meeting>the 41st Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.47,486.38,214.51,8.07;8,330.48,497.42,214.44,8.07;8,330.48,508.34,20.03,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,382.75,486.38,144.73,8.07">Knowledge Master Educational Software</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hallmarks</surname></persName>
		</author>
		<ptr target="http://www.greatauk.com/" />
	</analytic>
	<monogr>
		<title level="j" coord="8,533.47,486.38,11.51,8.07;8,330.48,497.42,14.88,8.07">PO Box</title>
		<imprint>
			<biblScope unit="volume">998</biblScope>
			<date type="published" when="2002">2002</date>
			<pubPlace>Durango, CO 81302</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.47,520.34,214.53,8.07;8,330.48,531.26,214.66,8.07;8,330.48,542.30,85.63,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,476.89,520.34,68.10,8.07;8,330.48,531.26,105.13,8.07">The Use of External Knowledge in Factoid QA</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>C-Y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,453.88,531.26,91.26,8.07;8,330.48,542.30,59.29,8.07">Proceedings of the TREC 2001 Conference</title>
		<meeting>the TREC 2001 Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.47,554.30,214.44,8.07;8,330.48,565.22,212.14,8.07;8,330.48,576.14,85.63,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,447.64,554.30,97.26,8.07;8,330.48,565.22,106.38,8.07">IBM&apos;s Statistical Question Answering System-TREC-11</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,452.92,565.22,89.70,8.07;8,330.48,576.14,59.29,8.07">Proceedings of the TREC 2002 Conference</title>
		<meeting>the TREC 2002 Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.47,588.14,214.43,8.07;8,330.48,599.18,214.41,8.07;8,330.48,610.10,181.33,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,459.27,588.14,85.62,8.07;8,330.48,599.18,199.86,8.07">Automatic speech summarization based on sentence extraction and compaction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,330.48,610.10,82.49,8.07">Proceedings of ICASSP</title>
		<meeting>ICASSP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.47,622.10,214.42,8.07;8,330.48,633.02,216.31,8.07;8,330.48,644.06,214.43,8.07;8,330.48,654.98,59.36,8.07" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,452.43,622.10,92.46,8.07;8,330.48,633.02,117.54,8.07">Automatically Evaluating Answers to Definition Questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno>LAMP- TR-119/CS-TR-4695/UMIACS-TR-2005-04</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="9,71.63,25.94,214.54,8.07;9,71.64,36.86,214.55,8.07;9,71.64,47.90,214.44,8.07;9,71.64,58.82,214.44,8.07;9,71.64,69.74,20.03,8.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,144.42,25.94,141.75,8.07;9,71.64,36.86,214.55,8.07;9,71.64,47.90,21.90,8.07">Question Answering from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.86,47.90,175.22,8.07;9,71.64,58.82,210.60,8.07">Proceedings of Twelfth International Conference on Information and Knowledge Management (CIKM 2003)</title>
		<meeting>Twelfth International Conference on Information and Knowledge Management (CIKM 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,80.18,214.65,8.07;9,71.64,91.22,214.67,8.07;9,71.64,102.14,214.78,8.07;9,71.64,113.06,85.75,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,268.80,91.22,17.51,8.07;9,71.64,102.14,106.54,8.07">LCC Tools for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Badulescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bolohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.28,102.14,91.14,8.07;9,71.64,113.06,59.41,8.07">Proceedings of the TREC 2002 Conference</title>
		<meeting>the TREC 2002 Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,123.62,214.52,8.07;9,71.64,134.54,214.42,8.07;9,71.64,145.46,214.69,8.07;9,71.64,156.50,128.09,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,174.27,123.62,111.88,8.07;9,71.64,134.54,151.39,8.07">The Informative Role of Word-Net in Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,241.81,134.54,44.25,8.07;9,71.64,145.46,214.69,8.07;9,71.64,156.50,47.94,8.07">Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources</title>
		<meeting>the NAACL 2001 Workshop on WordNet and Other Lexical Resources<address><addrLine>Pittsburgh PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,166.94,214.55,8.07;9,71.64,177.86,214.44,8.07;9,71.64,188.78,214.54,8.07;9,71.64,199.82,214.67,8.07;9,71.64,210.74,20.03,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,156.78,166.94,129.40,8.07;9,71.64,177.86,77.47,8.07">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,170.45,177.86,115.62,8.07;9,71.64,188.78,214.54,8.07;9,71.64,199.82,126.62,8.07">Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on research and development in information retrieval<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,221.18,214.53,8.07;9,71.64,232.22,214.53,8.07;9,71.64,243.14,149.44,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,225.96,221.18,60.20,8.07;9,71.64,232.22,164.44,8.07">Use of WordNet Hypernyms for Answering What-Is Questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,253.80,232.22,32.37,8.07;9,71.64,243.14,123.10,8.07">Proceedings of the TREC 2002 Conference</title>
		<meeting>the TREC 2002 Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,253.58,214.53,8.07;9,71.64,264.50,214.44,8.07;9,71.64,275.54,194.51,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,246.82,253.58,39.34,8.07;9,71.64,264.50,130.42,8.07">Probabilistic Question Answering on the Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grewal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,223.60,264.50,62.49,8.07;9,71.64,275.54,119.89,8.07">Proc. of the 11th international conference on WWW</title>
		<meeting>of the 11th international conference on WWW<address><addrLine>Hawaii, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,285.98,214.78,8.07;9,71.64,296.90,213.77,8.07;9,71.64,307.94,214.44,8.07;9,71.64,318.86,143.34,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,250.91,285.98,35.50,8.07;9,71.64,296.90,197.34,8.07">Statistical QA-Classifier vs. Re-ranker: What&apos;s the difference?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,71.64,307.94,214.44,8.07;9,71.64,318.86,116.97,8.07">Proceedings of the ACL Workshop on Multilingual Summarization and Question Answering</title>
		<meeting>the ACL Workshop on Multilingual Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,329.30,214.55,8.07;9,71.64,340.22,214.44,8.07;9,71.64,351.26,86.71,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,154.98,329.30,131.19,8.07;9,71.64,340.22,58.98,8.07">Automatic Question Answering: Beyond the Factoid</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,147.56,340.22,138.52,8.07;9,71.64,351.26,60.36,8.07">Proceedings of the HLT/NAACL 2004: Main Conference</title>
		<meeting>the HLT/NAACL 2004: Main Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.63,361.70,214.66,8.07;9,71.64,372.62,214.56,8.07;9,71.64,383.66,158.78,8.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,214.58,361.70,71.71,8.07;9,71.64,372.62,214.56,8.07;9,71.64,383.66,15.34,8.07">A Statistical Pattern Recognition Approach to Question Answering using Web Data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,103.77,383.66,100.30,8.07">Proceedings of Cyberworlds</title>
		<meeting>Cyberworlds</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
