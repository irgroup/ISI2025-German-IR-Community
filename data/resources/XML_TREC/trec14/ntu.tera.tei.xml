<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,124.38,60.23,363.21,12.58">National Taiwan University at Terabyte Track of TREC 2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.32,87.18,72.36,9.88"><forename type="first">Ming-Hung</forename><surname>Hsu</surname></persName>
							<email>mhhsu@nlg.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.12,87.18,65.72,9.88"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,124.38,60.23,363.21,12.58">National Taiwan University at Terabyte Track of TREC 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A9887F3B13465248B01BB9F96240267</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are three tasks in the Terabyte track of TREC 2005, i.e. Efficiency, Ad hoc and Named page finding. We participated in all the tasks and use different retrieval methods to deal with each task, aiming to vary the retrieval method according to the different characteristics of different tasks. In Ah hoc task, we adopt the technique of query-specific clustering. In Named page finding task, we cared more about the information of document title and anchor text of out-links.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first year we participated in Terabyte Track. The primary goal of this track is to develop an evaluation methodology for terabyte-scale document collections. Besides, efficiency and scalability issues are also concerned. Some different criteria are used in evaluation because the information needs of the web users often vary a lot. There are three tasks in this track, i.e., Efficiency, Ad hoc, and Named page finding. Our retrieval methods for each task are mainly based on Okapi <ref type="bibr" coords="1,174.10,382.50,11.73,9.88" target="#b5">[6]</ref>, with some variants according to the characteristics of different tasks. In this paper, we focus on Ad hoc task and Named page finding task since we simply used Okapi to retrieve documents based on content words in Efficiency task.</p><p>In Ad hoc task, we employed clustering technique to improve retrieval performance. A scoring function was adopted to rank clusters that were generated from the top N documents retrieved by Okapi, i.e., query-specific clustering <ref type="bibr" coords="1,97.23,452.52,11.69,9.88" target="#b0">[1]</ref>.</p><p>In Named page finding task, the information from the document title and anchor text is very useful and important to identify the named pages, so that we increased the weight of document title and anchor text when computing the relevance score of a document to a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preprocessing and Indexing</head><p>All the documents in the corpus were stemmed using Porter's algorithm <ref type="bibr" coords="1,374.34,553.50,11.71,9.88" target="#b2">[3]</ref>, and all words except stop words were indexed. Titles (i.e., the words within &lt;title&gt; and &lt;/title&gt; in html format) of documents are extracted and indexed additionally, so as anchor text of out-links of documents. The two additional indices will be used in Named page finding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Ad Hoc Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation and Description of Our Method</head><p>In a typical ad hoc retrieval task, the IR system is requested to retrieve as many relevant documents to some topics (queries) as possible. For most topics in Ad Hoc tasks of TREC, there are usually tens of relevant documents. For example, topics of TREC9 (451-500) have 2,617 relevant documents, about 52 relevant documents per topic. Intuitively, as the number of documents increases, the number of relevant documents increases, too. Under this situation, we tried to cluster relevant together in the ranked list retrieved by Okapi (so called query-specific clustering), to improve retrieval performance. Clustering hypothesis <ref type="bibr" coords="2,421.96,74.40,12.83,9.88" target="#b1">[2]</ref> has been verified to be held in the manner of query-specific clustering <ref type="bibr" coords="2,238.05,88.44,11.59,9.88" target="#b0">[1]</ref>.</p><p>Our method is a two-stage approach. In the first stage, we used Okapi to retrieve 10,000 documents. In the second stage, the top N (N≦10,000) documents retrieved in the first stage were clustered. After that, those clusters were scored and ranked by a heuristic function. In the final ranked list, all documents in a higher-scored cluster would be ranked higher than those in a lower-scored cluster, and the intra-cluster ranking of documents would follow their ranks in the first stage.</p><p>For the reason of efficiency, we used Bi-Section K-Means, which has shown to be an efficient and high-quality clustering algorithm, for query-specific clustering <ref type="bibr" coords="2,329.39,186.42,11.60,9.88" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ranking Clusters</head><p>In the ranked list returned by an IR system based on probability model, a document with a higher rank is more possible to be relevant to the query. According to this property, we proposed a simple cluster scoring function. This function, which is in terms of the rank of a document and the general performance of the IR system, estimates the probability of the document to be relevant, and further the quality of certain cluster. The general performance of Okapi here is the P@R of testing the topics used in last year's terabyte track, where R is one of {5, 10, 15, 20, 30, 100, 200, 500, 1000}. The results are listed in Table <ref type="table" coords="2,291.71,306.42,4.12,9.88" target="#tab_0">1</ref>. Different values of R represent the upper bounds of rank levels of document rank, so we partitioned documents into nine rank levels. Assume the function L(r) maps a document with rank r into certain rank level. That is, L(1)=1, L(6)=2, L(11)=3, and so on. According to Table <ref type="table" coords="2,552.80,334.44,4.12,9.88" target="#tab_0">1</ref>, we can estimate the relevant probabilities of documents in different rank levels. For example, the relevant probability of a document with rank 16, i.e., the 4 th rank level, can be estimated by (20 * P@20 -15 * P@15) ／ (20-15) ＝ 0.4202</p><p>In this way, we can derive Table <ref type="table" coords="2,195.35,408.42,5.48,9.88">2</ref> from Table <ref type="table" coords="2,254.55,408.42,4.12,9.88" target="#tab_0">1</ref>. It shows the relevant probability RP(L) related to rank level L.</p><p>Let C i denotes the ith cluster, R ij denotes the rank of the jth document in C i and |C i | denotes total number of documents in C i . The cluster scoring function S(C i ) determines the average relevant probability of documents in C i :</p><formula xml:id="formula_0" coords="2,214.44,465.01,209.20,29.94">∑ = = | | 1 )) ( ( | | 1 ) ( i C j ij i i R L RP C C S (1)</formula><p>After scoring clusters, all the clusters were ranked according to their scores, and the final ranked list is generated.  <ref type="table" coords="2,193.07,695.40,4.12,9.88">2</ref>, we also used the topics of last year to determine the number of clusters, K, which is a parameter for query-specific clustering using Bi-Section K-Means algorithm. In the first stage of our method, 10,000 documents were retrieved and regarded as the baseline (without re-ranking), but only the top N will be and re-ranked. By testing our method with the topics of last years, K=10 and N=1,000 showed a better result. However, no matter what values of K and N were used, our simple cluster scoring function didn't outperform the baseline in average precision (AP). On the other hand, interestingly, our method exhibited potential for the improvement in P@10. As most users browse only a few of the top-ranked documents when they search the web, the improvement in P@10 may be useful and meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results and Discussion</head><p>Table <ref type="table" coords="3,78.90,162.42,5.48,9.88" target="#tab_1">3</ref> shows the results of three runs we submitted in this task. There are three evaluation criteria, i.e., average precision, P@10, and Binary preference (Bpref). NTUAH1 is the run using BM25 on full documents. The run NTUAH2 used BM250 to retrieve passages, which are dynamically determined, so it is time-consuming. NTUAH3 is the result of our method. It is obvious that our simple two-stage approach is inferior to the baseline, NTUAH1, and passage retrieval a little outperforms full document retrieval, whatever the evaluation criterion is. There are several reasons for the out-of-expected performance of our two-stage method. The first is that our simple cluster scoring function deeply depends on the performance of the first stage, i.e., the original ranked list retrieved by Okapi. When the performance of the first stage is not good enough, our cluster scoring function usually performs worse. The second reason is that the number of truly relevant documents has effects on ranking clusters. In other words, if there are fewer relevant documents in the corpus, it is more difficult to rank clusters. For the 50 topics in this task, the median of the numbers of relevant documents is 172. Comparing with NTUAH1, our method performs better in 26 topics. In the remaining 24 topics, 17 of them have relevant documents fewer than 172. This result reflects the second reason mentioned above. The third reason is the relevant probability (refers to Table <ref type="table" coords="3,551.97,330.42,4.57,9.88">2</ref>) estimated by last year's topics is much different from the result of this year's. Table <ref type="table" coords="3,435.08,344.40,5.48,9.88">4</ref> shows the performance of Okapi on this year's topics. In Table <ref type="table" coords="3,216.66,358.44,4.12,9.88">4</ref>, it's presented that P@R decreases whenever R increases. However, it's not the same condition in Table <ref type="table" coords="3,188.09,372.42,4.12,9.88">2</ref>. In Table <ref type="table" coords="3,245.72,372.42,4.12,9.88">2</ref>, P@R does not decrease obviously between R=5 and R=30. The difference between Table <ref type="table" coords="3,164.41,386.40,5.48,9.88">2</ref> and Table <ref type="table" coords="3,218.12,386.40,5.48,9.88">4</ref> directly influences the result of our cluster scoring function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Named Page Finding Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation and Description of Our Method</head><p>Named page finding task is much different from traditional ad hoc task in the aspect of the number of "correct answers". The goal of Named page finding is to find a specific page or its "near duplicates" with near rank one, so using only content of the document to identify the relevance between the document and the query is obviously not sufficient for this task. In the environment with terabyte-scale corpus and without support of extremely expensive hardware, we tried to utilize titles and anchor texts of out-links of documents to improve the search result which is retrieved based on only document contents, document title and anchor text are commonly considered as informative <ref type="bibr" coords="4,105.57,74.40,11.64,9.88" target="#b4">[5]</ref>. Besides the original index of document contents, two additional indices are produced for the titles and the anchor text of out-links, respectively. Our method is described as follows:</p><p>For each query, we perform BM25 retrieval on all the three indices (i.e., content, title, and anchor text) and merge the three ranked lists. The results are merged by a linear combination of scores (which have been normalized) of the documents, hence the relevant score value (RSV) of document Di is</p><formula xml:id="formula_1" coords="4,179.16,167.14,277.89,11.81">RSV(D i )＝C C ．S C (D i )＋C T ．S T (D i )＋C AT ．S AT (D i )<label>(2)</label></formula><p>where S C (D i ), S T (D i ) and S AT (D i ) represent the score of D i in the list retrieved on index of content, title, and anchor text, respectively. C C , C T , and C AT are their weights and C C ＋C T ＋C AT ＝1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results and Discussion</head><p>Table <ref type="table" coords="4,78.31,257.40,5.48,9.88" target="#tab_2">5</ref> shows our results of three runs. NTUNF1 is the baseline, i.e., document retrieval using BM25. NTUNF3 is the result of passage retrieval using BM250. NTUNF2 is our method that utilizes the information of document title and anchor text of out-links. Passage retrieval did not clearly outperform document retrieval. That indicates the information of document content was insufficient to deal with Named page finding. Our method got slightly improvement in the percentage of named pages retrieved at top 10, but results of the three runs exhibited no significant difference in MRR. However, it didn't mean that document titles and anchor text had no influences on the performance. By comparing the results of individual topics in NTUNF1 and in NTUNF2, we found that many topics got much different performance in the two runs. For example, the named page of topic 619 was ranked at 2 in NTUNF1 but was ranked at 21 in NTUNF2. This example indicated that document title and anchor text were intuitively informative but also very noisy, especially for this task. How to utilize the information reliably and robustly is necessary for future work. Figure <ref type="figure" coords="4,109.80,544.92,5.48,9.88" target="#fig_0">1</ref> shows the performance difference of RR (reciprocal of rank) between NTUNF1 and NTUNF2 for each topic. The x-axis stands for topic ID and the y-axis stands for the performance difference, i.e. for the performance of each topic, its RR value in NTUNF1 subtracting that in NTUNF2. For the points in Figure <ref type="figure" coords="4,526.65,572.94,4.11,9.88" target="#fig_0">1</ref>, if the y-coordinate is smaller than 0, it means our method performs better than baseline for that topic, otherwise it means that our method performs worse for that topic. It's showed that our method improves the baseline for many topics but there are also some topics got worse result at the same time. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,148.80,278.10,314.49,9.88"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The difference of RR values between NTUNF1 and NTUNF2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,75.18,562.44,438.68,142.84"><head>Table 1 .</head><label>1</label><figDesc>The performance of Okapi on TREC2004 topics</figDesc><table coords="2,75.18,576.90,438.68,128.38"><row><cell>R</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>1000</cell></row><row><cell cols="10">P@R 0.4612 0.4633 0.4694 0.4571 0.4469 0.3720 0.3073 0.1934 0.1235</cell></row><row><cell></cell><cell></cell><cell cols="6">Table 2. Relevant probability related to rank level</cell><cell></cell><cell></cell></row><row><cell>L</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell cols="10">RP(L) 0.4612 0.4654 0.4816 0.4202 0.4265 0.3399 0.2426 0.1175 0.0536</cell></row><row><cell cols="2">Besides Table 1 and Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,101.70,418.44,412.16,154.37"><head>Table 3 .</head><label>3</label><figDesc>Our Results of Ad Hoc task</figDesc><table coords="3,101.70,434.58,412.16,138.23"><row><cell></cell><cell></cell><cell>Run-ID</cell><cell></cell><cell>AP</cell><cell></cell><cell>P@10</cell><cell cols="2">Bpref</cell><cell></cell></row><row><cell></cell><cell cols="3">NTUAH1 (Okapi-Doc)</cell><cell>0.3023</cell><cell></cell><cell>0.59</cell><cell cols="2">0.3201</cell><cell></cell></row><row><cell></cell><cell cols="3">NTUAH2 (Okapi-Psg)</cell><cell>0.3233</cell><cell></cell><cell>0.6</cell><cell cols="2">0.3419</cell><cell></cell></row><row><cell></cell><cell cols="3">NTUAH3(Okapi-D+Clst)</cell><cell>0.2425</cell><cell></cell><cell>0.506</cell><cell cols="2">0.29</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">Table 4. The performance of Okapi on TREC2005 topics</cell><cell></cell><cell></cell></row><row><cell>R</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>30</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>1000</cell></row><row><cell cols="10">P@R 0.6520 0.5900 0.5533 0.5400 0.5180 0.4282 0.3459 0.2174 0.1383</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,144.36,429.42,304.50,81.52"><head>Table 5 .</head><label>5</label><figDesc>Our results of Named page finding task</figDesc><table coords="4,144.36,445.56,304.50,65.38"><row><cell>Run-ID</cell><cell>ARR</cell><cell>%top10</cell><cell>%fail</cell></row><row><cell>NTUNF1 (Doc)</cell><cell>0.385</cell><cell>52</cell><cell>20.2</cell></row><row><cell>NTUNF2 (D＋T＋AT)</cell><cell>0.387</cell><cell>51.6</cell><cell>20.2</cell></row><row><cell>NTUNF3 (Psg)</cell><cell>0.388</cell><cell>51.2</cell><cell>19.4</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,77.46,337.08,483.66,9.88;5,77.46,355.08,423.69,9.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,347.97,337.08,213.15,9.88;5,77.46,355.08,147.59,9.88">The effectiveness of query-specific hierarchic clustering in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,240.00,355.08,180.52,9.88">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="559" to="582" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.46,373.08,370.52,9.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,182.81,373.08,138.33,9.88">The cluster hypothesis revisited</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,339.90,373.08,49.98,9.88">SIGIR 1985</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="188" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.46,391.08,370.04,9.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,167.35,391.08,143.20,9.88">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,317.46,391.08,36.79,9.88">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.46,409.08,483.53,9.88;5,77.46,427.08,142.58,9.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,299.34,409.08,217.69,9.88">A comparison of document clustering techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,537.84,409.08,23.15,9.88;5,77.46,427.08,110.13,9.88">KDD Workshop on Text Mining</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.46,445.08,483.70,9.88;5,77.46,463.08,264.80,9.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,288.44,445.08,272.72,9.88;5,77.46,463.08,51.74,9.88">Engineering a multi-purpose test collection for Web Retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,136.74,463.08,172.53,9.88">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,77.46,481.08,483.53,9.88;5,77.46,499.08,221.98,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,222.60,481.08,80.28,9.88">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,328.92,481.08,232.07,9.88;5,77.46,499.08,41.00,9.88">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
