<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.77,123.63,290.81,15.49">Pitt at TREC 2005: HARD and Enterprise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.50,157.54,49.45,10.76"><forename type="first">Daqing</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.58,157.54,69.26,10.76"><forename type="first">Jae-Wook</forename><surname>Ahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Sciences</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.77,123.63,290.81,15.49">Pitt at TREC 2005: HARD and Enterprise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A7980F50B7D7E0797A9FC21C383DB3F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Pittsburgh team participated in two tracks for TREC 2005: the High Accuracy Retrieval from Documents (HARD) track and the Enterprise Retrieval track.</p><p>The goal of Pitt's HARD study in TREC 2005 was to examine the effectiveness of applying Self Organizing Maps (SOM) as a visual presentation tool and as a clustering tool in the context of HARD tasks, especially its role in clarification form generation. Our experiment results demonstrate that SOM can be used as a clustering tool to generate terms for query expansion based on interactive relevance feedback. It produced significant improvement over the baseline when measured by R-Prec. However, its effectiveness of being a visualization tool for users to make relevance feedback still needs careful examination and further studies.</p><p>Our goal in this year's enterprise search track was to study the effect of query expansion based on an expansion corpus in retrieving emails from an email corpus. The expansion corpus consisted of the WWW, People and ESW sub-collections of the W3C test collection. The results indicate that query expansion based on the expansion corpus can achieve significant improvement over the no expansion baselines. However, there is no significant difference to the simpler query expansion approach using blind relevance feedback. Interestingly the terms used in these two query expansion approaches are different, with averagely only 6 term overlap among 20 possible terms. Further study is needed for examining the effect of combining these two approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">HARD track 1.Overview</head><p>Searching for information is increasingly common in people's life. Modern techniques based on "free text" indexing and ranked retrieval have proven to be scalable and robust. Batch mode information retrieval (IR), which essentially studies retrieval algorithms, receives a great deal of attention. However, the initiative of searching for information, ultimately, lies to human. It is people who pose the questions, interpret what they read, and determine when their needs have been met. Especially in modern retrieval process, end users, who are not necessary search experts nor domain experts, leverage easy access to full text to support increasingly focused exploratory searches via iterative refinement <ref type="bibr" coords="1,88.19,640.73,81.01,9.82">[Marchionini1995]</ref>. Therefore, the ultimate goal of retrieval systems is not to generate the best possible ranked list for a given search query, but to provide the best information access mechanisms to users so that they can easily find needed information, and have a pleasant search experience.</p><p>High Accuracy Retrieval from Documents (HARD) is a track in TREC trying to address the problem of studying interaction between human users and retrieval systems, but at the same time keeping the TREC tradition of examining retrieval algorithms and achieving cross-site comparison <ref type="bibr" coords="1,81.37,722.02,51.40,9.82">[Allan2003]</ref>.</p><p>The goal of Pitt's HARD study in TREC 2005 is to examine the effectiveness of using Self Organizing Maps (SOM) as a visual presentation tool and as a clustering tool in the context of HARD tasks, especially its role in clarification forms generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Self Organizing Maps</head><p>Self Organizing Map (SOM) is a technique invented by Professor Teuvo Kohonen which reduces the dimensions of data through the use of self-organizing neural networks <ref type="bibr" coords="2,377.84,175.67,66.35,9.82">[Kohonen2000]</ref>. It has been used for data visualization and natural language processing tasks <ref type="bibr" coords="2,355.00,189.21,120.76,9.82">[Lagus et al.1996, Lin1997]</ref>. The way that SOM reduces high data dimensions is by producing a map of usually two dimensions which plot the similarities of the data by grouping similar data items together. So SOM is a natural tool for visualizing and clustering data based on their similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Training and Baselines</head><p>We adopted Indri 2.0 as our retrieval system. Indri is a state-of-art retrieval system developed by University of Massachusetts Amherst and Carnegie Mellon University (http://www.lemurproject.org/indri/). It has similar rich query context as Inquery system that was also developed by University of Massachusetts Amherst.</p><p>The training was performed on 50 HARD03 topics on AQUAINT collection, which contains 320380 documents. The search queries were extracted from the title and the description part of the HARD topics for the run HDTRAN-PLAIN, and plus the terms from the named entities in the narrative part of the topic statement automatically identified by BBN's Identifinder<ref type="foot" coords="2,444.75,373.57,3.99,7.18" target="#foot_0">1</ref> for the run HDTRAN2-PLAIN. Our training indicated that HDTRAN-PLAIN achieved better Mean Average Precision (MAP) and R-Precision (R-Prec) than HDTRAN2-PLAIN. This is why almost all our further studies were based on the former way of obtaining topic information (see Table <ref type="table" coords="2,432.26,416.16,3.94,9.82">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>selected docs selected terms weight MAP R-Prec HDTRAN-PLAIN 0.3647 0.3818 HDTRAN2-PLAIN 0.3324 0.3437 HDTRAN-BRF2020@0.3 20 20 0.3 0.3709 0.3919 HDTRAN-BRF2020@0.5 20 20 0.5 0.4168 0.4238 HDTRAN-BRF2020@0.8 20 20 0.8 0.4127 0.4190 HDTRAN-BRF1520@0.5 15 20 0.5 0.4154 0.4200 HDTRAN-BRF2520@0.5 25 20 0.5 0.4149 0.4235 HDTRAN-BRF2015@0.5 20 15 0.5 0.4102 0.4191</p><p>Table <ref type="table" coords="2,90.18,586.76,4.25,9.82">1</ref>: The training on 50 HARD topics. The parameters of the run in bold font was selected as the parameters for the evaluation baseline HDEVAL for generating clarification forms.</p><p>We also examined the blind relevance feedback (BRF) mechanism in Indir system. As shown in Table <ref type="table" coords="2,103.94,639.37,4.09,9.82">1</ref>, BRF with the right parameters can achieve greatly improvement on MAP and R-Prec over the runs without (say HDTRAN-PLAIN). Therefore, we used the BRF setting of HDTRAN-BRF2020@0.5 for generating the baseline runs HDEVAL and HDEVAL2 for our initial HARD submission<ref type="foot" coords="2,100.17,678.07,3.99,7.18" target="#foot_1">2</ref> , and used HDEVAL for generating our clarification forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Clarification Forms</head><p>Unlike the HARD topics in past two years, there is no metadata associated with this year's HARD topics <ref type="bibr" coords="3,92.23,118.66,34.84,9.82">[He and</ref><ref type="bibr" coords="3,129.81,118.66,162.04,9.82">Demner-Fushman2003, He et al.2004</ref>]. However, the requirements to the clarification were relaxed to allow more complex form structures. Our focus on clarification forms was at the usage of SOM. We explored two different but commonly usages of SOMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1">CF1: Graphic Based Approach</head><p>Our first approach to the clarification forms was using SOM to generate a graphic representation of the returned documents. The maps were designed in such way that all similar returned documents would be grouped into the same or adjacent cells on the map. Therefore, effectively, we visualize the "clusters" of returned documents<ref type="foot" coords="3,210.13,234.87,3.99,7.18" target="#foot_2">3</ref> . Figure <ref type="figure" coords="3,94.89,550.95,4.25,9.82">1</ref>: A Graphic Based Clarification Form. The gray scale indicates the similarity of a cell to its surrounding cells. The text is the top 20 terms associated with a cell.</p><p>To construct the graphic based clarification forms, for each topic, we selected top 400 returned documents from the HDEVAL. Based on the content of these documents, we extracted a 1000 word vector as the representation of each documents. The weights of the terms were calculated using BM25. The software used for generating SOMs was SOMPAK version 3.1 developed by the SOM Programming Team of the Helsinki University of Technology Laboratory of Computer and Information Science <ref type="bibr" coords="3,119.86,659.99,91.08,9.82">[Kohonen et al.1996]</ref>.</p><p>The SOM training was divided into two phases. Firstly, there was the ordering phase during which the vectors for the map cells are generated and ordered. In this phase, the initial neighborhood radius was set at 10 and decreased to one during the training. The initial learning rate was 0.05 and it also decreased to zero.</p><p>Secondly, there was the fine tune of the values of the vectors. In this phase, the initial neighborhood radius was set at 3 and the initial learning rate was 0.02. All 400 documents were fed sequentially for training for each of the phases. The topology of the maps generated here was rectangular and the neighborhood function was Gaussian.</p><p>The SOMs generated in the above steps were displayed with a Java applet to visualize the map and the terms for each cell. We used the gray scales of the cells to represent its similarity with its surrounding eight cells. The similarity was calculated based on the cosine value of the vectors of the cells. The method was that the similar a cell is with its surrounding cells, the brighter its gray scale will be. Therefore, if a cell was identical to its surrounding cells, it was filled with white color, and if it is totally different to its surrounding cells, its color is black. Anything in between will have a gray scale corresponding to it. Users were able to move the cursor on each cell to view the top 20 most representative terms associated with the cell. If he liked the terms, he could select the cell, and then move to review another cell. More than one cell could be selected for this task. Figure <ref type="figure" coords="4,467.64,246.79,5.46,9.82">1</ref> shows a SOM map for topic HARD-325.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2">CF2: Text Based Approach</head><p>Our second approach to the clarification forms was using a more traditional text based presentation. However, different to previous methods, we used SOM as a clustering tool to cluster documents into clustering based on their content similarities. The document representation and the SOM construction were identical to the methods used for graphical base CFs.</p><p>From the maps, we extracted two sets of terms and their associated weights in the SOM. The first one is from the cell of the map that is most similar the query. We obtained this information by calculating the cosine value between the vector of each cell and that of the query. Top 20 terms and their associated weights in the vector of the cell were extracted to be put into the clarification forms.</p><p>Although SOM can be used to group similar documents together, it does not represent the concept of clusters in the map. It only has cells, in which contain similar documents. Cells that are adjacent to each other are more similar than cells that are apart, but it is not guaranteed that two adjacent cells would belong to the same cluster. For our text based clarification forms, we have to present terms from different clusters of documents, so we need to construct the clusters of documents based on the cells of documents in the map. Our method was that each cell was treated as a unit, and they are grouped into clusters of cells by using single link clustering method. The similarity between cells was calculated based on cosine, and the threshold was set at 0.85. Based on the clustering results, we then selected top 20 terms and their weights from the top 6 biggest clusters of cells. The reason to choose clusters based on their size was because we assume that the cells containing some relevant documents would be similar to each other, whereas cells contain irrelevant documents would contain rather random content. Therefore, the clusters with many cells have higher chance to contain relevant documents than those with few cells. Figure <ref type="figure" coords="4,257.43,580.96,5.46,9.82" target="#fig_0">2</ref> shows an example of the text based clarification forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Incorporating Clarification Forms</head><p>Both graphic based and text based clarification forms would return a set of terms based on the selection of annotators. The major technique we used for incorporating the results from clarification forms was term based query expansion.</p><p>In CF1 -the graphic base clarification forms, we not only knew the terms selected, but also their weights in the SOMs, so we explored the usage of not only those terms but also their weights in the query expansion. For terms selected in CF2 -the text based clarification forms, we did not obtained weights from SOMs for each individual expanded terms. To differentiate the original query from the expanded terms, we allocated 0.75 relative weight to the former, and 0.25 to the latter across all the topics and runs from all clarification forms. We also explored evidence combination method for combining the results from multiple runs. We used the "CombSUM" method for combining several rank lists into one <ref type="bibr" coords="5,384.78,303.28,88.72,9.82">[Fox and Shaw1994]</ref>. This method reranks the documents based on the sum of their normalized scores in each rank list. This method assumes equal weights for all rank lists. We combined the rank lists from four different runs. The first one is the baseline run HDEVAL used for generating clarification forms. We also included another baseline run HDEVAL2, whose difference to HDEVAL was that the queries of the former also included the terms related to the named entities marked up in the topic narrative parts in the topic statements. The rest two are the two runs (HDEVAL-CF1WW and HDEVAL-CF2) based on the clarification forms CF1 and CF2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Results and Discussion</head><p>The final evaluation results gave us a different view of the two baseline runs. This time, HDEVAL2 achieved better results than HDEVAL when looking at both MAP and R-Prec. However, their difference is not statistical different (for example, p = 0.782 in the Wilcoxon Matched-Pairs Signed-Ranks Test when comparing to the average precision of the two runs)<ref type="foot" coords="5,335.51,487.42,3.99,7.18" target="#foot_3">4</ref> . Using CF1, the graphic based clarification forms, actually hurt the performance. Comparing to the retrieval effectiveness of the run HDEVAL, that of HDEVAL-CF1WW decreased about 15% relatively when measured by MAP, and 5.5% relatively when measured by R-Prec. The decrease in MAP is statistical significant (p = 0.0084). However, this bad result should not rule out the usefulness of the graphic based clarification form using SOMs. Some feedbacks from the NIST annotators about the graphic based clarification forms were mostly concentrated on unclear instructions, and too many similar cells, which makes the selection of the cells really hard. We need further studies to identify the exact failure of the graphic based clarification forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Our CF2, the text based clarification forms generated positive result without problems. When examining via R-Prec, the preferred measure in HARD, HDEVAL-CF2NOB achieved 14% relative increase over the baseline HDEVAL. When measured by MAP, the increase is smaller, but it is still 7% relative increase. The increase measured by R-Prec is statistical significant with the p value as 0.01. The increase measured by MAP is not statistical significant (p = 0.227).</p><p>In general, as shown in the training, BRF is a valid query expansion technique that would bring in positive results. One interesting question to ask here is whether or not BRF still make sense after the query expansion using terms from clarification forms. This was the purpose of the run HDEVAL-CF2B225. Again taking the advantage of the BRF mechanism implemented in Indri search engine, we set the BRF parameters as extracting 20 terms from top 20 documents and use 0.5 as the relative weight allocation between the query and new BRF terms (that is the two have the same weight). We obtained somewhat mixed signals. When looking at MAP, BRF after query expansion based on clarification forms achieved another 5% relative increase, but it generated 1% relative decrease when measured by R-Prec. None of these difference are statistical different. However, comparing to the baseline HDEVAL, HDEVAL-CF2B225 still achieved 12% relative increase in R-Prec, and the increase is statistical significant with the p value as 0.03. Even though HDEVAL-CF2B225 achieved 13% relative increase in MAP against HDEVAL, the difference is not statistical significant (p = 0.08).</p><p>The evidence combination run did not perform well. It indeed achieved better results than the two baseline runs HDEVAL and HDEVAL2, which are the two runs used in the combination. In addition, the improvement over HDEVAL is statistical significant not only measured by R-Rrec (p = 0.001), but also measured by MAP (p = 0.029). However, it does not performed as well as the best of the four runs HDEVAL-CF2B225, although the difference is not significant (p=0.32).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Enterprise Track</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The goal of the Enterprise track is to study the issues related to searching documents of an enterprise (organization). The tasks defined for this year's experiments are expert search and email search. The latter has two subtasks: search for known emails in the collection (called "known item search") and search for emails discussing certain topics (called "discussion topic search"). The collection used for this year's experiment is W3C test collection, which was based on the crawls at W3C.com websites in June 2004. The W3C test collection consists of several sub-collections, of which "email" subcollection is the target corpus that the returned emails should be drawn from. The email sub-collection contains 198,394 emails. The other three W3C sub-collections used in our studies were "www", "esw", and "people" corpora, which contains 45975, 19605, and 1016 documents respectively.</p><p>Term mismatch is a major problem in information retrieval <ref type="bibr" coords="6,334.87,612.59,63.65,9.82">[Crestani2002]</ref>. This indicates that due to complexity and flexibility of language use, the terms existing in the documents and that in the query could be different even though the two terms might express the same concept. This problem becomes worse when the documents are short. This is because short documents do not provide enough space for the various expressions of a concept to be presented in the documents.</p><p>Searching for emails usually faces even worse term mismatch problem. This is because the average emails size tends to be smaller than average articles. For example, the average size of emails in W3C collection is 9.8kb, much shorter than the average size of the Web pages in W3C collection, which is 23.8kb.</p><p>As discussed in <ref type="bibr" coords="6,151.81,734.53,112.75,9.82">[Singhal and Pereira1999]</ref>, a common approach to overcome the term mismatch problem is through expansion. Only by adding more representative terms from relevant or potential relevant documents, can retrieval system get better chance to observe all possible terms associated with the expression of a concept. One key factor that affects the effectiveness of using expansion techniques is that the expansion corpus should be topically related to the query and the target collection. Such requirement may not be easily satisfied in some other retrieval tasks, however we hope that it will be the case for our enterprise retrieval task. The target collection is the W3C email sub-collection (referred as email corpus in this report), and all the search topics were developed on this email corpus. The expansion corpus we used was the combination of www, esw, and people sub-collections (referred as the expansion corpus). We hope that the fact that both corpora belong to the same organization -W3C.com-would means that the expansion corpus can be viewed as the comparable collection to the email corpus which would effectively reduce the term mismatch problem in our email retrievals.</p><p>Therefore, our goal in this year's enterprise search track is to study the effect of query expansion using the expansion corpus in retrieving emails for our email corpus. The other research question is about the comparison between query expansion with the expansion corpus and that with blind relevance feedback on top returned emails. We studied both research questions in the two subtasks: known item search and discussion topic search.</p><p>Same as in our participation to HARD track, the retrieval system we used was Indri 2.0 developed by University of Massachusetts Amherst and Carnegie Mellon University. We made two modifications to the Indri system to make it be able to index emails documents and Web documents, and to make the Indri system to print out the terms and their weights used by Indri system for its query expansion based on blink relevance feedback.</p><p>During indexing, we found that the email corpus contained some empty or duplicated emails. After cleaning, there are total 173146 documents in the email corpus. Our cleaning also included reformatting the email documents to remove unnecessary information. The result format is shown in Figure <ref type="figure" coords="7,94.71,422.93,4.09,9.82">3</ref>. All fields in the emails were indexed.</p><p>The expansion corpus in total contains 66596 documents, with average document length at 19.3kb. We used an in-house developed script to strip off html tags, and reformat the documents into a standard trec format. All documents in the expansion corpus were kept, and their new format is shown in Figure <ref type="figure" coords="7,94.71,477.13,4.09,9.82" target="#fig_1">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Known Item Search</head><p>The known item search (KIS) task assume that the searcher knows that an email in the collection contains the information that he/she wants, and the task for the retrieval system is to locate and return the email at the top of the rank list.</p><p>The search topics for KIS is different to normal TREC topics in that it contains only the title part, which is usually a few keywords (see Figure <ref type="figure" coords="7,259.83,582.14,3.94,9.82">5</ref>).</p><p>Our query expansion algorithm for KIS is straightforward. It involves the five steps presented in Figure <ref type="figure" coords="7,94.71,609.24,4.09,9.82">6</ref>.</p><p>We explored various parameters for the query expansion algorithm using the 25 available training topics. Our training study demonstrated that the retrievals obtained the best results with the query expansion from the top 20 terms selected from top 20 documents. Our training study also demonstrated that the relative weight allocation between the original query and the expanded terms did affect the performance, and it seems that the Indri system achieved the top performance (measured by Mean Reciprocal Precision (MRP)) when the allocation is about 0.7 to 0.8 for the original query, and 0.3 to 0.2 for the expanded terms (see Table <ref type="table" coords="7,231.02,704.09,3.94,9.82" target="#tab_1">3</ref>). This is reasonable consider KIS is a high precision search, so expanded terms, which are not confirmed in their relevance to the search, should not be weighted too much. &lt;DOC&gt; &lt;DOCNO&gt;lists-000-0012197&lt;/DOCNO&gt; &lt;RECEIVED&gt;Fri Feb 13 03:31:25 2004&lt;/RECEIVED&gt; &lt;ISORECEIVED&gt;20040213083125&lt;/ISORECEIVED&gt; &lt;SENT&gt;Fri, 13 Feb 2004 17:31:24 +0900&lt;/SENT&gt; &lt;ISOSENT&gt;20040213083124&lt;/ISOSENT&gt; &lt;NAME&gt;Daigo Matsubara&lt;/NAME&gt; &lt;EMAIL&gt;daigo@w3.org&lt;/EMAIL&gt; &lt;SUBJECT&gt;new list&lt;/SUBJECT&gt; &lt;ID&gt;nm8isib2xer.wl@natto.w3.mag.keio.ac.jp&lt;/ID&gt; &lt;TO&gt;copras-public@w3.org&lt;/TO&gt; &lt;TEXT&gt; charset="US-ASCII" expires="-1" List_Name: copras-public Requester_Email: rigo ListPurpose: This list is the public mailing-list for the copras EU-Project. This project is set up to coach IST-Projects under IST-Framework 6 into the standardization process. This list will be used to interface publicly with IST-Projects Maintaining_Activity: Copras EU-Project --Daigo Matsubara / W3C Systems Team / mailto:daigo@w3.org &lt;/TEXT&gt; &lt;/DOC&gt; Figure 3: An example of reformatted document in the email corpus Our submission runs on the final 125 evaluation topics showed the similar picture. As shown in Table <ref type="table" coords="8,90.90,448.28,4.09,9.82" target="#tab_0">2</ref>.3, using the expansion corpus do achieve significant improvement over no expansion baseline run using the two relative weight allocation obtained in training. The Wilcoxon Matched-Pairs Signed-Ranks Test was used for statistical significant test, and the p value for the difference between the "KITRAN-BASE" and "KITRAN-WWW-QE@0.7", and "KITRAN-WWW-QE@0.8" are both at 0.00058.</p><p>However, when the blind relevance feedback based on the top retrieve emails from the email corpus have an appropriate relative weight allocation (say 0.99 for the original query vs 0.5 for the original query), it can produce comparable improvement over the non-expansion base run (KIEVAL-BASE) as the WWW query expansion did. As shown in Table</p><p>We then examined the effect of query expansion by WWW expansion corpus and that by BRF approach, and found that these two approaches not only achieved similar effect on overall retrieval Runs weight for original query weight for the expanded terms MRP KITRAN-BASE 0.3622 KITRAN-WWW-QE@0.5 0.5 0.5 0.3400 KITRAN-WWW-QE@0.6 0.6 0.4 0.3987 KITRAN-WWW-QE@0.7 0.7 0.3 0.4193 KITRAN-WWW-QE@0.7 0.8 0.2 0.4017  effectiveness, but also expressed similar impact to individual topics. This is reflected in the following observations:</p><p>• Among 125 search topics, there were 49 topics have MRP changes greater or equal than 0.02 between the run with query expansion and the run without. 32 of them have the difference between the change of the KIEVAL-WWW-QE@0.8 run and that of the KIEVAL-BRF QE@0.99 run smaller than or equal to 0.01.</p><p>• Among the 76 topics that had MRP changes smaller than 0.02, only 32 topics were due to the base run has better MRP score than the average 0.2537. All other 44 topics were eight has 0 or lower MRP scores than 0.2537.</p><p>However, the terms used for query expansion in KIEVAL-BRF-QE@0.99 and KIEVAL-WWW-QE@0.8 runs only share in average 6 common terms among 20 possible terms across 125 topics. This is consistent with Harman's study showing that different query expansion mechanisms select different terms. We have not done studies on the effect of combining the two set of terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion Topic Search</head><p>The scenario for discussion topic search is that the user is searching for emails discussing the pros or cons of an argument. Although it would be nice to be able to identify whether the emails are for 1. step 1: select a query Q1 from the batch query file; 2. step 2: using Indri search engine to search the expansion corpus for query Q1; 3. step 3: based on predefined parameters, the Indri system will return top N terms selected from top M documents in the research result of step 2; 4. step 4: based on predefined weight allocation, the expanded query Q2 is a combination of original query Q1 and the expanded terms from step 3.</p><p>5. step 5: go to step 1. or against an argument, it is not the task of discussion topic search to figure that out, at least not in this year's task. The task is to return emails that contain such discussions. There were 50 topics ("issues"), generated by all the participants in the discussion topic search, and the target collection is the W3C email corpus. A correct relevant email is defined as an email which contributes a pro or con relating to the topic, in new (not quoted) text. We adopted the similar approach in the discussion topic search as that in the known item search. We incorporated the expansion corpus used in known item search for the query expansion in the discussion topic search, and used the similar relative weight allocation that was used in known item search.</p><p>We also explored the usage of email threading in retrieving discussion topic emails. We treated two emails sharing the same thread if one is the antecedent of the other in the thread chain, or the two share the same antecedent in the thread chain. We used the threading information to adjust the ranking of documents in two ways. The first one is aggressive approach, where if a document in the rank list shares the same thread with a document in the top N position in the rank list, it will be moved to the top position (the run based on this approach is DTEVAL-BIG) . The other approach is more conservative. The documents that share the same thread with a document in the top N position of the rank list will get a predefined booster. The run DTEVAL-SML1 used top 10 document with the booster 0.8, whereas the run DTEVAL-SML2 looked at top 50 documents with the booster factor 0.8.</p><p>Our analysis shows that the aggressive is a bad strategy, whereas the conservative strategies did achieved improvement over the run that did not using the thread information. The improvement is statistical significant when measured by MAP (p=0.040) for map, and by Precision at 10 (p=0.044).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>In this paper, we have presented the studies we conducted in the participation to TREC 2005 in the tracks of HARD and enterprise. The goal of Pitt's HARD study in TREC 2005 is to examine the Runs MAP R-Prec P@10 DTEVAL 0.1846 0.1906 0.2831 DTEVAL-BIG 0.028 0.0932 0.0169 DTEVAL-SML1 0.2184 0.2241 0.3271 <ref type="bibr" coords="11,183.27,138.92,115.63,9.82">DTEVAL-SML2 0.1949</ref><ref type="bibr" coords="11,311.00,138.92,72.11,9.82">0.1911 0.3203</ref> Table <ref type="table" coords="11,149.98,173.74,4.25,9.82">5</ref>: The results of discussion topic searches on the evaluation topics. effectiveness of using Self Organizing Maps (SOM) as a visual presentation tool and a clustering tool in the context of HARD tasks, especially its role in clarification forms generation. Our experiment results demonstrate that SOM can be used as a clustering tool to generate terms for user's selection and thus for query expansion. It produced significant improvement over the baseline when measured by R-Prec. However, it needs careful design and the user should be given well prepared instructions when using SOM as a visualization tool for users to make relevance feedback. Our further work on HARD studies is at analyzing the failure of the graphic based clarification forms, and exploring a more elaborated approach for evidence combination that we studies in our CLEF 2005 experiment <ref type="bibr" coords="11,63.50,315.38,79.32,9.82">[He and Ahn2005]</ref>.</p><p>Our goal in this year's enterprise search track is to study the effect of query expansion using the expansion corpus in retrieving emails for our email corpus. The expansion corpus consisted of the WWW, People and ESW sub-collections of the W3C test collection. The results indicate that query expansion based on the expansion corpus can achieve significant improvement over the baselines. However, there is no significant difference to the simpler query expansion approach based on blind relevance feedback. Interestingly the terms used in these two query expansion approaches are different, with averagely only 6 term overlap among 20 possible terms. Our Further study in Enterprise track is to finish the analysis of discussion topic search and the examination of the effect of combining the two approaches for query expansion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,190.37,257.39,185.70,9.82"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A Text Based Clarification Form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,126.69,365.62,313.11,9.82;9,63.50,386.28,29.89,7.83;9,63.50,398.24,149.43,7.83;9,63.50,410.19,292.90,7.83;9,63.50,422.14,35.87,7.83"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of reformatted document in the expansion corpus &lt;top&gt; &lt;num&gt; Number: KI33 &lt;/num&gt; &lt;title&gt; WSD WG f2f ws reference examples &lt;/title&gt; &lt;/top&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.43,513.12,389.51,128.33"><head>Table 2 :</head><label>2</label><figDesc>The HARD evaluation results</figDesc><table coords="5,88.43,513.12,389.51,93.51"><row><cell></cell><cell cols="3">CF QE terms with weights CFQE+BRF</cell><cell>MAP R-Prec</cell></row><row><cell>HDEVAL</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>0.2577 0.2903</cell></row><row><cell>HDEVAL2</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>0.2637 0.2981</cell></row><row><cell cols="2">HDEVAL-CF1WW CF1</cell><cell>yes</cell><cell>no</cell><cell>0.2202 0.2743</cell></row><row><cell cols="2">HDEVAL-CF2NOB CF2</cell><cell>no</cell><cell>no</cell><cell>0.2765 0.3296</cell></row><row><cell cols="2">HDEVAL-CF2B225 CF2</cell><cell>no</cell><cell cols="2">yes, 20,20,0.5 0.2908 0.326</cell></row><row><cell>HDEVAL-COMB</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>0.2771 0.3242</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,63.50,720.50,439.52,23.38"><head>Table 3 :</head><label>3</label><figDesc>The effect of different relative weight between the original query and the expanded terms measured by Mean Reciprocal Precision of the Searches on 25 known item training topics.</figDesc><table coords="9,63.50,80.28,370.61,258.89"><row><cell>&lt;DOC&gt;</cell></row><row><cell>&lt;DOCNO&gt;www-000-10696137&lt;/DOCNO&gt;</cell></row><row><cell>&lt;TITLE&gt;Context I&lt;/TITLE&gt;</cell></row><row><cell>&lt;CREATOR&gt;Don Box&lt;/CREATOR&gt;</cell></row><row><cell>&lt;TITLE&gt;box.ppt&lt;/TITLE&gt;</cell></row><row><cell>&lt;TEXT&gt;</cell></row><row><cell>http://www.w3.org/2000/03/xp65435/box.ppt</cell></row><row><cell>what is soap</cell></row><row><cell>don box</cell></row><row><cell>sun/netscape bof</cell></row><row><cell>january 25, 2000</cell></row><row><cell>soap philosophy</cell></row><row><cell>First invent no new technology</cell></row><row><cell>SOAP simply codifies existing practice of using HTTP+XML as an</cell></row><row><cell>application protocol</cell></row><row><cell>... ... ...</cell></row><row><cell>&lt;&lt;/TEXT&gt;</cell></row><row><cell>&lt;/DOC&gt;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,71.90,233.31,422.58,138.46"><head>Table 4 :</head><label>4</label><figDesc>The Mean Reciprocal Precision of Searches on 125 Known Item topics.</figDesc><table coords="10,71.90,233.31,422.58,103.64"><row><cell></cell><cell>Figure 6: The query expansion algorithm</cell><cell></cell><cell></cell></row><row><cell>Runs</cell><cell cols="3">weight for original query weight for the expanded terms MRP</cell></row><row><cell>KIEVAL-BASE</cell><cell></cell><cell></cell><cell>0.2577</cell></row><row><cell>KIEVAL-WWW-QE@0.7</cell><cell>0.7</cell><cell>0.3</cell><cell>0.3325</cell></row><row><cell>KIEVAL-WWW-QE@0.8</cell><cell>0.8</cell><cell>0.2</cell><cell>0.3353</cell></row><row><cell>KIEVAL-BRF-QE@0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.2442</cell></row><row><cell>KIEVAL-BRF-QE@0.99</cell><cell>0.99</cell><cell>0.01</cell><cell>0.3413</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,79.63,701.56,219.24,8.07"><p>http://www.bbn.com/speech/docs/datasheets/idnt-022103.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,79.63,712.73,423.05,8.07;2,63.50,723.69,28.63,8.07"><p>The difference between HDEVAL and HDEVAL2 is the same as that between HDTRAN-PLAIN and HDTRAN2-PLAIN.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,79.63,733.50,397.15,8.07"><p>The map does not actually show the clusters of documents. We will come back to this point in later discussion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,79.63,729.75,423.05,8.07;5,63.50,740.72,121.37,8.07"><p>the Wilcoxon Matched-Pairs Signed-Ranks Test is the statistical significant test used throughout this report. Therefore, we will not mention it every time.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">James Allen</rs> for organizing HARD track, <rs type="person">Ian Soboroff</rs>, <rs type="person">Nick Craswell</rs>, and <rs type="person">Arjen P. de Vries</rs> for coordinating Enterprise track.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,122.29,570.78,380.73,9.82;11,76.21,584.32,281.31,9.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,221.02,570.78,282.00,9.82;11,76.21,584.32,45.01,9.82">Hard track overview in trec 2003 high accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,141.95,584.32,164.44,9.82">The Twelfth Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="11,134.41,606.84,368.56,9.82;11,76.21,620.39,426.75,9.82;11,76.21,633.94,87.66,9.82" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,232.33,606.84,270.63,9.82;11,76.21,620.39,189.83,9.82">Using semantic and phonetic term similarity for spoken document retrieval and spoken query processing</title>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Physica-Verlag GmbH</publisher>
			<biblScope unit="page" from="363" to="375" />
			<pubPlace>Heidelberg, Germany, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,63.50,656.45,439.39,9.82;11,76.21,670.01,306.99,9.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,304.10,656.45,147.19,9.82">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">Shaw</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,473.06,656.45,29.83,9.82;11,76.21,670.01,230.70,9.82">Proeddings of the 2nd Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,63.50,692.52,439.53,9.82;11,76.21,706.07,355.26,9.82;12,63.50,84.20,439.52,9.82;12,76.21,97.75,426.69,9.82;12,76.21,111.30,140.20,9.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,150.77,692.52,131.38,9.82;11,323.14,692.52,179.89,9.82;11,76.21,706.07,86.17,9.82;12,68.95,84.20,310.75,9.82;12,418.34,84.20,84.69,9.82;12,76.21,97.75,297.74,9.82">HARD Experiment at Maryland: From Need Negotiation to Automated HARD Process</title>
		<author>
			<persName coords=""><forename type="first">Ahn</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,182.31,706.07,221.97,9.82;12,396.16,97.75,106.74,9.82;12,76.21,111.30,112.96,9.82">Proceedings of Cross-Language Evaluation Forum</title>
		<meeting>Cross-Language Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2003">2005. 2005. 2005. 2003. 2003</date>
		</imprint>
	</monogr>
	<note>Proceedings of Text REtrival Conference (TREC)</note>
</biblStruct>

<biblStruct coords="12,63.50,133.82,439.52,9.82;12,76.21,147.36,426.81,9.82;12,76.21,160.91,162.22,9.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,180.47,147.36,322.55,9.82;12,76.21,160.91,20.46,9.82">Improving passage retrieval using interactive elicition and statistical modeling</title>
		<author>
			<persName coords=""><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,116.50,160.91,117.03,9.82">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,63.50,183.43,439.52,9.82;12,76.21,196.97,426.81,9.82;12,76.21,210.53,426.81,9.82;12,76.21,224.08,21.52,9.82" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,76.21,196.97,240.87,9.82">SOMPAK: The Self-Organizing Map Program Package</title>
		<author>
			<persName coords=""><surname>Kohonen</surname></persName>
		</author>
		<idno>A31</idno>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct coords="12,137.66,246.59,300.17,9.82;12,63.50,269.10,439.52,9.82;12,76.21,282.66,426.81,9.82;12,76.21,296.21,426.79,9.82;12,76.21,309.76,313.83,9.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,408.70,269.10,94.32,9.82;12,76.21,282.66,295.83,9.82">Self-organizing maps of document collections: A new approach to interactive exploration</title>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Teuvo Kohonen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Maps</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Springer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,160.23,296.21,342.77,9.82;12,76.21,309.76,73.00,9.82">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Simoudis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</editor>
		<editor>
			<persName><surname>Fayyad</surname></persName>
		</editor>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining<address><addrLine>Menlo Park, California</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">2000. 1996. 1996</date>
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct coords="12,113.20,332.27,389.72,9.82;12,76.21,345.83,153.04,9.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,184.87,332.27,167.21,9.82">Map displays for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,361.00,332.27,141.91,9.82;12,76.21,345.83,101.85,9.82">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="54" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,151.99,368.33,350.88,9.82;12,76.21,381.89,333.88,9.82" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,266.25,368.33,236.62,9.82;12,76.21,381.89,201.63,9.82">Inforamtion seeking in electronic environments. Cambridge Series on Human-Computer Interaction</title>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Marchionini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,63.50,404.40,439.52,9.82;12,76.21,417.95,377.58,9.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,395.94,404.40,107.08,9.82;12,76.21,417.95,67.12,9.82">Document expansion for speech retrieval</title>
		<author>
			<persName coords=""><forename type="first">Pereira</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,162.90,417.95,226.34,9.82">Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
