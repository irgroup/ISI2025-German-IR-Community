<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,193.44,114.69,225.11,15.53">The TREC 2005 Terabyte Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.12,148.42,105.63,10.97"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<email>claclark@plg.uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.76,148.42,62.46,10.97"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<email>fscholer@cs.rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.68,196.18,62.71,10.97"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
							<email>ian.soboroff@nist.gov</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,193.44,114.69,225.11,15.53">The TREC 2005 Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A3FC7297C2B179022C55325697C445EA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Terabyte Track explores how retrieval and evaluation techniques can scale to terabyte-sized collections, examining both efficiency and effectiveness issues. TREC 2005 is the second year for the track. The track was introduced as part of TREC 2004, with a single adhoc retrieval task. That year, 17 groups submitted 70 runs in total. This year, the track consisted of three experimental tasks: an adhoc retrieval task, an efficiency task and a named page finding task. 18 groups submitted runs to the adhoc retrieval task, 13 groups submitted runs to the efficiency task, and 13 groups submitted runs to the named page finding task. This report provides an overview of each task, summarizes the results and discusses directions for the future. Further background information on the development of the track can be found in last year's track report <ref type="bibr" coords="1,477.55,400.66,10.86,10.91" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Document Collection</head><p>All tasks in the track use a collection of Web data crawled from Web sites in the gov domain during early 2004. We believe this collection ("GOV2") contains a large proportion of the crawlable pages present in gov at that time, including HTML and text, along with the extracted contents of PDF, Word and postscript files. The collection is 426GB in size and contains 25 million documents. In 2005, the University of Glasgow took over the responsibility for distributing the collection. In 2004, the collection was distributed by CSIRO, Australia, who assisted in its creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Adhoc Task</head><p>An adhoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. For each topic, participants create a query and generate a ranked list of the top 10,000 documents for that topic. For the 2005 task, NIST created and assessed 50 new topics. An example is provided in figure <ref type="figure" coords="1,345.82,626.50,4.19,10.91" target="#fig_0">1</ref>.</p><p>As is the case for most TREC adhoc tasks, a topic describes the underlying information need in several forms. The title field essentially contains a keyword query, similar to a query that might be entered into a Web search engine. The description field provides a longer statement of the topic requirements, in the form of a complete sentence or question. The narrative, which may be a full &lt;top&gt; &lt;num&gt; Number: 756 &lt;title&gt; Volcanic Activity &lt;desc&gt; Description: Locations of volcanic activity which occurred within the present day boundaries of the U.S. and its territories. &lt;narr&gt; Narrative: Relevant information would include when volcanic activity took place, even millions of years ago, or, on the contrary, if it is a possible future event. For the adhoc task, an experimental run consisted of the top 10,000 documents for each topic. To generate a run, participants could create queries automatically or manually from the topics. For most experimental runs, participants could use any or all of the topic fields when creating queries from the topic statements. However, each group submitting any automatic run was required to submit at least one automatic run that used only the title field of the topic statement. Manual runs were encouraged, since these runs often add relevant documents to the evaluation pool that are not found by automatic systems using current technology. Groups could submit up to four runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/top&gt;</head><p>The pools used to create the relevance judgments were based on the top 100 documents from two adhoc runs per group, along with two efficiency runs per group. This yielded an average of 906 documents judged per topic (min 347, max 1876). Assessors used a three-way scale of "not relevant", "relevant", and "highly relevant". A document is considered relevant if any part of the document contains information which the assessor would include in a report on the topic. It is not sufficient for a document to contain a link that appears to point to a relevant web page, the document itself must contain the relevant information. It was left to the individual assessors to determine their own criteria for distinguishing between relevant and highly relevant documents. For the purpose of computing the effectiveness measures, which require binary relevance judgments, the relevant and highly relevant documents were combined into a single "relevant" set.</p><p>In addition to the top 10,000 documents for each run, we collected details about the hardware and software configuration, including performance measurements such as total query processing time. For total query processing time, groups were asked to report the time required to return the top 20 documents, not the time to return the top 10,000. It was acceptable to execute a system twice for each run, once to generate the top 10,000 documents and once to measure the execution time for the top 20 documents, provided that the top 20 documents were the same in both cases.</p><p>Figure <ref type="figure" coords="2,123.89,666.82,5.39,10.91">2</ref> provides an summary of the results obtained by the eight groups achieving the best results according to the bpref effectiveness measure <ref type="bibr" coords="2,317.75,680.38,10.77,10.91" target="#b2">[3]</ref> the run with the highest bpref and the run with the fastest time. The first two columns of the table identify the group and run. The next three columns provide the values of three standard effective measures for each run: bpref, mean average precision (MAP) and precision at 20 documents (p@20) <ref type="bibr" coords="3,109.61,351.34,10.77,10.91" target="#b2">[3]</ref>. The last two columns list the number of CPUs used to generate the run and the total query processing time. When the fastest and best runs are compared within groups, the trade-off between efficiency and effectiveness is apparent. This trade-off is further explored in the discussion of the efficiency results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Efficiency Task</head><p>The efficiency task extends the adhoc task, providing a vehicle for discussing and comparing efficiency and scalability issues in IR systems by defining better methodology to determine query processing times. Nonetheless, the validity of direct comparisons between groups is limited by the range of hardware used, which varies from desktop PCs to supercomputers. Thus, participants are encouraged to compare techniques within their own systems or to compare the performance of their systems to that of public domain systems.</p><p>Ten days before the new topics were released for the adhoc task, NIST released a set of 50,000 efficiency test topics, which were extracted from the query logs of an operational search engine. Figure <ref type="figure" coords="3,106.37,559.06,5.39,10.91">3</ref> provides some examples. The title fields from the new adhoc topics were seeded into this topic set, but were not distinguished in any way. Participating groups were required to process these topics automatically; manual runs were not permitted for this task.</p><p>Participants executed the entire topic set, reporting the top-20 results for each query and the total query processing time for the full set. Query processing time included the time to read the topics and write the final submission file. The processing of topics was required to proceed sequentially, in the order the topics appeared in the topic file. To measure effectiveness, the results corresponding to the adhoc topics were extracted and added into the evaluation pool for the adhoc task. Since the efficiency runs returned only the top 20 documents per topic, they did not substantially increase the pool size.  Figure <ref type="figure" coords="5,123.05,379.30,5.39,10.91">4</ref> summarizes the results for the eight groups achieving the best results, based on p@20. Once again, the figure lists both the best and fastest run from each group. In addition to the query processing times and number of CPUs, the table also includes the estimate of total hardware cost provided by each participating group.</p><p>To illustrate the range of results seen within the track, and to provide a sense of the trade-offs between efficiency and effectiveness, figure 5 plots p@20 against total query processing time for all 32 runs submitted to the efficiency track. Note that a log scale is used to plot query processing times. The range in both dimensions is quite dramatic.</p><p>The results plotted in figure <ref type="figure" coords="5,224.02,487.66,5.39,10.91" target="#fig_1">5</ref> were generated on a variety of hardware platforms, with different costs and configurations. To adjust for these differences, we attempted two crude normalizations. Figure <ref type="figure" coords="5,106.25,514.78,5.39,10.91">6</ref> plots p@20 against total query processing time, normalized by the number of CPUs. The normalization was achieved simply by multiplying the time by the number of CPUs. Figure <ref type="figure" coords="5,507.39,528.34,5.39,10.91">7</ref> plots p@20 against total query processing time, normalized by hardware cost, with the times adjusted to a typical uniprocessor server machine costing $2,000. In this case, the normalization consisted of multiplying the time by the cost and dividing by 2,000. Both normalizations have the effect of moving the points sharply away from the upper left-hand corner, making the trade-offs in this area more apparent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Named Page Finding Task</head><p>Named page finding is a navigational search task, where a user is looking for a particular resource. In comparison to an adhoc task, a topic for a named page finding task usually has one answer: the resource specified in the query. The objective of the task, therefore, is to find a particular page in the GOV2 collection, given a topic that describes it. For example, the query "fmcsa technical support" would be satisfied by the Federal Motor Carrier Administration's technical support page. Named page topics were created by envisaging a bookmark recovery process. Participants were presented with a randomly selected page from the GOV2 collection. If the page had any identifiable features, and looked like something that a real user might want to read, remember, and re-find at a later point in time, the participant was asked to write a query. Specifically, the task was: "write a query to retrieve the page, as if you had seen it 15 minutes ago, and then lost it". This process resulted in an initial list of 272 queries, each with one corresponding target page. After manual inspection, 20 of these were discarded because they were "topic finding" in nature. We believe that collection browsing facilities would have made topic creation far easier than merely viewing successive random pages, but this facility was not available at the time of topic creation.</p><p>Although named page topics are typically assumed to have a single correct answer, the topic creation process outlined above raises two issues: first, there may be exact duplicates of the target document in the collection; and second, the target may not be specified clearly enough to rule out topically similar pages as plausible answers.</p><p>The first issue was resolved by searching for near-exact duplicates of the target pages within the GOV2 collection. This was done with the deco system <ref type="bibr" coords="7,356.98,291.58,10.77,10.91" target="#b0">[1]</ref>, which uses a lossless fingerprinting technique for the detection of duplicate documents. Pools formed from the top 1000 answers from all 42 runs that were submitted for this year's task (around 1.5 million unique documents) were searched. All near-exact duplicates were included in the qrels file.</p><p>In the context of past TREC Web Tracks, the second issue was sometimes resolved by requiring the creation of "omniscient" queries; that is, each query is tested and, if it retrieves similar (but not identical) documents in the collection that could also be considered to be plausible answers, it is discarded. However, discarding such queries distances the experimental process from a real-world web search task: a user generally does not know in advance if a named page query is specific enough to only identify a single resource. For the Terabyte Track, we therefore chose to retain such queries, and treated them as having a single "correct" answer. As a result of the change in methodology, we expected that the named page finding task would be harder than previously experienced.</p><p>Of the 252 topics used for the named page finding task, 187 have a single relevant answer (that is, there are no exact duplicates that match the canonical named page in the answer pool). However, some pages repeat often in the collection (the highest number of duplicate answer pages were identified for topic 778, with 4525 repeats).</p><p>Figure <ref type="figure" coords="7,123.05,508.30,5.39,10.91">8</ref> summaries the results of the named page finding task. The performance of the runs is evaluated using three metrics:</p><p>• MRR: The mean reciprocal rank of the first correct answer.</p><p>• % Top 10: The proportion of queries for which a correct answer was found in the first 10 search results.</p><p>• % Not Found: The proportion of queries for which no correct answer was found in the results list.</p><p>The figure lists the best run from the top eight groups by MRR. In addition, the figure indicates the runs that exploit link analysis techniques (such as pagerank), anchor text, and document structure (such as giving greater weight to terms appearing in titles). While reasonable results can be achieved without exploiting these web-related document characteristics, most of the top runs incorporate one or more of them. 6 The Limits of Pooling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Aside from scaling TREC and research retrieval systems, a primary goal of the terabyte track is to determine if the Cranfield paradigm of evaluation scales to very large collections, and to propose alternatives if it doesn't. In the discussions which led to the current terabyte track, the hypothesis was that in very large collections we would not be able to find a good sample of relevant documents using the pooling method. If judgments are not sufficiently complete, then runs which were not pooled will retrieve unjudged documents, making those runs difficult to measure. Depending on the run and the reason that the judgments are incomplete, this can result in a biased test collection. According to MAP, unjudged documents are assumed irrelevant and a run may score lower than it should. The bpref measure can give artificially high scores to a run if they do not retrieve sufficient judged irrelevant documents. One method for determining if pooling results in insufficiently complete judgments is to remove a group's pooled runs from the pools, and measure their runs as if they had not contributed their unique relevant documents. This test does reveal that the terabyte collections should be used with some caution. For 2004, the mean difference in scores across runs when the run's group is held out is 9.6% in MAP, and the maximum is 45.5%; on the other hand, this was the first year of the track, and overall system effectiveness was not very good. This year, the mean difference is 3.9% and the maximum is 17.7%, much more reasonable but still somewhat higher than we see in newswire.</p><p>Another approach is to examine the documents to see if they seem to be biased towards any particular retrieval approach. We have found that the relevant documents in both terabyte collections have a very high occurrence of the title words from the topics, and that this occurrence is much higher than we see in news collections for ad hoc retrieval. More formally, the titlestat measure for a set of documents D is defined to be the percentage of documents in D that contain a title word, computed as follows. For each word in the title of a topic that is not a stop word, calculate the percentage of the set D that contain the word, normalized by the maximum possible percentage. <ref type="foot" coords="9,125.76,486.22,4.19,7.29" target="#foot_0">1</ref> For a topic, this is averaged over all words in the title, and for a collection, averaged over all topics. A maximum of 1.0 occurs when all documents in D contain all topic title words; 0.0 means that no documents contain a title word at all. Titlestat can be thought of as the occurrence of the average title word in the document set. Titlestat can be measured for any set of documents. For the relevant documents (titlestat rel)in the terabyte collections, we obtain 0.889 for the 2004 collection and 0.898 for 2005. In contrast, the TREC-8 ad hoc collection (TREC CDs 4 and 5 less the Congressional Register) has a titlestat rel of 0.688. For the WT10g web collection, the TREC-9 ad hoc task relevant documents have a titlestat rel of 0.795, and TREC-10 is 0.761.</p><p>Why are the terabyte titlestats so high? We feel that this is directly due to the size of the collection. In nearly any TREC collection, the top ranked documents are going to reflect the title words, since (a) title-only runs are often required, (b) even if they are not required, the title words are often used as part of any query, and (c) most query expansion will still weight the original query terms (i.e., the title words) highly. So it's certainly expected that the top ranks will be dominated by documents that contain the title words. For GOV2, the collection is so large that the title words have enormous collection frequency compared to the depth of the assessment pool. The result of this is that the pools are completely filled with title-word documents, and documents without title words are simply not judged.</p><p>Figure <ref type="figure" coords="10,123.29,142.54,5.39,10.91" target="#fig_3">9</ref> illustrates this phenomenon using the titlestat of the pools, rather than of the judged relevant documents. The first point at x = 0 is the titlestat of the pool from depth 1-100, the pool depth used this year (0.778). In contrast, the titlestat of the TREC-8 pools is 0.429. Subsequent points in the graph show the titlestat of the pool from depth 101-200, 201-300, and so forth. Each pool is cumulative with respect to duplicates, meaning that if a document was pooled at a shallower depth it is not included in a deeper pool stratum. In order to get to lower titlestat depths, we would have had to pool very deep indeed. In any event, these titlestats indicate that the pools are heavily biased towards documents containing the title words, and may not fairly measure runs which do not use the title words in their query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">The Future of the Terabyte Track</head><p>Our analysis of title-word occurrence within the terabyte pools and relevance judgments indicates that the terabyte collections may be biased towards title-only runs. This is a serious concern for a TREC collection, and for the 2006 adhoc task we intend to pursue several strategies to build a more reusable test collection. In part, greater emphasis will be placed on the submission of manual runs, expanding the variety of relevant documents in the pools to include more documents that contain few or none of the query terms and increasing the re-usability of the collection. Users of the 2004 and 2005 collections should be very cautious. We recommend the use of multiple effectiveness measures (such as MAP and bpref) and careful attention to the number of retrieved unjudged documents.</p><p>In addition, the evaluation procedure may be modified to reduce the influence of contentequivalent documents in the collection. Using the 2004 topics as a case study, Bernstein and Zobel <ref type="bibr" coords="10,527.57,445.18,11.49,10.91" target="#b1">[2]</ref> present methods for identifying these near-duplicate documents and discover a surprisingly high level of inconsistency in their judging. Moreover, these near duplicates represent up to 45% of the relevant documents for given topics. This inconsistency and redundancy has a substantial impact on effectiveness measures, which we intend to address in the definition of the 2006 task.</p><p>Along with the adhoc task, we plan to run a second year of the efficiency and named page finding tasks, allowing groups to refine and test methods developed this year. In the case of the efficiency task, we are developing a detailed query execution procedure, with the hope of allowing more meaningful comparisons between systems.</p><p>Planning for 2006 is an ongoing process. As our planning progresses, it is possible that we may add an efficiency aspect to the named page finding task, and a "snippet retrieval" aspect to the adhoc retrieval task. A substantial expansion of the test collection remains a long-term goal, if the track continues in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,229.32,295.06,152.97,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Adhoc Task Topic 756</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,215.52,346.30,176.78,10.91"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Efficiency vs. Effectiveness</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.64,356.50,338.26,10.91"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Efficiency vs. Effectiveness (normalized by number of CPUs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,136.08,454.30,334.38,10.91"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Titlestat in 100-document strata of the 2005 terabyte pools.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,328.52,680.38,210.57,10.91"><head></head><label></label><figDesc>. When possible, we list two runs per group:</figDesc><table coords="3,80.28,73.66,451.08,178.67"><row><cell>Group</cell><cell>Run</cell><cell cols="3">bpref MAP p@20 CPUs Time (sec)</cell></row><row><cell>umass.allan</cell><cell>indri05AdmfS</cell><cell>0.4279 0.3886 0.5980</cell><cell>6</cell><cell>5162</cell></row><row><cell></cell><cell>indri05Aql</cell><cell>0.3714 0.3252 0.5650</cell><cell>6</cell><cell>62</cell></row><row><cell cols="2">hummingbird.tomlinson humT05xle</cell><cell>0.4264 0.3655 0.6230</cell><cell>1</cell><cell>50000</cell></row><row><cell></cell><cell>humT05l</cell><cell>0.3659 0.3154 0.5800</cell><cell>1</cell><cell>5700</cell></row><row><cell>uglasgow.ounis</cell><cell>uogTB05SQE</cell><cell>0.4178 0.3755 0.6180</cell><cell>8</cell><cell>1000</cell></row><row><cell>uwaterloo.clarke</cell><cell>uwmtEwtaPt</cell><cell>0.3884 0.3451 0.5760</cell><cell>2</cell><cell>63</cell></row><row><cell></cell><cell>uwmtEwtaD02t</cell><cell>0.2887 0.2173 0.4490</cell><cell>2</cell><cell>3</cell></row><row><cell>umelbourne.anh</cell><cell>MU05TBa2</cell><cell>0.3771 0.3218 0.5730</cell><cell>1</cell><cell>10</cell></row><row><cell>ntu.chen</cell><cell>NTUAH2</cell><cell>0.3760 0.3233 0.5630</cell><cell>1</cell><cell>734</cell></row><row><cell></cell><cell>NTUAH1</cell><cell>0.3555 0.3023 0.5400</cell><cell>1</cell><cell>270</cell></row><row><cell>dublincityu.gurrin</cell><cell>DCU05AWTF</cell><cell>0.3603 0.3021 0.5600</cell><cell>5</cell><cell>120</cell></row><row><cell>tsinghua.ma</cell><cell cols="2">THUtb05SQWP1 0.3553 0.3032 0.5330</cell><cell>1</cell><cell>1800</cell></row></table><note coords="3,181.68,277.06,244.09,10.91"><p>Figure 2: Adhoc Results (top eight groups by bpref)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,88.56,671.76,303.77,8.97"><p>In rare cases, a title word will have a collection frequency smaller than |D|.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Nick Craswell</rs>, <rs type="person">David Hawking</rs> and others at <rs type="institution">CSIRO</rs> for their assistance with the creation of the GOV2 collection, and <rs type="person">Doug Oard</rs> who hosted our Web crawler at the <rs type="institution">University of Maryland</rs>. The members of the <rs type="institution">Search Engine Group at RMIT University</rs> helped in the creation of the named page topics for this year's <rs type="person">Terabyte Track. Finally</rs>, we thank <rs type="person">Yaniv Bernstein</rs>, who kindly made his deco software available for the identification of duplicate answer pages.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,88.89,212.02,450.20,10.91;11,88.92,225.58,449.85,10.91;11,88.92,239.14,93.64,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,259.81,212.02,273.88,10.91">A scalable system for identifying co-derivative documents</title>
		<author>
			<persName coords=""><forename type="first">Yaniv</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,102.34,225.58,368.92,10.91">Proceedings of the Symposium on String Processing and Information Retrieval</title>
		<meeting>the Symposium on String Processing and Information Retrieval<address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="55" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.89,261.70,450.26,10.91;11,88.92,275.26,450.17,10.91;11,88.92,288.82,259.20,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,265.09,261.70,225.64,10.91">Redundant documents and search effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Yaniv</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,518.31,261.70,20.84,10.91;11,88.92,275.26,450.17,10.91;11,88.92,288.82,57.82,10.91">Proceedings of the 2005 ACM CIKM International Conference on Information and Knowledge Management</title>
		<meeting>the 2005 ACM CIKM International Conference on Information and Knowledge Management<address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="736" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.89,311.26,450.35,10.91;11,88.92,324.82,450.37,10.91;11,88.92,338.38,319.91,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,281.87,311.26,235.25,10.91">Retrieval evaluation with incomplete information</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,88.92,324.82,450.37,10.91;11,88.92,338.38,153.57,10.91">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,88.89,360.94,450.39,10.91;11,88.92,374.50,449.98,10.91;11,88.92,387.94,265.17,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,308.56,360.94,210.85,10.91">Overview of the TREC 2004 Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,88.92,374.50,269.11,10.91">Proceedings of the Thirteenth Text REtrieval Conference</title>
		<meeting>the Thirteenth Text REtrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
	<note>See trec.nist.gov</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
