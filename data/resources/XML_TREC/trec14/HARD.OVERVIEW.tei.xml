<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,172.27,112.05,267.47,15.11;1,161.58,133.97,288.85,15.11">HARD Track Overview in TREC 2005 High Accuracy Retrieval from Documents</title>
				<funder>
					<orgName type="full">Center for Intelligent Information Retrieval</orgName>
				</funder>
				<funder ref="#_5bb2eTa">
					<orgName type="full">SPAWARSYSCEN-SD</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,274.51,175.03,62.98,10.48"><forename type="first">James</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,172.27,112.05,267.47,15.11;1,161.58,133.97,288.85,15.11">HARD Track Overview in TREC 2005 High Accuracy Retrieval from Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCF7374682362D46B7BE1750F0821A12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TREC 2005 saw the third year of the High Accuracy Retrieval from Documents (HARD) track. The HARD track explores methods for improving the accuracy of document retrieval systems, with particular attention paid to the start of the ranked list. Although it has done so in a few different ways in the past, budget realities limited the track to "clarification forms" this year. The question investigated was whether highly focused interaction with the searcher be used to improve the accuracy of a system. Participants created "clarification forms" generated in response to a query-and leveraging any information available in the corpus-that were filled out by the searcher. Typical clarification questions might ask whether some titles seem relevant, whether some words or names are on topic, or whether a short passage of text is related.</p><p>The following summarizes the changes from the HARD track in TREC 2004 <ref type="bibr" coords="1,408.49,400.79,53.59,8.74" target="#b1">[Allan, 2005]</ref>:</p><p>• There was no passage retrieval evaluation as part of the track this year.</p><p>• There was no use of metadata this year.</p><p>• The evaluation corpus was the full AQUAINT collection. In HARD 2003 the track used part of AQUAINT plus additional documents. In HARD 2004 it was a collection of news from 2003 collated especially for HARD.</p><p>• The topics were selected from existing TREC topics. The same topics were used by the Robust track <ref type="bibr" coords="1,96.91,516.11,69.17,8.74" target="#b10">[Voorhees, 2006]</ref>. The topics had not been judged against the AQUAINT collection, though had been judged against a different collection.</p><p>• There was no notion of "hard relevance" and "soft relevance", though documents were judged on a trinary scale of not relevant, relevant, or highly relevant.</p><p>• Clarification forms were allowed to be much more complex this year.</p><p>• Corpus and topic development, clarification form processing, and relevance assessments took place at NIST rather than at the Linguistic Data Consortium (LDC).</p><p>• The official evaluation measure of the track was R-precision.</p><p>The HARD track's Web page may also contain useful pointers, though is not guaranteed to be in place indefinitely. As of early 2006, it was available at http://ciir.cs.umass.edu/research/hard.</p><p>For TREC 2006, the HARD track is being "rolled into" the Question Answering track. The new aspect of the QA track is called "ciQA" for "complex, interactive Question Answering." The goal of ciQA is to investigate interactive approaches to cope with complex information needs specified by a templated query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Process</head><p>The HARD track proceeded as follows. This process follows roughly that of past years' tracks, though it simpler because passage retrieval was not an issue.</p><p>At the end of May, the track guidelines were finalized. Sites knew then that the evaluation corpus would be the AQUAINT collection (see Section 4), so could begin indexing the data and/or training their systems (see Section 7).</p><p>On June 15, 2005, participating sites received the set of 50 test topics from NIST (see Section 5).</p><p>Three weeks later, on July 7, sites had to submit the "baseline" ranked lists produced by their system (see Section 8). These runs ideally represented the best that the sites could do with only "classic" TREC topic information.</p><p>On the same day, sites were permitted to submit sets of clarification forms, where each set contained a form for each topic in the test set. The clarification form could contain almost anything that the site felt an answer would be useful for improving the accuracy of the query (e.g., possibly relevant passages, keywords that might reflect relevance). See Section 9 for more details.</p><p>For the next two weeks, assessors at NIST filled out clarification forms for the topics. On July 25, the clarification form responses were shipped to the sites.</p><p>On August 8, the sites submitted new "final" ranked lists that utilized information from the clarification forms (see Section 10).</p><p>Between then and early September, the assessors judged documents for relevance (see Section 6). Relevance assessments ("qrels") were made available to the researchers on September 9, 2005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participation</head><p>A total of 16 sites submitted 122 runs for the track. The following breakdown shows how many runs each site submitted, broken down by baseline and final runs, as well as the number of clarification forms submitted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># runs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HARD Corpus</head><p>For TREC 2005, the HARD track used the AQUAINT corpus. That corpus is available from the Linguistic Data Consortium for a modest fee, and was made available to HARD participants who were not a member of the LDC for no charge. The LDC's description of the corpus<ref type="foot" coords="3,348.46,127.89,3.97,6.12" target="#foot_0">1</ref> is:</p><p>The AQUAINT Corpus, Linguistic Data Consortium (LDC) catalog number LDC2002T31 and isbn1-58563-240-6 consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST).</p><p>The corpus is roughly 3Gb of text and includes 1,033,461 documents (about 375 million words of text, according to the LDC's web page). All documents in the collection were used for the HARD evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Topics</head><p>Topics were selected from among existing TREC topics that almost no system was able to handle well in previous years. Because those old topics were to be judged on a new corpus (AQUAINT), they were manually vetted to ensure that at least three relevant documents existed in the AQUAINT corpus. These topics were also used by the TREC 2005 Robust track <ref type="bibr" coords="3,260.79,366.00,69.18,8.74" target="#b10">[Voorhees, 2006]</ref>.</p><p>The topic numbers used were: <ref type="bibr" coords="3,207.11,386.53,332.89,8.74;3,72.00,398.49,468.00,8.74;3,72.00,410.44,264.01,8.74">303, 307, 310, 314, 322, 325, 330, 336, 341, 344, 345, 347, 353, 354, 362, 363, 367, 372, 374, 375, 378, 383, 389, 393, 394, 397, 399, 401, 404, 408, 409, 416, 419, 426, 427, 433, 435, 436, 439, 443, 448, 622, 625, 638, 639, 648, 650, 651, 658, and 689</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Relevance judgments</head><p>Topics were judged for relevance by the same assessor who answered the clarification forms for the topic (see Section 9 for more information on clarification forms). In the first two years of HARD, that same person also created the original topic statement; however, because topics were re-used, it was not possible to use the same person for the original step. No attempt was made to ensure that this year's assessor's notion of relevance would match that of the original assessor.</p><p>Six assessors worked on the fifty topics, as follows:</p><p>Assessor </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Training data</head><p>The data collections from the HARD tracks of TREC 2003 <ref type="bibr" coords="4,324.35,105.56,50.54,8.74" target="#b0">[Allan, 2004</ref><ref type="bibr" coords="4,374.89,105.56,45.40,8.74" target="#b0">] and 2004</ref><ref type="bibr" coords="4,422.73,105.56,55.08,8.74" target="#b1">[Allan, 2005]</ref> were available for training. All of that data was made available to HARD track participants courtesy of the Linguistic Data Consortium. The data was provided for use only in the HARD 2005 evaluation with the expectation that it will be destroyed at the completion of the track (i.e., after the final papers are written). The HARD 2004 corpus and topics are now available for purchase from the LDC as catalogue numbers LDC2005T28 and LDC2005T29<ref type="foot" coords="4,130.12,163.76,3.97,6.12" target="#foot_2">2</ref> .</p><p>The TREC 2004 HARD track used a corpus of news from 2003, had 49 topics with several metadata fields. Topics, relevance judgments, and clarification forms were provided.</p><p>The TREC 2003 HARD track corpus was a set of 372,219 documents totaling 1.7Gb from the 1999 portion of the AQUAINT corpus, along with some US government documents from the same year (Congressional Record and Federal Register). The topics were somewhat like standard TREC topics, but included lots of searcher and query metadata. Topics, relevance judgments, and clarification forms were provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Baseline submissions</head><p>Submissions of baseline runs were in the standard TREC submission format used for ad-hoc queries. Up to 1000 documents were provided in rank order for each of the 50 topics. The details were in a file with lines containing a topic number, a document ID, the document's rank against that topic, and its score (along with some other bits of bookkeeping information). Every topic was required to have at least one document retrieved, and it could have anywhere from one to 1,000 documents.</p><p>Sites were asked to provide the following information:</p><p>1. Was this an entirely automatic run or a manual run? Two baseline runs were manual, all others were automatic.</p><p>2. Did you use the title, description, and/or narrative fields for this run? The runs included 9 using just the title field, 3 using just description, 8 combining title and description, and 10 also adding in the narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>To what extent did you use earlier relevance judgments on the topics? One run claimed to have used the judgments of these topics against prior TREC corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A short description of the run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Preference in terms of judging of this run? Only one baseline run per site was included in the judging pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Clarification forms</head><p>All 16 participating sites submitted at least one clarification forms: two submitted one form, ten submitted two forms, and four sites submitted three. All submitted forms were filled out, even though the track guidelines only guaranteed that two would be.</p><p>Clarification forms were filled out by the NIST assessors using the following platform:</p><p>• Redhat Enterprise Linux, version "3 workstation"</p><p>• 20-inch LCD monitor with 1600x1200 resolution, true color (millions of colors)</p><p>• Firefox Web browser, v1.0.3</p><p>• No assumption that the machine is connected to any network at all. (The goal was to have it disconnected from all networks of any sort, but that proved infeasible in the NIST environment.)</p><p>In past years, the contents of the clarification forms were strictly controlled to allow only a limited subset of HTML. This year, virtually all restrictions were lifted, meaning that sites could include Javacript, Java, images, or the like. The following restrictions were made:</p><p>• The forms had to assume they were running on a computer that is disconnected from all networks, so all necessary information had to be included as part of the form. If it required multiple files, they all had to be within the same directory structure. Sites could not assume that all of its clarification forms would be on the same computer.</p><p>• It was not possible to invoke any cgi-bin scripts</p><p>• It was not possible to write to disk Clarification forms could be presented in almost any layout, but had to include the following items:</p><p>• &lt;form action="/cgi-bin/clarification submit.pl" method="post"&gt; This indicates the script where the output was generated (all it did was output the selected information).</p><p>• &lt;input type="hidden" name="site" value="XXXXn"&gt; Here, "XXXX" is a 4-letter code designating the site (provided in the lead-up to the baseline submission) and "n" is a run number. The run numbers reflected the priority order of the form. That is, XXXX1 will be processed then XXXX2 and so on.</p><p>• &lt;input type="hidden" name="topicid" value="000"&gt; Indicates the topic number, a 3-digit code with zeros padding as needed (001 rather than 01 or 1).</p><p>• &lt;input type="submit" name="send" value="submit"&gt; This is the submit button that had to appear somewhere on the page.</p><p>In addition, sites were strongly encouraged to include somewhere on the page the topic number (e.g., "303") and the title of the topic to provide a sanity check that the annotators were, indeed, answering the correct questions.</p><p>For each submission, all clarification forms were put in a single directory (folder) with the name indicated (e.g., NIST1). Each clarification form inside that directory was also a directory with the name of the submission and the topic number (e.g., NIST1 043 for topic 43 of the NIST1 submission).</p><p>Inside that directory, the main clarification form was called index.html. It could access any files from within the directory hierarchy, using relative pathnames. For example, "logo.gif" would refer to the file NIST1/NIST1 043/logo.gif within the directory structure, and "../logo.gif" would refer to NIST1/logo.gif".</p><p>Sites were asked the following information about each submitted form:</p><p>1. Did you use clustering to generate this form?</p><p>2. Did you use text summarization, either extractive or generative?</p><p>3. Did you use document-level feedback? That is, did you ask the user to judge an entire document for relevance, even if you did so using a title, passage, or keywords from the document?</p><p>4. Did you ask the user to judge selected passages of text, independent of the documents they came from?</p><p>5. Did you ask the user to judge keywords for relevance, independent of the documents they came from?</p><p>6. If you used any techniques not listed above, briefly list them at the bullet-list level of detail.</p><p>7. Did you use any sources of information beyond the query and AQUAINT corpus and, if so, what were they?</p><p>The assessors spent no more than three minutes per form no matter how complex the form was. The three minutes included time needed to load the form, initialize it, and do any rendering, so unusually complex or large forms were implicitly penalized. At the end of three minutes, if the assessor had not pressed the "submit" button, the form was timed out and forcibly submitted (anything entered up to that point was saved).</p><p>NIST recorded the time spent on the form returned for each form. That information was returned in a separate file along with all of the clarification form responses. Assessors were never permitted more than 180 seconds per form, but some of the reported times were greater than 180 because of the time it took for the system to "shut down" a form if the time limit expired.</p><p>Clarification forms were presented to annotators in an order to minimize the chance that one form would adversely (or positively) impact the use of another form. Tables <ref type="table" coords="6,355.72,343.98,4.98,8.74" target="#tab_2">1</ref> and<ref type="table" coords="6,386.99,343.98,4.98,8.74" target="#tab_4">2</ref> shows the rotation that was used for the submitted clarification forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Final submissions</head><p>Final submissions incorporated information gleaned from clarification forms and combined that with any other retrieval techniques to achieve the best run possible.</p><p>The following questions were asked for each submission:</p><p>1. Which of your baseline runs is an appropriate baseline? There were 26 submissions that indicated that the final run did not have a corresponding baseline run. This often reflected a site's providing a new "baseline" or trying out a technique that was developed after the baseline runs and so had no corresponding baseline.</p><p>2. Which of your clarification forms was used to generated this final run? There were 33 final runs that indicated they did not use a clarification form.</p><p>3. Other than the clarification form's being answered, was this an entirely automatic run or a manual run? Only four of the final runs were marked as being manual runs; the remaining 88 were automatic.</p><p>4. Did you use the title, description, and/or narrative fields for this run? Here, 28 runs used just the title, 2 used just the description, 39 combined the title and description, and 23 also included the narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>To what extent did you use earlier relevance judgments on the topics? A total of 13 runs indicated that they used the earlier relevance judgments.</p><p>6. A short description of the run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>What is the preference in terms of judging of this run? Only one final run from each site was included in the judging pool.  <ref type="table" coords="7,506.69,445.74,3.88,8.74" target="#tab_4">2</ref>). The rows of the table correspond to topics and the columns to clarification forms from sites. For example, the form indicates that NCAR's primary clarification form (NCAR1) will be the 28th considered for topic 1, the 29th for topic 2, ..., the 1st for topic 8, and so on. Similarly, for topic 1, the assessor first did INDI1's form (see Table <ref type="table" coords="7,119.37,493.56,3.88,8.74" target="#tab_2">1</ref>), then that for CASP1, then UIUC1's, followed by MEIJ1's, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Overview of submissions</head><p>As mentioned above, sites participated. The following statistics provide some details of the submissions. Note that the information is largely self-reported and has not been rigorously verified, so it is possible that it may be somewhat inaccurate.</p><p>A total of 30 baseline runs were submitted from 15 sites. One of those 15 sites made use of the earlier judgments for the topics (on a different corpus and using a different assessor).</p><p>A total of 35 sets of clarification forms were submitted. The average time per form on a single question was 116.5 seconds, with a minimum of five seconds and a maximum of 180 seconds. (In fact, one query's form reported taking 676 seconds, but the more than 8 additional minutes were presumably consumed by the system trying to force the form to close after the three minutes had expired.)</p><p>Every site had at least one form that took the full three minutes, and many had a dozen or two that took that long. The University of Massachusetts had the distinction of being the only site that used the full  three minutes of annotator time for every form. Those forms were apparently designed to collect as much information as possible during clarification time for later processing to determine which questions were most useful <ref type="bibr" coords="8,100.56,501.48,96.07,8.74" target="#b4">[Diaz and Allan, 2006]</ref>.</p><p>A total of 92 final runs were submitted across the 16 sites. Of those, three runs made use of the past judgments. Different sites used different parts of the topics for their runs:</p><p>• 28 runs were title-only queries</p><p>• 2 runs were description-only queries</p><p>• 38 runs combined the title and description</p><p>• 24 runs included the narrative along with the title and description All runs were automatic (not counting clarification form interaction) except for those submitted by the University of Maryland, where experiments used a trained intermediary to collect potentially useful information for a clarification form <ref type="bibr" coords="8,173.37,674.66,70.06,8.74" target="#b6">[Lin et al., 2006]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Discussion</head><p>System output was evaluated by R-precision, defined as precision at R documents retrieved, where R is the number of known relevant documents in the collection.</p><p>Figure <ref type="figure" coords="9,104.20,552.97,4.98,8.74" target="#fig_1">1</ref> shows overall performance as impacted by clarification forms. Recall that when a final run was submitted, sites were asked to indicate which of their baseline runs was used as a starting point. The graph includes a point for each such baseline-final pair. Because (by chance) different baseline runs never had the same score, points that make up vertical lines represent multiple final runs that used the same baseline run.</p><p>For example, the run at baseline R-precision of 0.3291 was used for four final runs that had R-precision ranging from 0.3024 to 0.3547.</p><p>Point colors and shape reflect which portions of the topic were used for the query, though the differences may not be easily visible in a grayscale print. The (excellent) outlier labeled "T+D (man)" in the upper right is the manual run from the University of Maryland. The red triangle at baseline 0.1599 and final 0.2581, labeled "Title (classic RF)", is a special run created by NIST, and is discussed further below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.1">General observations</head><p>Just considering baseline runs, the automatic runs had R-precision scores ranging from 0.1116 to 0.3291. Using the title, description, and narrative seemed to be helpful, since four different sites achieved comparable scores. However, some baselines without the narrative performed just as well, and the best automatic baseline used only the title.</p><p>Ultimately, the goal of the HARD track was to explore the value added by clarification forms. That means that it is the improvement from baseline to final that is more interesting. In the graph, points below the y = x line had final runs that were worse than their corresponding baseline runs; those above the line improved. Most of the sites were able to improve on their baseline performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.2">Classic relevance feedback</head><p>In previous years of the HARD track, there was a concern that simple relevance feedback of documents might be a simpler and more effective type of clarification form. To explore that issue this year, NIST volunteered to provide a form that was purely relevance feedback. To do that, NIST ran a baseline system and then created a clarification form that included the top-ranked documents, asking that they be judged as relevant or not. The baseline system was Prise3, a system based on the Lucene open source IR engine, so it used a tf-idf style of retrieval. Prise3 was the same system used to create new topics for other tracks this year. The title field to retrieve the top 50 documents.</p><p>The clarification form listed, along with the query's title and description, the title of the top 50 documents.</p><p>The assessor could click on a link to see the full text of a document if needed. The assessor used his or her three minutes to judge as many documents as possible, and then a new query was created using that information. Because Prise3 did not support relevance feedback at that time, the final run version 11.0 of the well-known SMART system. The system was tuned using the Robust 2004 topics on the past corpus, without paying any special attention ot the topics that were re-used for HARD this year. The tuning used the top-ranked five relevant documents on that corpus, as an estimate of what might come back from the clarification forms. The tuned parameters were the weighting scheme (ltc.lnc), the number of feedback terms to select (50), and the Rocchio parameters (α = 4, β = 2).</p><p>The red triangle in Figure <ref type="figure" coords="10,190.80,466.27,4.98,8.74" target="#fig_1">1</ref> shows the performance of the NIST feedback runs, where the baseline performance is the Prise3 system and the final performance is for the SMART system. The point is dramatically above the y = x line, showing the dramatic improvement (more than 60%) this approach can cause. Unfortunately, the baseline was substantially below the better-performing systems, making it difficult to know whether simple relevance feedback would be equally effective at different qualities of baseline system. The results suggest that having a "pure" relevance feedback clarification form from every system might be a useful point for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3">Results by query</head><p>To a limited degree, it appears that better performing baselines result in larger gains from the clarification form. Figure <ref type="figure" coords="10,131.23,613.39,4.98,8.74" target="#fig_2">2</ref> shows a breakdown of the same runs with each query represented. The graph shows a clear suggestion that it is easier to improve better-performing queries, but also demonstrates that poor-performing queries can be improved and have more room for improvement.</p><p>Another way of looking at the same question is to explore the absolute gain as a function of baseline Rprecision. Figure <ref type="figure" coords="10,149.88,669.79,4.98,8.74" target="#fig_3">3</ref> shows the same queries as Figure <ref type="figure" coords="10,307.87,669.79,3.88,8.74" target="#fig_2">2</ref>, but the y-axis shows the gain rather than value of R-precision. There is a very slight trend toward lower gain given higher baseline R-precision, but the fit is poor and the slope is almost horizontal. The graph suggests a strong negative correlation to the eye, but it is an artifact of the absolute loss being capped by the value of baseline R-precision-that is, if the baseline R-precision is 0.02, it is not possible to lose more than 0.02, but the gain can be quite large.</p><p>Figure <ref type="figure" coords="11,103.95,533.97,4.98,8.74" target="#fig_4">4</ref> shows the absolute gain as a function of the number of relevant documents in the query. Again, there is a very weak trend toward more gain given more relevant documents in the pool. But the graph very clearly shows that the variance of the gain is large across all queries, regardless of the number of relevant documents they have.</p><p>Finally we consider the possibility that gain is correlated with the amount of time spent in clarification forms. Figure <ref type="figure" coords="11,135.89,602.32,4.98,8.74" target="#fig_5">5</ref> shows that having annotators spend more time providing clarification information did not in and of itself increase realized gain. (Any effect may be obscured because a third of the interactions with annotators were truncated at 180 seconds, meaning we do not know how much time they actually might have spent.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.4">Comparing two individual runs</head><p>It is illuminating to compare two runs that did well in the overall evaluation. We will consider the top performing title-only queries from two different groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Run MASStrmS is the automatic run with highest final R-precision. It started with baseline run</head><p>MASSbaseTEE3 that had R-precision of 0.3291. It incorporated information from clarification form MASS1 and then achieved a final R-precision of 0.3547. That represents a 0.0256 gain in R-precision, an 8% relative improvement.</p><p>2. Run UIUChCFB3 is the automatic run with second highest final R-precision. It started with baseline run UIUC05Hardb0 that had R-precision of 0.2723. It incorporated information from clarification form UIUC3 and then achieved a final R-precision of 0.3355. That represents a 0.0623 gain in R-precision, a 23% relative improvement.</p><p>Figure <ref type="figure" coords="12,103.04,658.17,4.98,8.74" target="#fig_6">6</ref> shows scatterplots of baseline and final R-precision values for the two runs, with UMass' run on the left and UIUC's on the right. For most queries in the UMass results, the final runs are almost identical to the baseline runs. However, a handful of queries with very low baseline scores show remarkable improvement, accounting for most of the gain in that system. This run appears to represent a very conservative query modification strategy, a reasonable choice given the high quality baseline. The UIUC run, in contrast, shows dramatic changes between the baseline and final runs. A large number of queries improve and a handful are significantly harmed. The strategy here is clearly much riskier and often pays off handsomely, trimming much of the baseline performance difference between UMass and UIUC.</p><p>Finally we do a direct comparison of how queries performed in the two systems. Figure <ref type="figure" coords="13,449.08,522.01,4.98,8.74" target="#fig_7">7</ref> has an entry on the x-axis for every query. The queries are sorted by the final R-precision value of the query in the UIUChCFB3 run, the solid (blue) line that degrades smoothly from the upper left to the lower right. The corresponding baseline performance for that query is represented by (blue) diamonds.</p><p>The UMASStrmS final R-precision values are represented by the jagged (brown) line that roughly follows the trend of the UIUChCFB3 line, with the (red) triangles indicating baseline effectiveness.</p><p>Query effectiveness at the two sites follows a similar trend, but huge differences are common, with each site out-performing another by large margins in some cases. For example, query 651 show comparable baseline performance for the two sites, but successful clarification only by UIUC. Query 409 shows roughly the opposite result. Query 389 shows a case where UMass had substantially higher baseline performance, but UIUC's final run topped the UMass effectiveness by a good bit.</p><p>Comparing two systems provides only a glimpse of what is happening during clarification and final runs. It does suggest that different approaches work better for different queries, leading to the obvious question of whether it is possible to combine the clarification forms or to predict when one style is likely to be more </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Conclusion</head><p>Several sites were able to show appreciable average gains from using clarification forms. None of the gains was consistently dramatic, however, begging the question of whether the time spent clarifying a query was a worthwhile investment. Further amplifying that question, it is worth pointing out that the best best automatic Robust track run beat all of the automatic baseline and final HARD track runs. (Of course, it is unknown whether a clarification form based on that run would improve the results further.)</p><p>This year there was an interesting variety of clarification forms tried. Forms of user-assisted query expansion were very popular, but sites also considered relationships between terms <ref type="bibr" coords="14,442.45,654.27,41.92,8.74" target="#b4">[Diaz and</ref><ref type="bibr" coords="14,487.70,654.27,52.31,8.74;14,72.00,666.22,74.90,8.74">Allan, 2006, Yang et al., 2006]</ref>, passage feedback <ref type="bibr" coords="14,234.17,666.22,96.08,8.74" target="#b4">[Diaz and Allan, 2006]</ref>, incorporated summarization <ref type="bibr" coords="14,466.80,666.22,68.95,8.74" target="#b5">[Jin et al., 2006]</ref>, and even used elaborate visualizations based on self organizing maps <ref type="bibr" coords="14,379.37,678.18,83.21,8.74" target="#b4">[He and Ahn, 2006]</ref>. The track itself did not provide clear support for any of these approaches. It is important to note that the clarification forms do not represent "interactive information retrieval" experiments. They provide a highly focused and very limited type of interaction that can (potentially) improve the effectiveness of document retrieval. Whether these clarification forms can be deployed in a way that pleases a user or that will actually be used is an entirely different question, one that would have to be tested in a more realistic environment.</p><p>After a three-year run, TREC 2005 was the end of the HARD track. For TREC 2006, it is being made part of the Question Answering track as "ciQA", or "complex, interactive Question Answering." The goal of ciQA is to investigate interactive approaches to cope with complex information needs specified by a templated query.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,234.59,87.50,143.04,13.01;9,116.21,375.46,6.51,10.83;9,99.93,348.32,22.78,10.83;9,106.43,321.21,16.27,10.83;9,99.93,294.07,22.78,10.83;9,106.43,266.96,16.27,10.83;9,99.93,239.81,22.78,10.83;9,106.43,212.67,16.27,10.83;9,99.93,185.56,22.78,10.83;9,106.43,158.42,16.27,10.83;9,99.93,131.31,22.78,10.83;9,106.43,104.17,16.27,10.83;9,128.44,392.27,6.51,10.83;9,199.40,392.27,16.27,10.83;9,275.23,392.27,16.27,10.83;9,351.11,392.27,16.27,10.83;9,426.95,392.27,16.27,10.83;9,502.78,392.27,16.27,10.83;9,277.38,411.29,87.85,10.83;9,84.14,247.15,10.83,30.60;9,84.14,210.68,10.83,33.20;9,405.82,249.34,22.12,10.83;9,405.82,264.92,26.67,10.83;9,405.82,280.49,22.44,10.83;9,405.82,296.07,56.27,10.83;9,405.82,311.65,52.04,10.83;9,405.82,327.22,87.16,10.83;9,405.82,342.80,18.55,10.83"><head>Final</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,72.00,445.74,468.00,8.74;9,72.00,457.70,275.39,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of R-precision values in baseline runs and runs after using a clarification form (only runs that identified a corresponding baseline run are included).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,72.00,445.74,468.00,8.74;11,72.00,457.70,468.00,8.74;11,72.00,469.65,93.32,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of R-precision values in baseline and final runs on a query-by-query basis. Each query from each run pair represented in Figure 1 is represented by a point. The y = x line is shown as well as a linearly fit trend line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,132.39,445.74,347.23,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing absolute gain in R-precision to baseline R-precision value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,88.86,445.74,434.29,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparing absolute gain in R-precision to the number of relevant documents for a query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,72.00,445.74,467.99,8.74;14,72.00,457.70,467.99,8.74;14,72.00,469.65,431.64,8.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Comparing absolute gain in R-precision to time spent in the clarification form. Note that the density of scores at 180 seconds corresponds to the maximum time allowed in a form. The handful of scores beyond 180 seconds represent clarification forms that were difficult to "shut down" (see Section 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,72.00,263.66,467.99,8.74;15,72.00,275.62,212.37,8.74"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of baseline and final R-precision values for the MASStrmS run (left) and for the UIUChCFB3 run (right), broken down by query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,72.00,445.74,468.00,8.74;16,72.00,457.70,468.00,8.74;16,72.00,469.65,183.82,8.74"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A query-by-query comparison of baseline and final R-precision values for the MASStrmS and UIUChCFB3 runs in Figure 6. Each query is a point on the x-axis; the queries are ordered by the final R-precision score of the UIUChCFB3 run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,81.16,441.34,373.32"><head>Table 1 :</head><label>1</label><figDesc>Rotation used to fill out clarification forms (the right edge of the table continues in Table</figDesc><table coords="7,98.63,81.16,414.71,340.29"><row><cell>NCAR1</cell><cell>MARY1</cell><cell>INDI2</cell><cell>STRA2</cell><cell>UIUC3</cell><cell>UIUC1</cell><cell>NCAR3</cell><cell>TWEN2</cell><cell>PITT1</cell><cell>YORK2</cell><cell>CASP1</cell><cell>CASS2</cell><cell>NCAR2</cell><cell>PITT2</cell><cell>MASS1</cell><cell>SAIC1</cell><cell>YORK1</cell><cell></cell></row><row><cell>T1</cell><cell>28</cell><cell></cell><cell>23</cell><cell>5</cell><cell>19</cell><cell>3</cell><cell>20</cell><cell>12</cell><cell>15</cell><cell>16</cell><cell>2</cell><cell>17</cell><cell>32</cell><cell>22</cell><cell>29</cell><cell>7</cell><cell>21</cell></row><row><cell>T2</cell><cell>29</cell><cell></cell><cell>24</cell><cell>6</cell><cell>20</cell><cell>4</cell><cell>21</cell><cell>13</cell><cell>16</cell><cell>17</cell><cell>3</cell><cell>18</cell><cell>33</cell><cell>23</cell><cell>30</cell><cell>8</cell><cell>22</cell></row><row><cell>T3</cell><cell>30</cell><cell></cell><cell>25</cell><cell>7</cell><cell>21</cell><cell>5</cell><cell>22</cell><cell>14</cell><cell>17</cell><cell>18</cell><cell>4</cell><cell>19</cell><cell>34</cell><cell>24</cell><cell>31</cell><cell>9</cell><cell>23</cell></row><row><cell>T4</cell><cell>31</cell><cell></cell><cell>26</cell><cell>8</cell><cell>22</cell><cell>6</cell><cell>23</cell><cell>15</cell><cell>18</cell><cell>19</cell><cell>5</cell><cell>20</cell><cell>1</cell><cell>25</cell><cell>32</cell><cell>10</cell><cell>24</cell></row><row><cell>T5</cell><cell>32</cell><cell></cell><cell>27</cell><cell>9</cell><cell>23</cell><cell>7</cell><cell>24</cell><cell>16</cell><cell>19</cell><cell>20</cell><cell>6</cell><cell>21</cell><cell>2</cell><cell>26</cell><cell>33</cell><cell>11</cell><cell>25</cell></row><row><cell>T6</cell><cell>33</cell><cell></cell><cell>28</cell><cell>10</cell><cell>24</cell><cell>8</cell><cell>25</cell><cell>17</cell><cell>20</cell><cell>21</cell><cell>7</cell><cell>22</cell><cell>3</cell><cell>27</cell><cell>34</cell><cell>12</cell><cell>26</cell></row><row><cell>T7</cell><cell>34</cell><cell></cell><cell>29</cell><cell>11</cell><cell>25</cell><cell>9</cell><cell>26</cell><cell>18</cell><cell>21</cell><cell>22</cell><cell>8</cell><cell>23</cell><cell>4</cell><cell>28</cell><cell>1</cell><cell>13</cell><cell>27</cell></row><row><cell>T8</cell><cell>1</cell><cell></cell><cell>30</cell><cell>12</cell><cell>26</cell><cell>10</cell><cell>27</cell><cell>19</cell><cell>22</cell><cell>23</cell><cell>9</cell><cell>24</cell><cell>5</cell><cell>29</cell><cell>2</cell><cell>14</cell><cell>28</cell></row><row><cell>T9</cell><cell>2</cell><cell></cell><cell>31</cell><cell>13</cell><cell>27</cell><cell>11</cell><cell>28</cell><cell>20</cell><cell>23</cell><cell>24</cell><cell>10</cell><cell>25</cell><cell>6</cell><cell>30</cell><cell>3</cell><cell>15</cell><cell>29</cell></row><row><cell>T10</cell><cell>3</cell><cell></cell><cell>32</cell><cell>14</cell><cell>28</cell><cell>12</cell><cell>29</cell><cell>21</cell><cell>24</cell><cell>25</cell><cell>11</cell><cell>26</cell><cell>7</cell><cell>31</cell><cell>4</cell><cell>16</cell><cell>30</cell></row><row><cell>T11</cell><cell>4</cell><cell></cell><cell>33</cell><cell>15</cell><cell>29</cell><cell>13</cell><cell>30</cell><cell>22</cell><cell>25</cell><cell>26</cell><cell>12</cell><cell>27</cell><cell>8</cell><cell>32</cell><cell>5</cell><cell>17</cell><cell>31</cell></row><row><cell>T12</cell><cell>5</cell><cell></cell><cell>34</cell><cell>16</cell><cell>30</cell><cell>14</cell><cell>31</cell><cell>23</cell><cell>26</cell><cell>27</cell><cell>13</cell><cell>28</cell><cell>9</cell><cell>33</cell><cell>6</cell><cell>18</cell><cell>32</cell></row><row><cell>T13</cell><cell>6</cell><cell></cell><cell>1</cell><cell>17</cell><cell>31</cell><cell>15</cell><cell>32</cell><cell>24</cell><cell>27</cell><cell>28</cell><cell>14</cell><cell>29</cell><cell>10</cell><cell>34</cell><cell>7</cell><cell>19</cell><cell>33</cell></row><row><cell>T14</cell><cell>7</cell><cell></cell><cell>2</cell><cell>18</cell><cell>32</cell><cell>16</cell><cell>33</cell><cell>25</cell><cell>28</cell><cell>29</cell><cell>15</cell><cell>30</cell><cell>11</cell><cell>1</cell><cell>8</cell><cell>20</cell><cell>34</cell></row><row><cell>T15</cell><cell>8</cell><cell></cell><cell>3</cell><cell>19</cell><cell>33</cell><cell>17</cell><cell>34</cell><cell>26</cell><cell>29</cell><cell>30</cell><cell>16</cell><cell>31</cell><cell>12</cell><cell>2</cell><cell>9</cell><cell>21</cell><cell>1</cell></row><row><cell>T16</cell><cell>9</cell><cell></cell><cell>4</cell><cell>20</cell><cell>34</cell><cell>18</cell><cell>1</cell><cell>27</cell><cell>30</cell><cell>31</cell><cell>17</cell><cell>32</cell><cell>13</cell><cell>3</cell><cell>10</cell><cell>22</cell><cell>2</cell></row><row><cell>T17</cell><cell>10</cell><cell></cell><cell>5</cell><cell>21</cell><cell>1</cell><cell>19</cell><cell>2</cell><cell>28</cell><cell>31</cell><cell>32</cell><cell>18</cell><cell>33</cell><cell>14</cell><cell>4</cell><cell>11</cell><cell>23</cell><cell>3</cell></row><row><cell>T18</cell><cell>11</cell><cell></cell><cell>6</cell><cell>22</cell><cell>2</cell><cell>20</cell><cell>3</cell><cell>29</cell><cell>32</cell><cell>33</cell><cell>19</cell><cell>34</cell><cell>15</cell><cell>5</cell><cell>12</cell><cell>24</cell><cell>4</cell></row><row><cell>T19</cell><cell>12</cell><cell></cell><cell>7</cell><cell>23</cell><cell>3</cell><cell>21</cell><cell>4</cell><cell>30</cell><cell>33</cell><cell>34</cell><cell>20</cell><cell>1</cell><cell>16</cell><cell>6</cell><cell>13</cell><cell>25</cell><cell>5</cell></row><row><cell>T20</cell><cell>13</cell><cell></cell><cell>8</cell><cell>24</cell><cell>4</cell><cell>22</cell><cell>5</cell><cell>31</cell><cell>34</cell><cell>1</cell><cell>21</cell><cell>2</cell><cell>17</cell><cell>7</cell><cell>14</cell><cell>26</cell><cell>6</cell></row><row><cell>T21</cell><cell>14</cell><cell></cell><cell>9</cell><cell>25</cell><cell>5</cell><cell>23</cell><cell>6</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>22</cell><cell>3</cell><cell>18</cell><cell>8</cell><cell>15</cell><cell>27</cell><cell>7</cell></row><row><cell>T22</cell><cell>15</cell><cell></cell><cell>10</cell><cell>26</cell><cell>6</cell><cell>24</cell><cell>7</cell><cell>33</cell><cell>2</cell><cell>3</cell><cell>23</cell><cell>4</cell><cell>19</cell><cell>9</cell><cell>16</cell><cell>28</cell><cell>8</cell></row><row><cell>T23</cell><cell>16</cell><cell></cell><cell>11</cell><cell>27</cell><cell>7</cell><cell>25</cell><cell>8</cell><cell>34</cell><cell>3</cell><cell>4</cell><cell>24</cell><cell>5</cell><cell>20</cell><cell>10</cell><cell>17</cell><cell>29</cell><cell>9</cell></row><row><cell>T24</cell><cell>1 7</cell><cell>1</cell><cell>1 2</cell><cell>2 8</cell><cell>8</cell><cell>2 6</cell><cell>9</cell><cell>1</cell><cell>4</cell><cell>5</cell><cell>2 5</cell><cell>6</cell><cell>2 1</cell><cell>1 1</cell><cell>1 8</cell><cell>3 0</cell><cell>1 0</cell></row><row><cell>T25</cell><cell>18</cell><cell></cell><cell>13</cell><cell>29</cell><cell>9</cell><cell>27</cell><cell>10</cell><cell>2</cell><cell>5</cell><cell>6</cell><cell>26</cell><cell>7</cell><cell>22</cell><cell>12</cell><cell>19</cell><cell>31</cell><cell>11</cell></row><row><cell>T26</cell><cell>19</cell><cell></cell><cell>14</cell><cell>30</cell><cell>10</cell><cell>28</cell><cell>11</cell><cell>3</cell><cell>6</cell><cell>7</cell><cell>27</cell><cell>8</cell><cell>23</cell><cell>13</cell><cell>20</cell><cell>32</cell><cell>12</cell></row><row><cell>T27</cell><cell>20</cell><cell></cell><cell>15</cell><cell>31</cell><cell>11</cell><cell>29</cell><cell>12</cell><cell>4</cell><cell>7</cell><cell>8</cell><cell>28</cell><cell>9</cell><cell>24</cell><cell>14</cell><cell>21</cell><cell>33</cell><cell>13</cell></row><row><cell>T28</cell><cell>21</cell><cell></cell><cell>16</cell><cell>32</cell><cell>12</cell><cell>30</cell><cell>13</cell><cell>5</cell><cell>8</cell><cell>9</cell><cell>29</cell><cell>10</cell><cell>25</cell><cell>15</cell><cell>22</cell><cell>34</cell><cell>14</cell></row><row><cell>T29</cell><cell>22</cell><cell></cell><cell>17</cell><cell>33</cell><cell>13</cell><cell>31</cell><cell>14</cell><cell>6</cell><cell>9</cell><cell>10</cell><cell>30</cell><cell>11</cell><cell>26</cell><cell>16</cell><cell>23</cell><cell>1</cell><cell>15</cell></row><row><cell>T30</cell><cell>23</cell><cell></cell><cell>18</cell><cell>34</cell><cell>14</cell><cell>32</cell><cell>15</cell><cell>7</cell><cell>10</cell><cell>11</cell><cell>31</cell><cell>12</cell><cell>27</cell><cell>17</cell><cell>24</cell><cell>2</cell><cell>16</cell></row><row><cell>T31</cell><cell>24</cell><cell></cell><cell>19</cell><cell>1</cell><cell>15</cell><cell>33</cell><cell>16</cell><cell>8</cell><cell>11</cell><cell>12</cell><cell>32</cell><cell>13</cell><cell>28</cell><cell>18</cell><cell>25</cell><cell>3</cell><cell>17</cell></row><row><cell>T32</cell><cell>25</cell><cell></cell><cell>20</cell><cell>2</cell><cell>16</cell><cell>34</cell><cell>17</cell><cell>9</cell><cell>12</cell><cell>13</cell><cell>33</cell><cell>14</cell><cell>29</cell><cell>19</cell><cell>26</cell><cell>4</cell><cell>18</cell></row><row><cell>T33</cell><cell>26</cell><cell></cell><cell>21</cell><cell>3</cell><cell>17</cell><cell>1</cell><cell>18</cell><cell>10</cell><cell>13</cell><cell>14</cell><cell>34</cell><cell>15</cell><cell>30</cell><cell>20</cell><cell>27</cell><cell>5</cell><cell>19</cell></row><row><cell>T34</cell><cell>27</cell><cell></cell><cell>22</cell><cell>4</cell><cell>18</cell><cell>2</cell><cell>19</cell><cell>11</cell><cell>14</cell><cell>15</cell><cell>1</cell><cell>16</cell><cell>31</cell><cell>21</cell><cell>28</cell><cell>6</cell><cell>20</cell></row><row><cell>T35</cell><cell>28</cell><cell></cell><cell>23</cell><cell>5</cell><cell>19</cell><cell>3</cell><cell>20</cell><cell>12</cell><cell>15</cell><cell>16</cell><cell>2</cell><cell>17</cell><cell>32</cell><cell>22</cell><cell>29</cell><cell>7</cell><cell>21</cell></row><row><cell>T36</cell><cell>29</cell><cell></cell><cell>24</cell><cell>6</cell><cell>20</cell><cell>4</cell><cell>21</cell><cell>13</cell><cell>16</cell><cell>17</cell><cell>3</cell><cell>18</cell><cell>33</cell><cell>23</cell><cell>30</cell><cell>8</cell><cell>22</cell></row><row><cell>T37</cell><cell>30</cell><cell></cell><cell>25</cell><cell>7</cell><cell>21</cell><cell>5</cell><cell>22</cell><cell>14</cell><cell>17</cell><cell>18</cell><cell>4</cell><cell>19</cell><cell>34</cell><cell>24</cell><cell>31</cell><cell>9</cell><cell>23</cell></row><row><cell>T38</cell><cell>31</cell><cell></cell><cell>26</cell><cell>8</cell><cell>22</cell><cell>6</cell><cell>23</cell><cell>15</cell><cell>18</cell><cell>19</cell><cell>5</cell><cell>20</cell><cell>1</cell><cell>25</cell><cell>32</cell><cell>10</cell><cell>24</cell></row><row><cell>T39</cell><cell>32</cell><cell></cell><cell>27</cell><cell>9</cell><cell>23</cell><cell>7</cell><cell>24</cell><cell>16</cell><cell>19</cell><cell>20</cell><cell>6</cell><cell>21</cell><cell>2</cell><cell>26</cell><cell>33</cell><cell>11</cell><cell>25</cell></row><row><cell>T40</cell><cell>33</cell><cell></cell><cell>28</cell><cell>10</cell><cell>24</cell><cell>8</cell><cell>25</cell><cell>17</cell><cell>20</cell><cell>21</cell><cell>7</cell><cell>22</cell><cell>3</cell><cell>27</cell><cell>34</cell><cell>12</cell><cell>26</cell></row><row><cell>T41</cell><cell>34</cell><cell></cell><cell>29</cell><cell>11</cell><cell>25</cell><cell>9</cell><cell>26</cell><cell>18</cell><cell>21</cell><cell>22</cell><cell>8</cell><cell>23</cell><cell>4</cell><cell>28</cell><cell>1</cell><cell>13</cell><cell>27</cell></row><row><cell>T42</cell><cell>1</cell><cell></cell><cell>30</cell><cell>12</cell><cell>26</cell><cell>10</cell><cell>27</cell><cell>19</cell><cell>22</cell><cell>23</cell><cell>9</cell><cell>24</cell><cell>5</cell><cell>29</cell><cell>2</cell><cell>14</cell><cell>28</cell></row><row><cell>T43</cell><cell>2</cell><cell></cell><cell>31</cell><cell>13</cell><cell>27</cell><cell>11</cell><cell>28</cell><cell>20</cell><cell>23</cell><cell>24</cell><cell>10</cell><cell>25</cell><cell>6</cell><cell>30</cell><cell>3</cell><cell>15</cell><cell>29</cell></row><row><cell>T44</cell><cell>3</cell><cell></cell><cell>32</cell><cell>14</cell><cell>28</cell><cell>12</cell><cell>29</cell><cell>21</cell><cell>24</cell><cell>25</cell><cell>11</cell><cell>26</cell><cell>7</cell><cell>31</cell><cell>4</cell><cell>16</cell><cell>30</cell></row><row><cell>T45</cell><cell>4</cell><cell></cell><cell>33</cell><cell>15</cell><cell>29</cell><cell>13</cell><cell>30</cell><cell>22</cell><cell>25</cell><cell>26</cell><cell>12</cell><cell>27</cell><cell>8</cell><cell>32</cell><cell>5</cell><cell>17</cell><cell>31</cell></row><row><cell>T46</cell><cell>5</cell><cell></cell><cell>34</cell><cell>16</cell><cell>30</cell><cell>14</cell><cell>31</cell><cell>23</cell><cell>26</cell><cell>27</cell><cell>13</cell><cell>28</cell><cell>9</cell><cell>33</cell><cell>6</cell><cell>18</cell><cell>32</cell></row><row><cell>T47</cell><cell>6</cell><cell></cell><cell>1</cell><cell>17</cell><cell>31</cell><cell>15</cell><cell>32</cell><cell>24</cell><cell>27</cell><cell>28</cell><cell>14</cell><cell>29</cell><cell>10</cell><cell>34</cell><cell>7</cell><cell>19</cell><cell>33</cell></row><row><cell>T48</cell><cell>7</cell><cell></cell><cell>2</cell><cell>18</cell><cell>32</cell><cell>16</cell><cell>33</cell><cell>25</cell><cell>28</cell><cell>29</cell><cell>15</cell><cell>30</cell><cell>11</cell><cell>1</cell><cell>8</cell><cell>20</cell><cell>34</cell></row><row><cell>T49</cell><cell>8</cell><cell></cell><cell>3</cell><cell>19</cell><cell>33</cell><cell>17</cell><cell>34</cell><cell>26</cell><cell>29</cell><cell>30</cell><cell>16</cell><cell>31</cell><cell>12</cell><cell>2</cell><cell>9</cell><cell>21</cell><cell>1</cell></row><row><cell>T50</cell><cell>9</cell><cell></cell><cell>4</cell><cell>20</cell><cell>34</cell><cell>18</cell><cell>1</cell><cell>27</cell><cell>30</cell><cell>31</cell><cell>17</cell><cell>32</cell><cell>13</cell><cell>3</cell><cell>10</cell><cell>22</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,135.17,445.74,341.67,8.74"><head>Table 2 :</head><label>2</label><figDesc>Continuation of Table 1; this table appears to the right of that table.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,87.24,697.28,343.62,6.99"><p>At http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002T31 as of May</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2006" xml:id="foot_1" coords=""><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="4,87.24,697.78,452.75,6.99;4,72.00,707.25,83.31,6.99"><p>Described at http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2005T28 and . . . LDC2005T29, respectively, as of May 2006.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>As do most tracks in TREC, the HARD track owes a debt of gratitude to <rs type="institution">Ellen Voorhees at NIST</rs>, who not only provided the typical TREC infrastructure, but who was an important contributor in the shaping of the track's goals and evaluation approaches. The Linguistic Data Consortium kindly provided training data to participants and words of wisdom to NIST organizers. <rs type="person">Stephanie Strassel</rs> and <rs type="person">Meghan Glenn</rs> of the LDC were particularly generous with their time and support.</p><p>Finally, the track would not have been successful without the involvement of its participants-not only by participating, but by designing the track during discussions in person and on-line. <rs type="person">Diane Kelly</rs> at the <rs type="institution">University of North Carolina</rs> should be singled out for rapid production of the rotation tables used to assign clarification forms to annotators.</p><p>The track organization at UMass was supported in part by the <rs type="funder">Center for Intelligent Information Retrieval</rs> and in part by <rs type="funder">SPAWARSYSCEN-SD</rs> grant number <rs type="grantNumber">N66001-02-1-8903</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the author's and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5bb2eTa">
					<idno type="grant-number">N66001-02-1-8903</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,76.15,523.98,463.85,8.74;16,82.52,535.94,457.48,8.74;16,82.52,547.89,87.26,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,210.94,523.98,329.06,8.74;16,82.52,535.94,23.80,8.74">HARD track overview in TREC 2003: High accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan ; Allan</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="16,131.20,535.94,117.55,8.74">Proceedings of TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,76.15,566.50,463.85,8.74;16,82.52,578.45,457.48,8.74;16,82.52,590.41,87.26,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,210.94,566.50,329.06,8.74;16,82.52,578.45,23.80,8.74">HARD track overview in TREC 2004: High accuracy retrieval from documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan ; Allan</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="16,131.20,578.45,117.55,8.74">Proceedings of TREC 2004</title>
		<meeting>TREC 2004</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,609.01,468.00,8.74;16,82.52,620.97,457.49,8.74;16,82.52,632.92,134.86,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,200.62,620.97,188.49,8.74">University of Strathclyde at TREC HARD</title>
		<author>
			<persName coords=""><forename type="first">Baillie</forename></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="16,416.98,620.97,118.39,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,651.53,468.00,8.74;16,82.52,663.48,457.48,8.74;16,82.52,675.44,308.72,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,285.17,663.48,254.82,8.74;16,82.52,675.44,26.13,8.74">Rutgers information interaction lab at TREC 2005: Trying HARD</title>
		<author>
			<persName coords=""><surname>Belkin</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="16,132.04,675.44,115.29,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,72.00,694.04,468.00,8.74;16,82.52,706.00,454.20,8.74;17,72.00,75.16,468.00,8.74;17,82.52,87.11,205.97,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,306.35,694.04,233.65,8.74;16,82.52,706.00,173.60,8.74;17,282.62,75.16,186.87,8.74">When less is more: Relevance feedback falls short and term expansion succeeds at HARD 2005</title>
		<author>
			<persName coords=""><forename type="first">Allan</forename><forename type="middle">;</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="16,277.52,706.00,115.29,8.74;17,490.34,75.16,49.66,8.74;17,82.52,87.11,62.07,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005<address><addrLine>Ahn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006. 2006. 2006</date>
		</imprint>
	</monogr>
	<note>Proceedings of TREC 2005</note>
</biblStruct>

<biblStruct coords="17,72.00,107.04,468.00,8.74;17,82.52,118.99,334.20,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,341.26,107.04,198.74,8.74;17,82.52,118.99,53.99,8.74">SAIC &amp; University of Virginia at TREC 2005: HARD track</title>
		<author>
			<persName coords=""><forename type="first">Jin</forename></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,157.53,118.99,115.29,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,138.92,468.00,8.74;17,82.52,150.87,342.75,8.74;17,72.00,170.80,468.00,8.74;17,82.52,182.75,391.21,8.74;17,72.00,202.68,468.00,8.74;17,82.52,214.64,457.49,8.74;17,82.52,226.59,162.41,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,295.29,138.92,244.71,8.74;17,82.52,150.87,40.68,8.74;17,417.96,170.80,122.05,8.74;17,82.52,182.75,110.56,8.74;17,532.53,202.68,7.47,8.74;17,82.52,214.64,341.68,8.74">A menagerie of tracks at maryland: HARD, enterprise, QA, and genomics, oh my</title>
		<author>
			<persName coords=""><forename type="first">Fu ;</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kudo</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,166.08,150.87,115.29,8.74;17,214.54,182.75,115.29,8.74;17,447.14,214.64,92.86,8.74;17,82.52,226.59,18.51,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005">2006. 2006. 2005. 2006. 2006. 2006. 2006</date>
		</imprint>
	</monogr>
	<note>Proceedings of TREC 2005</note>
</biblStruct>

<biblStruct coords="17,72.00,246.52,468.00,8.74;17,82.52,258.47,205.97,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,285.67,246.52,184.25,8.74">NLPR at TREC 2005: HARD experiments</title>
		<author>
			<persName coords=""><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhao ; Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,490.34,246.52,49.66,8.74;17,82.52,258.47,62.07,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,278.40,467.99,8.74;17,82.52,290.35,433.25,8.74;17,72.00,310.28,468.00,8.74;17,82.52,322.23,457.48,8.74;17,82.52,334.19,99.43,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,522.85,278.40,17.15,8.74;17,82.52,290.35,152.66,8.74;17,394.78,310.28,145.22,8.74;17,82.52,322.23,278.02,8.74">Interactive construction of query language models -UIUC TREC 2005 HARD track experiments</title>
		<author>
			<persName coords=""><surname>Rode</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,256.58,290.35,115.29,8.74;17,382.64,322.23,115.73,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006. 2006. 2006</date>
		</imprint>
	</monogr>
	<note>Proceedings of TREC 2005</note>
</biblStruct>

<biblStruct coords="17,72.00,354.11,468.00,8.74;17,82.52,366.07,405.63,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,468.34,354.11,71.66,8.74;17,82.52,366.07,125.45,8.74">Experiments for HARD and Enterprise tracks</title>
		<author>
			<persName coords=""><surname>Vechtomova</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,228.96,366.07,115.29,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,76.46,385.99,463.53,8.74;17,82.52,397.95,205.97,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,254.20,385.99,216.05,8.74">Overview of the TREC 2005 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees ; Voorhees</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,490.34,385.99,49.66,8.74;17,82.52,397.95,62.07,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,417.87,468.00,8.74;17,82.52,429.83,334.20,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,397.70,417.87,142.30,8.74;17,82.52,429.83,53.99,8.74">York University at TREC 2005: HARD track</title>
		<author>
			<persName coords=""><forename type="first">Wen</forename></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,157.53,429.83,115.29,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,449.75,468.00,8.74;17,82.52,461.71,457.49,8.74;17,82.52,473.66,194.26,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,200.55,461.71,254.84,8.74">WIDIT in TREC 2005 HARD, Robust, and SPAM tracks</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,478.21,461.71,61.80,8.74;17,82.52,473.66,50.36,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,72.00,493.59,468.00,8.74;17,82.52,505.54,457.48,8.74;17,82.52,517.50,87.26,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,404.42,493.59,135.59,8.74;17,82.52,505.54,256.92,8.74">Relevance feedback by exploring the different feedback sources and collection structure</title>
		<author>
			<persName coords=""><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="17,364.56,505.54,118.09,8.74">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
