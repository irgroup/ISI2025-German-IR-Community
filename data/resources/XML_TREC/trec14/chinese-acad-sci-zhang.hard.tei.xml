<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.52,82.43,392.22,14.40;1,184.80,113.63,225.69,14.40">Relevance Feedback by Exploring the Different Feedback Sources and Collection Structure</title>
				<funder ref="#_9HrAY4E">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_dsZVu7H">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,164.94,154.91,69.36,9.73"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
							<email>junlin01@iscas.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Open System &amp; Chinese Information Processing Center Institute of Software</orgName>
								<orgName type="institution">The Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.30,154.91,33.36,9.73"><forename type="first">Le</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Open System &amp; Chinese Information Processing Center Institute of Software</orgName>
								<orgName type="institution">The Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.66,154.91,57.36,9.73"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Open System &amp; Chinese Information Processing Center Institute of Software</orgName>
								<orgName type="institution">The Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.02,154.91,51.36,9.73"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Open System &amp; Chinese Information Processing Center Institute of Software</orgName>
								<orgName type="institution">The Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.52,82.43,392.22,14.40;1,184.80,113.63,225.69,14.40">Relevance Feedback by Exploring the Different Feedback Sources and Collection Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BC9D63DBB01B62A8C23EB79DF1C080EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In HARD track of HARD 2005, we classify the 50 queries into 7 categories and make use of 3 kinds of feedback sources in various tasks. We find that the different kinds of queries perform differently in feedback tasks and the "CASE " and "EVENT" queries are more sensitive to the feedback source. We also explore the internal structure of corpus and try to estimate the distribution of relevant documents within sub-collections. The experiments show that this technology is partly effective and the main existing problem is how to predict the distribution more precisely.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We participated in Hard track of HARD 2005 and our research mainly focuses on the following 3 aspects: 1. To classify all 50 hard queries into 7 categories to see whether the different kind of queries have various effects in feedback tasks; 2.Try to use various feedback sources to observe whether they perform equally in feedback tasks; 3.To explore the internal structure of corpus and try to estimate the distribution of relevant documents within sub-corpus according to the relevance feedback results.</p><p>We can draw the preliminary conclusions from the experimental results: 1.The different kinds of queries perform differently in feedback tasks and the "CASE " and "EVENT" queries are more sensitive to the feedback source. 2. The technology of exploring the internal structure of corpus in feedback task is partly effective and the main existing problem is how to predict the distribution more precisely.</p><p>In the following parts of the paper, we will describe our research goals and experimental results more clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Research Goals</head><p>In HARD track of TREC 2005, we focus our research on the following 3 goals:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Category</head><p>The queries of 2005 HARD track are the hardest queries selected from the previous TREC tests. As stressed by Cronen-Townsend et.al. <ref type="bibr" coords="1,395.07,716.00,12.76,10.80" target="#b0">[1]</ref>, poorly-performing queries considerably hurt the effectiveness of an IR system. Many research work has been done to predict the difficulty of query <ref type="bibr" coords="1,299.60,747.20,13.65,10.80" target="#b1">[2,</ref><ref type="bibr" coords="1,313.25,747.20,9.10,10.80" target="#b2">3,</ref><ref type="bibr" coords="1,322.35,747.20,9.10,10.80" target="#b3">4,</ref><ref type="bibr" coords="1,331.45,747.20,9.10,10.80" target="#b4">5,</ref><ref type="bibr" coords="1,340.54,747.20,9.10,10.80" target="#b5">6]</ref> So the first thing for us is to classify these hardest queries into several categories and analyze the reason why this type of query can't be satisfactorily processed by current IR technology.</p><p>The following table shows the query category we made in TREC 2005:</p><p>Table <ref type="table" coords="2,221.34,454.82,4.78,10.80">1</ref>.Details of Query Category Apparently the failure of the "CASE" and "EVENT" queries is due to the reason that information need is too general to find the relevant documents just under the current "bag of words" framework. As for the other type of queries, there is no apparent clue which can explain why the current IR technology fails to find good results. However, a very common problem by the TF.IDF approach <ref type="bibr" coords="2,431.36,548.42,14.63,10.80" target="#b6">[7,</ref><ref type="bibr" coords="2,445.99,548.42,9.76,10.80" target="#b7">8]</ref>(no matter what the category the query belongs to) is that the retrieved top documents always focus on just one theme even though the information need contains several themes. These irrelevant documents occupy most top positions. For example, the topic of query number 394 is "home schooling" while the top retrieved documents focus on just "home" or "schooling" instead of both of them. This indicates that the proximity information is a very important factor to improve IR performance under the TFIDF paradigm for many queries.</p><p>After classifying the query into several categories, we want to know weather the different type of queries will have different effect on IR performance in feedback tasks. The experimental results tell us that the query category really has different effect on the feedback performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Various Feedback Sources</head><p>To evaluate the effect of various relevance feedback sources, we make use of 3 The corpus of TREC2005 HARD track (AQUAINT) , the corpus of TREC2004 HARD track(a collection of news from 2003 collated especially for HARD) and the web(using the Google to find out the relevant documents). We want to know answers of the following 2 questions:</p><p>1. Weather the different feedback source will bring different feedback effect? 2. Does the query category have effect on these various feedback source? If the answer is yes, what kind of effect it will be? We firstly retrieval the different results from the above-mentioned 3 different corpus and extract the titles and 10 keywords of the top 15 initially retrieved document to form the CF for relevance judgment. Then the initial query is changed by adding the title and the keywords of all relevant documents into it to process the next retrieval (they are 3 different runs).</p><p>We call the AQUAINT corpus "inside corpus" and the other 2 corpuses as "outside corpus" in the following part of this paper for easier description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring the Collection Structure in Feedback Procedure</head><p>The AQUAINT corpus consisted of three different newspapers: Xinhua news(XIE), New York times(NYT) and AWP. We suppose that the different news source may focus on different topics. For given query topic, the distribution of the relevant documents within these 3 sub-collections may different. For example, Xinhua news will have a bigger probability to publish the report about "three gorges project" than the other 2 news sources. So we plan to estimate this "relevant document distribution probability" through the feedback and hope this estimation parameter can be used to facilitate the IR performance. Here the probability can be regarded as the possibility that which sub-collection a relevant document should belongs to for any given query.</p><p>The first problem is how to estimate the parameters of the relevant document distribution within the 3 different news source given the query topic. We estimate the parameters as the following steps: Firstly, the relevant documents within top 15 search results (which are judged by NIST) are collected to form the "relevant doc set" for any given query topic. It's not hard to see which sub-set each document came from because the first 3 chars of document's name contain this information. For example, if the name of the document is "AWP200308122", we know that this document comes from AWP sub-collection. After labeling each relevant document with the news source, we can estimate the "relevant document distribution probability" as following :</p><p>(1) The second problem is how to tune the ranks of retrieved documents by applying the distribution parameters. We re-rank the initial retrieval result by the following formula:</p><formula xml:id="formula_0" coords="3,140.16,670.09,175.33,23.88">R R P i i = ) ( 3 ),<label>( 2 )</label></formula><p>(2)</p><formula xml:id="formula_1" coords="4,128.16,127.47,173.07,14.03">i p ore OriginalSc FinalScore * ) 1 ( * ∂ - + ∂ =</formula><p>where =0.7 and p i is the probability computed by formula 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effect of Various Relevance Feedback Source</head><p>Table <ref type="table" coords="4,182.35,375.56,4.50,10.80">2</ref>. Performance of different feedback source In order to observe the effect of various relevance feedback sources, we design 4 runs in our experiments :cassbase, cassgoogle, casstopdoc and cassself. Cassbase is a blind feedback run by adding the 40 keywords extracted from top 15 retrieved documents (from AQUAINT corpus) into the initial query and this run is regarded as the baseline. Cassgoogle is a run which use the WEB as feedback source and the title and 10 keywords of each relevant document (which are judged by NIST ) are added into initial query to perform another retrieval. The casstopdoc and cassself are like the cassgoogle run except the different feedback sources. The casstopdoc run use the corpus of 2004 HARD track as feedback source and cassself run use the AQUAINT corpus(corpus of 2005 HARD track) as the feedback source.</p><p>The experimental results listed in table <ref type="table" coords="4,309.91,547.40,6.00,10.80">2</ref> show that the "outside corpus" as the feedback source decrease the IR performance as a whole compared with the baseline run while the "inside corpus" greatly increase the performance.</p><p>However, we analyze the performance of each query topic and found out that the query category effect the performance greatly. For easier description, we can compare the results of cassgoogle with cassself run. We found that the performance of most queries (20 queries among all 22 queries) from "CASE" and "EVENT" category in cassgoogle run decrease dramatically compared with the cassself run. However, for the other type of query, sometimes the cassgoogle win and sometimes cassself win. Among all 28 queries which don't belong to "CASE" and "EVENT" category, 16 of them outperform the cassself in cassgoogle run. The bad performance of "CASE" and "EVENT" category query are main reasons to explain the failure of the "outside corpus" compared with."inside corpus" as feedback source.</p><p>We can draw the following conclusions: 1.</p><p>Using the "outside corpus" as the feedback source, the query category will be the main factor to decide whether the feedback source will help increase the IR performance. For most "CASE" and "EVENT" queries, it will decrease the IR performance if the "outside corpus" is used as the feedback source. While for other type of query, the effect of "outside corpus" as feedback source still need further research. That is, the "CASE"and "EVENT" query are much more sensitive to the feedback source compared with other type of query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Compared with Blind feedback run, casstopdoc run decrease the performance slightly while cassgoogle decrease dramatically. We thought it's maybe because the TREC corpus are all news paper and the google search result vary very much in the text format.</p><p>As for the reason why the "CASE" and "EVENT" query are more sensitive to the feedback source, we thought it's maybe the relevant document of this type of query focus on concrete cases which involve many proper names or concrete event, So these documents share little information and the information from "outside corpus" will give little help to find relevant information in another corpus. While the "CASE" query will be effective to use the "inside corpus" as the feedback source because there are similar reports in the same corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exploring the Collection Structure</head><p>Table <ref type="table" coords="5,196.55,625.16,4.17,10.80">3</ref>. The details of re-ranking runs We design 4 group experiments to test the re-ranking approach described in section 2 according to the formula 2. We can see from table 3 that the re-ranking technology through exploring the collection structure fails to increase the IR performance and with the increase of the initial retrieval performance, the negative effect has increased.</p><p>We also analyze each query to find out the reason why this technology fails. For convenient description, we take the cassself and cassselfre run as example(2 runs in group 4). It's found that 20 queries benefit from this technology, 5 queries remain unchanged scores and the other 25 queries suffer from the technology. As for the performance of other 3 groups, it's almost the same 20 queries benefit from the technology.</p><p>We thought whether the distribution parameter is precise or not have important effect on the performance. If the estimation is near the true distribution of the relevant document within sub-collections, the technology will help to increase the IR performance (sometimes dramatically change the performance) while it will has negative effect if the parameters estimation is far from the truth. So the conclusion is the technology is sometime effective and the problem of this technology is how to estimate the distribution more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In HARD track of HARD 2005, we classify the all 50 query into 7 categories and make use of 3 kind of different feedback source in various tasks. We find that the different kinds of queries perform differently in feedback tasks and the "CASE " and "EVENT" queries are more sensitive to the feedback source. We also explore the internal structure of corpus and try to estimate the distribution of relevant documents within sub-collections, the experiments show that this technology is partly effective and the main existing problem is how to predict the distribution more precisely.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,96.00,154.40,409.85,295.08"><head>Category Name Queries which belongs to the category Feature patterns of query</head><label></label><figDesc></figDesc><table coords="2,96.00,186.08,409.85,263.40"><row><cell cols="5">different collections as feedback source in our experiments:</cell><cell></cell><cell></cell></row><row><cell>CASE</cell><cell>307/325/353/374/378</cell><cell cols="5">1. identify concrete cases of</cell></row><row><cell></cell><cell>/383/389/393/394/426/</cell><cell cols="3">something…</cell><cell></cell><cell></cell></row><row><cell></cell><cell>439/622/650/689</cell><cell cols="2">2. identify</cell><cell cols="2">individual</cell><cell>or</cell></row><row><cell></cell><cell></cell><cell cols="3">corp.which ….</cell><cell></cell><cell></cell></row><row><cell>EVENT</cell><cell>322/336/347/354/362/36</cell><cell cols="5">1."identify instances of doing</cell></row><row><cell></cell><cell>7</cell><cell cols="2">something…"</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>/408/448</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>REASON</cell><cell>363/397/401/409/436</cell><cell cols="5">1.what are the causes of</cell></row><row><cell></cell><cell>/639</cell><cell cols="2">something…</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RELATIONSHIP 310/330/427/443</cell><cell cols="5">1.find document that discuss A</cell></row><row><cell></cell><cell></cell><cell>and B…</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEASURE</cell><cell>341/344/435</cell><cell cols="4">1."what steps has been taken…"</cell><cell></cell></row><row><cell>STATUS</cell><cell>416</cell><cell>1.what</cell><cell>is</cell><cell>the</cell><cell>status</cell><cell>of</cell></row><row><cell></cell><cell></cell><cell cols="2">something…</cell><cell></cell><cell></cell><cell></cell></row><row><cell>COMMON</cell><cell>303/314/345/372/375</cell><cell cols="3">Common questions</cell><cell></cell><cell></cell></row><row><cell></cell><cell>/399/404/419/433/625</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>/638/648/651/658</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>Partly supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No.<rs type="grantNumber">60203007</rs> and the new star plan of science &amp; technology of Beijing under Grant No.<rs type="grantNumber">H020820790130</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dsZVu7H">
					<idno type="grant-number">60203007</idno>
				</org>
				<org type="funding" xml:id="_9HrAY4E">
					<idno type="grant-number">H020820790130</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,109.10,482.49,396.15,9.45;6,90.00,498.09,415.22,9.45;6,90.00,513.69,338.34,9.45" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,349.95,482.49,135.22,9.45">Predicting query per-formance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,90.00,498.09,415.22,9.45;6,90.00,513.69,158.54,9.45">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.89,529.29,396.25,9.45;6,90.00,544.89,415.25,9.45;6,90.00,560.49,376.12,9.45" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,311.82,529.29,193.32,9.45;6,90.00,544.89,132.49,9.45">Query difficulty, robustness, and selective application of query expansion</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,244.14,544.89,261.11,9.45;6,90.00,560.49,160.83,9.45">Adcances in Information Retrieval, Proceedings of the 26th European Conference on IR Research</title>
		<meeting><address><addrLine>Sunderland UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.61,576.09,398.64,9.45;6,90.00,591.69,416.32,9.45;6,92.64,607.29,152.64,9.45" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,268.31,576.09,236.94,9.45;6,90.00,591.69,181.44,9.45">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,278.89,591.69,222.89,9.45">In ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.15,622.89,398.08,9.45;6,97.20,638.49,174.91,9.45" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,315.36,622.89,172.26,9.45">Error analysis of difficult TREC topics</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandhakavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,97.20,638.49,139.63,9.45">Proceedings of ACM SIGIR 2003</title>
		<meeting>ACM SIGIR 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>poster</note>
</biblStruct>

<biblStruct coords="6,106.14,658.11,399.10,9.45;6,90.00,673.71,99.41,9.45" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,216.35,658.11,228.78,9.45">Overview of the TREC 2004 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,453.55,658.11,51.69,9.45;6,90.00,673.71,94.34,9.45">TREC 2004 Conference Note Book</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.35,689.31,396.65,9.45;6,90.00,704.91,386.33,9.45;6,90.00,720.51,77.71,9.45" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,181.43,689.31,304.39,9.45">Locating question difficulty through explorations in question space</title>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,90.00,704.91,313.88,9.45">Proceeding of the first ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting>eeding of the first ACM/IEEE-CS joint conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="251" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,110.57,736.11,394.54,9.45;6,90.00,751.71,209.17,9.45" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,183.45,736.11,321.66,9.45;6,90.00,751.71,43.76,9.45">The SMART Retrieval System-Experiments in Automatic Document Processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">.</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971. 1971</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,104.87,76.41,325.16,9.45" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,174.73,76.41,191.42,9.45">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
