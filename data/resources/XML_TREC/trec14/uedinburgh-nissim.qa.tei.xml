<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.73,81.09,282.54,12.91">Question Answering with QED at TREC-2005</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.57,122.75,52.24,10.76"><forename type="first">Kisuh</forename><surname>Ahn</surname></persName>
						</author>
						<author>
							<persName coords="1,261.03,122.75,50.23,10.76"><forename type="first">Johan</forename><surname>Bos</surname></persName>
						</author>
						<author>
							<persName coords="1,319.64,122.75,87.81,10.76;1,407.45,120.46,1.49,7.86"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.63,136.70,45.10,10.76"><forename type="first">Dave</forename><surname>Kor</surname></persName>
						</author>
						<author>
							<persName coords="1,245.26,136.70,80.04,10.76"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
						</author>
						<author>
							<persName coords="1,341.23,136.70,79.14,10.76"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.73,81.09,282.54,12.91">Question Answering with QED at TREC-2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">498DD1941550F0611914890E17A09DB6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the system developed by the University of Edinburgh and the University of Sydney for the TREC-2005 question answering evaluation exercise. The backbone of our question-answering platform is QED, a linguistically-principled QA system. We experimented with external sources of knowledge, such as Google and Wikipedia, to enhance the performance of QED, especially for reranking and off-line processing of the corpus. For factoid and list questions we performed significantly above the median accuracy score of all participating systems at TREC 2005.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The QA evaluation exercise at TREC consists in automatically finding answers for a collection of questions arranged by different topics, or, to use the TREC terminology, targets. Questions can be either factoids, asking for a unique short answer, or list-questions, asking for a set of answers. Each series of questions ends with an other-question, which is a request of providing all relevant information about the target which was not already asked in the other questions. Here is an example: TARGET: Russian Submarine Kursk sinks 66.1 (factoid) When did the submarine sink? 66.2 (factoid) Who was the on-board commander of the submarine? 66.3 (factoid) The submarine was part of which Russian fleet? 66.4 (factoid) How many crewmen were lost in the disaster? 66.5 (list) Which countries expressed regret about the loss? 66. <ref type="bibr" coords="1,103.69,713.88,39.69,8.07">6 (factoid)</ref> In what sea did the submarine sink? 66.7 (list) Which U.S. submarines were reportedly in the area? 66.8 other The answers must be found in the Aquaint corpus, a collection of over a million newspaper articles from three American newspapers dating from 1998-2000. A response is evaluated as correct if it exactly answers the question (in an exhaustive but not overinformative way) and if it is accompanied by an appropriate document from the Aquaint corpus supporting the answer.</p><p>In this paper we describe the TREC-2005 entry of the Universities of Edinburgh and Sydney for the questionanswering evaluation exercise. This is the third time we participate in the TREC-QA campaign. Compared to the previous years <ref type="bibr" coords="1,375.40,436.34,87.31,8.97" target="#b11">(Leidner et al., 2003;</ref><ref type="bibr" coords="1,466.80,436.34,68.91,8.97" target="#b0">Ahn et al., 2004)</ref>, the performance of our system improved considerably.</p><p>The most interesting aspect of the QED system is that it is linguistically principled, combining symbolic with statistical approaches. QED uses one and the same theory and implementation for the analysis of both the question and the documents, employing detailed semantic representations and inference techniques to match possible answer-sentences with the question.</p><p>With respect to its architecture, QED is a fairly traditional QA system, which is composed of a standard sequence of modules: Question Analysis, Document Retrieval, Passage Analysis, Answer Extraction, Answer Reranking. Section 2 describes QED in more detail.</p><p>Although the main focus of research is on factoid questions, this year we also put additional effort into developing techniques dedicated to process list questions (resulting in a subcomponent called LiQED <ref type="bibr" coords="1,489.06,640.99,46.65,8.97" target="#b10">(Kor, 2005)</ref>, questions asking for titles of published works and otherquestions (see Section 3).</p><p>We also experimented by integrating an additional processing pipeline that views question answering in a radically different way, namely TOQA, a QA system developed by Kisuh Ahn (see Section 4).</p><p>Finally, in Section 5, we present the results obtained at TREC 2005 and an evaluation of the individual components in QED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The QED System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analysis</head><p>Like most traditional QA systems, the first stage of processing in QED is analysing the question. Each question is interpreted in the context of the TREC target, and processed along the following steps: tokenisation; syntactic analysis; semantic interpretation; and question typing.</p><p>Tokenisation is performed with NLProcessor 1 . Syntactic analysis is based on Combinatory Categorial Grammar (CCG), using a robust wide-coverage parser <ref type="bibr" coords="2,273.34,240.92,25.46,8.97;2,72.00,252.88,72.11,8.97" target="#b3">(Clark and Curran, 2004)</ref>. The output of syntactic analysis is a CCG-derivation, as shown in Figure <ref type="figure" coords="2,220.15,264.83,4.98,8.97">1</ref> (Underneath each word is the CCG category used in its analysis. Horizontal lines show the result of category combination, each labelled underneath with the resulting category. At the right end of the horizontal lines a symbol denotes the combinatorial rules used: &gt; for forward application, &lt; for backward application, &gt; B for forward composition, and &gt; P for the punctuation rule.).</p><formula xml:id="formula_0" coords="2,74.52,380.32,205.90,139.04">&gt; N &gt; N &gt; N N/N N/N N/N N Russian submarine Kursk sinks &gt;P S[wq] &gt; S[wq] &gt;B S[q]/PP &gt; S[q]/(S[b]\NP) &gt; NP[nb] S[wq]/(S[q]/PP) (S[q]/(S[b]\NP))/NP NP[nb]/N N (S[b]\NP)/PP .</formula><p>When did the submarine sink ?</p><p>Figure <ref type="figure" coords="2,127.65,539.79,3.88,8.97">1</ref>: CCG derivation for question 66.1</p><p>The example in Figure <ref type="figure" coords="2,173.09,563.94,4.98,8.97">1</ref> shows the target incorrectly analysed as a noun (N), due to "sinks" being incorrectly assigned the category N, and the question correctly analysed as a wh-question (category S[wq]). Semantic interpretation is based on Discourse Representation Theory, DRT <ref type="bibr" coords="2,165.98,623.84,97.14,8.97" target="#b9">(Kamp and Reyle, 1993)</ref>, and semantic representations are built on the basis of the CCGderivation output by the parser <ref type="bibr" coords="2,203.30,647.75,72.91,8.97" target="#b1">(Bos et al., 2004;</ref><ref type="bibr" coords="2,280.80,647.75,17.99,8.97;2,72.00,659.70,21.45,8.97" target="#b2">Bos, 2005)</ref>. DRSs are defined as ordered pairs of a set of discourse referents and a set of DRS-conditions. See Figure <ref type="figure" coords="2,86.85,683.61,4.98,8.97">2</ref> for an example DRS for question 66.1 and its target. The top of Figure <ref type="figure" coords="2,393.15,274.88,4.98,8.97">2</ref> shows the set of discourse referents (four of semantic type "individual" and one of semantic type "event"), while the bottom shows the set of DRS-conditions. The first six conditions come from the target, where "sink" has been incorrectly analysed as a noun, yielding "sink(x0)", and the other four from the question, where "sink" has been correctly analysed as an event "sink(x4)" whose subject is correctly resolved to the same russian submarine as in the target "arg1(e4,x1)".</p><p>The question type is determined on the basis of the semantic content of the question. In QED we distinguish a hierarchy of 12 main question types: reason, manner, definition, color, count, measure, date, location, name, abbreviation, publication, and general. Each of these main types is further divided into various subtypes. Questions introduce a special DRS-condition of the form answer(x,T,S) for a question type T and subtype S. We call this the answer slot; answer slots play an important role in answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Prefetching and Passage Selection</head><p>The Aquaint document collection, which forms the basis for TREC-2005, was pre-processed and tokenised offline with NLProcessor. The result was indexed with the Lemur search engine <ref type="bibr" coords="2,400.73,580.54,106.61,8.97" target="#b12">(Ogilvie and Callan, 2002)</ref>. Using ranked document retrieval, we obtained the best 1,000 documents from Lemur, supplying the target phrase as a query. Hence for the entire series of questions related to a single target, the same set of documents was used.</p><p>Since our approach involves full parsing to obtain detailed semantic representations in later stages, we need to reduce the amount of text to be processed to a fraction of each document. Passages spanning two sentences were selected from the returned documents based on the presence of at least one of the words from the target phrase, which were weighted (based on the number of target words found) to get an overall ranking of the passages. Again, the passages are the same for all the questions associated with the same target.</p><p>Document retrieval as such is relatively unimportant for the success of QED. At this stage of processing we aim for high recall and ignore precision, by selecting a high number of documents and passages, and narrowing down this pool of potential answers as late as possible in the processing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Analysis</head><p>As with the questions, we used the CCG-parser to parse the passages and then build DRSs on the basis of the derivations output by the parser. The CCG-parser also performs POS-tagging <ref type="bibr" coords="3,168.38,244.39,111.57,8.97">(Curran and Clark, 2003a)</ref> and named entity recognition <ref type="bibr" coords="3,182.03,256.34,112.38,8.97">(Curran and Clark, 2003b)</ref>, identifying named entities from the standard MUC-7 data set (locations, organisations, persons, dates, times and monetary amounts).</p><p>Each passage is translated into a single DRS; hence a DRS can span several sentences. A set of DRS normalisation rules are applied in a post-processing step, thereby dealing with active-passive alternations, inferred semantic information, normalisation of date expressions, and the disambiguation of noun-noun compounds. The resulting DRS is enriched with information about the original surface word-forms and POS-tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Extraction</head><p>The answer extraction component takes as input a DRS for the question, and the set of DRSs for selected passages. It extracts answer candidates from the passages by matching the question-DRS and a passage-DRS, using a relaxed unification method and a scoring mechanism indicating how well the DRSs match each other.</p><p>Matching takes advantage of Prolog unification, using Prolog variables for all discourse referents in the question-DRSs, and Prolog atoms in passage-DRSs. It attempts to unify all terms of the question-DRSs with terms in a passage-DRS, using an A * search algorithm. Each potential answer is associated with a score. High scores are obtained for perfect matches (i.e., standard unification) between terms of the question and passage, low scores for less perfect matches (i.e., obtained by "relaxed" unification). Less perfect matches are granted for different semantic types, predicates with different argument order, or terms with symbols that are semantically related (hypernymy) according to WordNet <ref type="bibr" coords="3,255.35,641.11,43.45,8.97;3,72.00,653.07,21.45,8.97">(Fellbaum, 1998)</ref>.</p><p>After a successful match, the answer slot is identified with a particular discourse referent in the passage-DRS. This is made possible by the fact that DRS-conditions and discourse referents are co-indexed with the surface word-forms of the source passage text. This information is used to generate an answer string, simply by collecting the words that belong to DRS-conditions with discourse referents denoting the answer. Finally, all answer candidates are output in an ordered list. Duplicate answers are eliminated, but answer frequency information (F) is retained and added to each answer in this final list. Figure <ref type="figure" coords="3,328.41,146.89,4.98,8.97">3</ref> shows an example output file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Reranking</head><p>The basic ranking algorithm for potential answers is fairly straightforward. Only two features are used: the matching score, and the frequency of similar answers. If there are different answers with the same matching score, frequency will be used to order them.</p><p>This simple method yields a reasonable ranking. However, in many cases the correct answer is ranked high but not highest, as for 99.3: TARGET: Woody Guthrie</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>99.3</head><p>Where was Guthrie born?' Initially, QED produced the following ranked answer candidates for 99.3 (answer 2 and 4 are correct):</p><p>1. Britain 2. Okemah, Okla.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Newport 4. Oklahoma 5. New York</head><p>To deal with this problem, we experimented with external additional knowledge, namely the World Wide Web accessed via the Google API, to generate a possibly more accurate ranking. This technique is also known as "answer validation", and can be seen as a tie-breaker between the top-N answers.</p><p>In more detail, this method works as follows. For each of the N-best answer candidates, we take the semantic representation of the question and fill the answer slot with the answer candidate. From this we generate a set of declarative sentences (covering all morphological variations). The generated sentences are submitted as strict (within quotes) queries for Google. Any information from the target which is not included in the generated sentence (for this example "Woody") is added as a query term in order to constrain the search space. The queries and number of hits returned for each of the queries (in brackets) are shown below. The returned Google-counts are used as the deciding factors to rerank the N-best answers. Note that we generate several queries for each answer candidate and we sum the returned hits. In this example, the answers would be reranked as follows:</p><p>1. Oklahoma (7 + 42) 2. Okemah, Okla. (1 + 10) 3. New York (0 + 2) 4. Britain (0 + 0) 5. Newport (0 + 0)</p><p>For this example, reranking correctly promoted "Oklahama" to best answer.</p><p>3 Dedicated Processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Processing Publication-Questions</head><p>For questions of type publication (titles of creative works, such as books, records, plays, etc.), one of the way answer candidates are recognised by the answer extraction module is by quotation marks. However not all titles in the corpus are decorated with quotes.</p><p>To overcome this problem we adopted a strategy that exploits information encoded in http://www. amazon.com, a commercial website equipped with a large database containing information on several types of publications, especially useful for printed material and music. This database is accessible via an API. The basic strategy is to collect a list of titles relevant to the question target and to mark them with quotes in the corpus. QED also assigns a subtype to questions of type publication, e.g. book, so we used predetermined rules to map such subtypes to Amazon categories. The selected category (e.g. music) together with the question target (e.g. Nirvana), was then searched over the whole of Amazon's database. All resulting hits were collected and matched back into the portions of the Aquaint corpus retrieved for a given question. Quotes were then added around each successful match in the text.</p><p>For all questions that underwent such procedure, we parsed the quotes-enriched passages a second time, since the parser has further clues to determine whether a given phrase is an instance of a publication. After this, the standard procedure followed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Processing List Questions</head><p>This year, besides using QED to generate answers for list questions, we also introduced a new approach to answering list questions. List questions are interesting because they offer an opportunity for a Question Answering system to directly examine the relationship between a question and its answers. The new approach takes a question and an existing set of answers, generated by QED in our case, and uses these answers as examples to identify more answers of similar nature. In essence, this is a bootstrapping method for expanding our initial set of answers. This approach is built into our new list question anwering module, LiQED <ref type="bibr" coords="4,379.89,497.06,44.49,8.97" target="#b10">(Kor, 2005)</ref>.</p><p>The general approach takes a question and an initial set of potential answers, identifies some context that is shared between the question and two or more answers, then extrapolates from this shared context to expand on the initial set of answers with new and distinct answers. This shared context can be expressed in several forms, such as text patterns, logical forms or branches in parse trees. The initial set of answers can contain a mix of both correct and wrong answers. Typically, only shared contexts from correct answers are identified. The reason, using parlance from physics, is that typically wrong answers create "interference" while correct answers mutually "reinforce" each other. In general, this approach requires a minimum of two correct answers in the answer set before any shared context can be identified. Although we did not implement this, the identified shared context can conceivably be used in reverse to verify correct answers in the initial answer set. Specifically for LiQED, the shared context we use comprises surface text patterns. In essence the new module takes answers generated by QED as examples, automatically identifies common shared surface text patterns and finally uses these patterns to look for more answers. This is achieved for each question in two phases: a pattern generation phase, and an answer extraction phase.</p><p>In the pattern generation phase, we identify sentences in the Aquaint corpus that contain the question target and one answer. We noticed that a significant number of sentences contain some form of long distance dependency separating the question target and answer. In order to capture these dependencies, our text patterns are able to match either one contiguous fragment or two separate fragments within a single sentence. This is achieved by only considering relevant terms that fall within a threechunk window around either a question target or an answer. The function R(x) measures the relevance of each term by determining if the term frequently occurs near the question target and/or answer:</p><formula xml:id="formula_1" coords="5,73.13,335.40,216.03,28.40">R(x) = 1 N N ∑ i=1 w q 1 - D(x i , T i q ) S(T i q ) + w a 1 - D(x i , T i a ) S(T i a )</formula><p>In the formula above, N is the number of occurrences of term x in the Aquaint corpus. T q and T a are respectively the set of terms found in the question target and the potential answer. D(x, T ) measures the distance between term x and the nearest term in T while S(T ) is the longest span within a sentence between two terms in T or the start or end of the sentence. w q and w a are weights that give importance to either the question target or answer. Using a threshold, we retain the most relevant terms in each sentence. This leaves N sentence fragments that are compared in a pairwise manner using the Smith-Waterman <ref type="bibr" coords="5,72.00,508.71,118.96,8.97" target="#b13">(Smith and Waterman, 1981)</ref> and Gotoh <ref type="bibr" coords="5,240.99,508.71,57.81,8.97" target="#b7">(Gotoh, 1982)</ref> word alignment algorithm, to identify common patterns. We filter away irrelevant patterns, leaving only patterns that contain both the question target and answer as our set of automatically generated patterns.</p><p>Figure <ref type="figure" coords="5,110.83,569.11,4.98,8.97">4</ref> shows some of the patterns generated for the question "What movies was Bing Crosby in?". Square brackets in the pattern indicate there is some long distance dependency gap between question target and answer, asterisk characters indicate a small gap of 1-3 terms. Using these generated surface text patterns, we search the Aquaint corpus again for matching sentences. The text patterns narrow down to a three-chunk window in the sentence that potentially contains an answer.</p><p>To extract the answers, we use a simplified form of ensemble learning by combining evidence from our text patterns, a named entity tagger and other external information sources. For example, with questions expect-ing organization names as answers, we can extract answers by choosing all distinct phrases that fall within the three-chunk window and are labeled as organizations by a named entity tagger. External sources of information include book and film titles extracted from Amazon as well as hyponyms extracted using Google queries <ref type="bibr" coords="5,313.20,146.89,55.75,8.97" target="#b8">(Hearst, 1992)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Processing Other-Questions</head><p>Other-questions were and still are the poor relation of QED. Not much effort is put into processing these: answers to other-questions are sentences identified in the corpus that contain the target as subject or object. No sophisticated techniques were used to filter out duplicated or repeated answers. All nuggets that the system had already provided as answers to other questions for the same target were considered as redundant and removed. We considered this setting as a baseline for answering otherquestions.</p><p>To improve on the baseline we collected a set of "important" words, and used this set to select interesting answers from the set of answers generated by the baseline. The important words were extracted by the correctly judged answers generated for the other-questions of TREC-2004, that is cue words that might make a sentence worth considering. Example of such "important" words are 'first', 'best', 'award', 'invented', and similar. This list was collected by semi-automatically producing a word-frequency list and removing stop words.</p><p>As an additional improvement, we exploited the online encyclopedia, Wikipedia (http://en.wikipedia. org/wiki/Main_Page). For a given question, the system sent the TREC target as a query to the Wikipedia's built-in search engine. If there was a direct match between the target and the Wikipedia topic retrieved by the search engine, then the system fetched the found Wikipedia article. Otherwise, the article of the closest Wikipedia topic, as judged by the built-in Wikipedia search engine was used. Finally, if there was no match at all, the system did not attempt to answer the question.</p><p>The Wikipedia article is an XML file whose meta-data includes information about the categories the article belongs to. Wikipedia categories are vast and somewhat unconstrained, since the individual who authors the article can specify any number of pre-existing or even new categories. This makes such information a possibly very rich and valuable description of the target entity.</p><p>Finally, in order to extract an appropriate supporting document for the information provided, we used boolean search with the category descriptions as the queries to the orginial Aquaint document collection. This was done by first finding interesting facts about a target in Wikipedia, and then aiming to localise these nuggets of information in the TREC corpus. 4 Topic-based QA TOQA (Topic Oriented Question Answering) is a selfcontained question answering system and was used in TREC 2005 to complement QED. The philosophy of TOQA is to rely heavily on off-line processing of the Aquaint corpus, to yield speedy answer retrieval in realtime. For the present exercise, TOQA was ran in an independent pipeline to produce answer candidates that were combined with QED answer candidates to form the best answer-candidate pool. TOQA is still a system in development, and only factoid questions pertaining to namedentity answers were processed with TOQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Off-Line Processing</head><p>TOQA does most of its work off-line. This preprocessing stage is aimed at identifying all types of expressions in the Aquaint corpus that could serve as potential answers in a question-answering exercise. We call these topics. TOQA extracts topics together with their contexts creating a new document for each distinct topic. The document for an identified topic is the set of all the sentences found in the corpus that say something about this topic. Each of these documents is then indexed to enable efficient retrieval. To constrain the search space, separate indices are created depending on the different topic types. For the present version of the system, only expressions that have been identified as relating to some named entity type, such as person, location and organization, have been targeted as topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Retrieval</head><p>Answering questions in TOQA boils down to retrieving the most relevant topic document created in the aforementioned process. The title of the retrieved document (the topic itself), serves as the succinct answer to the question. Note that the answer can be retrieved using an ordinary information retrieval method. First, the index to be used is decided by the answer-type of the question identified. Then, the base query is formulated by concatenating the target words to the question, from which the stop-words had been removed. Finally, this query is fed into the off-the-shelf IR component Lemur Version 2.2 <ref type="bibr" coords="6,86.76,701.24,106.27,8.97" target="#b12">(Ogilvie and Callan, 2002)</ref> to retrieve the relevant answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document Retrieval</head><p>As explained, TOQA does not rely on the original passage or document for the answer extraction, but rather on a newly created set of documents that contains information originally spread across several documents of the Aquaint collection. For this reason, the document supporting the answer that TOQA provides can only be found via a post-processing phase. This consists in forming a new query by combining the base query for which the answer was retrieved with the retrieved answer candidate. This new query is then used with Lemur to retrieve the most relevant Aquaint document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Three runs were submitted, all with different parameters with respect to the treatment of factoids, list, and otherquestions. Let's first consider the factoid questions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACTOID-questions (run descriptions)</head><p>Run A (Edin2005A) QED Run B (Edin2005B) QED + Google reranking Run C (Edin2005C) QED + Google reranking + TOQA + paraphrases</p><p>We expected Run B to outperform Run A by gaining around 10% in accuracy, based on results on the training data of TREC 2004. Run C was partly an experimental run using newly developed techniques that we hadn't had tested thoroughly, such as the use of paraphrases in background knowledge, and TOQA.</p><p>For list questions, our first run comprises the top twelve answers generated by QED and serves as a baseline for comparison. Our second run is a combination of the top ten answers from QED and LiQED while our final run takes the top seven answers from QED, LiQED and TOQA. The number of answers were mainly chosen to balance precision and recall of the combined answer set. The reranking procedure using Google improved the results but only slightly. On the positive side, it transformed 10 wrong or inexact answers in Run A into correct answers, but it also transformed 8 correct answers into wrong, inexact, or unsupported answers. This seems a promising way of doing reranking, but there is clearly space for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIST questions (run descriptions)</head><p>Overall, our results were roughly as we had anticipated and we were satisfied with the performance of QED: the accuracy scores were significantly higher than the median accuracy score of all participating systems (0.152). However, we were slightly disappointed with the high number of inexact answers. Closer inspection of these cases revealed that some of the QED answers would have been judged as exact answers in previous TREC campaigns. A case in point are compound expressions such as "Atlantabased" (88.1), "Dominican-born" (100.1), "Milwaukeebased" (119.2) and "Vienna-based" (128.4), where the expected correct answer was Atlanta, Dominican Republic, etc. Also for abbreviation-questions our system generated a relatively high number of inexact answers. These shortcomings relative to inexact answers seem easy to overcome.</p><p>Just for the record, we mention the results for dealing with factoid questions that have no known answer (in the corpus). There were 17 of those. A correct response to these questions would be NIL. For the three runs, our system generated a total of 43, 44 and 40 NIL answers, with only 3, 3 and 2 being correct.</p><p>List Questions There were 93 list questions in total. QED achieved an average F-score of 0.081 for Run A, of 0.075 for Run B, and of 0.074 for Run C. Although we were hoping to see Run B perform better than Run A, this was not the case. A post-submission analysis revealed that the majority of answers returned by LiQED were evaluated as unsupported or inexact. Still all obtained F-scores were higher than the median average Fscore of all runs (0.053).</p><p>Other-Questions Since we didn't do anything sophisticated for dealing with other-questions, the results were rather meager compared to what our system achieves on factoid and list questions: Run A yielded an F-score of 0.070, Run B of 0.095, and Run C of 0.102. Our best score (Run C) is still below the median average F-score of all participating systems (0.156).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluating Question Analysis</head><p>Despite the detailed linguistic analysis, our approach to question analysis is fairly robust and gives good results.</p><p>Of the 455 questions at TREC-2005 (363 factoids, 93 list), we were able to analyse 426 (93.6%) "approximately" correctly. There were 9 questions for which the parser failed to find a derivation, and 20 questions that got a wrong parse. The example in Figure <ref type="figure" coords="7,492.77,425.36,4.98,8.97">2</ref> illustrates an approximately correct output: it is an almost perfect analysis, with the definite description "the submarine" in the question resolved to the antecedent "Russian submarine Kursk" in the target. The only mistakes are in the target, where "sinks" is analysed as a noun and "Russian submarine Kursk" is analysed as simple noun-noun modification. Errors like these hardly affect the performance of the overall system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Component Evaluation</head><p>For future work it is valuable to be able to identify which module in the system has initial responsibility for losing an answer. To do this thoroughly for each question is however rather time-consuming. In the context of this year's TREC results, we performed an analysis into how QED behaved with respect to one particular questiontype, namely date-questions.</p><p>There where 67 date-questions in the TREC-2005 test set of questions (15% of the 455 total number of factoid and list questions). For Run A, we counted the number of correct answers available after each stage of processing (this was not just a syntactic check, also the exactness and relevant document were taken into account to mark an answer as correct). The table below shows the results, together with the loss of correct answers caused by each component. These results suggest that substantial improvements can be made at the beginning of the processing pipeline (document retrieval plus passage selection) and at the end of it (reranking) in order to obtain the most significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Future Work</head><p>Given the current performance of QED and our experience with the system, we can identify the following areas for future work.</p><p>• More fine-grained set of question types. The current set is not specific enough to cover all varieties of questions that occur in general QA tasks.</p><p>• Improving the Named Entity recognition module. A large proportion of the question types rely on the correct identification of named entities.</p><p>• Improving the document retrieval and passage selection modules.</p><p>• Given that in many cases the correct answer is ranked high but not highest, a more sophisticated reranking module should considerably improve the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,148.01,149.22,315.97,8.97"><head></head><label></label><figDesc>Figure 4: Answer-finding text patterns for "What movies was Bing Crosby in?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,321.42,620.59,218.58,101.36"><head></head><label></label><figDesc>Russian President Vladimir Putin on Tuesday declared Wednesday a day of mourning for the crew of the sunken nuclear submarine Kursk. " The Russian nuclear submarine Kursk sank in the Barents Sea on August 12, 2000, its crew perished. NYT20000814.0435 0.733333 4 In 1989 a Soviet nuclear submarine sank off north Norway, killing 42 of the 69-man crew. The Komsomolets submarine, the prototype of the Mike-class sub, now lies rusting, along with its nuclear torpedoes, on the sea bed of the Barents. NYT20000828.0399 0.688889 7 Given that secrecy, and the likelihood that the Russians will not fully share what they learn even if they recover the wreckage, it will be difficult to learn with any certainty what happened to the Kursk. In 1968, an American submarine, the Scorpion, sank in the Atlantic near the Azores.</figDesc><table coords="3,321.42,620.59,218.58,44.75"><row><cell>Document</cell><cell>Score</cell><cell>F Answer</cell></row><row><cell>XIE20000822.0059</cell><cell cols="2">0.755556 2 Figure 3: QED extracted answers for question 66.1</cell></row><row><cell cols="3">4. Woody "Guthrie born in Oklahoma" (7)</cell></row><row><cell cols="3">Woody "Guthrie are OR is OR was OR were born in Ok-</cell></row><row><cell>lahoma" (42)</cell><cell></cell><cell></cell></row><row><cell cols="3">5. Woody "Guthrie born in New York" (0)</cell></row><row><cell cols="3">Woody "Guthrie are OR is OR was OR were born in New</cell></row><row><cell>York" (2)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1. Woody "Guthrie born in Britain" (0)</cell></row><row><cell></cell><cell></cell><cell>Woody "Guthrie are OR is OR was OR were born in</cell></row><row><cell></cell><cell></cell><cell>Britain" (0)</cell></row><row><cell></cell><cell></cell><cell>2. Woody "Guthrie born in Okemah, Okla." (1)</cell></row></table><note coords="3,333.13,667.23,206.87,8.07;3,333.13,677.20,74.97,8.07;3,321.42,693.95,148.96,8.07;3,333.13,703.91,206.87,8.07;3,333.13,713.88,31.12,8.07"><p><p>Woody "Guthrie are OR is OR was OR were born in Okemah, Okla." (10) 3. Woody "Guthrie born in Newport" (0)</p>Woody "Guthrie are OR is OR was OR were born in Newport" (0)</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,534.25,226.80,8.97;8,81.96,545.20,216.83,8.97;8,81.96,556.16,216.83,8.97;8,81.96,567.12,216.83,8.97;8,81.96,578.08,216.84,8.97;8,81.96,589.04,216.84,8.97;8,81.96,600.00,193.04,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,228.74,556.16,70.06,8.97;8,81.96,567.12,136.72,8.97">Question answering with qed and wee at trec-2004</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Kisuh Ahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiphaine</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Smillie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.32,578.08,96.48,8.97;8,81.96,589.04,131.72,8.97">Proceedings of the Thirteenth Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Thirteenth Text Retrieval Conference<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct coords="8,72.00,618.24,226.80,8.97;8,81.96,629.20,216.83,8.97;8,81.96,640.16,216.84,8.97;8,81.96,651.12,216.83,8.97;8,81.96,662.08,165.24,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,146.00,629.20,152.80,8.97;8,81.96,640.16,99.51,8.97">Wide-Coverage Semantic Representations from a CCG Parser</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.42,640.16,96.38,8.97;8,81.96,651.12,216.83,8.97;8,81.96,662.08,73.94,8.97">Proceedings of the 20th International Conference on Computational Linguistics (COLING &apos;04)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING &apos;04)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,680.32,226.80,8.97;8,81.96,691.28,216.84,8.97;8,81.96,702.24,216.84,8.97;8,81.96,713.20,27.40,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,161.88,680.32,136.92,8.97;8,81.96,691.28,52.43,8.97">Towards wide-coverage semantic interpretation</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,157.88,691.28,140.93,8.97;8,81.96,702.24,187.13,8.97">Proceedings of Sixth International Workshop on Computational Semantics IWCS-6</title>
		<meeting>Sixth International Workshop on Computational Semantics IWCS-6</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,75.16,226.80,8.97;8,323.16,86.12,216.84,8.97;8,323.16,97.08,216.83,8.97;8,323.16,108.03,187.89,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,448.59,75.16,91.41,8.97;8,323.16,86.12,118.11,8.97">Parsing the WSJ using CCG and Log-Linear Models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.98,86.12,76.03,8.97;8,323.16,97.08,216.83,8.97;8,323.16,108.03,111.02,8.97">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,126.96,226.80,8.97;8,323.16,137.92,216.83,8.97;8,323.16,148.88,216.84,8.97;8,323.16,159.84,216.83,8.97;8,323.16,170.80,216.84,8.97;8,323.16,181.76,19.50,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,497.98,126.96,42.01,8.97;8,323.16,137.92,212.93,8.97">Investigating GIS and smoothing for maximum entropy taggers</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,334.33,148.88,205.68,8.97;8,323.16,159.84,216.83,8.97;8,323.16,170.80,88.48,8.97">Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)</title>
		<meeting>the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,200.69,226.80,8.97;8,323.16,211.65,216.83,8.97;8,323.16,222.61,216.83,8.97;8,323.16,233.56,216.83,8.97;8,323.16,244.52,67.80,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,500.71,200.69,39.28,8.97;8,323.16,211.65,213.00,8.97">Language independent NER using a maximum entropy tagger</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,334.61,222.61,205.39,8.97;8,323.16,233.56,128.95,8.97">Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03)</title>
		<meeting>the Seventh Conference on Natural Language Learning (CoNLL-03)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,263.45,226.80,8.97;8,323.16,274.41,163.99,8.97" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,463.55,263.45,76.45,8.97;8,323.16,274.41,93.62,8.97">WordNet. An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,293.34,226.80,8.97;8,323.16,304.30,216.84,8.97;8,323.16,315.26,115.39,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,375.12,293.34,164.88,8.97;8,323.16,304.30,73.68,8.97">An improved algorithm for matching biological sequences</title>
		<author>
			<persName coords=""><surname>Gotoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,417.63,304.30,118.20,8.97">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="705" to="708" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,334.19,226.80,8.97;8,323.16,345.15,216.84,8.97;8,323.16,356.10,216.83,8.97;8,323.16,367.06,136.71,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,423.97,334.19,116.03,8.97;8,323.16,345.15,130.45,8.97">Automatic aquisition of hyponyms from large text corpora</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,478.81,345.15,61.19,8.97;8,323.16,356.10,216.83,8.97;8,323.16,367.06,68.12,8.97">Proceedings of the Fourteenth International Conference on Computational Linguistics</title>
		<meeting>the Fourteenth International Conference on Computational Linguistics<address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,385.99,226.80,8.97;8,323.16,396.95,216.83,8.97;8,323.16,407.91,216.84,8.97;8,323.16,418.87,42.88,8.97" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,464.30,385.99,75.70,8.97;8,323.16,396.95,216.83,8.97;8,323.16,407.91,172.83,8.97">From Discourse to Logic. An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,437.80,226.80,8.97;8,323.16,448.76,216.83,8.97;8,323.16,459.72,115.17,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,386.44,437.80,153.56,8.97;8,323.16,448.76,62.44,8.97">Improving answer precision and recall of list questions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Kor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>School of Informatics, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="8,313.20,478.65,226.80,8.97;8,323.16,489.60,216.83,8.97;8,323.16,500.56,216.83,8.97;8,323.16,511.52,216.83,8.97;8,323.16,522.48,216.84,8.97;8,323.16,533.44,216.84,8.97;8,323.16,544.40,140.18,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,501.63,500.56,38.37,8.97;8,323.16,511.52,212.35,8.97">The QED open-domain answer retrieval system for TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiphaine</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,335.30,522.48,204.70,8.97;8,323.16,533.44,71.55,8.97">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="595" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,563.33,226.80,8.97;8,323.16,574.29,216.84,8.97;8,323.16,585.25,168.98,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,448.95,563.33,91.05,8.97;8,323.16,574.29,50.39,8.97">Experiments using the lemur toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,391.25,574.29,148.75,8.97;8,323.16,585.25,99.66,8.97">Proceeding of the 2001 Text Retrieval Conference (TREC 2001)</title>
		<meeting>eeding of the 2001 Text Retrieval Conference (TREC 2001)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,313.20,604.17,226.80,8.97;8,323.16,615.13,216.83,8.97;8,323.16,626.09,105.07,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,475.90,604.17,64.10,8.97;8,323.16,615.13,132.09,8.97">Identification of common molecular subsequences</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Waterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,463.28,615.13,76.71,8.97;8,323.16,626.09,43.33,8.97">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="195" to="197" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
