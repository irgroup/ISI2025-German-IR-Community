<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,120.84,81.04,370.18,12.91">Employing Two Question Answering Systems in TREC-2005</title>
				<funder>
					<orgName type="full">Advanced Research Development Activity (ARDA)&apos;s Advanced Question Answering for Intelligence (AQUAINT) Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,43.68,134.03,87.21,10.76"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,139.58,134.03,72.70,10.76"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.34,134.03,78.09,10.76"><forename type="first">Christine</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.91,134.03,83.06,10.76"><forename type="first">Mitchell</forename><surname>Bowden</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.36,134.03,71.83,10.76"><forename type="first">Andrew</forename><surname>Hickl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,497.37,134.03,70.62,10.76"><forename type="first">Patrick</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Computer Corporation Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,120.84,81.04,370.18,12.91">Employing Two Question Answering Systems in TREC-2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0F7196FC796689E3567DE315D7CA285</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2005, the TREC QA track had two separate tasks: the main task and the relationship task. To participate in TREC 2005 we employed two different QA systems. PowerAnswer-2 was used in the main task, whereas PALANTIR was used for the relationship questions. For the main task, new this year is the use of events as targets in addition to the nominal concepts used last year. Event targets ranged from a nominal event such as "Preakness 1998" to a description of an event as in "Plane clips cable wires in Italian resort". There were 17 event targets total. Unlike nominal targets, which most often act as the topic of the subsequent questions, events provide a context for the questions. Therefore, targets representing events had questions that asked about participants in the event, about characteristics of the vent and furthermore, had temporal constraints. Also many questions referred to answers of previous questions. To complicate matters, several answers could be candidate for the anaphors used in follow-up questions, but salience mattered. This introduced new complexities for the coreference resolution. Consider the following example:</p><p>Target 136 -Shiite Q136.1</p><p>Who was the first Imam of the Shiite sect of Islam?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q136.2</head><p>Where is his tomb?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q136.3</head><p>What was this person's relationship to the Prophet Mohammad? Q136. <ref type="bibr" coords="1,101.09,604.68,4.50,8.07">4</ref> Who was the third Imam of Shiite Muslims? Q136.5</p><p>When did he die?</p><p>In the above target set, questions Q136.2 and Q136.3 refer back to the answer of question Q136.1; question Q136.5 back-refers to the answer for question Q136.4. Because of this, if the QA system fails to locate the right answer for question Q136.1, the chances of getting the correct response for the following two questions are greatly decreased. Furthermore, this leads to the potential for ambiguity when the system attempts to answer question Q136.5 if the answer to the most recent question is either not found or incorrect. Compare with the situation of target 27 in TREC 2004.</p><p>Target 27 -Jennifer Capriati Q27.2 Who is her coach? Q27.3 Where does she live?</p><p>In question Q27.3, "she" refers to the target, Jennifer Capriati, which is also the antecedent for the pronoun "her" in Q27.2. But, if the answers are also included in the candidate set for the pronouns, when processing question Q27.3, two different entities become candidates: both Jennifer Capriati and her coach -the answer to Q.27.2.</p><p>Reference resolution is not the only linguistic phenomenon that had to be tackled in TREC 2005. The main task for Q/A required various forms of inference. We describe them when we report on the PowerAnswer-2 Q/A system at TREC 2005. In the paper we also describe the approach we used to answer complex questions for the TREC 2005 Relationship Task. For that task we used the PALANTIR Q/A system The rest of the paper is organized as follows. In Section 2 we discuss the architecture of PowerAnswer-2. In Section 3 we detail the method of exploiting redundancy on the Web. Section 4 presents the role of the logical prover in the results obtained this year. Section 5 discusses the processing of questions that have temporal constraints. Section 6 reports on the processing of "other" questions. Section 7 described the processing of relationship questions with PALANTIR, which is described in Section 8. Section 9 lists and discusses the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The PowerAnswer-Q/A System</head><p>As illustrated in Figure <ref type="figure" coords="1,407.79,689.32,3.77,8.97" target="#fig_0">1</ref>, The PowerAnswer-2 Q/A System has three different modules: the question processing (QP) module, the passage retrieval (PR) module and  the answer processing (AP) module. The role of the QP module is to determine ( <ref type="formula" coords="2,170.49,283.00,3.92,8.97">1</ref>) the expected answer type and (2) to select the keywords used in retrieving relevant passages. The PR module ranks passages that are retrieved, while the AP determines the extraction of the candidate answers. All modules have access to a syntactic parser, a named entity recognizer and a reference resolution system. To improve the statistical methods used for answer selection, we took advantage of redundancy in large corpora, specifically in this case, the Internet. As the size of a document collection grows, a question answering system is more likely to pinpoint a candidate answer that closely resembles the surface structure of the question. Such an intuition has been verified by <ref type="bibr" coords="2,212.84,426.52,85.93,8.97" target="#b1">(Breck, et al., 2001)</ref> and empirically re-enforced by several QA systems. The Web-Boosting Strategy module uses features which are described in Section 3. These features have the role of correcting the errors in answer processing that are produced by the selection of keywords, by syntactic and semantic processing and by the absence of pragmatic information. The ultimate decision for selecting answers is based on logical proofs.</p><p>Before selecting the answer, an abductive proof of its correctness is performed by using the COGEX logical prover. Details of the operation of COGEX were presented in <ref type="bibr" coords="2,112.15,575.80,97.55,8.97" target="#b9">(Moldovan et al., 2003)</ref> and <ref type="bibr" coords="2,230.80,575.80,67.72,8.97;2,72.00,587.80,21.66,8.97" target="#b10">(Moldovan et al. 2005)</ref>. To perform the abductive inference, the question and each candidate answer need to be transformed in logical representations, which rely on the syntactic, semantic and reference resolution information already used in the QA modules. The process of translating a question or an answer in logical transformations is illustrated in Figure 2. As illustrated in Figure <ref type="figure" coords="2,201.11,659.56,3.77,8.97" target="#fig_0">1</ref>, the inputs to COGEX consist of the question logical form (QLF), the answer logical form (ALF) and a set of axioms modeling world knowledge.</p><p>The QLF and the ALF are produced in a three-layered  The first layer relies on the syntactic parse and the named entity recognition, available also to the QA modules, as was illustrated in Figure <ref type="figure" coords="2,416.66,414.04,3.77,8.97" target="#fig_0">1</ref>. The second layer relies on the recognition of semantic relations processed by the semantic parser reported in <ref type="bibr" coords="2,429.95,438.04,82.57,8.97" target="#b0">(Bixler et al. 2005)</ref>. The third layer represents temporal contextual information that is produced by temporal ordering of events, anchoring events in time intervals and normalizing temporal expressions. The temporal context representation was introduced in <ref type="bibr" coords="2,360.78,497.80,90.34,8.97" target="#b10">(Moldovan et al. 2005)</ref>.</p><p>The QLF and the ALF are not the only inputs to COGEX. As illustrated in Figure <ref type="figure" coords="2,455.16,521.80,5.03,8.97" target="#fig_0">1</ref> a set of world axioms also participate in the proof of the answer. There are five sources of world knowledge axioms: (1) axioms derived from the eXtended WordNet, available from http://xwn.hlt.utdallas.edu; (2) ontological axioms generated by the JAGUAR knowledge acquisition tool described in <ref type="bibr" coords="2,357.55,593.56,82.18,8.97" target="#b0">(Bixler et al. 2005)</ref> ; (3) Linguistic Axioms handcrafted to account for several linguistic phenomena, e.g. possessives, appositions, nominal coreference; (4) a semantic calculus described in <ref type="bibr" coords="2,440.61,629.44,94.99,8.97" target="#b10">(Moldovan et al. 2005)</ref>; and (5) temporal reasoning axioms available from the SUMO knowledge base. The SUMO knowledge base was described in <ref type="bibr" coords="2,381.88,665.20,90.47,8.97" target="#b12">(Niles and Pease 2001)</ref>.</p><p>The proof of the abduction performed by COGEX scores each candidate answer, thus allowing the answer selection (AS) module to chose the exact answers when high confidence is given to the abductive proof. When the abductions fail or are obtained with low confidence, the AS module selects the highest-ranking answer provided by the AP module.</p><p>In 2005, we focused on advanced textual inference techniques that involve temporal inference. Furthermore, we continued development on automatic axiom generation for linguistic entailment as motivated by the RTE Pascal Challenge <ref type="bibr" coords="3,144.52,158.80,84.95,8.97">(Fowler, et al., 2005)</ref>. To address the "OTHER" question type, we developed new methods of extracting novel, interesting nuggets through the use of the Suggested Upper Ontology <ref type="bibr" coords="3,201.20,194.68,93.35,8.97" target="#b12">(Niles and Pease 2001)</ref>, templates and entity associations as well as patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Exploiting Answer Redundancy on the Web</head><p>The "web-boosting" features are based on a web strategy that utilizes general linguistic patterns in order to construct a series of search engine queries. The answers from the web documents are extracted by considering their redundancy. Furthermore, the most redundant answer is added to the keyword features used in the AP module to extract answers. For example, for the question Q124.5, the illustrated web query allows to find redundant answers which leads to another ranking of the most relevant answers produced by the AP module. PowerAnswer-2 processes the document hits produced by the set of queries, extracts the exact answers, and computes the most probable answer from this set using tiling <ref type="bibr" coords="3,96.94,521.92,45.52,8.97" target="#b8">(Lin, 2002)</ref>, and the answer confidence assigned by PowerAnswer-2. A boost is then given to answers returned from the TREC collection that best match the answers returned by the web strategy, a larger boost given for higher frequency.</p><p>Question Answering systems that utilize syntactic and nominal coreference features are likely to select answers with high question word overlap. By voting with high precision results from the web can prevent the question answering system from extracting syntactically and lexically similar, but incorrect answers, as is the case for question Q111.3:</p><p>From the above examples, it is clear that the web is a powerful external resource for any open domain question answering system. LCC's TREC 2005 results showed that "web-boosting" provided an added value of 69/331% to the final factoid score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Role of the Logic Prover</head><p>COGEX performs a proof of the question over the candidate passages and scores them according to their syntactic and semantic similarity to the question. In this way, COGEX, operating on world knowledge axioms reranks, extracts, and scores the top N candidates. For example, to process question 106.2, COGEX utilizes derivational morphology available from WordNet <ref type="bibr" coords="3,462.90,271.24,66.88,8.97" target="#b2">(Fellbaum 1998)</ref> to automatically generate an axiom linking the verb "lose" with the adjective "losing". This axiom provides the necessary linguistic knowledge for COGEX to accurately verify that "the losing team" is entailed by "Padres", and not "Yankees", the statistically extracted answer. Lexical chains <ref type="bibr" coords="3,387.17,557.80,130.76,8.97" target="#b11">(Moldovan and Novischi 2002)</ref> continued to be a important resource for COGEX. Lexical chains are derived from the links in WordNet and provide a mechanism for measuring the semantic similarity between keywords in the question and keywords in the answer. The similarity measure associated with each lexical chain is used by COGEX when it scores candidate answers. In the example for Q91.4, lexical chains generate the necessary link between "buy" and "own". This link is transformed into an axiom and used by COGEX to extract "IDG Books Worldwide" as the correct answer to the question. For the TREC 2005 factoid questions, COGEX generated an enhancement of 12.4% to the final factoid score.</p><p>With the introduction of events as targets in TREC 2005, a large set of questions required the resolution of temporal constraints in candidate answers. Of the 455 questions from the list and factoid track 16% contained temporal references. To meet this anticipated need, LCC's temporal context reasoning system, described in <ref type="bibr" coords="4,244.47,153.64,54.06,8.97;4,72.00,165.64,31.49,8.97">(Moldovan et al. 2005</ref>) was incorporated into PowerAnswer-2.</p><p>The approach taken by LCC for temporal reasoning in QA can be summarized as:</p><p>1. Detect absolute dates in the question and prefers passages that match the detected temporal constraints of the question. 2. Discover events related by temporal signals in the question and candidate answers. 3. Perform temporal unification between the question and the candidate answers and boost answers that match the temporal constraints of the question Additionally, passage retrieval required a temporal index of all the absolute dates detected in the document collection. The temporal index operates on absolute dates, relative dates as well as date ranges. For example, in the question 106.5, the expression "1998" is marked as the required temporal context for the question, and the following query is executed:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q106.5: What is the name of the winning manager of 1998 Baseball World Series?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>winning AND manager AND baseball AND world AND series AND date: <ref type="bibr" coords="4,160.96,436.56,90.98,8.07">[19980101 TO 19981231]</ref> To discover the answer, events anchored by temporal expressions need to be processed. Additionally, we discover temporally related events both in questions and candidate answers. Events linked by temporal constraints are represented as a triple (S,E1,E2) which consists of a temporal signal S, e.g. "during", "after", and its corresponding event arguments E1 and E2. To produce such triplets, we also had to perform: (1) the disambiguation of signal words and (2) the attachment of events to signal words. When no temporal relations were detected in the candidate passage, the document time-stamp served as the default context based.</p><p>The temporal reasoning module integrated into PowerAnswer-2 employs two context unification modules. A full first-order logic reasoning engine was selected when the question contained a temporal event with no absolute date reference, such as "How old was Bing Crosby when he died?", and a a light-weight special purpose reasoner for questions that specified an absolute date, such as, "Who was president of DePauw in 1999?".</p><p>The general purpose temporal reasoner is incorporated in COGEX. The events and temporal relations from the question and answer are converted into a Suggested Upper Merged Ontology (SUMO) <ref type="bibr" coords="4,444.23,87.16,95.77,8.97" target="#b12">(Niles and Pease 2001)</ref> logic representation and COGEX is used to perform context resolution between the question and answer texts. This approach works for unifying temporal relations based on signal words as well as for ordering questions (first, second, third). If the temporal constraints of the question can not be unified with those in a candidate answer, the answer is discarded from the answer list. For example, question Q137.4 has a containment relation between "bombardment" and "the 1950's". Further "1950's" is normalized to the range <ref type="bibr" coords="4,494.21,206.68,46.12,8.97">[19500101,</ref><ref type="bibr" coords="4,313.20,218.68,41.51,8.97">19501231]</ref>. The range contains the time-stamp of expression"August, 1958", which constrains the event "bombardment" from the correct candidate answer. Although 16% of the list and factoid questions in the TREC 2005 test set specified temporal constraints, the temporal reasoner only added a 2% value to the overall system performance. Due to the high degree of keyword overlap in the questions and the candidate answers, PowerAnswer-2 without the temporal reasoner often ranked the correct answer in position one. The temporal reasoner only re-enforced the selected answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Processing of 'Other' Questions</head><p>The inherent challenge of "other" questions in the TREC QA Track is the filtering and selection of interesting and novel nuggets from a large corpus. The passage recall for information about a target is typically overwhelming, and pruning these passages to pick the best nuggets is time consuming and difficult. This year PowerAnswer-2 experimented with two new techniques for nugget selection that complement the existing "definition" pattern-based method. The three methods are:</p><p>[1] Nuggets discovered by question patterns. Returning vital nuggets that are not captured by a definition pattern requires a strategy that seeks characteristics of the target. Since targets can be classified in several target classes, it is natural to generate questions that seek the characteristic of each class. We replied on 33 target classes (e.g. animal, actor, musician, literature) resulting from the analysis previous TREC question sets. To gen-erate the target classes, we used a Na ïve Bayes Classifier that employs features such as WordNet synsets, stemmed surface forms of the tokens, and named entity classes. For example, for the target Bing Crosby, the system classified the target as MUSICIAN PERSON, resulting in the selection of the following set of questions:</p><p>Example: Bing Crosby Target Class: musician person What is the name of the band of X? What record company is X with? Where was X born? What kind of singer is X? When was X born?</p><p>Where was X born?</p><p>The nugget discovered by such questions is:</p><p>Tacoma-born Bing Crosby is officially named No. 1 box-office star by Quigley Poll.</p><p>[2] Nuggets discovered by entity classes. Relevant nuggets of information can be characterized by associations with other named entities in the collection. For this reason we used the semantic classifications generated by our named entity recognizer to discover such relations when looking for relevant passages. The nugget discovered by such questions is:</p><p>Akira Kurosawa, renowned Japanese filmmaker, dies at 88.</p><p>[3] Nuggets discovered by patterns. The traditional method employed by PowerAnswer to extract nuggets is to execute a definition pattern matching module. A list of over 150 positive and negative precomputed patterns is loaded into memory. The target is inserted into these patterns and the resulting query is submitted to an index including stopwords and punctuation. These are high-precision patterns that indicate information of a definitional nature. For example:</p><p>Example: Russian submarine Kursk Pattern: 3 1 ANSWER-target , which Answer: The joint British and Norwegian began on Sunday attempt to rescue any survivors on board the sunken Russian submarine Kursk, which is lying on the sea bed in the Barents Sea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Answering Relationship Questions</head><p>With the accuracy of today's best factoid Q/A systems nearing (and in some cases, exceeding) the 70% F thresh-old, work in automatic Q/A has begun to focus on the answering of complex questions. Although researchers have not yet agreed upon a standard definition of exactly what constitutes a complex (or "relationship") question, for the purposes of this paper, we claim that a relationship question can be defined as a natural language question whose information need cannot be associated with a single semantic answer type from an idealized ontology of semantic entity or event types.</p><p>Unlike factoid questions, which presuppose that a single correct answer can be found that completely satisfies all of the information requirements of the question, relationship questions often seek multiple and different types of information and do not presuppose that one single answer could meet all of its information needs simultaneously. For example, with a factoid question like Who are the members of the Rat Pack?, we assume that a user is looking for a list of names (specifically, person names) who were a part of the Rat Pack. In this case, users do not expect systems to return additional related information (such as which member of the Rat Pack was considered its leader), as the answer itself is sufficient to meet the information need of the question. In fact, returning more than the requested information is undesirable, as the system would be more informative than necessary and violate the Gricean Maxim of Quantity. In contrast, with a relationship question like What impact did the Rat Pack have on the rise of the Las Vegas tourism industry?, the wider focus of this question indicates that users may not have a clearly defined (or pragmatically restrictive) information need, and therefore would be amenable to receiving additional supporting information that was relevant to their overall goal.</p><p>In this section, we describe the approach we used to answer complex questions for the TREC 2005 Relationship Task. Since we believe that answering relationship questions depends on sophisticated representation of the information need of these complex questions, we have implemented an approach which employs three question representation strategies to find answers: (1) an approach based on keyword selection, (2) an approach based on topic representation, and (3) an approach based on automatic lexicon generation. We show that by combining results from each of these components, we can achieve high levels of performance with almost no manual processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">The PALANTIR Complex Question-Answering System</head><p>We chose to extend LCC's PALANTIR Q/A system for the TREC 2005 Relationship Task. Developed for interactive question-answering applications like LCC's FERRET Dialog System, PALANTIR incorporates techniques for key-word selection, passage retrieval, and answer ranking that were developed to address the types of complex questions that are typically asked by users in an interactive Q/A dialog. Furthermore, since complex questions lack a single identifiable semantic answer type, we felt that PALANTIR's multiple answer finding strategies could be leveraged to find a wider range of answers than the question processing and answer justification modules implemented in POWERANSWER-2. Before relationship questions for this task are submitted to PALANTIR, questions are first manually processed to resolve pronouns and other referring expressions and to remove instances of ellipsis. Questions are then sent to an automatic question decomposition module which uses a set of heuristics to break complex questions into a set of syntactically simpler questions. Keywords are then extracted by a keyword selection module which detects collocations and ranks keywords with an approximation of their importance. Documents are then retrieved and submitted to a trio of answer finding strategies. Candidate answers from each strategy are then merged and ranked; the 7 top-ranked passages are returned as answers. A block diagram for our system is presented in Figure <ref type="figure" coords="6,280.08,338.20,3.77,8.97" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>As in our submission to the 2004 AQUAINT Relation-ship Pilot, coreference and ellipsis resolution were done manually for each question. Relationship questions were automatically syntactically decomposed using heuristics that split conjoined NPs and lists into separate questions and extracted embedded questions. For example, a complex question context like The analyst is concerned with a possible relationship between the Cuban and Congolese governments. Specifically, the analyst would like to know of any attempts by these governments to form trade or military alliances. was automatically split into the three questions in Figure <ref type="figure" coords="6,393.03,194.68,3.77,8.97">4</ref>. Semantic question decomposition of the type described in <ref type="bibr" coords="6,410.77,206.68,76.82,8.97" target="#b5">(Hickl et al., 2004)</ref> was not performed for any questions in this year's task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyword Selection</head><p>The following techniques were employed for keyword selection: Collocation Detection. When doing keyword selection for complex questions, precise collocation detection is necessary. For example, in order to retrieve documents concerning the Organization of African States, systems must retrieve documents containing the collocation as a whole, and not just its individual tokens. Keyword Ranking. PALANTIR assigns a weight to each keyword extracted from a complex question as part of its document retrieval strategy. Based on an approach first outlined in <ref type="bibr" coords="6,358.51,517.12,92.37,8.97" target="#b10">(Moldovan et al., 2004)</ref>, weights are assigned heuristically based on a rough approximation of the keyword's overall importance to a query. For example, in the current version of PALANTIR, the highest weights were assigned to proper names (NNPs), followed by comparative and superlative adjectives, ordinal numbers, and quoted text. Keyword Expansion. Synonyms and alternate forms for each keyword were added from a database of similar terms developed for past TREC Q/A evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Retrieval</head><p>As with the TREC 2004 version of PALANTIR <ref type="bibr" coords="6,495.91,653.44,44.22,8.97;6,313.20,665.32,52.59,8.97" target="#b10">(Moldovan et al., 2004)</ref>, we used an preprocessed and indexed version of the TREC Q/A corpus that had been previously annotated with part-of-speech information, syntactic parse information, and named entity information taken from LCC's CICEROLITE named entity recognition soft-ware. After keyword selection is complete, PALANTIR's document retrieval system uses a state machine-based approach that iteratively drops keywords of lesser importance in order to find the most relevant documents. Once a set of documents has been retrieved, documents are segmented into sentences and text passages are extracted that contain clusters of keywords. Passage length is determined dynamically based on the number of keywords found within a set of sentences; the average passage length is three sentences.</p><p>Once a set of passages have been retrieved, PALANTIR employs three different strategies to find answers. The first strategy, known as Keyword Density, ranks candidate answers using a score based on the number, weight, and relative position of the question words (and alternations) found in the passages. (A version of this approach was the only strategy used in our submission in the 2004 AQUAINT Relationship Pilot.) The second strategy uses sophisticated Topic Representations to select candidate answers that contain specific topic-relevant words and relations derived from the set of documents retrieved during document retrieval. Finally, PALANTIR uses an approach based on Lexicon Generation to automatically expand keywords that may denote a set of terms (e.g. South American countries, Latin America, high-tech weaponry) to their full membership.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyword Density Strategy</head><p>PALANTIR's Keyword Density <ref type="bibr" coords="7,200.76,406.48,98.03,8.97" target="#b10">(Moldovan et al., 2004)</ref> heuristic assigns a score to each passage returned during passage retrieval. Top passages are run through a series of redundancy filters to remove duplicate (or overlapping) answers. Two sets of features are then used to rank the remaining passages: Surface Ranking. 50 of the more than 80 surface features used in PALANTIR for factoid Q/A were chosen to rank answers to relationship questions. Features were selected based on their compatibility with passage-length answers as well as for the overall speed and performance of the system. Relation Ranking. In addition, candidate answer passages were ranked based on a series of relational features. This relation-based ranking considers a set of features based on a dependency parse of both the question and each of the sentences in the candidate answer passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Representation Strategy</head><p>In this approach, we used two different topic representation strategies to rank candidate answer passages. Similar to the approach employed by LCC's LITE-GISTEXTER question-directed summarization system <ref type="bibr" coords="7,235.12,665.32,63.52,8.97;7,72.00,677.32,23.48,8.97" target="#b6">(Lacatusu et al., 2004)</ref> for the DUC 2005 summarization evaluations, this approach assumes that answers to complex relationship questions can be identified by selecting those passages that contain a preponderance of topic terms and topic re-lations.</p><p>PALANTIR's Topic Representation strategy uses two different topic representations to identify the set of relevant sentences that should be included in a summary: topic signatures ( ¢¡ ¤£</p><p>) and enhanced topic signatures ( ¢¡ ¦¥</p><p>). Originally developed for single-document summarization <ref type="bibr" coords="7,360.55,146.92,91.78,8.97" target="#b7">(Lin and Hovy, 2000)</ref>, the topic signature algorithm computes a weight for each term in a document cluster based on its relative frequency in a relevant set of documents. (Complete details of our topic signature implementation are provided in <ref type="bibr" coords="7,473.81,194.68,66.04,8.97;7,313.20,206.68,20.96,8.97" target="#b6">(Lacatusu et al., 2004)</ref>.) In DUC 2005, we also experimented with the enhanced topic signatures first described by <ref type="bibr" coords="7,492.50,218.68,47.57,8.97;7,313.20,230.56,21.66,8.97" target="#b4">(Harabagiu, 2004)</ref>. Unlike Lin and Hovy's topic signatures (which are limited to sets of individual terms), Harabagiu's enhanced topic signatures can be used to discover a set of relevant relations that exist between topic signature terms and to provide each relation with a weight depending on its overall significance to the topic being modeled. With enhanced topic signatures, topics are represented as the set of relevant relations that exist between topic signature terms: §¡ ¥ © ! #" %$ &amp; (' #£ #" 0) 1£ %2 43 53 63 7 (' 98 @" A) B8 C2 ED F2</p><p>, where ' 9G is a binary relation between two topic concepts. Two different forms of topic relations are considered by this approach: (1) syntax-based relations that exist between the verbs and their arguments; and (2) contextbased relations (C-relations) that exist between entities. We calculate enhanced topic signatures in the manner described in <ref type="bibr" coords="7,354.43,409.96,71.27,8.97" target="#b4">(Harabagiu, 2004)</ref>. Examples of ¢¡ £ and ¢¡ ¥</p><p>for Question 9 (The analyst is concerned with a possible relationship between the Cuban and Congolese governments. Specifically, the analyst would like to know of any attempts by these governments to form trade or military alliances.) are presented in Table <ref type="table" coords="7,447.80,469.72,3.77,8.97" target="#tab_7">1</ref>.</p><p>Candidate answer passages are assigned a composite score equal to the sum of the weights of all of the topic terms and/or relations they contained; answers are ranked according to this score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexicon Generation Strategy</head><p>Although relationship questions do not feature semantic answer types, they do often include non-specific entities which denote a set of individuals. For example, in question topic 23, The analyst is interested in knowing which South American countries are involved in nuclear proliferation., the NP South American countries denotes the set of countries found on the continent of South America. Although named entity recognition systems (NER) such as LCC's CICEROLITE have been used successfully identify sets of candidate answers for more than 150 different kinds of answer types, no current NER system includes enough semantic types to identify the extension of every possible set-denoting entity. With relationship questions, knowing the full extension for a particular entity -like South American countries -can pro- vide an invaluable source of keywords that can be used to find additional relevant information. In order to answer relationship questions like the above, we have implemented a system based on a weakly-supervised learning approach described in <ref type="bibr" coords="8,178.06,497.68,98.38,8.97" target="#b13">(Thelen and Riloff, 2002</ref>) that can identify a set of entities semantically related to a set of automatically-generated seed tokens. Crucial to this approach is the generation of a large database of syntactic frames which are used to approximate the different types of semantic relationships that can exist between entities.</p><p>We populated this database with a variety of extraction patterns first written for LCC's CICERO information extraction software.</p><p>In this strategy, decomposed questions are sent to an Answer Type Term Detection module used in PALANTIR's factoid Q/A system. NPs that are detected as potential answer type terms are sent to a Seed Generation module that searches in a frame database for potential matches. If more than 5 matches are found in the database, the NP is sent to a Lexicon Generation module which uses the <ref type="bibr" coords="8,86.98,689.32,103.55,8.97" target="#b13">(Thelen and Riloff, 2002)</ref> method to generate potential expansions. For example, given the NP South American countries, this approach returns two additional South American countries: Brazil and Argentina. Once lexicon generation is complete, each original question (not including the generated terms) is sent to PALANTIR's Keyword Selection module and candidate answers are generated as with the Keyword Density strategy. Terms identified by Lexicon Generation are used to filter candidate answers: only answers containing the generated terms are returned as final answers to relationship questions.</p><p>In future work, we plan to experiment with different applications of this technique in order to best determine how to use terms identified by the Lexicon Generation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Merging</head><p>Answers from each answer-finding strategy were combined using an Answer Merging module. The Answer Merging module first combined the top answers from each strategy for a single decomposed question. This merging is done by heuristically normalizing the scores that each strategy assigns to every answer. Duplicate and overlapping answers are filtered again using the same filters employed by the Keyword Density strategy. Once the answers for every decomposed question are ranked, at least the top three answers from each decomposed question are presented as the answers to the complex question. non-redundant, non-overlapping candidate answers as answers to each relationship question. strategy's answers were separately top answers from each strategy are merged answers to every decomposed question are then to the complex question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results</head><p>The following This year, our two submissions for the Relationship Task differed in terms of the total number of keywords considered in document retrieval. In Run 1, only a limited (high-confidence) set of alternations were used; Run 2 used a much larger set of alternations for each question. Table <ref type="table" coords="8,337.53,653.08,5.03,8.97">9</ref> compares performance of those two runs.</p><p>For both runs, the F-Measure was above the median score for all groups: Run 1 scored 0.204, while Run 2 received a 0.179 score. Although we kept our manual processing of question to a minimum -using only automatic methods for keyword expansion and syntactic question decomposition -we did perform manual resolution of coreference and ellipsis for 14 of the 25 scenarios. (Both of our runs were considered to be in the set of "manually processed" systems by the NIST assessors.) When only "manually processed" scores were considered, only Run 1 was above the median; Run 2 has the median score among the 9 groups. Since we believed that including a greater number of keyword alternations would enable us to find a wider range of answers, we were surprised by the fact that Run 2 received a somewhat lower score than Run 1. However, we doubt if this result is truly significant: Run 2 featured only 1.1 more keywords than Run 1 on average -a difference which resulted in the loss of only 5 total nuggets overall.</p><p>In addition to the overall F-measure, we calculated two additional recall measures to help interpret our results: Document Retrieval Recall and Passage Retrieval Recall. We define Document Retrieval Recall as the percentage of vital nuggets returned in the documents under consideration; similarly, Passage Retrieval Recall is defined as the percentage of vital nuggets returned in the passages under consideration. (When relationship questions were decomposed into sets of subquestions, we computed a single recall (Document or Passage) measure which combines results from each of the subquestions.)  Document Retrieval Recall for both runs was simliar, just above 50%. This is substantially lower than typical factoid Document Retrieval Recall, which for the factoid version of PALANTIR on TREC 2004 was above 90%. We believe this is for two reasons. First, relationship questions contain many more keywords on average than factoid questions. In order to be successful in answering relationship questions, systems must identify exactly which keywords should be submitted to a document retrieval query; inclusion of less relevant keywords could result in the retrieval of spurious documents. Second, without an overt semantic answer type, relationship Q/A systems cannot make use of the valuable semantic information provided by available named entity recognition systems. Passage Retrieval Recall for both runs was slightly lower than their respective Document Retrieval Recall numbers. This is similar to factoid Passage Retrieval Recall, which is usually around 10% lower than Document Retrieval Recall. We believe our substantially lower Answer Recall is due to our answer ranking algo-rithm. In future work, we will experiment with novel answer ranking techniques that will enable us to keep more of the answers we retrieve. Table <ref type="table" coords="9,349.41,217.84,5.03,8.97">9</ref> presents a comparison of the three answerfinding strategies in PALANTIR. We found that the traditional density strategy -first introduced in the AQUAINT 2004 Relationship Pilot -remains dominant, providing 83% of our total answers, 85% of vital nuggets, and 87% of okay nuggets. Although the Topic Representation-and Lexicon Generation-based strategies were used much less frequently, they did contribute approximately 14% of the total nuggets (vital and okay) that system returned. We are encouraged by the fact that our overall F-measurecombining results from all three strategies -is higher than the performance of any individual strategy. Although the sample size is rather small, these results suggest that using a hybrid approach that combines results from multiple answer-finding strategies may be effective for these types of relationship questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,222.60,239.08,166.86,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of PowerAnswer-2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,345.72,345.88,161.95,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Logical Form Transformations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,72.00,660.16,226.84,8.97;6,72.00,672.04,49.04,8.97"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of LCC's PALANTIR Complex Q/A System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,318.60,242.79,216.08,5.59;6,318.60,249.75,215.87,5.53;6,318.60,256.71,206.28,5.53;6,318.60,270.99,65.59,5.53;6,322.40,274.40,5.80,11.60;6,331.68,282.99,199.21,5.59;6,321.96,289.95,38.02,5.53;6,363.50,281.30,5.70,11.60;6,372.84,289.95,157.90,5.59;6,321.96,296.91,56.49,5.53;6,382.20,288.30,5.70,11.60;6,391.44,296.91,139.32,5.59;6,321.96,303.87,83.33,5.59"><head>Complex Question :</head><label>:</label><figDesc>The analyst is concerned with a possible relationship between the Cuban and Congolese governments. Specifically, the analyst would like to know of any attempts by these governments to form trade or military alliances. Syntactic Decomposition ¢¡ : What possible relationship is there between the Cuban and Congolese governments? ¤£ : What attempts by the Cuban and Congolese governments to form trade alliances? ¤¥ : What attempts by the Cuban and Congolese governments to form military alliances?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,357.24,333.16,138.92,8.97"><head>Figure</head><label></label><figDesc>Figure 4: Question Decomposition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,78.00,73.84,216.57,349.31"><head>Table 1 :</head><label>1</label><figDesc>Signatures for question ¢¡</figDesc><table coords="8,78.00,73.84,216.57,326.20"><row><cell>Topic Signature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>congo(pn)</cell><cell>1961</cell><cell>congolese(pn)</cell><cell>1374</cell></row><row><cell>cuba(pn)</cell><cell>1322</cell><cell>kabila(pn)</cell><cell>1278</cell></row><row><cell>cuban(a)</cell><cell>895</cell><cell>rebel(n)</cell><cell>846</cell></row><row><cell>rwandan(pn)</cell><cell>819</cell><cell>rwanda(pn)</cell><cell>795</cell></row><row><cell>castro(pn)</cell><cell>440</cell><cell>kinshasa(pn)</cell><cell>421</cell></row><row><cell>uganda(pn)</cell><cell>371</cell><cell>ugandan(pn)</cell><cell>338</cell></row><row><cell>troop(n)</cell><cell>202</cell><cell>embargo(n)</cell><cell>200</cell></row><row><cell>zimbabwe(pn)</cell><cell>175</cell><cell>laurent kabila(pn)</cell><cell>171</cell></row><row><cell>eastern congo(pn)</cell><cell>156</cell><cell>angola(pn)</cell><cell>154</cell></row><row><cell>president laurent kabila(pn)</cell><cell>153</cell><cell>ally(n)</cell><cell>153</cell></row><row><cell>cuban(n)</cell><cell>125</cell><cell>cuban(pn)</cell><cell>116</cell></row><row><cell>first(a)</cell><cell>113</cell><cell>tutsi(pn)</cell><cell>112</cell></row><row><cell>havana(pn)</cell><cell>109</cell><cell>rebellion(n)</cell><cell>106</cell></row><row><cell>namibia(pn)</cell><cell>100</cell><cell>war(n)</cell><cell>99</cell></row><row><cell>hutu(pn)</cell><cell>90</cell><cell>island(n)</cell><cell>88</cell></row><row><cell>mobutu sese seko(pn)</cell><cell>87</cell><cell>cease(v)</cell><cell>85</cell></row><row><cell>fidel castro(pn)</cell><cell>75</cell><cell>dictator(n)</cell><cell>73</cell></row><row><cell>exile(n)</cell><cell>64</cell><cell>rebel(a)</cell><cell>59</cell></row><row><cell>soldier(n)</cell><cell>59</cell><cell>border(n)</cell><cell>57</cell></row><row><cell>accuse(v)</cell><cell>56</cell><cell>fight(v)</cell><cell>54</cell></row><row><cell>zambia(pn)</cell><cell>53</cell><cell>back(v)</cell><cell>51</cell></row><row><cell>Enhanced Topic Signature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Congolese -rebel</cell><cell>103</cell><cell>NE:LOCATION -policy</cell><cell>83</cell></row><row><cell>Congolese -NE:PERSON</cell><cell>65</cell><cell>Congolese -NE:OTHER</cell><cell>55</cell></row><row><cell>dictator -Mobutu Sese Seko</cell><cell>55</cell><cell>Rwandan -troop</cell><cell>55</cell></row><row><cell>NE:LOCATION -back</cell><cell>52</cell><cell>dictator -NE:OTHER</cell><cell>47</cell></row><row><cell>Congolese -government</cell><cell>45</cell><cell>NE:OTHER -rebel</cell><cell>43</cell></row><row><cell>rebel -group</cell><cell>39</cell><cell>back -rebel</cell><cell>38</cell></row><row><cell>ally -NE:LOCATION</cell><cell>38</cell><cell>food -medicine</cell><cell>35</cell></row><row><cell>dia -NE:PERSON</cell><cell>35</cell><cell>Hutu -militia</cell><cell>35</cell></row><row><cell>Cuban -NE:PERSON</cell><cell>34</cell><cell>Rwandan -soldier</cell><cell>33</cell></row><row><cell>food -sale</cell><cell>33</cell><cell>rebel -try</cell><cell>33</cell></row><row><cell>NE:PERSON -government</cell><cell>33</cell><cell>cash -transfer</cell><cell>31</cell></row><row><cell>PPE:DATE TIME -genocide</cell><cell>31</cell><cell>NE:LOCATION -war</cell><cell>30</cell></row><row><cell>charter -flight</cell><cell>30</cell><cell>back -NE:PERSON</cell><cell>28</cell></row><row><cell>Rwandan -government</cell><cell>28</cell><cell>NE:LOCATION -most</cell><cell>28</cell></row><row><cell>Lead -move</cell><cell>28</cell><cell>send -troop</cell><cell>28</cell></row><row><cell>Cold -War</cell><cell>28</cell><cell>Hutu -rebel</cell><cell>28</cell></row><row><cell>come -power</cell><cell>28</cell><cell>PPE:NUMBER -troop</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,313.20,460.12,226.70,119.61"><head>Table 2 :</head><label>2</label><figDesc>table illustrates the final results of Language Computer's efforts in the 2005 TREC main QA track obtained by PowerAnswer-2. Results in the main task.</figDesc><table coords="8,377.16,506.88,96.55,51.51"><row><cell></cell><cell>PowerAnswer-2</cell></row><row><cell>Factoid</cell><cell>0.713</cell></row><row><cell>List</cell><cell>0.468</cell></row><row><cell>Other</cell><cell>0.228</cell></row><row><cell>Overall</cell><cell>0.534</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,87.00,461.92,196.61,8.97"><head>Table 3 :</head><label>3</label><figDesc>The two submitted results of PALENTIR.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,319.20,121.08,227.06,78.01"><head>Table 4 :</head><label>4</label><figDesc>Strategy comparison (Run 1).</figDesc><table coords="9,319.20,121.08,227.06,51.51"><row><cell>Strategies</cell><cell cols="3">Total Vital Okay</cell><cell>Rec</cell><cell>Prec</cell><cell>F</cell></row><row><cell>Density</cell><cell>154</cell><cell>23</cell><cell cols="3">21 0.259 0.065 0.167</cell></row><row><cell>Lexicon</cell><cell>3</cell><cell>1</cell><cell cols="3">1 0.020 0.005 0.015</cell></row><row><cell>Topic</cell><cell>29</cell><cell>3</cell><cell cols="3">2 0.105 0.021 0.072</cell></row><row><cell>Combined</cell><cell>186</cell><cell>27</cell><cell cols="3">24 0.306 0.079 0.204</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the researchers and engineers from <rs type="affiliation">Language Computer Corporation</rs> for their valuable contributions to this work. This work, as our broader research in QA is supported by the <rs type="funder">Advanced Research Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,313.20,547.48,226.63,8.97;9,323.16,558.52,216.71,8.97;9,323.16,569.44,216.71,8.97;9,323.16,580.36,216.81,8.97;9,323.16,591.40,84.13,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,483.37,547.48,56.46,8.97;9,323.16,558.52,216.71,8.97;9,323.16,569.44,123.29,8.97">Using Knowledge Extraction and Maintenance Techniques to Enhance Analytical Performance</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bixler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,461.13,569.44,78.75,8.97;9,323.16,580.36,216.81,8.97;9,323.16,591.40,9.71,8.97">Proceedings of the 2005 International Conference on Intelligence Analysis</title>
		<meeting>the 2005 International Conference on Intelligence Analysis<address><addrLine>Washington D.C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,613.96,226.54,8.97;9,323.16,624.88,216.67,8.97;9,323.16,635.80,216.69,8.97;9,323.16,646.84,216.82,8.97;9,323.16,657.76,216.72,8.97;9,323.16,668.68,216.71,8.97;9,323.16,679.72,45.06,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,489.86,624.88,49.97,8.97;9,323.16,635.80,216.69,8.97;9,323.16,646.84,60.54,8.97">Looking under the hood: Tools for diagnosing your question answering engine</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rooth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,408.26,646.84,131.72,8.97;9,323.16,657.76,216.72,8.97;9,323.16,668.68,216.71,8.97;9,323.16,679.72,40.55,8.97">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answering</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answering</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,702.28,226.68,8.97;9,323.16,713.20,86.43,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,404.53,702.28,135.35,8.97;9,323.16,713.20,34.72,8.97">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,75.16,226.64,8.97;10,81.96,86.08,216.80,8.97;10,81.96,97.12,216.74,8.97;10,81.96,108.04,216.71,8.97;10,81.96,118.96,46.96,8.97" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Stephan</surname></persName>
		</author>
		<title level="m" coord="10,172.31,86.08,126.46,8.97;10,81.96,97.12,216.74,8.97;10,81.96,108.04,216.71,8.97;10,81.96,118.96,17.54,8.97">Applying COGEX to Recognize Textual Entailment In Prooceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,137.92,226.87,8.97;10,81.96,148.84,194.77,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,161.76,137.92,133.17,8.97">Incremental topic representations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.75,148.84,179.60,8.97">Proceedings of the 20th COLING Conference</title>
		<meeting>the 20th COLING Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,167.80,226.77,8.97;10,81.96,178.72,22.64,8.97;10,126.79,178.72,172.03,8.97;10,81.96,189.76,216.68,8.97;10,81.96,200.68,216.59,8.97;10,81.96,211.60,76.25,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,126.79,178.72,172.03,8.97;10,81.96,189.76,125.10,8.97">Experiments with interactive questionanswering in complex scenarios</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.94,189.76,73.71,8.97;10,81.96,200.68,216.59,8.97;10,81.96,211.60,51.09,8.97">Proceedings of the Workshop on the Pragmatics of Question Answering at HLT-NAACL</title>
		<meeting>the Workshop on the Pragmatics of Question Answering at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,230.56,226.72,8.97;10,81.96,241.48,216.61,8.97;10,81.96,252.52,22.64,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,81.96,241.48,109.71,8.97">Lite-gistexter at DUC 2004</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hickl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nezda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.83,241.48,84.74,8.97;10,81.96,252.52,18.11,8.97">Proceedings of DUC 2004</title>
		<meeting>DUC 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,271.48,226.74,8.97;10,81.96,282.40,216.72,8.97;10,81.96,293.32,169.48,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,192.43,271.48,106.31,8.97;10,81.96,282.40,175.60,8.97">The automated acquisition of topoic signatures for text summarization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,280.82,282.40,17.86,8.97;10,81.96,293.32,165.10,8.97">Proceedings of the 18th COLING Conference</title>
		<meeting>the 18th COLING Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,312.28,226.66,8.97;10,81.96,323.20,216.82,8.97;10,81.96,334.24,216.69,8.97;10,81.96,345.16,149.70,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,130.96,312.28,167.70,8.97;10,81.96,323.20,152.46,8.97">The Web as a Resource for Question Answering: Perspectives and Challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,249.29,323.20,49.49,8.97;10,81.96,334.24,216.69,8.97;10,81.96,345.16,93.95,8.97">Proceedings of the third International Conference on Language Resources and Evaluation</title>
		<meeting>the third International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,364.12,226.76,8.97;10,81.96,375.04,216.71,8.97;10,81.96,385.96,216.70,8.97;10,81.96,397.00,216.71,8.97;10,81.96,407.92,216.61,8.97;10,81.96,418.84,54.14,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,110.48,375.04,188.20,8.97;10,81.96,385.96,11.50,8.97">Cogex: A Logic Prover for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,118.28,385.96,180.38,8.97;10,81.96,397.00,216.71,8.97;10,81.96,407.92,189.51,8.97">Proceedings of the Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference</title>
		<meeting>the Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
	<note>HLT-2003</note>
</biblStruct>

<biblStruct coords="10,72.00,437.80,226.75,8.97;10,81.96,448.72,216.82,8.97;10,81.96,459.76,216.71,8.97;10,81.96,470.68,150.69,8.97;10,72.00,489.64,217.03,8.97;10,89.52,504.52,209.29,8.97;10,81.96,515.56,74.06,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,267.71,437.80,31.04,8.97;10,81.96,448.72,216.82,8.97;10,81.96,459.76,7.79,8.97;10,89.52,504.52,209.29,8.97">Temporal Context Representation and Reasoning To appear in</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu ; Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.51,459.76,206.15,8.97;10,81.96,470.68,150.69,8.97;10,72.00,489.64,9.70,8.97">the Proceedings of the Nineteenth Internation Joint Conference on Aritificial Intelligence D</title>
		<imprint>
			<date type="published" when="2004">2005. 2004 2004</date>
		</imprint>
	</monogr>
	<note>PowerAnswer-2: Experiments and Analysis over</note>
</biblStruct>

<biblStruct coords="10,72.00,534.40,226.65,8.97;10,81.96,545.44,216.62,8.97;10,81.96,556.36,73.54,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,221.46,534.40,77.20,8.97;10,81.96,545.44,82.85,8.97">Lexical Chains for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.76,545.44,103.82,8.97;10,81.96,556.36,18.11,8.97">Proceedings of COLING 2002</title>
		<meeting>COLING 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="674" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,575.32,226.62,8.97;10,81.96,586.24,216.70,8.97;10,81.96,597.16,216.59,8.97;10,81.96,608.20,127.63,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,190.17,575.32,108.46,8.97;10,81.96,586.24,35.25,8.97">Towards a Standard Upper Ontology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.50,586.24,171.16,8.97;10,81.96,597.16,212.12,8.97">Proceedings of the 2nd International Conference on Formal Ontology in Information Systems</title>
		<meeting>the 2nd International Conference on Formal Ontology in Information Systems<address><addrLine>Ogunquit, Maine</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">2001. October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,627.04,226.76,8.97;10,81.96,638.08,216.70,8.97;10,81.96,649.00,216.69,8.97;10,81.96,659.92,211.07,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,201.45,627.04,97.31,8.97;10,81.96,638.08,216.70,8.97;10,81.96,649.00,31.74,8.97">A bootstrapping method for learning semantic lexicons using extraction pattern contexts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,136.26,649.00,162.39,8.97;10,81.96,659.92,206.85,8.97">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
