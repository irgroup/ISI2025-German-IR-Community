<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,240.00,72.00,132.00,12.00;1,220.00,84.00,172.00,12.00">JHU/APL at TREC 2005: QA Retrieval and Robust Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.00,106.75,71.78,11.00"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.22,106.75,69.79,11.00"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Research and Technology Development Center The</orgName>
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,240.00,72.00,132.00,12.00;1,220.00,84.00,172.00,12.00">JHU/APL at TREC 2005: QA Retrieval and Robust Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">29B404B86EAB62D0A920FE610468C691</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Johns Hopkins University Applied Physics Laboratory (JHU/APL) focused on the Robust and Question Answering Information Retrieval (QAIR) Tracks at the 2005 TREC conference. For the Robust Track, we attempted to use the difference in retrieval scores between the top retrieved and the 100th document to predict performance; the result was not competitive. For QAIR, we augmented each query with terms that appeared frequently in documents that contained answers to questions from previous question sets; the results showed modest gains from the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAIRCUT</head><p>The Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) [3] is a document retrieval system developed at the Applied Physics Laboratory. It uses a traditional inverted index, a unigram language model for its similarity metric <ref type="bibr" coords="1,358.00,326.75,11.96,11.00" target="#b1">[2,</ref><ref type="bibr" coords="1,376.00,326.75,7.98,11.00" target="#b3">4]</ref>, and a flexible tokenizer. The tokenizer supports words, stems, character n-grams, word n-grams and phrases. We have focused on language-independent techniques in developing HAIRCUT. It has been evaluated in TREC, CLEF and NTCIR in at least sixteen languages, and routinely performs among the top systems for both monolingual and translingual ad hoc retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering Information Retrieval Track</head><p>We were interested in whether we could identify terms that would be indicative of answers to particular types of questions. For example, it is reasonable to think that words such as famous, himself and died might be found frequently in or near answers to PERSON questions. If such terms can be accurately identified, they might be used to influence a document's ranking. We used the <ref type="bibr" coords="1,132.40,469.75,51.51,11.00">TREC-2002</ref><ref type="bibr" coords="1,198.12,469.75,19.20,11.00">TREC- , -2003</ref><ref type="bibr" coords="1,250.74,469.75,20.78,11.00">TREC- , and -2004</ref> questions and judgments to identify indicator terms. Our process begins by identifying a small taxonomy of question types, comprising HOW, HOW_MANY, WHAT_IS, WHEN, WHERE, WHO and OTHER questions. To automatically assign a question to its question type, we first parse the question using the Charniak Parser <ref type="bibr" coords="1,381.60,502.75,11.82,11.00" target="#b0">[1]</ref>. We then use a simple patternmatching approach to map linearized parse trees onto question types. Any question that does not match a pattern is assigned to an 'OTHER' category.</p><p>Once the training questions are partitioned into question types, we want to use those assignments to identify indicator terms. We exploited the relevance judgments (qrels) from TRECs 2002 through 2004 for this purpose. For each question type, we identify two document sets: those that were listed as 'relevant' for questions of that type, and those that appeared in the judgments but that were judged not relevant. The QA judgments are not binary, but have four relevance values (descriptions taken from the track guidelines):</p><p>• incorrect: the answer-string does not contain a correct answer or the answer is not responsive;</p><p>• unsupported: the answer-string contains a correct answer but the document returned does not support that answer; • non-exact: the answer-string contains a correct answer and the document supports that answer, but the string contains more than just the answer (or is missing bits of the answer); • correct: the answer-string consists of exactly a correct answer and that answer is supported by the document returned. For our purposes, we treat correct and non-exact as relevant, and all others as non-relevant.</p><p>Given these two document sets (judged relevant and judged non-relevant), we want to extract terms that are prominent in the relevant documents but not in the non-relevant documents. To do so, we first consider each document set separately, extracting words that appear frequently in that set, but relatively infrequently in the collection as a whole. We use a home-brew 'affinity' statistic for this purpose, but other measures, such as mutual information or the Dice coefficient, might work as well. The result is an ordered list of scored terms. We then score each term by the difference of its scores in the two sets. Finally, we select the top scoring terms as the expansion terms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Sample augmentation terms for three question categories</head><p>We generated augmentation terms for each question type. Sample augmentation terms are show in Table <ref type="table" coords="2,102.00,470.75,4.17,11.00">1</ref>. Some augmentation terms are clearly sensible, such as north, near and miles for WHERE questions. Others, such as rock and ago, make less intuitive sense. Using retrieved but non-relevant documents to provide terms that should not appear in the augmentation lists eliminates most question-specific terms. However, there would certainly appear to be a fair amount of noise in these augmentation term lists. Nonetheless, we used the unaltered lists in our experiments.</p><p>To process each question, we first assign it a question type. We then build a new query, comprising the terms from the original query, plus the expansion terms for the selected question type. We weight query terms at a ratio of 100:1 relative to the expansion terms. This weighting was selected arbitrarily with no experimentation; a more accurate tuning of this parameter might lead to significant additional gains from the technique. Finally, we process the augmented query normally; we used HAIRCUT with words as indexing terms, a unigram language model with α=0.5, and no blind relevance feedback.</p><p>Our results showed a modest improvement from the use of this technique. Mean average precision without augmentation was 0.3322, while with augmentation it was 0.3417. The latter score ranked fifth among the submitted systems. Augmentation produced a 3% relative improvement. This gain was achieved with a simplistic question type assignment mechanism, and with no tuning of the weighting parameter. It is reasonable to expect that further gains might be seen if these two weaknesses were addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Track</head><p>In the Robust we looked for a simple predictor of retrieval effectiveness. We use a unigram language model for our similarity metric, and we were curious whether the document scores could tell us anything about performance. We therefore correlated various combinations of the scores of the top document, the tenth document, and the 100th document, with the average precision of the query, using the TREC-2004 Robust Track data as training data. We found the best correlations when taking the ratio of the score of the 100th document with the score of the top document. For 5-grams, the correlation was 0.57, while for words it was 0.53. We therefore used this ratio as our predictor for success on each query.</p><p>We were also interested in whether the use of phrases mined from the target collection could improve performance. We used suffix arrays <ref type="bibr" coords="3,254.00,203.75,13.05,11.00" target="#b4">[5]</ref> to identify all high frequency phrases in the document collection. We deleted leading and trailing closed class words from these phrases, and added the resulting phrase list as additional indexing terms.</p><p>We submitted five runs:</p><p>• apl05pd: description-only run using phrases.</p><p>• apl05prf: description-only run using phrases and blind relevance feedback.</p><p>• apl05pt: title-only run using phrases.</p><p>• apl05cmb: a combination of a word run, a character 5-gram run, apl05pd and apl05prf. We used z-score normalization to combine the runs, and weighted the four runs evenly. • apl05p50: Under the theory that high-performing queries are more likely to benefit from blind relevance feedback, we selected each query's answer from either apl05pd or apl05prf according to whether it was predicted to be in the lower performing half or the upper performing half respectively. We used the ratio described above to predict into which half a query would fall. Performance was uniformly poor relative to the field. Results are shown in Table <ref type="table" coords="3,458.00,368.75,4.17,11.00" target="#tab_0">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Using ratios of language model retrieval scores to predict retrieval performance was not particularly effective relative to other techniques described at TREC-2005; we do not recommend its use. Assigning different document priors based on question type can produce a boost in retrieval performance. Our technique of adding query terms based on the question type is a way to exploit nonuniform document priors in any retrieval system without modifying the index. Our experiments were not finely tuned; our question type assignment was simplistic, and we tried only a single query term weight assignment. We nonetheless got a performance boost from the technique in the TREC-2005 QAIR task, as well as in our own cross-validation experiments on the question sets from the three prior TRECs. We therefore recommend the technique as a way to produce a modest boost in retrieval effectiveness for question answering systems. Careful tuning of the approach would likely increase that boost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,368.75,467.78,183.00"><head>Table 2 .</head><label>2</label><figDesc>. Based on these results, we cannot recommend the simple query difficulty metric we used in these studies. Performance on APL's five Robust Track runs.</figDesc><table coords="3,199.00,414.75,192.99,115.00"><row><cell>Run</cell><cell>MAP</cell><cell>Geometric MAP</cell></row><row><cell>apl05pd</cell><cell>0.1411</cell><cell>0.0593</cell></row><row><cell>apl05prf</cell><cell>0.1654</cell><cell>0.0504</cell></row><row><cell>apl05pt</cell><cell>0.1374</cell><cell>0.0663</cell></row><row><cell>apl05cmb</cell><cell>0.1709</cell><cell>0.0890</cell></row><row><cell>apl05p50</cell><cell>0.1528</cell><cell>0.0607</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,88.97,93.75,440.35,11.00;4,90.00,104.75,437.55,11.00;4,90.00,115.75,94.00,11.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,156.15,93.75,162.50,11.00">A maximum-entropy-inspired parser</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,348.14,93.75,181.18,11.00;4,90.00,104.75,112.15,11.00">Proceedings of the First Meeting of the North American Chapter</title>
		<meeting>the First Meeting of the North American Chapter<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,88.35,137.75,412.39,11.00;4,90.00,148.75,304.00,11.00" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,152.26,137.75,228.16,11.00">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Telematics and Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct coords="4,88.58,170.75,410.96,11.00;4,90.00,181.75,231.00,11.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,239.49,170.75,197.48,11.00">The HAIRCUT information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,453.48,170.75,46.07,11.00;4,90.00,181.75,143.69,11.00">The Johns Hopkins APL Technical Digest</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="14" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,88.44,203.75,448.12,11.00;4,90.00,214.75,441.24,11.00;4,90.00,225.75,205.00,11.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,273.39,203.75,241.11,11.00">A hidden Markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,90.00,214.75,441.24,11.00;4,90.00,225.75,40.18,11.00">Proceedings of the 22nd Annual International ACM SIGIR Conference on R&amp;D in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on R&amp;D in Information Retrieval<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,88.78,247.75,68.71,11.00;4,181.12,247.75,357.19,11.00;4,90.00,258.75,434.28,11.00;4,90.00,269.75,174.00,11.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,257.18,247.75,281.13,11.00;4,90.00,258.75,176.18,11.00">Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,295.20,258.75,229.08,11.00;4,90.00,269.75,85.77,11.00">Proceedings of the ACL Workshop on Very Large Corpora, Montreal</title>
		<meeting>the ACL Workshop on Very Large Corpora, Montreal</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
