<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,202.32,30.92,207.30,12.93">AnswerFinder at TREC 2005</title>
				<funder ref="#_9aaT59e">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.60,60.55,87.09,10.77"><forename type="first">Diego</forename><surname>Molla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology</orgName>
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.12,60.55,137.51,10.77"><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology</orgName>
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,202.32,30.92,207.30,12.93">AnswerFinder at TREC 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32E931528BA28A03816AF1CF7A765913</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AnswerFinder has been completely redesigned for TREC 2005. The new architecture allows a fast development of question-answering systems for their deployment in the TREC tasks and other applications. The AnswerFinder modules use XML to express the services they provide, and they can be queried with XML for their services. The QA method now incorporates graphbased methods to compute the answerhood of a sentence and pin-point the answer. The system uses a set of graph-based rules that are learnt automatically. Unfortunately the system could not be completed and debugged before the TREC deadline and the runs did not fare well. Currently we are debugging and evaluating the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>AnswerFinder is a research oriented question answering system that focuses on incorporating symbolic information in the process. One of the more interesting aspects we are currently investigating is how far automatically induced structural, symbolic information helps finding actual answers.</p><p>This article describes several aspects of this year's TREC QA submission. Firstly, the architecture of the system, which has been completely redesigned this year, will be introduced in section 2. This includes a description of the framework of the system and the different subsystems that can currently be used. Secondly, in Sections 3, 4 and 5 we discuss a new method of finding answers in sentences based on automatically inferred Logical Graph rules. We will describe the LG rules, how to learn them, and how to apply them to sentences to find actual answers. In Section 6 we give an overview of the parameters we used in the two runs that have been submitted to the TREC competition. Running the system indicated some existing problems of the current implementation that will be solved for next year. In Section 7 the problems will be treated briefly and solutions will be provided.</p><p>2 The Architecture of AnswerFinder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The AnswerFinder system can be divided into several phases. The process is entirely questiondriven and forming a pipeline architecture. We recognise the following phases:</p><p>Question Analysis The first phase of An-swerFinder is the analysis of the question.</p><p>During this phase question type classification is performed to determine what sort of answer we are looking for, such as location, person, etc. Also, shallow semantics of the question are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Selection</head><p>The next phase is the selection of the documents that are likely to contain the answer. This phase is performed based on the information of the question. Only the selected documents are considered in the following phases.</p><p>Sentence Selection Using the information extracted during the question analysis phase, sentences that are likely to contain the answer are extracted from the selected documents. Only these sentences are processed further.</p><p>Answer Selection The information taken from the question is matched against the selected sentences and based on this, the exact answer is extracted and returned.</p><p>In this project we mainly focus on the representation of questions and sentences in the text documents. We have implemented different question type classification methods <ref type="bibr" coords="1,519.13,660.14,20.83,10.91;1,315.00,672.74,97.97,10.91">(van Zaanen et al., 2005)</ref> and are investigating different representations of the shallow semantics of the question and the texts. Previously, we have used Minimal Logical Forms <ref type="bibr" coords="2,243.13,24.74,53.96,10.91;2,72.00,37.34,81.31,10.91">(Mollá and Gardiner, 2004b)</ref> and in this article we will describe Logical Graphs as a representation of the semantics. In addition to computing overlap of semantic units (which was done in the past), the Logical Graphs are also used to find exact answers in the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Requirements</head><p>When redesigning AnswerFinder, we first recognised several requirements. In addition to performance requirements (speed, memory usage, accuracy of finding answers, etc.), we have identified two requirements that have a high impact on the design. These requirements are most important from the view that AnswerFinder is a research and development system.</p><p>Flexibility The system should be flexible in several ways. Although it is being developed for the TREC competition, it should also be possible to easily modify it to handle different situations. For example, different input and output formats (of documents, questions, and answers), types of questions and answers, and new algorithms (in all the phases) should be easily integrable in the system. This allows for easy testing of new ideas in different environments.</p><p>Configurability Having a system with many different algorithms that can be used in the phases, it should be easy to configure the system to run using specific parameters. Parameters in this case mean, not only the actual values needed in the algorithms, but also selecting a particular algorithm in a phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Usage</head><p>From the user's point of view, AnswerFinder works as follows. Firstly, AnswerFinder can be queried for the functionality it provides. When a functionality request is sent, AnswerFinder replies with an XML document containing all the functionality it provides. This document shows for each of the phases which algorithm can be used and it also indicates if multiple algorithms can be used in a certain phase at the same time. For example, sentence selection can be done based on different kinds of information, which is implemented by different algorithms.</p><p>Once the provided functionality is known, the user can create an XML request by simply selecting which functionality is needed. This re-quest is then sent off to AnswerFinder. An-swerFinder will combine the right modules and apply it to the data. Finally, the answers found by AnswerFinder are sent back to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation</head><p>AnswerFinder is implemented using C++, where the object-oriented paradigm is extensively used. Not only data is represented by class instances, also all algorithms are contained in class hierarchies.</p><p>Each of the phases in AnswerFinder uses a separate class hierarchy, where each algorithm that can be used in a particular phase is represented by a class. The different algorithms are registered to a builder, which is a class that can be requested to create objects it knows about. The builder can also be asked to provide information about the classes it knows of.</p><p>When a user request in the form of an XML document is received by the general An-swerFinder algorithm builder, this document is analysed and the appropriate parts of the document are sent to the different builders for each phase in the system. These builders create instances of the algorithms that are requested in the XML document and all these instances are put together by the AnswerFinder algorithm builder. This system is then applied to the data that is present in the request and the final answer is sent back to the user.</p><p>Adding a new algorithm for a particular phase in the system is easy. A new class needs to be defined in the particular class hierarchy for the phase and it should register itself with the builder. Since a user can ask the builder what algorithms are available, the new algorithm is automatically incorporated in the system and can be used directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Available Settings</head><p>To allow AnswerFinder to answer questions, the user has to select which functionality is required. AnswerFinder currently implements the following functionality (Figure <ref type="figure" coords="2,463.73,572.18,4.21,10.91" target="#fig_0">1</ref>).</p><p>First of all, the Question Analysis module is currently implemented using a regular expression classifier as described by <ref type="bibr" coords="2,463.92,609.98,70.55,10.91;2,315.00,622.46,60.76,10.91">Mollá and Gardiner (2004a)</ref>. Next, for the Document Selection phase, the pre-selected document ranking provided by NIST is used. Following that, the Sentence Preselection module is composed of a cascade of filters. Each filter scores the input sentences and returns a ranked list of the top n sentences (where n is the parameter called limit in Figure <ref type="figure" coords="3,145.57,263.66,4.82,10.91" target="#fig_0">1</ref>) with their score. In addition, some of the filters may also return a list of candidates to exact answers (together with their score) which are passed to the Answer Extraction module. Four filters are currently implemented:</p><p>Word Overlap This filter includes a stop word list to indicate that certain words should not be used in the computation of the word overlap.</p><p>Grammatical Relations Overlap This filter uses the grammatical relations developed by <ref type="bibr" coords="3,135.01,431.06,95.81,10.91" target="#b0">Carroll et al. (1998)</ref> and has been described by <ref type="bibr" coords="3,157.21,443.66,131.70,10.91">Mollá and Gardiner (2004a)</ref>.</p><p>Logical Form Overlap This filter uses the output of the Connexor parser<ref type="foot" coords="3,238.08,473.58,4.23,5.55" target="#foot_0">1</ref> to produce logical forms and has been described by <ref type="bibr" coords="3,93.84,499.82,131.70,10.91">Mollá and Gardiner (2004a)</ref>. The filter returns candidates to exact answers.</p><p>Logical Graph Overlap This filter is new in our participation in TREC 2005 and will be described in this article. The filter returns candidates to exact answers.</p><p>Finally, the Answer Extraction module combines and ranks the answer candidates found by the Sentence Preselection module. Also, the system integrates the output of a named entity recogniser by incorporating the named entities that are compatible with the expected answer type. We are using LingPipe<ref type="foot" coords="3,289.32,666.18,4.23,5.55" target="#foot_1">2</ref> , although we have also tried using Annie, the named entity recogniser from GATE<ref type="foot" coords="3,487.56,274.98,4.23,5.55" target="#foot_2">3</ref> . The output of this module is a ranked list of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Logical Graphs</head><p>An innovation in the version of AnswerFinder presented for TREC 2005 is the use of a graph notation for the representation of the logical contents of answer sentences. This is what we call the Logical Graphs (LGs). These Logical Graphs are inspired in Conceptual Graphs <ref type="bibr" coords="3,378.26,401.42,60.75,10.91" target="#b5">(Sowa, 1979)</ref>, though in contrast with Sowa's approach, LGs do not attempt to encode the full semantics of a sentence. Following the principles of the logical forms of last year's AnswerFinder system <ref type="bibr" coords="3,458.15,451.58,76.31,10.91;3,315.00,464.18,61.18,10.91">(Mollá and Gardiner, 2004b)</ref>, LGs aim at representing the basic logical content required for question answering and they avoid the representation of wellknown problematic concepts such as quantification, plurality, tense, and aspect. Figure <ref type="figure" coords="4,120.61,487.34,5.45,10.91">2</ref> shows various examples of LGs. These examples are taken from the examples used in Sowa's Conceptual Graphs website 4 and, while there is no space here to explain the differences and similarities between our LGs and Sowa's Conceptual Graphs, the interested reader may consult Sowa's examples and compare the graphs. Our first example shows the use of a relation labelled 1 to express the subject of the go event, and two relations, labelled to and by, that represent two prepositions. The second example shows the use of lattice structures to represent complex entities (such as the ones formed when a conjunction is used). This use of lattices is inspired on the treatment of plurals and complex events (Link, 1983; Mollá,   4 http://www.jfsowa.com/cg/index.htm 1997). Finally, the third example shows the expression of clauses and control verbs. These examples only cover a few of the linguistic features but we hope they will suffice to show the expressive power of the LGs.</p><p>The Logical Graphs are constructed automatically from the logical forms used in last year's AnswerFinder <ref type="bibr" coords="4,384.49,411.74,134.97,10.91">(Mollá and Gardiner, 2004b)</ref> and they present a simplification of these logical forms. The conversion from a logical form to a LG is shown in Table <ref type="table" coords="4,429.85,449.42,4.22,10.91" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Logical Graph Rules</head><p>To use the LGs to extract the answer, we have devised a method to learn Logical Graph Rules (LGRs) and apply the learnt rules to a question/answer candidate sentence pair. These</p><p>LGRs are based on the concepts of graph overlap and path between two subgraphs in a graph <ref type="bibr" coords="4,315.00,563.42,141.66,10.91" target="#b3">(Mollá and van Zaanen, 2005)</ref>.</p><p>Each rule r contains three components:</p><p>r o An overlap between a question and its answer sentence.</p><p>r p A path between the overlap and the actual answer in the answer sentence.</p><p>r a A graph representing the exact answer.</p><p>For further detail about the definition and properties of graph overlaps and paths, see  <ref type="bibr" coords="5,238.09,43.45,106.87,9.96">(e.g "object(john,o1,[x1]</ref>)") into concepts labelled with the noun ("john") and indexed with the entity ("x1"). The index is required to allow the possibility of two different concepts having the same label. For example, the sentence The big ball hit the small ball would produce two different concepts labelled as "ball". The reification of the object ("o1") is ignored in the LG. 2. Convert all event/state predicates (e.g. "evt <ref type="bibr" coords="5,294.13,100.21,63.03,9.96">(eat,e3,[x1,x2]</ref>)") into concepts labelled with the verb ("eat") and indexed with the reification ("e3"). For every argument position, add one relation from the newly created concept to the concept indexed with the argument and labelled according to the position of the argument. 3. Convert all property predicates introduced by adjectives and adverbs (e.g. "prop <ref type="bibr" coords="5,457.43,145.57,57.99,9.96">(hard,p1,[x2]</ref>)") into concepts labelled with the adjective/adverb ("hard") and indexed with the reification ("p1"). Add one relation from the newly created concept to the concept indexed with the argument ("x2").</p><p>The relation is labelled "prop". 4. Convert all other property predicates (e.g. "prop <ref type="bibr" coords="5,316.51,191.05,60.26,9.96">(to,p2,[e2,x4]</ref>)") into relations labelled with the property label ("to"), which connect from the first argument ("e2") to the second argument ("x4"). The reification ("p2") is ignored. 5. Convert all predicates of compound nouns (e.g. "compound noun(x4,x5)") into relations labelled "compound noun" that connect the first argument to the second argument. 6. Convert all logical operators (e.g. "log op <ref type="bibr" coords="5,283.22,248.05,64.12,9.96">(and,e2,[e3,e4]</ref>)") into relations that connect the reification ("e2") to every argument, and labelled like the logical operator ("and"). 7. Convert all general dependencies (e.g. "dep <ref type="bibr" coords="5,295.04,270.97,62.82,9.96">(27000,d6,[x6]</ref>)") into concepts labelled with the dependency label ("27000") and indexed with the reification ("d6"). Add a relation labelled "dep" that connects the newly created concept to the concept indexed with the argument ("x6"). 8. Convert all lattice relations (e.g. "x34&lt;x35") and any remaining two-place predicates into relations labelled with the predicate name ("&lt;") and connecting the first entity ("x34") to the second entity ("x35"). The Rule (r o in regular lines, r p in dashed lines, r a in thick lines)</p><p>Figure <ref type="figure" coords="5,144.13,492.50,4.22,10.91">3</ref>: A Logical Graph rule <ref type="bibr" coords="5,72.00,522.62,148.62,10.91" target="#b3">(Mollá and van Zaanen, 2005)</ref>. For the purposes of this article it suffices to say that, given the possible existence of repeated concept labels and relation labels in a LG, there may be several possible overlaps between two graphs. When this happens, the graph with the largest size is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning of Logical Graph Rules</head><p>With the help of a training set of questions and sentences containing the answers, a set of LGRs can be learnt. Figure <ref type="figure" coords="5,178.94,660.14,5.45,10.91">3</ref> shows an example of a rule learnt between two sentences. The graph notation has been simplified by replacing the relation vertices with labelled edges.</p><p>FOR every question/answerSentence pair G q = the graph of the question G s = the graph of the answer sentence G a = the graph of the exact answer FOR every overlap O between G q and G s FOR every path P between O and G a Build a rule R of the form The algorithm for learning rules is fairly straightforward and is shown in Figure <ref type="figure" coords="5,504.29,534.14,4.22,10.91" target="#fig_1">4</ref>.</p><formula xml:id="formula_0" coords="5,356.88,434.05,39.12,32.83">R o = O R p = P R a = G a</formula><p>Rules learnt with this algorithm are very specific to the question/answer pair. For example, the rule in Figure <ref type="figure" coords="5,400.94,572.90,5.45,10.91">3</ref> would only trigger for questions about Peter and it would not trigger, say, for the question Where was Mary born?. To generalise a rule we use a simple method:</p><p>• Concepts generalise to " " (that is, concepts that would unify with anything).</p><p>• Relations do not generalise (relations express syntactic or semantic relations and it is not advisable to over-generalise them).</p><p>The generalisation of concepts applies to every concept except those that belong to a specific list of "stop concepts" (in analogy to the idea of stop words in Information Retrieval). The current list of stop concepts is: and, or, not, nor, if, otherwise, have, be, become, do, make Rules are weighted according to the following formula. The weight W of a rule r is computed on the basis of its ability to detect the exact answer in the training corpus:</p><formula xml:id="formula_1" coords="6,105.00,202.58,36.93,10.91">W(r) =</formula><p># correct answers found # answers found</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Graph-based Question Answering</head><p>To find if a sentence s with graph S answers a question q with graph Q, all learnt LGRs are tested. A rule r triggers iff its rule pattern r o is a subgraph of Q (that is, the overlap between r o and Q is r o ). When that happens, the graph of the question is expanded with the concepts and relations of the rule path r p , producing a new graph Q rp . The resulting graph is more likely to produce a high overlap with an answer sentence similar to the one that generated the rule and, most importantly, the graph contains an indication of where the answer is located.</p><p>Once the graph of the question has been expanded with the rule path, one only needs to compute the overlap between this expanded graph and that of the answer sentence ovl(Q rp , S). If the overlap retains part of the exact answer that was marked up by the graph rule r a , then we have found a possible answer.</p><p>The above method will cover simple cases, but it needs to be extended to cover two special cases that arise from the fact that the question/sentence pairs that generated the rule are likely to be different from the actual question and sentence being tested. First of all, several rules may trigger, and each rule may extract a different answer. Consequently, there are several answer candidates and the system needs to choose one of them. Second, it is possible that the overlap between the extended graph and the sentence does not contain the complete answer but part of it. We will proceed to explain these two cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Answer Ranking</head><p>To identify the correct answer among a set of possible answers it is necessary to establish a measure of "answerhood" so that the correct answer has a higher score than the other candidates. The rule weight gives an indication of the quality of the answer extracted. But we also need to keep in account the degree of similarity between the sentence that created the rule and the answer sentence being tested. For this we use the size of the overlap between the extended graph of the test question and the graph of the test answer size(ovl(Q rp , S)). Thus, the "answerhood" A(s) or likelihood that a sentence s with graph S contains the answer to a question q with graph Q is the product of the weight of the rule used W(r) and the size of the overlap:</p><formula xml:id="formula_2" coords="6,349.56,213.62,155.80,11.41">A(s) = W(r) × size(ovl(Q rp , S))</formula><p>The size of a graph overlap is computed as the weighted sum of all concepts and relations in the overlap. The weight W i of a concept or relation i in the overlap is determined using a variant of the Inverse Document Frequency (IDF) measure used in Document Retrieval. The actual formula that we use is:</p><formula xml:id="formula_3" coords="6,384.00,328.46,84.60,25.79">W i = 1 log N log N n</formula><p>n = total number of sentences using the concept (or relation) i N = total number of sentences</p><p>The formula includes the constant factor 1/ log N to ensure that the value ranges between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Answer Expansion</head><p>Sometimes the overlap between Q rp and S does not contain the complete answer but only the head of the answer. For example, suppose that the generalised rule of Figure <ref type="figure" coords="6,461.06,512.66,5.45,10.91">3</ref> is used for the sentence pair: Q: Where was Andrew born?</p><p>A: Andrew's birthplace was the city of Frankfurt</p><p>The overlap between the expanded graph of the question and the answer sentence (see Figure <ref type="figure" coords="6,333.96,623.54,4.82,10.91">5</ref>) would say that the answer is city, which is not correct. We need to expand the answer found. The process of answer expansion is very simple:</p><p>1. Start with the part of the exact answer marked up in the rule r a that appears in 2. Choose a non-processed concept in A and label it as processed. Take the corresponding concept c in S and add to A all outgoing relations from c and their destination concepts.</p><p>3. Repeat step 2 until all the concepts of A have been processed.</p><p>An example of answer expansion is shown in Figure <ref type="figure" coords="7,107.53,597.38,4.22,10.91">5</ref>. From this example we can see that the resulting answer may not be judged exact according to the NIST guidelines for the evaluation of answers. Still, we decided to keep the answer expansion on the grounds that otherwise no answer would have been found at all in cases like that of the example. We have not measured the impact of the answer expansion in the performance of AnswerFinder yet.</p><p>6 Parameters Used for the TREC 2005 Competition</p><p>This year, we submitted two runs to the TREC competition. The first run (af run1) was used as a baseline, which used roughly the same parameters of the submission used in last year's competition. The second run (af run2) tested the Logical Graphs as a replacement to the logical forms.</p><p>Both submissions used the same basic settings. This included document selection based on the documents pre-selected by the PRISE search engine and provided by NIST. For each of the questions, only the top 50 documents were used. Question type classification was done using a regular expression based classifier and LingPipe was used as the named entity recogniser.</p><p>The only differences between runs are, first, the number of sentences selected by the Word Overlap module, and second, the nature of the module following the Word Overlap module (Table <ref type="table" coords="7,349.20,306.98,4.21,10.91" target="#tab_3">2</ref>).</p><p>The first run, which tested the use of minimal logical forms similarly to last year's submission, started off with selecting the 100 best matching sentences according to word overlap between the question and the sentences. These 100 sentences were converted into minimal logical forms, which finds some possible answers. The set of possible answers was then extended with the named entities present in the sentences.</p><p>The second run worked on the best 5 sentences selected based on word overlap. Logical Graphs were computed on these sentences and matched with the Logical Graph of the question. This introduced some possible answers as described above. Again, named entities were used to expand the set of possible answers.</p><p>Finally, the set of possible answers found by the systems were ranked based on the score that was computed during the process of finding of possible answers. Answers found by the named entity recogniser increased the score by 1, similar to answers extracted by logical forms. Logical Graphs increased the score by the product of the confidence of the rule and the graph measure, as described in Section 5.1.</p><p>Answers to factoid questions are then the single best answer according to the ranked possible answers. List type questions are answered by returning all the answers found and other type questions are simply considered to be a list type question where the question is What is topic ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Due to time pressures it became impossible to debug and fine-tune the system before the TREC deadline. Consequently the results of the two runs submitted were very disappointing. The run af run1 on the factoid questions had an accuracy of 0.028 (10 correct answers out of 362), the precision of recognising no answer was 0.077 and the recall of recognising no answer was 0.176. The answers to the list questions were not better: an average F score of 0.008. None of the "other" questions were correctly answered.</p><p>The run af run2 performed even worse than af run1. The accuracy on the factoid questions was 0.014 (5 correct answers), the precision of recognising no answer was 0.075 and the recall of recognising no answer was 0.235. The average F score on the list questions was 0 and again, none of the "other" questions had correct answers.</p><p>Generating the answers of both runs already indicated several problems with the current implementation. We rely on some external systems to compute intermediate results. For example, named entities are found by LingPipe, which is run as an external process. The way this is done now, is simply too time consuming. This restricted us to use only an extremely limited number of sentences to find answers in. If the answer is not present in the 100 (af run1) or 5 (af run2) sentences, the system will obviously not find a correct answer no matter how well these systems work. A count of the sentences that matched Ken Litkowski's patterns indicated that, when 100 sentences were selected by the Word Overlap module, only 20.98% of the questions retrieved a sentence matching the corresponding pattern (this is what is called selection accuracy in the table). This figure is reduced to 11.53% of the questions if 5 sentences were preselected. Note that there may be sentences that contain the correct answer but who do not match any of the corresponding patterns, and there may be sentences that do not contain the answer but who match some of the patterns. Therefore, we also ran Litkowski's patterns on the final output of our system, giving the results of Table <ref type="table" coords="8,384.36,173.42,4.22,10.91" target="#tab_4">3</ref>. The table shows that the use of patterns is an acceptable approximation of the selection accuracy.</p><p>A simplified system that uses only the Logical Graphs to find the answers and ignores any information about the question type and the named entities gives better results in our current evaluations. There may be several reasons for this. Firstly, the number of sentences selected by word overlap simply may not contain the answers, but we also suspect that the cause of the poor results is a bug in the system.</p><p>There are some more issues that create problems. Firstly, AnswerFinder currently tokenises all text it processes (questions and documents). The idea behind this is that it is easy to keep track of the possible answers in a standardised way. However, the external tools that are used (Connexor, LingPipe, Annie, etc.) perform their own tokenisation. This introduces problems since a mapping between the different tokenisation methods needs to be defined.</p><p>Another, related, problem is that the different methods that find possible answers may in fact return substrings of another possible answer. For example, one possible answer may be the large house around the corner, whereas another possible answer may be the substring the large house. The system currently treats these as completely different answers. The fact that substrings of another possible answer are also found as possible answers may count as extra evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this article, we first presented the new AnswerFinder implementation. Because An-swerFinder is a research project, the system should be highly flexible. Systems are built dynamically and new methods can easily be integrated.</p><p>Next, a new representation of shallow seman- tics was discussed. Logical Graphs allow a compact description of the semantics of questions and sentences. Not only are they visually clear (in contrast to logical forms), they can be used in the shape of logical graph rules to find possible answers in the text.</p><p>In the near future, we wish to understand why the results of this TREC competition are so much below our expectations. Although here we described some reasons why this is the case, more extensive testing is still under way. We also want to further investigate the learning of symbolic representations in the context of question answering. Minimal Logical Forms and Logical Graphs are first steps in that direction. Of course, this also means that different scoring metrics need to be tested. Finally, the bugs and design issues of the current system will need to be fixed. This again requires more testing, but it also requires more significant modifications, such as offset annotation instead of tokenisation of text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,222.02,467.98,10.91;3,72.00,234.62,92.32,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Functionality of AnswerFinder. The dashed arrows indicate a hypothetical configuration and its parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,347.88,490.34,159.38,10.91"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning of graph rules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,80.52,396.62,208.17,10.91"><head></head><label></label><figDesc>Figure 5: Graph-based Question Answering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,315.00,526.94,225.18,163.19"><head></head><label></label><figDesc>Like Sowa's Conceptual Graphs, our Logical Graphs are directed, bipartite graphs with two types of vertices, namely concepts and relations: Concepts Examples of concepts are objects dog, table, events and states run, love, and properties red, quick.</figDesc><table coords="3,315.00,616.46,225.18,73.67"><row><cell></cell><cell>john</cell><cell>1</cell><cell>go</cell><cell>to</cell><cell>boston</cell></row><row><cell></cell><cell></cell><cell></cell><cell>by</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">bus</cell><cell></cell></row><row><cell></cell><cell cols="5">John is going to Boston by bus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hard</cell></row><row><cell>person</cell><cell>1</cell><cell>be</cell><cell></cell><cell>between</cell><cell>prop</cell></row><row><cell></cell><cell></cell><cell>rock</cell><cell>≤</cell><cell></cell><cell>≤</cell><cell>place</cell></row><row><cell cols="6">A person is between a rock and a hard place</cell></row><row><cell>tom</cell><cell>1</cell><cell>believe</cell><cell></cell><cell>2</cell></row><row><cell></cell><cell cols="2">mary</cell><cell>1</cell><cell>want</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>marry</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Relations Relations act as links between con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">cepts. Examples of relations would be</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">grammatical roles and prepositions. How-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ever, to facilitate the production of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Logical Graphs we have decided to use re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">lation labels that are relatively close to the</cell></row></table><note coords="4,420.48,228.17,23.47,9.96;4,191.04,272.30,228.55,10.91;4,214.92,294.86,182.29,10.91;4,93.84,323.78,203.20,10.91;4,93.84,336.38,203.14,10.91;4,93.84,348.98,203.08,10.91;4,93.84,361.46,203.20,10.91;4,93.84,374.06,203.24,10.91;4,93.84,386.54,203.07,10.91;4,93.84,399.14,203.22,10.91;4,93.84,411.74,203.21,10.91;4,93.84,424.22,203.25,10.91;4,93.84,436.82,203.17,10.91;4,93.84,449.30,203.18,10.91;4,93.84,461.90,151.02,10.91"><p><p><p>sailor Tom believes that Mary wants to marry a sailor Figure</p>2</p>: Examples of Logical Graphs syntactic level. For example, instead of using labels related to thematic roles such as agent, patient, and so forth, we use syntactic roles subject, object, etc. Furthermore, to avoid resuming any debate about the possible names of the syntactic roles, we have decided to use numbers. Thus, the relation 1 indicates the link to the first argument of a verb (that is, what is usually a subject). The relation 2 indicates the link to the second argument of a verb (usually the direct object), and so forth.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,83.52,31.34,368.13,22.07"><head>Table 1 :</head><label>1</label><figDesc>Conversion from a Logical Form to a Logical Graph 1. Convert all object predicates</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,72.00,31.34,372.91,115.31"><head>Table 2 :</head><label>2</label><figDesc>Parameters of the runs submitted to TREC 2005</figDesc><table coords="8,72.00,41.54,369.25,105.11"><row><cell>Run</cell><cell>Document</cell><cell>Word</cell><cell>LF</cell><cell>Graph</cell></row><row><cell>Name</cell><cell>Selection</cell><cell>Overlap</cell><cell cols="2">Patterns Patterns</cell></row><row><cell cols="3">af run1 limit = 50 limit = 100</cell><cell>yes</cell><cell>no</cell></row><row><cell cols="2">af run2 limit = 50</cell><cell>limit = 5</cell><cell>no</cell><cell>yes</cell></row><row><cell cols="3">Only answers that have a type that matches the</cell><cell></cell><cell></cell></row><row><cell cols="3">question type are returned, but if none of them</cell><cell></cell><cell></cell></row><row><cell cols="2">matches, this requirement is dropped.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,72.00,31.34,467.92,74.39"><head>Table 3 :</head><label>3</label><figDesc>Results of the factoid questions. The Selection Accuracy column indicates the percentage of questions that have a selected sentence matching Ken Litkowski's answer patterns (see text).</figDesc><table coords="9,159.84,56.78,292.17,48.95"><row><cell>Run</cell><cell cols="4">TREC Litkowski N Preselected Selection</cell></row><row><cell>Name</cell><cell>Evaluation</cell><cell>Patterns</cell><cell cols="2">Sentences Accuracy</cell></row><row><cell>af run1</cell><cell>2.8%</cell><cell>3.6%</cell><cell>100</cell><cell>20.98%</cell></row><row><cell>af run2</cell><cell>1.8%</cell><cell>1.1%</cell><cell>5</cell><cell>11.53%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,88.56,688.48,106.18,8.97"><p>http://www.connexor.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,88.56,699.28,134.53,8.97"><p>http://www.alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,331.56,699.27,74.09,8.97"><p>http://gate.ac.uk/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is funded by the <rs type="funder">Australian Research Council</rs>, <rs type="grantName">ARC Discovery Grant</rs> no. <rs type="grantNumber">DP0450750</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9aaT59e">
					<idno type="grant-number">DP0450750</idno>
					<orgName type="grant-name">ARC Discovery Grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,72.00,506.06,225.08,10.91;9,82.92,518.66,214.19,10.91;9,82.92,531.14,166.23,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,143.06,518.66,154.05,10.91;9,82.92,531.14,70.21,10.91">Parser evaluation: a survey and a new proposal</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Sanfilippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,176.28,531.14,66.58,10.91">Proc. LREC98</title>
		<meeting>LREC98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,544.94,225.01,10.91;9,82.92,557.42,213.51,10.91;9,82.92,570.02,214.18,10.91;9,82.92,582.50,214.02,10.91;9,82.92,595.10,214.02,10.91;9,82.92,607.70,198.89,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,187.23,544.94,109.78,10.91;9,82.92,557.42,213.51,10.91;9,82.92,570.02,41.21,10.91">The logical analysis of plurals and mass terms: a lattice-theoretical approach</title>
		<author>
			<persName coords=""><forename type="first">Godehard</forename><surname>Link</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,82.92,595.10,214.02,10.91;9,82.92,607.70,24.63,10.91">Meaning, Use and Interpretation of Language</title>
		<editor>
			<persName><forename type="first">Rainer</forename><surname>Bauerle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christoph</forename><surname>Schwarze</surname></persName>
		</editor>
		<editor>
			<persName><surname>Arnim Von Stechov</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>de Gruyter</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="250" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,621.38,225.08,10.91;9,82.92,633.86,214.04,10.91;9,82.92,646.46,214.01,10.91;9,82.92,659.06,214.17,10.91;9,82.92,671.54,213.87,10.91;9,82.92,684.14,195.20,10.91;9,72.00,697.82,225.09,10.91;9,325.92,125.90,214.15,10.91;9,325.92,138.50,200.46,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,279.17,621.38,17.92,10.91;9,82.92,633.86,214.04,10.91;9,82.92,646.46,209.14,10.91;9,173.91,684.14,104.21,10.91;9,72.00,697.82,225.09,10.91;9,325.92,125.90,122.79,10.91">Answerfinder -question answering by combining lexical, syntactic and semantic information</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Gardiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,149.52,671.54,84.77,10.91">Proc. ALTW 2004</title>
		<editor>
			<persName><forename type="first">Ash</forename><surname>Asudeh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cécile</forename><surname>Paris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>ALTW 2004<address><addrLine>Sydney, Australia; Buckland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Macquarie University. Diego Mollá and Mary Gardiner. 2004b. An-swerfinder at TREC 2004</note>
</biblStruct>

<biblStruct coords="9,315.00,151.58,224.83,10.91;9,325.92,164.06,214.15,10.91;9,325.92,176.66,167.21,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,325.92,164.06,214.15,10.91;9,325.92,176.66,13.17,10.91">Learning of graph rules for question answering</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,361.92,176.66,85.61,10.91">Proc. ALTW 2005</title>
		<meeting>ALTW 2005<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,189.74,224.94,10.91;9,325.92,202.34,213.63,10.91;9,325.92,214.82,183.33,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,410.52,189.74,129.42,10.91;9,325.92,202.34,208.68,10.91">Aspectual Composition and Sentence Interpretation: a formal approach</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="9,315.00,227.90,224.94,10.91;9,325.92,240.50,196.95,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,423.26,227.90,116.67,10.91;9,325.92,240.50,29.76,10.91">Semantics of conceptual graphs</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">F</forename><surname>Sowa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,378.96,240.50,75.29,10.91">Proc. ACL 1979</title>
		<meeting>ACL 1979</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,253.58,225.10,10.91;9,325.92,266.06,214.11,10.91;9,325.92,278.66,214.22,10.91;9,325.92,291.14,213.91,10.91;9,325.92,303.74,213.91,10.91;9,325.92,316.34,213.89,10.91;9,325.92,328.82,19.23,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,422.41,266.06,117.62,10.91;9,325.92,278.66,103.31,10.91">Classifying sentences using induced structure</title>
		<author>
			<persName coords=""><forename type="first">Luiz</forename><surname>Menno Van Zaanen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Augusto</forename><surname>Pizzato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,469.44,291.14,70.39,10.91;9,325.92,303.74,213.91,10.91;9,325.92,316.34,152.85,10.91">String Processing and Information Retrieval: 12th International Conference, SPIRE 2005</title>
		<editor>
			<persName><forename type="first">Mariano</forename><surname>Consens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gonzalo</forename><surname>Navarro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="139" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,315.00,341.90,225.08,10.91;9,325.92,354.50,214.00,10.91;9,325.92,366.98,214.03,10.91;9,325.92,379.58,157.50,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</author>
		<title level="m" coord="9,389.88,354.50,150.04,10.91;9,325.92,366.98,52.42,10.91">The Thirteenth Text REtrieval Conference</title>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
