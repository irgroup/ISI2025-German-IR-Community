<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,370.00,43.16,75.80,11.84">at TREC 2005</title>
				<funder>
					<orgName type="full">School of Information Systems</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.22,69.21,65.74,10.16"><forename type="first">Alan</forename><surname>Woodley</surname></persName>
						</author>
						<author>
							<persName coords="1,205.46,69.21,56.02,10.16"><forename type="first">Chengye</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName coords="1,269.84,69.21,63.70,10.16"><forename type="first">Tony</forename><surname>Sahama</surname></persName>
						</author>
						<author>
							<persName coords="1,342.32,69.21,50.46,10.16"><forename type="first">John</forename><surname>King</surname></persName>
						</author>
						<author>
							<persName coords="1,416.68,69.21,64.19,10.16"><forename type="first">Shlomo</forename><surname>Geva</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="laboratory">Information Retrieval and Web Intelligent Group</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postBox>PO Box 2434</postBox>
									<postCode>4001</postCode>
									<settlement>Brisbane</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,370.00,43.16,75.80,11.84">at TREC 2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBFD4F382632BB57210FE2121907C1CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>{ap.woodley</term>
					<term>c4.lu}@student.qut.edu.au | {t.sahama</term>
					<term>j5.king</term>
					<term>s.geva}@qut.edu.au</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Information Retrieval and Web Intelligence (IR-WI) research group is a research team at the Faculty of Information Technology, QUT, Brisbane, Australia. The IR-WI group participated in the Terabyte and Robust track at TREC 2005, both for the first time. For the Robust track we applied our existing information retrieval system that was originally designed for use with structured (XML) retrieval to the domain of document retrieval. For the Terabyte track we experimented with an open source IR system, Zettair and performed two types of experiments. First, we compared Zettair's performance on both a high-powered supercomputer and a distributed system across seven midrange personal computers. Second, we compared Zettair's performance when a standard TREC title is used, compared with a natural language query, and a query expanded with synonyms. We compare the systems both in terms of efficiency and retrieval performance. Our results indicate that the distributed system is faster than the supercomputer, while slightly decreasing retrieval performance, and that natural language queries also slightly decrease retrieval performance, while our query expansion technique significantly decreased performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.0">Introduction</head><p>Information Retrieval (IR) is one of the most influential and challenging fields of study in information technology. QUT's Information Retrieval and Web Intelligence (IR-WI) group is a team of researchers investigating IR and other associated technologies such as: data mining; web intelligence; and recommendation systems. In previous years our group has participated in other information retrieval workshops -most notably the Initiative for the Evaluation of XML Retrieval (INEX) -however, 2005 is the first year that we have participated in TREC. We focused our attention on the Terabyte and Robust tracks.</p><p>Our approach in the Terabyte track was different to most other TREC participants. Rather than produce our own information retrieval system, we decided to investigate the variation in performance of an open source search engine in two different scenarios, first, when executed on different hardware models, and second, when different queries are used as input. The search engine we used was the Zettair system, developed by the Search Engine Group at the Royal Melbourne Institute of Technology. Zettair is an open source search engine that was also used in the 2004 Terabyte track <ref type="bibr" coords="1,100.51,454.78,85.54,8.48" target="#b0">(Billbereck et al, 2004)</ref>.</p><p>The first of our experiments applied Zettair to two different hardware models. The first hardware model was a high performance supercomputer that produced a single index. The second hardware model was a set of seven midrange personal computers, each of which produced a separate index. This allowed us to compare the performance of a power supercomputer versus a distributed system of standard personal computers, both in terms of efficiency and standard information retrieval metrics.</p><p>The second of our experiments was to observe the variation in Zettair's performance when different input was used. We used three input variations. First, as a baseline we used the terms contained in each of the topic's title tags. Second we used a natural language interface to derive keywords from each of the topics' description tags. Third, we augmented the topics' original title with plural/singular variations, stems and synonyms derived from the Porter stemmer <ref type="bibr" coords="1,482.27,573.75,50.48,8.48">(Porter,1980)</ref> and Wordnet <ref type="bibr" coords="1,130.51,584.61,63.65,8.48" target="#b3">(Fellbaum, 1998)</ref>.</p><p>The majority of this paper is focused on our participation in the Terabyte track, with the exception of Section 6 that discusses our participation in the Robust track. Section 2 describes Zettair, the open source information retrieval system used for the experiments. Section 3 describes the experiments we performed detailing the two hardware models and the variations in input. Section 4 describes and presents results of the experiments we performed on the 2004 query set. Section 5 describes and presents the results of our 4 runs for the 2005 query set. Section 7 provides concluding remarks as well as a discussion on the future research we intend to perform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0">The Open Source Information Retrieval System (Zettair)</head><p>Zettair is an open source IR system available under a BSD license from http://www.seg.rmit.edu.au/zettair. It participated in TREC for the first time in 2004 <ref type="bibr" coords="1,269.54,715.60,89.80,8.48" target="#b0">(Billbereck et al, 2004)</ref>. Zettair can extract text from SGML-based languages. For indexing, it uses and efficient algorithm <ref type="bibr" coords="1,304.96,726.46,88.95,8.48" target="#b5">(Heinz &amp; Zobel, 2003)</ref>, however, it does not use in-place merging <ref type="bibr" coords="2,113.54,42.22,87.60,8.48">(Moffett &amp; Bell, 1995)</ref> and uses a variable-byte compression scheme instead of Golumb encoding <ref type="bibr" coords="2,491.20,42.22,41.47,8.48;2,79.32,53.08,35.44,8.48" target="#b9">(Scholer et al., 2002)</ref>. The index saves the full word position, which allows for phrase searches. It uses a single pass indexing algorithm, which generates compressed postings in memory saves them to disk and then merges them together to form a single, continuous list with a B+Tree vocabulary structure. Zettair supports multiple ranking metrics, however, we used the Okpai BM25 metric <ref type="bibr" coords="2,190.88,85.54,71.43,8.48" target="#b6">(Jones et al., 2000)</ref> using the formula: Note that in formula 1 term contributions for terms occurring in more than half of the documents in the collection are negative; hence, a small positive term contribution is used instead. Next we describe how we implemented Zettair on single index and distributed systems, and the input variations we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0">Research Methodology</head><p>The aim of our research was to observe the variation in performance of an open source information retrieval system when applied to different hardware models and when different sources of input are used. We used two hardware models: a single index created on a high performance supercomputer; and multiple indexes created on a distributed system of midrange personal computers. We used three different sources of input: the topics' title tag that was used as a baseline, keywords derived from the topics' description tag via a natural language interface; and the topics' title tag augmented with plural/singular variations, synonyms and stems. Here we describe our experiments in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single Index System</head><p>The aim of the single index experiments was to test the feasibility of searching a (half) terabyte collection on a high performance computer. We used Queensland University of Technology's High Performance Computing (HPC) supercomputer for our experiments. The HPC supercomputer was purchased in 2000 for US$55,000. It consists of 10 nodes, each with two 3.4 Gigahertz processors and 4 Gigabytes of RAM. However, during our experiments we shared the HPC supercomputer with other users and processes, hence, we only had access to 4 of the possessors. We copied the terabyte collection onto the HPC supercomputer, separating it evenly amongst the 10 nodes. Then we executed the Zettair indexer on the collection as a whole and created a single 43.7 Gigabyte index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Distribution</head><p>The aim of system distribution experiments was to test the feasibility of searching a (half) terabyte collection on a network of standard personal computers. We tested our system on computers in one of our student computer laboratories. These computers cost about US$1,500 (3 GHz Pentium 4, with 1 Gigabyte RAM), and are a reasonable approximation of a midrange computer system that could be found in a home, school or office. We divided our system randomly into seven sub-collections. Each collection was stored on a 500 Gigabyte external hard drive (Lacie "Big Disk"). The hard drives were connected to the computers via USB 2 connection. Each sub-collection had 39 directories, totally 61 Gigabytes. We separated the collection into sub-collections randomly. We executed the Zettair indexer to each of the sub-collections. This produced an index size between 4 Gigabytes and 10 Gigabytes per sub-collection. We then used Zettair to search each of indexes and saved the top 10,000 documents per topic per sub-collection to a results file. The time taken to index and search each sub-collection is presented in section 4.2. Finally, we had to merge the results file together to produce a submission file for comparison with the single index. Merging strategies have a well established history in information retrieval <ref type="bibr" coords="3,377.83,218.20,54.47,8.48" target="#b2">(Callan, 2000)</ref>. We used two relatively simple strategies referred to as Relevance Merge and Round Robin Merge. For Relevance Merge we simply merged together the results generated by each sub-collection and sorted by their relevance score. Note that this score used only local -rather than global -information, therefore, all term weights were defined by statistics derived solely from each sub-collection. For Round Robin Merge we used a two stage ranking strategy. First we grouped together documents according to their rank in the original sub-collection. So all the documents ranked in position 1 were grouped together, followed by all documents ranked in position 2, and so on. Secondly, within each group documents were ranked according to their original (local) relevance score. After the merging, the top 10,000 documents were chosen from each algorithm were chosen to produce the submission, as per specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Natural Language Processing</head><p>The second set of experiments investigated the use of natural language processing (NLP). Specifically, the use of natural language queries (NLQs) to derive users' content requirements by using the description tags as input. There has been an extensive amount of research on the use of NLP in IR, both in TREC itself <ref type="bibr" coords="3,409.79,370.90,122.90,8.48;3,79.32,381.64,16.95,8.48">(Strzalkowski &amp; Sparack Jones, 1996</ref><ref type="bibr" coords="3,96.27,381.64,101.52,8.48" target="#b14">, Strzalkowski et al., 1997</ref><ref type="bibr" coords="3,197.79,381.64,106.50,8.48" target="#b16">, Strzalkowski et al., 1998)</ref> and in IR in general <ref type="bibr" coords="3,388.80,381.64,71.81,8.48">(Strazlkowski,1999</ref><ref type="bibr" coords="3,460.60,381.64,72.11,8.48;3,79.32,392.50,16.94,8.48" target="#b12">, K. Sparck Jones, 1999</ref><ref type="bibr" coords="3,96.26,392.50,83.93,8.48" target="#b10">, A. F. Smeaton, 1999</ref><ref type="bibr" coords="3,180.19,392.50,159.46,8.48" target="#b7">, D. D. Lewis and K. Sparck Jones, 1996)</ref>. However, to handle the queries we used our own natural language system, NLPX <ref type="bibr" coords="3,202.15,403.30,53.67,8.48">(Woodley and</ref><ref type="bibr" coords="3,258.29,403.30,96.31,8.48">Geva 2004, Woodley and</ref><ref type="bibr" coords="3,356.95,403.30,42.60,8.48">Geva 2005)</ref>. NLPX was designed specially for use in the processing of structured (XML) queries, and has previously participated in INEX's NLP track <ref type="bibr" coords="3,498.69,414.10,34.13,8.48;3,79.32,424.96,55.25,8.48" target="#b4">(Geva &amp; Sahama, 2005)</ref>. To our knowledge this is the first time that a NLP system specifically designed for XML-IR has been used in traditional document-level retrieval.</p><p>Handling structured NLQs is more complex than traditional NLQs, since the NLP system must derive both the content and structural need of users. Therefore, all we provide here is a summary of how we used NLPX to derive users' content requirement from NLQs. A more detailed description of how NLPX can be found in our earlier work. For our experiments we treated the TREC descriptions as if they were Content Only queries in INEX. First we augmented the NLQ with their part-of-speech tags using the Brill tagger <ref type="bibr" coords="3,305.95,500.68,46.52,8.48" target="#b1">(Brill, 1994)</ref>. The Brill tagger is a grammatical rule-based tagger than has a performance comparable to most stochastic taggers (~95%). Then we derived important noun phrases from the NLQ using a set of queries derived from our previous work. The terms contained within these phrases were used as input for the Zettair search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query Expansion</head><p>Our final set of experiments investigated query expansion, that is, augmenting topics with additional query terms. Three methods of query expansion were investigated: plurals and singular expansion; stemming; and synonym expansion. Plural and singulars were added using lexical-based heuristics to determine the plural form of a singular term <ref type="bibr" coords="3,494.97,599.26,34.28,8.48;3,79.32,610.12,21.75,8.48">(and viceversa)</ref>. Wordnet <ref type="bibr" coords="3,143.46,610.12,65.87,8.48" target="#b3">(Fellbaum, 1998)</ref> was used to add stems and synonyms to the topics. Wordnet is a linguistic resource inspired by psycholinguistic theories of human lexical memory. Wordnet groups words are into 'synsets', each representing a separate concept. Stemming was performed in two steps. First the Porter stemmer <ref type="bibr" coords="3,460.73,631.72,53.98,8.48">(Porter, 1980)</ref> was performed on the existing query terms to derive each of their stems. Then the Wordnet database was searched to find terms with the same stem. Similarly, synonyms were added to the query by searching for terms in the Wordnet database that belonged to the same synset as the query terms. As outlined in Table <ref type="table" coords="3,388.41,664.18,3.55,8.48" target="#tab_2">4</ref>.1, we used several different query expansions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0">2004 Experiments</head><p>Prior to submitting official runs for the 2005 Terrabyte track, we experimented using the 2004 query set. By evaluating the runs using the standard TREC evaluation module (trec_eval) and recording timestamps, we were able to measure how successful our approaches both in terms of system performance and efficiency. Furthermore, by analysing the performance of the 2004 experiments we were able to predict which experiments would be most successful and/or most interesting for submission as official 2005 runs. Here, we describe the experimental process in detail and present their results both in terms of efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Process</head><p>Our experiments can be categorized into two parts: frontend and backend. The frontend consisted of the queries input into Zettair from the natural language processing and query expansion experiments, while the backend consisted of the different hardware models used during the single index and system distribution experiments. To test the frontend we performed a baseline experiment (Base) consisting of the original terms in the topic, followed by an additional five (5) query expansions experiments as outlined in table 4.1. Each was these experiments was repeated for the original title terms and the terms derived from the description using NLPX, giving us a total of 12 query sets input into Zettair. To test the backend we input these queries into both our single index and distributed indexes. Our two different merging algorithms (Rank by Relevance and Round-Robin) were performance at a later stage. Overall this gave us a set of thirty-six (36) experiments performed on the 2004 query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Name Description Base</head><p>The baseline experiment consisting of the original topic terms SP</p><p>The topic terms augmented with singular/plural derivatives SP.Stem</p><p>The topic terms augmented with singular/plural derivatives and stems SP.Syn</p><p>The topic terms augmented with singular/plural derivatives and synonyms SP.Syn.Stem</p><p>The topic terms augmented with singular/plural derivatives, synonyms and stems SP.Syn.Stem.SynStem The topic terms augmented with singular/plural derivatives, synonyms, stems and the stems of the synonyms Table <ref type="table" coords="4,251.89,319.84,3.55,8.48" target="#tab_2">4</ref>.1. Query Expansion Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency Results</head><p>Here we present the time it took to perform our experiments. There are two datasets that we focused on, the indexing time and querying time. Graph 4.1 and Table <ref type="table" coords="4,257.38,375.16,3.91,8.48" target="#tab_2">4</ref>.2 present the indexing time, measured in hours, for each of the seven sub-collections as well as the single index (SI) system. Note that the seventh sub-collection took much longer -almost twice as long -to index than the other sub-collections. For this reason we have included two averages, one that is the average for the 7 sub-collections, while the second average excludes the seventh sub-collection. The results indicate that the distributed system is significantly faster than the single index system -around six times as fast. This is not unexpected, since each of the sub-collections only one-seventh of the original collection. However, one must consider that the single index was created on a high-end super computer that costs US$55,000 while the distributed system was created on seven mid-range personal computers that cost $US1,500 each or $US10,500 in total. Hence, the distributed system is much more economically efficient than the supercomputer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Time</head><p>0:00:00 1:00:00 2:00:00 3:00:00 4:00:00 5:00:00 6:00:00 7:00:00 8:00:00 9:00:00 10:00:00 11:00:00 12:00:00  <ref type="table" coords="5,160.77,42.22,3.91,8.48" target="#tab_2">4</ref>.3 present the querying time, measured in hours, for each of the seven sub-collections as well as the single index (SI) system. Once again the seventh sub-collection took much longer to query than the other subcollections. Therefore we have again included two averages. For clarity Graph 4.2 only plots the times for single index and the average of sub-collections 1 -6. Not surprisingly, the more terms augmented to the query, the longer it took to execute, hence, the queries augmented with synonyms execute slower than the queries without synonyms, a situation magnified for the single index system. To illustrate, Table <ref type="table" coords="5,328.86,96.28,3.91,8.48" target="#tab_2">4</ref>.4 presents the ratio in query time between the supercomputer and the average query time for sub-collections 1 -6. In queries that contain synonyms, highlighted in grey in Table <ref type="table" coords="5,135.09,118.00,3.52,8.48" target="#tab_2">4</ref>.4, the distributed system was between 14 and 35 times faster than the single index system, while in queries without synonyms its was 'only' 1.5 to 6 times faster. </p><formula xml:id="formula_0" coords="4,197.58,652.26,254.25,3.94">1 2 3 4 5 6 7 S I A v g ( 1 -7 ) A v g ( 1 -6 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Times</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieval Performance Results</head><p>Tables 4.5, 4.6 and 4.7 present the performance results of the 2004 query set. We present results of the MAP, Bpref and number of relevant results at 10 documents, since these were the official metrics used in the 2005 Terabyte track. Here we discuss the relevant performance of the experiments across the entire 2004 query set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Baseline</head><p>Overall, the best performing experiment was the baseline system that had a single index backend, used the title as input and did not perform any query expansion. While this could be described as disappointing, it was not unexpected since several of the techniques used are known to degrade performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">System Distribution</head><p>The distributed systems outperformed the single index systems in the Bpref metric (avg. 15%), but the trend was the opposite for both the MAP metric (avg. 12%) and P@10 metric (16%). Furthermore, there was not much difference between the two types of merging algorithms, however, the Relevance Merge algorithm tended to slightly outperform the Round Robin Merge algorithm on the MAP and Bpref (&lt;1%) metric. Interestingly, the two merging algorithms performed exactly the same under the P@10 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Natural Language Processing</head><p>Generally, the standard ad-hoc (title as input) outperformed the natural language processing system (description as input) in the MAP (avg 30%), Bpref (avg 9%) and P@10% (avg 27%) metrics. The degradation was especially severe when natural language processing was combined with synonym query expansion. However, when no query expansion was used the natural language processing system outperformed the standard ad-hoc system in the Bpref metric (avg 4%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Query Expansion</head><p>Query expansion tended to decrease performance dramatically in the MAP (86%) Bpref (30%), and P@10 (49%) metrics, with the exception of singular/plural expansion which outperformed the baseline using the Bpref (4%) and P@10 metrics (7%). The performance decrease of query expansion was far worse when the description was taken as input rather than the title, and when synonyms were added to the query rather than stems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.0">2005 Experiments</head><p>We conducted our experiments using the 2005 query set in the same manner as we did for the 2004 query set. Once again 36 experiments were conducted; however, only 4 of them were allowed to be submitted as official TREC runs. Here, we present 2005 runs and their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">2005 Runs</head><p>We based our decision on which runs we could submit for TREC 2005 both on our analysis of our 2004 experiments, and the experiments we thought would provide the most interesting discussion at the workshop. In particular, we wanted to address the three main areas of our research: system distribution; natural language processing; and query expansion. Here we describe the runs, and outline our justification for selecting them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Baseline (QUT05TBEn)</head><p>For out baseline system we had a single index backend, used the title as input and performed no query expansion. This was the best performing experiment in the corresponding 2004 query set and was the logical choice to use as a baseline for the other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">System Distribution (QUT05TBMRel)</head><p>The first comparison we wanted to make was between a single index and distributed system. In order to make a valid comparison with our baseline system we used the same input (title without query expansion) and only changed the backend from a single index system to distributed system. We used the Relevance Rank merging method since that slightly outperformed the Round Robin method in our experiments on the 2004 query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Natural Language Processing (QUT05DBEn)</head><p>Our second comparison was between a system using natural language queries as input and a system using standard keywords input. Like our baseline system, we used a single index backend and performed no query expansion. As input we used the topics' description tags and parsed it our natural language processor -NLPX -to derive important content terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Query Expansion (QUT05TSynEn)</head><p>Our final comparison was between a system with a query expansion and a system without. Like the baseline system we used a single index backend, however, we performed both plural/singular and synonym expansion on the topic's title element. As with our 2004 experiments we used heuristics for plural/singular expansions and Wordnet to find query term synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">2005 Results</head><p>Here we present the results of our official 2005 runs. Table <ref type="table" coords="8,304.29,141.58,3.94,8.48">5</ref>.1 shows the overall MAP, Bpref and P@10 for each of our four runs, as well as the average maximum, minimum and median values of the other 2005 TREC participants. We also present three graphs that display a topic-by-topic MAP of the runs. Each of the graphs compares the baseline with other systems. We have sorted the graphs' topics according the baseline's MAP value rather than by topic number. Here, we discuss the performance of each system in comparison with the baseline, and outline the queries in which one system performed significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Baseline (QUT05TBEn)</head><p>Graph 5.1 presents the MAP results for our baseline system in comparison with the maximum and median values of the other 2005 participants. This provides a guide to see how well the system performed in absolute terms against other participants. Generally, the baseline performed similarly to the median, however, it performed poorly against the maximum. As with our 2004 experiments the baseline system generally performed better than the other runs. However, there were some specific topics where the other runs outperformed the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Maximum Median</head><p>Graph 5.1. Topic-by-Topic MAP -Baseline vs Other Participants</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">System Distribution (QUT05TBMRel)</head><p>Graph 5.2 presents the MAP results for our baseline system in comparison with the distributed system. Since the input for both systems was the same (title without any query expansion) this graph provides a means to observe the effect that different hardware models -single index and multiple indexes on a distributed system -have on system performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP</head><p>Baseline (Single Index) Distributed</p><p>Graph 5.2. Topic-by-Topic MAP -Baseline vs. Distributed System Overall the distributed system performed very well in comparison with the baseline system, with only a 3% decrease in MAP and 1 % decrease in Bpref; although, it did suffer a degradation of 24% under the P@10 metric. The lack of degradation under the MAP and Bpref was unanticipated given we divided our corpus randomly and that we used a naïve method of collection fusion. However, the high degradation at P@10, could mean that while the distributed system retrieved a similar number of relevant results as the single index system but ranked them lower. Interestingly, the distributed system significantly outperformed the baseline (&gt;10% under MAP and Bpref) in topic numbers 752, 761, 773, 776, 787, and 788 however, it was significantly outperformed by the baseline systems in topic numbers 767, 774, 780, 781 and 800.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Natural Language Processing (QUT05DBEn)</head><p>Graph 5.3 presents the MAP results for our baseline system in comparison with the natural language processing and query expanded systems. Since the hardware model for all three systems was the same (single index), this graph provides a means to observe the effect that different inputs -title, keywords derived from a natural language query, and a query expanded with synonyms -have on system performance. Graph 5.3. Topic-by-Topic MAP -Baseline vs. Different Inputs</p><p>Overall, the natural language system also compared favourably to the baseline system, outperforming it by 2% under the Bpref metric. However, under the MAP metric it suffered degradation of 15%, and under the P@10 metric a degradation of 29%. Unfortunately, performance degradation is common when using natural language processing and information retrieval <ref type="bibr" coords="9,163.23,709.00,62.74,8.48" target="#b11">(Smeaton, 1997)</ref>. However, examples of topics that it significantly outperformed the baseline include 766, 779, 784 and 796 while examples of topics were it was outperformed by the baseline include 752, 780, 788, 791 and 799.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Query Expansion (QUT05TSynEn)</head><p>The query expansion run performed the worst in comparison with the other experiments. The baseline system outperformed it by 115% under the MAP metric, 53% under the Bpref metric and 61% under the P@10 metric (Graph 5.3). While these results were disappointing they were not completely unexpected given the naïve method in which synonyms were chosen for the query. If more advanced methods such as part of speech recognition or word sense disambiguation had been used then the performance may have improved. However, the experiment did outperform the baseline in a number of topics such as 760,765, 777 and 787 but was significantly outperformed by the baseline in topics such as 759, 767, 781, 790 and 791.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.0">Robust Track Participation</head><p>In previous years, our group has participated in the Initiative for the Evaluation of XML Retrieval (INEX) with a dedicated XML search engine. This year we participated in the Robust track at TREC and are trying to discover the difference between document-level information and the XML information retrieval. As the TREC collection is "well formatted" XML-like (SGML) documents, we intended to use our indexer and search engine with only minor changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Indexing</head><p>The documents were indexed using an inverted file approach that was designed for XML retrieval. The words were stemmed using porter stemmer and stop words were removed from the index to reduce the size of index file. Our index file is in Microsoft access format. Due to the size limitation of access, each file must be less than 2GB. Thus, we split the whole collection into 5 sub-collections. We eventually ported the system to SQL Server (still distributed)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Searching</head><p>In our XML oriented search-the score of an element (in this case the DOC element) was computed using the following formula:</p><p>(3)</p><p>Here n is the number of unique query terms contained within the element. When a phrase is found n is incremented by the number of terms in the phrase, instead of 1. This rewards a phrase more heavily than a non-phrase set of the same term in the element. The term N is an integer -we used the value N=5. The term N n-1 scales up the score of elements having multiple distinct query terms, and phrases even more. The system is not sensitive to the value of N -we experimented with N=5 to 50 with little difference in results. The term f i is the frequency of the i th query term in the collection. The term t i is the frequency of the i th query term in the element.</p><p>The usual approach to phrase searching is based on term proximity. We implemented this in the usual manner.</p><p>Because our search engine is geared towards finely grained XML documents, we also have a concept of a partial phrase -words that appear in the same context (say sentence or even paragraph) but do not strictly constitute a phrase are regarded as a partial phrase and will be given higher score. We treated partial phrases as phrases in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Searching a distributed collection</head><p>The AQUAINT collection was vertically and randomly split into 5 sub-collections, each of which was searched independently. Finally the results were merged together. Results were ranked locally, but without normalizing scoresto allow meaningful comparisons between scores obtained from searching different partitions of the collection. The underlying assumption is that global collection statistics are similar with large sub-collections and the non-thematic vertical split. This assumption is not entirely accurate, and there is a small tradeoff in reduced precision for increased speed which can be very high in a federated collection setting. We leave out the discussion of distributed searches since it is outside the scope of this Robust track investigation.</p><formula xml:id="formula_1" coords="10,257.40,423.39,97.63,31.83">∑ = - + + = n i i i n f t N L 1 1 ) 1 log( ) 1 log(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">2005 results</head><p>The official submission performed very poorly. Our MAP is only 0.0294. This was surprising and so after analyzing the results we discovered a bug in the Indexer, introduced when changing from INEX to TREC, and from Microsoft Access to SQL Server. Unfortunately about 20% of the term postings were lost. Regretfully we only just met the submission deadlines and so did not fix the problem. Discussion of these results is meaningless. Therefore, we discuss results that we have subsequently obtained after fixing the indexer and re-indexing the collection, with the official qrels, as shown in table <ref type="table" coords="11,146.71,118.72,3.55,8.48">6</ref>.1. The intended submission has a much better result, albeit still below the median result over all submissions TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUT_Official</head><p>The TREC collection contains many documents that have no fine grained structure -just a single large &lt;text&gt; element. On the other hand, typical XML documents contain fine structure -for instance, the INEX XML collection contains paragraph sized XML leaves. Our node scoring approach to XML documents aims for retrieval of specific nodes, not entire documents. It exploits granularity by rewarding nodes that contain more of the query terms. This is usually done in text retrieval through proximity scoring of one kind or another -which our ranking system does not apply since it assumes fine grained XML elements. Since the TREC collection contains numerous DOC elements that are very large, often a document will contain a few or all of the query terms, but not in the same context. Our XML ranking strategy does not account for this and will therefore find many false positive results. The search engine fails to find many relevant documents until P30. The query for 404 is "talk, peace, Ireland". Our search engine will give higher rank to documents that have more matches over the terms "peace", "talks", and even "peace talks", but not necessarily in the context of "Ireland". Since a strict phrase containing all 3 rarely occurs -even in the correct context -our search engine relies on the fine granularity of XML text elements to implicitly identify when the 3 occur in the same context (say sentence or paragraph.). As the terms "talk" and "peace" are very common in the collection many irrelevant documents will be ranked highly and this strategy fails with the structure-less TREC documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Query Expansion</head><p>In our experiment, two methods of query expansion were investigated: plural/singular expansion and Porter stemming. Plurals and singulars were added using lexical-based heuristics to determine the plural form of a singular term (and vice-versa.) Porter stemming was performed on the query terms and retrieval was then based on term stems rather than on the query terms. The results are shown as table <ref type="table" coords="11,272.18,523.96,3.54,8.48">6</ref> Searches with plural/singular expansion and with porter stemming provide similar performance. The Porter stemmer did provide some benefit, but slowed down the search by increasing the number of term postings accessed. Topic by topic examination reveals the usual behaviour -sometimes the stemmer improves precision and sometimes it degrades it. For instance, in topic 408, we are locking for "tropical storms" where the term "tropical" shares a Porter stem with "tropicality", "tropicalization", "tropicalize", "tropically", "tropicals". However most of these are irrelevant to our topic. This may or may not lead to precision penalties, but will always lead to performance penalties. In this case (408) the average precision is 0.0710 for porter stemming and 0.0961 in plural/singulars. With more advanced methods such as part of speech analysis or word sense disambiguation performance may be improved by stemming -we intend to study this in future evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.0">Conclusion</head><p>We performed a set of experiments on an existing open source information retrieval system. We performed two sets of experiments. Our first set of experiments compared the performance of Zettair on a high performance supercomputer with a distributed system of seven midrange personal computers. Our results indicate that the distributed system was more efficient than the supercomputer, both in terms of speed and economics, we have been able to achieve comparable retrieval performance. Our second set of experiments used three different set of inputs: a standard TREC title; a natural language query and an expanded query. Our results indicate that the natural language query had a retrieval performance comparable with the standard title while the expanded query was significantly worse. Interestingly however, all of the experiments outperformed the baseline in some topics. We will continue to research these areas and in particular investigate the topics where the experiments outperformed the baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,113.22,118.12,10.94,8.48;2,79.32,139.60,27.70,8.48;2,79.32,161.20,2.62,8.48;2,101.34,161.20,59.34,8.48;2,79.32,172.06,6.81,8.48;2,101.37,172.06,161.65,8.48;2,79.32,182.92,6.43,9.14;2,101.34,182.92,182.97,8.48;2,79.32,193.66,231.15,9.20;2,79.32,215.38,13.66,8.48;2,113.22,247.90,10.94,8.48;2,79.32,269.44,27.70,8.48;2,79.32,291.04,49.23,9.20;2,79.32,301.90,4.71,8.48;2,102.04,301.90,31.23,8.48;2,79.32,312.70,155.02,9.14;2,79.32,323.50,206.95,8.49"><head></head><label></label><figDesc>documents in the collection ƒ t = Number of documents that a term t occurs in ƒ d , t = Number of times that a term t occurs in document d and L d = Length of document d in bytes, AL = Average document length over the collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,477.17,250.96,55.63,8.48;10,79.32,261.82,406.65,8.48;10,79.32,272.68,453.45,8.48;10,79.32,283.42,453.44,8.48;10,79.32,294.28,453.42,8.48;10,79.32,305.08,453.44,8.48"><head></head><label></label><figDesc>Term postings consist of XPath to the containing element, and position within the XPath context. For instance, the posting { /document[10]/body[1]/chapter[3]/section[5]/paragraph[2] , 23 } identifies the precise position of a term in an XML document with some self-explanatory structure. The indexer was originally developed for INEX and it is basically used for indexing XML documents. When indexing, the indexer will record the term, the term position in the context (context position), the term position in the article (global position), the context name (XPATH) and also the article ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,79.32,684.22,453.51,19.28"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="2,430.68,684.22,102.15,8.48"><row><cell>.1 presents the directories</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,83.76,168.96,459.72,563.35"><head>Table 4 .</head><label>4</label><figDesc>4. Ratio of Query Time of Single Index vs. Distributed System</figDesc><table coords="5,83.76,168.96,459.72,551.53"><row><cell></cell><cell>1:30:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1:20:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1:10:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time (Hours)</cell><cell>0:30:00 0:40:00 0:50:00 1:00:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0:20:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0:10:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0:00:00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Tit le. Ba se</cell><cell>Tit le. SP</cell><cell cols="7">Tit le. SP .S yn Tit le. SP .S tem Tit le. SP .S yn .S tem Tit le. SP .S yn .S tem .S yn St em De sc rip tio n.B as e De sc rip tio n.S P De sc rip tio n.S P. Sy n De sc rip tio n.S P. St em De sc rip tio n.S P. Sy n.S tem De sc rip tio n.S P. Sy n.S tem .S yn St em</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Single Index</cell><cell cols="2">Avg(1-6)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Graph 4.2. Query Times</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg</cell><cell>Avg</cell></row><row><cell cols="2">Experiment Name</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>SI</cell><cell>(1-7)</cell><cell>(1-6)</cell></row><row><cell>Title.Base</cell><cell></cell><cell cols="2">01:22</cell><cell cols="6">01:23 01:11 01:04 01:07 01:10 02:15</cell><cell cols="2">05:59 01:22 01:13</cell></row><row><cell>Title.SP</cell><cell></cell><cell cols="2">01:26</cell><cell cols="6">01:24 01:08 01:07 01:12 01:14 02:31</cell><cell cols="2">07:45 01:26 01:15</cell></row><row><cell>Title.SP.Syn</cell><cell></cell><cell></cell><cell>01:40</cell><cell cols="6">01:33 01:22 01:15 01:17 01:20 02:43</cell><cell cols="2">45:40 01:36 01:24</cell></row><row><cell>Title.SP.Stem</cell><cell></cell><cell></cell><cell>03:59</cell><cell cols="6">03:20 02:20 02:23 02:22 02:43 16:15</cell><cell cols="2">09:12 04:46 02:51</cell></row><row><cell>Title.SP.Syn.Stem</cell><cell></cell><cell></cell><cell>04:06</cell><cell cols="6">03:30 02:28 02:23 02:28 02:55 17:34</cell><cell cols="2">47:23 05:03 02:58</cell></row><row><cell cols="2">Title.SP.Syn.Stem.SynStem</cell><cell></cell><cell>04:19</cell><cell cols="6">03:36 02:37 02:33 02:34 03:05 18:48</cell><cell cols="2">44:41 05:22 03:07</cell></row><row><cell>Description.Base</cell><cell></cell><cell></cell><cell>01:19</cell><cell cols="6">01:17 01:13 01:08 01:11 01:09 01:29</cell><cell cols="2">03:10 01:15 01:13</cell></row><row><cell>Description.SP</cell><cell></cell><cell></cell><cell>01:21</cell><cell cols="6">01:20 01:14 01:06 01:17 01:14 01:43</cell><cell cols="2">04:52 01:19 01:15</cell></row><row><cell>Description.SP.Syn</cell><cell></cell><cell></cell><cell>01:47</cell><cell cols="6">01:41 01:30 01:24 01:28 01:26 02:12</cell><cell cols="2">55:00 01:38 01:33</cell></row><row><cell>Description.SP.Stem</cell><cell></cell><cell></cell><cell>06:38</cell><cell cols="6">04:27 03:22 03:04 03:06 03:41 23:05</cell><cell cols="2">06:27 06:46 04:03</cell></row><row><cell cols="2">Description.SP.Syn.Stem</cell><cell></cell><cell>07:08</cell><cell cols="8">05:14 03:30 03:15 03:18 04:02 28:44 1:19:58 07:53 04:25</cell></row><row><cell cols="4">Description.SP.Syn.Stem.SynStem 07:47</cell><cell cols="8">05:48 03:38 03:29 03:32 04:14 28:46 1:16:48 08:11 04:45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Table 4.3. Query Times</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Experiment Name</cell><cell></cell><cell>Ratio</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Title.Base</cell><cell></cell><cell></cell><cell></cell><cell>4.94</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Title.SP</cell><cell></cell><cell></cell><cell></cell><cell>6.18</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Title.SP.Stem</cell><cell></cell><cell></cell><cell></cell><cell>3.22</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Title.SP.Syn</cell><cell></cell><cell></cell><cell></cell><cell>32.43</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Title.SP.Syn.Stem</cell><cell></cell><cell></cell><cell>15.94</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Title.SP.Syn.Stem.SynStem</cell><cell></cell><cell>14.31</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Description.Base</cell><cell></cell><cell></cell><cell></cell><cell>2.60</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Description.SP</cell><cell></cell><cell></cell><cell></cell><cell>3.87</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Description.SP.Stem</cell><cell></cell><cell></cell><cell>1.59</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Description.SP.Syn</cell><cell></cell><cell></cell><cell>35.61</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Description.SP.Syn.Stem</cell><cell></cell><cell></cell><cell>18.14</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Description.SP.Syn.Stem.SynStem</cell><cell></cell><cell>16.19</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="11,79.32,293.68,453.47,75.27"><head></head><label></label><figDesc>Consider for instance topic 404, with precision details as the table below.</figDesc><table coords="11,124.80,326.62,362.41,42.33"><row><cell>Position</cell><cell>P5</cell><cell>P10</cell><cell>P15</cell><cell>P20</cell><cell>P30</cell><cell>P100</cell><cell>P200</cell><cell>P500</cell><cell>P1000</cell></row><row><cell>Precision</cell><cell cols="9">0.6000 0.5000 0.4000 0.3500 0.400 0.3700 0.3050 0.2420 0.1570</cell></row><row><cell cols="2">Documents 3</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>12</cell><cell>37</cell><cell>61</cell><cell>121</cell><cell>157</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Table 6.2: Precision of topic 404</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="11,128.40,523.96,323.35,77.91"><head></head><label></label><figDesc>.3.</figDesc><table coords="11,128.40,556.90,323.35,44.97"><row><cell></cell><cell>No expansion</cell><cell>Plural and Singular</cell><cell>Porter stemming</cell></row><row><cell>MAP</cell><cell>0.1448</cell><cell>0.1578</cell><cell>0.1613</cell></row><row><cell>P10</cell><cell>0.3740</cell><cell>0.3860</cell><cell>0.3840</cell></row><row><cell></cell><cell cols="3">Table 6.1. MAP, P10 Results with expansion</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.0">Acknowledgments</head><p>We would like to thank the staff at the <rs type="institution">High Performance Computing laboratory</rs>, in particular Ashley Wright and <rs type="person">Mark Barry</rs>, without whom we would not have been able to complete this research. We would also like to thank <rs type="person">Daniel Tao</rs> who assisted us with laboratory experiments. Finally, we would like to thank the <rs type="institution">QUT</rs>'s School of Software Engineering and <rs type="person">Data Communications</rs> and <rs type="funder">School of Information Systems</rs> for funding part of this project.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,79.32,272.68,453.43,8.48;12,79.32,283.42,453.47,8.48;12,79.32,294.28,277.97,8.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,509.17,272.68,23.58,8.48;12,79.32,283.42,94.72,8.48">RMIT University at TREC 2004</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Billbereck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cannane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chattaraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yiannis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,193.14,283.42,200.56,8.48">The Thirteenth Text REtrieval Conference (TREC-13)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,305.08,453.52,8.48;12,79.32,315.88,341.22,8.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,116.65,305.08,255.43,8.48">Some Advances in Transformation-Based Part of Speech Tagging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,391.32,305.08,141.52,8.48;12,79.32,315.88,178.00,8.48">Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94)</title>
		<meeting>the Twelfth National Conference on Artificial Intelligence (AAAI-94)<address><addrLine>Seattle, Washington, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,326.74,453.50,8.48;12,79.32,337.54,172.20,8.48" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,120.99,326.74,123.75,8.48">Distributed information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,341.58,326.74,131.32,8.48">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="127" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,348.34,300.61,8.48" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,132.90,348.34,158.05,8.48">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,359.20,342.64,8.48" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,163.43,359.20,105.52,8.48">The NLP task at INEX 2004</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sahama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,285.72,359.20,51.57,8.48">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="50" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,370.00,453.34,8.48;12,79.32,380.80,253.17,8.48" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,170.23,370.00,217.44,8.48">Efficient single-pass index construction for text databases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,408.78,370.00,123.88,8.48;12,79.32,380.80,152.17,8.48">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="713" to="729" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,391.66,453.48,8.48;12,79.32,402.40,399.82,8.48;12,79.32,413.26,326.94,8.48" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,271.44,391.66,261.37,8.48;12,79.32,402.40,93.22,8.48;12,130.06,413.26,122.61,8.48">A probabilistic model of information retrieval: development and comparative experiments</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,178.83,402.40,195.24,8.48">Parts 1&amp;2., Information Processing &amp; Management</title>
		<editor>
			<persName><surname>Program</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">2000. 1980</date>
		</imprint>
	</monogr>
	<note>An algorithm for suffix stripping</note>
</biblStruct>

<biblStruct coords="12,79.32,424.12,453.40,8.48;12,79.32,434.86,97.78,8.48" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,222.53,424.12,213.19,8.48">Natural Language Processing for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,442.92,424.12,89.80,8.48;12,79.32,434.86,16.71,8.48">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="101" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,445.72,453.40,8.48;12,79.32,456.58,173.94,8.48" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,197.22,445.72,180.60,8.48">In-situ generation of compressed inverted files</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A H</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,395.70,445.72,137.02,8.48;12,79.32,456.58,76.14,8.48">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="537" to="550" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,467.32,453.46,8.48;12,79.32,478.18,453.42,8.48;12,79.32,489.04,62.00,8.48" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,277.10,467.32,218.36,8.48">Compression of inverted indexes for fast query evaluation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yiannis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,512.10,467.32,20.68,8.48;12,79.32,478.18,358.71,8.48">Proc. ACM-SIGIR International Conference on Research and Development in Information Retrieval</title>
		<meeting>ACM-SIGIR International Conference on Research and Development in Information Retrieval<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,499.78,453.44,8.48;12,79.32,510.64,324.56,8.48" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,139.01,499.78,235.70,8.48">Using NLP or NLP Resources for Information Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,391.64,499.78,49.15,8.48;12,463.32,499.78,69.44,8.48;12,79.32,510.64,80.51,8.48">Natural Language Information Retrieval</title>
		<meeting><address><addrLine>Dordrecht, NL</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publisher</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
	<note>Strzalkowski</note>
</biblStruct>

<biblStruct coords="12,79.32,521.50,453.47,8.48;12,79.32,532.24,453.50,8.48;12,79.32,543.10,84.72,8.48" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,143.63,521.50,298.95,8.48;12,79.32,532.24,392.90,8.48">Information Extraction -A Multidisciplinary Approach to an Emerging Information Technology</title>
		<author>
			<persName coords=""><forename type="middle">A F</forename><surname>Smeaton</surname></persName>
		</author>
		<editor>M. Pazienza</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="115" to="138" />
		</imprint>
	</monogr>
	<note>Information Retrieval: Still Butting Heads with Natural Language Processing</note>
</biblStruct>

<biblStruct coords="12,79.32,553.90,453.46,8.48;12,79.32,564.70,265.49,8.48" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,151.21,553.90,170.83,8.48">What is the role of NLP in text retrieval?</title>
		<author>
			<persName coords=""><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,338.86,553.90,193.93,8.48;12,79.32,564.70,33.23,8.48">Strzalkowski (ed) Natural Language Information Retrieval</title>
		<meeting><address><addrLine>Dordrecht, NL</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publisher</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,575.56,444.04,8.48" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,172.56,575.56,152.20,8.48">Natural Language Information Retrieval</title>
		<editor>Strazlkowski, T.</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Kluwer Academic Publisher</publisher>
			<pubPlace>Dordrecht, NL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,586.36,453.50,8.48;12,79.32,597.16,453.43,8.48;12,79.32,608.02,325.86,8.48" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,233.02,586.36,213.44,8.48">Natural Language Information Retrieval TREC-6 Report</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Carballo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,197.04,597.16,187.27,8.48">The Sixth Text REtrieval Conference (TREC-6)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="347" to="366" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,618.82,453.43,8.48;12,79.32,629.62,453.48,8.48;12,79.32,640.48,144.52,8.48" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,216.33,618.82,85.93,8.48">NLP Track at TREC-5</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sparck Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,497.04,618.82,35.71,8.48;12,79.32,629.62,148.57,8.48">The Fifth Text REtrieval Conference (TREC-5)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">500</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,651.28,453.41,8.48;12,79.32,662.07,453.47,8.48;12,79.32,672.93,393.52,8.48" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,79.32,662.07,217.68,8.48">Natural Language Information Retrieval: TREC-7 Report</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Järvinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Voutilainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,486.48,662.07,46.31,8.48;12,79.32,672.93,137.82,8.48">The Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>NIST, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,683.73,453.49,8.48;12,79.32,694.53,385.17,8.48" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,194.37,683.73,244.28,8.48">NLPX-An XML-IR System with a Natural Language Interface</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Woodley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,461.55,683.73,71.26,8.48;12,79.32,694.53,178.24,8.48">Proceedings of the Australasian Document Computing Symposium</title>
		<meeting>the Australasian Document Computing Symposium<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12-13">December 13 2004</date>
			<biblScope unit="page" from="71" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,79.32,705.39,453.46,8.48;12,79.32,716.19,453.50,8.48;12,79.32,726.99,244.62,8.48" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,194.01,705.39,79.45,8.48">NLPX at INEX 2004</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Woodley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,296.70,705.39,236.08,8.48;12,79.32,716.19,324.89,8.48">Advances in XML Information Retrieval: Third International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2004, Dagstuhl</title>
		<title level="s" coord="12,79.32,726.99,90.43,8.48">Revised Selected Papers</title>
		<meeting><address><addrLine>Germany; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">December 6-8, 2004. 2005</date>
			<biblScope unit="page">3493</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
