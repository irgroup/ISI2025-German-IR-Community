<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,68.04,109.59,458.97,16.19">Tianwang Search Engine at TREC 2005: Terabyte Track</title>
				<funder ref="#_bTU2rgA">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_7xUu2g8">
					<orgName type="full">PRC Ministry of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.98,131.30,65.24,10.79"><forename type="first">Yan</forename><surname>Hongfei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="laboratory">PENG Bo Network and Distribution System Laboratory</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.93,131.30,51.00,10.79"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="laboratory">PENG Bo Network and Distribution System Laboratory</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.98,131.30,46.55,10.79"><forename type="first">Jiaji</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="laboratory">PENG Bo Network and Distribution System Laboratory</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,68.04,109.59,458.97,16.19">Tianwang Search Engine at TREC 2005: Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F4C6393E1B246E56C1F79CF185B0150A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Search Engine, Evaluation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tianwang for the first time participated in all three tasks of the Terabyte Track of TREC 2005 to explore its performance. All three tasks, including the adhoc task (find all the relevant documents with high precision), the efficiency task (find top- 20 results for each of 50k-entry queries with efficiency and scalability) and the named page finding task (sometimes search a page by name), are based on a 426GB collection of 25.2 million pages taken from the .gov Web domain ("GOV2"). In the adhoc task with 50 topics, Tianwang returned at least one relevant document in top 10 for 42 topics. In the efficiency task, Tianwang returned at least one relevant document in top 20 for 44 of the 50 quires. In the named page task with 252 topics, Tianwang returned a desired page in top 10 for 99 topics; meanwhile, it failed to find a correct one for 120 topics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Tianwang <ref type="bibr" coords="1,105.69,467.48,60.66,7.88">[Tianwang,2005]</ref> is a search engine developed and maintained by the network and distributed system laboratory of Peking University. As a retrieval system with the capability of searching billions of web pages, it is an ideal baseline system for experimenting on TREC tasks.</p><p>Tianwang works in a general model which most of search engines adopted, consisting of three components, a crawling process, an organizing process and a servicing process. The crawling process takes charge of collecting pages from the Web and stores them as an input for the following process. Then the organizing process answers for dereplicating pages fed from the previous process and creating indices for the remaining data. Finally the servicing process shows an interface to users for searching.</p><p>Our goals of this year's participating terabyte track were modest, to complete runs with the Tianwang system (software and partial idle machines), and to gain experience in the procedure of evaluations. The experience is important, so we could do better organizing the Chinese Web Tack of the CWIRF <ref type="bibr" coords="1,232.83,653.72,53.30,7.88">[CWIRF,2005]</ref> with the CWT100g collection <ref type="bibr" coords="1,159.31,664.70,60.29,7.88">[CWT100g,2004]</ref>.</p><p>Actually we built a search engine based on the GOV2 data. Its searching results are shown on Figure <ref type="figure" coords="1,187.52,691.52,3.29,7.88">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. The GOV2 Search Engine</head><p>The rest of the paper gives out an experimental work for terabyte retrieval including the adhoc task, the efficiency task and the named page finding task. We first describe the collection and tasks, then give the indexing environment, servicing process, experimental results, and finally discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COLLECTION AND TASK SUMMARY</head><p>In this section, we conduct terabyte retrieval on GOV2 data to show the performance of Tianwang.</p><p>The terabyte track of TREC 2005 used the GOV2 corpus, which is made up of about 25,205,179 documents crawled from the .gov Web domain, comprising about 426 GB of document source. There are three tasks in the terabyte track. The first one is the traditional task --ad-hoc retrieval. There are 50 topics. Each topic is presented in the usual TREC format, as a tagged document with three fields that summarize the information need at different levels of detail: a short 'title' consisting of just a few terms, a longer, more detailed 'description' field, and a multi-sentence 'narrative' field.</p><p>For each topic, participants create a query and submit a ranking of the top documents (no more than 10,000) for that topic.</p><p>The efficiency task is to find top-20 results for each topic of 50,000 entries which are mined from query logs of an operational search engine. Queries must be created automatically from these topics; manual runs are not permitted for this task.</p><p>The named page finding task is to search for a page by name. In such cases, an effective search system will return that page at or near rank one. Roughly 150 new topics will be created for this task. A run consists of the top 1000 documents for each topic. No manual or interactive query modification is permitted in this task.</p><p>Furthermore, for each run, groups should record and report the system characteristics, such as indexing time in minutes, total number of CPUs in system and total processing time for all topics in seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INDEXING ENVIRONMENT</head><p>We use eight idle machines/nodes of Tianwang system, which are available at that moment, to index all of the GOV2 corpus documents. Each machine has dual Xeon 2.8GHz CPUs with 2GB RAM running Red Hat Enterprise Linux AS release 4.</p><p>As far as TREC tasks are concerned, substituting the crawling process with the GOV2 corpus is a straightforward way to fulfill the TREC tasks with Tianwang. Due to different storage formats of TREC and Tianwang <ref type="bibr" coords="2,150.31,309.20,110.86,7.88">[TianwangStorageFormat,2003]</ref>, it is necessary to transform the GOV2 into the data which can be accepted by the organizing process of Tianwang. Because tasks require returning all correct documents, we should bypass the dereplicating step and build indices directly. Then using the servicing process generates all the required results according to the topics. Tianwang, as a full text retrieving system, is built with technologies of word-level segmentation and word position information. The organizing process first distributes all its input documents among different nodes in terms of site domain names. Nodes are independent each other, and indices are built on each of them. The two times in-memory inverted indexing algorithm is applied on each node. First we construct many inverted indices for small scale document set which is suitable for putting into the available memory. Then we execute multi-merging and construct the full indices of each node. The key steps of indexing process are listed as follows:</p><p>Parsing pages. According to the HTML grammar, we analyze each page's tags and structures, apply Chinese word segmentation and English grammar analyzer and extract index terms. During the procedure of analyzing, we record document frequencies (DF) of each index term and term frequencies (TF) of each index term per document, and get a lexicon file. Meanwhile, we save the parsing results to an analyzing result file.</p><p>Building temporary inverted files. In terms of statistic DF and TF, it is easy to estimate the length of each index term. Then we reserve memory according to the length and reload the analyzing result file produced last step. The corresponding inverted index is done in memory and saved the result to the disk.</p><p>Merging small inverted indices. The step begins with the multi-lane merging algorithm, compress index term coding, and generate the final inverted index file.</p><p>The index references documents for 21,593,661 of 25,205,168 documents of the GOV2 corpus. Total indexing time of eight machines is 1,514 minutes. The size of on-disk file structures/the final index file is 45.5GB. The reasons of missing 3,611,507 documents is still unknown, maybe lying in the process of transforming data format (TREC format to Tianwang format) or indexing documents due to some invalid documents or Tianwang software bugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SERVICING PROCESS</head><p>Figure <ref type="figure" coords="2,340.89,182.66,4.38,7.88">2</ref> illustrates the servicing framework of Tianwang <ref type="bibr" coords="2,309.12,192.92,41.48,7.88">[Peng,2004]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. The Servicing Framework of Tianwang</head><p>The user first specifies a User Information Need which is parsed, at SE Service Point. Then Retrieval Agent collects retrieving results from all nodes and shows the ranked documents to the user. Relevant ranking is a linear combination of many ingredients <ref type="bibr" coords="2,528.06,398.96,16.82,7.88;2,309.12,409.22,38.63,7.88" target="#b1">[Lei, et al.,2001]</ref>. Among them, Boolean&amp;VM OP is the most important ranking policy which is constructed on the Boolean model and the vector model <ref type="bibr" coords="2,355.73,429.74,49.26,7.88">[Salton,1971]</ref>. Metadata can be date, document format, site name, class label, etc. Global properties can be page rank <ref type="bibr" coords="2,324.24,450.32,60.21,7.88" target="#b2">[Page, et al.,1998</ref>], authoritative site lists, relevance feedback, manual compilation etc. Based on technologies of the natural language processing, Semantic constrains recognizes special semantic relations in each document.</p><p>In the experiments of terabyte track, our ranking policy is the combination of Boolean&amp;VM OP and Global properties (page rank). We directly input title fields of topics to Tianwang without any changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>We submitted 2 runs for the adhoc task, without and with page rank (TWTB05AD01 and TWTB05AD02 respectively), 1 run for the efficiency task (TWTB05EF01), and three runs for the named page finding run, without and with different applying methods of page rank (TWTB05NP01, TWTB05NP03 and TWTB05NP03). All submission runs are illustrated in the Table <ref type="table" coords="2,483.06,614.96,3.29,7.88" target="#tab_0">1</ref>. If a run using technologies of link analysis, anchor text or other document structure, there will be a 'yes' tag in the corresponding filed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We archived the goal for the terabyte track, submitting all runs for three tasks of the track, and gained much experience in the procedure of evaluations.</p><p>The problems we encounter are TREC format and part of not indexed documents.</p><p>Each document in the GOV2 corpus is identified by a DocNo, which is not consisted with identifies in the Tianwang data. So we have to map the URLs of runs to corresponding DocNos. In the organizing process, Tianwang transformed the URLs into a decoding format, such as "%20" becoming " ". To guarantee the mapping process, we transformed the "url2id" file, provided with the GOV2 corpus, into the decode format -"url2id.decode".</p><p>Unfortunately, there are still a few entries in the submitting runs which could not find corresponding DocNos. We have not find where the bug lies. Additionally, there are 3,611,507 documents of the GOV2 corpus were not indexed due to unknown reasons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,301.68,213.43,234.69,505.71"><head>Table 1 . Submitted Runs</head><label>1</label><figDesc></figDesc><table coords="2,317.22,682.52,209.63,36.62"><row><cell>Run</cell><cell>Link</cell><cell>Anchor</cell><cell>Other Doc</cell></row><row><cell></cell><cell>Analysis</cell><cell>Text</cell><cell>Structure</cell></row><row><cell cols="2">TWTB05EF01 -</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,58.68,291.74,220.35,96.62"><head>Table 2 . Submitted Runs for the GOV2 using TREC topics 751-800 and top 10,000 documents</head><label>2</label><figDesc></figDesc><table coords="3,58.68,316.64,217.44,71.72"><row><cell>Run</cell><cell>Ranking policy</cell><cell cols="2">P@10 MAP</cell></row><row><cell></cell><cell>(Bleean&amp;VM OP *a</cell><cell></cell></row><row><cell></cell><cell>+(1-a)*</cell><cell></cell></row><row><cell></cell><cell>pagerank*100)</cell><cell></cell></row><row><cell cols="2">TWTB05AD01 a=1</cell><cell>0.351</cell><cell>0.1128</cell></row><row><cell cols="2">TWTB05AD02 a=0.2</cell><cell>0.350</cell><cell>0.1118</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,71.58,411.38,195.39,71.72"><head>Table 3 . Submitted Runs for the GOV2 using TREC efficiency_topics 1-50,000 and top 20 documents</head><label>3</label><figDesc></figDesc><table coords="3,74.70,436.28,187.37,46.82"><row><cell>Run</cell><cell>Ranking policy</cell><cell>P@10</cell></row><row><cell></cell><cell>(Bleean&amp;VM OP *a</cell><cell></cell></row><row><cell></cell><cell>+(1-a)* pagerank*100)</cell><cell></cell></row><row><cell cols="2">TWTB05EF01 a=1</cell><cell>0.285</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,52.50,506.12,233.59,101.06"><head>Table 4 . Submitted Runs for the GOV2 using TREC np_topics 601-872 and top 1,000 documents Run Ranking policy (Bleean&amp;VM OP *a +(1-a)* pagerank*100)</head><label>4</label><figDesc></figDesc><table coords="3,74.70,531.08,185.96,76.10"><row><cell></cell><cell>MRR</cell></row><row><cell>TWTB05NP01 a=1</cell><cell>0.262</cell></row><row><cell>TWTB05NP02 a=0.2</cell><cell>0.261</cell></row><row><cell>TWTB05NP03 a=0</cell><cell>0.100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENTS</head><p>This work described therein is supported in part by <rs type="funder">PRC Ministry of Education</rs> grant <rs type="grantNumber">20030001076</rs> and by <rs type="funder">NSFC</rs> grant <rs type="grantNumber">60435020</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7xUu2g8">
					<idno type="grant-number">20030001076</idno>
				</org>
				<org type="funding" xml:id="_bTU2rgA">
					<idno type="grant-number">60435020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,313.52,356.05,89.64,10.50;3,309.12,368.42,233.60,8.10;3,344.16,378.50,80.49,8.10" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>References [cwirf</surname></persName>
		</author>
		<ptr target="http://www.cwirf.org/" />
		<title level="m" coord="3,376.61,368.42,161.49,8.10">Chinese Web Information Retrieval Forum</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,313.84,388.64,175.56,8.10;3,344.16,398.72,197.81,8.10;3,344.16,408.80,14.07,8.10;3,309.12,418.88,229.69,8.10;3,344.16,429.02,184.42,8.10;3,344.16,439.10,195.59,8.10;3,344.16,449.18,87.87,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="3,360.24,429.02,159.22,8.10">Improved relevance ranking in WebGather</title>
		<author>
			<persName coords=""><surname>Lei</surname></persName>
		</author>
		<ptr target="http://www.cwirf.org/SharedRes/DataSet/cwt100g.html" />
	</analytic>
	<monogr>
		<title level="m" coord="3,313.84,388.64,33.07,8.10;3,379.16,388.64,106.72,8.10">Chinese Web test collection</title>
		<imprint>
			<date type="published" when="2001">2004. 2001. 2001</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="410" to="417" />
		</imprint>
	</monogr>
	<note>CWT100g</note>
</biblStruct>

<biblStruct coords="3,309.12,459.26,71.02,8.10;3,396.71,459.26,139.59,8.10;3,344.16,469.40,172.56,8.10;3,344.16,479.48,198.24,8.10;3,344.16,489.56,105.84,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="3,390.89,469.40,125.82,8.10;3,344.16,479.48,97.67,8.10">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName coords=""><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,453.70,479.48,88.71,8.10;3,344.16,489.56,81.57,8.10">Stanford Digital Library Technologies Project</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="3,313.41,499.64,206.84,8.10;3,344.16,509.78,164.73,8.10;3,344.16,519.86,187.50,8.10;3,344.16,529.94,31.55,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="3,402.65,499.64,117.60,8.10;3,344.16,509.78,164.73,8.10;3,344.16,519.86,62.87,8.10">On Efficiency Optimization and Effectiveness Evaluation of Search Engine Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<publisher>PhD</publisher>
			<biblScope unit="page">106</biblScope>
		</imprint>
		<respStmt>
			<orgName>Peking University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="3,313.08,540.02,211.94,8.10;3,344.16,550.16,189.21,8.10;3,344.16,560.24,178.07,8.10;3,309.12,570.32,169.85,8.10;3,344.16,580.40,107.20,8.10" xml:id="b4">
	<analytic>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Salton</surname></persName>
		</author>
		<ptr target="http://www.tianwang.com" />
	</analytic>
	<monogr>
		<title level="m" coord="3,403.02,540.02,122.00,8.10;3,344.16,550.16,185.07,8.10">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall Inc</publisher>
			<date type="published" when="1971">1971. 1971</date>
		</imprint>
	</monogr>
	<note>Tianwang,2005</note>
</biblStruct>

<biblStruct coords="3,313.50,590.54,118.35,8.10;3,449.14,590.54,69.28,8.10;3,344.16,600.62,97.78,8.10;3,344.16,610.70,162.27,8.10" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><surname>Tianwangstorageformat</surname></persName>
		</author>
		<ptr target="http://net.pku.edu.cn/~webg/src/twformat/" />
		<title level="m" coord="3,449.14,590.54,69.28,8.10;3,344.16,600.62,93.41,8.10">Tianwang storage format of raw web pages</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
