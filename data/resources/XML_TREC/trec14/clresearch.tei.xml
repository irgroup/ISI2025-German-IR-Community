<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.84,78.96,363.61,9.28">Exploring Document Content with XML to Answer Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,253.44,108.00,105.71,8.59"><forename type="first">Kenneth</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
							<email>ken@clres.com</email>
							<affiliation key="aff0">
								<orgName type="department">CL Research</orgName>
								<address>
									<addrLine>9208 Gue Road Damascus</addrLine>
									<postCode>20872</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.84,78.96,363.61,9.28">Exploring Document Content with XML to Answer Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9389EBFCEC8474E7918FE679783C873D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CL Research participated in the question answering track in TREC 2004, submitting runs for the main task, the document relevance task, and the relationship task. The tasks were performed using the Knowledge Management System (KMS), which provides a single interface for question answering, text summarization, information extraction, and document exploration. These tasks are based on creating and exploiting an XML representation of the texts in the AQUAINT collection. Question answering is performed directly within KMS, which answers questions either from the collection or from the Internet projected back onto the collection. For the main task, we submitted one run and our average per-series score was 0.136, with scores of 0.180 for factoid questions, 0.026 for list questions, and 0.152 for "other" questions. For the document ranking task, the average precision was 0.2253 and the R-precision was 0.2405. For the relationship task, we submitted two runs, with scores of 0.276 and 0.216, the first run was the best score on this task. We describe the overall architecture of KMS and how it permits examination of the question-answering task and strategies within TREC, but also in a real-world application in the bioterrorism domain. We also raise some issues concerning the judgments used for evaluating TREC results and their possible relevance in a wider context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In TREC 2002, CL Research examined the potential of using XML-tagged documents for question answering <ref type="bibr" coords="1,167.70,434.64,86.36,8.59" target="#b0">(Litkowski, 2003)</ref> and showed that hand-developed XPath expressions could obtain extremely good results when compared with the best sytems. In TREC 2003 <ref type="bibr" coords="1,484.38,448.80,55.12,8.59;1,72.00,462.96,25.90,8.59">(Litkowski, 2004)</ref>, initial efforts at the automatic creation of XPath expressions achieved very limited results. In TREC 2004 <ref type="bibr" coords="1,132.00,477.12,83.55,8.59" target="#b3">(Litkowski, 2005)</ref>, significantly better results were obtained as core XML functionality was implemented. CL Research's participation in TREC question-answering is rooted in the Knowledge Management System (KMS), which provides an integrated framework for question answering, text summarization, information extraction, and document exploration. In 2005, KMS was used in a demonstration project of question-answering and summarization in the biomedical domain, allowing the examination of various strategies for obtaining answers and document content in a wider range of non-factoid question types. This project provided some background for KMS modifications that could be employed in TREC question-answering. This paper describes extensions to the KMS architecture and how they were used in TREC 2005, particularly for the relationship task.</p><p>Section 2 presents the TREC QA task descriptions. Section 3 describes the KMS, specifically components for processing texts and for performing particular NLP tasks. Section 4 provides our question answering results, particularly our experience in handling different types of questions, our performance in the document ranking task, and our perspectives in framing the relationship task (noting its similarities to answering "other" questions in the main task and to the Document Understanding Conference's topic descriptions). Section 5 presents our overall summary and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Description</head><p>The TREC 2005 QA used the AQUAINT Corpus of English News Text on two CD-ROMs, about one million newswire documents from the Associated Press Newswire, New York Times Newswire, and Xinhua News Agency. These documents were stored with SGML formatting tags <ref type="bibr" coords="2,72.00,150.96,82.48,8.59">(XML compliant)</ref>.</p><p>For the main task of the QA track, participants were provided with 75 targets, primarily names of people, groups, organizations, and events, viewed as entities for which definitional information was to be assembled. For each target, a few factual questions were posed, totaling 362 factoid questions for the 75 targets (e.g., for the target event "Plane clips cable wires in Italian resort", two factoid questions were "When did the accident occur?" and "How many people were killed?"). One or two list questions for each target were also posed for most of the targets (e.g., "Who were on-ground witnesses to the accident?"); there were 93 list questions. Finally, for each target, "other" information was to be provided, simulating an attempt to "define" the target. Each target was used as a search query against the AQUAINT corpus. NIST provided the full text of the top 50 documents, along with a list of the top 1000 documents.</p><p>Participants were required to answer the 362 factoid questions with a single exact answer, containing no extraneous information and supported by a document in the corpus. A valid answer could be NIL, indicating that there was no answer in the document set; NIST included 17 questions for which no answer exists in the collection. For these factoid questions, NIST evaluators judged whether an answer was correct, inexact, unsupported, or incorrect. The submissions were then scored as percent of correct answers. For the list questions, participants returned a set of answers (e.g., a list of witnesses); submissions were given F-scores, measuring recall of the possible set of answers and the precision of the answers returned. For the "other" questions, participants provided a set of answers. These answer sets were also scored with an F-score, measuring whether the answer set contained certain "vital" information and how efficiently peripheral information was captured (based on answer lengths).</p><p>Participants in the main task were also required to participate in the document-ranking task by submitting up to 1000 documents, ordered by score. Instead of providing an exact answer, participants were required to submit only the identifier of the document deemed to contain an answer. Document ranks were to be provided for 50 questions, with at least one document for each question. Scoring for this task used standard measures of recall (how many of the relevant documents were retrieved) and precision (how many of those retrieved were actually relevant). Summary measures are the average precision for all relevant documents and R-precision, the precision after R documents have been retrieved, where R is the number of relevant documents for the question.</p><p>For the relationship task, participants were provided with TREC-like topic statements to set a context, where the topic was specific about the type of relationship being sought (generally, the ability of one entity to influence another, including both the means to influence and the motivation for doing so). The topic ended with a question that is either a yes/no question, which is to be understood as a request for evidence supporting the answer, or a request for the evidence itself. The system response is a set of information nuggets that provides evidence for the answer. An example is shown in Figure <ref type="figure" coords="2,159.36,660.72,6.00,8.59" target="#fig_0">1</ref> (along with a comparable topic used in DUC 2005). Answers are scored for the relationship task in the same manner as the for the "other" questions of the main task.</p><p>CL Research submitted one run for the main and document-ranking tasks and two runs for the relationship task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship Topic 11:</head><p>The analyst is interested in Argentina's intentions in the Falkland Islands. Specifically, the analyst wants to know of any ongoing or planned talks between Argentina and Great Britain over the island's future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC d324e:</head><p>How have relations between Argentina and Great Britain developed since the 1982 war over the Falkland Islands? Have diplomatic, economic, and military relations been restored? Do differences remain over the status of the Falkland Islands? The Knowledge Management System</p><p>The CL Research KMS is a graphical interface that enables users to create repositories of files (of several file types) and to perform a variety of tasks against the files. The tasks include question answering, summarization, information extraction, document exploration, semantic category analysis, and ontology creation. The text portions of files (selected according to DTD elements) are processed into an XML representation; each task is then performed with an XML-based analysis of the texts. KMS also includes modules to perform web-based question answering (acting as a wrapper to Google) by reformulating questions into canonical forms and to search a Lucene index of document repositories by reformulating questions into a boolean search expression. These modules can be used to obtain answers to questions and to project those answers back onto document repositories, in this case, the AQUAINT collection.</p><p>KMS uses lexical resources as an integral component in performing the various tasks. Specifically, KMS employs dictionaries developed using its DIMAP dictionary maintenance programs, available for rapid lookup of lexical items. CL Research has created DIMAP dictionaries for a machine-readable version of the Oxford Dictionary of English, WordNet, the Unified Medical Language System (UMLS) Specialist Lexicon (which provides a considerable amount of syntactic information about general, non-medical lexical items), The Macquarie Thesaurus, and specialized verb and preposition dictionaries. These lexical resources are used seamlessly in a variety of ways in performing various tasks, described in more detail below.</p><p>KMS consists of a large number of modules. These modules are also used in two additional programs that perform more specialized processing. The DIMAP Text Parser uses the parsing and XML-generation components for background processing of large numbers of texts, such as the document sets for the QA tasks. The XML Analyzer, primarily used to enable detailed examination of XML versions of the documents for diagnostic purposes, also includes special processing routines to select elements from XML files for further processing (similar to what may be accomplished with XSLT transformations).</p><p>When performing a task with KMS, results are maintained in XML representations. For example, all answers to questions are retained and saved answers can be viewed without going through question answering again. Answers include the file they came from, the document within the file, the score, an exact answer, and t he sentence from which they were extracted. (During development, as question answering routines are modified, new answer sets replace existing answer sets.) Typically, the number of answers to a question is very large, although not infrequently, no answers or only one answer is returned. For TREC tasks, these answer files are processed with Perl scripts to create the TREC submissions following the required format. For the main task, only the first answer is used, and a NIL answer is returned when no answers have been generated. For the list and "other" questions of the main task, the maximum number of answers is limited to 25 and 20, respectively. For the document-ranking task, all answers (up to a maximum of 1000) were returned with their scores. For the relationship task, up to 40 answers were returned.</p><p>Improving the performance of KMS in its various tasks requires improved characterization of the texts, development of appropriate strategies for performing the tasks, and the use of efficient mechanisms for evaluating performance. XML provides a convenient formalism for representation and analysis, but this approach involves frequent reformulations to capture and recognize large numbers of linguistic phenomena. Improvements come from first dealing with low-level phenomena, sometimes idiosyncratically, and then, as patterns emerge, from generalizing over several phenomena. In general, improved characterization of linguistic phenomena leads to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Changes in Text Processing and XML Analyses</head><p>As described for TREC 2004 (see <ref type="bibr" coords="4,272.75,490.80,78.43,8.59" target="#b3">Litkowski, 2005</ref> and early references for greater detail), modifications to KMS are continually being made in virtually all supporting modules. Changes are made to the parser in an attempt to ensure improved parse trees. During the last year, a significant change has been made in the dynamic creation of dictionary entries used in parsing. The parser has a built-in general-purpose dictionary. Procedures were developed to make use of the UMLS Specialist Lexicon of biomedical terminology in a project where documents pertaining to a biopathogen were being processed. Through the use of this lexicon, as well as WordNet, it was possible to create entries for phrases so that they could be recognized as single units in processing texts. Many changes were also made to the parse tree analyzer that identifies important discourse constituents (sentences and clauses, discourse entities, verbs and prepositions) and creates an XMLtagged version of the document. It is difficult to assess the benefits of changes to these major components of KMS. In evaluating improvements in question-answering performance, only a small percentage of failures are usually attributed to failures in these modules. Overall, the changes in these modules result in ever-larger size of the XML representations of the texts; the size of the XML files is roughly 10 times the size of the original documents, up from 6.7 during TREC 2003 and 10 percent larger than for TREC 2004. Questions in the QA track are processed using the same text processing mechanisms; their XML representations are used as the basis for answering the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Changes to KMS Question Answering Routines</head><p>In reporting on KMS for TREC 2004, we described in detail the major types of functions employed: XML, linguistic, dictionary, summarization, and miscellaneous string and pattern matching. In each category, the number of such functions has increased during the past year, particularly in support of other functionality in KMS. Last year, we also described our procedures for using past results as the basis for making changes in handling questions of particular types. We have continued to follow those procedures.</p><p>As described in previous years, KMS question-answering is based on identifying the question type (including modifying a question such as "What year" into a When question) and then using idiosyncratic routines to build an XPath expression for each question type. The XPath expression consists of two components: one part to retrieve sentences satisfying boolean-style conditions and the other part to focus on a particular element of the retrieved sentence. The focal element is either a potential answer or a pivot that can be used to examine the surrounding context for the answer. Typically, the specifications in the XPath expression are very inclusive to permit a large set of potential answers. The questions-specific routines for examining candidate answers scores the answer type and the surrounding context to exclude most candidates and to rank the remaining answers. KMS returns both an exact answer and the sentence from which it came for the convenience of the user.</p><p>As demonstrated in TREC 2002, it is possible to develop XPath expressions by hand to obtain virtually all of the correct and exact answers. However, the automatic creation of XPath expressions is considerably more difficult and time-consuming. CL Research has used many techniques in the early TREC QA tasks when the primary mechanism was the use of database technology for storing semantic relation triples. Incorporation of these techniques into an XML-based approach is not yet complete. During preparations for TREC 2005, we were able to work in detail with only When, Where, and HowMany question types (slightly less than 40 percent of this year's factoid questions).</p><p>Some changes involved implementation of simple pattern recognition procedures, such as the attachment of a year to a month and day or the attachment of a state or country to a city name. Other changes involved the addition of improved and more efficient methods for manipulating and querying the XML representation. The major qualitative shift involved the representation and exploitation of the "context" of a question, that is, the text surrounding the specification in the question of the focal element or answer type. Several layers of contextual representation have been developed (such as whole noun phrases, adjective and noun constituents, and prepositional relations); these representations were variously exploited in answering different question types.</p><p>As indicated above, KMS includes a component that uses a Google search to answer a question. As with answers against a repository, web answers are maintained in their own file, in this case showing the web address of the document answering the question. KMS can use the TREC question set and have them submitted to Google, or a user can pose any question and the question is saved in a repository of questions.</p><p>Answering a question using Google (or any web-based search engine) goes through a slightly different strategy than against a repository. The main difference is how the question is analyzed and formulated for submission to Google. Given the breadth of the Internet, there is a strong likelihood that a question can be answered using a canonical form. For example, to the "When was John Glenn born?", it is likely that "John Glenn was born in" will exist as an exact phrase and that a search for the entire phrase as given will return documents where the displayed Google results will contain the answer.</p><p>Each question must be reformulated into an exact query. KMS analyzes the question type and its constituent terms to determine what is likely to be the best form. For the most part, the key transformation involves the placement, number, and tense of the verb. For example, "How did James Dean die?" has to be transformed to "James Dean died". KMS uses the UMLS lexicon for this purpose, since it contains information about the "regularity" of a verb form and how it's various tenses are formed. In its search results, Google displays a snippet that usually consists of one or two sentences, along with some additional partial sentences. KMS examines the snippet and extracts the whole sentence containing the exact phrase in the query. This single sentence is then treated as a document and is parsed and processed into an XML representation. Since Google usually returns 10 hits per page, the corpus to answer a question usually consists of 10 single-sentence documents. After extracting these sentences, the usual KMS mechanisms are employed to analyze the questions and extract the answers, with results displayed according to the score for each sentence. When the Google answer is reliably extracted (and this occurred for 72 of the 362 questions), the answer is projected back onto the document collection and is used to "force" an answer, which may or may not be found in the repository.</p><p>When a projected answer is not found in the document set that has been used for the question (the top 50 documents provided by NIST), the major terms in the question are combined with the projected answer and the AQUAINT collection is searched using Lucene with this boolean expression to obtain a maximum of 5 more documents, which must not be in the original set. These 5 documents are then processed in the usual way and another attempt is made to find the projected answer. When found, this is then the answer that is used in the TREC submission. This method only obtained 9 additional answers in TREC 2005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answering Relationship Questions</head><p>As shown in Figure <ref type="figure" coords="6,199.88,490.80,4.50,8.59" target="#fig_0">1</ref>, questions for the relationship task were posed in terms describing what information was desired by an analyst. Figure <ref type="figure" coords="6,285.23,504.96,6.00,8.59" target="#fig_0">1</ref> also showed the topic description used in DUC 2005 for one topic for which a summary was to be created. The similarity between the two is a strong suggestion that the lines between summarization and relationship questions is somewhat blurred. In addition, as specified in the task description, answers to relationship questions were to be evaluated in the same way as the "other" questions in the main task. This suggests that there is also a close tie between the "other" questions and the relationship questions. These observations influenced the strategy we used in responding to the relationship task. As will be discussed below in reporting on our results for this task, this task raised the issue of how all the different question types might fit within a larger context of question-answering strategies to server user needs.</p><p>Unlike the main task, no top documents were provided for the relationship task. Since CL Research has little expertise in indexing and search technology, a somewhat truncated approach was used. For the relationship subtask, we experimented with two modes of operation, both of which constitute a truncated version of the KMS document exploration functionality (which is specifically designed to examine relationships). Each relationship topic was reformulated into a simple Lucene boolean query to retrieve no more than 25 documents from the collection. In creating the search query, only content words from the topic statement were used, i.e., excluding terms like "the analyst is interested in ...," none of which would generally be included in an actual search query. In several cases, the initial search query included too many AND expressions and returned no documents, in which case the query was scaled back until it returned some documents. The goal for this phase was to obtain 25 documents. The topic was then reformulated into a single question which attempted to capture the essence of what the analyst was seeking. Figure <ref type="figure" coords="7,356.16,388.80,6.00,8.59" target="#fig_1">3</ref> shows the Lucene search queries and the associated question for seven of the relationship questions.</p><p>We did not attempt to answer the 25 relationship questions directly using the fact-based routines in KMS. Instead, we "forced" KMS to answer the questions as if they were definition questions. In KMS, special routines are used to answer the "other" portion of the main task, focusing either on Who or What as an item to be defined. These routines are specifically designed to look for definition patterns in text, such as copular verbs, appositives, and parenthetical expressions. Points are given for the presence of these constructs, matches with words defining terms as available in machine-readable dictionaries or WordNet glosses, and the presence of the term to be defined. Two runs were made. For the first run, definition-style answers were obtained with KMS definition pattern-matching routines as described. For the second run, scoring boosts to "definition" sentences were given based on the "context" analysis routines, where the question as posed was analyzed into qualifiers identifying noun, adjectives, phrases, and semantic relationships (as indicated by prepositions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TREC 2005 Question-Answering Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Task</head><p>CL Research submitted one run for the main task of the TREC 2005 question-answering track. All answers were generated in KMS. The question set provided by NIST was first converted into a form for parsing and processing into an XML representation. The top 50 documents for the 75 targets were also processing into an XML representation. The questions are displayed in KMS, each with a checkbox used to select which questions are to be answered. All questions were selected and answered automatically, generating an answer file as described above. A Perl script was then used to create an answer file in the format required for a QA track submission.</p><p>Conversion of the NIST question set involved minor formatting changes (using a different tag set) and a more considerable anaphora replacement algorithm. For TREC 2005, this was performed in a Perl script (the same as used for TREC 2004), rather than attempting to use the potential capabilities of KMS for capturing anaphoric references. The Perl script identified all referring expressions in the questions, including anaphors such as her, it, and their and definite noun phrases such as "the organization." The script kept track of the type of anaphors so that the "other" question could be converted to either "Who is" or "What is". The revised question set was added to KMS as a question list from which question selection could be made.</p><p>Table <ref type="table" coords="8,137.04,233.04,6.00,8.59" target="#tab_0">1</ref> shows the summary score for the CL Research QA run, along with the median score for all participating teams, broken down by major question type. The table also shows an unofficial score based on CL Research assessment of our answers in conjunction with the factoid patterns and a subjective assessment of the results for the list and "other" questions. The table also shows an adjusted score for the factoid component based on a determination of whether a document with a known answer was present in the collection used for answering the question; this row shows the score that would result based only on the question-answering performance and not the retrieval performance. In our submission for the main task, 95 NIL answers were submitted; our precision on these answers was only 5/95 or 0.053, while our recall on NIL answers was 5/17 or 0.294. We expect that this low precision was due in large part to the unavailability of the answers in the collection that was used for each question. As can be seen, CL Research scored somewhat higher than the median for the factoid component and this was somewhat higher than our performance last year. In reviewing the official results, however, we observed a very high number of "inexact" judgments and looked at these answers more closely in relation to the answer patterns. In at least 17 cases (0.049), we were perplexed by the judgments. In some cases, our answers were identical to what had been scored as correct for other submissions. In other cases, e.g., "How many openings ..." with our answer of "17 openings", the inexact judgment seems to distort what KMS was actually returning. We include our assessments for the list and "other" questions without yet having assessed these answers in detail, based on the experience with the factoid judgments and with our scores for these components last year and the results of our performance on the relationship task (described below) which seem somewhat at odds with the official score.</p><p>Tables <ref type="table" coords="8,141.84,632.88,6.00,8.59">2</ref> shows the unofficial and adjusted scores for the factoid questions by question type. These scores reflect the focus described above for the question types we were able to examine in detail. In particular, these detailed results show that we have considerably improved our performance on When, Where, and HowMany questions, although only a little more effort was spent on HowMany questions. This table also shows where considerably more work is required. In particular, the poor performance on the WhatIs and WhatNP questions reflects, in considerable measure, the fact that we have not yet incorporated the necessary routines for taking advantage of WordNet hierarchies and thesaurus groupings into the XML processing that we used in earlier years. The effect of this poor performance also carries over to our performance on the list questions, all of which essentially require the identification of a hierarchical relationship. As indicated above, the main difference between our official runs was the use of strict definition question criteria (clr05r2) or the expansion to include consideration of contextual information (clr05r1). This difference shows up in the number of answers that are generated. As can be seen, use of contextual information returned a considerably larger number of sentences for consideration. This suggests that casting a wider net (i.e., essentially using fewer terms for retrieval), combined with useful criteria for ranking the sentences was successful. Even though a higher recall was achieved and more sentences submitted (with a maximum of 40 sentences for any individual question), the precision was not compromised.</p><p>To examine the question of expanding the document collection, we examined how much was lost by our "naive" boolean searches. By examining the judgment set for the all runs submitted in the relationship task, we were able to determine that 105 documents were identified by other teams that were not included in the maximum of 25 documents we included in the set we processed. This is an average of four documents per question. When we added these documents to our collection, resulting in the unofficial run clr05r3 shown in Table <ref type="table" coords="11,286.13,204.72,4.50,8.59" target="#tab_1">4</ref>, an additional 600 sentences were generated in our answer set. However, this resulted in only 3 additional submissions under our criterion of a maximum of 40 answers per question. As shown in Table <ref type="table" coords="11,290.69,233.04,4.36,8.59" target="#tab_1">4</ref>, adding these documents did not improve our score, but rather resulted in a significant drop in recall and a smaller drop in precision, with an overall significant drop in F-score.</p><p>Next, we identified documents based on official nuggets which no submission found. To do this, we used keywords in the nugget with Lucene to identify documents in which the answers might be found. We were able to identify 40 documents that contained a sentence that we judged to be an appropriate answer for the nugget. However, after completing this effort, we found that 15 of the documents were already in our collection, but we had not returned the sentence. The remaining 25 new documents thus added only one additional document per question to our collection (clr05r4). Adding these documents again did not improve our score, but rather led to another small drop in recall, precision, and F-score.</p><p>In both clr05r3 and clr05r4, the effect of adding documents and retrieving more candidate sentences resulted in bumping down good answers to lower positions in the ranking and had the effect of removing some entirely, thus explaining the lower scores. This results suggests once again that better methods for assessing individual sentences is the key to improving our results in this task.</p><p>To score these two unofficial runs automatically, we made use of two criteria. The first criterion was exact: an answer that had been judged correct officially was merely moved to a different position in the ranking. The second criterion was more subjective. We attempted to measure the overlap between the words in the official nugget and the candidate answer. We eliminated stop words from this assessment and then used the criterion that 40 percent of the words had to be present in a candidate answer. The 40 percent criterion was reached after some experimentation (higher percentages eliminated many correct answers, while smaller percentages returned too many). This 40 percent criterion may have some utility in assessing answers for the "other" questions on the main task.</p><p>Table <ref type="table" coords="11,138.24,572.88,6.00,8.59" target="#tab_2">5</ref> shows the effect of the number of answers submitted. To obtain the score for 39 answers, we first removed the last answer and rescored the results based on the official judgment. We continued in this way down to our results with a maximum of one answer submitted. These results show that our selection of 40 answers was quite fortuitous and resulted in almost the maximum score that our system could have obtained. Thus, any further improvements in our system would have to result from a better selection and ranking of candidate sentences. The above analyses suggest that we can obtain further improvements in our system. For this task, we can expect that summarization techniques for measuring overlap or redundancy may be useful. In addition, improvements may be made in better evaluations of answers, perhaps attempting to tailor the system to spheres of influence. It is also possible that better assessment of paraphrases and recognition of hyponymic sentences would benefit the task. However, it seems that completeness is lost and making such improvements is very difficult. This raises the question of whether alternative methods might be useful for answering relationship questions.</p><p>In a parallel project in the bioterrorism domain, we experimented with the use of information extraction techniques to identify pertinent pieces of information. Within the KMS framework, we found that it was relatively easier to construct XPath expressions (using string, syntactic, and semantic criteria) to identify key items (such as people, organizations, and countries) and their relations. An important aspect of this search approach is that results are not affected by growing documents sets where it is necessary to rank candidate answers.</p><p>To examine this approach in the relationship task, using the first question about the al-Qaeda network, we used KMS to identify all mentions of "persons". This resulted in 2967 hits in KMS (easily scrollable). This search identified many people that were part of the al-Qaeda network, but that had not been identified in the official nugget set. These results also identified many groups that were part of al-Qaeda, none of which were included in the official nugget set, even though this was a significant part of the topic description. Once having identified a list of people, it was then possible to focus a search in more detail for specific individuals. Thus, we were able to identify 40 sentences that referred to a specific person, including 59 anaphoric references to this individual, in 11 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary and Conclusions</head><p>Our participation in TREC 2005 question-answering track, particularly in the relationship task, raises several issues. Although the track has maintained a focus on answering factoid questions, the attempt to broaden the scope, first with the "other" questions and now with the relationship questions, seems to be moving into areas where it has considerable overlap with other NLP technologies. This is most evident with the similarity between the questions in the relationship task and the topic descriptions used in the Document Understanding Conference for producing topic-based summaries.</p><p>The similarities among the tasks (factoid questions, list questions, "other" questions, and relationship questions) suggests the need for a still broader conceptualization of the questionanswering track. Suggestions have been made that all of the information included in the answers to these various types of questions might fit within a templatized notion of the question-answering task. In this conceptualization, it may be desirable that the main task should be the automatic development and completion of templates pertaining to a target of interest.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,178.80,194.16,257.58,8.59"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Relationship Topic and DUC 2005 Topic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,142.80,290.40,327.48,8.59"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Relationship Lucene Queries and Definition Questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,95.28,402.54,416.09,54.89"><head>Table 1 . Summary Scores for the Main Task (with median)</head><label>1</label><figDesc></figDesc><table coords="8,95.28,414.54,416.09,42.89"><row><cell></cell><cell>Factoid (0.152)</cell><cell>"Other" (0.156)</cell><cell>List (0.053)</cell><cell>Overall</cell></row><row><cell>Official</cell><cell>0.180</cell><cell>0.152</cell><cell>0.026</cell><cell>0.135</cell></row><row><cell>Unofficial</cell><cell>0.282</cell><cell>0.260</cell><cell>0.049</cell><cell>0.218</cell></row><row><cell>Adjusted</cell><cell>0.374</cell><cell>0.260</cell><cell>0.049</cell><cell>0.264</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,152.16,550.14,304.58,90.89"><head>Table 4 . TREC-14 Relationship Task Summary</head><label>4</label><figDesc></figDesc><table coords="10,152.16,562.14,304.58,78.89"><row><cell></cell><cell cols="2">Official</cell><cell cols="2">Unofficial</cell></row><row><cell>Measure</cell><cell>clr05r1</cell><cell>clr05r2</cell><cell>clr05r3</cell><cell>ckr05r4</cell></row><row><cell>Answers generated</cell><cell>2849</cell><cell>801</cell><cell>3455</cell><cell>3549</cell></row><row><cell>Answers submitted</cell><cell>948</cell><cell>638</cell><cell>951</cell><cell>951</cell></row><row><cell>Recall</cell><cell>0.457</cell><cell>0.345</cell><cell>0.414</cell><cell>0.407</cell></row><row><cell>Precision</cell><cell>0.074</cell><cell>0.074</cell><cell>0.069</cell><cell>0.068</cell></row><row><cell>F-Score</cell><cell>0.276</cell><cell>0.216</cell><cell>0.244</cell><cell>0.241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,180.00,76.86,251.89,126.89"><head>Table 5 . Effect of Answers Submitted in Relationship Task</head><label>5</label><figDesc></figDesc><table coords="12,181.20,88.86,239.90,114.89"><row><cell cols="2">clr05r1</cell><cell>clr05r2</cell><cell></cell></row><row><cell>Number of</cell><cell></cell><cell>Number of</cell><cell></cell></row><row><cell>Answers</cell><cell>F-Score</cell><cell>Answers</cell><cell>clr05r3</cell></row><row><cell>40</cell><cell>0.276</cell><cell>40</cell><cell>0.216</cell></row><row><cell>39</cell><cell>0.278</cell><cell>39</cell><cell>0.216</cell></row><row><cell>38</cell><cell>0.281</cell><cell>38</cell><cell>0.216</cell></row><row><cell>37</cell><cell>0.284</cell><cell>37</cell><cell>0.217</cell></row><row><cell>36</cell><cell>0.276</cell><cell>36</cell><cell>0.213</cell></row><row><cell>...</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>0.059</cell><cell>1</cell><cell>0.007</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document Ranking Task</head><p>For the document ranking task, CL Research's average precision was 0.2253 and R-precision was 0.2405. These results were slightly below the results shown for the best runs from the top 13 groups (0.2445 and 0.2596 for average precision and R-precision, respectively). These results are consistent with our position for the main task. While document ranking is not a primary criterion that CL Research will use for assessing question-answering performance, this task is useful primarily for identifying relevant documents and will be used in diagnosing problems with our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relationship Task</head><p>For the relationship task, we submitted two runs (clr05r1 and clr05r2), with scores of 0.276 and 0.216. The first run, which used the second method described above, was the best score on this task; the second run was the 4 th best score among 11 submissions. The results by question number are shown in Table <ref type="table" coords="9,166.34,594.24,4.38,8.59">3</ref>. The table shows that KMS performed better with each method for different questions, suggesting that a mixture of strategies is appropriate. The table also highlights questions for which one of the runs achieved the best score. For clr05r1, the best score was achieved on 6 of the questions; for clr05r2, the best score was achieved on 2 of the questions.</p><p>The table also shows how many documents were present in the set that was used to respond to the relationship questions, identified as either present or not present. These columns are based on the judgment set for the relationship questions from all participating teams. While this judgment set is known to be an incomplete representation, since not all nuggets were found, it does suggest that CL Research's retrieval performance also had a significant effect on the overall score. To examine our performance in this task in more detail, we asked several questions: • What is the effect of qualifiers in answering relationship questions? • What was the effect of expanding the document set? • What was the effect of changing the number of answers submitted? • Are there other ways to obtain answers to these questions? Table <ref type="table" coords="10,102.00,508.08,6.00,8.59">4</ref> summarizes our official and unofficial results, identifying measures of performance for the two official runs (clr05r1 and clr05r2) and two unofficial runs (clr05r3 and clr05r4).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,108.00,162.24,431.93,8.59;13,72.00,176.40,467.90,8.59;13,72.00,190.56,279.85,8.59" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,232.36,162.24,256.08,8.59">Question Answering Using XML-Tagged Documents</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,242.88,176.40,193.48,8.59">The Eleventh Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2003. 2002</date>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.00,204.72,432.29,8.59" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="13,227.04,204.72,282.10,8.59">Use of Metadata for Question Answering and Novelty Tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In E.</note>
</biblStruct>

<biblStruct coords="13,72.00,218.88,467.90,8.59;13,72.00,233.04,279.85,8.59" xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="13,256.08,218.88,248.86,8.59">The Twelfth Text Retrieval Conference (TREC 2003)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,108.00,247.20,432.22,8.59;13,72.00,261.36,467.83,8.59;13,72.00,275.52,467.93,8.59" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,228.00,247.20,312.22,8.59;13,72.00,261.36,93.26,8.59">Evolving XML and Dictionary Strategies for Question Answering and Novelty Tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,406.56,261.36,133.27,8.59;13,72.00,275.52,55.32,8.59">The Twelfth Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaithersburg, MD., TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2005. 2004. 2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,72.00,289.68,397.12,8.59" xml:id="b4">
	<monogr>
		<ptr target="http://trec.nist.gov/pubs/trec13/t13_proceedings.html" />
		<title level="m" coord="13,72.00,289.68,74.39,8.59">Proceedings CD</title>
		<meeting>CD</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
