<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.80,113.97,291.62,17.97">FDUQA on TREC2005 QA Track</title>
				<funder ref="#_2xfes2h">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.46,139.97,34.16,9.45"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<email>ldwu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.53,139.97,69.97,9.45"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.86,139.97,55.48,9.45"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<email>zhouyaqian@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.41,139.97,69.08,9.45"><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
							<email>zs_zhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.64,139.97,34.10,9.45"><forename type="first">Fen</forename><surname>Lin</surname></persName>
							<email>fenglin@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.80,113.97,291.62,17.97">FDUQA on TREC2005 QA Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">87C81BA5D0EEB1CE8937D6D4B32FC0BC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this year's QA Track, we participant in the main and document ranking task and do not take part in the relation task. We put the most effort in factoid and definition questions, and very little on list questions and document ranking task.</p><p>For factoid questions, we use three QA systems: system 1, system 2 and system 3. System 1 is very similar to our last year's system <ref type="bibr" coords="1,341.22,301.79,80.72,9.45" target="#b0">[Wu et al, 2004]</ref> except two main modifications. One is adding an answer validation-feedback scheme. The other is an improved answer projection module. System 2 is a classic QA system that does not use Web. System 3 is a pattern-based system that we used in TREC 2002 evaluation. The main contribution for factoid question is two improvements for Web-based QA system and the system combination.</p><p>For definition question, we attempt to utilize both the existing definitions in the Web knowledge bases and the automatically generated structured patterns. Effective methods are adopted to make full use of these resources, and they promise high quality response to definition questions.</p><p>For list questions, we use a pattern-based method to find more answers other than those found in the processing of factoid question.</p><p>For document ranking task, we only collect the outputs from document searching or answer projection module.</p><p>In the following, Section 2, 3, 4 will describe our algorithms to factoid, list and definition questions separately. Section 5 will present our results in TREC 2005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Factoid Question</head><p>In order to answer factoid questions, we use three QA systems: system 1, system 2 and system 3. System 1 is very similar to our last year's system <ref type="bibr" coords="1,358.65,615.35,71.97,9.45" target="#b0">[Wu et al, 2004]</ref> except two main modifications. One is adding an answer validation-feedback scheme. The other is an improved answer projection module. System 2 is a classic QA system that does not use Web. System 3 is a pattern-based system that we used in TREC 2002 evaluation.</p><p>Table <ref type="table" coords="1,134.27,676.07,5.68,9.45">1</ref> illustrates the experimental results for these systems that test on TREC 2004's question set. FDUQA13 is the old system for TREC 2004, which is the baseline for system 1. The Combination represents the combined system of system 1, 2, 3. Obviously, the combined system achieves the best performance. System1 is best among the single systems and also better than it baseline, FDUQA13.</p><p>In the following sections, we will describe our best single system, the Web-base QA system and the combined methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Web-based system</head><p>Figure <ref type="figure" coords="2,139.42,342.95,5.68,9.45">1</ref> describes the process of our Web-based factoid question answering. This system bases on our last year's system. The main improvements include: adding an answer validation-feedback scheme, and improving the answer projection module. The next sections will describe the two improvements, and the other modules can be found in <ref type="bibr" coords="2,428.60,388.43,69.64,9.45" target="#b0">[Wu et al, 2004]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Answer Validation-Feedback scheme</head><p>In this year, we attempt to use the answer validation-Feedback scheme to improve the performance. The main idea is that the answer generation procedure terminates until the top ranked answer are very confident.</p><p>The Query Generation module generates a list of queries from strict to loose according to the question and target, and provides them to the Web Retrieval module one by one.</p><p>The Web Retrieval module uses the query to search the Web and return snippets to the answer generation module five by five, but no more than 200 snippets.</p><p>The Answer Generation module extracts the answer from the snippets and ranking the answers. The procedure is stop until the top 1 answer is confident.</p><p>Currently, an answer is considered as confident if the occurrence number of the answer string is not less than five.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Answer Projection</head><p>Answer projection is to find the support document(s) in certain corpus (AQUAINT for TREC task). It is a document retrieval problem and can also fit the demand of document ranking task of this year. Therefore, we do not specially develop a system for document ranking task.</p><p>For answer projection, the answer string is used as an additional key phrase, besides the queries of the question. In our system, the very strict query generation strategy is undertaken. There are four kinds of queries generated from question and target, as illustrated in following from strict to loose.</p><p>1. key phrases of question and target 2. key phrases of question and target except the verb phrases 3. key words (noun, adj, verb) of question and target 4. key words of question and target except the verb phrases There are two kinds of allowed distribution of the key phrases/words, as illustrated in following from strict to loose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">within three sentences 2. within a document</head><p>There are two kinds of methods to use the answer string, as illustrated in following from strict to loose.</p><p>1. add the answer phrase to the queries, and search the documents 2. only use queries generated from question and target, and use answer phrase to filter the documents</p><p>The ranking strategy is: the distribution of the key phrases/words is the first key; the query strictness of query generation is the second key; and the methods of using the answer string is the third.</p><p>The procedure of answer projection uses the combination of the three strategies by the order according to the ranking strategy, and is terminated when any support document is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Combination</head><p>The Web-based system is still underdeveloped, in order to capture the strongpoint of our currently developing and previous systems, six heuristic rules are applied to combine the results, as illustrated in following.</p><p>1. If the answer strings of system 1 and 2 are same, select the result of system2 2. else if the answer string of system 1 is NIL and system 2's is not NIL, and if the score of system 2 is larger than 0.8, select the result of system2 3. else if the answer string of system 1 is not NIL and system 2's is NIL, and if the score of system 1 is less than 0.5 or the Answer type belongs to the number class (except DAT), select the result of system2 4. if the answer string of system 1 is not NIL, the score of system 1 is less than that of system 2 and the answer type belongs to PRN , LCN or DAT, select the result of system2 5. if system 3 uses the strict rules to find the answer and the occurrence of answer string is not less than 5, and if the answer type is ABBR or the question is short and its interrogative word is when/where and the score of system 1 is less than 1.0, select the result of system3 6. if none of the above rules can be applied, select the result of system 1 These rules are validated by the experimental results on the data of TREC 2004. Rule 1 -4 means when system 1 is not very confident about the answer of a question, system 2's result is prefer. Rule 5 means when system 3 is very confident about certain types of questions, system 3's result is prefer. Rule 6 means if none of rule 1 -5 is work then use the result of system 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">List Question</head><p>We use patterns to extract answers for list question. The flow chart of FDUQA on list question is illustrated in figure 2. First, the question is approached by factoid question processing module (we use the Web-based factoid QA system as described in section 2), and top 10 candidate answers are used as seeds for list question processing module. Second, the seeds are put into the sentence corpus and new candidate answers are extracted by patterns. At last, candidate answers are filtered by answer filtering module and final answers are put out.</p><p>Figure <ref type="figure" coords="5,231.70,380.21,5.68,9.45">2</ref> Flow Chart of FDUQA on List Question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pattern matching</head><p>The answers of a list question always appear in a paratactic structure. We obtained these structures from the past TREC list questions, and built a pattern corpus consisting of these structures. Last year, we use target and each seed as query to search in AQUAINT corpus, and the results are collected as sentence corpus. In this year, all the answer sentences supporting the seeds in factoid question processing module are collected to build a temporary sentence corpus.</p><p>One example for pattern matching is as follows:</p><p>For question "What is OPEC countries", we first treat it as a factoid question and answer "Iraq" was found. Then we use "Iraq" as a seed and put it into sentence corpus. There is a sentence which supports "Iraq" in factoid question processing module: "OPEC members are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern matching Answer Filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seeds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate answers</head><p>Saudi Arabia , Iran , Kuwait , Qatar , United Arab Emirates , Algeria , Nigeria , Iraq , Libya , Ecuador , Gabon , Indonesia and Venezuela." Through pattern P4, "Saudi Arabia", "Iran" ,"Kuwait", "Qatar" , "United Arab Emirates" , "Algeria" , " Nigeria" , "Libya" , "Ecuador" , "Gabon" , "Indonesia" and "Venezuela" are extracted out as candidate answers. And all the candidate answers are put into answer validation modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Filtering</head><p>We find right answers from candidate answers after answer filtering module. Here are some rules for answer filtering:</p><p>Proper noun phrase filtering: The candidate answer which is not proper noun phrase will be filtered. Stop words filtering: A stop words list is used to filter the candidate answers.</p><p>The candidate answer which is the same as other seeds will be thrown off. At last, candidate answers are ranked by their frequencies and top ranked candidate answers will be put out as right answers.</p><p>The qualities of patterns and seed answers affect the performance of list question very much. Wrong seed answers will bring even worse results by patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Definition Question</head><p>In order to automatically identify definition sentences from a large collection of documents, we utilize both the existing definitions in the Web knowledge bases and the automatically generated structured patterns. Effective methods are adopted to make full use of these resources, and they promise high quality response to definition questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Overview</head><p>We adopt a general architecture for definition QA. The system consists of five modules: target classification, document processing, Web knowledge acquisition, structured pattern generation and definition extraction. The flow chart for definition question of FDUQA is in figure3.</p><p>First, a question target is input, and the target classification module identifies the target type based on a few heuristic rules. The question targets are classified into several types through this module, such as person, organization and other thing. This target type is used in the Web knowledge acquisition module <ref type="bibr" coords="6,279.12,670.91,95.03,9.45" target="#b3">[Zhang et al, 2005]</ref> to determine which kind of knowledge bases will be searched, and it is also used to determine which set of pattern will be adopted.</p><p>Second, the document processing module generates the candidate sentence set according to the target term. This module has three steps, document retrieval, relevant sentence extraction and redundancy removal. After these steps, we get the candidate sentence set of this target, which can be expressed as S A {A 1 , A 2 , … A k , …, A m }, where A k (k=1..m) is a candidate answer in the set and m is the total number of the candidate answers. Third, the Web knowledge acquisition module acquires the definitions of the target term from the Web knowledge base (KB). We search the definitions about the target from a number of online knowledge bases. These knowledge bases are the WordNet glosses and other online dictionaries such as the biography dictionary at www.encyclopedia.com. The definitions from them often supply knowledge that can be exploited directly and they are quite helpful to answering definition questions. We choose several authoritative KBs that cover different kinds of concept to achieve our goal. If we can find the definitions of question target from these sources, we use them to score the candidate sentences. (More detail can be found in <ref type="bibr" coords="7,98.19,610.31,69.81,9.45" target="#b0">[Wu et al, 2004]</ref>)</p><p>In very few situations, no definitions can be found from the Web KBs, we form a centroid (i.e. a vector of words with their frequency) of the candidate sentence set to score the candidate sentences. The assumption is that words co-occurring frequently with the target in the corpus are more important ones for answering the question.</p><p>Fourth, we automatically generate several sets of structured patterns based on the training set, and then we score the candidate sentences using these patterns. (The generation of the structured patterns will be described in Section 4.2) At last, the definition extraction module extracts the definition from the candidate sentence set based on the knowledge got from the Web knowledge base and the structured patterns. We will describe the detail of the definition extraction module in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Structured Pattern Generation</head><p>Since definitional patterns can filter out those statistically highly-ranked sentences that are not definitional, and bring those definition sentences that are written in certain styles for definitions but are not statistically significant into the answer set <ref type="bibr" coords="8,380.65,241.07,69.87,9.45" target="#b4">[Cui et al 2004]</ref>, we employ some automatically generated structured pattern to reinforce our Web knowledge based method.</p><p>We accumulate definition question-answer pairs from the all the submitted runs of the TREC12 and TREC13 QA tasks for use as training data. We generate a set of patterns for each kind of target (i.e. person, organization, and other thing) respectively.</p><p>For each kind of target, we firstly form a training set, which consists of the answer sentences to all the questions whose target term belong to this kind in the training corpus.</p><p>Secondly, in order to form general patterns, we substitute a general tag "&lt;TARGET&gt;" for those question targets in the answer sentences. We consider the context around the "&lt;TARGET&gt;", and the context is modeled as a window centered on "&lt;TARGET&gt;" according to the pre-defined size w. Thus we get fragments with size 2w+1 (w is set to 2 in our experiments) including the target term.</p><p>At last, we calculate the recurrence of these fragments in the training set, and the fragments with high occurrence are considered as the structured patterns. The frequency of the fragment in the training set is used as the weight of this pattern. A few top ranked patterns of the type "Person" are listed in Table <ref type="table" coords="8,262.09,483.83,4.24,9.45">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structured</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Definition Extraction</head><p>For each candidate sentence A i in the set S A , we calculate its importance Score i as follows:</p><p>1.</p><p>Calculate its similarity with the definition from Web KBs <ref type="bibr" coords="8,393.68,695.51,69.89,9.45" target="#b0">[Wu et al, 2004]</ref>, which is expressed as SimScore i ; 2.</p><p>Substitute a general tag "&lt;TARGET&gt;" for those question targets in A i ;</p><p>3.</p><p>Apply hard matching between A i and each structured patterns of this target type, and the pattern match score of A i , PatScore i is set to the weight of the pattern matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>( )</p><formula xml:id="formula_0" coords="9,143.46,161.40,246.38,12.33">1 = + • + • = β α PatScore β SimScore α score i i i</formula><p>. The weights α and β are fixed based on experiment, and they are set to 0.7 and 0.3 respectively in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Rank the sentences of set S A based on Score i (i=1..m), and the top ones are chosen as the definition of the target term. The results reveal that the Web knowledge bases and the structured patterns are effective resources to definition question answering, and the method presented gives an appropriate framework for answering definition question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We submitted two runs for the main task and document ranking task of TREC14 QA Track: FDUQA14A and FDUQA14B. In the two runs, the algorithms used to answer factoid questions are different. FDUQA14A is system 1, while FDUQA14B is the combination system, as described in section 2. The results of list questions in the runs are just the same. As to definition questions, difference between the two runs is FDUQA14B combines the structured patterns while FDUQA14A not. From this table, we can see that system combination can achieve better performance than the single Web-based system. The algorithm we use to answer definition questions is quite promising. The list questions are answered not so well.</p><p>Although FDUQA14B has a better performance than FDUQA14A in main task, FDUQA14B is not as good as FDUQA14A in document ranking task. The reason is that we do not try to find as many relevant documents as possible but only try to find the best supporting documents. This may affect the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,174.00,699.29,247.11,9.45"><head>Figure</head><label></label><figDesc>Figure 1 System Architecture for Web-base QA system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,137.95,481.43,185.36,9.45;5,107.82,496.55,399.92,9.45;5,128.28,511.67,157.90,9.45;5,107.82,526.91,188.82,9.45;5,107.82,542.09,219.23,9.45;5,107.82,557.27,186.84,9.45;5,107.82,572.45,399.92,9.45;5,87.42,587.57,420.27,9.45;5,87.42,602.75,44.23,9.45"><head></head><label></label><figDesc>Here are some examples of our patterns: P1. (?:including|include|included|includes|involve|involving|involves|involved) (NP like)? &lt;A&gt; , &lt;A&gt; and | or | as well as &lt;A&gt; P2. such as &lt;A&gt; ((,&lt;A&gt;)* and | or &lt;A&gt;)? P3. like &lt;A&gt;(,&lt;A&gt;)* (and | or | as well as &lt;A&gt;)? P4. (&lt;A&gt;,)* &lt;A&gt; (and|or|as well as) &lt;A&gt; These patterns are expressed in regular expression. Tag &lt;A&gt; here means the answer. Top 10 answers found by factoid question processing module are used as seeds in pattern matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,186.36,466.19,243.42,9.45"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Flow Chart for Definition Question of FDUQA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,132.00,453.95,326.86,181.71"><head>Table 3</head><label>3</label><figDesc>Performance of FDUQA Runs in TREC 2005</figDesc><table coords="9,132.00,453.95,326.86,166.10"><row><cell></cell><cell></cell><cell>FDUQA14A</cell><cell>FDUQA14B</cell></row><row><cell cols="2">Final Score</cell><cell>0.192</cell><cell>0.205</cell></row><row><cell></cell><cell>#correct</cell><cell>86</cell><cell>94</cell></row><row><cell></cell><cell>#unsupported</cell><cell>21</cell><cell>17</cell></row><row><cell>Factoid Question</cell><cell>#inexact</cell><cell>14</cell><cell>14</cell></row><row><cell></cell><cell>#wrong</cell><cell>241</cell><cell>237</cell></row><row><cell></cell><cell>Accuracy</cell><cell>0.238</cell><cell>0.260</cell></row><row><cell>List Question</cell><cell>Average F score</cell><cell>0.056</cell><cell>0.055</cell></row><row><cell>Definition Question</cell><cell>Average F score</cell><cell>0.231</cell><cell>0.232</cell></row><row><cell>Document Ranking</cell><cell>Average precision R-Precision</cell><cell>0.1602 0.1813</cell><cell>0.1435 0.1595</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">60435020</rs>. We are very thankful to <rs type="person">Ningyu Chen</rs>, <rs type="person">Feng Ji</rs>, <rs type="person">Xiaofeng Yuan</rs> and <rs type="person">Jian Huang</rs> for their contributions in our work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2xfes2h">
					<idno type="grant-number">60435020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,87.42,299.51,420.23,9.45;10,106.80,315.77,400.90,9.45;10,106.80,331.97,135.64,9.45" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,455.11,299.51,52.55,9.45;10,106.80,315.77,99.64,9.45">FDUQA on TREC2004 QA Track</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.04,315.77,270.45,9.45">Proceedings of the Thirteenth Text REtreival Conference</title>
		<meeting>the Thirteenth Text REtreival Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.42,348.29,420.29,9.45;10,106.26,364.55,316.63,9.45" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,404.78,348.29,102.93,9.45;10,106.26,364.55,37.57,9.45">FDUQA on TREC2003 QA Task</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongping</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lan</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.74,364.55,127.60,9.45">Proceedings of the TREC-12</title>
		<meeting>the TREC-12<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.42,380.75,420.30,9.45;10,106.26,397.01,401.44,9.45;10,106.26,413.27,137.88,9.45" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,473.48,380.75,34.24,9.45;10,106.26,397.01,242.62,9.45">FDU at TREC2002: Filtering, QA, Web and Video Tasks</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,361.14,397.01,140.88,9.45">Proceedings of the TREC-11</title>
		<meeting>the TREC-11<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.42,429.41,420.37,9.45;10,106.80,445.67,401.01,9.45;10,106.80,461.93,182.30,9.45" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,360.60,429.41,147.19,9.45;10,106.80,445.67,124.85,9.45">Answering Definition Questions using web knowledge base</title>
		<author>
			<persName coords=""><forename type="first">Zhushuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,254.83,445.67,252.98,9.45;10,106.80,461.93,150.98,9.45">Proceedings of the 2nd International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.42,478.13,420.36,9.45;10,106.80,494.39,401.00,9.45;10,106.80,510.65,201.56,9.45" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,346.58,478.13,161.19,9.45;10,106.80,494.39,217.58,9.45">A comparative Study o Sentence Retrieval for Definitional Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,349.87,494.39,157.93,9.45;10,106.80,510.65,170.10,9.45">Proceedings of the 27th Annual International ACM SIGIR Conference</title>
		<meeting>the 27th Annual International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
