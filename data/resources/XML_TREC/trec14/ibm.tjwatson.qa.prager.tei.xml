<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.30,87.88,263.35,16.65">IBM&apos;s PIQUANT II in TREC2005</title>
				<funder ref="#_YTCkMnk">
					<orgName type="full">Disruptive Technology Office</orgName>
					<orgName type="abbreviated">DTO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,118.50,124.34,100.74,10.80"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.13,124.34,80.92,10.80"><forename type="first">Krzysztof</forename><surname>Czuba</surname></persName>
							<email>kczuba@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>21 st Floor</addrLine>
									<postCode>1440, 10018</postCode>
									<settlement>Broadway, New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.16,124.34,68.84,10.80"><forename type="first">Pablo</forename><surname>Duboue</surname></persName>
							<email>duboue@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,430.51,124.34,58.33,10.80"><forename type="first">John</forename><surname>Prager</surname></persName>
							<email>jprager@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.30,87.88,263.35,16.65">IBM&apos;s PIQUANT II in TREC2005</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7216A025747484364EA95E5AA164367A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This year, the PIQUANT II system we used in the TREC QA track is an improved version over the reengineered system we used in last year's entry <ref type="bibr" coords="1,426.53,254.30,95.50,10.80;1,90.00,268.10,25.85,10.80" target="#b1">[Chu-Carroll et al., 2005]</ref>. Our system adopts a multi-agent approach to question answering. In this framework, a question is submitted to multiple agents, each adopting a different question answering strategy and/or consults a different information source to produce a set of answers, which are then combined using a voting scheme to determine the overall system's answer(s) to the question. In our 2005 system, we have made improvements along several dimensions, by improving the performance of select answering agents, by developing two new agents, and finally, by improving our answer resolution algorithm for combining answers from individual agents. In this paper, we describe these improvements and their impact on the factoid, list, and other subtasks in the main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIQUANT II System Overview</head><p>As described in <ref type="bibr" coords="1,176.13,425.72,128.73,10.80" target="#b1">[Chu-Carroll et al., 2005]</ref>, PIQUANT II is built on a modular and extensible architecture that supports component plug-and-play, distributed client-server deployment, and supports well-defined APIs for typical QA system components. Figure <ref type="figure" coords="1,90.00,467.12,6.00,10.80">1</ref> shows how PIQUANT II was configured for the TREC 2005 factoid and list subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 PIQUANT II as Configured for TREC 2005 Factoid &amp; List Questions</head><p>The question pre-processing component takes a question, which may contain referring expressions or ellipses, and a target as input, and performs any transformation needed to produce a self-contained question. This question is then sent to five answering agents in parallel. Of the five answering agents employed by our TREC 2005 system, the statistical query agent and the KSP agent, which answers questions from structured knowledge sources, remain unchanged from previous year's system. We improved the performance of the predictive annotation agent and increased the coverage of the patternbased agent. Furthermore, we developed a web agent that utilizes the web as an information source and a constraint agent based on, and in run IBM05C3PD used instead of, the predictive annotation agent and employs an automatic answer verification process to improve accuracy. The answers returned by each of the five agents are combined using a machine-learned voting scheme in our answer resolution module. Finally, the answer justification component attempts to locate a passage in a reference corpus to justify an answer found from an alternative source (such as in our structured database for the KSP agent or on the web for the web agent). Figure <ref type="figure" coords="3,124.83,211.70,6.00,10.80">2</ref> shows PIQUANT II as it was configured for answering questions in the "other" subtask. In addition to improving the two agents we developed for this subtask in 2004, which were each used in one of the two runs we submitted last year, we developed an answer resolution component to combine the nuggets returned by both agents to produce a combined nugget list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 PIQUANT II as Configured for TREC 2005 Other Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIQUANT II Components for Factoid/List Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Preprocessor</head><p>The goal of the question preprocessor in the factoid and list question setting is to resolve referring expressions and ellipses against the question target to produce a self-contained natural language question for subsequent processing. In general, we eliminate anaphors and introduce the target so that a better search engine query could be formulated. Our assumption was that anaphors in a question always refer to the target and not to an answer to a previous question in a sequence. This has been borne out in the question set in which only a handful of anaphors could be interpreted as referring to an answer.</p><p>Anaphors were replaced with the target string in a way that attempted to keep the resulting question grammatical. In the case of the ambiguous pronoun "her", we relied on a deep syntactic parser, ESG <ref type="bibr" coords="3,232.08,480.20,76.11,10.80" target="#b3">[McCord, 1989]</ref>, to identify it as a possessive or accusative.</p><p>We replaced an occurrence of a possessive pronoun with "X's" where X is the target. All other pronominal anaphors were replaced with the target.</p><p>Based on the sample event targets that were available as training data, we implemented a restriction on the maximum length of targets that could replace anaphors. Another restriction was on the capitalization within a target to prevent targets like "The return of Hong Kong to China" from getting embedded in questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answering Agents</head><p>In this section, we discuss enhancements we made to the predictive annotation agent as well as describe a newly developed agent, the constraint agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictive Annotation Agent</head><p>Analyzed Passage Question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 Retrieval Component with Feedback Loops</head><p>For the most part, the question analysis and answer selection components of our predictive annotation agent remain unchanged from our 2004 system. The key improvements we made were in the document/passage retrieval stage in the following two aspects. First, our earlier system treats the document retrieval and passage ranking processes as a one-stage process. A single query is given to this retrieval component and this query is both used to retrieve relevant documents using the JuruXML search engine <ref type="bibr" coords="4,90.00,365.24,102.18,10.80" target="#b0">[Carmel et al., 2003]</ref> and to identify the most relevant 1-3 sentence passages from these documents. In our new retrieval architecture, which is shown in Figure <ref type="figure" coords="4,446.00,379.04,4.50,10.80">3</ref>, we separated the document retrieval stage from the passage ranking stage and allowed for different queries to be considered for each of the two processes so that certain query terms which may be present to set the context of the document, but which may not be repeated in all 1-3 sentence windows, can be taken into account in the document retrieval stage but not in passage ranking. Second, in our new architecture, we introduced feedback loops <ref type="bibr" coords="4,90.00,461.84,134.29,10.80" target="#b4">[Pasca and Harabagiu, 2001</ref>] to allow the system to initially target high precision queries and gradually relax the queries should they prove to be overly constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraint Agent</head><p>The constraint agent is an extension of the work first presented in <ref type="bibr" coords="4,408.65,519.38,92.99,10.80" target="#b5">[Prager et al, 2004]</ref> and is the first time we employed it in any of our TREC runs. We used the constraint agent, which invokes the predictive annotation agent recursively only on factoid questions, although in principle it could be used on any type. Simply put, it operates by generating a candidate answer set in the usual way, then inverts the question and substitutes each candidate answer in turn, and examining if the inverted answer matches a term in the original question. This can be illustrated by means of an example. Question 110.2 was (substituting the target) "When was the club Lions Club International founded"? The internally generated initial candidate answer set was, in order of preference, "1997, 1917, â€¦". The constraint agent automatically generated the inverted question "What club was founded in X?", and substituted in turn the values in the initial candidate answer list for X. These new inverted questions are then submitted again to the predictive annotation agent and the top answer for "What club was founded in 1917?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>Engine Passage Engine Query Document Query Passages Query Relaxation Query Relaxation was indeed the Lions Club International, thus providing confirming evidence for 1917. On the other hand, the answer for "What club was founded in 1997?" was another entity, thus refuting the answer 1997. The recomputed score for 1917 was higher than the recomputed score for 1997, so the former was returned.</p><p>The algorithm used for determining whether and how to adjust the original candidate list consists applying a decision tree to the confidences of the answers in the forward and inverted directions. The parameters and thresholds used in the decision tree were determined by machine learning over training data. The particular implementation of the constraint agent we used for TREC just looked at the top two candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Resolution Component</head><p>In our TREC 2004 system, we adopted an answer resolution strategy that combines answers returned by each answering agent using an equal a priori probability. In 2005, we developed a new answer resolution component that uses a linear combination of the agent's a priori weight based on its prior performance and the confidence score the agent assigned to an answer. In addition, through experimentation with parameter training based on questions with different answer types, we found that for our set of five agents and their respective performance, adopting a different set of a priori weights for questions seeking dates as answers is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIQUANT II Components for "Other" Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA-by-Dossier Agent</head><p>The QA-by-Dossier agent is built on the observation that some of the interesting properties of a target entity can be predicted by simply knowing the type of the entity. For example, knowing the target is a person, then occupation, birth and death dates and other life-cycle information are probably going to be at the very least "okay", if not "vital". Thus by preparing a number of subquestions (such as "When was X born?", "Where is Y headquartered?") keyed on type we could call our QA system recursively and generate information nuggets to return. We understood this strategy would not necessarily capture surprising or unexpected information nuggets, but we hoped that our profile agent, using a collocation plus idf-based approach, would cover those cases.</p><p>In 2004 the QA-byDossier agent did not work terribly well, primarily because the information on the assessors nugget lists was very heterogeneous, and did not seem to follow any consistent usage model (such as preparing an obituary or encyclopedia article, on which our questions were based). For 2005 we hoped that after discussions of these issues both by us and others <ref type="bibr" coords="5,241.03,606.68,122.85,10.80" target="#b1">[Chu-Carroll et al., 2004</ref><ref type="bibr" coords="5,363.87,606.68,133.00,10.80" target="#b2">, Hildebrandt et al., 2004]</ref>, the nugget lists would be more consistent, so we continued this approach, but tried to expand the nugget net by using a broader and deeper set of questions.</p><p>In TREC 2004 we simply classified the target as Person, Organization or Thing, with a small number (3-12) of corresponding questions. For 2005 we classified the target into one of 20 entity categories, including subtypes of the former three (e.g. Inventor, Athlete, Entertainer, Writer) and orthogonal ones (e.g. Place, Religion). Questions were generated by hand for each of the categories. These were arranged in a hierarchy, so that Inventor had Inventor-specific questions ("What did X invent?"), but also inherited from Person ("Where was X born?"). In an attempt to capture the surprising facts, we asked questions like "What crime did X commit?", again keyed on type (Person in this case), relying on the fact that if that person was not a criminal, the QA system will return either no answer or an answer with very low confidence (and thus will not be chosen for output by the QA-by-Dossier agent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Profile Agent</head><p>The profile agent extracts relevant information in a three-stage process. In the first stage, short passages about the target are extracted from the reference corpus. In the second stage, entities that are strongly associated with the target are identified from within these passages. In the third stage, a subset of the extracted passages is chosen to convey the relationship between the selected entities and the target. In our 2004 system, in the entity selection phase, the extracted passages are processed by the ESG parser and all common nouns are identified and normalized. Those nouns that occurred more frequently than expected based on their idf value are selected as candidate entities. In our 2005 system, we improved upon this entity selection process by considering not just co-occurrence information of nouns, but all concepts in the passages and weighing those concepts that are syntactically related to the target more strongly than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Performance and Analysis</head><p>We submitted three runs to the TREC 2005 QA track, whose results as scored by the NIST assessors are shown in Table <ref type="table" coords="6,281.50,414.14,4.50,10.80" target="#tab_1">1</ref>. The first run, IBM05C3PD employed five answering agents for factoid and list questions as shown in Figure <ref type="figure" coords="6,418.31,427.94,4.50,10.80">1</ref>, with the constraint agent selected to represent the constraint/predictive annotation agent alternatives. When combining agent answers, the answer resolution component assigned different a priori weights to agents based on answer types, and in answering the "other" questions, both the QA-by-Dossier agent and the profile agent were employed. In runs IBM05L1P and IBM05L3P, the predictive annotation agent was used instead of the constraint agent in both runs, and only the profile agent was used for the "other" questions. As our results show, the impact of the different system configurations on our overall score is minimal. The largest difference is in the addition of the QA-by-Dossier agent in our first run, affecting 5 out of 65 questions 1 and increasing our F-score from 0.192 to 0.206. Note that some of the output of the agent was redundant with that of the profile agent and some were precluded in scoring because the same question was asked, albeit with different words, in the factoid section. The QA-by-Dossier agent performance also suffered because we had no subquestions for the Event type target, which turned out to be quite prevalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Across the factoid questions, the constraint agent materially changed only 12 answers, but of these only two went from "Wrong" to "Right" or "Inexact", and two went the other way. Follow-up experimentation has shown that the constraint agent works better when the scores associated with candidate answers are close to confidences or probabilities of being correct. However, in our system development prior to TREC the only consideration put on the scores was that the better answer should have a higher score, rather than represent confidence in the absolute sense, which appears to be necessary for the constraint mechanism to be effective. We are currently addressing this issue to improve the performance of the constraint agent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,537.80,392.85,74.04"><head>Table 1 Assessed Scores for Submitted Runs</head><label>1</label><figDesc></figDesc><table coords="6,90.00,537.80,392.85,53.58"><row><cell></cell><cell>Factoid</cell><cell>List</cell><cell>Other</cell><cell>Overall</cell></row><row><cell>IBM05C3PD</cell><cell>.323</cell><cell>.131</cell><cell>.206</cell><cell>.246</cell></row><row><cell>IBM05L1P</cell><cell>.323</cell><cell>.131</cell><cell>.192</cell><cell>.242</cell></row><row><cell>IBM05L3P</cell><cell>.326</cell><cell>.131</cell><cell>.192</cell><cell>.244</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,95.70,699.36,419.58,9.02;6,90.00,710.82,206.37,9.02"><p>We have discovered after TREC that a bug in the expected input for the QA-by-Dossier agent resulted in decreased impact of the agent on our TREC results.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supposed in part by the <rs type="funder">Disruptive Technology Office (DTO)</rs>'s <rs type="programName">Advanced Question Answering for Intelligence (AQUAINT) program</rs> under contract number <rs type="grantNumber">H98230-04-C-1577</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YTCkMnk">
					<idno type="grant-number">H98230-04-C-1577</idno>
					<orgName type="program" subtype="full">Advanced Question Answering for Intelligence (AQUAINT) program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>In this paper, we described our PIQUANT II system as configured in the TREC 2005 QA runs. Our efforts this year focused on algorithmic improvements of individual answering agents as well as development of new agents employing different question answering strategies. Our resulting system ranked 3 rd out of 30 participating systems in the TREC QA main task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,90.00,505.04,422.27,10.80;7,90.00,518.84,355.55,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,434.67,505.04,77.60,10.80;7,90.00,518.84,148.15,10.80">Searching XML documents via XML fragments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mandelbrod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,246.25,518.84,129.89,10.80">Proceedings of SIGIR 2003</title>
		<meeting>SIGIR 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,546.44,398.67,10.80;7,90.00,560.24,308.58,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,90.00,560.24,143.90,10.80">IBM&apos;s PIQUANT II in TREC</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,266.94,560.24,125.38,10.80">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2004">2005. 2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,587.84,395.00,10.80;7,90.00,601.64,265.62,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,266.98,587.84,218.02,10.80;7,90.00,601.64,89.81,10.80">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,187.68,601.64,137.95,10.80">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,629.24,424.64,10.80;7,90.00,643.04,307.98,10.80" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,185.63,629.24,329.01,10.80;7,90.00,643.04,237.95,10.80">Slot grammar: A system for simpler construction of practical natural language grammars. Natural Language and Logic</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mccord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="118" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,670.64,422.58,10.80;7,90.00,684.44,136.31,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,255.97,670.64,188.25,10.80">High Performance Question/Answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,452.58,670.64,60.00,10.80;7,90.00,684.44,42.31,10.80">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,74.84,399.29,10.80;8,90.00,88.64,416.26,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,314.27,74.84,175.02,10.80;8,90.00,88.64,216.81,10.80">Question answering using constraint satisfaction: QA-by-Dossier-with-Constraints</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,315.00,88.64,121.79,10.80">Proceedings of ACL 2004</title>
		<meeting>ACL 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="575" to="582" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
