<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.67,94.61,374.56,12.62">External Knowledge Sources for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.18,128.13,60.91,10.52"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.99,128.13,93.81,10.52"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.14,128.13,91.17,10.52"><forename type="first">Gary</forename><surname>Borchardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.13,128.13,90.37,10.52"><forename type="first">Alexis</forename><surname>Brownell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.33,142.08,64.87,10.52"><forename type="first">Sue</forename><surname>Felshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.30,142.08,78.52,10.52"><forename type="first">Daniel</forename><surname>Loreto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.32,142.08,129.03,10.52"><forename type="first">Jesse</forename><surname>Louis-Rosenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,456.16,142.08,40.51,10.52"><forename type="first">Ben</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,145.93,157.46,81.70,10.52"><forename type="first">Federico</forename><surname>Mora</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.15,157.46,86.63,10.52"><forename type="first">Stephan</forename><surname>Stiller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.88,154.57,79.92,13.41"><forename type="first">Ã–zlem</forename><surname>Uzuner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,427.94,157.46,86.04,10.52"><forename type="first">Angela</forename><surname>Wilcox</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.67,94.61,374.56,12.62">External Knowledge Sources for Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">220A34C35FCC0A092F179EBD2A812E5D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction MIT CSAIL's entries for the TREC Question Answering track <ref type="bibr" coords="1,204.36,266.34,79.10,9.57" target="#b16">(Voorhees, 2005)</ref> focused on incorporating external general-knowledge sources into the question answering process. We also explored the effect of document retrieval on factoid question answering, in cooperation with a community focus on document retrieval. For the new relationship task, we present a new passage-retrieval based algorithm emphasizing synonymy, which performed best among automatic systems this year.</p><p>Our most prominent new external knowledge source is the Wikipedia<ref type="foot" coords="1,245.81,427.59,4.23,6.99" target="#foot_0">1</ref> , and its most useful component is the synonymy implicit in its subtitles and redirect link structure. Wikipedia is also a large new source of hypernym information.</p><p>The main task included factoid questions, for which we modified the freely available Web-based Aranea question answering engine; list questions, for which we used hypernym hierarchies to constrain candidate answers; and definitional 'other' questions, for which we combined candidate snippets generated by several previous definition systems using a new novelty-based reranking method inspired by <ref type="bibr" coords="1,118.33,619.84,88.79,9.57" target="#b0">(Allan et al., 2003)</ref>.</p><p>Our factoid engine, Aranea<ref type="foot" coords="1,242.97,632.05,4.23,6.99" target="#foot_1">2</ref> (Lin and <ref type="bibr" coords="1,295.39,634.00,26.06,9.57;1,103.18,647.55,24.25,9.57">Katz, 2003)</ref>, uses the World Wide Web to find candidate answers to the given question, and then projects its best candidates onto the newspaper corpus, choosing the one best sup-ported. Candidate generation uses snippets from Google and Teoma, and this year from Yahoo and from the newspaper corpus as well. This year, we added an Aranea module that, in the spirit of our approach to list questions, boosts candidate answers which are hyponyms of the question focus. Finally, we rank answers using a combination of Web-based and corpusbased prominence rather than using only the top Web-based answer.</p><p>Our list engine <ref type="bibr" coords="1,433.52,367.27,103.64,9.57" target="#b15">(Tellex et al., 2003)</ref> retrieves passage-sized chunks of text relevant to the question using information retrieval techniques, and projects onto them the fixed lists associated with the question focus. This year, we augmented our knowledge base with lists extracted from Wikipedia, and attempted to use a relevant Wikipedia article to initially search for answers, paralleling Aranea's approach. As in previous years, question analysis was performed primarily by START<ref type="foot" coords="1,551.99,500.81,4.23,6.99" target="#foot_2">3</ref>  <ref type="bibr" coords="1,338.46,516.31,60.36,9.57" target="#b7">(Katz, 1988;</ref><ref type="bibr" coords="1,404.34,516.31,56.12,9.57" target="#b8">Katz, 1997;</ref><ref type="bibr" coords="1,465.97,516.31,85.90,9.57" target="#b6">Katz et al., 2002)</ref>. This year, in anticipation of new event topics, we expanded START's question analysis for TREC to include relative clauses and some additional constructions.</p><p>Our definition engine combines snippet rankings from several independent components. We identify syntactic structures associated with definitional context a priori, then match topics against the resulting database of target-nugget pairs <ref type="bibr" coords="1,435.08,652.85,121.65,9.57" target="#b5">(Hildebrandt et al., 2004;</ref><ref type="bibr" coords="1,338.46,666.40,78.83,9.57" target="#b4">Fernandes, 2004)</ref>. We also rank snippets from IR using a tf*idf-based score which heavily favors matching targets. Finally, we use a keyword-based novelty score to select the best answers. This year, we incorporated external knowledge by adding Wikipedia-based synonymy, and testing two methods of selecting snippets based on matches with a Wikipedia article.</p><p>Our relationship engine scores snippets from Lucene using precision-and recall-like measures: recall is based on how many of the synonym groups in the question were covered, and their relative importance, while precision is based on a heuristic semantic distance from each synonym group to the words in a candidate passage that were used to fill it. Prior context outside the snippet is permitted to augment recall. We generated results based on manual and heuristic-based automatic question analysis.</p><p>We will describe each of the systems in more detail below, and expand on official results to explore component contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Retrieval</head><p>Underlying each component of our question answering system is keyword-based document retrieval using Lucene 4 . We explored three modifications to the default (baseline) query behavior: idf weighting, idf-based backoff, and idf-based backoff treating the most important ("anchor") phrases in the question as undroppable. These strategies are described in greater detail below, and summarized in Figure 1. Our list and definition systems use a single Wikipedia article, which we selected via a combination of Lucene and Google queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baselines</head><p>Each year NIST distributes the top 1000 document results of its PRISE document retrieval system for each question along with the questions, so that participating teams need not do their own document retrieval. A second baseline was the default Lucene behavior, as specified by a disjuction of the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Idf Backoff</head><p>Our first experimental document retrieval strategy uses successive conjunctive queries, 4 http://lucene.apache.org/ gathering up to 1000 hits by successively dropping the lowest idf term from the query. For each term, the final query specifies all inflectional variants in a disjunction <ref type="bibr" coords="2,439.16,129.64,66.55,9.57;2,287.43,143.19,24.25,9.57" target="#b2">(Bilotti et al., 2004)</ref>. This strategy was used in the majority of our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anchored Backoff</head><p>A new experimental strategy made use of "anchor" words or phrases identified by START in the question. These anchor terms may be named entities or known collocations. Term disjunctions in this strategy include derivational as well as inflectional term variants, and for multiword terms they allow a fixed distance between the terms. The overall strategy is still to drop least important terms first, but now non-anchor words are successively dropped and re-added for each anchor word that is dropped. The minimal query then is the term disjunction for the single most important anchor term. In a variant of this strategy, we can look for anchors using the list knowledge base (Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Idf Weighting</head><p>The relationship engine issues a single disjunctive query for each topic, but weights each term by the portion of the entire query's idf that it and its synonyms are responsible for. In the case where no two terms are synonymous, this is just each term's own idf. Where two words are synonymous, each term's weight will be the sum of both terms' idfs. We treat other variation, such as morphology, as we do synonyms. Unlike the other two strategies, we do not expand each query term to be a disjunction of that term's inflectional variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Wikipedia</head><p>The first step in employing Wikipedia for a question is finding the relevant article for a topic. Topics in previous years were restricted to simple noun phrases, and over 90% of them appeared in Wikipedia in some form. Of the 75 topics this year, 10 were "event" noun phrases like "1998 Nagano Olympics" and 4 were headline-like events, like "Liberty Bell 7 space capsule recovered from ocean". Fewer of these appear in Wikipedia. We found the correct Wikipedia article for 87% of the noun phrase topics, for 70% of the noun phrase event topics, and for none of the headline topics-81% accuracy overall. We found relevant articles in Wikipedia by varying capitalization and noun number, looking for topic words in the body as well as the title of an article, and as a last resort doing a Google search restricted to Wikipedia's main namespace. We resorted to Google for 39 topics, among them all 14 of the incorrect articles mentioned above. Some Wikipedia articles were not about, but only briefly mentioned, the topic of interest; if matching content was low, then the matching paragraphs were used as if they formed an article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Document Retrieval Results</head><p>Our document retrieval strategies did not yield different results in the top 50 candidates that matter for factoid question answering. Some systems like our definition engine may use more than this number of documents, and our anchor-based model yielded a significantly higher recall over all 1000 documents than our baseline. We showed that strategies using Lucene and inflectional variation offer abovemedian performance on this task. (See Fig- <ref type="table" coords="3,338.46,88.99,28.52,9.57">ure 2)</ref> 3 Wikipedia Synonymy Many system components made use of synonymy information extracted from Wikipedia. This information is implicit in subtitles and the redirects between pages: "TWA800" and "Trans World Airlines flight 800" redirect to "TWA flight 800", and "Woodrow Wilson Guthrie" redirects to "Woody Guthrie". This sort of synonymy is relatively ad-hoc, and unpredictable in the sense that humans unfamiliar with the particular domains of these synonymy facts would also have trouble or uncertainty in deciding whether the pairs were synonyms. In these cases, the encyclopedic knowledge of synonymy can be instrumental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Factoid Questions</head><p>We have been using the Aranea system for question answering for four years, and it has recently become open source. We submitted one run based on the latest open-source Aranea ("Aranea04", adapted minimally to our topic-based infrastructure), and two runs using an improved Aranea ("Aranea05"). We used the two improved runs to test the difference in end-to-end performance between the baseline and anchor document retrieval strategies described above. (See results in Figure <ref type="figure" coords="3,544.00,486.12,4.24,9.57">3</ref>.) Attention to Topic: Aranea04 treats each question separately, relying on question analysis to substitute the topic before processing begins. Aranea05 uses the format introduced at TREC13, and performs a preliminary search on the topic alone. Subsequent Web queries use the concatenation of the topic and question, and scores of candidates that were prominent in the topic alone are damped.</p><p>Attention to Focus: Like list questions, factoid questions often include a noun-phrase question focus. For "In what sea did the submarine sink?", the focus would be "sea". We used WordNet to find hyponyms of this focus, where available, and boosted scores of candidate answers that matched such hyponyms, so that "Barents Sea" would become a more likely candidate than "icy Barents". Figure <ref type="figure" coords="4,86.23,332.83,4.24,9.57">2</ref>: Document retrieval results: While our experimental document retrieval strategies performed below baseline, they nevertheless yielded above-baseline performance in final question answering. We evaluate recall at 50 because our factoid engine, Aranea, used the top 50 documents for projection, and indeed the best recall at 50 (PRISE) also showed the best factoid projection MRR. We show document and factoid evaluation on the 48 factoid questions out of 50 total questions for which document retrieval was directly evaluated. We also show corresponding factoid results for all 362 factoid questions, though these cannot be directly compared with document retrieval results. The MRR prj column indicates the mean reciprocal rank of the correct factoid answer over the 48 relevant questions in the Aranea projection step, which finds candidate answers in the top 50 Lucene documents and ranks these by quality of match-thus this is the component that directly translates document retrieval performance to an effect on factoid answering performance. Factoid performance here reflects the Aranea05 system. For our anchored and idf backoff experiments, recall at 10 or 50 is out of an average of 31.5Â±17 supporting documents per question. Missing values (?) could not be measured without detailed results for best and median runs. Factoid 362 results cannot be measured for oracular document retrieval because human assessments were not performed. run (doc+factoid) R @ 50 correct / nonzero all questions csail1 (idf bck+A04) .561Â±.10 9 / 41 .207Â±.04 csail2 (anchors+A05) .548Â±.10 13 / 42 .273Â±.05 csail3 (idf bck+A05) .561Â±.10 13 / 41 .260Â±.05</p><p>Figure <ref type="figure" coords="4,87.25,637.47,4.24,9.57">3</ref>: Factoid question answering results: Our three conditions are shown. Ranking did not have an effect for the 48 factoid questions where document rankings were evaluated, but it did have an effect overall. The difference in answering performance on the restricted question set is marginally significant (p = .052). The difference in answering performance on the entire question set is significant for Aranea05 over Aranea04 (p &lt; 0.01), but not between the two document retrieval strategies for Aranea05 (p = .113). Integrated answer projection: Once a set of candidate answers is found on the Web, Aranea04 selects the top answer and finds a best match in the newspaper corpus. On the assumption again that the newspapers might be a good source of information, Aranea05 instead scores the top 10 Webbased answers and combines Web-based and newspaper-based scores using F-measure to select its best answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Factoid Results</head><p>We observed a significant improvement in question answering performance due to changes to Aranea. We did not observe a great difference in results due to document retrieval (see Figure <ref type="figure" coords="5,162.92,590.23,4.24,9.57">3</ref>), but this is unsurprising, because the differences between our document retrieval strategies themselves were not significant.</p><p>In analyzing individual components, we found that attention to topic may have been helpful, while attention to the question focus may have been detrimental, but neither difference was large (see Figure <ref type="figure" coords="5,227.37,702.90,4.24,9.57">4</ref>). Combining webbased and newspaper-based rankings, however showed a clear win (see Figure <ref type="figure" coords="5,252.91,730.00,4.24,9.57">5</ref>). Figure <ref type="figure" coords="5,376.09,159.33,4.24,9.57">5</ref>: Factoid score and MRR of the web rankings alone, the projected rankings alone, and the ranking combining these with F-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">List Questions</head><p>Our list question architecture identifies an answer type for each question, retrieves newspaper articles, identifies contexts with query terms, and selects phrases matching known answer types from those contexts. We reorder these candidates using frequency and return the top 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Answer Types</head><p>The START Natural Language Question Answering System identifies the focus of a list question, a noun phrase most descriptive of the expected answer type. Three of START's internal functions were exposed in a TRECspecific API, and enhanced to work with a wider array of questions:</p><p>â€¢ Noun-phrase parsing for the topic itself,</p><p>â€¢ anaphoric substitution to place the topic into each question as appropriate, and</p><p>â€¢ focus extraction to find for each question the type of answer sought.</p><p>In anticipation of the more complex topics this year, we improved START's handling of relative clauses and other complex noun phrase constructions, and its handling of anaphoric hypernyms of the antecedent. We incorrectly assumed that no questions would refer to noun phrases from previous questions, or to the answers to those questions. 5 START also identifies possible focus phrases for each list question and offers several reformulations for further analysis. For example the reformulations for "Name famous people who have been Rhodes scholars" included:</p><p>â€¢ "famous people who have been Rhodes</p><p>scholars" â€¢ "famous people"</p><p>â€¢ "Rhodes scholars"</p><p>â€¢ "people"</p><p>We hypothesized that a larger number of lists and a wider variation of list names would improve both coverage and specificity: coverage by matching more question focuses, and specificity by matching more specific reformulations that had smaller associated lists.</p><p>We expanded our answer type knowledge base using Wikipedia. Lists in Wikipedia fall into three categories also observed in other kinds of corpora: A list might be the entire purpose and content of an article, it might make up part of a larger description, or the article might mention that it is about a particular topic, in which case the set of articles on that topic is a list. In Wikipedia we found 48,412 full-article lists, 166,263 lists within larger articles, and more than a million category mentions. In comparison to the 3000 lists we used last year, and to the 150 the year before, these represent a very large potential increase in coverage. Wikipedia thus provides what is to our knowledge the largest source of manually generated list information.</p><p>We used only full-article lists because they afforded the most straight-forward means to associate the list with a descriptive phrase: their article title. We generated alternate names for each list by heuristically removing modifiers, and by using Wikipedia subtitles and link structure (see Section 3).</p><p>For one experimental condition we used our lists from last year (csail1) where for the other conditions (csail2 and csail3) we added the full-article Wikipedia lists.</p><p>For each possible question focus, and each possible reformulation of that focus from START, we select all matching list names, and treat the union of matching list members as the answer type, the set of possible answers to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Candidate Generation</head><p>Candidates are generated from an answer type (a set of possible known answers), and a text, by looking for instances of the answer type within the text. For many answer types we also generated "guess answer" candidates using a named entity recognizer. Candidate scores were assigned based on quality of the reformulation used and the proximity of question keywords to the candidate list item.</p><p>In csail1 and csail2 we used newspaper documents as the text for selecting list answers (using baseline and anchors document retrieval respectively), while in csail3 we used a Wikipedia article. After finding answers in a Wikipedia article, we used Aranea's new projection module to find those answers again in the newspaper corpus, that being our final target corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Answer Selection</head><p>Answer selection is based on the scores assigned during candidate generation, the number of times a candidate is proposed, heuristic filters and a top-k cutoff. List answers were always used in preference to guess answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">List Results</head><p>Adding Wikipedia lists to our knowledge base improved the recall upper bound for 2003 (32% to 39%) and 2004 (33% to 39%), but did not do so for 2005 (32% for both plain and Wikipedia). The changes in the underlying document collection used account for about 1% of the precision changes shown.</p><p>Figure <ref type="figure" coords="6,332.77,607.63,5.45,9.57">6</ref> summarizes our list results. Figure <ref type="figure" coords="6,333.71,621.61,5.45,9.57">7</ref> summarizes the differences in list results for each system due to the various answer type components. The difference in lists-2004 result between csail1 and the other two runs is due to a bug: we removed knowledge about people, so all questions requiring a person answer type fell back on the other two answer type sources, usually the IdentiFinder "guess" answer source <ref type="bibr" coords="6,397.88,730.00,88.59,9.57" target="#b1">(Bikel et al., 1999)</ref>. It is difficult to separate the influence of this bug from the influence of adding the Wikipedia hyponyms, so we are reluctant to draw conclusions from these data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Definition Questions</head><p>We call the final "Other" question in each topic a "definition" question, seeking nuggets of interesting information about the topic that were not addressed in the previous questions.</p><p>Our baseline sought candidate nuggets in three ways: by looking up the topic in a pre-compiled database of definitional contexts <ref type="bibr" coords="7,132.91,269.05,129.71,9.57" target="#b5">(Hildebrandt et al., 2004;</ref><ref type="bibr" coords="7,270.30,269.05,51.15,9.57;7,103.18,282.60,24.25,9.57" target="#b4">Fernandes, 2004)</ref>, by searching the corpus for a short context that includes many keywords from a Webster's Dictionary definition for the topic, and by simply positing each sentence from the top retrieved documents. This strategy used Wikipedia synonyms of the target for matching, if available.</p><p>In two experimental conditions, we also generated candidate nuggets by looking for newspaper sentences that had a high overlap with the first paragraph of the best Wikipedia article according to the BLEU metric <ref type="bibr" coords="7,294.93,431.73,21.22,9.57;7,103.18,445.28,91.13,9.57" target="#b14">(Papineni et al., 2001)</ref>. Our three experimental conditions were: without Wikipedia/BLEU (csail1), with Wikipedia/BLEU (csail2), and with Wikipedia/BLEU on newspaper articles where anaphora had been resolved.</p><p>We ranked candidates first by topic accuracy, and then by tf*idf of non-topic terms, where tf is frequency within the set of candidates. Topic accuracy is an F-measure based on word-based precision and recall of the topic; "Clinton" in a candidate where the topic is "Hillary Clinton" would have 100% precision and 50% recall. Wikipedia synonyms received 100% F-measure.</p><p>We subsequently removed candidates that were too similar, again based on keywords, to nuggets that had been selected at lower rank. The algorithm is described and evaluated in detail in <ref type="bibr" coords="7,145.29,689.36,97.23,9.57" target="#b12">(Marton et al., 2006)</ref>. This similarity score takes idf into account, but focuses on how many of the keywords are new vs. old. We did not use Wikipedia synonymy in this similarity computation, though it might have been a good idea.</p><p>We used various heuristic score-based cutoffs, and submitted at most 24 sentences for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Definition Results</head><p>We observed no significant differences between our systems in their ability to find nuggets for definitional or "Other" questions, nor in any particular strategy of our system, but we do note that there is a significant difference in response length favoring our new BLEU-based strategy (Figure <ref type="figure" coords="7,419.69,269.67,4.24,9.57">8</ref>). Though length is the deciding factor, we would have done better to include more results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Relationship Questions</head><p>Our approach to the relationship track is based on passage-retrieval methods. Banking on mutual disambiguation among the question terms, we extend standard passage retrieval scoring with a precision-and recall-like approach to synonymy. We used as one source of synonyms a thesaurus we developed for last year's pilot, inspired by the "spheres of influence" in the task description. Unlike other passage ranking systems, we also incorporate an effect of prior context. As with definition questions, we incorporate a model of novelty to iteratively select the best and most novel passage at each rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Question Analysis</head><p>Training data were available from a pilot conducted last year. Many question keywords are extraneous or misleading, most notably "The analyst ...", but also the subsequent "... is interested in ..." and more. Our question analysis aims primarily to eliminate these noncontributory phrases, using regular expressions developed on questions from 2004. Some phrases are salient only together, e.g., "United Nations" should be penalized if the words are separated. We marked those phrases that appeared in our synonymy knowledge bases. Figure <ref type="figure" coords="8,86.71,340.40,4.24,9.57">7</ref>: Credit assignment for correct list answers. All shows correct answers from the entire set of candidates, while sel. and precision include only those submitted, after answer selection. Total precisions are high because document support is not considered. Sums of rows are higher than total because some answers were selected by multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List</head><p>We would have liked to exclude some background information phrases, e.g., "Osama Bin Laden" from the first question, "We know Osama Bin Laden is in charge, but what other organizations or people are involved...". We can do so with manual question analysis, but were unable to exclude them in our automatic run.</p><p>It was possible to mark a word or phrase as "important", but the automatic question analysis did not attempt to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Candidate Ranking</head><p>Sentence ranking is based on precision-and recall-like measures. Each question term is assigned a weight based on its idf. Words that are synonymous according to our lexicons are pooled and their weights summed. The weights of words in the final sentence, and of some other useful terms, are boosted.</p><p>We retrieve 500 newspaper articles using Lucene on a disjunction of all terms with the weights described above. Synonymous terms from the question are included in the Lucene query as well, each with the pooled weight. We note each document's Lucene DocScore.</p><p>Candidate recall reflects how many terms from the query had matching terms in the candidate, or its prior context, as a portion of the sum of idf s in the query. Each matching term or synonym contributes its matching query term's full idf to recall, however poor the match.</p><p>Candidate precision reflects how well the terms in the candidate matched the terms in the query. Exact term matches have a similarity of 1, and other similarity values come from each source of synonyms. Some, for example in the manually built thesaurus, are manually assigned. If more than one variant of a query term appears in the candidate, then the variants reinforce each other, so that the combined similarity for those terms is one minus the product of their dissimilarities. The precision score for the candidate is the average similarity of all matching terms.</p><p>The final candidate score combines candidate precision, candidate recall, and a docu- The bleu strategy is useful, and appears to be orthogonal to whether the newspaper sentence is a "definitional context". Bleu shows contributions of using the BLEU similarity metric to choose newspaper sentences that were most similar to sentences from the first paragraph of the best Wikipedia article. The lucene strategy simply selected corpus sentences that matched the target. Database is our collection of definitional contexts. Webster shows the contribution of selecting newspaper sentences by their similarity with a dictionary definition. The identical numbers of bleu matches is coincidencethe answers judged correct are not the same. The answers from the database are the same for csail2 and csail3.</p><p>ment score using F-measures:</p><formula xml:id="formula_0" coords="10,64.62,116.24,193.34,10.77">F Î²=3 (F Î²=2 (precision, recall), DocScore)</formula><p>Given the ranked list of candidate sentences, we then use our keyword-based novelty filtering algorithm to select those to display. Of those, we submitted the top 24 candidates for each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Synonymy</head><p>A central component of the algorithm above is the synonym knowledge base 6 . We used synonyms from Wikipedia as described in Section 3, nominalizations from Nomlex <ref type="bibr" coords="10,226.18,276.35,44.25,9.57;10,52.16,289.90,56.81,9.57" target="#b11">(Macleod et al., 1998)</ref>, and the small thesaurus we developed for the pilot last year. We also treated case and morphology as synonymy.</p><p>Our thesaurus has a two-level structure: a synset level has a high precision and contains closely related terms, while a topic level relates financial terms, exchange of goods terms, terms related to crime, and nine other topics.</p><p>In our experience, candidate selection is more sensitive to the relative precisions of the different kinds of synonyms than to the absolute weights. Our manually created thesaurus was grouped into topics inspired by the "spheres of influence" (SOI) from the task definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Results</head><p>The relationship task was comparable to the definition task in difficulty, with top partiallymanual and fully-automatic systems performing at 28% and 23% respectively.</p><p>Our two experimental conditions were whether the process above was performed manually or automatically. Due to a singlecharacter human error we submitted our same automatic run for evaluation twice. The slight difference in our results is actually due to annotator error.</p><p>That automatic run performed well (see Figure <ref type="figure" coords="10,72.60,682.93,9.09,9.57" target="#fig_1">11</ref>). We have used Nuggeteer <ref type="bibr" coords="10,227.66,682.93,42.76,9.57;10,52.16,696.48,24.25,9.57" target="#b13">(Marton, 2006)</ref>, a Pourpre-like automatic analysis tool, and Pourpre itself (Lin and Demner-Fushman,   6 We use the term "synonym" loosely here. Bars are cumulative: in blue are the number of responses shared between systems-there was much variability; in red the number of correct responses as judged by TREC assessors; in yellow the number above that which ought to be correct because the exact responses were judged correct in other systems; finally the total number of responses for each system. Systems are identified by their run id. We show additional analysis of component contributions in <ref type="bibr" coords="11,218.39,395.53,98.21,9.57" target="#b12">(Marton et al., 2006)</ref>.</p><p>Using Nuggeteer, we estimate that our run with manual question analysis would have performed only slightly better than our fully automatic run, with F-measure 0.269Â±.0864 Fmeasure (recall=0.4652, precision=0.0671).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Contributions</head><p>We submitted three runs for the main tasksummarized in Figure <ref type="figure" coords="11,211.06,529.07,12.36,9.57" target="#fig_0">13</ref>-in which we tested the effects of document retrieval and of large external resources for question answering. In particular, we:</p><p>â€¢ Tested the effect of two document retrieval strategies on document ranking and on end-to-end factoid accuracy, finding surprisingly that better document retrieval did not locally correlate with better end-to-end question answering.</p><p>7 Pourpre and Nuggeteer are both ultimately similar to the Qaviar system for automatic 250-byte factoid assessment <ref type="bibr" coords="11,141.29,711.40,75.62,7.86" target="#b3">(Breck et al., 2000)</ref>, but though open source, no version of Qaviar was publicly available at the time Pourpre and Nuggeteer were developed.</p><p>â€¢ Used Wikipedia's link structure as a robust source of synonyms in a number of question answering tasks. Figure <ref type="figure" coords="12,108.62,289.93,9.29,9.57" target="#fig_0">13</ref>: Experimental conditions and overall system performance in the main task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,103.18,184.65,218.27,9.57;3,103.18,198.20,218.28,9.57;3,103.18,211.75,218.27,9.57;3,103.18,225.30,218.27,9.57;3,103.18,238.84,218.27,9.57;3,103.18,252.39,218.27,9.57;3,103.18,265.94,218.27,9.57;3,103.18,279.49,218.26,9.57;3,103.18,293.04,28.82,9.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Document Query Strategies: small "a" is the idf of query term capital "A", and a &gt; b &gt; c &gt; d. Semicolons indicate a subsequent query, executed if not enough documents have been retrieved. N E refers to a named entity or some other anchor term (previously undistinguished as term B) which is deemed to have special significance to the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,287.43,456.29,218.27,9.57;10,287.43,469.84,218.27,9.57;10,287.43,483.39,39.88,9.57;10,346.22,483.39,159.47,9.57;10,287.43,496.94,218.27,9.57;10,287.43,510.49,218.27,9.57;10,287.43,524.04,218.27,9.57;10,287.43,537.59,218.26,9.57;10,287.43,551.14,218.27,9.57;10,287.43,564.69,218.27,9.57;10,287.43,578.24,218.27,9.57;10,287.43,591.79,218.27,9.57;10,287.43,605.34,32.15,9.57;10,288.57,210.15,216.00,225.10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Relationship system performance: official scores are shown for the top six systems.Bars are cumulative: in blue are the number of responses shared between systems-there was much variability; in red the number of correct responses as judged by TREC assessors; in yellow the number above that which ought to be correct because the exact responses were judged correct in other systems; finally the total number of responses for each system. Systems are identified by their run id.</figDesc><graphic coords="10,288.57,210.15,216.00,225.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,103.18,278.77,218.26,9.57;11,103.18,292.32,218.27,9.57;11,103.18,305.87,218.26,9.57;11,103.18,319.42,99.64,9.57;11,104.32,85.48,215.99,172.24"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Answer cutoff vs. performance as estimated by Nuggeteer and by Pourpre. In hindsight we might have done better to include slightly more results.</figDesc><graphic coords="11,104.32,85.48,215.99,172.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,52.16,103.55,468.01,213.22"><head></head><label></label><figDesc></figDesc><graphic coords="4,52.16,103.55,468.01,213.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,103.18,87.88,218.27,255.95"><head></head><label></label><figDesc>Aranea04 searches only the Web for candidate answers, but we felt that the corpus might occasionally have relevant information. Thus as we added new Web search engines like Yahoo, we also added the top Lucene results from the newspaper corpus, just as if they had come from a Web search engine.</figDesc><table coords="5,103.18,87.88,218.27,161.11"><row><cell cols="3">New Modules Score MRR web</cell></row><row><cell>-Topic, -Focus</cell><cell>.248</cell><cell>.278</cell></row><row><cell>-Topic</cell><cell>.248</cell><cell>.278</cell></row><row><cell>-Focus</cell><cell>.254</cell><cell>.270</cell></row><row><cell>Full system</cell><cell>.251</cell><cell>.270</cell></row><row><cell cols="3">Figure 4: Factoid score and MRR of web-</cell></row><row><cell cols="3">based candidates, ablating the new Topic and</cell></row><row><cell cols="2">Focus modules from Aranea05.</cell><cell></cell></row><row><cell cols="2">Newspaper candidates:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,103.18,154.71,453.55,404.66"><head></head><label></label><figDesc>Definition official results: none of the F-measure differences are significant (p &gt; .40). Nuggets is the number of nuggets assigned. Items is the number of responses with at least one nugget. #Returned is the number of responses returned. Char/resp is the number of nonwhitespace characters per response, on average. Zero is the number of qids with no correct nuggets. The length of csail1 nuggets is significantly greater than the lengths of the other two (p &lt; .0001); however, the difference between csail2 and csail3 is non-significant (p = .20).</figDesc><table coords="9,103.18,154.71,453.55,404.66"><row><cell></cell><cell cols="4">Definition Official Results</cell><cell></cell></row><row><cell>run (id)</cell><cell cols="6">nuggets items #returned char/resp zero F (Î² = 3)</cell></row><row><cell>csail1 (91): syns</cell><cell>154</cell><cell></cell><cell>144</cell><cell>1554</cell><cell>142</cell><cell>34 0.1557Â±0.0389</cell></row><row><cell>csail2 (96): +bleu</cell><cell>145</cell><cell></cell><cell>132</cell><cell>1571</cell><cell>118</cell><cell>38 0.1606Â±0.0471</cell></row><row><cell>csail3 (101): +anphr</cell><cell>150</cell><cell></cell><cell>139</cell><cell>1591</cell><cell>118</cell><cell>39 0.1602Â±0.0479</cell></row><row><cell cols="5">Figure 8: Definition Corrected</cell><cell></cell></row><row><cell>run</cell><cell cols="5">nuggets items zero F (Î² = 3)</cell></row><row><cell cols="2">csail1</cell><cell>156</cell><cell>148</cell><cell cols="2">33 0.1593 Â± 0.0391</cell></row><row><cell cols="2">csail2</cell><cell>154</cell><cell>152</cell><cell cols="2">37 0.1689 Â± 0.0485</cell></row><row><cell cols="2">csail3</cell><cell>159</cell><cell>155</cell><cell cols="2">37 0.1745 Â± 0.0495</cell></row><row><cell cols="7">Figure 9: Corrected results: if a string was assigned a nugget in any run, that nugget was</cell></row><row><cell cols="7">assigned automatically here. This only applies to complete string matches. Differences are still</cell></row><row><cell cols="7">non-significant, and this result emphasizes the variability of judgements, reversing the difference</cell></row><row><cell>between csail2 and csail3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Definition Breakdown by Contributor</cell></row><row><cell>run</cell><cell cols="2">bleu</cell><cell cols="4">lucene database webster total</cell></row><row><cell>csail1</cell><cell></cell><cell cols="2">103/1050</cell><cell>50/495</cell><cell cols="2">1/9 1554</cell></row><row><cell cols="4">csail2 111/1190 22/ 253</cell><cell>11/124</cell><cell cols="2">1/4 1571</cell></row><row><cell cols="4">csail3 111/1201 28/ 262</cell><cell>11/124</cell><cell cols="2">0/4 1591</cell></row><row><cell cols="5">Figure 10: Credit assignment for definitional nuggets:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,349.84,140.46,206.89,50.22"><head>â€¢</head><label></label><figDesc>Presented a new method for sentence retrieval based on a new model of synonymy and context, and showed state-of-the-art performance on the relationship task.</figDesc><table coords="12,86.54,87.92,384.77,183.59"><row><cell></cell><cell>csail1</cell><cell></cell><cell>csail2</cell><cell>csail3</cell></row><row><cell>Factoid</cell><cell cols="2">Aranea2004</cell><cell cols="2">Aranea2005</cell><cell>Aranea2005</cell></row><row><cell>Doc Rank</cell><cell cols="2">baseline</cell><cell cols="2">baseline</cell><cell>anchors</cell></row><row><cell>List (kb/corpus)</cell><cell cols="4">plain/newspaper wiki/newspaper wiki/wiki</cell></row><row><cell cols="2">Other (wiki projection) none</cell><cell></cell><cell>BLEU</cell><cell>BLEU+anaphora</cell></row><row><cell></cell><cell cols="2">Factoid</cell><cell>List</cell><cell>Other</cell></row><row><cell></cell><cell cols="4">accuracy F(Î²=1) F(Î²=3)</cell></row><row><cell></cell><cell>best</cell><cell>.713</cell><cell>.468</cell><cell>.248</cell></row><row><cell></cell><cell>csail1</cell><cell>.207</cell><cell>.110</cell><cell>.156</cell></row><row><cell></cell><cell>csail2</cell><cell>.273</cell><cell>.088</cell><cell>.161</cell></row><row><cell></cell><cell>csail3</cell><cell>.260</cell><cell>.100</cell><cell>.160</cell></row><row><cell></cell><cell>median</cell><cell>.152</cell><cell>.053</cell><cell>.156</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,119.77,711.36,112.98,7.47"><p>http://en.wikipedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,119.77,721.09,170.51,8.39;1,103.18,731.98,94.15,7.47"><p>http://www.umiacs.umd.edu/ âˆ¼ jimmylin/ projects/aranea.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,355.04,731.98,127.10,7.47"><p>http://start.csail.mit.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,355.04,701.44,201.68,7.86;5,338.46,711.40,46.43,7.86"><p>Several questions did, in fact refer to the previous answer,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4" coords="5,387.93,711.40,168.78,7.86;5,338.46,721.37,218.26,7.86;5,338.46,731.33,182.77,7.86"><p>including 68.5, 68.7, 71.4, 81.3, 84.2, 84.3,  120.4, 120.6, 136.3, 136.5, 137.3, and also possibly 67.2,  67.3, 70.6, 81.4, 84.5, 114.6, 131.6, and 137.6.   </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,338.46,249.32,218.27,8.74;11,349.37,260.28,207.35,8.74;11,349.37,271.24,207.35,8.74;11,349.37,282.19,157.47,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,376.13,260.28,180.59,8.74;11,349.37,271.24,44.96,8.74">Retrieval and novelty detection at the sentence level</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Bolivar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,416.84,271.24,139.88,8.74;11,349.37,282.19,94.37,8.74">Proceedings of the ACM SIG in Information Retrieval</title>
		<meeting>the ACM SIG in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.46,303.97,218.27,8.74;11,349.37,314.93,207.36,8.74;11,349.37,325.89,207.36,8.74;11,349.37,336.85,71.40,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,476.52,314.93,80.21,8.74;11,349.37,325.89,108.96,8.74">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,473.72,325.89,78.48,8.74">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.46,358.62,218.27,8.74;11,349.37,369.58,207.35,8.74;11,349.37,380.54,207.35,8.74;11,349.37,391.50,207.37,8.74;11,349.37,402.46,207.36,8.74;11,349.37,413.41,92.80,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,378.89,369.58,177.83,8.74;11,349.37,380.54,207.35,8.74;11,349.37,391.50,21.92,8.74">What works better for question answering: Stemming or morphological query expansion?</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Bilotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,390.86,391.50,165.87,8.74;11,349.37,402.46,207.36,8.74;11,349.37,413.41,64.04,8.74">Proceedings of the SIGIR 2004 Workshop IR4QA: Information Retrieval for Question Answering</title>
		<meeting>the SIGIR 2004 Workshop IR4QA: Information Retrieval for Question Answering</meeting>
		<imprint>
			<date type="published" when="2004-07">2004. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.46,435.19,218.26,8.74;11,349.37,446.15,207.35,8.74;11,349.37,457.11,207.35,8.74;11,349.37,468.07,207.36,8.74;11,349.37,479.02,207.36,8.74;11,349.37,489.98,207.36,8.74;11,349.37,500.94,154.79,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,438.61,457.11,118.11,8.74;11,349.37,468.07,207.36,8.74;11,349.37,479.02,65.58,8.74">How to evaluate your question answering system every day ... and still get real work done</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,442.82,479.02,113.90,8.74;11,349.37,489.98,207.36,8.74;11,349.37,500.94,122.74,8.74">Proceedings of the second international conference on Language Resources and Evaluation (LREC2000)</title>
		<meeting>the second international conference on Language Resources and Evaluation (LREC2000)</meeting>
		<imprint>
			<date type="published" when="2000-06">2000. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.46,522.72,218.27,8.74;11,349.37,533.68,207.36,8.74;11,349.37,544.63,168.26,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,457.68,522.72,99.05,8.74;11,349.37,533.68,203.47,8.74">Answering definitional questions before they are asked. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.46,566.41,218.27,8.74;11,349.37,577.37,207.35,8.74;11,349.37,588.33,207.36,8.74;11,349.37,599.29,207.36,8.74;11,349.37,610.24,207.36,8.74;11,349.37,621.20,207.36,8.74;11,349.37,632.16,146.26,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,377.02,577.37,179.70,8.74;11,349.37,588.33,102.51,8.74">Answering definition questions with multiple knowledge sources</title>
		<author>
			<persName coords=""><forename type="first">Wesley</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,476.97,588.33,79.76,8.74;11,349.37,599.29,207.36,8.74;11,349.37,610.24,207.36,8.74;11,349.37,621.20,207.36,8.74;11,349.37,632.16,111.81,8.74">Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting, HLT/NAACL-04</title>
		<meeting>the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting, HLT/NAACL-04</meeting>
		<imprint>
			<date type="published" when="2004-04">2004. April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,338.46,653.94,218.27,8.74;11,349.37,664.90,207.35,8.74;11,349.37,675.85,207.35,8.74;11,349.37,686.81,207.36,8.74;11,349.37,697.77,207.36,8.74;11,349.37,708.73,207.36,8.74;11,349.37,719.69,207.36,8.74;11,349.37,730.65,53.92,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,529.05,675.85,27.67,8.74;11,349.37,686.81,207.36,8.74;11,349.37,697.77,82.11,8.74">Omnibase: Uniform access to heterogeneous data for question answering</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alton</forename><surname>Jerome Mc-Farland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baris</forename><surname>Temelkuran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,458.13,697.77,98.59,8.74;11,349.37,708.73,207.36,8.74;11,349.37,719.69,207.36,8.74;11,349.37,730.65,22.68,8.74">Proceedings of the 7th International Workshop on Applications of Natural Language to Information Systems (NLDB 2002)</title>
		<meeting>the 7th International Workshop on Applications of Natural Language to Information Systems (NLDB 2002)</meeting>
		<imprint>
			<date type="published" when="2002-06">2002. June</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,323.58,218.27,8.74;12,63.07,334.54,207.36,8.74;12,63.07,345.49,207.35,8.74;12,63.07,356.45,152.96,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,134.67,323.58,135.76,8.74;12,63.07,334.54,40.56,8.74">Using English for indexing and retrieving</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,123.98,334.54,146.45,8.74;12,63.07,345.49,207.35,8.74;12,63.07,356.45,88.65,8.74">Proceedings of the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</title>
		<meeting>the 1st RIAO Conference on User-Oriented Content-Based Text and Image Handling</meeting>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,374.98,218.26,8.74;12,63.07,385.94,207.36,8.74;12,63.07,396.90,207.36,8.74;12,63.07,407.86,178.14,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,143.29,374.98,127.13,8.74;12,63.07,385.94,127.03,8.74">Annotating the World Wide Web using natural language</title>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,220.77,385.94,49.66,8.74;12,63.07,396.90,207.36,8.74;12,63.07,407.86,109.60,8.74">Proceedings of the Conference on the Computer-Assisted Searching on the Internet</title>
		<meeting>the Conference on the Computer-Assisted Searching on the Internet</meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,426.39,218.27,8.74;12,63.07,437.35,207.36,8.74;12,63.07,448.31,207.36,8.74;12,63.07,459.27,102.75,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,63.07,437.35,207.36,8.74;12,63.07,448.31,38.71,8.74">Automatically evaluating answers to definition questions</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,111.24,448.31,29.61,8.74">LAMP</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<date type="published" when="2005-02">2005. February</date>
			<pubPlace>College Park</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,477.80,218.26,8.74;12,63.07,488.76,207.35,8.74;12,63.07,499.72,207.36,8.74;12,63.07,510.67,207.36,8.74;12,63.07,521.63,207.36,8.74;12,63.07,532.59,137.46,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,213.46,477.80,56.96,8.74;12,63.07,488.76,207.35,8.74;12,63.07,499.72,182.79,8.74">Question answering from the web using knowledge annotation and knowledge mining techniques</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,63.07,510.67,207.36,8.74;12,63.07,521.63,207.36,8.74;12,63.07,532.59,83.08,8.74">Proceedings of the 12th International Conference on Information and Knowledge Management (CIKM 2003)</title>
		<meeting>the 12th International Conference on Information and Knowledge Management (CIKM 2003)</meeting>
		<imprint>
			<date type="published" when="2003-11">2003. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,551.12,218.26,8.74;12,63.07,562.08,207.36,8.74;12,63.07,573.04,207.36,8.74;12,63.07,584.00,167.61,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,63.07,573.04,182.40,8.74">NOMLEX: A lexicon of nominalizations</title>
		<author>
			<persName coords=""><forename type="first">Catherine</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leslie</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruth</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,63.07,584.00,124.26,8.74">Proceedings of EURALEX&apos;98</title>
		<meeting>EURALEX&apos;98</meeting>
		<imprint>
			<date type="published" when="1998-08">1998. August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,602.53,218.26,8.74;12,63.07,613.49,207.36,8.74;12,63.07,624.45,207.36,8.74;12,63.07,635.41,207.36,8.74;12,63.07,646.37,207.36,8.74;12,63.07,657.32,207.36,8.74;12,63.07,668.28,207.36,8.74;12,63.07,679.24,34.09,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,127.55,613.49,142.87,8.74;12,63.07,624.45,207.36,8.74;12,63.07,635.41,131.59,8.74">Component analysis of retrieval approaches to the TREC question answering track&apos;s nugget-based subtasks</title>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.04,635.41,45.39,8.74;12,63.07,646.37,207.36,8.74;12,63.07,657.32,207.36,8.74;12,63.07,668.28,202.83,8.74">Submitted to Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006)</title>
		<imprint>
			<date type="published" when="2006-08">2006. August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,52.16,697.77,218.26,8.74;12,63.07,708.73,207.36,8.74;12,63.07,719.69,207.35,8.74;12,63.07,730.65,73.09,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,169.52,697.77,100.90,8.74;12,63.07,708.73,207.36,8.74;12,63.07,719.69,47.35,8.74">Nuggeteer: Automatic nugget-based evaluation using descriptions and judgements</title>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<idno>1721.1/30604</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,250.50,719.69,19.92,8.74;12,63.07,730.65,27.33,8.74">MIT CSAIL</title>
		<imprint>
			<date type="published" when="2006-01">2006. January</date>
		</imprint>
	</monogr>
	<note type="report_type">Work Product</note>
</biblStruct>

<biblStruct coords="12,287.43,323.58,218.27,8.74;12,298.34,334.54,207.36,8.74;12,298.34,345.49,207.36,8.74;12,298.34,356.45,207.36,8.74;12,298.34,367.41,207.36,8.74;12,298.34,378.37,75.75,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,408.24,334.54,97.46,8.74;12,298.34,345.49,203.20,8.74">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,312.41,356.45,193.29,8.74;12,298.34,367.41,207.36,8.74;12,298.34,378.37,46.63,8.74">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002)</meeting>
		<imprint>
			<date type="published" when="2001-07">2001. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,287.43,398.30,218.27,8.74;12,298.34,409.26,207.35,8.74;12,298.34,420.21,207.35,8.74;12,298.34,431.17,207.36,8.74;12,298.34,442.13,207.36,8.74;12,298.34,453.09,207.36,8.74;12,298.34,464.05,149.94,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,472.22,409.26,33.47,8.74;12,298.34,420.21,207.35,8.74;12,298.34,431.17,99.11,8.74">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,424.97,431.17,80.73,8.74;12,298.34,442.13,207.36,8.74;12,298.34,453.09,207.36,8.74;12,298.34,464.05,121.47,8.74">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003)</meeting>
		<imprint>
			<date type="published" when="2003-07">2003. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,287.43,483.98,218.27,8.74;12,298.34,494.93,110.86,8.74" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,390.79,483.98,114.92,8.74;12,298.34,494.93,106.71,8.74">Overview of the trec 2005 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
