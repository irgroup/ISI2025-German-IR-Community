<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,69.75,62.08,472.25,16.65;1,164.25,82.33,282.79,16.65">Building on Redundancy: Factoid Question Answering, Robust Retrieval and the &quot;Other&quot;</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,132.00,136.55,90.02,10.80"><forename type="first">Dmitri</forename><surname>Roussinov</surname></persName>
							<email>dmitri.roussinov@asu.edu</email>
						</author>
						<author>
							<persName coords="1,399.00,136.55,72.03,10.80"><forename type="first">Michael</forename><surname>Chau</surname></persName>
							<email>mchau@business.hku.hk</email>
						</author>
						<author>
							<persName coords="1,140.25,192.05,73.62,10.80"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
							<email>filatova@cs.columbia.edu</email>
						</author>
						<author>
							<persName coords="1,364.50,192.05,140.21,10.80"><forename type="first">José</forename><forename type="middle">Antonio</forename><surname>Robles-Flores</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<country>ESAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Information Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,69.75,62.08,472.25,16.65;1,164.25,82.33,282.79,16.65">Building on Redundancy: Factoid Question Answering, Robust Retrieval and the &quot;Other&quot;</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">354D9CD23127124F9381D3A29A9E0387</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have explored how redundancy based techniques can be used in improving factoid question answering, definitional questions ("other"), and robust retrieval. For the factoids, we explored the meta approach: we submit the questions to the several open domain question answering systems available on the Web and applied our redundancy-based triangulation algorithm to analyze their outputs in order to identify the most promising answers. Our results support the added value of the meta approach: the performance of the combined system surpassed the underlying performances of its components. To answer definitional ("other") questions, we were looking for the sentences containing re-occurring pairs of noun entities containing the elements of the target. For robust retrieval, we applied our redundancy based Internet mining technique to identify the concepts (single word terms or phrases) that were highly related to the topic (query) and expanded the queries with them. All our results are above the mean performance in the categories in which we have participated, with one of our robust runs being the best in its category among all 24 participants. Overall, our findings support the hypothesis that using as much as possible textual data, specifically such as mined from the World Wide Web, is extre mely promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FACTOID QUESTION ANSWERING</head><p>The Natural Language Processing (NLP ) task, which is behind Question Answering (QA) technology, is known to be Artificial Intelligence (AI) complete: it requires the computers to be as intelligent as people, to understand the deep semantics of human communication, and to be capable of common sense reasoning. As a result, different systems have different capabilities. They vary in the range of tasks that they support, the types of questions they can handle, and the ways in which they present the answers. By following the example of meta search engines on the Web <ref type="bibr" coords="1,305.37,521.34,101.70,8.77" target="#b17">(Selberg &amp; Etzioni, 1995)</ref>, we advocate combining several fact seeking engines into a single "Meta" approach. Meta search engines (sometimes called metacrawlers) can take a query consisting of keywords (e.g. "Rotary engines"), send them to several portals (e.g. Google, MSN, etc.), and then combine the results. This allows them to provide better coverage and specialization. The examples are MetaCrawler <ref type="bibr" coords="1,478.04,555.84,79.62,8.77;1,54.00,567.84,263.02,8.77">(Selberg &amp; Etzioni, 1995), 37.com (www.37.com), and Dogpile (www.dogpile.com)</ref>. Although, the keyword based meta search engines have been suggested and explored in the past, we are not aware of the similar approach tried for the task of open domain/corpus question answering (fact seeking).</p><p>The practical benefits of the meta approach are justified by general consideration: eliminating "weakest link" dependency. It does not rely on a single system which may fail or may simply not be designed for a specific type of tasks (questions). The meta approach promises higher coverage and recall of the correct answers since different QA engines may cover different databases or different parts of the Web. In addition, the meta approach can reduce subjectivity by querying several engines; like in the real-world, one can gather the views from several people in order to make the answers more accurate and objective. The speed provided by several systems queried in parallel can also significantly exceed those obtained by working with only one system, since their responsiveness may vary with the task and network traffic conditions. In addition, the meta approach fits nicely into a becoming-popular Web services model, where each service (QA engine) is independently developed and maintained and the meta engine integrates them together, while still being organizationally independent from them. Since each engine may be provided by a commercial company interested in increasing their advertising revenue or a research group showcasing their cutting edge technology, the competition mechanism will also ensure quality and diversity among the services. Finally, a meta engine can be customized for a particular portal such as those supporting business intelligence, education, serving visually impaired or mobile phone users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta Approach Defined</head><p>We define a fact seeking meta engine as the system that can combine, analyze, and represent the answers that are obtained from several underlying systems (called answer services throughout our paper). At least some of these underlying services (systems ) have to be capable of providing candidate answers to some types of questions asked in a natural language form, otherwise the overall architecture would not be any different from a single fact seeking engine which are typically based on a commercial keyword search engines, e.g. Google. The technology behind each of the answer services can be as complex as deep semantic NLP or as simple as shallow pattern matching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact Seeking Service</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web address</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges Faced and Addressed</head><p>Combing multiple fact seeking engines also faces several challenges. First, the output formats of them may differ: some engines produce exact answer (e.g. START), some other present one sentence or an entire snippet (several sentences) simi lar to web search engines, as shown in Figures <ref type="figure" coords="2,233.44,682.59,3.87,8.78" target="#fig_0">1</ref><ref type="figure" coords="2,237.31,682.59,3.87,8.78">2</ref><ref type="figure" coords="2,237.31,682.59,3.87,8.78" target="#fig_1">3</ref><ref type="figure" coords="2,241.18,682.59,3.87,8.78">4</ref>. Table <ref type="table" coords="2,277.58,682.59,4.88,8.78" target="#tab_0">1</ref> summarizes those differences and other capabilities for the popular fact seeking engines. Second, the accuracy of responses may differ overall and have even higher variability depending on a specific type of a question. And finally, we have to deal with multiple answers, thus removing duplicates, and resolving answer variations is necessary. The issues with merging search results from multiple engines have been already explored by MetaCrawler <ref type="bibr" coords="3,109.80,67.59,105.61,8.78" target="#b17">(Selberg &amp; Etzioni, 1995)</ref> and fusion studies in information retrieval (e.g. <ref type="bibr" coords="3,415.91,67.59,93.55,8.78">Vogt &amp; Cottrell, 1999)</ref> but only in the context or merging lists of retrieved text documents. We argue that the task of fusing multiple short answers, which may potentially conflict or confirm each other, is fundamentally different and poses a new challenge for the researchers. For example, some answer services (components) may be very precise (e.g. START), but cover only a small proportion of questions. They need to be backed up by less precise services that have higher coverage (e.g. AskJeeves). However, backing up may easily result in diluting the answer set by spurious (wrong) answers. Thus, there is a need for some kind of triangulation of the candidate answers provided by the different services or multiple candidate answers provided by the same service. Triangulation, a term which is widely used in intelligence and journalism, stands for confirming or disconfirming facts, by using multiple sources. <ref type="bibr" coords="3,153.00,434.34,95.33,8.77" target="#b13">Roussinov et al. (2004)</ref> went one step further than using the frequency counts explored earlier by <ref type="bibr" coords="3,54.00,446.34,84.30,8.77" target="#b8">Dumais et al. (2002)</ref> and groups involved in TREC competitions. They explored a more fine-grained triangulation process which we also used in our prototype. Their algorithm can be demonstrated by the following intuitive example. Imagine that we have two candidate answers for the question "What was the purpose of the Manhattan Project?": 1) "To develop a nuclear bomb" 2) "To create an atomic weapon". These two answers support (triangulate) each other since they are semantically similar. However, a straightforward frequency count approach would not pick this similarity. The advantage of triangulation over simple frequency counting is that it is more powerful for less "factual" questions, such as those that may allow variations in the correct answers.</p><p>In order to enjoy the full power of triangulation with factoid questions (e.g. Who is the CEO of IBM?), the candidate answers have to be extracted from their sentences (e.g. Samuel Palmisano), so they can be more accurately compared with the other candidate answers (e.g. Sam Palmisano). That is why the meta engine needs to possess answer understanding capabilities as well, including such crucial capability as question interpretation and semantic verification of the candidate answers to check that they belong to a desired category (person in the example above).</p><p>Figure <ref type="figure" coords="4,85.03,369.84,3.72,8.78">5</ref>. The Meta approach to fact seeking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact Seeking Engine Meta Prototype: Underlying Technologies and Architecture</head><p>In the first version of our prototype, we included several freely available demonstrational prototypes and popular commercial engines on the Web that have some QA (fact seeking) capabilities, specifically START, AskJeeves, BrainBoost and ASU QA (Table <ref type="table" coords="4,83.99,453.84,3.72,8.78" target="#tab_0">1</ref>, Figures <ref type="figure" coords="4,129.24,453.84,3.60,8.78" target="#fig_0">1</ref><ref type="figure" coords="4,132.84,453.84,3.60,8.78">2</ref><ref type="figure" coords="4,132.84,453.84,3.60,8.78" target="#fig_1">3</ref><ref type="figure" coords="4,136.44,453.84,3.60,8.78">4</ref>). We also added Wikipedia to the list. Although it does not have QA capabilities, it provides good quality factual information on a variety of topics, which adds power to our triangulation mechanism. Google was not used directly as a service but BrainBoost and ASU QA are already using it among the other major keyword search engines. The meta-search part of our system was based on the MetaSpider architecture <ref type="bibr" coords="4,364.88,488.34,78.46,8.77" target="#b2">(Chau et al., 2001;</ref><ref type="bibr" coords="4,447.37,488.34,73.89,8.77" target="#b3">Chen et al., 2001)</ref>. Multithreads are launched to submit the query to fetch the candidate answers from each service. After these results are obtained, the system performs answer extraction, triangulation and semantic verification of the results, based on the algorithms from <ref type="bibr" coords="4,54.00,522.84,97.72,8.77" target="#b13">Roussinov et al. (2004)</ref>. Figure <ref type="figure" coords="4,191.89,522.84,4.88,8.78">5</ref> summarizes the overall process. For the TREC competition, we applied the answer projection algorithm, same as last year, that tried to find the best supporting document within the TREC collection (Aquaint) by matching the words from the question and the target.</p><p>We have been maintaining a working prototype on the web (http://qa.wpcarey.asu.edu/) since August 2004 and have already accumulated 1000+ questions that we can use to test our future research hypothesis and fine-tune our algorithms. The prototype has been featured in Information Week <ref type="bibr" coords="4,257.13,585.84,64.37,8.77" target="#b4">(Claburn, 2005)</ref> as one of the promising directions in the "Web Search of Tomorrow."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing on 2004 questions</head><p>Before the answer submission deadline this year, we fine-tuned the weights given to the underlying answer services and evaluated our meta approach. We used the set of 200 test questions and regular expression answer keys from the Question-Answering Track of the TREC 2004 conference <ref type="bibr" coords="4,260.32,663.84,130.98,8.77" target="#b20">(Voorhees and Buckland, 2004)</ref>. Although various metrics have been explored in the past, we used mean reciprocal rank (MRR) of the first correct answer as in the <ref type="bibr" coords="4,447.78,675.09,53.49,8.77">TREC-s 2001</ref><ref type="bibr" coords="4,501.27,675.09,27.62,8.77">, 2002</ref><ref type="bibr" coords="4,532.28,675.09,25.33,8.77;4,54.00,687.09,80.80,8.77" target="#b8">and in Dumais et al. (2002)</ref>. This metric assigns a score of 1 to the question if the first answer is correct. If only the second answer is correct, the score is ½, the third correct results in 1/3, etc. The drawback of this metric is that it is not the most sensitive since it only considers the first correct answer, ignoring what follows. However, it is still more sensitive than the TREC 2004 and 2005 official metrics that only look at the first answer. We did not use the "degree of support" of the answer within the document as part of the metric due to its known difficulty <ref type="bibr" coords="5,300.53,67.59,44.88,8.78" target="#b12">(Lin, 2005)</ref>, and thus only checked if the answer was correct, which is sometimes called "lenient" evaluation, to which the concerns of Lin et al. do not apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-reference resolution</head><p>We used the same heuristic rules as last year <ref type="bibr" coords="5,237.43,122.34,94.32,8.78" target="#b13">(Roussinov et al., 2004)</ref> to resolve the necessary co-references, similarly to how it was done by most participants. Specifically, we replaced the pronouns with the targets. (E.g. converting "How fast can it fly?" into "How fast can F16 fly?"). The target was also appended at the end of the question if it was not already present in the question. The approach worked correctly in 75% of the cases. Most errors were caused by events (new this year). However, our approach happened to be res ilient to those types of question interpretation errors since it was based on the redundancy rather than on the accurate interpretation. Consider the following example: Target = "Miss Universe 2000 crowned", Question = "What country did the winner represent?" The correct interpretation would be rather difficult at the current state of the art of AI since it would require knowing that "Miss Universe" event is supposed to have a winner, finding the winner, and substituting the winner name to the original question. However, for our redundancy based approach it was not essential. The question that we sent to the answering services was "What country did the winner represent Miss Universe 2000 crowned?" Although the question sound awkward, the underlying services still returned plenty of snippets related to "Miss Universe 2000." Since the beginning of the question matched the pattern "What \T did \Q \V" (where T = "country", \Q = "the winner" and \V = "represent") our semantic verification mechanism was deliberately looking for mentioning of countries, but not the other types such as people names, dates, etc. This often resulted in correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Official Runs Submitted</head><p>We submitted two official runs: 1) ASUQA01 that used only Google as underlying source of answers and was not essentially different from the system used last year, and 2) ASUQA02 that used our meta engine, which was essentially equivalent to using our last year answer extraction and triangulation engine on the answers obtained from the underlying answer services listed in Table <ref type="table" coords="5,117.00,349.59,3.52,8.78" target="#tab_0">1</ref>. As we expected, ASUQA02 was significantly better than ASUQA01in the measured accuracy : 0.180 vs. 0.149 . It was also within 20% results across all participants (0.152).</p><p>Our finding corroborates the findings in the more general domain of web searching, in which meta-approach results in better coverage than each individual search engine. In the case of using QA technology, which is known to be very knowledge intensive and expensive to create, it is extremely challenging and also important for the meta engine to be at least as good as the best underlying component, otherwise the correct answer can be missed and diluted by erroneous ones. As our empirical finding illustrates, the triangulation algorithms that we have employed has successfully overcome that challenge. Since our triangulation was capitalizing on the inherent redundancy on the web or among the answers, this result also testifies to the power of redundancy based techniques.</p><p>Both NLP-based and the approaches that require elaborate manually created patterns have a strong advantage: they can be applied to smaller collections (e.g. corporate repositories) and provide good performance. However, because expensive knowledge engineering is required to build such systems and possibly the entailing intellectual property issues, none of the known top performing systems has been made publicly open to the other researches for follow up investigations. As result, it is still unknown what approaches exactly work in different conditions, for example how well they would extend outside of TREC domain. On the other side, the algorithms behind the systems that do not require extensive knowledge engineering, but still demonstrate reasonable performance, have been available open to the public, e.g. <ref type="bibr" coords="5,412.50,534.09,87.77,8.77" target="#b8">(Dumais et al., 2002;</ref><ref type="bibr" coords="5,504.75,534.09,52.05,8.77;5,54.00,546.09,54.49,8.77" target="#b13">Roussinov &amp; Robles, 2004)</ref>. We believe that from the research perspective, those transparent "knowledge-light" systems and approaches are no less interesting that the top commercial systems since they allow replication and independent testing by the other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DOCUMENT RANKING</head><p>The task of "document ranking," defined for some factoid questions, was to order documents in the target collection so as to maximize the likelihood of having the answer in the top returned documents. Since we did not use the target collection to obtain the answer, but rather obtained the answer through triangulation of web answers, we only used document ranking to perform the answer projection at the end. For this, we used bm25 ranking function from Lemur 3.1 1 . The query consisted of the question, the target and the answer merged together. We submitted the obtained order as our official document ranking, same with both our runs. Our result (mean R-precision of 0.3032) was significantly above the mean across all participants (0.1666) and ranked #6. We believe this was because we "looked ahead:" retrieved documents when already having a certain answer in mind rather than retrieving documents based only on the target and the question. This result also indicates the promise of t he redundancy based approach to rank documents in order to respond to the factoid information request (question) when precise answer is not really required, which simulates many practical situations, e.g. searching the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANSWERING "OTHER" QUESTIONS</head><p>"Other" Questions in TREC 2005</p><p>For the first time questions of the "other" type were studied within TREC 2004 QA evaluation. "Other" questions substituted definition questions studied within TREC 2003 <ref type="bibr" coords="6,248.83,174.09,70.31,8.78" target="#b19">(Voorhees, 2003)</ref> and "asked for additional information about the target that was not covered by previous [factoid and list questions] in the series" <ref type="bibr" coords="6,343.30,185.34,69.12,8.78" target="#b20">(Voorhees, 2004)</ref>. "Other" question was asked about every target. The text snippets submitted as potential answers to the "other" question were checked whether they contained "vital" or "okay" nuggets about the target. The vital nuggets "must appear in a definition for that definition to be good." <ref type="bibr" coords="6,54.00,219.84,67.94,8.78" target="#b19">(Voorhees, 2003)</ref>. "Non-vital nuggets act as don't care conditions in that the assessor believes the information in the nugget to be interesting enough that returning the information is acceptable in, but not necessary for, a good response" <ref type="bibr" coords="6,512.08,231.09,44.35,8.78;6,54.00,243.09,20.69,8.78" target="#b20">(Voorhees, 2004)</ref>. The presence in the answer of the 'okay' nuggets did not increase the recall and did not decrease the precision.</p><p>Our approach was almost identical to the one described in <ref type="bibr" coords="6,291.00,260.34,151.42,8.77" target="#b9">(Filatova and Hatzivassiloglou, 2003)</ref> and relied on redundancy as well. It capitalized on the fact that multiple co-occurrences of the target with the named entities within one sentence could potentially capture the most interesting (vital or OK) properties of the target.</p><p>One of the crucial innovations of the TREC 2005 QA Track was that the targets , for which the questions were formulated, included not only things, organizations and people but also events. For example, some of the event targets used in TREC 2005 were : "Miss Universe 2000 crowned", "France wins World Cup in soccer", and "Crash of Egypt Air Flight 990". Clearly, answers to the "other" question for the event targets cannot rely on the patterns created to extract definition information.</p><p>In our system we did not make a distinction between event targets and targets requiring definition-related nuggets. We applied the same technique to extract potential answers to the "other" question for all the targets. Besides, we did not use any external corpora but extracted potential answers from the list of ranked documents provided by NIST. Our procedure of answer extraction had two stages: (i) gather statistics about word triplet co-occurrences from the documents provided for each target by NIST; (ii) extract text snippets corresponding to the most frequent word triplets.</p><p>Word triplets. We analyzed only those top 50 documents which were provided by NIST for each target. Using BBN IdentiFinder <ref type="bibr" coords="6,106.17,438.84,76.32,8.77" target="#b1">(Bikel et al., 1999)</ref> we identified the named entities present in those documents. We used the named entities of the following types: DATE, LOCATION, ORGANIZATION, PERSON, and TIME. We also identified all the nouns and verbs using part-of-speech tagger from Alembic Workbench <ref type="bibr" coords="6,300.22,462.09,71.84,8.77" target="#b7">(Day et al., 1997)</ref> and incorporated the ten most frequent nouns into the list of named entities. We used HFNN tag for them in the examples listed in Table <ref type="table" coords="6,448.01,473.34,3.22,8.78" target="#tab_1">2</ref>.. From the target related documents we extracted all the pairs of named entities that appeared within one sentence. We preserved only those pairs of named entities, where at least one of the two elements was a substring of or equal to the entire target. E.g. for "1998 Nagano Olympic Games," it was enough to contain "Nagano Olympic Games." For each target we obtained a list of triplets consisting of two named entities (or frequent nouns) and a verb or a noun that appeared between those two. For each triplet in the list we obtained the number of its occurrences in the top 50 documents provided by NIST for the target. Table <ref type="table" coords="6,518.51,531.09,4.88,8.78" target="#tab_1">2</ref> presents two triplets extracted for the target "OPEC". Sentence selection. We preserved only the triplets that occurred more than once in the top 50 documents provided by NIST for the target. For each triplet, we chose the longest sentence containing this triplet. If the sentence was longer than 250 characters then we truncated it at the nearest punctuation mark or the beginning of the sentence to the left of the first elements of the triplet; and for the nearest punctuation mark or the end of the sentence to the right of the third elements of the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Official Results and Discussion</head><p>Though all the text snippets in our output were extracted according to different triplets, some of the text snippets contained similar (repeating) information. E.g. we output "Japan's most famous film director, Akira Kurosawa, died at his home Sunday at the age of 88, Kyodo news agency reported." and "Japan's internationally renowned film director Akira Kurosawa died Sunday at age 88." However, irrespectively of how many times a nugget judged "vital" or "OK" was repeated (e.g. "Kurosawa died at age 88"), it received credit only once, which resulted in low precision because the systems were penalized for the total length of all returned snippets. On the other side, the recall of our system was high. We believe this is because the important information was indeed typically repeated in the target collection. Overall, our technique worked well: the average F-measure for our system (0.171) was above the median average (0.156). This shows the promise of a simple redundancy based approach to answering definitional ('other') TREC questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBUST RETRIEVAL</head><p>Past experience with TREC topics indicates that while the query expansion based on blind feedback (adding the terms form the top returned documents to the query) is the mo st effective way of improving performance, it is not effective on the worst topics due to a phenomenon commonly known as "query drift:" when the top documents are irre levant, so are the added terms.</p><p>Since this year topics have been selected from the worst prior year topics, we were deliberately looking into different expansion strategies. Inspired by the success of our <ref type="bibr" coords="7,268.54,443.34,109.41,8.77" target="#b15">(Roussinov &amp; Zhao, 2003)</ref> and other researchers' <ref type="bibr" coords="7,474.91,443.34,80.53,8.77" target="#b11">(Kwok et al., 2004)</ref> work on Internet mining, we developed a method called Context Specific Similarity Discovery (CSSD), the details of which can be found in <ref type="bibr" coords="7,118.31,466.59,94.51,8.77">(Roussinov et al., 2005)</ref>. The idea behind the method is to identify the concepts (single word terms or phrase) that are highly related to the topic (query). We believe using the Internet for this purpose provides much more data for statistical analysis compared to using only the target collection itself (e.g. Aquaint) when it is implemented using blind (pseudo) relevance feedback and its variants.</p><p>In this year TREC, we submitted our query to Google and built so called Internet language model for it. We designed and trained a special formula for the probability of being related to the topic using logistic regression and proceeded through the following steps:</p><p>Step 1. The proper combination of the title and description was merged into a single query and sent to Google.</p><p>Step 2. The full text of the top 200 pages returned by Google was downloaded as the mining corpus (the "ore").</p><p>Step 3. Each term (a sequence of up to 3 consecutive words) in the mining corpus (ore), was assigned the probability of being "related to the topic" by approximating the logistic regression on the deviation from the randomness when the values of this probability was approaching 1, specifically as following:</p><formula xml:id="formula_0" coords="7,90.00,634.59,107.99,8.77">Pr(t) = 1 -exp (-(s -1) / a)</formula><p>, where s = signal to noise ratio of the term, estimated as: s = (df m / N m ) / (df w / W), where df m was the number of occurrences of the term in the mining corpus, N m was the number of pages in the mining corpus, df w was the number pages on the Web in which the terms occurs, obtained by querying Google, W was the total number of pages covered by Google, set to 3,000,000,000 at the time, df m / N m represented "signal", while df w / W represented the "noise." For the non related term, we would expect the proportion of the pages within the mining corpus that have this term to be the same as the proportion of the pages having this term on the entire Web. The ratio of those two proportions represented the deviation from randomness within the mining corpus.</p><p>The adjustment parameter a defined how "steep" the probability curve was relatively to the signal to noise ratio. We set a to .5 by visually inspecting the related concepts for the different topics from the preceding years. With this value, the signal to noise ratio of 1.5 would give the probability of 1 -exp (-1) = .63. The signal to noise ratio of 2.5 would result in p = .86, etc. In this application, the outcome was not sensitive to the value of parameter a since we only needed to select the top most deviated from the background terms and did not need the actual probability estimate. However, it would still be needed for a more general application of the approach, e.g. as in <ref type="bibr" coords="8,261.00,201.09,75.56,8.78" target="#b0">Belkin at al. (2005)</ref>. Table <ref type="table" coords="8,78.39,402.09,4.88,8.78" target="#tab_3">4</ref> summarizes our official robust runs. We used two models for expanding the query: Linear and Structured Query.</p><p>Linear model used the baseline created by BM25 retrieval model from Lemur 3.1 package with the default parameters enhanced with the pseudo relevance feedback with the parameters estimated based on Robust 2004 topics: feedbackDocCount = 20, feedbackCoefficient = .3, feedbackTermCount = 100). To expand the original query, we implemented our own module using the available C++ source in Lemur package. The top 1000 documents from the baseline were re-ranked according to the following score: score = original score + expansion coefficient * expansion score, where the expansion score was obtained using BM25 ranking with the default parameters for the query consisting only of the top 10 mined terms obtained after step 3 described above.</p><p>Structured Query used StructQueryEval application from Lemur 3.1 combining all the expanded terms under a single #swum operator and the specified expansion coefficient.</p><p>Overall, our results were very encouraging: our "description only" run was the best among all participants; our "title + description" runs were well above the median across all participants. Our "title only" run was below the median, however we conjecture that some groups mislabeled their "title + description" runs as "title only", which seems to be the only explanation why the best "title only" score and "title + description" scores were identical up to the all 4 digits reported. Since a limited number of official runs to submit does not allow to test the importance of each of the technique and parameter involved, we run additional tests varying the expansion factor. Figure <ref type="figure" coords="9,334.14,266.34,4.88,8.78">6</ref> illustrates how Internet-based expansion improves the baseline obtained by using BM25 combined with pseudo relevance feedback (PRF). Since this year topics were a subset of last year topics, we were able to run some additional tests with this year topics and last year judgments. The PRF parameters were optimized first, then the Internet-based expansion was applied as described above under "Linear Model." The expansion achieved additional improving even on top of PRF baseline, with the peak improvement at approximately 8%. Figure <ref type="figure" coords="9,82.76,531.84,4.88,8.78" target="#fig_3">7</ref> shows the mean average precision (MAP) as the function of the expansion factor when the mining-based expansion was applied to the weak (no PRF) baseline. The maximum value of .247 was achieved at the expansion factor of .5. The optimal improvement was statistically significant at the level of alpha &lt;.05. Figure <ref type="figure" coords="10,82.68,259.59,4.88,8.78" target="#fig_4">8</ref> shows the MAP as the function of the expansion factor when the mining-based expansion was applied to the (strong) baseline. The maximum value of .298 was achieved at .2. The maximum improvement was very small and not statistically significant at the level of alpha =.1. For large expansion factors the effect was negative possibly due to too much drift from the original query. However, the prior research (e.g. <ref type="bibr" coords="10,110.51,512.34,92.85,8.77">Roussinov et al., 2005)</ref> and our experience with 2004 test sets indicated that the range up to .4 is always "safe", with the peak typically around .1-.2. Same behavior of the safe and optimal values have been noticed with the PRF technique as well. In general, determining the exact values of the optimal expansion factor in practical applications and while testing research hypothesis can be done using machine learning paradigms (e.g. <ref type="bibr" coords="10,344.81,546.84,96.32,8.77">Roussinov &amp; Fan, 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS AND FUTURE DIRECTIONS</head><p>From the competitive aspect, our results are encouraging: in all the (sub) categories that we entered (factoid, other, document ranking, robust) our runs are above the median performance across all participants, with QA document ranking ranked #6 and Robust runs being well over the mean. In the "description only" condition within Robust retrieval our run was the best among all participants. Our redundancy based Internet mining technology was also beyond several runs submitted by Rutgers group <ref type="bibr" coords="10,54.00,638.34,80.97,8.77" target="#b0">(Belkin et al., 2005)</ref>, with also good performance. Based on those results and the algorithmic approaches described in this paper we conclude that redundancy based approaches to question answering and document retrieval are likely to be an interesting research direction, the conclusion that is well in line with the modern understanding that large amounts of training data are crucial for successful machine learning applications, possibly even more important than the choice of algorithms or investments into manually codified knowledge. Specifically, we have been able to capitalize on the vast amount of textual data available on the Web and made accessible by commercial search engines. The topical diversity of this data allowed to easily find thousands of pages about each topic or target and successfully analyze it to mine for the answers (as repeated patterns) or for the suitable expansions for the topic queries (as a set of more frequent than in background terms). At the moment, we are working on a number of extensions to the reported work, specifically: designing accurate expansion techniques based on language models, making answer patterns more "semantic," and trying the developed technology within practical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,54.00,325.59,158.90,8.77;2,270.00,325.59,171.55,8.77;2,86.25,105.00,192.75,210.00"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of START output. Figure 2. Example of Btainboost output.</figDesc><graphic coords="2,86.25,105.00,192.75,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,54.00,405.84,171.01,8.77;3,270.00,405.84,162.01,8.77;3,81.75,164.25,186.75,232.50"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example of Ask Jeeves output.Figure 4. Example of ASU QA output.</figDesc><graphic coords="3,81.75,164.25,186.75,232.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,279.00,59.00,238.21,9.02"><head>Figure</head><label></label><figDesc>Figure 5. Fact Seeking Meta Engine: How it Works</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,54.00,514.35,467.24,8.10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Improving the performance over the weak baseline by mining-based expansion (based on year 2005 judgments).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,54.00,230.34,501.77,8.78;10,54.00,242.34,50.19,8.78"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Improving the performance over the strong baseline by mining -based expansion (based on year 2005 judgments).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,59.25,483.09,493.31,8.77"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Improving the performance over the weak baseline by mining-based expansion. The year 2004 judgments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,54.00,438.84,502.93,191.78"><head>Table 1 . The fact seeking services involved, their characteristics and performances in the evaluation on the 2004 questions. * and ** indicate 0.1 and .05 levels of statistical significance of the difference from the best accordingly.</head><label>1</label><figDesc></figDesc><table coords="2,273.00,438.84,283.93,20.02"><row><cell>Output Format</cell><cell>Organization/System Performance in our</cell></row><row><cell></cell><cell>evaluation (MRR)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,54.00,542.34,503.29,97.27"><head>Table 2 . A sample of word triplets for target 128 "OPEC".</head><label>2</label><figDesc>Table 3 presents two triplets extracted for the target "1998 Nagano Olympic Games."</figDesc><table coords="6,115.50,594.84,262.40,44.77"><row><cell>Count</cell><cell>Triplet</cell></row><row><cell>21</cell><cell>price/[HFNN] -barrel/NN -OPEC/[ORGANIZATION]</cell></row><row><cell>11</cell><cell>OPEC/[ORGANIZATION] -world/NN -oil/[HFNN]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,115.50,83.85,320.21,63.01"><head>Table 3 . A sample of word triplets for target 96 "1998 Nagano Olympic Games".</head><label>3</label><figDesc></figDesc><table coords="7,115.50,102.09,236.96,44.78"><row><cell>Count</cell><cell>Triplet</cell></row><row><cell>7</cell><cell>City/[HFNN] -host/VB -Games/[HFNN]</cell></row><row><cell>5</cell><cell>Nagano/[LOCATION] -get/VB -games/[HFNN]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,61.50,219.09,481.39,173.77"><head>Table 4 ASU Official Robust Runs.</head><label>4</label><figDesc></figDesc><table coords="8,69.75,219.09,473.14,156.52"><row><cell>Tag</cell><cell>Original Query</cell><cell>Expansion</cell><cell>Expansion</cell><cell>Geometric</cell><cell>Best</cell><cell>Median</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>Coefficient</cell><cell>MAP over</cell><cell>Geometric</cell><cell>Geometric MAP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>all topics</cell><cell>MAP of all</cell><cell>of all participants</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>participants</cell><cell></cell></row><row><cell>ASUBE</cell><cell>Title + Desc</cell><cell>Linear</cell><cell>.1</cell><cell>0.1840</cell><cell>0.2326</cell><cell>0.1256</cell></row><row><cell>ASUBE3</cell><cell>Title + Desc</cell><cell>Linear</cell><cell>.3</cell><cell>0.1772</cell><cell>0.2326</cell><cell>0.1256</cell></row><row><cell>ASU DIV</cell><cell>Title + Desc</cell><cell>Structured</cell><cell>.3</cell><cell>0.1400</cell><cell>0.2326</cell><cell>0.1256</cell></row><row><cell></cell><cell></cell><cell>Query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASUDE</cell><cell>Desc</cell><cell>Linear</cell><cell>.3</cell><cell>0.1784</cell><cell>0.1784</cell><cell>0.1028</cell></row><row><cell>ASUTI</cell><cell>Title</cell><cell>Structured</cell><cell>.3</cell><cell>0.0616</cell><cell>0.2326</cell><cell>0.1293</cell></row><row><cell></cell><cell></cell><cell>Query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,54.00,70.28,503.97,175.58"><head>Improving the p erformance of the baseline obtained by pseudo relevance feedback using Internet-based expansion tested on 2004 collection and judgments.</head><label></label><figDesc></figDesc><table coords="9,54.00,70.28,272.01,164.33"><row><cell></cell><cell>0.154</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.152</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP</cell><cell>0.144 0.146 0.148</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.142</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.138</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>1.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Expansion Factor</cell><cell></cell></row><row><cell cols="2">Figure 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,60.00,705.09,117.64,8.77"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was partially supported by <rs type="institution">Dean's Award of Excellence, W.P. Carey School of Business, Arizona State University</rs>, Summer 2005.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,54.00,187.59,503.94,8.78;11,72.00,198.84,437.30,8.78" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,527.25,187.59,30.69,8.78;11,72.00,198.84,231.22,8.78">Rutgers Information Interaction Lab at TREC 2005: Trying HARD</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwizdka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,322.75,198.84,107.16,8.78">proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005">Nov. 15-18, 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,216.84,503.62,8.78;11,72.00,228.09,42.58,8.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,265.75,216.84,169.62,8.78">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,444.00,216.84,74.48,8.78">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,245.34,503.31,8.78;11,72.00,257.34,299.81,8.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,228.73,245.34,205.54,8.78">Personalized Spiders for Web Search and Analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,444.75,245.34,112.56,8.78;11,72.00,257.34,128.17,8.77">Proceedings of the 1st Joint Conference on Digital Libraries</title>
		<meeting>the 1st Joint Conference on Digital Libraries<address><addrLine>Roanoke, Virginia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">2001. June 2001</date>
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,274.59,503.13,8.77;11,72.00,285.84,349.24,8.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,263.97,274.59,242.89,8.77">MetaSpider: Meta-searching and Categorization on the Web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,516.00,274.59,41.13,8.77;11,72.00,285.84,265.68,8.77">Journal of the American Society for Information and Science and Technology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1134" to="1147" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,303.84,33.94,8.77;11,108.95,303.84,10.86,8.77;11,140.82,303.84,26.59,8.78;11,188.43,303.84,14.32,8.78;11,223.84,303.84,44.90,8.77;11,289.83,303.84,29.00,8.77;11,339.93,303.84,47.52,8.78" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,140.82,303.84,26.59,8.78;11,188.43,303.84,14.32,8.78;11,223.84,303.84,39.91,8.78">Search For Tomorrow</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Claburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Information</note>
</biblStruct>

<biblStruct coords="11,408.55,303.84,25.55,8.77;11,455.19,303.84,25.83,8.78;11,502.12,303.84,12.38,8.77;11,535.59,303.84,22.32,8.77;11,72.00,315.09,323.90,8.77" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Week</surname></persName>
		</author>
		<ptr target="http://www.informationweek.com/story/showArticle.jhtml?articleID=159905922" />
		<imprint>
			<date type="published" when="2005-03-28">March 28, 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,332.34,503.90,8.77;11,72.00,344.34,258.81,8.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,306.00,332.34,122.28,8.77">Predicting query performance</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,451.57,332.34,106.34,8.77;11,72.00,344.34,230.56,8.77">Proceedings of the ACM Conference on Research in Information Retrieval (SIGIR)</title>
		<meeting>the ACM Conference on Research in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,361.59,503.43,8.77;11,72.00,372.84,289.69,8.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,424.04,361.59,133.39,8.77;11,72.00,372.84,119.37,8.77">Mixed-Initiative Development of Language Processing Systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aberdeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kozierok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vilain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,208.50,372.84,148.74,8.77">Proceedings of the ANLP Conference</title>
		<meeting>the ANLP Conference</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,390.84,503.93,8.77;11,72.00,402.09,486.94,8.77;11,72.00,413.34,133.31,8.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<title level="m" coord="11,299.36,390.84,258.57,8.77;11,72.00,402.09,483.21,8.77">Web Question Answering: Is More Always Better? Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08-11">2002. August 11-15</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,431.34,504.31,8.77;11,72.00,442.59,159.19,8.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,241.57,431.34,299.58,8.77">Domain -independent detection, extraction, and labeling of Atomic Events</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,72.00,442.59,154.74,8.77">Proceedings of the RANLP Conference</title>
		<meeting>the RANLP Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,459.84,502.55,8.77;11,72.00,471.84,483.83,8.77;11,72.00,483.09,47.27,8.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,227.18,459.84,329.37,8.77;11,72.00,471.84,50.39,8.78">A case for interaction: A study of interactive information retrieval behavior and effectiveness</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Koenemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,143.30,471.84,330.78,8.77">Proceedings of the Human Factors in Computing Systems Conference (CHI&apos;96)</title>
		<meeting>the Human Factors in Computing Systems Conference (CHI&apos;96)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,500.34,503.65,8.77;11,72.00,512.34,448.69,8.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,343.22,500.34,209.43,8.77">TREC2004 Robust Track Experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,173.73,512.34,211.85,8.77">Proceedings of the Twelve Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Twelve Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2004. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,529.59,503.94,8.77;11,72.00,540.84,192.64,8.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,115.50,529.59,244.67,8.77">Evaluation of Resources for Question Answering Evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,366.75,529.59,191.19,8.77;11,72.00,540.84,164.44,8.77">Proceedings of ACM Conference on Research and development in information retrieval</title>
		<meeting>ACM Conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,558.84,501.96,8.77;11,72.00,570.09,311.93,8.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,261.75,558.84,263.23,8.77">Experiments with Web QA System and TREC2004 Questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,543.96,558.84,12.00,8.78;11,72.00,570.09,131.69,8.77">the proceedings of TREC conference</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-11-16">2004. November 16-19, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,587.34,502.74,8.77;11,72.00,599.34,214.69,8.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,206.91,587.34,268.43,8.77">Discretization Based Learning Approach to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,497.25,587.34,59.49,8.77;11,72.00,599.34,210.34,8.77">proceedings of 2005 Conference on Human Language Technologies</title>
		<meeting>2005 Conference on Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,616.59,504.02,8.77;11,72.00,627.84,161.77,8.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,214.96,616.59,295.97,8.77">Automatic Discovery of Similarity Relationships through Web Mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,523.50,616.59,34.52,8.78;11,72.00,627.84,63.86,8.77">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="149" to="166" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,645.84,503.85,8.77;11,72.00,657.09,286.69,8.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,238.72,645.84,313.81,8.77">Mining Context Specific Similarity Relationships Using The World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roussinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fan</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,82.69,657.09,271.66,8.77">proceedings of 2005 Conference on Human Language Technologies</title>
		<meeting>2005 Conference on Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,674.34,502.74,8.77;11,72.00,686.34,267.60,8.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,202.92,674.34,250.86,8.77">Multi-Service Search and Comparison using the MetaCrawler</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Selberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,463.50,674.34,93.24,8.77;11,72.00,686.34,115.01,8.77">Proceedings of the 4th World Wide Web Conference</title>
		<meeting>the 4th World Wide Web Conference<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-12">1995. December 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,54.00,703.59,432.70,8.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,143.25,703.59,172.37,8.77">Fusion Via a Linear Combination of Scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,322.50,703.59,86.94,8.77">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,54.00,56.34,453.94,8.78" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</author>
		<title level="m" coord="12,241.50,56.34,241.15,8.78">Proceedings of the Twelve Text Retrieval Conference TREC</title>
		<meeting>the Twelve Text Retrieval Conference TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,54.00,73.59,467.44,8.78" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</author>
		<title level="m" coord="12,241.50,73.59,254.89,8.78">Proceedings of the Thirteenth Text Retrieval Conference TREC</title>
		<meeting>the Thirteenth Text Retrieval Conference TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
