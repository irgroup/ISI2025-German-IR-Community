<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,87.84,96.78,418.98,15.49">The University of Sheffield&apos;s TREC 2005 Q&amp;A Experiments</title>
				<funder ref="#_vxkXuAR">
					<orgName type="full">UK Engineering and Physical Sciences Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.60,134.21,87.75,10.76"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
							<email>r.gaizauskas@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.57,134.21,96.17,10.76"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
							<email>m.greenwood@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.61,134.21,69.74,10.76"><forename type="first">Henk</forename><surname>Harkema</surname></persName>
							<email>h.harkema@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.56,148.25,61.05,10.76"><forename type="first">Mark</forename><surname>Hepple</surname></persName>
							<email>m.hepple@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.97,148.25,80.58,10.76"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
							<email>saggion@dcs.shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.81,148.25,71.28,10.76"><forename type="first">Atheesh</forename><surname>Sanka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,87.84,96.78,418.98,15.49">The University of Sheffield&apos;s TREC 2005 Q&amp;A Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FDD9FD87D4DE0F7AED3DF145529B0B3B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our entries in the TREC 2005 QA evaluation continue experiments carried out as part of TREC 2004. Hence here, as there, we report work on multiple approaches to both the main and document ranking tasks. For query generation and document retrieval we explored two approaches, one based on Lucene<ref type="foot" coords="1,333.12,282.59,3.97,6.97" target="#foot_0">1</ref> , the other on MadCow, an in-house boolean search engine. For answer extractrion we have also explored two approaches, one a shallow approach based on semantic typing and question-answer word overlap, the other based on syntactic analysis and logical form matching. As well as continuing independent development of these multiple approaches, we have also concentrated on developing common resources to to be shared between these approaches in order to allow for more principled comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Common Resources</head><p>Our entries to the TREC QA evaluation in 2003 <ref type="bibr" coords="1,243.73,371.38,96.94,8.97" target="#b4">[Gaizauskas et al., 2003]</ref> and 2004 <ref type="bibr" coords="1,380.14,371.38,96.82,8.97" target="#b3">[Gaizauskas et al., 2004]</ref> were produced using two independently implemented QA systems. These systems relied on independently developed resources which, while containing much that was duplicated, did not fully overlap. This made it difficult and unfair to compare directly the performance of our two systems. Many of these resource have now been unified to provide a single basic knowledge store from which the approaches can draw information. This work is documented in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AQUAINT Indexing</head><p>Lucene: Two of our three runs use Lucene to index and access the AQUAINT collection. In previous years the approaches to index creation were not consistent and so later processing was in some cases carried out over different document sets making comparisons between our approaches problematic at best. For our runs which use Lucene to access the AQUAINT collection a single document processing and indexing approach has now been adopted. Each document is split into separate paragraphs using the embedded SGML paragraph tags. All remaining SGML tags are then removed and each paragraph is added to the Lucene index along with the unique document ID and associated date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MadCow:</head><p>To improve are results using the MadCow boolean search engine we implemented a semantic filter which discards documents that do not contain an entity of the same type as the expected answer type. Determining the expected answer type is a two stage process.</p><p>In the first stage the question is parsed using SUPPLE <ref type="bibr" coords="1,268.93,583.53,97.18,8.97" target="#b2">[Gaizauskas et al., 2005]</ref> which employs specific question grammars that output in the final semantic representation of the question a unary question predicate referred to as the qvar. The second stage maps the qvar to one of the known semantic entity types using mostly hand-crafted lookup tables. The result is a semantic type which represents the expected answer type (EAT) of the question. Table <ref type="table" coords="1,438.04,613.41,4.98,8.97" target="#tab_0">1</ref> contains some example questions along with their qvar and EAT. Consider the question "What city is Disneyland in?", SUPPLE determines that the qvar is city. The qvar is then mapped through lookup tables which allows us to determine that the correct EAT for a qvar of city is Location:locType=city. If the question grammar is unable to find a question variable then the EAT for the question is null. The EAT is classified as either general or specific. General EATs are those that specify just a high-level semantic type, for example Organization, Location, Person. Specific EATs are those which specify a lower-level semantic type or an attribute of a high-level type as well. For example Measurement:kind=number, Organization:orgType=company, Location:locType=city, and Person:gender=male are all specific EATs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Target and Question Processing</head><p>Both our approaches to QA assume that each question is asked and answered independently of any other. This is different to the current TREC approach where a target is first supplied and then a set of questions is asked related to that target.</p><p>In 2004 we used two simple approaches to deal with merging targets and questions: pronoun replacement and appending the target to the question. Neither method was ideal and both failed to produce acceptable results. For the 2005 evaluation we adopted a single approach based on both pronominal and nominal coreference resolution.</p><p>For example consider the seven questions for target 75, Merck &amp; Co., and the processed questions which result: Note that this approach does not always result in a independent question, for example question 75.5 cannot be answered without reference to the target. In cases such as these the target is simply appended to the question to enable relevant documents to be located. It should be clear, however, that in the other questions the target has been successfully inserted into the question including the addition of possessives where necessary.</p><p>In this year's test set 40 of the 455 factoid and list questions could not be modified to insert the target. This has consequences during retrieval and answering of the question. The insertion failed mainly because the reference to the target was made by a nominal expression or an ellipsis instead of a pronominal expression (the first flight for space shuttles or the center for Berkman Center for Internet and Society). Of the remaining 405 questions whilst the target was inserted in the question an a few questions this resulted in badly formed or misleading questions. For example question 70.4 was What was the affiliation of the plane? for the target Plane clips cable wires in Italian resort for which our approach produced the question What was the affiliation of Plane clips cable wires in Italian resort?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Entity Detection and Normalization</head><p>These merged resources include gazetteer lists and semantic entity recognisers which together allow us to recognise a large number of distinct entity types in free text. This both extends our ability to recognise semantic entities and provides a solid foundation upon which our multiple strategies can be built.</p><p>Whilst this work provides a solid foundation for our two QA systems it does not address the problem of there being multiple ways of representing identical pieces of information. The answers to many questions can be represented in many ways and as most QA systems rely at least in part on the frequency of occurrence of competing candidate answers being able to accurately compare candidate answers is important. To this end all dates and numbers were normalised to a standard format. Dates are all converted to a standard numerical format including resolving partial or descriptive dates (such as today or tomorrow) against the date of the newswire article. Numbers, both isolated and within measurements, are converted to a plain numeric form, i.e. 3000, 3,000, and three thousand are all represented as 3000.</p><p>3 Approaches to Answering Factoid and List Questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Shallow Multi-Strategy Approach</head><p>Originally introduced as a baseline system for comparison with our main entry in TREC 2003 <ref type="bibr" coords="2,435.24,702.58,97.54,8.97" target="#b4">[Gaizauskas et al., 2003]</ref>, our shallow multi-strategy approach (SMS) has continued to be improved and is now no longer considered a baseline system. The systems was described in some detail by <ref type="bibr" coords="2,269.04,722.49,95.53,8.97" target="#b3">Gaizauskas et al. [2004]</ref> and so we will concentrate just on detailing the main modifications to the system.</p><p>Expanding the Question Hierarchy Using WordNet: Whilst expanding the answer type hierarchy using WordNet proved useful in our TREC 2004 experiments a number of problems did arise. The main issue was that some entries in WordNet should not be used directly to find answers. For example words such as researchers, soldiers, chemists, etc.</p><p>should not be used directly but should instead be linked back to the Person type within the answer hierarchy. For the current evaluation the WordNet expansion has been tightly integrated with the question hierarchy to enable this mapping, which should increase the performance of the approach.</p><p>Just Guess the Answers: As the TREC guidelines state that all list questions are known to have answers within the AQUAINT collection those systems which cannot find an answer and are therefore forced to return a dummy response not to be penalised. In this case the best strategy is to simply guess a number of answers. This is because given the evaluation metric there is no difference between returning a single wrong answer or 100 wrong answers. When our approach fails to find an answer to a list question it guesses answers by assuming that correct answers will occur frequently in relevant documents and that they will be fully contained within noun phrases. Twenty hopefully relevant documents are retrieved and all noun chunks are extracted from them using a version of the <ref type="bibr" coords="3,419.66,171.45,116.96,8.97" target="#b7">Ramshaw and Marcus [1995]</ref> base NP chunker<ref type="foot" coords="3,124.44,180.71,3.97,6.97" target="#foot_1">2</ref> .</p><p>The noun phrases are then clustered by assuming that two noun phrases are equivalent if the non-stopwords in one are all present in the other. The longest phrase is then used to represent the cluster. These clusters are ranked using a scoring function (the same as that used to rank the answers to factoid questions). Given that a unique answer a to question q has been seen C a times by the answer extraction component within the retrieved documents the most likely of which occurred in sentence s then the answer is scored using the equation:</p><formula xml:id="formula_0" coords="3,239.16,253.11,300.10,23.52">score(a, q, s) = C a * |q ∩ s| |q| (1)</formula><p>This scoring function takes into account the fact that it is more likely that a correct answer will not only appear frequently in relevant documents but will also come from a sentence which contains many (if not all) the question words.</p><p>If less than ten clusters are found then all are returned as answers to the list question otherwise the first ten chunks are returned along with any others which have a score above 0.08 (chosen by empirical testing over questions from previous TREC evaluations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching on Logical Forms</head><p>QA-LaSIE performs partial syntactic and semantic analysis of questions and candidate answer bearing documents and then performs matching over the derived logical form representation. The system has been described in detail in past TREC proceedings (see, e.g. <ref type="bibr" coords="3,175.52,393.81,97.00,8.97" target="#b5">Greenwood et al. [2002]</ref>) and here we will only describe modifications carried out since it last participated in TREC 2004 <ref type="bibr" coords="3,182.65,403.77,96.82,8.97" target="#b3">[Gaizauskas et al., 2004]</ref>.</p><p>Parsing with Semantic Entities: This year we made use of SUPPLE <ref type="bibr" coords="3,339.24,430.65,97.78,8.97" target="#b2">[Gaizauskas et al., 2005]</ref>, a freely-available, open source natural language parsing system, implemented in Prolog<ref type="foot" coords="3,311.04,439.91,3.97,6.97" target="#foot_2">3</ref> . Entities identified by the semantic entity detection and normalization procedure documented in Section 2.3 are passed to the parser via a mapping process. The entity types we considered this year were: Building, Color, Date, Email, Location, Measurement, Money, Organization, Person, and Quote. These semantic entities are mapped into noun phrases with specific semantics. As an illustration, the expression 23 inches is mapped into a noun phrase with the following semantics: measurement(e1), count(e1,23), measurement type(e1,distance), name(e1,'23 inches')</p><p>Answer Ranking: The answer scoring mechanism this year uses document ranking in addition to the score we used in previous years <ref type="bibr" coords="3,117.01,562.90,97.66,8.97" target="#b3">[Gaizauskas et al., 2004]</ref>. Document rank information is used when two answers have the same sore, the answer found in a document with lower rank -thus more relevant -is proposed first.</p><p>4 Approaches to Answering 'Other' Questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Bare Target + Filter + Reduce Approach</head><p>This system, introduced and described in <ref type="bibr" coords="3,220.76,638.62,94.11,8.97" target="#b3">Gaizauskas et al. [2004]</ref>, was used almost unchanged from the system evaluated in TREC 2004 -the only changes being minor bug fixes.</p><p>This system assumes that each nugget can be fully contained within a single sentence and so sentences are selected from the corpus only if they contain the target as it appears in the question; no use of coreference was made to increase the number of matching sentences. Each sentence was retained if it did not overlap more than 70% with any sentence already in the definition. The process stopped either when there were no more sentences to process or the definition had reached 4000 characters in length. This approach while effective still results in much repetition within the resulting definitions.</p><p>In an attempt to remove more of the redundant sentences from the definitions a second filtering step was introduced. This second filter works by calculating the sum of the percentage overlap of increasingly longer n-grams. The n-grams considered range from length 1 (a single token) to length s which is the length of the shortest of the two sentences being compared. From limited testing a cutoff level of 50 was determined with pairs of sentences having a score above this being deemed equivalent. To increase the number of nuggets returned, rather than reduce the amount of text, the system was updated to create initially a definition of up to 5000 characters. The second filter is then applied and the resulting definition is trimmed to the first x sentences that produce a definition of 4000 characters. While filtering the sentences allows the system to remove some of the redundancy from the generated queries, it is clear that returning whole sentences still results in considerable redundant text being included in the definition. Rather than attempting to extract the salient details from the sentences we attempted to determine a number of phrases and clauses which while being redundant could also be removed from the sentence without affecting either the meaning or flow of the text. To allow the system to answer questions in real-time we only attempt to find redundant words or phrases using shallow methods which do not require intensive processing. This rules out detection of redundant phrases which would require full syntactic or semantic parsing to identify. A number of sources were consulted for possible ways in which redundant phrases could be both identified and safely removed <ref type="bibr" coords="4,341.60,184.41,84.17,8.97" target="#b1">[Dunlavy et al., 2003</ref><ref type="bibr" coords="4,425.76,184.41,110.77,8.97;4,56.64,194.37,76.54,8.97">, Purdue University Online Writing Lab, 2004]</ref>. The phrases were removed from the sentences before any filtering is applied, as previous work in summarization has shown this to be the most effective point at which to remove redundant clauses <ref type="bibr" coords="4,448.21,204.33,80.98,8.97" target="#b0">[Conroy et al., 2004]</ref>.</p><p>The words and phrases which were deemed redundant and easily removable were: imperative sentences, gerund clauses, leading adverbs, sentence initial expletives, redundant category labels, unnecessary determiners and modifiers, circumlocutions, unnecessary that and which clauses, and noun forms of verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Target Enrichment + Filter Approach</head><p>This approach to answer other questions has changed very little from the same approach we described for the 2004 TREC evaluation <ref type="bibr" coords="4,99.36,295.06,94.88,8.97" target="#b3">[Gaizauskas et al., 2004]</ref>.</p><p>The approach requires each target to be classified as a person (who question) or other type of entity (what question).</p><p>Having performed semantic entity detection on the target we used the following procedure to identify the target, its type, and any additional context for the target:</p><p>• If the target contains a person's name, then we extract the first 'named' person in the target, considering as context any text to the left and right of the named entity (e.g. for Abraham in the Old Testament the target is Abraham while in the Old Testament is the context). The question is considered to be of type who.</p><p>• If the target contains an organization's name, then the first organization is extracted, and the text to the right and left of the target is considered to be the context (e.g., for ETA in Spain the target is ETA while in Spain is the context).</p><p>The question is considered of type what.</p><p>• Otherwise the less discriminative word of the input text is considered the target and any other words are used as context (e.g., for medical condition shingles the target is shingles and the context is medical condition). The question is considered to be of type what.</p><p>When searching the web for definitional passages using the approach described by <ref type="bibr" coords="4,392.22,470.85,125.83,8.97" target="#b9">Saggion and Gaizauskas [2004]</ref>, we use (exact) definitional patterns in the Google query (e.g.,Abraham was a) as well as the identified context (e.g. in the Old Testament).</p><p>The parameters used for the runs SHEF05MC and SHEF05LC are as follows: the maximum number of characters for an answer was 4000 bytes, the maximum number of nuggets per target was 14, and 1000 documents returned by the document retrieval system (MadCow or Lucene) were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We submitted the following three runs for evaluation in both the main and document ranking tasks and the performance of these runs is discussed in the following sections.</p><p>shef05lmg This run answers factoid and list questions using the SMS approach of Section 3.1 and the bare target, filter, and reduce approach of Section 4.1 to answer other questions. Documents are retrieved from the AQUAINT collection using Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHEF05LC</head><p>This run uses the logical form matching approach of Section 3.2 to answer factoid and list questions along with the target enrichment and filter approach of Section 4.2 to answer other questions. Documents are retrieved from the AQUAINT collection using Lucene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHEF05MC</head><p>This run is identical to SHEF05LC apart from the fact that it retrieves filtered documents from the AQUAINT collection using MadCow.</p><p>All three runs made use of the semantic entity detection and normalization of Section 2.3 and the target/question processing of Section 2.2. And all three runs used just the top twenty documents retrieved by their IR approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document Ranking Task</head><p>Before reporting the results of our three entries in the document ranking task it should be noted that whilst both shef05lmg and SHEF05LC runs were produced using Lucene the document ranking evaluations will differ. matching approach the SMS approach analyses each question to see if it can be answered before retrieving any documents, i.e. if after question analysis it is clear that the SMS approach can not answer the question then processing of the question stops. For such questions no documents were retrieved and a single dummy doc ID was returned<ref type="foot" coords="5,446.40,274.79,3.97,6.97" target="#foot_3">4</ref> . This lowers the document ranking score for shef05lmg without affect the ability to answer the questions, i.e. the difference in document ranking scores has no affect on later processing.</p><p>The full results<ref type="foot" coords="5,117.00,310.91,3.97,6.97" target="#foot_4">5</ref> for our three document ranking runs can be seen in Table <ref type="table" coords="5,352.11,312.94,3.74,8.97" target="#tab_2">2</ref>.</p><p>Interestingly Table <ref type="table" coords="5,134.25,327.94,4.98,8.97" target="#tab_2">2</ref> shows that precision and recall are not useful for comparing document retrieval runs for QA as the coverage results (where coverage at rank n is the percentage of questions for which at least one answer bearing document is contained within the top n documents retrieved <ref type="bibr" coords="5,259.41,347.85,127.21,8.97" target="#b8">[Roberts and Gaizauskas, 2004]</ref>) show that SHEF05LC is capable of answering more questions than SHEF05MC (41 compared to 40) yet has lower precision and recall. This leads us to stress the importance of choosing the correct evaluation metric and to suggest that coverage (and answer redundancy) be more widely adopted. Also contained in Table <ref type="table" coords="5,155.67,392.61,4.98,8.97" target="#tab_2">2</ref> is an evaluation of MadCow without filtering. This shows that performing filtering increases the coverage of the retrieved documents (76% to 78%) which it is hoped will improve the end-to-end performance of the QA approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Task</head><p>Table <ref type="table" coords="5,81.48,458.38,4.98,8.97" target="#tab_3">3</ref> shows the results of our three entries at the factoid, list, and other questions as well as the combined per-series score. Only limited failure analysis of the three runs has as yet been performed: shef05lmg: There were 8 targets for which shef05lmg was unable to answer any of the questions (i.e. the series score was 0) although no analysis has yet been undertaken to see if there is a pattern in these failings. For 15 of the other questions whilst at least one nugget was found, no vital nuggets were found. Thus, the score for these questions was zero, even though useful information had been found. This clearly effects the score of this run for both the question's own score and the per-series score.</p><p>SHEF05LC: There were 16 targets for which SHEF05LC was unable to answer any of the questions and 10 other questions for which nuggets were found but no vital nuggets were returned giving a score of 0. Again, no analysis of these failures has yet been conducted.</p><p>SEHF05MC: There were 16 targets for which SHEF05MC was unable to answer any of the questions (no comparison with the 16 targets for which SHEF05LC was unable to answer any questions has yet been carried out) and 5 other questions for which nuggets were found but no vital nuggets were returned giving a score of 0. Again no analysis of these failures has yet been conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Much more analysis of results needs to be carried out before firm conclusions can be drawn. However, certain observations are worth making now. First, while the document ranking task showed that using Lucene gave better coverage (i.e. more questions could be answered), using MadCow actually resulted in higher scores across all three question types. This clearly shows that performing document retrieval for use within question answering systems is a complex topic which requires further detailed investigation.</p><p>Secondly, both our approaches to answering other questions performed worse than expected (approximately half the score obtained in the TREC 2004 evaluations <ref type="bibr" coords="6,218.68,84.81,98.50,8.97" target="#b3">[Gaizauskas et al., 2004]</ref>. We imagine that this is due to the inclusion of event targets which were more complex than we expected.</p><p>Finally, having standardised many tasks and resources common to our approaches in order to allow us to compare approaches fairly, it seems that the SMS approach to answer extraction consistently outperforms the logical form matching approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,85.44,58.74,425.34,109.21"><head>Table 1 :</head><label>1</label><figDesc>Questions with qvar predicates and EAT annotation</figDesc><table coords="2,85.44,58.74,425.34,87.51"><row><cell>Question</cell><cell>qvar</cell><cell>Exact Answer Type</cell></row><row><cell>1894 How far is it from Earth to Mars?</cell><cell>measure</cell><cell>Measurement:kind=number</cell></row><row><cell>1898 What city is Disneyland in?</cell><cell>city</cell><cell>Location:locType=city</cell></row><row><cell cols="3">1909 What business was the source of John D. Rockefeller's fortune? business Organization</cell></row><row><cell>1924 When was the first hair dryer made?</cell><cell>date</cell><cell>Date:kind=date</cell></row><row><cell>1935 What color is the top stripe on the U.S. flag?</cell><cell>color</cell><cell>Color</cell></row><row><cell>1950 Who created the literary character Phineas Fogg?</cell><cell>person</cell><cell>Person</cell></row><row><cell>13.2 What actor is used as Jar Jar Binks voice?</cell><cell>actor</cell><cell>Person:gender=male</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,436.96,779.26,99.77,8.97"><head>Table 2 :</head><label>2</label><figDesc>Unlikely the logical form Summary of results from our three document ranking entries.</figDesc><table coords="5,130.68,58.66,334.60,153.69"><row><cell></cell><cell cols="2">Retrieved</cell><cell cols="2">Known</cell><cell></cell><cell></cell><cell>% Coverage</cell></row><row><cell>Run Tag</cell><cell cols="7">Total Relevant Relevant Precision Recall At Rank 20</cell></row><row><cell>shef05lmg</cell><cell>789</cell><cell>155</cell><cell></cell><cell>1575</cell><cell>0.196</cell><cell>0.098</cell><cell>70</cell></row><row><cell>SHEF05LC</cell><cell>883</cell><cell>186</cell><cell></cell><cell>1575</cell><cell>0.211</cell><cell>0.118</cell><cell>82</cell></row><row><cell>SHEF05MC</cell><cell>937</cell><cell>216</cell><cell></cell><cell>1575</cell><cell>0.231</cell><cell>0.137</cell><cell>78</cell></row><row><cell>MadCow</cell><cell>1000</cell><cell>219</cell><cell></cell><cell>1575</cell><cell>0.219</cell><cell>0.139</cell><cell>76</cell></row><row><cell></cell><cell>Run Tag</cell><cell cols="2">Factoid</cell><cell>List</cell><cell cols="2">Other Combined</cell></row><row><cell></cell><cell cols="2">shef05lmg</cell><cell>0.202</cell><cell cols="2">0.076 0.160</cell><cell>0.165</cell></row><row><cell></cell><cell>SHEF05LC</cell><cell></cell><cell>0.110</cell><cell cols="2">0.035 0.158</cell><cell>0.103</cell></row><row><cell></cell><cell>SHEF05MC</cell><cell></cell><cell>0.116</cell><cell cols="2">0.039 0.172</cell><cell>0.114</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,175.08,225.10,244.60,8.97"><head>Table 3 :</head><label>3</label><figDesc>Summary of results from our three main task entries.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,71.64,729.87,149.99,7.83"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,71.64,750.95,467.98,9.91"><p>We use the Java re-implementation available from http://www.dcs.shef.ac.uk/ ∼ mark/phd/software/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,71.64,765.22,466.83,8.97;3,71.64,775.18,223.89,8.97"><p>http://nlp.shef.ac.uk/research/supple. This is essentially the same parser as used before, but it has been packaged up and made available to the community.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,71.64,740.50,420.04,8.97"><p>We simply returned the first document in the APW section of the collection namely APW19980601.0003.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,71.64,754.78,465.66,8.97;5,71.64,764.74,465.27,8.97;5,71.64,774.69,281.90,8.97"><p>The results for SHEF05MC differ slightly from the official results. On rare occasions MadCow allowed more than 20 docs to be returned for a question. These documents were not used in later processing and hence inflate the document ranking scores while obscuring the actual data used in later processing.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Our thanks to the <rs type="funder">UK Engineering and Physical Sciences Research Council</rs> for funding this research through their studentships programme and under research grant <rs type="grantNumber">GR/R91465/01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vxkXuAR">
					<idno type="grant-number">GR/R91465/01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,56.64,230.26,480.51,8.97;6,66.60,240.22,383.83,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,375.72,230.26,161.43,8.97;6,66.60,240.22,58.85,8.97">Left-Brain/Right-Brain Multi-Document Summarization</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><forename type="middle">D</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>O'leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,143.91,240.22,227.06,8.97">Proceedings of the Document Understanding Conference</title>
		<meeting>the Document Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,258.22,480.24,8.97;6,66.60,268.18,470.21,8.97;6,66.60,278.13,306.52,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,211.31,268.18,305.00,8.97">Performance of a Three-Stage System for Multi-Document Summarization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">M</forename><surname>Dunlavy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><forename type="middle">D</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Van Halteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,66.60,278.13,277.65,8.97">Proceedings of the Document Understanding Conference (DUC 2003)</title>
		<meeting>the Document Understanding Conference (DUC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,296.01,480.66,8.97;6,66.60,305.97,303.53,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,296.42,296.01,240.89,8.97;6,66.60,305.97,64.43,8.97">SUPPLE: A Practical Parser for Natural Language Engineering Applications</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Greenwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,149.20,305.97,192.17,8.97">International Workshop on Parsing Technologies</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,323.97,480.18,8.97;6,66.60,333.93,369.78,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,421.92,323.97,114.90,8.97;6,66.60,333.93,123.17,8.97">The University of Sheffield&apos;s TREC 2004 Q&amp;A Experiments</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,208.24,333.93,199.07,8.97">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,351.81,480.29,8.97;6,66.60,361.77,469.18,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,521.44,351.81,15.49,8.97;6,66.60,361.77,222.58,8.97">The University of Sheffield&apos;s TREC 2003 Q&amp;A Experiments</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Sargaison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,307.52,361.77,199.07,8.97">Proceedings of the 12th Text REtrieval Conference</title>
		<meeting>the 12th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,379.65,480.49,8.97;6,66.60,389.61,228.26,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,300.40,379.65,218.00,8.97">The University of Sheffield TREC 2002 Q&amp;A System</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,66.60,389.61,199.19,8.97">Proceedings of the 11th Text REtrieval Conference</title>
		<meeting>the 11th Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,407.61,481.55,8.97;6,66.60,417.58,365.68,8.97" xml:id="b6">
	<monogr>
		<ptr target="http://owl.english.purdue.edu/handouts/print/general/glconcise.html" />
		<title level="m" coord="6,219.58,407.61,194.92,8.97">Conciseness: Methods of Eliminating Wordiness</title>
		<imprint>
			<date type="published" when="2004-07">July 2004. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Purdue University Online Writing Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,435.46,480.31,8.97;6,66.60,445.42,202.65,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,211.35,435.46,211.55,8.97">Text Chunking Using Transformation-Based Learning</title>
		<author>
			<persName coords=""><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,439.91,435.46,97.04,8.97;6,66.60,445.42,152.71,8.97">Proceedings of the Third ACL Workshop on Very Large Corpora</title>
		<meeting>the Third ACL Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,463.42,480.63,8.97;6,66.60,473.38,245.04,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,204.96,463.42,262.99,8.97">Evaluating Passage Retrieval Approaches for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,487.95,463.42,49.32,8.97;6,66.60,473.38,216.33,8.97">Proceedings of 26th European Conference on Information Retrieval</title>
		<meeting>26th European Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,56.64,491.25,480.16,8.97;6,66.60,501.21,135.15,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,227.67,491.25,193.15,8.97">Mining on-line sources for definition knowledge</title>
		<author>
			<persName coords=""><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,441.75,491.25,95.05,8.97;6,66.60,501.21,17.93,8.97">Proceedings of FLAIRS 2004</title>
		<meeting>FLAIRS 2004<address><addrLine>Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
