<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.84,125.04,358.33,15.15;1,241.62,146.95,128.77,15.15">Dublin City University at the TREC 2005 Terabyte Track</title>
				<funder ref="#_R6Xv2mp">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.89,179.49,70.71,10.48"><forename type="first">Paul</forename><surname>Ferguson</surname></persName>
							<email>pferguson@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing &amp; Adaptive Information Cluster</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.94,179.49,70.77,10.48"><forename type="first">Cathal</forename><surname>Gurrin</surname></persName>
							<email>cgurrin@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing &amp; Adaptive Information Cluster</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.02,179.49,87.30,10.48"><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<email>asmeaton@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing &amp; Adaptive Information Cluster</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.98,179.49,70.13,10.48"><forename type="first">Peter</forename><surname>Wilkins</surname></persName>
							<email>pwilkins@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing &amp; Adaptive Information Cluster</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Glasnevin, Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.84,125.04,358.33,15.15;1,241.62,146.95,128.77,15.15">Dublin City University at the TREC 2005 Terabyte Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F633AF1402EF11FECAA90050CE3C564</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the 2005 Terabyte track in TREC Dublin City University participated in all three tasks: Adhoc, Efficiency and Named Page Finding. Our runs for TREC in all tasks were primarily focussed on the application of "Top Subset Retrieval" to the Terabyte Track. This retrieval utilises different types of sorted inverted indices so that less documents are processed in order to reduce query times, and is done so in a way that minimises loss of effectiveness in terms of query precision. We also compare a distributed version of our Físréal search system [1][2] against the same system deployed on a single machine.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As in the 2004 Terabyte Track the experiments were run on the GOV2 collection. This is a collection of over 25 million documents crawled from the .gov domain in early 2004.</p><p>The 2005 Terabyte track consisted of three task:</p><p>1. Adhoc: The aim of this task was to investigate the performance of systems on a static set of documents with a set of previously unseen topics. For this task NIST provided the participants with 50 new topics to search for relevant documents on. Participants and asked to return a ranked set of the 10,000 most relevant documents for each of these topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Efficiency:</head><p>The aim of the efficiency task was to provide a means for comparing efficiency and scalability issues in IR systems. For this task participants were given 50,000 topics that had been mined from the search logs of an operation search engine. For each of these topics the participants reported the top 20 results, along with the total query processing time for the full set.</p><p>3. Named Page Finding: For the named-page finding task participants were given a set of 252 topics, each of which specified a document by name. The goal of the task was to return this page as close to the number 1 rank as possible. For each of the topics the participants were asked to return their top 1000 results.</p><p>In this paper section 2 will outline the search engine setup that was used to run our experiments. This will describe the different types of indices that we used as well as the two system architectures that we employed. Section 3 will describe our experiments for each of the tasks: Adhoc, Efficiency and Named page finding. Finally in section 4 we will draw conclusions from our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Físréal Search Engine Setup</head><p>Our runs for TREC 2005 in all tasks were mainly focussed on the experimentation with different types of sorted inverted indices that allow only a subset of documents associated with each term in the query to be evaluated and still retain effective search results, while at the same time processing the query much quicker (as the number of documents being considered for each term is reduced). This process of "top subset retrieval" has been explained in greater detail in <ref type="bibr" coords="2,431.66,110.80,9.22,7.86" target="#b2">[3]</ref>. This type of retrieval requires an index that sorts the document IDs and their corresponding term frequency for each term so that the most important documents are towards the top of the list for each term. For these experiments we utilised three different types of sorted indices to evaluate this process. These types of indices are described in detail in the sections 2.1.1, 2.1.2 and 2.1.3.</p><p>Another element that we wished to investigate in our experiments was whether we could utilise the text within certain HTML markup elements in the document, that when combined with standard text results could give us an improvement over the standard text only. For this we constructed separate indices for certain tags as described in section 2.1.4. We provided runs in both the adhoc and named page finding tasks to investigate this.</p><p>For these experiments (and most specifically for the efficiency task) we also wanted to compare our previously used distributed system architecture <ref type="bibr" coords="2,322.76,231.34,9.73,7.86" target="#b0">[1]</ref>[2] and compare its performance with the same system running on a single machine. An outline of both these architectures: distributed and single machine, is given in section 2.2.1 and 2.2.2 respectfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inverted Index Types</head><p>We created different inverted index files for terms occurring in documents and sorted the entries (document IDs and their corresponding term frequency) in different ways as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">BM25 Sorting</head><p>This BM25 index consists of a sorted list of Document IDs for each term by using the Okapi BM25 value <ref type="bibr" coords="2,135.10,359.68,9.22,7.86" target="#b4">[5]</ref>. This should provide an inverted index that for each term has a sorted list of document IDs and their associated term frequency, with the most influential documents for a given term at the top of this list. This type of index acts as a baseline index to compare the performance of our other indices against, as it uses a standard form of document ranking, in the postings list or in the final retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">WeightedTF Sorting</head><p>The WeightedTF index is sorted using a metric known as weightedTF sorting, which was introduced in <ref type="bibr" coords="2,148.74,455.72,9.73,7.86" target="#b2">[3]</ref> and is defined as follows:</p><formula xml:id="formula_0" coords="2,212.73,475.15,287.91,8.35">W eight tf = log(BiDistavg + e) × log(T F + e)<label>(1)</label></formula><p>where e is the base of natural logarithms and BiDistavg is a measure of the distance of each document from the average document length, calculated as:</p><formula xml:id="formula_1" coords="2,206.82,522.93,293.82,26.17">BiDistavg = dl t avg dl if dlt ≤ avg dl 1 -dl-avg dl max dl -avg dl otherwise (2)</formula><p>where avg dl is the average and max dl is the maximum document length in the collection. This sorting scheme works as follows: BiDistavg is a measure of the distance of the document length from the average document length, and is smaller the further the document length deviates from the average. When this is combined with the normalised TF to calculate W eight tf , it gives a good measure of the term's overall influence on the document. Again for each term in the inverted index the document IDs and their associated term frequencies are sorted based on this measure so that the more important documents are towards the top of the list. This provides a mechanism to process only a subset of these documents for each term in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Document ID Sorting</head><p>As described by Najork &amp; Wiener in <ref type="bibr" coords="2,266.45,675.67,9.73,7.86" target="#b3">[4]</ref> a breadth-first crawl (as used to crawl the GOV2 collection) the higher quality pages are discovered early in the crawl. Assuming this to be true we could factor this into a metric to sort documents by. The GOV2 collection is organised into 274 separate directories (000 -273) which reflect the order in which the documents were crawled i.e. the documents in the lower number directories were crawled first. The GOV2 document IDs are of the form "GX000-01-0000000" where in this case the "GX000" part of the ID represents that this came from the 000 directory (and so was found early in the crawl). When creating the index we sorted the document IDs and their associated term frequencies for each term based on the following metric:</p><formula xml:id="formula_2" coords="3,227.83,361.67,272.81,19.74">DID weight = 1 log (α + e) × log (tf + e)<label>(3)</label></formula><p>where α represents the number of the directory that the document came from, e is the base of the natural log and tf is the term frequency of the document for that particular term. This essentially gives all the documents from the same directory the same weighting, but gives an increased weighting to the folders with lower numbers (i.e. those that are crawled first) and penalises those that are crawled later. This is then combined with the normalised term frequency(tf ) of the term in the document to reflect the term's influence in the document, as well as the quality of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Tag Indices</head><p>It is believed that certain HTML tags contain text that is more representative of the content of the document than other text in the document. For example, text in the title tag would generally be more reflective of the content of the document than the body text of the document, and should therefore be given more weighting in retrieval. In order to infer this extra weighting in addition to the sorted indices that have been described we also created separate indices for the text contained in the following HTML tags: Bold, Italic, Title, Underline, Meta. We considered other tags to be included however based on our experiments on the TREC Terabyte 2004 Adhoc task we used this final set of tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Engine Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Distributed</head><p>Using a distributed architecture we distributed the GOV2 collection evenly across four machines, each being a Pentium 4, 2.6GHz with 1GB of RAM. The queries were handled by an aggregate machine which broadcast the queries to each of the four search machines (or "leaf servers"), who processed the query and send back their results to the aggregate machine. The aggregate then merged these results and produced the final output.</p><p>The interaction between each of the machines and a "user" machine can be seen from Figure <ref type="figure" coords="3,111.36,697.46,3.59,7.86" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Single Machine</head><p>In contrast to the distributed set up described previously we felt it would be an interesting comparison to search the collection using a single machine. For this we used a Pentium 4, 2.6GHz with 2GB of RAM. The entire collection was indexed on this machine and queries were run locally on this machine.</p><p>3 Terabyte Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adhoc Task</head><p>For this task we submitted four automatic runs (processed with no human intervention), using only the title text for each topic. All of these TREC runs used the distributed architecture (as in section 2.2.1). Our first run (DCU05ABM25) used a BM25 sorted index (as described in section 2.1.1). Our second run (DCU05ADID) uses a Document ID sorted index (section 2.1.3). Our third run (DCU05AWTF) uses a WeightedTF sorted index (section 2.1.2).</p><p>All the above runs achieved the same average query time of 2.4 seconds. Each of these runs processed a maximium of 100,000 documents for each term in the query using the approach of "top subset retrieval" described previously.</p><p>For our final adhoc run (DCU05ACOMBO) we aimed to combine the results from multiple inverted indices outlined in section 2.1.4, with the results of our baseline text results (i.e. DCU05BM25). It was hoped that we could combine the results from these multiple HTML tags in order to give an increase in performance. We combined these different results together using weighted fusion, in which we gave each of the results from the separate tag indices specific weights according to their importance. These weights for each set of results were chosen based on training runs using the Terabyte 2004 Adhoc topics and query relevance judgments. Although with an average query time of 19.4 seconds this run took much longer than the previous runs, we were more concerned with achieving an improvement over the baseline DCU05BM52 run. As can be seen from Table <ref type="table" coords="4,181.69,395.83,4.61,7.86" target="#tab_0">1</ref> this run achieved a slight increase over the baseline in terms of MAP and BPREF. We also present a Precision-Recall graph, showing the precision-recall tradeoffs of all our runs for the adhoc task in Figure <ref type="figure" coords="4,264.76,417.75,3.59,7.86" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficiency Task</head><p>For the efficiency task we had two runs (DCU05DISTDID &amp; DCU05DISTWTF) that used the distributed search system where the first run accessed a document ID sorted index (section 2.1.3) while the latter used a WeightedTF sorted index (section 2.1.2). Each of these runs processed a "top subset" of at most 50,000 documents for each term in the query. Then, as a comparison to the distributed setup, our other runs (DCU05WTF &amp; DCU05WTFQ) were both run on a single machine and both used a WeightedTF sorted index. DCU05WTF processed a subset of at most 50,000 documents for each term in the query, while DCU05WTFQ only processed a maximium of 20,000 documents per term, in order to provide a faster throughput of queries.</p><p>As the top 20 results for each topic were to be returned to NIST for evaluation we present the results for precision at 20 (P20), as well as the query time for each run in Table <ref type="table" coords="4,436.74,663.57,3.59,7.86" target="#tab_1">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adhoc Precision-Recall Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Named Page Finding Task</head><p>For this task we used a similar approach to the adhoc task to investigate if the method of "top subset retrieval" is conducive to the task of finding a specific document.</p><p>As with the adhoc task three of our runs (DCU05NPBM25, DCU05NPDID &amp; DCU05NPWTF) were conducted using our distributed architecture (section 2.2.1), each of which accessed a different sorted index: DCU05NPBM25 used the BM25 sorted index, DCU05NPDID used the Document ID sorted index and DCU05NPWTF used the WeightedTF sorted index. Also, akin to our adhoc task, we submitted a run that combined the results of our baseline run (DCU05NPBM25) with the results from multiple indices (as described in section 2.1.4), using weights trained on the adhoc topics from 2004.</p><p>The performance figures for these runs can be seen in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our chief objective in these experiments was to further evaluate the effectiveness of the approach of "top subset retrieval" in the Terabyte Track in TREC 2005. In the adhoc task our experiments were quite effective as all runs performed better in terms of MAP, P10 and Bpref than the median of the results from all participants, while our combination run which merged results from multiple tags gained an increase in performance over our baseline run.</p><p>The efficiency task provided us with an ideal opportunity to evaluate and contrast the performance of a distributed version of our system using five machines against a similar system on a single machine. The runs DCU05DISTWTF and DCU05WTF provide a direct comparison between the two types of systems. Although the distributed system achieves a higher precision at 20(P20) of 0.529 as opposed to 0.488 on the single machine, the single machine provides a faster query throughput of 0.87 seconds, as opposed to 0.97 seconds. Our "quick" run DCU05WTFQ also achieved satisfactory results: The difference between this run and the DCU05WTF was that the former accessed a maximum top subset of 20,000 documents per term, as opposed to 50,000, in order to process queries faster. It achieved this goal with reduction in query time from 0.87 seconds to 0.35 seconds with a relatively small drop off of 0.022 in P20.</p><p>With our participation the named page finding task we aimed to investigate if the process of "top subset retrieval" was suitable for returning a specific page. Our performance figures for this task (in term of reciprocal rank) lie slightly below that of the average figures achieved by other groups. The explanation for this would be in large part be due to the use of a rather small top subset of 100,000 documents for each term. This would mean that if the relevant document was not ranked in the top 100,000 documents for any of the terms of the query then it would not be scored for retrieval. This could easily be remedied by choosing to process a larger subset of documents for each term, however this would also adversely affect the query time (depending on the size of the subset chosen). Also the fact that our combination run for this task (DCU05NPCOMBO) performed below that of the baseline of (DCU05NPBM25) may have been due to the fact that the weights chosen to merge the results from different tag indices were trained on the 2004 adhoc topics. Furthermore, as both the tasks and the two types of queries vary i.e. the topics for adhoc are often quite vague and are generally relevant to several documents, in contrast to the much more concise queries for the named page finding task that are relevant to only one page. There is most likely gains to be made by experimenting with the way in which these sources of information are merged, as well as possibly incorporating other sources of information such as anchor text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,213.20,265.85,185.61,8.74;3,216.38,84.92,179.74,165.44"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributed Search Architecture.</figDesc><graphic coords="3,216.38,84.92,179.74,165.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,196.27,267.84,219.47,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision Recall Graph for Adhoc Runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,200.29,447.64,211.41,67.92"><head>Table 1 :</head><label>1</label><figDesc>Adhoc Runs and Performance</figDesc><table coords="4,200.29,457.38,211.41,58.18"><row><cell>Run Name</cell><cell cols="2">MAP BPREF</cell><cell>P10</cell></row><row><cell>DCU05ABM25</cell><cell>0.2887</cell><cell>0.3098</cell><cell>0.588</cell></row><row><cell>DCU05ADID</cell><cell>0.2886</cell><cell>0.3154</cell><cell>0.594</cell></row><row><cell>DCU05AWTF</cell><cell>0.3021</cell><cell>0.3267</cell><cell>0.592</cell></row><row><cell cols="2">DCU05ACOMBO 0.2916</cell><cell>0.3191</cell><cell>0.578</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,184.32,298.61,243.36,69.85"><head>Table 2 :</head><label>2</label><figDesc>Efficiency Runs and Performance</figDesc><table coords="5,184.32,310.28,243.36,58.18"><row><cell>Run Name</cell><cell>P20</cell><cell>Query Time(seconds)</cell></row><row><cell>DCU05DISTDID</cell><cell>0.5210</cell><cell>0.97</cell></row><row><cell cols="2">DCU05DISTWTF 0.5290</cell><cell>0.97</cell></row><row><cell>DCU05WTF</cell><cell>0.4880</cell><cell>0.87</cell></row><row><cell>DCU05WTFQ</cell><cell>0.4660</cell><cell>0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,344.65,509.66,29.96,7.86"><head>Table 3</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,179.24,539.56,253.53,69.85"><head>Table 3 :</head><label>3</label><figDesc>Named Page Finding Runs and Performance</figDesc><table coords="5,179.24,551.23,253.53,58.18"><row><cell>Run Name</cell><cell cols="2">Recip. Rank Num. in Top 10</cell></row><row><cell>DCU05NPBM25</cell><cell>0.32</cell><cell>115 (45.6%)</cell></row><row><cell>DCU05NPDID</cell><cell>0.256</cell><cell>98 (38.9%)</cell></row><row><cell>DCU05NPWTF</cell><cell>0.254</cell><cell>96 (38.1%)</cell></row><row><cell>DCU05NPCOMBO</cell><cell>0.287</cell><cell>100 (39.7%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement: This work was supported by <rs type="funder">Science Foundation Ireland</rs>, under grant number <rs type="grantNumber">03/IN.3/I361</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_R6Xv2mp">
					<idno type="grant-number">03/IN.3/I361</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,126.87,473.72,373.77,8.74;6,126.86,485.68,373.77,8.74;6,126.86,497.63,373.77,8.74;6,126.86,509.59,273.09,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,390.16,485.68,110.47,8.74;6,126.86,497.63,310.57,8.74">Experiments in Terabyte Searching, Genomic Retrieval and Novelty Detectionn for TREC-2004</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Boydell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Camous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gaughan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,450.42,497.63,50.21,8.74;6,126.86,509.59,194.78,8.74">Proc. Thirteenth Text Retrieval Conference (TREC-13)</title>
		<meeting>Thirteenth Text Retrieval Conference (TREC-13)</meeting>
		<imprint>
			<date type="published" when="2004-11">November 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.87,525.53,373.77,8.74;6,126.86,537.48,324.95,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,370.65,525.53,129.99,8.74;6,126.86,537.48,59.96,8.74">Físréal: A Low Cost Terabyte Search Engine</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,208.38,537.48,181.76,8.74">Proceedings of European Conference in IR</title>
		<meeting>European Conference in IR</meeting>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.87,553.43,373.77,8.74;6,126.86,565.38,373.78,8.74;6,126.86,577.34,373.77,8.74;6,126.86,589.29,135.63,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,390.63,553.43,110.01,8.74;6,126.86,565.38,165.26,8.74">Top Subset Retrieval on Large Collections using Sorted Indices</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wilkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,313.06,565.38,187.58,8.74;6,126.86,577.34,373.77,8.74;6,126.86,589.29,37.33,8.74">SIGIR &apos;05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="599" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.87,605.23,373.77,8.74;6,126.86,617.19,373.78,8.74;6,126.86,629.14,175.96,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,260.53,605.23,220.59,8.74">Breadth-First Crawling Yields High-Quality Pages</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,126.86,617.19,298.36,8.74">Proceedings of the 10th International World Wide Web Conference</title>
		<meeting>the 10th International World Wide Web Conference<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="114" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.87,645.08,373.77,8.74;6,126.86,657.04,373.78,8.74;6,126.86,668.99,373.77,8.74;6,126.86,680.95,369.31,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,266.27,645.08,234.37,8.74;6,126.86,657.04,174.47,8.74">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,319.65,657.04,180.99,8.74;6,126.86,668.99,373.77,8.74;6,126.86,680.95,34.57,8.74">SIGIR &apos;94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA; New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
