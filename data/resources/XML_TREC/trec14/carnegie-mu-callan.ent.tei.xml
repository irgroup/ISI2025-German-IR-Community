<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.50,164.85,272.26,15.11;1,164.84,186.77,281.57,15.11">Experiments with Language Models for Known-Item Finding of E-mail Messages</title>
				<funder ref="#_fpzSFew">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.10,219.25,63.24,10.48"><forename type="first">Paul</forename><surname>Ogilvie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,317.00,219.25,67.15,10.48"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@lti.cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.50,164.85,272.26,15.11;1,164.84,186.77,281.57,15.11">Experiments with Language Models for Known-Item Finding of E-mail Messages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FD03ACCD30ECBAA7FC91C471AB3B19C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present experiments using language models to rank e-mail messages for the Known-Item Finding task of the Enterprise track. We combine evidence from the text of the message, its subject, the text of the thread the in which the message occurs, and the text of messages that are in reply to the message. We find that the only statistically significant differences suggest that in addition to the text of the message, the subject is a very important piece of evidence. We also explore the use of a depth based prior, where emphasis is place on messages near the root of the thread structure, which has mixed results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we consider the task of known-item finding of email messages equivalent to other structured retrieval tasks. In doing so, we observe that email messages occur in a thread structure as well as have their own internal structure. Viewed in this way, we can consider this task to be identical to the XML component retrieval investigated at INEX <ref type="bibr" coords="1,346.63,518.59,9.97,8.74" target="#b1">[2]</ref>. We consider "documents" to be a thread of email messages and the components we wish to retrieve as the email messages, which are components contained in the documents.</p><p>Structuring email messages in this manner allows us to easily investigate incorporating evidence from the thread as well as structure within the email message itself. We do so using code to be soon released (scheduled for January 2007) as part of the Indri search engine in the Lemur toolkit <ref type="bibr" coords="1,401.46,590.32,9.96,8.74" target="#b0">[1]</ref>.</p><p>We use the Indri search engine to investigate two methods to combine evidence. One is a hierarchical language modeling approach investigated at INEX <ref type="bibr" coords="1,162.42,626.19,10.52,8.74" target="#b3">[4]</ref> <ref type="bibr" coords="1,172.94,626.19,10.52,8.74" target="#b5">[6]</ref> and the other is a post query combination method investigated for known-item finding of web pages <ref type="bibr" coords="1,280.71,638.14,9.96,8.74" target="#b4">[5]</ref>. Within these methods we investigate the use of the text of the emails, the email subjects, the text of the thread, and that of the subthread (messages in reply to a message). We also explore the use of a depth based prior, where depth corresponds to the depth of the message in the thread. The depth based prior was motivated by observations of the training topics.</p><p>Section 2 presents the mixture method followed by the post query combination method. It also describes how the depth prior is incorporated into ranking. Section 3 describes the implementation in Indri, estimation of the prior, and the evaluation methodology. Section 4 presents results on the training topics, while Section 5 presents the results on the test topics. The paper is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In one approach, we rank email messages by estimating the probability that the a language model estimated for the email message generated the query. We use simple unigram language models, which are multinomial probability distributions over words in the vocabulary. That is, a language model θ specifies P (w |θ ). Email messages are then ordered by P (Q |θ e ) = |Q| i=1 P (q i |θ e ) where θ e is the language model estimated for a particular email message e.</p><p>In order to estimate the language model θ e , we note that we would like to incorporate evidence from the message itself and from other messages in the thread in which e was found. With that in mind, we estimate θ e as a linear combination of several language models:</p><formula xml:id="formula_0" coords="2,228.50,405.89,144.99,9.96">P (w |θ e ) = λ em P w θ M LE(em)</formula><p>+λ ti P w θ M LE(ti) +λ th P w θ M LE(th) +λ su P w θ M LE(su)</p><formula xml:id="formula_1" coords="2,288.15,430.05,189.33,34.02">+λ co P w θ M LE(co)<label>(1)</label></formula><p>where em refers to the text of the email, ti refers to the title tag in the documents (the subject of the email), th refers to the text of the entire thread, su refers to the text of the subthread which corresponds to the email itself and all replies to the email, and co refers to the text of the entire collection. θ M LE(text) is the Maximum Likelihood Estimate of the multinomial language model on text, which is given by:</p><formula xml:id="formula_2" coords="2,207.41,550.62,270.07,22.31">P w θ M LE(text) = count of w in text length in words of text<label>(2)</label></formula><p>The other approach we investigate performs an equivalent of a meta-search ranking model where the different rankings are produced by the different ranking models. Here we rank by</p><formula xml:id="formula_3" coords="2,188.83,623.16,288.65,14.10">P (Q |θ em ) λem P (Q |θ ti ) λti P (Q |θ th ) λ th P (Q |θ su ) λsu<label>(3)</label></formula><p>where</p><formula xml:id="formula_4" coords="2,177.09,665.94,300.40,10.62">P (w |θ x ) = (1 -λ co )P w θ M LE(x) + λ co P w θ M LE(co)<label>(4)</label></formula><p>This model does allow relative weighting of the different structural components of messages in the thread. This model corresponds to the linear weighted combination of log probabilities, which we investigated in <ref type="bibr" coords="3,361.10,151.87,9.97,8.74" target="#b4">[5]</ref>. We will present results on ranking by P (Q |θ e ) and by the model specified in Equation 3. We will refer to ranking by P (Q |θ e ) as the mixture method and Equation 3 as the post query combination approach. Our official submissions used the post query combination approach Inspired by the usefulness of document priors for the similar task of knownitem finding on the Web <ref type="bibr" coords="3,241.22,223.60,9.96,8.74" target="#b2">[3]</ref>, we experimented with the use of prior probabilities of relevance based on the "depth" of the message. By "depth", we mean refer to the depth of the email in its thread structure. To incorporate the depth prior into either model, we multiple the score (P (Q |θ e ) or Equation <ref type="formula" coords="3,412.51,259.47,4.43,8.74" target="#formula_3">3</ref>) by:</p><formula xml:id="formula_5" coords="3,248.95,281.38,228.53,8.74">P (e is relevant |depth (e) )<label>(5)</label></formula><p>which we estimate from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section briefly discusses technical details of the implementation and the evaluation techniques that will be used in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation</head><p>Our experiments were performed in a locally enhanced version of the Indri search engine, which is a part of the Lemur toolkit <ref type="bibr" coords="3,326.94,428.26,9.96,8.74" target="#b0">[1]</ref>. The corpus was reorganized so that all messages in a thread are in the same document, where an email message was restructured as:</p><formula xml:id="formula_6" coords="3,133.77,472.81,130.75,103.94">&lt;lists&gt; [trec formatted document] &lt;responses&gt; [response 1] [response 2] ... [response n] &lt;/responses&gt; &lt;/lists&gt;</formula><p>where "[trec formatted document]" refers to an email in the w3c corpus with its "&lt;DOC&gt;" and "&lt;/DOC&gt;" tags and a "response" is a reply to the message formatted as other email messages surrounded by "&lt;lists&gt;" and "&lt;/lists&gt;" tags and also containing its responses. Thread structure was constructed from William Webber's in-reply-to file <ref type="bibr" coords="3,280.27,635.48,9.97,8.74" target="#b8">[9]</ref>.</p><p>Within Indri, we indexed the "lists", "responses", "DOC", and "title" tags, where the "title" tag corresponds to the subject of the original email. We Posterior stemmed terms using the Krovetz stemmer, but we did not use a stopword list. The original topics were converted into NEXI <ref type="bibr" coords="4,329.93,237.62,10.52,8.74" target="#b6">[7]</ref> queries to search the structured database using the simple rewrite:</p><formula xml:id="formula_7" coords="4,139.75,126.37,312.50,33.05">∝ Prior depth P (depth(E)|E is rel.) P (depth(E)) ∝ P (E is rel.|depth(E)) = 0 0.893 0.</formula><formula xml:id="formula_8" coords="4,133.77,268.01,156.91,8.30">//doc[about(., [topic terms])]</formula><p>where "[topic terms]" are the terms contained in the original topic. This query simply requests "doc" components (email messages) matching the original query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prior Estimation</head><p>We estimated the email message depth based prior for three categories of depths: zero, one, and two or more. To estimate the prior probability in Equation <ref type="formula" coords="4,469.73,354.72,3.88,8.74" target="#formula_5">5</ref>, we first estimate the posterior P (depth (E) |E is relevant ). We can estimate this directly from the training topics. When performing estimation, we used a Laplace estimator of the posterior in the topic set, as we had only 25 training topics:</p><formula xml:id="formula_9" coords="4,152.15,429.96,233.32,11.53">P (depth (E) = 0 |E is relevant ) = count(depth=0)+1</formula><p>number topics+3 = 25 28 = 0.893</p><formula xml:id="formula_10" coords="4,152.15,456.77,325.33,41.19">P (depth (E) = 1 |E is relevant ) = count(depth=1)+1 number topics+3 = 2 28 = 0.071 P (depth (E) ≥ 2 |E is relevant ) = count(depth≥2)+1 number topics+3 = 1 28 = 0.036<label>(6)</label></formula><p>The posterior distribution was used to estimate the prior probability of relevance for an email through the use of Bayes rule:</p><formula xml:id="formula_11" coords="4,140.62,535.83,336.86,33.27">P (E is relevant |depth (E) ) = P (depth (E) |E is relevant ) P (E is relevant) P (depth (E))<label>(7)</label></formula><p>P (E is relevant) was assumed constant across all email messages and P (depth (E)) was estimated by examination of email messages in the corpus. Note that we only estimated a value proportional to the prior during ranking, as P (E is relevant) was assumed constant. The values we estimated are presented in Table <ref type="table" coords="4,447.48,608.19,3.88,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measures</head><p>For evaluation measures we present the number found in the top 100 and mean reciprocal rank (MRR), placing emphasis on MRR. Mean reciprocal rank is the average over all queries of one divided by the rank the correct document was found in the top 100 results (zero if there is no correct document in the top 100 results). A MRR close to one is good and MRR places much emphasis on the systems ability to rank correct documents near the top of the list.</p><p>When we discuss statistical significance tests, we use a one-tailed pairwise comparison using the bootstrap and the Benjamini-Hochberg method for multiple test correction <ref type="bibr" coords="5,225.39,199.69,9.96,8.74" target="#b7">[8]</ref>. The bootstrap uses repeated samples of topics with replacement from the original topic set. MRR is computed over the samples. This allows the bootstrap to non-parametrically estimate what may happen on other topic sets. From this we can estimate whether a system configuration performs significantly better than another configuration of the system. As we do these comparisons many times, it is important to correct for multiple testing. Otherwise, we may incorrectly conclude that two system configurations behaved differently when the differences could be easily due to random chance. To correct for this, we use the Benjamini-Hochberg method. For more information on the bootstrap and correction for multiple tests, see <ref type="bibr" coords="5,363.67,307.29,9.96,8.74" target="#b7">[8]</ref>. The significance tests reported below use 5000 bootstrap samples and significance is tested at the 0.05 level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Topics</head><p>Tables <ref type="table" coords="5,165.72,385.97,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="5,194.63,385.97,4.98,8.74" target="#tab_2">3</ref> show the results of various combinations of using the subject, thread, and subthread information was well as the depth prior of the post query combination method and the mixture combination method. The parameter values were hand chosen and may not be optimal for either the training or testing set. From the tables we see that there is a trend that the subject and depth priors tend to help performance noticeably. We also observe that the post query combination method seems to perform better than the mixture method.</p><p>In order to get a better understanding of the differences found between configurations of the system in the training data, we performed one-tailed pairwise comparisons using the bootstrap test corrected for multiple testing using the Benjamini-Hochberg method. After running statistical significance tests, we found that there were very few significant differences at the 0.05 level.</p><p>The few cases where the significance test did find differences between systems involved comparing the post query combination method with weight on the subject of the email with the mixture language model method that did not place extra weight on words in the subject of the email, but did place some weight on the thread or subthread.</p><p>If we rely on the values for MRR on the training set, we would guess that the post query combination method using all features and the depth prior would result in the best performance on the evaluation set of topics. However, noting that this configuration was not significantly better than any of the other approaches tested, one should not be very confident in this guess.  <ref type="formula" coords="7,181.09,363.42,4.43,8.74" target="#formula_3">3</ref>) evaluated on the training topics. Our official runs have different results than those presented in this table. We have presented the performance of the official runs next to similar system configurations. We are still looking for the source of the disagreement in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dirichlet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Test Topics</head><p>Tables <ref type="table" coords="7,165.63,452.93,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="7,194.36,452.93,4.98,8.74" target="#tab_4">5</ref> show performance of the post query combination and mixture methods on the test topics. Due to some system differences, our official submissions had higher MRR than the similar system configurations in Table <ref type="table" coords="7,450.01,476.84,3.88,8.74" target="#tab_3">4</ref>. We have presented the official results beside these results for clarity. We first note that the depth based prior did not always improve performance as it did in the training set. This suggests that our training set was not representative of the test set with regards the thread depth of a known-item messages.</p><p>Another thing to observe is that the subject of the email message consistently provides valuable information for both the post query combination method and the mixture method. The mixture method seems to get additional benefit from the thread and subthread, while the post query combination method does not.</p><p>We also wish to observe that the lower performance of the mixture method when compared to the post query combination method on the test set has disappeared in the test topics. In fact, of the unofficial results, the mixture method using all features except the depth prior has the best mean reciprocal rank.</p><p>However, most of these differences are not statistically significant. the test topics, we also found few significant differences between the system configurations in the test topics. Variations of the post query combination method placing weight on the subject but not using the depth prior were better than both mixture and post query combination methods that did not use the subject or the depth prior. These observations are fairly consistent with those found in the training set, although more significant differences were found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We investigated the use of two combination methods to combine evidence in email messages for the task of known-item finding of email messages. The post query combination method is essentially a meta-search combination method that ranks each email document representation, then combines the scores from each representation using a weighted sum of the log of the scores from the representations. The mixture method combines the language models estimated from each representation. Both methods performed well on the corpus, although the mixture combination method may be more stable with the addition of new features.</p><p>When using the combination methods, we examined the subject of the messages as well as the content of the thread and subthread (all messages in reply to the message). The only universally helpful feature was the subject of the thread, which improved performance noticeably for both combination methods. In some cases, this improvement was statistically significant at the 0.05 level in both the training and testing topics. The only statistically significant differences found between configurations of the systems involved configurations using the subject performing better than configurations that did not use the method.</p><p>We also considered a depth based prior that looked promising on the training topics but did not perform well on the test topics. We would like to investigate this more for the final version of this paper.</p><p>We would also like to note that this was in part an experiment in using the Lemur toolkit <ref type="bibr" coords="9,218.06,211.65,9.97,8.74" target="#b0">[1]</ref>. Apart from the depth prior (which proved to not be very effective) and some data conversion scripts, we were able to use the toolkit without modification. This demonstrates the flexibility and effectiveness of the Lemur toolkit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,148.42,150.68,272.49,54.11"><head>Table 1 :</head><label>1</label><figDesc>Estimation of the thread depth based prior.</figDesc><table coords="4,306.53,150.68,102.67,8.74"><row><cell>674</cell><cell>1.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,133.77,137.40,343.71,490.33"><head>Table 2 :</head><label>2</label><figDesc>Results of various configurations of the post query combination method (Equation3) evaluated on the training topics.</figDesc><table coords="6,139.75,137.40,321.68,490.33"><row><cell></cell><cell cols="5">Subject Thread Subthread Depth Number MRR</cell></row><row><cell>Parameter µ</cell><cell>λ ti</cell><cell>λ th</cell><cell>λ su</cell><cell>Prior</cell><cell>Found</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.567</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.534</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0</cell><cell>YES</cell><cell>22 0.568</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0</cell><cell>NO</cell><cell>22 0.536</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.568</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.538</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0</cell><cell>YES</cell><cell>22 0.568</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0</cell><cell>NO</cell><cell>22 0.540</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.507</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.455</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>YES</cell><cell>22 0.507</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>NO</cell><cell>22 0.455</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.501</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.451</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>YES</cell><cell>22 0.506</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>NO</cell><cell>22 0.451</cell></row><row><cell>Dirichlet</cell><cell cols="5">Subject Thread Subthread Depth Number MRR</cell></row><row><cell>Parameter µ</cell><cell>λ ti</cell><cell>λ th</cell><cell>λ su</cell><cell>Prior</cell><cell>Found</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0.2</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.526</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0.2</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.500</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0.2</cell><cell>0</cell><cell>YES</cell><cell>22 0.546</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0.2</cell><cell>0</cell><cell>NO</cell><cell>22 0.502</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.550</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.520</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0</cell><cell>0</cell><cell>YES</cell><cell>22 0.550</cell></row><row><cell>700</cell><cell>0.72</cell><cell>0</cell><cell>0</cell><cell>NO</cell><cell>22 0.520</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.506</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.453</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>YES</cell><cell>22 0.506</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>NO</cell><cell>22 0.453</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>YES</cell><cell>22 0.501</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>NO</cell><cell>22 0.451</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>YES</cell><cell>22 0.506</cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>NO</cell><cell>22 0.451</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,133.77,641.01,343.71,20.69"><head>Table 3 :</head><label>3</label><figDesc>Results of various configurations of the mixture method (P (Q |θ e )) evaluated on the training topics.</figDesc><table coords="7,139.75,126.37,356.11,212.37"><row><cell></cell><cell cols="4">Subj. Thr. Subthr. Depth</cell><cell cols="2">Num. MRR Official Run</cell></row><row><cell>µ</cell><cell>λ ti</cell><cell>λ th</cell><cell>λ su</cell><cell cols="2">Prior Found</cell><cell>MRR Name</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0.02</cell><cell>YES</cell><cell>108 0.589</cell><cell>0.582 CMUallon</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0.02</cell><cell>NO</cell><cell>113 0.591</cell><cell>0.598 CMUnoprior</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0</cell><cell>YES</cell><cell>108 0.588</cell><cell></cell></row><row><cell>700</cell><cell>0.22</cell><cell>0.2</cell><cell>0</cell><cell>NO</cell><cell>113 0.591</cell><cell></cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0.02</cell><cell>YES</cell><cell>108 0.589</cell><cell></cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0.02</cell><cell>NO</cell><cell>113 0.591</cell><cell>0.601 CMUnoPS</cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0</cell><cell>YES</cell><cell>108 0.592</cell><cell></cell></row><row><cell>700</cell><cell>0.22</cell><cell>0</cell><cell>0</cell><cell>NO</cell><cell>113 0.588</cell><cell>0.596 CMUnoPSD</cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0.02</cell><cell>YES</cell><cell>107 0.560</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0.02</cell><cell>NO</cell><cell>111 0.531</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>YES</cell><cell>107 0.560</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0.2</cell><cell>0</cell><cell>NO</cell><cell>111 0.531</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>YES</cell><cell>106 0.551</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>NO</cell><cell>112 0.525</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>YES</cell><cell>106 0.550</cell><cell></cell></row><row><cell>700</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>NO</cell><cell>112 0.524</cell><cell>0.525 CMUnoPSDT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,133.77,351.47,343.71,20.69"><head>Table 4 :</head><label>4</label><figDesc>Results of various configurations of the post query combination method (Equation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,443.15,656.17,34.33,8.74"><head>Table 5 :</head><label>5</label><figDesc>Results of various configurations of the mixture method (P (Q |θ e )) evaluated on the training topics.</figDesc><table coords="7,443.15,656.17,34.33,8.74"><row><cell>As with</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>The authors would like to thank <rs type="person">William Webber</rs> for providing the extracted thread structure. This research was sponsored by <rs type="funder">National Science Foundation (NSF)</rs> grant no. <rs type="grantNumber">CCR-0122581</rs>. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implicit, of the NSF or the US government.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fpzSFew">
					<idno type="grant-number">CCR-0122581</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,149.27,415.95,328.21,8.74;9,149.27,427.91,111.06,8.74" xml:id="b0">
	<monogr>
		<ptr target="http://lemurproject.org/" />
		<title level="m" coord="9,149.27,415.95,324.36,8.74">The lemur toolkit for language modeling and information retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,149.27,446.82,328.21,8.74;9,149.27,458.78,328.22,8.74;9,149.27,470.73,135.76,8.74" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maalik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<title level="m" coord="9,355.25,446.82,122.24,8.74;9,149.27,458.78,323.03,8.74">Proc. of the Second Annual Workshop of the Initiative for the Evaluation of XML retrieval (INEX)</title>
		<meeting>of the Second Annual Workshop of the Initiative for the Evaluation of XML retrieval (INEX)<address><addrLine>Dagstuhl, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,149.27,489.65,328.21,8.74;9,149.27,501.60,328.22,8.74;9,149.27,513.56,328.22,8.74;9,149.27,525.51,143.94,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,342.59,489.65,134.89,8.74;9,149.27,501.60,118.40,8.74">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,286.32,501.60,191.16,8.74;9,149.27,513.56,328.22,8.74;9,149.27,525.51,26.37,8.74">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,149.27,544.43,328.21,8.74;9,149.27,556.38,328.22,8.74;9,149.27,568.34,173.54,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,268.57,544.43,208.91,8.74;9,149.27,556.38,26.42,8.74">Language models and structured document retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.36,556.38,281.13,8.74;9,149.27,568.34,142.34,8.74">Proceedings of the First Workshop of the INitiative for the Evaluation of XML Retrieval (INEX)</title>
		<meeting>the First Workshop of the INitiative for the Evaluation of XML Retrieval (INEX)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,149.27,587.25,328.21,8.74;9,149.27,599.21,328.22,8.74;9,149.27,611.16,328.21,8.74;9,149.27,623.12,186.42,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,268.17,587.25,209.31,8.74;9,149.27,599.21,47.81,8.74">Combining document representations for knownitem search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,217.29,599.21,260.19,8.74;9,149.27,611.16,230.42,8.74">Proc. of the 26th annual int. ACM SIGIR conf. on Research and development in informaion retrieval (SIGIR-03)</title>
		<meeting>of the 26th annual int. ACM SIGIR conf. on Research and development in informaion retrieval (SIGIR-03)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003-01">July 28-Aug. -1 2003</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,149.27,642.03,328.21,8.74;9,149.27,653.99,328.22,8.74;9,149.27,665.94,319.06,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,276.28,642.03,201.20,8.74;9,149.27,653.99,54.34,8.74">Using language models for flat text queries in xml retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,224.41,653.99,253.07,8.74;9,149.27,665.94,174.80,8.74">Proc. of the Second Annual Workshop of the Initiative for the Evaluation of XML retrieval (INEX)</title>
		<meeting>of the Second Annual Workshop of the Initiative for the Evaluation of XML retrieval (INEX)<address><addrLine>Dagstuhl, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,127.96,328.21,8.74;10,149.27,139.92,323.73,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,311.37,127.96,116.30,8.74">Narrow Extended XPath I</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<ptr target="http://inex.is.informatik.uni-duisburg.de:2004/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,149.27,159.84,208.93,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,218.16,159.84,66.42,8.74">All of Statistics</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.27,179.77,13.00,8.74;10,183.42,179.77,36.28,8.74;10,277.25,179.77,31.57,8.74;10,329.96,179.77,39.40,8.74;10,390.51,179.77,8.03,8.74;10,419.70,179.77,16.60,8.74;10,457.45,179.77,20.04,8.74;10,149.27,191.72,262.62,8.74" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<ptr target="http://www.cs.mu.oz.au/wew/w3c-lists-threads-wew.tar.gz" />
		<title level="m" coord="10,277.25,179.77,31.57,8.74;10,329.96,179.77,39.40,8.74;10,390.51,179.77,8.03,8.74;10,419.70,179.77,16.60,8.74;10,457.45,179.77,16.70,8.74">Thread structure of w3c lists</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
