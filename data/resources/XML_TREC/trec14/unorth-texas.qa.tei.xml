<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.46,74.38,355.13,10.16">UNT 2005 TREC QA Participation: Using Lemur as IR Search Engine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.50,112.24,73.57,10.16"><forename type="first">Jiangping</forename><surname>Chen</surname></persName>
							<email>jpchen@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postBox>P.O. Box 311068</postBox>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.99,112.24,76.43,10.16"><roleName>He Ge</roleName><forename type="first">Ping</forename><surname>Yu</surname></persName>
							<email>pingyu@unt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postBox>P.O. Box 311068</postBox>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.46,74.38,355.13,10.16">UNT 2005 TREC QA Participation: Using Lemur as IR Search Engine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE080157D3D66FEB3D7295DA15B5B445</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports our TREC 2005 QA participation. Our QA system EagleQA developed last year was expanded and modified for this year's QA experiments. Particularly, we used Lemur 4.1 (http://www.lemurproject.org/) as the Information Retrieval (IR) Engine this year to find documents that may contain answers for the test questions from the document collection. Our result shows Lemur did a reasonable job on finding relevant documents. But certainly there is room for further improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question Answering (QA) aims at identifying answers to users' natural language questions. A QA system can release the users from digesting large amount of text in order to locate particular facts or numbers. Therefore the research is much needed and has drawn great attention from several disciplines such as information retrieval, information extraction, and artificial intelligence.</p><p>TREC QA track has provided comparable QA system evaluation on sets of test questions since 1999. The degree of difficulty of the test questions has increased substantially in recent years, which push the research toward applying more sophisticated strategies and better understanding of English texts.</p><p>Question answering is very challenging due to the ambiguity of the questions, complexity of linguistic phenomena involved in the documents, and the difficulty to understand natural languages. A QA system typically contains multiple functional modules in order to find the answers from a large text collection. It takes a team several years of hard work in order to build an effective QA system. Our prototype QA system, named EagleQA, made use of available NLP (Natural Language Processing) tools and knowledge resources for question understanding and answer finding. We skipped information retrieval (IR) process in 2004 because we were overwhelmed in building the basic QA modules. This year's QA track required that each team also submit a list of documents that were used by the QA systems to find the answers in addition to the answers themselves. We started to consider expanding our current QA system to include a module for document retrieval.</p><p>Lemur becomes one of our candidates for IR search engines among others, such as Smart (ftp://ftp.cs.cornell.edu/pub/smart/) and Lucene (http://lucene.apache.org/java/docs/). The Lemur Toolkit (http://www.lemurproject.org/) is designed and developed by researchers from the Computer Science Department at the University of Massachusetts and the School of Computer Science at Carnegie Mellon University. The project is sponsored by the Advanced Research and Development Activity in Information Technology (ARDA) and by the National Science Foundation (NSF). There are several reasons to consider Lemur, including: a. Lemur supports document indexing and multiple well-known text retrieval models such as the TFIDF retrieval model, the Okapi BM25 retrieval function, and the InQuery (CORI) retrieval model (http://www.lemurproject.org/lemur/retrieval.html).</p><p>Other systems focus on only one method. b. The developer states that the toolkit is "under constant development for performance improvements as well as feature additions" (http://www.lemurproject.org/news.html), which we feel is a desired feature. Also, the forum provides quite good technical support. c. The system is designed as a research system, and is quite convenient to be used for TREC-type IR experiments because it accepts TREC document format and produces TREC-type results for evaluation. d. The toolkit is expandable and adaptable with available source codes. We can adapt Lemur for various purposes such as Cross-Language Information Retrieval and Question Answering.</p><p>We applied Lemur 4.1 to find relevant documents for 2004 QA test questions and found it returned more relevant documents for most of the questions than NIST search engine. Lemur was also used in our other experiments <ref type="bibr" coords="2,357.73,333.28,26.78,10.16">[2] [3]</ref>. Therefore, we decided to use Lemur as the search engine for this year's QA experiments. This paper describes the overall structure of our QA system, NLP tools and lexical resources employed by EagleQA, our QA methodology for TREC 2005, QA and document retrieval test results &amp; analysis, and our plan for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Overview</head><p>Current EagleQA system is comprised of 7 major modules or subsystems: a. Question Processing. Accept users' questions and performs part-of-speech tagging, phrase bracketing, keyword identification &amp; expansion, and answer type identification.</p><p>b. Document Retrieval. Apply Lemur 4.1 to find relevant documents for each question from the AQUAINT document collection.</p><p>c. Document Annotation. Apply LingPipe (http://www.alias-i.com/lingpipe/) and Minipar (Lin, 1994) to annotate English texts. LingPipe is used first to detect sentence boundaries, the identified sentences are sent to Minipar for part-of-speech tagging and named entity categorization. LingPipe can also perform named entity categorization and co-reference annotation. At last, we integrate the results of annotations from the two systems using an XML format.</p><p>d. Sentence Retrieval. Identify 500 non-duplicate sentences from the annotated documents as sentence candidates which may contain an answer to each test question.</p><p>The keywords and answer types obtained in Question Processing are utilized to find matched sentences for each factoid and list question. For "Other" questions, the sentence retrieval subsystem returns the sentences that match the target as answer candidates.</p><p>e. Web QA. Google.com (http://google.com) is utilized to find possible candidates of answers.</p><p>f. Answer Finding. Look for multiple evidences to identify and determine answers for factoid and list test questions. Factors that are taken into account when ranking an answer candidate include: 1) answer type; 2) weight of the sentence; 3) distance to keywords in the same sentence; and 4) whether it is a candidate returned by Web QA.</p><p>g including only articles, pronouns, and propositions. To form the query file for document retrieval, we added the target to each Factoid and List question, and replaced 'other' in the Other questions using their targets. Then we converted the questions into the format required by Lemur for retrieval. Our experience in other document retrieval experiments using <ref type="bibr" coords="4,119.35,124.90,46.46,10.16">Lemur [2]</ref>[3] demonstrated that Okapi BM25 retrieval model produced the best performance. Therefore, we only applied this model to return the retrieved documents for all our three QA runs. The default setting for Okapi BM25 was applied. The official results of document retrieval are in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Web QA</head><p>The Internet is a huge and unique knowledge base. Our Web QA subsystem attempts to make use of this knowledge resource by submitting the original test questions to Google.</p><p>The top 100 short summaries returned by Google are sent to the Documentation Module for annotation. Then the annotated texts were sent to Answer Finding Module to locate a list of answer candidates (about 20). Those answer candidates can later be used by Answer Finding module as a weighting factor. The above strategy was different from what we had done for TREC 2004. For TREC 2004 evaluation, we wrote a simple program concerning only the frequency of each term (a word or a phrase) and its category as the factors of answer candidate identification and ranking. This year, we want to apply a consistent approach to answer finding no matter the retrieved texts come from the Web or the predefined document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NLP tools and Knowledge Resources</head><p>EagleQA makes use of many freely available (for research purposes) natural language processing (NLP) software systems and knowledge resources in various modules or subsystems in order to find answers for the test questions. Web QA Finding answer candidates from the Web</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QA Methodology for 2005</head><p>Our QA strategy this year has not changed much from last year. We were not able to implement new strategies even though we have realized that EagleQA needs to be improved. In general, we employed similar strategy to Factoid questions and List questions except that a threshold was applied to List questions in order to determine the number of answers that should be returned. The threshold was predetermined based on experiments using previous years' test questions. Other questions are quite different from Factoid questions. Therefore we used a different approach to answer them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Factoid questions and List questions</head><p>Factoid questions and List questions were handled following the procedures of question processing, document retrieval, document annotation, sentence retrieval, Web QA, answer finding, and answer file formulation. EagleQA first performed a shallow coreference resolution to replace pronouns in questions with the actual targets. For example, in last year's question "Which was the first movie that he was in?" "he" was replaced by the target "James Dean". Then we used Minipar to tag each question and WordNet to expand nouns and verbs in the question. Final result for each question includes the answer type and a list of keywords with expansions if any. For example, Question: Which was the first movie that James Dean was in? Answer type: movie Keywords: movie(synonym: movie, film, picture, moving picture, moving-picture show, motion picture, motion-picture show, picture show, pic, flick) / James Dean After annotating the retrieved documents using LingPipe and Minipar in the Document Annotation module, the annotated texts were sent to Sentence Retrieval module to identify sentences that may contain answers to the test questions. Then the sentences were processed by Answer Finding module to identify the answer for each question. The description of sentence retrieval and answer finding was included in our last year's TREC paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other questions</head><p>The other questions were different from Factoid and List questions. Other questions require QA systems to provide information about the targets in addition to answers to the factoid and list questions for the same targets. We applied the same strategy as last year to find answers to Other questions. The only change is the threshold we used to cut off number of answers that should be returned for each Other question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results &amp; Analysis</head><p>We submitted three runs this year, namely UNTQA0501, UNTQA0502, and UNTQA0503 for the main task. Their official scores are listed in Table <ref type="table" coords="5,365.13,618.21,4.59,10.16" target="#tab_1">1</ref>. Following describes the differences on strategies of the three runs.</p><p>• UNTQA0501. This run did not use the results from the new Web QA module. The answers were solely based on answer finding from the AQUAINT collection;</p><p>• UNTQA0502. This run did not use the results from the new Web QA module either.</p><p>But a different answer type pattern file that includes patterns extracted from last year's test questions was applied by the Question Processing module to process List questions;</p><p>• UNTQA0503. This run used the results from the new Web QA module. It also applied the new answer type pattern file as UNTQA0502 to process List questions.</p><p>All the above three runs used the same document retrieval results returned by Lemur. Table <ref type="table" coords="6,120.58,162.88,6.10,10.16">2</ref> presents the document retrieval evaluation results for the 50 questions specified by NIST.</p><p>Compared to last year, EagleQA did worse this year for all the three types of questions. Its performance on factoid question is even a little bit lower than the median. The scores for the other two tasks are only a little bit higher than the medians. The unsatisfactory performance may due to the increase of the degree of difficulty in the test questions. But obviously, current system needs careful evaluation and big effort for improvement.</p><p>The three runs do not show significant difference even though the strategies were different on certain modules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Research</head><p>Still, our QA system EagleQA is at a very early development stage. We could not carry out further testing and development due to our work on other tasks such as crosslanguage question answering for NTCIR-5 this year. Fortunately, we have more time to work on EagleQA in 2006 and we plan to test something new on answer finding.</p><p>Our current answer finding strategy is problematic. The factors that are contributed to answer candidate identification are limited due to a lack of a semantic parser. The</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,226.12,428.01,490.82"><head></head><label></label><figDesc>. Answer File Formulation. Combine the answers for different types of questions such as factoid, list, and other questions into a submission file. It also removes duplicate answers from the list of answers to other questions if the answers are selected for any factoid or list questions for the same target.</figDesc><table coords="3,90.00,305.14,428.01,323.23"><row><cell>Question Processing</cell><cell cols="2">keywords Answer type and</cell><cell></cell><cell>Finding Answer</cell><cell>Answers to factoid &amp; list questions</cell></row><row><cell>Answer type identification, Keywords &amp; expansion identification</cell><cell>Keywords</cell><cell>Web QA finding using Answer</cell><cell>Answer candidates</cell><cell>Answer on multi-questions based identification &amp; ranking for factoid and list</cell><cell>Answer File Formulation generation by Answer file</cell></row><row><cell></cell><cell></cell><cell>Google</cell><cell></cell><cell>evidences</cell><cell>removing the</cell></row><row><cell>TREC</cell><cell></cell><cell></cell><cell>Sentence</cell><cell></cell><cell>duplicates</cell></row><row><cell>questions</cell><cell></cell><cell></cell><cell cols="2">candidates</cell><cell>Answers</cell></row><row><cell>Document Retrieval</cell><cell>Retrieved Documents</cell><cell>Document Annotation</cell><cell></cell><cell>Sentence Retrieval</cell><cell>to other questions</cell></row><row><cell>Document retrieval</cell><cell></cell><cell>POS tagging, Co-reference resolution,</cell><cell>Annotated docs</cell><cell>Answer weighting &amp; sentence</cell><cell>Answer file</cell></row><row><cell></cell><cell></cell><cell>NE</cell><cell></cell><cell>extraction</cell><cell></cell></row><row><cell></cell><cell></cell><cell>categorization</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Figure 2. EagleQA Architecture</cell><cell></cell></row><row><cell cols="6">Figure 1 is the system architecture of current EagleQA. Compared to last year's system,</cell></row><row><cell cols="6">Document Retrieval module has been added and Web QA module has been modified to</cell></row><row><cell cols="6">test a new method. Below we will further describe these two modules. Descriptions for</cell></row><row><cell cols="5">other modules were included in our 2004 TREC paper [1].</cell><cell></cell></row></table><note coords="3,90.00,656.25,114.84,10.16;3,90.00,668.79,404.13,10.16;3,90.00,681.45,280.56,10.16;3,90.00,694.11,383.22,10.16;3,90.00,706.78,407.24,10.16"><p><p>2.1 Document Retrieval</p>Document retrieval using Lemur is straight forward as Lemur has been intentionally designed for TREC-type information retrieval experiments (http://www.lemurproject.org/lemur/indexingfaq.html). The AQUAINT document collection was first indexed by Lemur using a simple manually created stop word list</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,403.17,426.01,310.58"><head>Table 1 . Incorporated NLP tools and knowledge resources Application s URLs if Obtained Online Modules that Use the Application Usage Description Lemur IR Toolkit</head><label>1</label><figDesc>Table 1 lists all the tools and knowledge resources used by our system.</figDesc><table coords="4,91.98,498.70,424.03,189.26"><row><cell></cell><cell>http://www.lemurproj</cell><cell>Document</cell><cell>English document indexing</cell></row><row><cell></cell><cell>ect.org/</cell><cell>Retrieval</cell><cell>and retrieval</cell></row><row><cell>LingPipe</cell><cell>http://www.alias-</cell><cell>Document</cell><cell>English sentence boundary</cell></row><row><cell></cell><cell>i.com/lingpipe/</cell><cell>Annotation</cell><cell>detection, named entity</cell></row><row><cell></cell><cell></cell><cell></cell><cell>annotation</cell></row><row><cell>Minipar</cell><cell>http://www.cs.ualbert</cell><cell>Document</cell><cell>English Part-of-Speech</cell></row><row><cell></cell><cell>a.ca/~lindek/minipar.</cell><cell>Annotation</cell><cell>tagging, information</cell></row><row><cell></cell><cell>htm</cell><cell></cell><cell>extraction, noun phrase</cell></row><row><cell></cell><cell></cell><cell></cell><cell>annotation</cell></row><row><cell>WordNet</cell><cell>(http://www.cogsci.pri</cell><cell>Question</cell><cell>Synonym and Hyponym</cell></row><row><cell></cell><cell>nceton.edu/~wn/</cell><cell>Processing,</cell><cell>extraction</cell></row><row><cell></cell><cell></cell><cell>Answer Finding</cell><cell></cell></row><row><cell>WordNet::Q</cell><cell>http://people.csail.mit</cell><cell>Question</cell><cell>Synonym and Hyponym</cell></row><row><cell>ueryData</cell><cell>.edu/u/j/jrennie/public</cell><cell>Processing,</cell><cell>extraction</cell></row><row><cell></cell><cell>_html/WordNet/</cell><cell>Answer Finding</cell><cell></cell></row></table><note coords="4,91.98,690.94,177.61,10.22;4,167.40,703.60,9.15,10.16"><p>Google.com http://www.google.co m</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,90.00,314.61,415.76,267.50"><head>Table 1 .</head><label>1</label><figDesc>Official QA Results Document retrieval using Lemur can return about 53.5% relevant documents. We feel this is a reasonable performance. Lemur can find at least one relevant document for 88% of the test questions (44 out of 50).</figDesc><table coords="6,90.00,333.76,404.25,184.64"><row><cell>Run</cell><cell cols="2">Factoid</cell><cell>List</cell><cell>Other</cell><cell>Average per-</cell></row><row><cell></cell><cell cols="2">(Accuracy )</cell><cell>(Average F )</cell><cell>( Average F )</cell><cell>series score</cell></row><row><cell>UNTQA0501</cell><cell cols="2">0.135</cell><cell>0.054</cell><cell>0.191</cell><cell>0.131</cell></row><row><cell>UNTQA0502</cell><cell cols="2">0.135</cell><cell>0.064</cell><cell>0.182</cell><cell>0.132</cell></row><row><cell>UNTQA0503</cell><cell cols="2">0.141</cell><cell>0.062</cell><cell>0.184</cell><cell>0.134</cell></row><row><cell>median</cell><cell cols="2">0.152</cell><cell>0.053</cell><cell>0.156</cell><cell>0.123</cell></row><row><cell>Best</cell><cell cols="2">0.713</cell><cell>0.468</cell><cell>0.248</cell><cell>0.534</cell></row><row><cell cols="4">Table 2. Official Document Retrieval Results</cell><cell></cell></row><row><cell>Run</cell><cell></cell><cell>Relevant</cell><cell>Retrieved</cell><cell>Average</cell><cell>R-Precision</cell></row><row><cell></cell><cell></cell><cell>Document</cell><cell>Relevant</cell><cell>Precision</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Document</cell><cell></cell></row><row><cell cols="2">The 50 questions</cell><cell>1575</cell><cell>841</cell><cell>0.3285</cell><cell>0.3205</cell></row><row><cell cols="2">in ALL UNT Runs</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ranking strategy for answer candidates needs training and tuning. The Answer Finding module will be the focus of our QA research in 2006.</p><p>The retrieval performance of Lemur will be further analyzed and we hope to find ways to improve document retrieval using this system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,94.58,162.94,69.99,10.16;7,90.00,188.14,401.52,10.16;7,108.00,200.80,388.75,10.16;7,108.00,213.46,374.81,10.16" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,393.12,188.14,98.40,10.16;7,108.00,200.80,239.24,10.16">UNT at TREC 2004: question answering combining multiple evidences</title>
		<author>
			<persName coords=""><forename type="first">References</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jiangping; Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shikun</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec13/papers/unorthtexas.qa.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,355.58,200.80,141.17,10.16;7,108.00,213.46,22.01,10.16">Online Proceedings of TREC 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,232.12,404.65,10.16;7,108.00,244.78,400.56,10.16;7,108.00,257.38,313.03,10.16;7,108.00,270.04,390.55,10.16;7,108.00,282.70,86.88,10.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,145.91,244.78,279.75,10.16">Chinese QA and CLQA: NTCIR-5 QA experiments at UNT</title>
		<author>
			<persName coords=""><forename type="first">Jiangping</forename><forename type="middle">;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rowena</forename><forename type="middle">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ping</forename><forename type="middle">;</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pok</forename><forename type="middle">;</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cong</forename><surname>Xuan</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings5/data/CLQA/NTCIR5-CLQA-ChenJ.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,435.14,244.78,73.42,10.16;7,108.00,257.38,91.01,10.16">Proceedings of NTCIR-5 workshop</title>
		<meeting>NTCIR-5 workshop<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12">2005. December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,90.00,295.36,408.32,10.16;7,108.00,308.02,382.65,10.16;7,108.00,320.62,216.45,10.16;7,108.00,333.28,385.72,10.16;7,108.00,345.94,78.89,10.16" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,328.99,295.36,169.33,10.16;7,108.00,308.02,200.26,10.16">Chinese information retrieval using Lemur: NTCIR-5 CIR experiments at UNT</title>
		<author>
			<persName coords=""><forename type="first">Jiangping</forename><forename type="middle">;</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rowena</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings5/data/CLIR/NTCIR5-CLIR-ChenJ.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="7,317.74,308.02,167.34,10.16">Proceedings of NTCIR-5 workshop</title>
		<meeting>NTCIR-5 workshop<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12">2005. December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
