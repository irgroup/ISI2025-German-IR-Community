<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.48,112.05,247.04,15.12">Million Query Track 2009 Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.33,144.53,76.57,10.48;1,227.90,142.92,1.41,6.99"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.05,144.53,61.94,10.48;1,297.00,142.92,1.88,6.99"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.14,144.53,47.31,10.48;1,351.45,142.92,1.88,6.99"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer &amp; Electrical Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.60,144.53,102.07,10.48;1,460.67,142.92,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Information Studies Department</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.48,112.05,247.04,15.12">Million Query Track 2009 Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4229F6D8F3DBB9DE643B70D98A9C925A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Million Query Track ran for the third time in 2009. The track is designed to serve two purposes: first, it is an exploration of ad hoc retrieval over a large set of queries and a large collection of documents; second, it investigates questions of system evaluation, in particular whether it is better to evaluate using many queries judged shallowly or fewer queries judged thoroughly.</p><p>Fundamentally, the Million Query tracks <ref type="bibr" coords="1,274.52,230.58,23.15,8.74">(2007)</ref><ref type="bibr" coords="1,297.66,230.58,4.63,8.74">(2008)</ref><ref type="bibr" coords="1,302.29,230.58,23.15,8.74">(2009)</ref> are ad-hoc tasks, only using complex but very efficient evaluation methodologies that allow human assessment effort to be spread on up to 20 times more queries than previous ad-hoc tasks. We estimate metrics like Average Precision fairly well and produce system ranking that (with high confidence) match the true ranking that would be obtained with complete judgments. We can answer budget related questions like how many queries versus how many assessments per query give an optimal strategy; a variance analysis is possible due to the large number of queries involved.</p><p>While we have confidence we can evaluate participating runs well, an important question is whether the assessments produced by the evaluation process can be reused (together with the collection and the topics) for a new search strategy-that is, one that did not participate in the assessment done by NIST. To answer this, we designed a reusability study which concludes that a variant of participating track systems may be evaluated with reasonably high confidence using the MQ data, while a complete new system cannot.</p><p>The 2009 track quadrupled the number of queries of previous years from 10,000 to 40,000. In addition, this year saw the introduction of a number of new threads to the basic framework established in the 2007 and 2008 tracks:</p><p>• Queries were classified by the task they represented as well as by their apparent difficulty.</p><p>• Participating sites could choose to do increasing numbers of queries, depending on time and resources available to them.</p><p>• We designed and implemented a novel in situ reusability study.</p><p>Section 1 describes the tasks for participants. Section 2 provides an overview of the test collection that will result from the track. Section 3 briefly describes the document selection and evaluation methods. Section 4 summarizes the submitted runs. In Section 5 we summarize evaluation results from the task, and Section 6 provides deeper analysis into the results. For TREC 2009, Million Query, Relevance Feedback, and Web track ad-hoc task judging was conducted simultaneously using MQ track methods. A number of compromises had to be made to accomplish this; a note about the usability of the resulting data is included in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Task Description</head><p>The basic task is ad hoc retrieval over a large set of queries. As with the Terabyte track before it, the challenge is running a very large number of queries over a very large index and still achieving good results. This aspect of the track has not changed since 2007. This year we added two additional (optional) tasks. The first was prediction of type of intent behind a query. A large sample of queries represents a wide space of tasks and information needs; treating them all as traditional deep informational search tasks does not accurately reflect the needs of users working with a system. Thus we allowed participants to try to predict whether a query was more "precision-oriented" (i.e. looking for a small, well-contained set of facts) or more "recall-oriented" (i.e. looking for deeper, more openended information). Based on these predictions, participants could choose to select an alternate retrieval algorithm. In addition to intent-type predictions, we allowed participants to try to predict whether a query would be easy for their system or hard, and likewise to try alternate methods depending on the prediction.</p><p>The second was query prioritization. We assigned each of the 40,000 queries a priority level, and a site could choose to do only the highest-priority queries (of which there were 1,000), the 1st and 2nd highest priority (4,000), 1st, 2nd, and 3rd highest (10,000), or all queries (40,000). This was a concession to the fact that most sites had not previously indexed such a large corpus and were concerned about being able to run such a large number of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Test Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus</head><p>This year we adopted the new ClueWeb09 crawl of the general web. The full collection contains over one billion web pages and other documents in 10 different languages; since this was the first year using such a large corpus, we elected to use the smaller "Category B" set of 50 million English pages in roughly 2TB of disk space. More information about the corpus can be found at http://boston.lti.cs.cmu.edu/Data/ clueweb09/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Queries</head><p>Queries were sampled from two large query logs. To help anonymize and select queries with relatively high volume, they were processed by a filter that converted them into queries with roughly equal frequency in a third query log.</p><p>Assessors selected short keyword queries from lists of 10 randomly selected from the set of 40,000. The random selection was biased to ensure that more high-priority queries would be presented. After selecting a query, assessors "backfit" it to a TREC topic by writing a description and narrative. In all, 634 topics were developed for the track; since Web/RF topics 1-50 were included in judging, the final total is 684 topics.</p><p>Query intent types were assigned by assessors when backfitting their queries. Exactly one of the following six categories, based on work by Rose and Levinson <ref type="bibr" coords="2,298.84,483.51,27.28,8.74" target="#b13">[RL04]</ref>, was chosen for each query; we then mapped the six categories into broader "precision-oriented" or "recall-oriented" classes:</p><p>• Navigational (precision): Find a specific URL or web page.</p><p>• Closed (precision) or directed information need: Find a short, unambiguous answer to a specific question.</p><p>• Resource (precision): Locate a web-based resource or download.</p><p>• Open (recall) or undirected information need: Answer an open-ended question, or find all available information about a topic.</p><p>• Advice (recall): Find advice or ideas regarding a general question or problem.</p><p>• List (recall): Find a list of results that will help satisfy an open-ended goal.</p><p>As an example of the classification task, the query "district of columbia department of motor vehicles" (from the 2008 1MQ track) might be classified as "precision-oriented", as its intent seems navigational in nature.</p><p>The query "vietnam vets and agent orange" might be classified as "recall-oriented", as it seems to reflect a broad, open-ended informational intent.</p><p>The queries fell into the broad precision/recall classes with 43% precision and 57% recall. Most of the recall types (indeed, most of the queries) were classified as "open or undirected information needs", with "advice" and "list" types making up just 6% of all recall queries. Precision types were more evenly distributed, with 44% "closed or directed information need", 33% "navigational", and 23% "resource". On average, among the 34 judgments per query in the MQ-only set 7.82 were judged relevant. The corresponding numbers for the 50 first queries are 70 relevant documents per query out of the 250 judged documents.</p><p>Judged queries were also assessed for difficulty by the Average-Average-Precision (AAP) score, the average of average precision estimates for a single query over all submitted runs. These were assigned automatically by partitioning the AAP score range into three intervals:</p><formula xml:id="formula_0" coords="3,86.94,214.21,140.14,48.22">• Hard: AAP ∈ [0, 0.06) • Medium: AAP ∈ [0.06, 0.17) • Easy: AAP ∈ [0.17, max]</formula><p>These intervals were chosen so that queries would be roughly evenly distributed. 38% of all queries were hard, 32% medium, and 30% easy; the "hard" class includes more queries because it includes every query for which no relevant documents were found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Judgments</head><p>Assessors were shown a full web page, with images included to the extent possible by referencing the original site. They were asked to judge a document either "not relevant", "not relevant but reasonable", "relevant", or "highly relevant". Each topic received either 32 or 64 judgments to documents ranked by submitted systems and selected by alternating between two methods described below. Some queries were designated for reusability study (see Section 5.1 below). For these, three of the eight participating sites were chosen to be held out of judgment collection. Each site was held out a roughly equal number of times, so each site contributed to and was held out from roughly the same number of queries (unless the site did queries beyond the high-priority set). All reusability-designated queries received 32 judgments; all non-reusability queries received 64.</p><p>In all, 684 queries received judgments, including the 50 that overlapped with the Web and Relevance Feedback tracks. On average, each query received 50 judgments, though the first 50 received many more due to the judging requirements of those tracks. The 634 MQ-only queries received an average of 34 judgments each. 26% of all judgments were "relevant" or "highly relevant", though that dropped to 23% among the MQ-only queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Selection and Evaluation Methods</head><p>For a full description of the two methods, we refer the reader to the 2007 Million Query Track overview and the original work cited below. The descriptions below focus on estimates of average precision and other measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MTC</head><p>The Minimal Test Collections (MTC) method works by identifying documents that will be most informative for understanding performance differences between systems by some evaluation measure (in this case average precision). Details on the workings of MTC can be found elsewhere [CPK + 08, CAS06, ACA + 07]. Here we focus on MTC estimates of evaluation measures.</p><p>First, we consider each document i to have a distribution of relevance p(X i ). If the document has been given judgment j = 0 or 1 (nonrelevant or relevant), then p(X i = j) = 1; otherwise the probability that the document is relevant is p(X i = 1) = p i . Then we consider a measure to be a random variable expressed as a function of document relevances X i . For example, precision can be expressed as a random variable that is a sum of document relevance random variables for those documents retrieved at ranks 1 through k.</p><p>If the measure is a function of document random variables, then the measure has a distribution over possible assignments of relevance to unjudged documents. Note that this applies to measures averaged over queries as well as to measures for a single query. This produces a distribution over possible rankings of systems: there is some probability that system 1 is better than system 2, which in turn is better than system 3; some probability that system 1 is better than system 3, which in turn is better than system 2, and so on. It can be shown that the maximum a posteriori ranking of systems is that in which systems are ranked by the expected values of the evaluation measure of interest over the possible relevance assignments.</p><p>Calculating the expectation of an evaluation measure is fairly simple. Given the probability that document i is relevant p i = p(X i = 1), we define:</p><formula xml:id="formula_1" coords="4,225.11,223.92,161.21,133.03">Eprec@k = 1 k k i=1 p i ER-prec ≈ 1 ER ER i=1 p i EAP ≈ 1 ER n i=1 p i /i + j&gt;i p i p j /j and ER = n i=1 p i .</formula><p>Note that there is an assumption that document relevances are independent, or conditionally independent given features used to estimate the distribution p(X i ).</p><p>Though MTC is designed for ranking systems (i.e. making decisions about relative differences in performance between systems), in this work we largely present expectations of evaluation measures for individual systems. Note that these expectations are not good estimates of actual values of evaluation measures; the most interesting quantities MTC provides are the probabilities of particular pairwise orderings of systems.</p><p>MTC evaluation is implemented in the mtc-eval package downloadable from the TREC web site or at http://ir.cis.udel.edu/ ~carteret/downloads.html (which will always have the latest version).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Estimating Probability of Relevance</head><p>The probability estimates p i are the output of a model of the log odds of relevance expressed as a linear combination of feature values:</p><formula xml:id="formula_2" coords="4,248.13,515.73,114.85,30.32">log p i 1 -p i = β 0 + v j=1 β j f ij</formula><p>where f ij is the value of feature j for document i. Note that this is simply a logistic regression classifier. We used two different sets of features to obtain relevance probabilities. One set, implemented by the expert utility in the mtc-eval distribution, consists of features extracted from the system rankings themselves. These are described in more detail by Carterette <ref type="bibr" coords="4,319.49,589.91,30.05,8.74" target="#b7">[Car07]</ref>.</p><p>The other set, described by Carterette and Allan <ref type="bibr" coords="4,299.80,601.87,26.57,8.74" target="#b12">[CS07]</ref>, uses similarities between documents as features: the value of f ij is the cosine similarity between documents i and j. The "feature documents" are those that have been judged for relevance to the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">statAP</head><p>In statistical terms, average precision can be thought of as the mean of a population: the elements of the population are the relevant documents in the document collection and the population value of each element is the precision at this document for the list being evaluated. This principle is the base for several recently proposed evaluation techniques [YA06, APY06, AP, ACA + 07]. StatAP is a sample-and-estimate technique defined by the following two choices.</p><p>Stratified Sampling, as developed by Stevens <ref type="bibr" coords="5,298.10,111.02,30.03,8.74" target="#b3">[BH83,</ref><ref type="bibr" coords="5,331.43,111.02,15.50,8.74" target="#b14">Ste]</ref>, is very straightforward for our application. Briefly, it consists of bucketing the documents ordered by a chosen prior distribution and then sampling in two stages: first sample buckets with replacement according to cumulative weight; then sample documents inside each bucket without replacement according to selection at the previous stage.</p><p>Generalized ratio estimator. Given a sample S of judged documents along with inclusion probabilities, in order to estimate average precision, statAP adapts the generalized ratio estimator for unequal probability designs <ref type="bibr" coords="5,107.95,182.75,31.46,8.74" target="#b15">[Tho92]</ref>.(very popular on polls, election strategies, market research etc). For our problem, the population values are precisions at relevant ranks; so for a given query and a particular system determined by ranking r(.) we have (x d denotes the relevance judgment of document d) :</p><formula xml:id="formula_3" coords="5,72.00,230.77,327.83,75.59">statAP = 1 R d∈S x d • prec@r(d) π d where R = d∈S x d π d ; prec@k = 1 k d∈S,r(d)≤k x d π d</formula><p>are estimates the total number of relevant documents and precision at rank k, respectively, both using the Horwitz-Thompson unbiased estimator <ref type="bibr" coords="5,244.90,328.78,31.48,8.74" target="#b15">[Tho92]</ref>.</p><p>Confidence intervals. We can compute the inclusion probability for each document (π d ) and also for pairs of documents (π df ); therefore we can calculate an estimate of variance, var(statAP ), from the sample, using the ratio estimator variance formula found in <ref type="bibr" coords="5,332.70,364.65,31.48,8.74" target="#b15">[Tho92]</ref>, pp. 78 (see [AP, ACA + 07] for details). Assuming the set of queries Q is chosen randomly and independently, and taking into account the weighting scheme we are using to compute the final MAP (see Results), we compute an estimator for the MAP variance var(statM AP ) = 1 ( q w q ) 2 q∈Q w 2 q • var(statAP q )</p><p>where w q is distribution weight proportional with the number of judgments made on query q. Assuming normally distributed statM AP values, a 95% confidence interval is given by ±2std or ±2 var(statM AP ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Note on TREC 2009 Judging</head><p>The MQ methods were also used to select documents for the Web track's ad hoc task and the Relevance Feedback track. Due to the varying interests of the tracks and the distinction between "Category A" runs that in principle could retrieve documents from the entire collection and "Category B" runs that were explicitly restricted to the smaller Category B subset of ClueWeb09, certain compromises had to be made in the application of these methods across tracks. The coordinators for these tracks, as a group, decided on the following approaches:</p><p>1. a depth-12 pool formed from all Category A runs would be judged for relevance to the 50 topics in common between all three tracks; 2. statAP and MTC would be used (jointly, alternating between the two for each topic) to select additional documents to judge from Category B runs for those 50 topics;</p><p>3. statAP and MTC would be used (jointly, alternating between the two for each topic) to select all documents to judge for all MQ track runs, which are all Category B runs by the track's guidelines. One result of these decisions is that statAP could not be used to evaluate any Category A run-it requires that sampling be done according to priors computed over all runs, which did not happen for Category A runs.</p><p>A second result is that the estimates of document relevance required by MTC are different for the different groups of runs: the erels.catA.1-50 file for Category A runs, the erels.catB.1-50 file for Category Bonly runs, and the erels docsim.20001-60000 file for the MQ runs. All three files can be downloaded from the MQ track site at http://ir.cis.udel.edu/million/data.html; this page also gives instructions on how to replicate and reuse data from the Web and MQ tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>The track received 35 runs from the 8 groups listed in Table <ref type="table" coords="6,333.13,290.01,3.87,8.74">4</ref>. Every group was allowed to submit up to five runs. All of the submitted runs were included in judgment collection for a roughly equal number of queries.</p><p>A new task introduced in this year's track is to classify queries based on underlying user intentions. Three groups, Sabir, NEU, and UIUC, submitted query class predictions.</p><p>The following is a brief summary of some of the submitted runs based on the information provided by the participating groups.</p><p>IIIT-Hyderabad submitted three runs: iiithAuEQ, iiithAuthPN, and iiithExpQry. The basic system was developed on top of Lucene using Hadoop infrastructure. They treat each document as a combination of separate fields such as title text, h1, image-alt, underlined, bold etc, but observed that a simpler approach of considering a document as document-url, title and body outperformed the earlier approach. To enhance recall, they expanded the queries using WordNet and also by combining the query with all possible subsets of tokens present in the query (excluding the stop words). To prevent query drift they experimented on giving selective boosts to different steps of expansion including giving higher boosts to sub-queries containing named entities as opposed to those that didn't. In fact, this run achieved highest precision among their other runs. Using simple statistics they identified authoritative domains such as wikipedia.org, answers.com, etc and attempted to boost hits from them, while preventing them from overly biasing the results.</p><p>IRRA submitted five runs: irra1mqa, irra1mqd, irra2mqa, irra2mqd and irra3mqd. Northeastern University submitted five runs: NeuSvmBase, NeuSvmHE, NeuSvmPR, NeuSvmPRHE and NeuSvmStefan. They used the default retrieval function provided by indri to retrieve 2000 documents, extracted features from both document text and link information, and then trained svmlight to learn the ranking function and then rank documents based on the learned function. The differences among the submitted runs how to train and use the svmlight. In particular, (1) NeuSvmBase trains the svmlight on MQ08 data, (2) NeuSvmHE trains svmlight on hard queries from MQ08 data and uses it on predicted hard queries. It also trains svmlight on easy queries from MQ08 data and uses it on predicted easy queries. (3) NeuSvmPR trains svmlight on recall-oriented queries from MQ08 data and uses it on predicted recall-oriented queries. It also trains svmlight on precision-oriented queries from MQ08 data and uses it on predicted precision-oriented queries. (4) NeuSVMPRHE is similar to the other runs, except that four categories, i.e., hard recall/easy precision/hard precision/easy recall/ are considered. (5) NeuSvmStefan trains on a different query log from a major web search engine for overlap between the top 100 documents retrieved by Microsoft Bing and the top 2000 documents retrieved by indri. For the query class prediction, they predicted hear/easy using Jensen-Shannon divergence among ranking features, and predicted precision/recall using a SVM classifier trained on MQ08 query precision/recall tags.</p><p>Sabir submitted five runs: Sab9mq1bf1, Sab9mq1bf4, Sab9mq2bf1, Sab9mqBase1 and Sab9mqBase4.  <ref type="bibr" coords="7,212.29,194.68,61.31,8.77">(Carterette)</ref> submitted five runs: udelIndDM, udelIndPR, udelIndRM, udelIndSP and udelIndri. udelIndri is the baseline indri run. udelIndDM is the run with Metzler and Croft dependence modeling. udelIndPR is the run with a PageRank document prior. udelIndRM is the run with Lavrenko and Croft relevance models. udelIndSP is the indri run with a "domain trust" document prior. Domain trust is based on the frequencies of occurrence of a domain on external, public-available URL and send mail whitelists and blacklists. The parameters of all the runs are trained in a quasi-semi-supervised fashion using MQ08 data.</p><p>University of Delaware (Fang) submitted five runs: UDMQAxBL, UDMQAxBLlink, UDMQAxQE, UDMQAxQEWP and UDMQAxQEWeb. All of the five runs used the axiomatic retrieval models to rank documents. UDMQAxBL is the baseline run with the F2-LOG axiomatic retrieval function. UDMQAxBLlink ranks documents with F2-LOG based on both document content and anchor text. The other three runs used the semantic term matching method proposed in the axiomatic retrieval models. All of these three runs expanded original query terms with their semantically related terms. The related terms are selected from different resources: document collection (i.e., category B) for UDMQAxQE, Wikipedia pages in the document collection for UDMQAxQEWP, and snippets returned by Web search engines for UDMQAxQEWeb.</p><p>University of Glasgow submitted two runs: uogTRMQdpA10 and uogTRMQdpA40. They used a MapReduce based indexer for the Terrier platform to index the ClueWeb09 collection. For retrieval, they investigated the application of a hyper-geometric parameter-free Divergence from Randomness weighting model.</p><p>University of Illinois at Urbana-Champaign submitted five runs: uiuc09Adpt, uiuc09GProx, uiuc09KL, uiuc09MProx and uiuc09RegQL. uiuc09KL is the baseline runs with KL-divergence retrieval model. uiuc09GProx and uiuc09MProx are two variaions of the positional relevance model (PRM) that exploits term proximity evidence so as to assign more weights to words closer to query words in feedback documents. uiuc09MProx estimates the PRM by first computing the joint probability of observing a word together with the query words at each position and then aggregating the evidence by summing over all the possible positions, and uiuc09GProx estimates the PRM by computing the association between each word and the query using documents and positions as "bridges". uiuc09Adpt used an adaptive retrieval model based on query classification results. And uiuc09RegQL is an improved document weighting in the relevance model by using a regression-based method to normalize query likelihood scores to approximate the probability of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The evaluation methods (MTC and statMAP) are described in Section 3. While both use Average Precision as a base metric, they are very different in both goal and approach. The MTC method is optimized for ranking runs by MAP; it does not attempt to directly estimate Mean Average Precision, but rather to estimate the correct ranking. The statMAP metric estimates Average Precision (per query) analogous to how election polls are conducted, using sampling. Overall we can expect MTC to produce a more accurate ranking of the runs, and statMAP a more accurate estimate of MAP.</p><p>Out of the 684 topics judged, 146 are common to all runs in the sense that all runs were involved in document selection. For the rest of the queries, several sites were held out in a round-robin fashion (details in the next section). Therefore the only consistent all-run comparison in terms of performance can be made  to estimate relevance. For statAP, the interval listed in the table is the 95% confidence interval computed under the assumptions explained in section 2.</p><p>A comparison of the two evaluation methods is presented in Figure <ref type="figure" coords="9,383.43,343.30,3.87,8.74" target="#fig_0">1</ref>. The two measure produce results on different scales, but linearly there is a reasonably strong correlation (ρ = 0.90, Kendall's τ = 0.80) which, as in previous Million Query tracks, we take as an indication of validity for both measures in the absence of comparison with a strong "ground truth" measurement of MAP. We note two outliers that the two methods do not agree on: UDMQAxQE and UDMQAxQEWP. Removing these improves the correlations to ρ = 0.95 and τ = 0.90, indicating high agreement. We have not yet determined why these two systems are different.</p><p>Table <ref type="table" coords="9,114.18,415.03,4.98,8.74">5</ref> presents the evaluation for each site independently over all topics for which the site participated in document selection. We call these sets the site baselines. It is difficult to make direct comparisons of runs between sites, because each site has its own set of baseline queries; but a within-site comparison is appropriate. Sites that chose to run more than just the first 1,000 highest-priority queries include those additional queries in their site baselines; those sites were UDel, SABIR, UOG, and IIITH, explaining why their totals are greater than the other sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reusability</head><p>One of the goals of the 2009 track was to assess the reusability of a large set of queries with very shallow judgments. In general, because query sets that have (relatively) complete judgments tend to be small (usually 50 queries), there is no data available to study this problem. Data from previous years' MQ tracks provides a large sample of queries, but not the complete judgments needed to simulate reusability. Thus we have designed an experiment for in situ reusability testing: each site can be evaluated over a set of queries the site contributed judgments to as well as a set of queries the site did not contribute to.</p><p>In this section we describe the experimental design and analysis. More detail is presented by Carterette et al. <ref type="bibr" coords="9,97.46,604.77,41.33,8.74" target="#b10">[CKPF10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Types of Reusability</head><p>We distinguish three types of reusability:</p><p>1. within-site reusability: a site uses a test collection to optimize and evaluate runs internally. </p><formula xml:id="formula_4" coords="11,214.38,302.31,193.86,45.92">+ + + + - - • • • • • • t n+30 - - + + + + T 3 • • •</formula><p>Table <ref type="table" coords="11,99.59,360.03,3.87,8.74">4</ref>: Illustration of proposed experimental design at the site level with m = 6 sites and k = 2 held out from each topic. Each column shows which topics a site contributed to. A + indicates that all of the sites' runs contributed judgments to the topic; -indicates that the sites' runs did not contribute judgments. Each subset T 1 . . . T b has the same contribution pattern as subset T 1 .</p><p>2. between-site reusability: a site creates new runs for a test collection wishes to compare their system to that of another site using the same test collection (presumably by looking at that site's published work).</p><p>3. baseline reusability: a site reuses a test collection created at TREC and wishes to compare their system to the systems that originally participated in the TREC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experimental Design and Analysis</head><p>The general form of the design is that there are n baseline topics to which all sites contribute judgments, and for each of the remaining N -n topics, k of m sites are chosen to be held out of judgment collection. This produces b subsets of the topic set (numbered T 1 . . . T b ) in which all m k possible contribution patterns are represented. By choosing the k sites in a round-robin fashion over all m k possibilities, we obtain a full randomized block design, as illustrated in Table <ref type="table" coords="11,284.21,576.25,3.87,8.74">4</ref>.</p><p>The design provides topics that can be used for each of the three types of reusability:</p><p>1. within-site: Within each subset T i , each site contributes to m-1 k topics and is held out from m-1 k-1 topics. Thus in addition to the n topics that all sites contribute to, each site contributes to b m-1 k topics that can be used as a site baseline, and to b m-1 k-1 topics that can be used for testing reusability by comparing results on those topics to results on the site baseline topics. In Table <ref type="table" coords="11,459.05,647.44,3.87,8.74">4</ref>, for instance, the within-site reusability set for site S 6 includes the first five topics in each subset, e.g. topics numbered n + 1 through n + 5 in subset T 1 . The within-site baseline includes the first n all-site baseline topics along with the last 10 in each subset, e.g. those numbered n + 6 through n + 15 in subset T 1 . Table <ref type="table" coords="12,100.06,145.64,3.87,8.74">5</ref>: Example of actual vs. expected agreement of 10 paired t-tests among five runs evaluated over a baseline set of topics (that the runs contributed to) and a reusability set (that the runs did not contribute to). Comparing the two tables using a χ 2 goodness-of-fit test produces a p-value of 0.88, meaning we cannot reject the hypothesis that the topics and judgments are reusable.</p><p>2. between-site: Within each subset T i , each pair of sites contributes to the same m-2 k topics and is held out of the same m-2 k-2 topics. The n + b m-2 k total topics those two sites contribute to form a baseline for comparisons between those sites. The b m-2 k-2 topics they were both held out from can be used to determine the between-site reusability. In Table <ref type="table" coords="12,347.34,251.34,3.87,8.74">4</ref>, the first topic in each subset can be used for testing reusability between sites S 5 and S 6 against the last six that both contributed to, along with the first n in the baseline.</p><p>3. baseline: Within each subset T i , there are m-2 k-1 topics that one site contributes to and another site does not. These topics can be used to evaluate comparing the non-contributing site to the contributing site. In Table <ref type="table" coords="12,158.51,318.33,3.87,8.74">4</ref>, if S 5 is the "participant baseline" and S 6 is the "new system", topics numbered n + 2 through n + 5 are part of the set used to test reusability.</p><p>Once we have relevance judgments, we can compute evaluation measures, rankings, and statistical significance on topics that systems contributed to and topics systems were held out from. If these evaluations disagree, we can reject the hypothesis that the collection may be reusable.</p><p>Carterette et al. present a detailed approach to testing agreement between sets of paired t-tests [CKPF10]: if t-test significance results between pairs of runs evaluated over one of the baseline sets disagree with t-test significance results between the same pairs evaluated over one of the reusability sets, then we reject the hypothesis that the collection is reusable. Table <ref type="table" coords="12,281.65,420.42,4.98,8.74">5</ref> illustrates the process of comparing results from 10 paired t-tests over two different sets of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Diagnostics</head><p>In this section we investigate some simple diagnostics to verify that there were no serious flaws in our experimental design. The main factor we will look at is whether any particular run or site had an undue effect on the judgments collected or the resulting evaluation results.</p><p>Did any site, when held out, cause fewer (or more) relevant documents to be found? On average, 24.13% of documents were judged relevant. It is the case that when the two best overall sites were held out, fewer relevant documents were found-queries for which UOG or EceUDel were held out had 22% relevant documents on average. It is also the case that when the worst overall site was held out, more relevant documents were found-queries for which NEU was held out had 25% relevant documents on average. No other site substantially changed the proportion of relevant documents when held out. Thus we can answer the question in the positive, but note that in practice these differences are miniscule, amounting to less than one relevant document per query for the number of judgments we made.</p><p>Did any site, when held out, have a strong effect on estimates of the number of relevant documents? To answer this, we looked at the statMAP estimate of R, the number of relevant documents (defined in Section 3.2). The average estimate of R is 85.77 relevant documents, with standard error 5.09. The average estimates for UOG and EceUDel are lower (76.08 and 79.74 respectively), but still within the 95% confidence interval of the mean. Interestingly, when UIUC is held out, R is 101.79 (± 9.16). It is surprising that holding out a well-performing site could increase R, though we note that the confidence intervals still overlap substantially.  Based on the above, it seems that there were no major problems with the experimental design as applied to the MQ track. Carterette et al. present deeper analysis on the design in general in a separate work <ref type="bibr" coords="13,493.51,311.27,41.32,8.74" target="#b10">[CKPF10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Results</head><p>Reusability results for MQ are illustrated in Figure <ref type="figure" coords="13,291.13,355.55,3.87,8.74" target="#fig_1">2</ref>, which shows statMAP (top) and MTC EMAP (bottom) scores of runs over (a) 145 baseline against 170 site baseline topics (left), (b) 170 site baseline against 160 site reuse topics (center), and (c) 145 baseline against 160 site reuse topics (right). Each run was evaluated over all the topics it contributed to and all the topics it was held out from, but since different sites contributed to different topics, no two sites were evaluated over exactly the same set of topics.</p><p>Differences in mean scores over baseline topics and mean scores over reusability topics for a given site may be due to a number of different effects: (1) the baseline and reuse topics are two different topic sets of different size; (2) apart from the site under study there are two other sites that did not contribute documents to each reusability topic; (3) the site under study itself did not contribute documents to the reuse topics (this is the actual effect we would like to quantify); and finally, (4) for this particular study the fact that both methods evaluate runs with a very small number of documents introduces some variability even in the baseline topics.</p><p>The plots in Figure <ref type="figure" coords="13,173.00,499.01,4.98,8.74" target="#fig_1">2</ref> attempt to separate the second and third effects. Essentially, the comparison of the mean scores between the 145 baseline topics and the 160 site reuse topics (right) summarizes the results of the reusability experiment, and it is what an actual new site would observe by using the MQ 2009 collection. StatMAP scores over the reuse topics are positively correlated with the statMAP scores over the baseline topics, though the correlation is rather weak. MTC EMAP scores over these two sets of topics are well correlated. One can consider the other two plots as the decomposition of the effects seen in the right plot. The left plot illustrates the effect of holding out sites other than the site under study. For the statMAP case this has a rather strong effect on the scores computed, though it is minimal for the MTC scores. The middle plots try to isolate the effect of holding out the site under study. As can be seen, this also has a strong effect on the statMAP scores, while the effect is mild in the case of the MTC scores.</p><p>The plots give a visual sense of reusability, suggesting within-site may be acceptable at the level of rank agreement if not score agreement, but between-site is likely not acceptable. To quantify this, we computed three correlation statistics. First we computed the overall Kendall's tau between the ranking induced by the scores in the two topic sets. This is a rough estimate of the between-site reusability. For statMAP scores this is 0.7643, while for MTC EMAP scores this is 0.8350, both of which are rather low. Next we computed the Kendall's τ among the runs of each individual site to estimate within-site reusability;  these. Note that the values are not comparable across sites since the number of runs compared affects the Kendall's τ values. Finally, we computed a τ -like correlation to quantify the ability to compare "new" runs to contributing participants. For each site, we count the number of its reusability runs that are correctly ordered against the baseline runs and the number that have been swapped with a baseline run. Every comparison involves exactly one run for that site; for this measure we do not compare two runs from the same site or two runs from a different site. The final value is determined identically to Kendall's τ ; the set of values can be seen in Table <ref type="table" coords="14,205.50,366.76,3.87,8.74" target="#tab_6">7</ref>.</p><p>The significance test agreement procedure, when applied to this data, suggests that there is not enough evidence to reject within-site reusability (p &gt; 0.5), but there is more than enough to reject between-site reusability (p &lt; 0.01). To explain how within-site reusability holds despite some of the low τ correlations in Table <ref type="table" coords="14,99.51,414.58,3.87,8.74" target="#tab_6">7</ref>, we note that τ is not able to capture anything about whether swaps are "reasonable". The lowest τ is -0.6 for UIUC, but by inspection (Fig. <ref type="figure" coords="14,258.60,426.53,4.43,8.74" target="#fig_1">2</ref>) UIUC's systems are all very close to each other. It is perfectly reasonable that they would be ordered differently over another set of topics, and thus the low τ is not a concern. For between-site reusability, however, we have seen that it is unlikely; that the χ 2 test confirms this is a point in its favor. The full contingency table for between-site reusability is shown in Table <ref type="table" coords="14,507.79,462.40,3.87,8.74" target="#tab_7">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Reusability Conclusions</head><p>Our conclusions are as follows:</p><p>1. We do not reject the hypothesis that it is possible to reuse MQ09 topics and judgments for withinsite comparisons, that is, comparisons between new runs that are developed by the same sites that contributed to the track.</p><p>2. We do reject the hypothesis that it is possible to reuse MQ09 topics and judgments for between-site comparisons, that is, comparisons between new runs developed by sites that did not contribute to the track.</p><p>Future work should investigate precisely why reusability failed and what could be done (e.g. more topics, more judgments, better MTC relevance models) to rectify the situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Query Class Evaluation</head><p>An optional task was to predict query intent type on two axes (precision-type vs recall-type, and hard vs easy), and potentially use an appropriate search strategy for each particular query type. Few sites attempted of the original queries is used and the systems rankings when all available queries are used.</p><p>For the system performance comparisons presented in Table <ref type="table" coords="16,358.13,502.38,4.98,8.74">5</ref> the 149 common to all systems queries were used. Out of these, 14 queries had no relevant documents found. In the analysis that follows we discard these queries 1 and use the remaining 135 queries -87 with 64 relevance judgments and 48 with 262 relevance judgments. The next step is to determine the extent to which the number of queries can be further reduced, and how to sample the categories to achieve similar results with less overall effort.</p><p>In the correlation studies that follow we employ three correlation statistics, (a) the Kendall's τ that corresponds to the minimum number of pairwise adjacent interchanges needed to convert one ranking into the other, (b) the AP-correlation <ref type="bibr" coords="16,224.46,586.07,36.94,8.74" target="#b17">[YAR08]</ref> that more heavily weights errors (discordant pairs of systems between the two rankings) towards the top of the rankings and thus identifies how much two different rankings agree over the effectiveness of good systems, and (c) d-rank <ref type="bibr" coords="16,371.77,609.98,30.05,8.74" target="#b8">[Car09]</ref>, a distance metric between two rankings that does not treat all errors equally; instead it accounts only for the significant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Correlation studies over subsets of queries</head><p>In this first study we sample queries without replacement from the pool of all 135 available queries, for different sample sizes. For each query subset size, 1000 query subsets are created. The systems mean average precision over each query subset is computed and all three correlation metrics (Kendall's τ , APcorrelation and d-rank) between the induced ranking of systems and the ranking of systems over the 135 queries are calculated.</p><p>The left-hand side plot in Figure <ref type="figure" coords="17,234.83,153.32,4.98,8.74" target="#fig_2">3</ref> shows the Kendall's τ and AP-correlation values on the y-axis with the number of queries in the query subset on the x-axis. Note that both metrics increase logarithmically with the number of queries. The solid red line indicates that MTC reaches a Kendall's τ of 0.9 after 60 queries and approximately 3,800 judgments, while statAP reaches the same Kendall's τ value after 80 queries and approximately 5,100 judgments. The AP-correlation values (dashed lines) are always lower than the Kendall's τ values indicating the difficulty in identifying the best performed systems. In particular, MTC reaches an AP-correlation of 0.9 after 80 queries and approximately 7,000 judgments, while statAP reaches the same value of AP-correlation after 92 queries and approximately 5,800 judgments.</p><p>Given that d-rank is in principle unbounded and the fact that its magnitude is highly dependent on the number of systems being ranked, the number of topics systems are evaluated over and the overall independence of the systems by the effectiveness metric, we perform a hypothesis test on the basis of d-rank to test whether the two rankings are the same <ref type="bibr" coords="17,281.16,284.83,31.58,8.74" target="#b8">[Car09]</ref> and report the p-values. A p-value sufficiently low (e.g. below 0.05) indicates that one can reject the hypothesis that the two rankings are the same with 95% confidence. When the p-values are greater than 0.05 the hypothesis cannot be rejected. The right-hand side plot in Figure <ref type="figure" coords="17,155.61,320.70,4.98,8.74" target="#fig_2">3</ref> shows the p-values of the rank distance hypothesis test on the y-axis with the number of queries on the x-axis. The blue line corresponds to statAP and the red to MTC. Note that opposite to Kendall's τ and AP-correlation the p-values increase approximately linearly with the number of queries. Furthermore, even with a subset of as low as 25 queries the hypothesis that the system rankings over that subset of queries and the rankings over all 135 queries are the same cannot be rejected. The p-values for MTC appear to be larger than the ones for statAP indicating that MTC leads faster to stable system rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Correlation studies for different query categories</head><p>We further consider whether a Kendall's τ and an AP-correlation of 0.9 may be reached with less effort when query types are sampled in different proportions. Out of the 135 common queries 87 were judged with respect to user intent (precision-oriented vs. recall-oriented) and query hardness (hard vs. medium vs. easy). We repeat the afore-described analysis by sampling queries at different proportions from each query category. Note that the empirical data to generate different splits is limited; as a result the maximum number of queries varies with the sampling proportion.</p><p>Figure <ref type="figure" coords="17,117.94,498.45,4.98,8.74" target="#fig_3">4</ref> shows the Kendall's τ (top row) and AP-correlation (bottom row) values for different number of queries and different sampling proportions from the hard, medium and easy categories both for statAP (left column) and MTC (right column). The limited number of queries per category does not allow us to calculate how many queries are needed to reach a 0.9 correlation. Given that extrapolation may not be statistically correct we only observe the rate of increase in the correlation scores. As it can be observed for statAP a query set biased towards easy queries (blue line with circular markers, green line with circular markers and black line plus markers) seem to lead faster to high Kendall's τ values. It is also apparent that a query set biased towards hard queries do not lead to rankings similar to the ones over all 135 queries. Similar conclusions can be drawn from the AP-correlation plot for statAP. Opposite to statAP, MTC appears to reach faster well correlated system rankings when the query set consists of queries of medium difficulty. This is particularly clear in the case of AP-correlation.</p><p>Given that the mean average precision scores are dominated by high the average precision values obtained over easy queries we repeated the same analysis by using the geometric mean of average precision values (GM AP = exp 1 n i log(AP i )). Figure <ref type="figure" coords="17,243.14,653.86,4.98,8.74" target="#fig_7">5</ref> shows the correlation scores as a function of the number of queries and the sampling proportions from the hard, medium and easy categories. Regarding statAP no clear 0%-0%-100% 0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0% 100%-0%-0% 33%-0%-66% 66%-0%-33% 33%-33%-33% 0%-0%-100% 0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0% 100%-0%-0% 33%-0%-66% 66%-0%-33% 33%-33%-33% 0%-0%-100% 0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0% 100%-0%-0% 33%-0%-66% 66%-0%-33% 33%-33%-33% 0%-0%-100% 0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0% 100%-0%-0% 33%-0%-66% 66%-0%-33% 33%-33%-33% 0%-0%-100% 0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0% 100%-0%-0% 33%-0%-66% 66%-0%-33% 33%-33%-33%   conclusions can be drawn, except that query sets biased towards easy queries demonstrate a slow increase in the ranking correlation. On the other hand medium difficulty queries seem to outperform any other query mixture when MTC is employed.</p><p>Figure <ref type="figure" coords="20,117.94,111.02,4.98,8.74">6</ref> shows the Kendall's τ (top row) and AP-correlation (bottom row) values for different number of queries and different sampling proportions from the precision-oriented and recall-oriented queries. As it can be observed both in the case of statAP and in the case of MTC, correlation values increase faster when the query set mostly consists of recall queries. Note that out of the 87 queries 33 are precision-oriented while 54 are recall-oriented (This particular proportion of queries approximately corresponds to the black line with the cross marker in the plots). Further note that the distribution of the precision-oriented queries over the query hardness categories is 36%, 36% and 28% over the hard, medium and easy categories, respectively. On the other hand, the distribution of the recall-oriented queries is 22%, 44% and 33% over the query hardness categories. This indicates that the recall-oriented queries are in general easier than the precision-oriented ones, which may explain the plots in Figure <ref type="figure" coords="20,267.17,218.62,3.87,8.74">6</ref>. Due to this we repeated the same analysis utilizing GMAP. The results of this analysis are illustrated in Figure <ref type="figure" coords="20,300.05,230.58,3.87,8.74" target="#fig_8">7</ref>. As it can be observed, the original query proportion along with the one with half of the queries being precision and the other half recall-oriented appear to lead faster to high correlation scores. On the other hand, MTC still appears to lead to high correlation scores when the query set consists of recall-oriented queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The Million Query Track ran for the third time in 2009. The track was designed to serve two purposes: first, it was an exploration of ad hoc retrieval over a large set of queries and a large collection of documents; second, it investigated questions of system evaluation, in particular whether it is better to evaluate using many queries judged shallowly or fewer queries judged thoroughly. The aspects of retrieval evaluations investigated was the reliability of the evaluation under this MQ evaluation setup and the reusability of the resulting test collection. A query intent and hardness prediction task investigated the ability of retrieval systems to correctly classify queries and possibly adapt the retrieval algorithms respectively.</p><p>Query Prediction Task. Even though only three sites attempted to classify queries based on their intent and hardness, the task appeared to be excessively hard. None of the sites that participated in the task did significantly better than random prediction.</p><p>Reusability. Conducting in situ reusability experiments we concluded that we cannot reject the hypothesis that it is possible to reuse MQ09 topics and judgments for within-site comparisons, that is, comparisons between new runs that are developed by the same sites that contributed to the track. However, we do reject the hypothesis that it is possible to reuse MQ09 topics and judgments for between-site comparisons, that is, comparisons between new runs developed by sites that did not contribute to the track. Future work should investigate precisely why reusability failed and what could be done (e.g. more topics, more judgments, better MTC relevance models, larger effectiveness differences) to rectify the situation.</p><p>Reliability. Conducting correlation studies based on three different statistics, Kendall's τ , AP-correlation and d-rank we concluded that to rank systems similarly to our 135 ground truth queries and 18,400 total judgments MTC needs about 60 to 80 queries and 3,800 to 5,100 judgments and statAP needs about 80 to 92 queries and 5,100 to 5,800 judgments. The d-rank analysis indicates that the system rankings are not significantly different with as low as 25-50 queries and 1,600 to 3,200 judgments. Furthermore, recall queries appear to be more useful in evaluation than the precision ones (both in the correlation studies and in the variance decomposition study). The analysis over query hardness when statAP is used heavily depends on the evaluation metric used. MTC appears to reach high correlation scores when medium hardness queries are used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,72.00,275.61,468.00,8.74;9,72.00,287.56,79.87,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Common 146 queries: Linear agreement of the two measures. Kendall's τ = 0.80, linear correlation coefficient ρ=0.90.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,72.00,254.98,468.00,8.74;13,72.00,266.93,434.59,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: StatMAP and MTC EMAP scores of systems over (a) 145 baseline against 170 site baseline topics, (b) 170 site baseline against 160 site reuse topics, and (c) 145 baseline against 160 site reuse topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="16,218.81,458.60,174.37,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Correlation between rankings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="18,72.00,345.13,468.00,8.74;18,72.00,357.09,459.61,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Kendall's tau (top row) and AP-correlation (bottom row) for statAP (left column) and MTC (right column) for different number of queries and different proportions from the hard/medium/easy categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="18,335.25,498.56,3.26,5.43;18,355.39,498.56,6.53,5.43;18,377.16,498.56,6.53,5.43;18,398.94,498.56,6.53,5.43;18,420.71,498.56,6.53,5.43;18,442.48,498.56,6.53,5.43;18,464.27,498.56,6.53,5.43;18,486.04,498.56,6.53,5.43;18,507.83,498.56,6.53,5.43;18,333.05,494.73,3.26,5.43;18,328.16,475.05,8.16,5.43;18,328.16,455.38,8.16,5.43;18,328.16,435.70,8.16,5.43;18,328.16,416.04,8.16,5.43;18,333.05,396.37,3.26,5.43;18,399.12,503.78,49.59,5.43;18,321.72,440.32,5.43,23.82;18,321.72,430.52,5.43,8.16;18,343.65,389.49,160.51,5.43"><head></head><label></label><figDesc>-Easy Queries in different proportions (MTC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="18,107.29,625.30,3.26,5.43;18,127.43,625.30,6.53,5.43;18,149.20,625.30,6.53,5.43;18,170.99,625.30,6.53,5.43;18,192.76,625.30,6.53,5.43;18,214.53,625.30,6.53,5.43;18,236.31,625.30,6.53,5.43;18,258.08,625.30,6.53,5.43;18,279.87,625.30,6.53,5.43;18,105.09,621.47,3.26,5.43;18,100.20,600.92,8.16,5.43;18,100.20,580.37,8.16,5.43;18,100.20,559.84,8.16,5.43;18,100.20,539.30,8.16,5.43;18,105.09,518.77,3.26,5.43;18,171.17,630.52,49.59,5.43;18,93.77,582.76,5.43,7.83;18,93.77,553.72,5.43,27.40;18,113.24,513.29,165.09,5.43"><head></head><label></label><figDesc>-Easy Queries in different proportions (statAP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="18,335.25,625.30,3.26,5.43;18,355.39,625.30,6.53,5.43;18,377.16,625.30,6.53,5.43;18,398.94,625.30,6.53,5.43;18,420.71,625.30,6.53,5.43;18,442.48,625.30,6.53,5.43;18,464.27,625.30,6.53,5.43;18,486.04,625.30,6.53,5.43;18,507.83,625.30,6.53,5.43;18,333.05,621.47,3.26,5.43;18,328.16,601.07,8.16,5.43;18,328.16,580.67,8.16,5.43;18,328.16,560.25,8.16,5.43;18,328.16,539.87,8.16,5.43;18,333.05,519.47,3.26,5.43;18,399.12,630.52,49.59,5.43;18,321.72,583.10,5.43,7.83;18,321.72,554.07,5.43,27.40;18,343.65,512.59,160.51,5.43"><head></head><label></label><figDesc>-Easy Queries in different proportions (MTC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="18,72.00,652.47,468.00,8.74;18,72.00,664.43,468.00,8.74;18,72.00,676.38,114.00,8.74"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Kendall's tau (top row) and AP-correlation (bottom row) for statAP (left column) and MTC (right column) for different number of queries and different proportions from the hard/medium/easy categories when GMAP is employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="19,72.00,655.32,468.00,8.74;19,72.00,667.28,468.00,8.74;19,72.00,679.23,160.03,8.74"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Kendall's tau (top row) and AP-correlation (bottom row) values for statAP (left column) and MTC (right column) for different number of queries and different proportions from the precision-/recall-oriented categories when GMAP is employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,79.18,73.96,450.32,66.46"><head>Table 1 :</head><label>1</label><figDesc>Groups participating in the 2009 Million Query track</figDesc><table coords="6,79.18,73.96,450.32,44.60"><row><cell>International Institute of Information Technology (SIEL)</cell><cell>IRRA</cell></row><row><cell>Northeastern University</cell><cell>Sabir Research, Inc.</cell></row><row><cell>University of Delaware (Carterette)</cell><cell>University of Delaware (Fang)</cell></row><row><cell>University of Glasgow</cell><cell>University of Illinois at Urbana-Champaign</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,522.84,672.58,17.16,8.74"><head></head><label></label><figDesc>The last two runs are the base case SMART runs, i.e., ltu.lnu, with no expansion. The other three runs used blind feedback method with ltu.Lnu as the initial runs. Specifically, (1) Sab9mq1bf1 assumed that top 25 documents are relevant and all other documents are non-relevant. 20 query terms are added to each query. The a,b,c weights for Rocchio feedback methods are respectively 32, 64, 128. (2) Sab9mq1bf4 assumed that top 15 documents are relevant and all other documents are non-relevant. 5 query terms are added to each query. The a,b,c weights for Rocchio feedback methods are respectively 32, 64, 128. (3) Sab9mq2bf1 assumed that top 25 documents are relevant and all other documents are non-relevant. 20 query terms are added to each query. The a,b,c weights for Rocchio feedback methods are respectively 32, 8, 0. They used Amchormap for query class prediction, ratio of sim at rank to sim at rank 0 for precision. Sort topics, and label top 25University of Delaware</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,73.72,468.00,608.45"><head>Table 2 :</head><label>2</label><figDesc>MQ09 runs evaluated by EMAP and statMAP over 146 topics to which all sites contributed judgments. EMAP estimates use relevance predictions based on document similarity features (the erels docsim.20001-60000 file). using these common 146 queries only. These results are presented in Table5. The best run (EMAP=0.12, statMAP=0.227) performs significantly worse than the performance of best runs in previous years ad hoc tracks; we speculate that the main reason for this is that most sites have not yet adapted their methods to the general web.Table5presents confidence scores. The MTC confidences can be interpreted as the probability that the run performs better (in terms of MAP) than the run below given the provided judgments and model used</figDesc><table coords="8,100.31,73.72,404.41,470.32"><row><cell>run</cell><cell cols="4">EMAP MTC confidence statMAP statMAP conf interval</cell></row><row><cell>UDMQAxQEWeb</cell><cell>0.124</cell><cell>N/A</cell><cell>0.227</cell><cell>± 0.032</cell></row><row><cell>UDMQAxQE</cell><cell>0.099</cell><cell>0.793</cell><cell>0.149</cell><cell>± 0.027</cell></row><row><cell>UDMQAxQEWP</cell><cell>0.090</cell><cell>0.594</cell><cell>0.133</cell><cell>± 0.030</cell></row><row><cell>uogTRMQdph40</cell><cell>0.089</cell><cell>0.503</cell><cell>0.198</cell><cell>± 0.026</cell></row><row><cell>uogTRMQdpA10</cell><cell>0.087</cell><cell>0.575</cell><cell>0.195</cell><cell>± 0.025</cell></row><row><cell>uiuc09GProx</cell><cell>0.086</cell><cell>0.511</cell><cell>0.183</cell><cell>± 0.023</cell></row><row><cell>uiuc09MProx</cell><cell>0.082</cell><cell>0.682</cell><cell>0.179</cell><cell>± 0.024</cell></row><row><cell>UDMQAxBL</cell><cell>0.079</cell><cell>0.538</cell><cell>0.192</cell><cell>± 0.024</cell></row><row><cell>uiuc09Adpt</cell><cell>0.079</cell><cell>0.500</cell><cell>0.180</cell><cell>± 0.023</cell></row><row><cell>uiuc09RegQL</cell><cell>0.079</cell><cell>0.521</cell><cell>0.175</cell><cell>± 0.022</cell></row><row><cell>udelIndSP</cell><cell>0.075</cell><cell>0.566</cell><cell>0.169</cell><cell>± 0.022</cell></row><row><cell>udelIndRM</cell><cell>0.073</cell><cell>0.540</cell><cell>0.155</cell><cell>± 0.020</cell></row><row><cell>udelIndDM</cell><cell>0.072</cell><cell>0.522</cell><cell>0.173</cell><cell>± 0.022</cell></row><row><cell>Sab9mq1bf1</cell><cell>0.071</cell><cell>0.513</cell><cell>0.130</cell><cell>± 0.029</cell></row><row><cell>uiuc09KL</cell><cell>0.071</cell><cell>0.505</cell><cell>0.171</cell><cell>± 0.020</cell></row><row><cell>udelIndri</cell><cell>0.069</cell><cell>0.538</cell><cell>0.169</cell><cell>± 0.021</cell></row><row><cell>Sab9mq1bf4</cell><cell>0.067</cell><cell>0.521</cell><cell>0.122</cell><cell>± 0.023</cell></row><row><cell>irra1mqa</cell><cell>0.066</cell><cell>0.507</cell><cell>0.156</cell><cell>± 0.024</cell></row><row><cell>Sab9mq2bf1</cell><cell>0.062</cell><cell>0.556</cell><cell>0.127</cell><cell>± 0.022</cell></row><row><cell>UDMQAxBLlink</cell><cell>0.059</cell><cell>0.531</cell><cell>0.144</cell><cell>± 0.022</cell></row><row><cell>iiithExpQry</cell><cell>0.055</cell><cell>0.539</cell><cell>0.130</cell><cell>± 0.034</cell></row><row><cell>udelIndPR</cell><cell>0.054</cell><cell>0.511</cell><cell>0.123</cell><cell>± 0.015</cell></row><row><cell>Sab9mqBase1</cell><cell>0.052</cell><cell>0.522</cell><cell>0.111</cell><cell>± 0.021</cell></row><row><cell>Sab9mqBase4</cell><cell>0.052</cell><cell>0.500</cell><cell>0.111</cell><cell>± 0.021</cell></row><row><cell>irra2mqa</cell><cell>0.049</cell><cell>0.540</cell><cell>0.132</cell><cell>± 0.024</cell></row><row><cell>irra1mqd</cell><cell>0.048</cell><cell>0.520</cell><cell>0.103</cell><cell>± 0.018</cell></row><row><cell>irra3mqd</cell><cell>0.048</cell><cell>0.509</cell><cell>0.105</cell><cell>± 0.019</cell></row><row><cell>iiithAuEQ</cell><cell>0.045</cell><cell>0.535</cell><cell>0.097</cell><cell>± 0.022</cell></row><row><cell>iiithAuthPN</cell><cell>0.045</cell><cell>0.535</cell><cell>0.096</cell><cell>± 0.022</cell></row><row><cell>irra2mqd</cell><cell>0.040</cell><cell>0.558</cell><cell>0.097</cell><cell>± 0.015</cell></row><row><cell>NeuSvmStefan</cell><cell>0.034</cell><cell>0.569</cell><cell>0.077</cell><cell>± 0.017</cell></row><row><cell>NeuSvmBase</cell><cell>0.024</cell><cell>0.679</cell><cell>0.079</cell><cell>± 0.019</cell></row><row><cell>NeuSvmPR</cell><cell>0.023</cell><cell>0.530</cell><cell>0.076</cell><cell>± 0.018</cell></row><row><cell>NeuSVMHE</cell><cell>0.023</cell><cell>0.507</cell><cell>0.078</cell><cell>± 0.017</cell></row><row><cell>NeuSvmPRHE</cell><cell>0.022</cell><cell>0.540</cell><cell>0.073</cell><cell>± 0.015</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,477.32,678.34,62.68,8.74"><head>Table 7</head><label>7</label><figDesc></figDesc><table coords="13,514.71,678.34,25.29,8.74"><row><cell>shows</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="14,72.00,145.64,468.00,91.77"><head>Table 6 :</head><label>6</label><figDesc>Observed versus expected agreement in significance results for between-site reusability aggregated over all Million Query 2009 sites. The χ 2 p-value is 0, indicating sufficient evidence to reject reusability.</figDesc><table coords="14,81.83,180.05,448.35,57.36"><row><cell></cell><cell></cell><cell cols="5">iiith IRRA NEU Sabir UDel ECEUdel UIUC uogTr</cell></row><row><cell>within-site τ</cell><cell cols="2">statAP 0.333</cell><cell>1.000 0.200 0.333 0.800</cell><cell cols="2">0.800 -0.600</cell><cell>1.000</cell></row><row><cell></cell><cell>MTC</cell><cell>0.333</cell><cell>0.800 1.000 1.000 0.600</cell><cell>0.800</cell><cell>0.800</cell><cell>1.000</cell></row><row><cell cols="3">participant comparison statAP 0.750</cell><cell>0.547 1.000 0.987 0.573</cell><cell>0.773</cell><cell>0.773</cell><cell>0.939</cell></row><row><cell></cell><cell>MTC</cell><cell>0.938</cell><cell>1.000 1.000 0.840 0.933</cell><cell>0.707</cell><cell>0.947</cell><cell>0.909</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="14,72.00,251.09,468.00,20.69"><head>Table 7 :</head><label>7</label><figDesc>Rank correlations based on Kendall's τ for site baseline to site reusability (top) and for comparison of site reusability to the "original" TREC runs excluding those treated as new (bottom).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="17,83.09,683.22,349.33,8.44"><head></head><label></label><figDesc>1 statAP does not produce estimates of AP values for queries that have no relevant documents</figDesc><table coords="18,93.77,78.23,420.59,250.37"><row><cell></cell><cell></cell><cell cols="5">Hard -Medium -Easy Queries in different proportions (statAP)</cell><cell></cell><cell></cell><cell cols="5">Hard -Medium -Easy Queries in different proportions (MTC)</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0%-0%-100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0%-0%-100%</cell></row><row><cell>Kendall's tau</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0%</cell><cell>Kendall's tau</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">100%-0%-0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">100%-0%-0%</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">33%-0%-66% 66%-0%-33%</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">33%-0%-66% 66%-0%-33%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">33%-33%-33%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">33%-33%-33%</cell></row><row><cell></cell><cell>0 0</cell><cell>10</cell><cell>20</cell><cell>30 Number of Queries 40 50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>0 0</cell><cell>10</cell><cell>20</cell><cell>30 Number of Queries 40 50</cell><cell>60</cell><cell>70</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell cols="5">Hard -Medium -Easy Queries in different proportions (statAP)</cell><cell></cell><cell></cell><cell cols="5">Hard -Medium -Easy Queries in different proportions (MTC)</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0%-0%-100%</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AP correlation</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0%-33%-66% 0%-66%-33% 0%-100%-0% 33%-66%-0% 66%-33%-0%</cell><cell>AP correlation</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">100%-0%-0% 33%-0%-66%</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">66%-0%-33%</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">33%-33%-33%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell>10</cell><cell>20</cell><cell>30 Number of Queries 40 50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>0 0.1</cell><cell>10</cell><cell>20</cell><cell>30 Number of Queries 40 50</cell><cell>60</cell><cell>70</cell><cell>80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="19,72.00,75.38,468.00,563.69"><head></head><label></label><figDesc>Figure 6: Kendall's tau (top row) and AP-correlation (bottom row) for statAP (left column) and MTC (right column) for different number of queries and different proportions from the precision-/recall-oriented categories.</figDesc><table coords="19,93.28,75.38,422.71,563.69"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Precision -Recall Queries in different proportions (MTC)</cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision -Recall Queries in different proportions (statAP)</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>Kendall's tau</cell><cell>0.4 0.6 0.8</cell><cell></cell><cell></cell><cell cols="2">0%-100% 25%-75% 33%-66% 50%-50% 75%-25%</cell><cell>Kendall's tau</cell><cell>0.5 0.6 0.7 0.4</cell><cell></cell><cell></cell><cell>0%-100% 25%-75% 33%-66% 50%-50% 75%-25%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">66%-33% 100%-0%</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell>66%-33% 100%-0%</cell></row><row><cell></cell><cell>0 0.2</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell><cell></cell><cell>0 0.2</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Precision -Recall Queries in different proportions (MTC)</cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision -Recall Queries in different proportions (statAP)</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>AP correlation</cell><cell>0.4 0.6 0.2</cell><cell></cell><cell></cell><cell cols="2">0%-100% 25%-75% 33%-66% 50%-50% 75%-25% 66%-33% 100%-0%</cell><cell>AP correlation</cell><cell>0.5 0.6 0.7 0.4 0.3</cell><cell></cell><cell></cell><cell>0%-100% 25%-75% 33%-66% 50%-50% 75%-25% 66%-33% 100%-0%</cell></row><row><cell></cell><cell>0 0</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell><cell></cell><cell>0 0.2</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision -Recall Queries in different proportions (statAP)</cell><cell></cell><cell></cell><cell>1</cell><cell cols="3">Precision -Recall Queries in different proportions (MTC)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>Kendall's tau</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell cols="2">0%-100% 25%-75% 33%-66%</cell><cell>Kendall's tau</cell><cell>0.5 0.6 0.7</cell><cell></cell><cell></cell><cell>0%-100% 25%-75% 33%-66%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">50%-50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50%-50%</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell cols="2">75%-25% 66%-33% 100%-0%</cell><cell></cell><cell>0.4 0.3</cell><cell></cell><cell></cell><cell>75%-25% 66%-33% 100%-0%</cell></row><row><cell></cell><cell>0 0</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell><cell></cell><cell>0 0.2</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell>1</cell><cell cols="3">Precision -Recall Queries in different proportions (statAP)</cell><cell></cell><cell></cell><cell>1</cell><cell cols="3">Precision -Recall Queries in different proportions (MTC)</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>AP correlation</cell><cell>0.2 0.4 0.6</cell><cell></cell><cell></cell><cell cols="2">0%-100% 25%-75% 33%-66% 50%-50% 75%-25% 66%-33% 100%-0%</cell><cell>AP correlation</cell><cell>0.4 0.6 0.2</cell><cell></cell><cell></cell><cell>0%-100% 25%-75% 33%-66% 50%-50% 75%-25% 66%-33% 100%-0%</cell></row><row><cell></cell><cell>0 0</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell><cell></cell><cell>0 0</cell><cell>20</cell><cell>40 Number of Queries 60</cell><cell>80</cell><cell>100</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>this. While we let the participants explain how such query-type strategy affected their performance, we are providing a "truth assessment" of these categories for the judged queries.</p><p>We present the effectiveness of the runs at predicting query category as a confusion matrix, separately for precision-recall (Table <ref type="table" coords="15,185.76,471.59,4.43,8.74">8</ref>) and hard-easy (Table <ref type="table" coords="15,292.50,471.59,3.87,8.74">9</ref>). The precision-recall assessment was performed by the relevance judge, by choosing exactly one out of 6 predefined query categories. Overall the sites that make precision-recall predictions did so on 68% of the queries. In the cases where a prediction was made, it was right 55% of the time.</p><p>The hard-easy assessment was obtained by partitioning the query-AAP (by statAP) score range into 3 intervals: hard = [0, 0.06), medium = [0.06, 0.17), easy = [0.17, max]. Overall the sites made hard-easy predictions on 60% of the queries. On these, counting as errors only assessed-hard-predicted-easy and assessed-easy-predicted-hard (that is, excluding the queries assessed "medium", which could be interpreted either way), site predictions were wrong 16% of the time. The system predictions (hard or easy) were assessed as "medium" in 20% of the cases.</p><p>As it can be viewed in both tables predicting the query intent and the query hardness appears to be an enormously hard task. In particular, systems choose to not make predictions on a large number of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Query Analysis</head><p>The aim of this section is to answer the following question: what is the number of queries needed to guarantee that, when systems are run over this many queries, their effectiveness scores reflect their actual performance? To answer this question we conduct a study based on the correlation of the systems rankings when a subset  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="21,78.16,96.98,18.47,8.74;21,96.63,95.40,6.12,6.12;21,103.24,96.98,436.75,8.74;21,120.96,108.93,406.60,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="21,168.60,108.93,215.39,8.74">Overview of the TREC 2007 Million Query Track</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blagovest</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Dachev</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kanoulas</surname></persName>
		</author>
		<idno>ACA + 07</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,405.56,108.93,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,128.86,419.04,8.74;21,120.96,140.81,71.45,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="21,270.93,128.86,264.76,8.74">A practical sampling strategy for efficient retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Pavlu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>technical report</note>
</biblStruct>

<biblStruct coords="21,120.96,160.74,419.05,8.74;21,120.96,172.70,331.07,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="21,350.99,160.74,189.02,8.74;21,120.96,172.70,119.33,8.74">A statistical method for system evaluation using incomplete judgments</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,261.92,172.70,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,192.62,419.04,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="21,259.96,192.62,157.73,8.74">Sampling With Unequal Probabilities</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R W</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hanif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,212.55,419.05,8.74;21,120.96,224.50,90.83,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="21,237.21,212.55,182.72,8.74">Test theory for assessing ir test collection</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bodoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,443.01,212.55,91.82,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,244.43,419.04,8.74;21,120.96,256.38,151.16,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="21,328.49,244.43,211.51,8.74;21,120.96,256.38,17.71,8.74">Blind men and elephants: Six approaches to trec data</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nien-Fan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,147.51,256.38,39.68,8.74">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="34" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,276.31,344.39,8.74" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brennan</surname></persName>
		</author>
		<title level="m" coord="21,211.08,276.31,99.42,8.74">Generalizability Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,296.23,419.04,8.74;21,120.96,308.19,53.68,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="21,194.08,296.23,200.38,8.74">Robust test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,416.36,296.23,91.12,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,328.11,419.05,8.74;21,120.96,340.07,419.04,8.74;21,120.96,352.02,279.67,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="21,193.03,328.11,237.61,8.74">On rank correlation and the distance between rankings</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,451.57,328.11,88.43,8.74;21,120.96,340.07,419.04,8.74;21,120.96,352.02,55.19,8.74">SIGIR &apos;09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="436" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,371.95,419.03,8.74;21,120.96,383.90,254.30,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,379.93,371.95,160.07,8.74;21,120.96,383.90,43.02,8.74">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><forename type="middle">K</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,185.15,383.90,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,403.83,419.05,8.74;21,120.96,415.78,265.97,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="21,430.14,403.83,109.86,8.74;21,120.96,415.78,122.86,8.74">Reusable test collections through experimental design</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,264.96,415.78,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,72.00,435.71,24.49,8.74;21,96.49,434.13,6.12,6.12;21,103.11,435.71,436.90,8.74;21,120.96,447.66,319.83,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="21,493.09,435.71,46.91,8.74;21,120.96,447.66,108.72,8.74">Evaluation over thousands of queries</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<idno>CPK + 08</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,250.68,447.66,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,467.59,419.04,8.74;21,120.96,479.54,189.41,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="21,279.02,467.59,242.72,8.74">Hypothesis testing with incomplete relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,120.96,479.54,89.20,8.74">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="643" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,499.47,419.05,8.74;21,120.96,511.42,388.86,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="21,291.79,499.47,174.07,8.74">Understanding user goals in web search</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danny</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,489.67,499.47,50.33,8.74;21,120.96,511.42,299.20,8.74">WWW &apos;04: Proceedings of the 13th international conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,531.35,419.05,8.74;21,120.96,543.31,403.21,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="21,190.53,531.35,295.75,8.74">Sampling without replacement with probability proportional to size</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">L</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,494.99,531.35,45.01,8.74;21,120.96,543.31,237.92,8.74">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="397" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.96,563.23,418.41,8.74" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sampling</surname></persName>
		</author>
		<title level="m" coord="21,266.97,563.23,242.50,8.74">Wiley Series in Probability and Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,583.16,419.05,8.74;21,120.96,595.11,253.94,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="21,279.88,583.16,260.13,8.74;21,120.96,595.11,42.89,8.74">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,185.48,595.11,89.20,8.74">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,120.95,615.04,419.04,8.74;21,120.96,626.99,419.05,8.74;21,120.96,638.95,419.04,8.74;21,120.96,650.90,419.04,8.74;21,120.96,662.86,44.69,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="21,376.21,615.04,163.79,8.74;21,120.96,626.99,89.28,8.74">A new rank correlation coefficient for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,285.05,638.95,254.95,8.74;21,120.96,650.90,290.94,8.74">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Sung-Hyon</forename><surname>Myaeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mun-Kew</forename><surname>Leong</surname></persName>
		</editor>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="587" to="594" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
