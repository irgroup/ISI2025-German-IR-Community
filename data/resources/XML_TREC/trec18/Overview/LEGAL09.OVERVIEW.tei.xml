<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,132.68,112.00,346.64,15.15">Overview of the TREC 2009 Legal Track</title>
				<funder>
					<orgName type="full">TREC Legal Track</orgName>
				</funder>
				<funder>
					<orgName type="full">Wachtell, Lipton</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,236.66,145.86,61.21,8.77"><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
							<email>bhedin@h5.com</email>
							<affiliation key="aff0">
								<orgName type="institution">H5</orgName>
								<address>
									<addrLine>71 Stevenson St</addrLine>
									<postCode>94105</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.57,187.71,93.63,8.77"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<email>stomlins@opentext.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.10,229.55,75.67,8.77"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
							<email>jason.baron@nara.gov</email>
							<affiliation key="aff2">
								<address>
									<postCode>20740</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.14,285.34,85.35,8.77"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff3">
								<orgName type="department">College of Information Studies and Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,132.68,112.00,346.64,15.15">Overview of the TREC 2009 Legal Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76B211557EF69FA1AE766BFA09B8F7A3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>tot=10</term>
					<term>hrel=4</term>
					<term>orel=1</term>
					<term>non=3</term>
					<term>gr=2 tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 28 (2006-C-4) tot=10</term>
					<term>hrel=9</term>
					<term>orel=0</term>
					<term>non=1</term>
					<term>gr=0 tot=10</term>
					<term>hrel=3</term>
					<term>orel=1</term>
					<term>non=6</term>
					<term>gr=0 31 (2006-C-7) tot=10</term>
					<term>hrel=6</term>
					<term>orel=3</term>
					<term>non=1</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=1</term>
					<term>non=8</term>
					<term>gr=0 36 (2006-D-3) tot=10</term>
					<term>hrel=0</term>
					<term>orel=7</term>
					<term>non=3</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 47 (2006-E-6) tot=6</term>
					<term>hrel=0</term>
					<term>orel=2</term>
					<term>non=4</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=2</term>
					<term>non=8</term>
					<term>gr=0 60 (2007-A-9) tot=4</term>
					<term>hrel=2</term>
					<term>orel=2</term>
					<term>non=0</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=2</term>
					<term>non=7</term>
					<term>gr=0 73 (2007-B-5) tot=10</term>
					<term>hrel=1</term>
					<term>orel=0</term>
					<term>non=9</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=3</term>
					<term>non=6</term>
					<term>gr=0 79 (2007-C-1) tot=9</term>
					<term>hrel=4</term>
					<term>orel=3</term>
					<term>non=2</term>
					<term>gr=0 tot=10</term>
					<term>hrel=1</term>
					<term>orel=2</term>
					<term>non=7</term>
					<term>gr=0 80 (2007-C-2) tot=10</term>
					<term>hrel=0</term>
					<term>orel=8</term>
					<term>non=2</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=2</term>
					<term>non=8</term>
					<term>gr=0 83 (2007-C-5) tot=10</term>
					<term>hrel=0</term>
					<term>orel=4</term>
					<term>non=6</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=10</term>
					<term>gr=0 85 (2007-C-7) tot=9</term>
					<term>hrel=0</term>
					<term>orel=0</term>
					<term>non=9</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=1</term>
					<term>non=9</term>
					<term>gr=0 89 (2007-D-1) tot=8</term>
					<term>hrel=2</term>
					<term>orel=6</term>
					<term>non=0</term>
					<term>gr=0 tot=10</term>
					<term>hrel=0</term>
					<term>orel=1</term>
					<term>non=9</term>
					<term>gr=0 Totals tot=106</term>
					<term>hrel=28</term>
					<term>orel=36</term>
					<term>non=40</term>
					<term>gr=2 tot=120</term>
					<term>hrel=7</term>
					<term>orel=15</term>
					<term>non=98</term>
					<term>gr=0</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2009 was the fourth year of the Legal Track, which focuses on evaluation of search technology for "discovery" (i.e., responsive review) of electronically stored information in litigation and regulatory settings. The track included two tasks: an Interactive task (in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback) and a Batch task (two-pass search in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass). This paper describes the design of the two tasks and presents the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Correction to 2008 Assessor Consistency Study</head><p>Last year's track overview paper <ref type="bibr" coords="33,219.53,654.10,15.50,8.74" target="#b12">[13]</ref> reported the results of a small assessor consistency study which was conducted as part of the Relevance Feedback task of the TREC 2008 Legal Track. For the study, a reference "oldrel08" run was created by randomly choosing 10 documents from the relevant documents in the training Topic Previously Judged Relevant (oldrel08Fixed)</p><p>Prev. Judged Non-relevant (oldnon08)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Until relatively recently, the principal uses of information retrieval techniques in law had focused on providing access to legislation, regulations, and judicial decisions. The goal of the Legal Track at the Text Retrieval Conference (TREC), by contrast, has been to assess the ability of information retrieval methods and technologies to meet the needs of the legal community for tools and methods capable of helping with the retrieval of electronic business records, an issue of increasing importance given the vast amount of information stored in electronic form to which access is increasingly desired in the context of current litigation. In this context, the problem is often referred to as "e-discovery," referring not to the broad goal of discovering new things, but rather to the task of producing specific records in response to a "discovery request."</p><p>In the first year (2006), 6 teams participated in the Legal Track's single (Ad Hoc retrieval) task <ref type="bibr" coords="1,512.46,584.35,9.96,8.74" target="#b8">[9]</ref>. In 2007, participation grew to 13 teams and two additional tasks were introduced (Interactive and Relevance Feedback) <ref type="bibr" coords="1,119.30,608.26,14.61,8.74" target="#b14">[15]</ref>. In 2008, 16 teams participated <ref type="bibr" coords="1,279.45,608.26,14.61,8.74" target="#b12">[13]</ref>; this year there were 15 participating teams. A notable trend has been the increasing number of commercial enterprises fielding teams; rising from 1 in each of the first two years to 3 in 2009 and now to 10 in 2009.</p><p>The basic design of the track is similar to other TREC tracks, with document collections, topic sets, relevance judgments, and evaluation measures that are shared by participants to create test collections with enduring value, to report results against which future work can be compared, and to foster the development of a research community that is well prepared to continue that work. The track is distinguished from others at TREC by the combination of a focus on business records as documents, representative discovery requests as topics, relevance judgments by legal professionals and law students, evaluation measures for retrieval of sets of documents, and (in one task) modeling an interactive search process. The track's two test collections, built from scanned documents and from email, will also be of interest more generally to researchers interested in conducting information retrieval experiments with those document types.</p><p>This has been a year of transition for the track, completing our work with scanned documents and beginning the process of developing an email test collection. The 2009 track therefore included two tasks, a Batch task using scanned documents that served to enrich the available relevance judgments for some previously developed topics, and an Interactive task that developed an initial set of relevance judgments for the newly developed email test collection.</p><p>The remainder of this paper is organized as follows. Section 2 describes the 2009 Interactive task; Section 3 describes the 2009 Batch task; Section 4 provides some follow-up discussion of assessor consistency studies from 2008 and 2007; and Section 5 concludes the paper with a few remarks on our present plans for 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Interactive Task</head><p>In 2008, the Legal Track, seeking to develop an exercise that modeled more completely and accurately the task of reviewing documents for responsiveness to a request for production in civil litigation, introduced a redesigned Interactive task (see the 2008 Interactive Task Guidelines <ref type="bibr" coords="2,376.61,297.20,10.30,8.74" target="#b6">[7]</ref>). The task saw participation from four teams (two academic and two commercial) and produced interesting results, both with regard to the effectiveness of the approaches evaluated and with regard to the evaluation design itself (see Overview of the TREC 2008 Legal Track <ref type="bibr" coords="2,181.75,333.07,14.76,8.74" target="#b12">[13]</ref>).</p><p>In 2009, the Legal Track again featured the Interactive task, this time with a new test collection and with a few minor modifications to the task design. The 2009 exercise saw participation from eleven teams (three academic and eight commercial). In this section, we summarize the results from the 2009 Interactive task. More specifically, we (i) briefly review the task design; (ii) summarize the specific features that defined the 2009 exercise; (iii) present the results obtained by the 2009 participants; (iv) expand on a few points that merit further analysis; and (v) summarize key lessons from the 2009 Interactive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Design</head><p>The Legal Track's Interactive task models the conditions and objectives of a review for responsiveness; that is to say, the task models the conditions and objectives of a search for documents that are responsive to a request for production that has been served during the discovery phase of a civil lawsuit. A full discussion of the circumstance modeled and of the general design of the exercise can be found in the 2008 task guidelines <ref type="bibr" coords="2,526.72,486.84,9.96,8.74" target="#b6">[7]</ref>. For purposes of the current overview, we briefly summarize the key features of the task.</p><p>• Complaint and Topics. Context for the Interactive task is provided by a mock complaint that sets forth the legal and factual basis for the hypothetical lawsuit that motivates the discovery requests at the heart of the exercise. Associated with the complaint are document requests that specify the categories of documents which must be located and produced. For purposes of the Interactive task, each of these document requests serves as a separate topic. The goal of a team participating in a given topic is to retrieve all, and only, documents relevant to that topic (as defined by the "Topic Authority;" see below).</p><p>• The Topic Authority. A key role in the task is played by the "Topic Authority." The Topic Authority plays the role of a senior attorney who is charged with overseeing a client's response to a request for production and who, in that capacity, must certify to the court that their client's response to the request is complete and correct (commensurate with a reasonable and good-faith effort). In keeping with that role, it is the Topic Authority who, taking into account considerations of genuine subject-matter relevance as well as pragmatic considerations of legal strategy and tactics, holds ultimate responsibility for deciding what is and is not relevant to a target topic (or, in real-world terms, what is and is not responsive to a document request). The Topic Authority's role, then, is to be the source for the authoritative conception of responsiveness that each participating team, in the role of a hired cohort of manual reviewers or of a vendor of document-retrieval services, will be asked to replicate across the full document collection. Each topic has a single Topic Authority, and each Topic Authority has responsibility for a single topic.</p><p>• Interaction with the Topic Authority. If it is the Topic Authority who defines the target (i.e., who determines what should and should not be considered relevant to a topic), it is essential that provision be made for teams to be able to interact with the Topic Authority in order to gain a better understanding of the Topic Authority's conception of relevance. In the Interactive task, this provision takes the following form. Each team can ask, for each topic for which it plans to submit results, for up to 10 hours of a Topic Authority's time for purposes of clarifying a topic. A team can call upon a Topic Authority at any point in the exercise, from the kickoff of the task to the deadline for the submission of results. How a team makes use of the Topic Authority's time is largely unrestricted: a team can ask the Topic Authority to pass judgment on exemplar documents; a team can submit questions to the Topic Authority by email; a team can arrange for conference calls to discuss aspects of the topic. One constraint that is placed on communication between the teams and their designated Topic Authorities is introduced in order to minimize the sharing of information developed by one team with another; while the Topic Authorities are instructed to be free in sharing the information they have about their topics, they are asked to avoid volunteering to one team specific information that was developed only in the course of interaction with another team.</p><p>• Participant submissions. Each team's final deliverable is a binary classification of the full population for relevance to each target topic in which it has chosen to participate.</p><p>• Effectiveness Metrics. Given the nature of the submissions (sets of documents identified as relevant to a topic), we look to set-based metrics to gauge effectiveness. In the Interactive task, the metrics used are recall, precision, and, as a summary measure of effectiveness, F 1 .</p><p>• Sampling and Estimation. In order to obtain estimates of effectiveness scores, we use stratified sampling and a two-stage sample assessment protocol. Further specifics are as follows.</p><p>-Sampling. The sets of documents submitted by the participants in a topic allow for a straightforward submission-based stratification of the document collection: one stratum contains the documents all participants submitted as relevant, another stratum contains the documents no participant submitted as relevant, and other strata will be defined for each of the other possible submission combinations. If, for example, there are 5 teams that participated in a topic, the collection will be partitioned into 2 5 = 32 strata. In creating samples, strata are represented largely in keeping with their full-population proportions. In order to ensure that a sufficient number of documents are drawn from all strata, however, some small strata may be over-represented, and some large strata under-represented, relative to their full-population proportions. Selection within a stratum is simple random selection without replacement.</p><p>-First-Pass Assessment. For purposes of assessment, the contents of each sample is randomly assigned to "bins" of approximately 500 documents and these bins are then distributed to teams of manual assessors. Each assessor, equipped with detailed assessment guidelines and provided with access to the Topic Authority, assesses each document in his or her bin for relevance to his or her assigned topic.</p><p>-Appeal and Adjudication. No matter how rigorous the quality control regimen of the first-pass assessment, it is to be expected that some errors will remain in the sets of assessments that are the output of the first-pass review. As a corrective measure, the Interactive task features an additional appeal/adjudication phase, whereby teams are given the opportunity to review the results of the first-pass assessment and appeal, to the Topic Authority, any assessments they believe are incorrect. The Topic Authority then renders a final judgment on all appealed assessments.</p><p>-Estimation. Once all appeals have been adjudicated, we are in a position to obtain estimates both of the full-population yield of relevant documents for each topic and of each participant's effectiveness scores (recall, precision, F 1 ) for each topic. For further detail on the estimation procedures followed in the Interactive task, see the appendix to the Overview of the TREC 2008 Legal Track <ref type="bibr" coords="4,173.51,122.98,14.61,8.74" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Specifics</head><p>Within the general framework just sketched, a few additional features defined the specific landscape of the 2009 version of the Interactive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Test Collection</head><p>For the 2009 Interactive task, a significant departure from the previous year's exercise was the use of a new document collection. Whereas, in 2008, we had used the IIT Complex Document Information Processing Test Collection, version 1.0, a collection which is based on documents released under the tobacco "Master Settlement Agreement" (see the overview of the TREC-2006 Legal Track for additional information on the IIT CDIP 1.0 collection <ref type="bibr" coords="4,178.47,273.32,10.30,8.74" target="#b8">[9]</ref>), we turned, in 2009, to a collection of emails that had been produced by Enron in response to requests from the Federal Energy Regulatory Commission (FERC). We turned to the new collection on the grounds (i) that, in its subject matter, it would support a wide range of new topics and (ii) that, as a collection of emails with attachments, the collection more closely approximated the collections that are the typical domains for real-world discovery searches. In the following, we elaborate further on the 2009 test collection, focusing specifically on the provenance and processing of the collection, the state of the final collection, and plans for future use of Enron emails. Provenance and processing. Although others have defined, in various ways, research collections from the Enron emails made available by FERC, we decided that it was in the best interest of the Legal Track to build a new Enron collection from scratch. We did so for two reasons.</p><p>First, the collection used by most researchers (assembled by MIT, SRI and CMU soon after the emails became available from the Federal Energy Regulatory Commission (FERC)) has a fundamental drawback, for the purposes of the Legal Track, in that it lacks email attachments. Discussions at TREC 2008 confirmed that track participants believed that attachments were an essential element of a test collection modeling current e-discovery practice, and so, if we were to use Enron email for the 2009 exercise, we would have to find a collection that included attachments.</p><p>Second, over the years, FERC has withdrawn some emails from the collection, for various reasons, and has also released additional emails when the basis for withholding those emails expired. The net effect has been a possible increase in the number of available emails over the past several years, emails that were not available to the researchers who built the earlier collections. For both of these reasons (attachments, additional data) we elected to build our new test collection using the set currently being distributed by Aspen Systems on behalf of FERC.</p><p>The steps we took to process the collection are as follows. The collection was obtained from Aspen Systems on March 21, 2005 by Clearwell. The original emails were from the mailboxes of about 150 employees of Enron Corporation. At the time of collection, these mailboxes contained messages created between 1998 and 2002. These emails were originally in Lotus Notes format. They were first converted into Microsoft personal folder (.pst) format by Clearwell. Clearwell first extracted user names from the .pst collection. They then cleaned up these names, where possible, using various heuristics to discern SMTP, X.400 or other address formats. After some data integrity checks, recognition of domain names, department names, group aliases, and other situations (e.g., "on behalf of" entries) was performed.</p><p>The .pst files were then loaded into a test exchange server and processed there to generate an Electronic Discovery Reference Model (EDRM) XML Document Element for each message. Each document element contains the original message and attachments in native form (as a .msg file) and text extracted from the message and from any attachments (as .txt files). The .msg format is a binary encoding of the message and its attachments, as specified by Microsoft's Messaging Application Programming Interface (MAPI). All significant MAPI properties present in the original .pst were maintained in the .msg file. Text extraction was performed using Oracle Outside-In text extraction software. Message-to-attachment parent-child relationships are indicated using a document numbering convention to facilitate construction of review units for responsive review, and Parent-Child relationship entries were additionally encoded in the XML.</p><p>We performed deduplication in two stages. First, exact duplicates were identified and automatically removed by Clearwell. Duplication detected at this stage resulted in retention of a single master copy, with only the XML metadata being retained for the removed duplicates. A second deduplication pass was performed at the University of Maryland using lexical similarity measures (i.e., recognizing messages with identical senders and recipients and nearly identical subject lines and body text). This pass, performed too late to permit regeneration of the EDRM XML, identified a substantial number of messages in which "load two" had been appended to the subject line by an earlier stage of automated processing that were otherwise duplicates. We therefore designated a single message from each set as being the message to be returned, and provided a list of detected duplicates to track participants. Near the submission deadline we discovered that we had inadvertently chosen the version with "load two" in the subject line, but at that point it was too late to change the designation of messages to be returned. We also learned of some cases in which apparently identical messages actually had different attachments (which had not been checked in our second-pass duplicate detection); this resulted in inadvertent removal from the submitted runs of some potentially unique messages.</p><p>The final test set. The resulting collection contained 569,034 unique messages. Together, the 569,034 unique messages have 278,757 attachments, bringing the test collection to a total of 847,791 documents (when parent emails and attachments are counted separately). This was the collection used for the 2009 Interactive task.</p><p>Looking forward. The version of the collection used in the 2009 Interactive task presented a number of issues; chief among these were (i) the appending of the string "load two" in the subject line, (ii) the failure, in some cases, to take into account attachments in identifying duplicate sets of messages, (iii) the absence, in the release of the collection, of the .pst files that were created from the Lotus Notes sources (some participants expressed interest in working directly from the .pst format), and (iv) the lack of a mapping between this collection and other Enron email collections.</p><p>We recognize that it is important to address these issues going forward. Our plan, for 2010, is to develop, in collaboration with the EDRM Data Set Project [1] and ZL Technologies (also a participant in the 2009 Interactive task) a new version of the Enron collection that will be free of the issues noted above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Topics &amp; Topic Authorities</head><p>The 2009 Interactive task featured an entirely new mock complaint (modeling a securities fraud class action). The complaint provided the foundation for seven document requests, each of which served as a separate topic for the purpose of the exercise. To each topic, a single Topic Authority was assigned, who, as noted above, served as the source for the authoritative conception of what was and was not relevant to the topic.</p><p>The topics and their Topic Authorities were as follows.</p><p>• Topic 201. All documents or communications that describe, discuss, refer to, report on, or relate to the Company's engagement in structured commodity transactions known as "prepay transactions."</p><p>-Topic Authority: Howard J. C. Nicols (Squire, Sanders, &amp; Dempsey).</p><p>• Topic 202. All documents or communications that describe, discuss, refer to, report on, or relate to the Company's engagement in transactions that the Company characterized as compliant with FAS 140 (or its predecessor FAS 125).</p><p>-Topic Authority: Michael Roman Geske (Aphelion Legal Solutions).</p><p>• Topic 203. All documents or communications that describe, discuss, refer to, report on, or relate to whether the Company had met, or could, would, or might meet its financial forecasts, models, projections, or plans at any time after January 1, 1999.</p><p>-Topic Authority: David Stanton (Pillsbury Winthrop Shaw Pittman).</p><p>• Topic 204. All documents or communications that describe, discuss, refer to, report on, or relate to any intentions, plans, efforts, or activities involving the alteration, destruction, retention, lack of retention, deletion, or shredding of documents or other evidence, whether in hard-copy or electronic form.</p><p>-Topic Authority: Maura Grossman (Wachtell, Lipton, Rosen &amp; Katz).</p><p>• Topic 205. All documents or communications that describe, discuss, refer to, report on, or relate to energy schedules and bids, including but not limited to, estimates, forecasts, descriptions, characterizations, analyses, evaluations, projections, plans, and reports on the volume(s) or geographic location(s) of energy loads.</p><p>-Topic Authority: Art Bieser (Hunton &amp; Williams).</p><p>• Topic 206. All documents or communications that describe, discuss, refer to, report on, or relate to any discussion(s), communication(s), or contact(s) with financial analyst(s), or with the firm(s) that employ them, regarding (i) the Company's financial condition, (ii) analysts' coverage of the Company and/or its financial condition, (iii) analysts' rating of the Company's stock, or (iv) the impact of an analyst's coverage of the Company on the business relationship between the Company and the firm that employs the analyst.</p><p>-Topic Authority: Christopher Boehning (Paul, Weiss, Rifkind, Wharton &amp; Garrison).</p><p>• Topic 207. All documents or communications that describe, discuss, refer to, report on, or relate to fantasy football, gambling on football, and related activities, including but not limited to, football teams, football players, football games, football statistics, and football performance.</p><p>-Topic Authority: K. Krasnow Waterman (LawTechIntersect).</p><p>Teams were allowed to participate in 1-4 topics, subject to the following constraints. First, in order to balance the load among Topic Authorities, we sometimes had to ask a team to take a topic other than its first choice. Second, given the subject matter of Topic 207, with which we assumed most participants would be already familiar, we required that any team that participated in that topic also participate in at least one of the other topics (201-206).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Participating Teams</head><p>The 2009 Interactive task saw participation from eleven teams (eight commercial and three academic), who, collectively, submitted a total of 24 single-topic runs. The eleven teams that submitted results for evaluation are as follows (full name followed by two-letter team ID):</p><p>• Applied Discovery (AD); It should be noted, with regard to the H5 team, that, as was also the case in the 2008 exercise, Bruce Hedin, a track coordinator and an employee of H5, did not take part in any way in H5's efforts; indeed, throughout the exercise, the H5 team observed a policy of having no communications with Bruce Hedin, on TREC-related matters, outside of the channels available to other task participants.</p><p>Teams were invited to ask to participate in as many, or as few, topics as they chose. Given constraints on the number of teams for which a Topic Authority could take responsibility (typically, a maximum of four teams, but, in some cases, fewer), we indicated that we might not be able to give all teams all of their choices and asked teams to rank their topic selections in order of preference. Topics were assigned largely on a first-come-first-serve basis; we also endeavored to give each team its top choice, utilizing the lowerranked selections for balancing the load across topics. Nine of the eleven teams received their first choice of topics; the two that did not (because the topic had already been fully subscribed) received their immediately next-highest selection.</p><p>Table <ref type="table" coords="7,115.00,218.62,4.98,8.74">1</ref> shows the number of runs submitted by each team for each topic; in the table, an empty cell represents no submissions for the given team-topic combination. </p><formula xml:id="formula_0" coords="7,183.86,309.32,245.68,125.82">CB 1 1 3 1 6 CS 1 1 1 3 EQ 1 1 2 H5 1 1 IN 1 1 LO 1 1 2 UB 1 1 UP 1 1 UW 1 1 1 1 4 ZL 2<label>2</label></formula><p>Total Runs 4 2 4 3 3 4 4 24</p><p>Table <ref type="table" coords="7,246.22,466.08,3.87,8.74">1</ref>: Runs submitted for each topic.</p><p>As can be seen from the table, in most cases, each team submitted, in accordance with the task guidelines, just one run for each topic it chose to be evaluated on. In two cases, however, teams asked for, and were given, permission to submit multiple runs for a single topic.</p><p>In the first case, the Cleary-Backstop team (CB) wished, for Topic 206, to have three submissions evaluated, with each of the three submissions representing a different level of effort (low, medium, high) in preparing the submission. Henceforth we designate these runs as "CB-Low," "CB-Mid," and "CB-High."</p><p>In the second case, the team from ZL Technologies wished, for Topic 203, to have two submissions evaluated. One submission represented the results obtained after pre-culling the collection by custodian; the other submission represented the results obtained when utilizing no pre-culling of the collection (i.e., accessing the full collection). As custodian-based culling of collections is not an uncommon tactic for reducing the volume of documents subject to review, the comparison of results obtained when using custodian-based culling to those obtained when not doing so could provide valuable information on the risks and rewards of the tactic. For further specifics on the approaches taken in preparing each submission, see ZL's contribution to the TREC-2009 proceedings <ref type="bibr" coords="7,210.45,655.37,14.61,8.74" target="#b15">[16]</ref>. Henceforth we designate these runs as "ZL-Cull" and "ZL-NoCull."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Assessors</head><p>As noted above, once the collection has been stratified for each topic and evaluation samples drawn, the contents of each sample is randomly divided into bins of approximately 500 documents. Each of these bins is then assigned to a first-pass assessor, who reviews the documents in the bin for relevance to his or her assigned topic.</p><p>For the 2009 Interactive task, assessors were drawn from two sources: (i) firms that provide professional review services and (ii) individual volunteers. The review of the samples for three of the seven Interactive topics (203, 204, and 207) was carried out by two firms that include professional document-review services among their offerings. The review of the samples for the remaining four topics (201, 202, 205, and 206) was carried out by individual volunteers (primarily law students, but also practicing attorneys and other legal professionals) each of whom reviewed one or, occasionally, two bins; in all, 48 individual volunteers participated in the first-pass review of these four topics.<ref type="foot" coords="8,316.20,211.52,3.97,6.12" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Unit of Assessment</head><p>As noted above, the Enron collection is a collection of emails. In evaluating the effectiveness of approaches to assessing the relevance of email messages, one must decide whether one wants to assess effectiveness at the message level (i.e., treat the parent email together with all of its attachments as the unit of assessment) or to assess effectiveness at the document level (i.e., treat each of the components of an email message (the parent email and each child attachment) as a distinct unit of assessment. (Sometimes, in past discussions, the term record has been used as a synonym for message).</p><p>For the 2009 Interactive task, after much discussion (in which some participants argued in favor of message-level assessment and others in favor of document-level assessment), we opted for an all-of-the-above approach that asked participants to submit their results at the document level (in order to enable documentlevel analysis) from which we would, by rule, derive message-level values (which would serve as the primary basis for evaluation).</p><p>In terms of submitting document-level assessments, participants were asked to make a separate assessment of the relevance of the content of each component (the parent email and each attachment) of an email message, while taking into account all components of the message in resolving instances of ambiguous cross-reference.</p><p>In terms of deriving message-level values, the rule applied was reasonably straightforward: a message counted as having been assessed as relevant if any of its components (parent email or attachment) had been assessed as relevant.</p><p>In measuring the effectiveness of the various approaches, our primary focus has been on effectiveness as measured at the message level; we do, however, supplement this message-level view with document-level analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Task Results</head><p>The 2009 Interactive task got under way, with the release of the final task guidelines <ref type="bibr" coords="8,453.07,530.81,10.52,8.74" target="#b7">[8]</ref> and of the mock complaint and associated topics <ref type="bibr" coords="8,218.33,542.77,9.96,8.74" target="#b2">[3]</ref>, on June 19, 2009. In this section, we summarize the results of the exercise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Team-TA Interaction</head><p>As noted above, the Interactive task permits teams to call on up to 10 hours (600 minutes) of a Topic Authority's time for purposes of clarifying the scope and intent of a topic. For the 2009 Interactive task, while the 10-hour limit was maintained for six of the seven topics, an exception to the rule was made in the case of one topic, Topic 205. For this topic, participants (who had experienced some difficulties in gathering the information they believed they needed to define the topic) requested, and were given, an additional four hours of time to consult with the Topic Authority, bringing the total time allowed for Team-TA interaction for this topic to 14 hours (840 minutes).</p><p>Figure <ref type="figure" coords="9,118.33,111.02,4.98,8.74" target="#fig_1">1</ref> summarizes the participants' use of the Topic Authorities' time for each topic. In the diagram, each bar represents the total time allowed for team-TA interaction (600 minutes for most topics, 840 minutes for Topic 205); the gray portion of the bar represents the amount of the permitted time that was actually used by a team (with the number of minutes used indicated just above the gray portion). As can be seen from the diagram, there is considerable variation in the extent to which teams utilized their allotted time for interacting with the Topic Authority: some teams used less than an hour of their available time, while others used seven hours or more. On the whole, however, teams tended to use considerably less than the maximum amount of time that they were allowed. Of the 24 runs submitted, 20 were prepared utilizing less than 50% of the time permitted for interacting with the Topic Authority; only 4 of the runs (202-CS; 204-H5; 205-CS; 205-EQ) were the result of utilizing more than 50% of the time allowed for Team-TA interaction. We consider below (Section 2.4.1) whether there is any correlation between the amount of time spent interacting with the Topic Authority in the preparation of a run and the effectiveness of the run that results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Submissions</head><p>Participants submitted their results on or before September 16, 2009 (or, in the case of Topic 205, for the reasons noted above, on or before September 30, 2009). Table <ref type="table" coords="9,360.37,557.45,4.98,8.74" target="#tab_1">2</ref> summarizes, at the message level, the submissions received for each topic. The table shows (for the complement of the union of all submissions; for the union of all submissions; for each submission; and for the intersection of all submissions): (i) the number of messages that belong to each designated subset, (ii) the proportion, out of all messages in the full collection, that each subset represents, and (iii) the proportion, out of the union of all submissions, that each subset represents. (Recall that the full collection consists of 569,034 messages.)</p><p>As can be seen from the table, there is considerable variation among the runs submitted for each topic; and finding which of the runs are most accurate will require examination of the results of our sampling and assessment protocol. Even prior to examining those results, however, we can make two observations simply on the basis of the participant submissions.</p><p>First, we note that the subset formed from the union of all submissions (i.e., the subset of messages found relevant by one or more teams) is, for most topics, a relatively small proportion of the collection. For Topics 201-204, that subset represents no more than 3% of the collection, meaning 97%, or more, of the collection was not found relevant by any participant; For only two topics (205 and 206) does the subset formed from the union of all submissions represent 10% or more of the collection. While we cannot estimate the true yield until we look at the results of sampling and assessment, the data from submissions alone suggest that the majority of our topics will be relatively low-yielding. Second, we note that the subset formed from the intersection of all submissions (i.e., the subset of messages found relevant by all teams), which we might call the "Consensus-R" subset, generally represents a very small proportion of the subset formed from the union of all submissions. For Topic 201, for example, the Consensus-R subset represents less than 2% of all messages submitted as R by at least one team. For five of the seven topics <ref type="bibr" coords="11,154.83,194.71,21.59,8.74">(201,</ref><ref type="bibr" coords="11,179.52,194.71,17.71,8.74">203,</ref><ref type="bibr" coords="11,200.32,194.71,17.71,8.74">204,</ref><ref type="bibr" coords="11,221.13,194.71,17.71,8.74">205,</ref><ref type="bibr" coords="11,241.93,194.71,17.27,8.74">206)</ref>, the Consensus-R subset represents less than 5% of all messages submitted as R by at least one team. For only one topic (202) does the Consensus-R subset represent more than 30% of all messages submitted as R by at least one team, and that is a topic with only two participants. For most topics, then, there is not a large "core" subset of relevant documents that all participants found relevant.</p><p>Of course, what ultimately matters is how closely each of the various submissions overlaps with the subset of messages that actually meet the Topic Authority's definition of relevance. In order to gauge that, we need to turn to sampling and assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Stratification &amp; Sampling</head><p>Once the submissions were received, the collection was stratified for each topic and evaluation samples were drawn. Stratification followed the submission-based design noted above (Section 2.1), whereby one stratum was defined for messages all participants found relevant (the "All-R" stratum), another for messages no participant found relevant (the "All-N" stratum), and others for the various possible cases of conflicting assessment among participants. The operative unit for stratification was the message, and messages were assigned intact (parent email together with all attachments) to strata.</p><p>Samples were composed following the allocation plan sketched above (Section 2.1), whereby strata are represented in the sample largely in accordance with their full-collection proportions. An exception to proportionate representation is made in the case of the very large All-N stratum, which is under-represented in the sample relative to its full-collection proportions, thereby allowing each of the R strata to be somewhat over-represented relative to their full-collection sizes. Selection within a stratum was made using simple random selection without replacement. The operative unit for selection into a sample was the message, and any message selected was included intact (parent email together with all attachments) in the sample.</p><p>Tables showing, for each topic, the stratum-by-stratum partitioning of the collection, the samples drawn from each stratum, and the pre-and post-adjudication assessments attached to those samples are provided in an appendix to this document (Appendix A). For purposes of this section, we present, in Table <ref type="table" coords="11,522.51,502.01,3.87,8.74" target="#tab_3">3</ref>, a high-level view of the outcome of the stratification and sample selection process. In the table, we aggregate, for each topic, the totals for each of the individual R strata into a single row (labeled "R Strata," with the number of individual strata so aggregated noted in parentheses) and present the view of collection and sample composition that results.</p><p>The table enables us to make a few observations. First, with regard to the size of samples, we see that the samples ranged from 2,729 messages (for Topic 201) to 3,975 messages (for Topic 204), with the average size of a sample being 3,458 messages. Counting by document (i.e., counting each parent email and each attachment separately), we see that the samples ranged in size from 5,710 documents (Topic 203), to 8,658 documents (Topic 207), with the average size of a sample coming to 7,041 documents. Differences in the sizes of the samples for each topic were largely a function of the availability of resources; the assessment of the sample for Topic 207, for example, was carried out by a firm that offers professional review services and that had the resources to review a larger sample than was reviewed for most topics.</p><p>Second, comparing the size of the set formed by aggregating the R strata to the size of the All-N stratum, we see, as we saw in the previous section (2.3.2), that, in the full collection, the R strata, collectively, represent a small proportion of the population, representing, for six of the seven topics, 10% or less of the messages  in the collection, and, for four of the seven topics, 3% or less of the messages. Looking at representation in the sample, we see that, in accordance with our sampling design, the R strata are represented in higher proportions, and the All-N stratum in lower proportions, than their full-collection proportions would dictate: for most samples, roughly one third of the sample is allocated to the R strata and two thirds to the All-N stratum. Topic 205, for which the R strata comprise about 14% of the full collection, is an exception to this general rule: for this topic, roughly two thirds of the sample is allocated to the R strata and one third to the All-N stratum. Third, comparing the representation of strata when we count by message to their representation when we count by document, we see that, for all topics, the R strata represent a higher proportion, of both the collection and the sample, when we count by document than they do when we count by message. Perhaps for reasons having to do with the nature of the messages that are relevant to the target topics, or perhaps for reasons having to do with the nature of the retrieval processes used in the evaluation, or perhaps for reasons having to do with both, messages submitted as relevant by at least one participant have, on average, a higher document-to-message ratio (i.e., have a greater number of attachments) than do messages not submitted as relevant by any participant. The doc-to-msg ratio for the full collection is 1.5; the doc-to-msg ratio for the set formed by aggregating the R strata ranges from 2.2 (Topic 205) to 4.8 (Topic 201), with an average across the seven topics of 3.3. Of course, given that the operative unit for stratification and sampling was the message, the inclusion of a document in an R stratum does not mean that the document itself was assessed as relevant by any participant; it just means that some component of the message to which the document belongs was submitted as relevant by at least one participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Assessment &amp; Adjudication</head><p>First-pass assessment. Once samples were drawn, the messages in each sample were randomly divided into "bins" of approximately 500 documents each. (Some bins had more, some less, than 500 documents, due to the fact that messages were assigned to bins intact (parent email together with all attachments), making it impossible to see that every bin had exactly 500 documents.) The bins were then distributed to first-pass assessors who, equipped with detailed assessment guidelines (largely compiled from the relevance guidance that the Topic Authority had provided the teams in the course of the exercise), assessed the documents in their bins for relevance to their assigned topics. As noted above (Section 2.2.4), the first-pass assessors included both professional document reviewers and individual volunteers. In all (aggregating the samples for the seven topics), a total of 100 bins were reviewed; these bins, collectively, contained 49,285 documents, representing a total of 24,206 messages.</p><p>In reviewing their bins, assessors were instructed to make a relevance judgment (relevant (R) or not relevant (N)) for every document in their bins. A small number of documents in each bin were such as not to permit a relevance judgment by the assessor (when, for example, the document image was missing or illegible; when the document was in a language other than English; or when the document exceeded 300 pages in length); in these cases, the assessor was instructed to leave the document "unjudged." (Sometimes these unjudged documents are referred to as "gray" documents.) Out of the 49,285 documents reviewed, 1,980 (about 4% of the total) were found to be not assessable for one of these reasons and so were left unjudged.</p><p>Appeal and adjudication. Once the first-pass assessors had completed their work, we provided each team with the full set of first-pass assessments for each topic in which they had participated, and invited them to appeal to the Topic Authority any assessments they believed had been made in error (i.e., out of keeping with the Topic Authority's conception of relevance). In order to assist teams in preparing their appeals, we also provided teams with the (message-level) probability of selection associated with each document in the sample and with preliminary (i.e., pre-adjudication) estimates of the recall, precision, and F 1 scores achieved in their submitted runs. In order to assist the Topic Authority in adjudicating appeals, teams were asked to prepare documents detailing the grounds for each appeal they were submitting. The Topic Authority then rendered a final relevance judgment on all appealed documents; there was no second round of appeal.</p><p>Table <ref type="table" coords="13,113.84,454.19,4.98,8.74" target="#tab_4">4</ref> summarizes, for each of the runs received in the 2009 Interactive task, the rates of agreement and disagreement with the first-pass assessments, the rate of appeal, and the rate of success of appeals. All data in the table are document-level data, as that is the level at which appeals were submitted and adjudicated, and so it is the level that is most informative when looking at the rates in which we are interested. The table contains, more specifically, the following data for each run.</p><p>1. Sample size. The number of documents in the evaluation sample for a given run.</p><p>2. Union of R assessments. The number of documents in the sample that were either assessed as R in the given run or assessed as R by the first-pass assessor.</p><p>3. Intersection of R assessments. The number of documents in the sample that were both assessed as R in the given run and assessed as R by the first-pass assessor; also expressed as a proportion of (2).</p><p>4. Conflicting assessments. The number of conflicting assessments in the sample (i.e., documents that were assessed as R in the run but as N by the assessor or were assessed as N in the run but as R by the assessor); also expressed as a proportion of (2).</p><p>5. Appealed assessments. The number of conflicting assessments that were appealed; also expressed as a proportion of (4).</p><p>6. Successful appeals. The number of appeals that were successful (i.e., that resulted in a first-pass assessment's being overturned); also expressed as a proportion of (5). The data in Table <ref type="table" coords="14,174.99,536.82,4.98,8.74" target="#tab_4">4</ref> provide an interesting view into how the various participants made use of the appeal/adjudication mechanism. For the moment, we make the following observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-Asr</head><p>With regard to the intersection of R assessments (cases in which a document was both submitted as R in a given run and judged as R by the first-pass assessor), we see that the rates of overlap are generally low. For five of the seven topics, we see no cases of run-assessor positive overlap (the intersection of R assessments out of the union of R assessments) that exceed 0.5; for Topic 202, we do find that, for both runs, overlap values are greater than 0.5 but still do not exceed 0.6; it is only with Topic 207 (on football-related gambling) that we see overlap rates greater than 0.7. For Topic 207 and, to a lesser extent, for Topic 202, participants and assessors seem to have had a good amount of common ground in judging relevance; for the other topics, however, there is a substantial amount of run-assessor disagreement, disagreement that would have to be resolved via the appeals process.</p><p>With regard to the number of conflicting assessments, we do see that, although generally a relatively small proportion of the sample as a whole, they typically represent a high proportion of the set of documents assessed as R either by the participant or by the assessor. (Note that the conflicting assessments and the intersection of R assessments do not necessarily sum to the union of R assessments; this is due to the fact that some documents in the union may have been left unjudged by the assessor (and so are not counted as a conflicting assessment).)</p><p>With regard to the rates of appeal, we see that there is considerable variation among participants, with some participants electing not to appeal any of their conflicts with the first-pass assessor and with others appealing over 90% of such conflicts. Most participants did, however, choose to submit at least some appeals; for only 5 of the 24 runs were no conflicts appealed. Most topics, moreover, had one or more participants who made at least moderate use of the appeals mechanism; Topic 206 is the only topic for which there was no run that had an appeals rate of at least 0.5 (see below (Section 2.3.6) for more discussion of Topic 206).</p><p>With regard to the success rates for appeals, we see that these rates are, across the board, high. No set of appeals had a success rate lower than 0.7 and ten had a success rate greater than 0.9. It is evident that, in many cases, the participants, through their prior interaction with the Topic Authority, had a good sense of the operative conception of relevance, and so were effective at identifying, and having corrected, assessor errors.</p><p>We return to these data below (Section 2.4.2), where we consider the effects of appeals on participant scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Final Results</head><p>With all appeals adjudicated and sample assessments finalized, we were in a position to calculate final estimates of the overall yield for each topic and of the recall, precision, and F 1 achieved in each run submitted by participants. Before turning to those estimates, we add two further notes by way of background to our calculations.</p><p>The first note concerns the derivation of message-level relevance values. In the discussion that follows, our primary focus is on message-level yields and on message-level scores. We noted above (Section 2.2.5) that derivation of message-level relevance values from document-level values was a fairly straightforward matter, with a message counting as relevant if any one of its components (parent email or attachment) was found relevant. While the rule so stated suffices for the vast majority of cases, allowance for the possibility that a component of a message might be left "unjudged" requires a slightly more elaborate formulation of the derivation rule. The full rule is as follows.</p><p>• For fully-judged messages (all components judged): R trumps N (i.e., a message is R if any one of its components is R). • For fully-unjudged messages (no components judged): the message is U (i.e., a message is U (unjudged) if all of its components are U). • For partially-judged messages (some components judged, some not), the following rules apply.</p><p>-In most cases, R trumps N, N trumps U.</p><p>-The one exception, which occurs in a very small number of cases, is when none of the judged components have been judged R, but at least one of the unjudged components has been submitted by a participant as R; in this case, the message is U (because the participant's R was not assessed).</p><p>The rule was so formulated in order to make maximum use of the assessments we had obtained, while still being fair to participants whose submitted R's were left unjudged.</p><p>The second note concerns the calculation of confidence intervals. For this report, we calculated confidence intervals utilizing the formulae detailed in the appendix to the 2008 Overview <ref type="bibr" coords="15,412.82,614.59,14.61,8.74" target="#b12">[13]</ref>. We are aware that some adjustments to these formulae are called for. More specifically, we note (i) that reflecting correlation in the computation of confidence intervals for ratios such as precision and recall would reduce our computed confidence intervals somewhat 2 and (ii) that, if our implicit assumption that every first-pass assessment of an unappealed document is correct turns out not to be true, that could in some cases place the actual values outside our computed confidence intervals (which reflect sampling error but not assessment error) <ref type="bibr" coords="16,521.73,75.16,14.61,8.74" target="#b16">[17]</ref>. For now, however, we believe that the reported confidence intervals are serviceable as rough gauges of the sampling error associated with the estimates.</p><p>We can now turn to the estimates themselves. Table <ref type="table" coords="16,333.55,111.02,4.98,8.74" target="#tab_6">5</ref> reports the estimated full-collection yield of relevant messages for each of the seven Interactive topics; yield is reported both as an absolute total and as a proportion of the full collection.  As can be seen from the table, our hypothesis, formed on the basis of the submission data alone (see Section 2.3.2), that the topics were generally low-yielding has been borne out by the sample data. No topic represents more than 5% of the collection (the closest being Topic 205, which represents 4.7% of the collection), and four of the seven topics (201-204) represent, individually, less than 1% of the collection.</p><p>Table <ref type="table" coords="16,115.19,400.12,4.98,8.74" target="#tab_8">6</ref> reports measures of how effective the participants were at retrieving those relevant messages. More specifically, the table reports, for each run submitted, estimates of the message-level recall, precision, and F 1 achieved in the run.</p><p>As can be seen from the table, with regard to the effectiveness of the approaches evaluated in this year's exercise, the post-adjudication scores show some encouraging signs. Of the 24 submitted runs, 6 (distributed across 5 topics: 201-UW, 202-UW, 203-UW, 204-H5, 207-UW, 207-CB) attained an F 1 score (point estimate) of 0.7 or greater. In terms of recall, of the 24 submitted runs, 5 (distributed across 4 topics: 201-UW, 203-UW, 204-H5, 207-UW, 207-CB) attained a recall score of 0.7 or greater; of these 5 runs, 4 (distributed across 3 topics: 201-UW, 204-H5, 207-UW, 207-CB) simultaneously attained a precision score of 0.7 or greater. Before, however, drawing any further conclusions from the data in Table <ref type="table" coords="16,400.42,507.72,3.87,8.74" target="#tab_8">6</ref>, it is useful to add additional topic-specific context to these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Topic-Specific Notes</head><p>Each topic has a story of its own, and the circumstances specific to each topic must always be kept in mind in evaluating results. In this section, we note some of the topic-specific circumstances that are salient to the 2009 results.</p><p>Topic 201. Topic 201 presented us with the interesting case of a "rogue" bin. The appeals for Topic 201 brought to light the fact that one of our first-pass assessors (who had reviewed a bin of 501 documents, representing 171 messages) must have fundamentally misunderstood his or her assignment. Although this assessor's bin would have been more or less of the same composition as the other bins (since messages are assigned to bins randomly once the full evaluation sample has been selected), the results of the first-pass assessment of the bin were significantly out of line with results for the other bins, both in terms of the proportion of documents assessed as R (64% vs. an average of 9% for the other bins) and in terms of the proportion of documents appealed (61% vs. an average of 8% for the other bins). The nature of this assessor's  R assessments was also curious: the assessor counted as R many documents that were not remotely relevant to 201 but may have been pertinent to one of the other topics (e.g., football). One hypothesis for this result is that the assessor, although given instructions and guidelines specific to 201, believed that he or she was supposed to review documents for relevance to any of the seven topics listed in the mock complaint.</p><p>Given the odd character of the assessments in the bin, we chose, rather than to burden the Topic Authority with a large number of additional appeals, simply to ignore the bin (both appealed and nonappealed documents) entirely. Since message-to-bin-assignment is random, this amounts to assuming that content of the bin was never selected into the sample. The remainder of the sample (a total of 13 bins) still represents an unbiased sample of the collection and the estimates that are derived from the sample still valid.</p><p>Topic 205. Topic 205, as noted above, represented a case in which participants experienced some difficulties in gathering the information they believed they needed to define the topic, and so requested, and were given, an additional four hours of time to consult with the Topic Authority as well as an additional two weeks, beyond the submission deadline set for the other topics, to submit their results. We believe that the allowance of additional time sufficed to address the issue that the participants had raised and that the results reported for this topic are as reliable as those reported for other topics.</p><p>Topic 206. Topic 206 represents the one topic, out of the seven featured in the 2009 exercise, for which we believe the post-adjudication results are not reliable. Recall that the Interactive task relies heavily on the appeals mechanism as a corrective on errors made in the first-pass assessment process. For Topic 206, this mechanism was used only very lightly: one of the two participants in the topic submitted no appeals and the other submitted only a small subset of their disagreements with the first-pass assessors. While such appeals as were made were indeed successful at identifying and correcting errors, it is very likely that, due to the light use of the appeals mechanism, many more errors remain uncorrected. We do not believe, therefore, that any valid conclusions can be drawn from the scores recorded for this topic; those scores are, in essence, still "pre-adjudication" scores and, as such, could vary vary substantially from the scores that would result if a more thorough vetting of the first-pass assessments was carried out.</p><p>The experience of this topic will serve as valuable input to potential modifications to the appeal and adjudication process we are considering for the 2010 exercise.</p><p>Topic 207. Topic 207 (on football-related gambling) is the topic for which, given the subject matter of the topic, we added the requirement that any team that participated in this topic also participate in at least one of the other topics (Section 2.2.2).</p><p>Having noted these considerations, we summarize the results in the form of a diagram. Figure <ref type="figure" coords="18,510.10,252.49,4.98,8.74" target="#fig_2">2</ref> plots each of the post-adjudication results for each of the 24 submitted runs on a precision-recall diagram. In the figure, topics are distinguished by shape as per the legend. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Further Analysis</head><p>The 2009 Interactive task produced a set of data that we believe will be a rich domain for further study and analysis. For purposes of this report, we confine our further analysis to brief observations on a few aspects of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Team-TA Interaction</head><p>Earlier (Section 2.3.1), we saw that there was considerable variation in the amount of time teams chose to spend with the Topic Authorities for the purpose of clarifying the intent and scope of the target topics; times ranged from zero minutes in two instances to 735 minutes in another instance. Such variation prompts the question of whether there is a correlation between the amount of time spent with a Topic Authority and retrieval effectiveness.</p><p>When we took up this question in the 2008 track overview <ref type="bibr" coords="19,337.60,153.32,14.61,8.74" target="#b12">[13]</ref>, we found that the 2008 Interactive results, though suggestive of a possible correlation between effectiveness and time spent with the TA (insofar as the most effective run was the one that resulted from the greatest usage of TA time), provided too few data points to serve as the basis for any firm conclusions.</p><p>Figure <ref type="figure" coords="19,118.26,201.14,4.98,8.74" target="#fig_3">3</ref> plots, for the 2009 exercise, retrieval effectiveness (as measured by post-adjudication F 1 scores) against time spent with the Topic Authority on the topic-clarification portion of the task; for purposes of this analysis, we have excluded the results for Topic 206. Looking at the chart, we see that there does appear to be a correlation between effectiveness and time spent with the Topic Authority. Submissions that resulted in low F 1 scores tend to have come from approaches that made little use of the Topic Authority's time; submissions that achieved high F 1 scores all made use of at least some of their available time with the Topic Authority. When we test this impression by calculating the Pearson product-moment correlation coefficient, however, we obtain a positive point estimate, but a very wide 95% confidence interval, one that in fact overlaps with zero: r = 0.424 (-0.022, 0.730). The data are suggestive, then, that one component of an effective retrieval approach is an effective method of interacting with the Topic Authority, but, with the data points we have, we cannot establish the significance of the effect. The 2009 Topic Authorities themselves have suggested that "the quality of the team/TA interaction was more important than the quantity." <ref type="bibr" coords="19,249.76,575.14,15.50,8.74" target="#b10">[11]</ref> We see, then, that the results from the 2009 exercise raise some interesting additional questions on the topic of the effects of interaction with the Topic Authority. In particular, which approaches to gathering information from the Topic Authority (telephone interviews, email questions, exemplar documents, etc.) are most effective and efficient? We look forward to examining these questions further in the 2010 exercise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Utilization of the Appeals Mechanism</head><p>Earlier we saw (Table <ref type="table" coords="19,171.48,667.24,4.43,8.74" target="#tab_4">4</ref>) that there was considerable variation in the extent to which participants utilized the appeals mechanism, with some participants appealing none of their conflicts with the first-pass assessors and other participants appealing over 90% of their conflicts. Such variation prompts the question of whether there is a correlation between the extent to which the appeals mechanism is utilized and the amount of improvement realized from pre-adjudication to post-adjudication results. Figure <ref type="figure" coords="20,117.73,357.75,4.98,8.74" target="#fig_4">4</ref> plots, for each run, the rate of appeal against the improvement realized. "Improvement realized" is quantified by calculation of the actual gain in F 1 for a run out of the potential gain in F 1 for that run. For example, a run that had a pre-adjudication F 1 of 0.2 and a post-adjudication F 1 of 0.6 would have an improvement realized measure of 0.4/0.8 = 0.5.</p><p>We again have a relatively small number of data points to work with, but, in Figure <ref type="figure" coords="20,453.50,405.57,3.87,8.74" target="#fig_4">4</ref>, we do see at least the suggestion of a correlation: teams that appeal a higher proportion of the disagreements between their submitted assessments and those of the first-pass assessor tend to realize a greater improvement in scores (from pre-to post-adjudication). When we test this impression by calculating the Pearson product-moment correlation coefficient, we find evidence of a positive correlation: r = 0.862 (0.703, 0.939).</p><p>Looking more closely at the appeal and adjudication data reported above (Table <ref type="table" coords="20,436.70,465.35,3.87,8.74" target="#tab_4">4</ref>), however, we see that the story may not be as simple as it appears at first glance. It is also the case that, for five of the seven topics, the run that had the highest appeals rate also had the highest positive overlap with the first-pass assessments; and that, for four of the seven topics, the run that had the highest appeals rate also had the highest success rate in having first-pass assessments overturned. In light of these data, we have to consider the possibility that a team that, through their interaction with the Topic Authority, gains a good sense of the operative conception of relevance, will be in a better position both to do well initially and to make effective use of the appeals mechanism.</p><p>We look forward to exploring this question further in the 2010 Legal Track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Document-Level and Post-Hoc Scoring</head><p>Document-level scores for each of the 2009 Interactive runs have been calculated and are reported in an appendix to the TREC-2009 proceedings <ref type="bibr" coords="20,247.06,617.23,12.80,8.74" target="#b4">[5]</ref>. Generally speaking, the document-level results closely tracked the message-level results.</p><p>For post-hoc scoring of new experimental result sets, the relevance judgments and evaluation software are available in the evalInt09.zip archive at http://trec.nist.gov/data/legal09.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Lessons from the 2009 Interactive Task</head><p>The 2009 Interactive task brought with it a greater number of topics, a greater number of participants, and, it must be admitted, a good amount of growing pains. Most importantly, however, the task has provided the Legal Track community with a rich set of data that we believe will be the basis for much productive research in the future; the task has also provided us with a number of lessons that we can carry forward to future studies. Among the lessons we would highlight are the following.</p><p>First, with regard to the methods evaluated, the 2009 Interactive task has shown that more than one method can be effective at retrieving responsive documents and that a method can be effective on a range of different topics. A subject for further exploration is the identification of those elements that the methods shown to be effective have in common.</p><p>Second with regard to the design of the Interactive task, the 2009 results bring to light some issues that deserve attention, including:</p><p>• making the adjudication process less dependent on participant initiative;</p><p>• making the appeal/adjudication process more efficient;</p><p>• addressing the challenge of sampling for very low frequency items (such as relevant documents in the "All-N" stratum); and</p><p>• managing the task in such a way that it adheres to its timelines.</p><p>We look forward to building on these lessons in 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Batch Task</head><p>The Batch task of the TREC 2009 Legal Track was a successor to the Ad Hoc and Relevance Feedback tasks of past years. The Batch task supports researching the effectiveness of the second-pass of two-pass search approaches to e-discovery (i.e., feedback approaches), and also the effectiveness of single-pass search approaches. The "Batch" name comes from "Batch Filtering" in the earlier TREC Filtering Track, in which all evidence regarding relevance was made available at the outset of the task (in contrast to "Adaptive Filtering," which had been designed to simulate active learning as the task progressed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scanned Document Collection</head><p>The collection to be searched in the Batch task was the same document collection as the 2006-2008 Legal Tracks, the IIT Complex Document Information Processing (CDIP) Test Collection, version 1.0 (referred to here as "IIT CDIP 1.0") which is based on scanned documents released under the tobacco "Master Settlement Agreement" (MSA). The University of California San Francisco (UCSF) Library, with support from the American Legacy Foundation, has created a permanent repository, the Legacy Tobacco Documents Library (LTDL), for tobacco documents <ref type="bibr" coords="21,247.77,549.26,14.61,8.74" target="#b13">[14]</ref>. The IIT CDIP 1.0 collection is based on a snapshot, generated between November 2005 and January 2006, of the MSA subcollection of the LTDL. The IIT CDIP 1.0 collection consists of 6,910,192 document records in the form of XML elements, that contain both metadata and the results of Optical Character Recognition (OCR). See the 2006 TREC Legal Track overview paper for additional details about the IIT CDIP 1.0 collection <ref type="bibr" coords="21,321.49,597.08,9.96,8.74" target="#b8">[9]</ref>. Although the original scanned documents are also available (on request), we do not know of any site that has used them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Selection</head><p>The Batch task re-used 10 of the "production request" topics from previous years. The structure of the topics was identical to those used in the Interactive task, with the addition of a negotiated Boolean query (for reference) and some snapshots from the negotiation history (these snapshots were also provided as Boolean queries).</p><p>The number of test topics this year <ref type="bibr" coords="22,248.14,99.07,17.71,8.74" target="#b9">(10)</ref> was lower than the 40-50 typically used in past query sets in part to compensate for the footprint of allowing deeper ranked submissions for each topic (up to 1.5 million documents per topic, which was 15 times more than the previous year). Reducing the number of topics also allowed denser sampling of the document pools (from assigning multiple assessors to a topic, as was done in the Interactive task), making possible more accurate estimates of evaluation measures.</p><p>Included in the 10 topics were the 3 topics from the Interactive task of 2008 (topics #102 (2008-I-1), #103 (2008-I-2) and #104 (2008-I-3)). These 3 topics had been seeded in the topic set of the 2008 Ad Hoc task in hopes of allowing comparison of that year's automatic runs to the Interactive runs. However, the Ad Hoc submission limit of 100,000 documents per topic turned out to be too low to allow a fair comparison in 2008. (For instance, the top-scoring Interactive system on topic 103 submitted 608,807 documents and achieved an F 1 of 0.71. The estimated number of relevant documents for this topic was 786,862. Hence a "perfect" Ad Hoc system could not achieve a recall of more than 13% (100,000/786,862) or an F 1 of more than 0.23 (2*1.0*0.13/(1.0+0.13)) on topic 103 under last year's guidelines.) This year the submission limit was increased to 1.5 million documents per topic.</p><p>Another reason for including the 3 past Interactive topics was that the submissions in past years of the Relevance Feedback task (2007 and 2008) had generally not achieved the gains anticipated compared to the baseline (non-feedback) runs. The Interactive topics had more past judgments available for system use than the other past topics (up to 6,500 judgments for topic 103). Furthermore, the Interactive task had an adjudication phase which likely further improved the quality of the relevance assessments for feedback. Also, the manual expert searchers participating in the Interactive task may have turned up some relevant documents for feedback that automatic runs might have been missed.</p><p>For contrast, also included in the 10 topics were 3 topics from the 2008 Ad Hoc task, a task which generally just included submissions from automated runs. One topic was randomly chosen for each of the 3 complaints. These were #105 (2008-F-1), #138 (2008-G-9) and #145 (2008-H-4).</p><p>Also included in the 10 topics were 2 topics from the 2007 Ad Hoc topic set. For these, the selection was from the 7 topics that had been re-used in the 2008 Relevance Feedback task (because more relevance assessments were available for those than for other 2008 topics). The 2 topics selected were the ones that had the most consistent estimated numbers of relevant documents between the two tasks (which might be indicative of consistent assessing across the two years). These topics were #80 (2007-C-2) and #89 ( <ref type="figure" coords="22,76.52,445.77,36.13,8.74" target="#fig_2">2007-D-1</ref>).</p><p>Also included in the 10 topics were 2 topics from the 2006 Ad Hoc topic set. These topics were selected from the 3 highest-priority Interactive topics of 2007, which included some manual expert submissions. Also, these topics had additional judgments from deep sampling from being used in the 2007 Relevance Feedback task. These topics were #7 (2006-A-2) and #51 (2006-E-10).</p><p>The 10 test topics were made available in the same XML format as the previous years (as fullL09.xml and shortL09.xml). They are now posted at http://trec.nist.gov/data/legal09.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Judgments</head><p>The past judgments (known as "training qrels" or "training judgments") were available to the participating systems in two files, qrelsL09.pass1 and qrelsL09.pass1 probs. (These files are now posted in the Batch-Topics2009.zip file at http://trec.nist.gov/data/legal09.html).</p><p>The relevance judgments were from the following sources: For topics 7 and 51, included were the judgments from the 2006 Ad Hoc task and the "residual" judgments (i.e., judgments omitting those already judged for the 2006 Ad Hoc task) from the 2007 Interactive and Relevance Feedback tasks.</p><p>For topics 80 and 89, included were the judgments from the 2007 Ad Hoc task and the residual judgments from the 2008 Relevance Feedback task.</p><p>For topics 102, 103 and 104, included were the post-adjudication judgments from the 2008 Interactive task.</p><p>For topics 105, 138 and 145, included were the judgments from the 2008 Ad Hoc task. For the sampling probabilities (the 5th column in qrelsL09.pass1 probs), when two different years were combined, the probabilities were set to "1.0" for the judgments from the earlier year and the residual sampling probabilities were preserved for the judgments from the later (residual) year. Mathematically, this approach simulated sampling from the later residual pool with the original judgments added back in.</p><p>Also, for cases of two years of judgments being used, because just residual judgments were used from the later year, there hence were no documents with two different judgments for the same topic in qrelsL09.pass1 or qrelsL09.pass1 probs. We note, however, that assessors in different years may have had different conceptions of relevance, and that no attempt had been made to standardize those conceptions.</p><p>The following list shows, for each of the 10 topics, the count of the number of relevance judgments in the provided training qrels files (including "gray" documents), the number judged relevant, and the number judged non-relevant: Topic 7: count=1269, rel=307, non=951 Topic 51: count=1361, rel=88, non=1259 Topic 80: count=1879, rel=734, non=1139 Topic 89: count=874, rel=201, non=607 Topic 102: count=4500, rel=1548, non=2887 Topic 103: count=6500, rel=2981, non=3440 Topic 104: count=2500, rel=92, non=2391 Topic 105: count=701, rel=156, non=540 Topic 138: count=600, rel=125, non=472 Topic 145: count=499, rel=200, non=297</p><p>For 5 of the 10 topics, the past relevant judgments distinguished between highly relevant documents and all other relevant documents (topics #80, 89, 105, 138, 145).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Measures</head><p>The goal of the systems was to specify the set of all relevant documents in the collection. The main measure for evaluating the accuracy of the submitted set was the F 1 measure (F 1 is 2 * Precision * Recall / (Precision + Recall), or 0 if both Precision and Recall are zero).</p><p>In order to also support evaluation with rank-based measures, the systems were requested to submit a ranked list of (up to) 1.5 million documents for each topic, ranking the documents in descending order of probability of (or, at the system's option, degree of) relevance to the topic.</p><p>For the set-based evaluation, the systems were required to specify a value K for each topic; that system's top-ranked K documents for that topic would form the set for evaluation with F 1 (hence sometimes the target measure is called F 1 @K).</p><p>Furthermore, the systems were required to specify a K h value for each topic for set-based evaluation with just Highly Relevant documents.</p><p>In contrast with the 2007 and 2008 Relevance Feedback tasks, residual evaluation was not used this year, so participating teams were advised to include previously judged documents in their submitted result sets (if their system considered them possibly relevant). If this year's sampling happened to draw documents that were previously judged, this year's assessors would re-assess them (with no knowledge of the previous assessment), and just this year's assessments would be used in this year's evaluation. Past judgments hence were not considered to be authoritative, but rather as one opinion of relevance for a sample of documents. This was intended to model the real-world issue that internally generated training data (e.g., generated locally by an e-discovery service provider) may not perfectly match the final authority's conception of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Submissions</head><p>4 research teams submitted a total of 10 runs for this year's Batch task by the deadline of August 4, 2009. Participating teams could submit as many as 3 runs. The participating groups were EMC -CMA -R&amp;D, Open Text Corporation, University of Waterloo, and Ursinus College. (The submissions from Open Text were labelled as "manual" because they were produced by the same individual who was involved in coordinating the task.) Please consult the papers from the participating teams for details of their submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Reference Runs</head><p>For set-based measures, the tables include 4 reference runs: fullset09 (the entire set of documents in the collection), oldnon09 (the set of documents judged non-relevant in previous years), oldrel09 (the set of documents judged relevant in previous years), and refL09B (the set of documents matching the final negotiated Boolean query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Evaluation</head><p>Evaluation was done entirely with new judgments (not the training judgments). These new judgments are sometimes referred to as the "test qrels" or "test judgments." As in previous years, it was not feasible to judge all 6,910,192 documents for every test topic, so a deep sampling method was used to estimate the scores, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.1">Pooling</head><p>As in the 2008 and 2009 Ad Hoc tasks, we formed a pool of documents for each topic consisting of all of the documents submitted by any of the 10 participant runs and the 4 reference runs. The inclusion of the fullset09 reference run meant that in practice all 6,910,192 documents in the collection were actually in the pool for every topic this year. The value-added of the other runs was that they affected the "hiRank" of each document (i.e., its highest rank in any run), which affected the sampling probabilities as described below.</p><p>Additionally, the pools included all of the submitted and reference runs from 2006, 2007 and 2008 (except for the random reference runs of 2007 and 2008). Again, the benefit of including these runs is that they would impact the sampling probabilities ("priors") so that the scoring of the past runs with the new judgments would tend to be more accurate (i.e., with narrower error bars) than they otherwise would be.</p><p>An issue discovered during pooling was that the training qrels for topics 7 and 51 included some docids that were not in the official XML version of the collection. (These are believed to have come from past expert runs that used the interactive Legacy Tobacco Document Library search engine, which was not limited to the snapshot that we had obtained for the IIT CDIP version 1.0 collection.) These documents were dropped from the pools (reducing the number to the expected 6,910,192 for each topic); however, they were not dropped until after the "hiRank" was recorded, so they may have had a minor impact on the sampling probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.2">Sampling</head><p>This year, the following formula was used for p(d), the probability of judging each document d in the pool for a topic:</p><formula xml:id="formula_1" coords="24,82.46,551.90,214.44,8.30">p(d) = min(1.0, ((1/5000)+(C/hiRank(d))))</formula><p>where hiRank(d) is the earliest (i.e., best) rank at which any included run retrieved document d, and C was chosen so that the sum of all p(d) (for all documents d in the pool) was 2500 (which was the number of documents that we expected could be judged). It turned out for two topics fewer than 2500 documents were judged; in these cases the final probabilities were corrected as described below.</p><p>For the reference runs, which did not rank the documents, for hiRank purposes each document was considered to be of rank equal to the number of documents in the run for that topic. For example, any document in fullset09 that was not in any other run would have been assigned a hiRank of 6,910,192.</p><p>For the 8 topics in which all 2500 documents were judged, the floor of 1/5000 in the formula implied that typically at least 1 in every 5000 documents were judged, which is approximately the same as the coarsest sampling for topic 104 of the Interactive task of the previous year (which also had 2500 documents judged, sampled from the entire collection).</p><p>The function of the C/hiRank(d) in the formula was to provide denser sampling of higher-ranked documents to improve the accuracy of measures at shallow depths, e.g. K &lt; 1000. The median C value turned out to be about 6 (ranging from 3.9 to 9.5, see the Appendix of these proceedings for the C value for each topic). A C value of 6 (when all 2500 documents were judged) would imply that at any depth K ≥ 6 we would expect to have at least the accuracy of 6 random sample points. But the floor of 1/5000 implied that we would have more accuracy for K &gt; 30,000; e.g. for K=100,000, the selection of 1 in 5000 implies the accuracy of 20 random sample points.</p><p>After the draw, the typical distribution of hiRanks in a sample of 2500 documents was found to be approximately as follows (based on using topic 138 as the most typical example as it had the median C value of 6.18): 10% were top-10 ranked (hiRank &lt;= 10) 10% had 10 &lt; hiRank &lt;= 100 10% had 100 &lt; hiRank &lt;= 1,000 10% had 1,000 &lt; hiRank &lt;= 20,000 10% had 20,000 &lt; hiRank &lt;= 100,000 10% had 100,000 &lt; hiRank &lt;= 500,000 10% had 500,000 &lt; hiRank &lt;= 1,000,000 8% had 1,000,000 &lt; hiRank &lt;= 1,500,000 22% were unsubmitted documents (i.e., hiRank=6,910,192)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.3">Binning</head><p>The binning approach this year was a little different than that used in the Ad Hoc tasks of the past couple years to allow multiple assessors to contribute judgments for the same topic (a practice that we first tried in the 2008 Interactive task).</p><p>This year, the draw of 2500 documents for each topic was randomly divided into 10 bins of 250 documents each. This approach would cause each bin to have approximately the same distribution of hiRanks as the initial draw.</p><p>Typically 5 volunteer assessors were assigned to each topic, with each assessor asked to complete 2 bins. In a few cases, an assessor completed just 1 bin or contributed extra bins. Only fully completed bins were kept for this year's official judgments. The result was that all 2500 documents were assessed for 8 of the 10 topics; the exceptions were #51 (1500 documents assessed) and #89 (1250 documents assessed). For these latter 2 topics, the judging probabilities in the official qrels were multiplied by 1500/2500 and 1250/2500 respectively to factor in the probability of the drawn document being in a judged bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Relevance Judgments</head><p>As in the past couple of years, we primarily sought out second-year and third-year law students who would be willing to volunteer as assessors. As in 2007 and 2008, the assessors used a Web-based platform developed by Ian Soboroff at NIST and hosted at the University of Maryland to view scanned documents and to record their relevance judgments. The assessors made their judgments based on the scanned images of the documents, not the rendering in XML that the participant systems typically worked with.</p><p>New this year was that the assessors were given 10 examples each of documents previously judged highly relevant, as relevant but not highly relevant, and as non-relevant in past assessing of the topic (if past assessing did not distinguish between highly relevant and relevant but not highly relevant, then just 10 example relevant documents and 10 example non-relevant documents were given). These examples were provided in hopes of improving consistency with past assessments, but the assessors were expected to make their own judgment if a previously judged document happened to be selected again by the sampling. The assessors (typically 5 per topic) were also encouraged to email each other when they had questions in hopes of further improving consistency of the assessing.</p><p>For last year's Interactive topics (#102, 103 and 104), this year's assessors were also provided with detailed guidelines created for these topics in 2008 <ref type="bibr" coords="26,295.00,111.02,9.96,8.74" target="#b1">[2]</ref>.</p><p>Each reviewed document was judged highly relevant, judged relevant, judged non-relevant, or left as "gray." Our "gray" category includes all documents that were presented to the assessor, but for which a judgment could not be determined. Among the most common reasons for this were documents that were too long to fully review (and that could not be easily recognized as relevant), or for which there was a technical problem with displaying the scanned document image.</p><p>The relevance judgments ("test qrels" or "test judgments") are available in the qrelsL09.probs file of the resultsL09.zip archive at http://trec.nist.gov/data/legal09.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Computing Evaluation Measures</head><p>The formulas for estimating the number of relevant, non-relevant and gray documents in the pool for each topic, and also for estimating precision and recall, were the same as in the 2007 Ad Hoc task <ref type="bibr" coords="26,484.74,252.94,14.61,8.74" target="#b14">[15]</ref>, and the formulas for estimating F 1 were the same as in the 2008 Ad Hoc task <ref type="bibr" coords="26,378.24,264.90,14.61,8.74" target="#b12">[13]</ref>.</p><p>The software used to compute the evaluation measures was version 2.4 of the l07 eval utility (which is available at http://trec.nist.gov/data/legal09.html).</p><p>For runs that did not contribute to the pools, the same estimation process can be used, albeit with possibly larger sampling errors (since post hoc use of the collection cannot influence hiRank() and thus must accept the sampling probabilities determined from the official submissions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Results</head><p>For this year's official submissions, the following 4 tables of mean scores over the 10 test topics are provided: A detailed glossary for the acronyms in the tables can be found in an Appendix document of these proceedings <ref type="bibr" coords="26,125.99,482.53,9.96,8.74" target="#b3">[4]</ref>. The Appendix document also includes 4 tables for each individual topic (i.e., an additional 40 tables).</p><formula xml:id="formula_2" coords="26,86.94,390.88,4.98,8.74">•</formula><p>These results help provide insight for several questions, such as the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.1">What scores were the systems able to attain with the hundreds of training examples?</head><p>The highest average F 1 score was just 0.21 (from the participant "watstack" system as per Table <ref type="table" coords="26,508.80,550.73,3.87,8.74" target="#tab_9">7</ref>). On individual topics, the highest F 1 score was 0.57 on topic #103 (as per Table <ref type="table" coords="26,411.68,562.68,9.96,8.74" target="#tab_1">21</ref> of <ref type="bibr" coords="26,436.90,562.68,10.30,8.74" target="#b3">[4]</ref>). On topic #51, the highest F 1 score was just 0.03 (as per Table <ref type="table" coords="26,266.41,574.64,4.98,8.74" target="#tab_6">5</ref> of <ref type="bibr" coords="26,286.06,574.64,10.30,8.74" target="#b3">[4]</ref>); we look at this topic more below. Intuitively, since F 1 is the harmonic mean of precision and recall (for individual topics), a typical F 1 score of 0.2 would imply that typically either precision or recall was below 0.2, which suggests that even with a lot of training examples, the task was still very challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.2">Were the results different if just counting "highly relevant" documents?</head><p>Just counting "highly relevant" documents as relevant, the highest average F 1 score was 0.19 (from the participant "watlogistic" system as per Table <ref type="table" coords="26,277.73,666.75,3.87,8.74" target="#tab_10">8</ref>). On individual topics, the highest F 1 score was 0.73 on topic #89 (as per Table <ref type="table" coords="26,177.23,678.70,9.96,8.74" target="#tab_18">14</ref> of <ref type="bibr" coords="26,200.96,678.70,10.30,8.74" target="#b3">[4]</ref>). On topic #104, the highest F 1 score was just 0.03 (as per 3.10.3 Would the scores have been much higher if the systems had thresholded their ranked lists optimally?</p><p>For the F 1 measure, it typically is best to threshold the ranked list at depth R, where R is the (estimated) number of relevant documents. The highest average F 1 @R score was still just 0.26 (from the participant "watstack" system as per Table <ref type="table" coords="29,215.36,179.22,3.87,8.74" target="#tab_11">9</ref>). On individual topics, the highest F 1 @R score was 0.58 on topic #103 (as per Table <ref type="table" coords="29,131.76,191.17,9.96,8.74" target="#tab_3">23</ref> of <ref type="bibr" coords="29,155.58,191.17,10.30,8.74" target="#b3">[4]</ref>). On topic #51, the highest F 1 @R score was just 0.03 (as per Table <ref type="table" coords="29,464.40,191.17,4.98,8.74" target="#tab_9">7</ref> of <ref type="bibr" coords="29,483.24,191.17,10.79,8.74" target="#b3">[4]</ref>) which we look at more below. So, at least for the top-scoring systems, better thresholding would not have dramatically increased the F 1 scores.</p><p>Note that, at depth R, precision, recall and F 1 are all the same. (Put another way, the popular "R-Precision" measure of ranked retrieval is equivalent to "F 1 @R" and also "Recall@R".) Hence the F 1 @R measure is easy to interpret. For example, a typical F 1 @R of 0.2 indicates that, even if a system's ranked list was thresholded optimally for the F 1 measure, the resulting set would typically have both precision and recall of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.4">Do we know if the systems did better than just randomly picking documents?</head><p>If the entire set of 6,910,192 documents had been retrieved, the average F 1 score would have been just 0.08 (as per the "fullset09" reference run listed in Table <ref type="table" coords="29,323.25,331.10,3.87,8.74" target="#tab_9">7</ref>). Most of the participant systems substantially outscored the fullset09 reference run.</p><p>The fullset09 result represents the highest F 1 that one could expect from a random run (because a random run would be expected to have approximately the same precision as fullset09 and could not produce higher recall than fullset09).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.5">How did this year's Batch systems do compared to last year's Interactive submissions?</head><p>Table <ref type="table" coords="29,100.32,423.21,9.96,8.74">11</ref> shows the set-based scores for topic #103 using all relevant documents. For this topic, several additional reference runs are available, including the 5 runs of last year's Interactive task. (The Interactive runs are renamed to have an "int08" prefix to emphasize that they were produced under different conditions than this year's runs. Most notably, the previous relevance assessments were not available as an input for last year's Interactive runs. More details of these runs and the other additional reference runs are in the glossary of the Appendix <ref type="bibr" coords="29,187.06,482.98,10.30,8.74" target="#b3">[4]</ref>.) We see that, with the 6500 relevance judgments as an input, some of this year's submissions were able to achieve the same or higher F 1 as any of last year's Interactive submissions. This is an encouraging result for feedback techniques, albeit based on just one topic.</p><p>Part of why the Batch systems scored higher in F 1 on topic #103 is that they chose a larger K value (e.g., 1 million) than the highest-scoring Interactive submission (608,807 documents), which was beneficial in part because the estimated numbers of relevant documents was higher with the new judgments (1,046,834) than the training judgments (786,862). (The training judgments used the post-adjudication judgments from 2008. In the pre-adjudication judgments of 2008, the estimated number of relevant documents was 914,528, which is closer to the estimate based on the new judgments, which did not go through an appeal process.)</p><p>Since the Batch systems ranked their documents, we can calculate what their precision, recall and F 1 would have been if they had set K=608,807 (the size of the highest-scoring Interactive submission set). (This calculation is done by re-running the l07 eval utility for the system with K set to 608,807 for topic #103.) For instance, we find for the highest-scoring Batch system of Table <ref type="table" coords="29,369.68,626.44,9.96,8.74">11</ref> (the "watstack" system) that, in its top-ranked 608,807 documents, it had a precision of 0.71, recall of 0.43 and F 1 of 0.54, which are all slightly higher than the highest-scoring Interactive submission (which scored, on this year's judgments, a precision of 0.69, recall of 0.40 and F 1 of 0.51). Hence the higher F 1 scores of some of the Batch runs is not just Table <ref type="table" coords="30,98.52,371.35,8.49,8.74">11</ref>: Set-based measures for topic 103 using all relevant documents (1,046,833.8 est. relevant documents) from choosing a larger K value. (Note that we have not checked whether any of these F 1 differences are statistically significant.)</p><p>As a tangential remark, the preceding result suggests that the F 1 score can actually be fairly stable over a wide range of threshold settings. For example, for the "watstack" system, increasing K from 608,807 to (its actual setting of) 1,000,000 decreases its precision by 12 points (from 0.71 to 0.59), increases its recall by 13 points (from 0.43 to 0.56), but shifts its F 1 by only 3 points (from 0.54 to 0.57).</p><p>It should be kept in mind that topic #103 was anticipated to be the topic for which feedback had the best chance of success, since this topic had the most training examples (6500, including 2981 examples of relevant documents), and the quality of the training examples was presumably improved by the adjudication process that was part of the 2008 Interactive task.</p><p>3.10.6 Why were the F 1 scores so low for topic #51?</p><p>For topic #51, the systems were given 88 examples of relevant documents (and 1259 examples of non-relevant documents). So why did no system attain an F 1 score above 0.03?</p><p>As mentioned earlier, the main issue was not picking the threshold K, since the highest F 1 @R score was still just 0.03.</p><p>One clue is that the training judgments for topic #51 produced an estimate of 95 relevant documents for the topic, whereas the new judgments produce an estimate of 26,404 relevant documents.</p><p>Most of the contribution to the estimate of 26,404 came from just 3 documents which were judged relevant: afr23a00 (weight 8295.1, only retrieved by fullset09), azt40e00 (weight 8010.9, hiRank 791,311), and bga62e00 (weight 7983.5, hiRank 726,803). (The most deeply sampled relevant documents for each topic are listed in an Appendix document of these proceedings <ref type="bibr" coords="30,323.22,673.17,10.30,8.74" target="#b5">[6]</ref>.) Based on our own look at these documents, it appears that these 3 judgments were "false positives." Each came from a different assessor (there were 3 assessors for this topic). Each assessor contributed 500 judgments (for a total of 1500 judged documents for this topic). Even if the assessing was 99% accurate, misjudging 1% of a collection of 7 million documents could lead to the estimated number of relevant documents being off by several thousand. For topics with large numbers of relevant documents (e.g., 100,000+), such errors would likely be just minor noise, but for "low-density" topics (i.e., topics with small true numbers of relevant documents, which appears to be the case for topic #51), these errors can be dominant for measures, such as recall and F 1 , that are based on the estimated total number of relevant documents in the collection. (Estimates of precision, however, would typically be less affected by a small error rate, though the accuracy of most measures of course is still subject to sampling error.)</p><p>We have seen evidence in the past of this kind of issue. For instance, we reported last year <ref type="bibr" coords="31,502.13,194.71,15.50,8.74" target="#b12">[13]</ref> that the assessing of the "random run" appeared to have a lot of false positives (approximately 1%). In the Interactive task of 2008, almost half of relevant judgments from the "All-N" stratum for Topic 103 were overturned on appeal (51 out of 111 as per Table <ref type="table" coords="31,288.84,230.58,9.96,8.74" target="#tab_16">12</ref> of <ref type="bibr" coords="31,313.37,230.58,15.50,8.74" target="#b12">[13]</ref>) and over all the strata the appeals reduced the estimated number of relevant documents from 914,528 to 786,862 (a reduction of 127,666, which is almost 2% of the full collection).</p><p>Our past Ad Hoc evaluations were less susceptible to this issue because the full collection was not pooled, and the evaluation essentially ignored documents that were not in the pool. (Unlike traditional TREC evaluations, our measures do not assume unpooled documents are non-relevant, but typically behave as if they had not been in the set being evaluated.) Of course, one could simulate last year's approach post-hoc by discarding judged documents of hiRank greater than 100,000 and then check the impact on the F 1 scores (but such a study is beyond the scope of this paper).</p><p>There was discussion at the conference about what to do in the future for this issue. One suggestion was to reassess high-weight relevant documents before releasing the results when overturning a few of them could make a dramatic difference.</p><p>Note that this issue does not necessarily affect the relative mean scores substantially. Indeed, if the issue dampens the scores for a topic (placing them all in a narrow range such as 0.00 to 0.03), the topic will have little impact on the relative mean scores over all the topics. 3.10.7 What scores would have resulted from just submitting the example relevant documents?</p><p>If just the relevant documents from the training qrels had been submitted, the average F 1 score would have been just 0.004 (as per the "oldrel09" reference run listed in Table <ref type="table" coords="31,373.13,466.14,3.87,8.74" target="#tab_9">7</ref>). The average recall of the relevant training examples was just 0.2%. This result also suggests that, if we had used "residual" evaluation as in some past years (i.e., discarded the training examples before scoring the runs) it would not have affected the scores much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.8">Was there any overlap in the training and new judgments?</head><p>For all 10 topics, there were some training documents that were in the new judging samples (which is unsurprising because the "oldrel09" and "oldnon09" reference runs were in the pools which influenced the "hiRank" of these documents). Hence some preliminary indications of assessor consistency can be gleaned from the tables. For example, of documents previously judged relevant (oldrel09 run), Table <ref type="table" coords="31,343.21,594.11,4.98,8.74" target="#tab_9">7</ref> shows that the estimated precision was just 0.78 this year (using all relevant documents). It also shows that, on average, 74 documents per topic that had previously been judged as relevant were re-assessed, with an average of 65 of them again being judged as relevant. The tables also contain results for past judged non-relevant documents (oldnon09).</p><p>Consistency results for individual topics are available in an Appendix of these proceedings <ref type="bibr" coords="31,483.59,641.94,9.96,8.74" target="#b3">[4]</ref>, including separate results for documents previously judged as highly relevant and as relevant but not as highly relevant (oldHrel09 and oldOrel09 reference runs) for the 5 topics whose past judgments distinguished between highly relevant and other relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.9">Did the systems outperform the reference Boolean query?</head><p>The reference final negotiated Boolean query had an average F 1 score of just 0.06 (as per the "refL09B" run listed in Table <ref type="table" coords="32,137.60,105.50,3.87,8.74" target="#tab_9">7</ref>). Most of the participant systems substantially outscored the reference Boolean run in F 1 . One problem for the Boolean query was that on average it had just 27,462 hits, and partly as a consequence its recall averaged less than 4%.</p><p>The reference Boolean run produced a mean precision of 0.39. More than half of the submitted (ranked) runs had a higher mean Precision@B (where B is the number of documents returned by the reference Boolean run), with the highest being 0.58 (from the participant "watstack" system as per Table <ref type="table" coords="32,473.42,165.28,3.87,8.74" target="#tab_11">9</ref>). Examining Recall@B shows a similar pattern.</p><p>In 2006 and 2007, the reference Boolean results had proven to be difficult to beat on average over the full topic set. Only 4 of those topics were in this year's test set (#7, #51, #80 and #89), and past performance compared to Boolean results was not a consideration when selecting them. It turned out that 3 of these 4 topics were ones for which past non-feedback approaches were able to outperform the Boolean query in Precision@B. The exception was topic #51, and once again none of the participant systems had a higher Precision@B than the reference Boolean query precision of 7% (as per Tables <ref type="table" coords="32,413.20,248.96,4.98,8.74" target="#tab_6">5</ref> and<ref type="table" coords="32,441.00,248.96,4.98,8.74" target="#tab_9">7</ref> of <ref type="bibr" coords="32,460.80,248.96,10.30,8.74" target="#b3">[4]</ref>). We speculate that the attempts to make the topic requests narrow in 2006 and 2007 (compared to 2008, as discussed last year <ref type="bibr" coords="32,112.98,272.88,15.50,8.74" target="#b12">[13]</ref>) made it easier to construct relatively successful Boolean queries in those years (though they still tended to be of limited precision and recall). In 2008 and 2009, the (mostly automated) participant approaches were more often relatively successful compared to the reference Boolean queries (though again, the precision and recall still tended to be less than desired).</p><p>3.10.10 Why was the Boolean query's recall so much lower this year than in past years?</p><p>As previously stated, according to the new judgments, the recall of the reference final negotiated Boolean query averaged just 4% over the 10 test topics, but according to the training judgments, its recall averaged 17%. Why such a difference? Table <ref type="table" coords="32,114.87,388.89,9.96,8.74" target="#tab_16">12</ref> breaks down the comparison by topic. For each topic, it shows B (the number of documents matched by the final negotiated Boolean query). In the remaining columns ("Est. Rel@B", "Est. Non@B", "Precision", "Total Est. Rel" and "Recall"), it shows both the estimate based on the (old) training judgments and the (new) test judgments.</p><p>(The horizontal lines of Table <ref type="table" coords="32,226.22,436.71,9.96,8.74" target="#tab_16">12</ref> break the topics into 4 groups based on "ancestry" as outlined in Section 3.2.)</p><p>The "Est. Rel@B" column shows that the training and test estimates of the number of relevant documents matched by the reference Boolean query were highly correlated, and the average estimates across topics were close to the same (14,275 based on the training judgments, 15,295 based on the test judgments). The estimated numbers of non-relevant documents matched also correlated highly. As a consequence, the estimated precision of the Boolean query correlated highly.</p><p>However, the "Total Est. Rel" column shows that the new test judgments produced estimates for the total number of relevant documents that were much larger than the estimates from the training judgments (on average, almost twice as large). This result led to the substantially lower recall estimates with the test judgments.</p><p>We suspect that the full collection sampling used this year, in combination with the suspected false positive rate discussed in Section 3.10.6, has led to total number of relevant documents being overestimated, and if so the reported recall and F 1 scores would tend to be lower than they should be. However, as the same judgments are used for scoring all systems, the relative scores should still be meaningful (within the expected accuracy limitations from sampling error). higher score is preferred.) LAM is the logit-average of '1-Recall' and 'Fallout' (after smoothing these inputs with an "epsilon" of 0.5 to prevent 0 or 1 inputs; e.g., for '1-Recall', instead of 1-(r/R), use 1-((r+ )/(R+2 )), and for 'Fallout', instead of n/N, use (n+ )/(N+2 )). The logit-average tends to emphasize whichever input is closer to a 0 or a 1 (or equivalently, whichever is further from 0.5).</p><p>In our experiments, LAM favored different systems than F 1 . LAM typically favored the highest-precision result, which typically was a small set of low recall and hence low F 1 . In particular, the best LAM score on average was from just submitting the set of example relevant documents, which had a recall of just 0.2% (as per the "oldrel09" listing in Table <ref type="table" coords="33,223.21,363.76,3.87,8.74" target="#tab_9">7</ref>).</p><p>For e-Discovery, the F 1 measure appears to favor the more desirable results. F 1 encourages both recall and precision and severely penalizes a low score in either of these components.</p><p>3.10.12 What were the most important results from the Batch task this year?</p><p>The biggest contribution from the Batch task would be that 10 new standard test topics are now available for systems to gauge their effectiveness at making use of training examples for discovery requests.</p><p>Perhaps the most encouraging result was that, by making use of the training examples, some of the automated systems were able to attain at least the same levels of effectiveness as the Interactive submissions of the previous year (which did not have the advantage of the training examples). Of course, last year was our first year of collecting large-scale interactive results, there were only a handful of submissions, and these were just on a small subset of the topics. This year's Interactive task has collected the results of interactive approaches for several more topics (on a different collection), which should enable future studies to investigate whether or not this year's Batch task result is typical.</p><p>An evaluation issue encountered with the full-collection sampling used this year was that a small false positive rate in the assessing appears to have substantially dampened the reported recall and F 1 scores for low-density topics. It would be advisable for future studies to anticipate this issue, either by preferring highdensity topics, or perhaps by scheduling up front a re-assessment phase for high-weight documents that can substantially affect the scoring (perhaps even before releasing pre-adjudication results, if the task includes an appeal process, such as the Interactive task of the most recent two years). judgments for each topic (or all of the relevant documents for the topic if there were fewer than 10). Similarly, a reference "oldnon08" run was created by randomly choosing 10 documents from the non-relevant documents in the training judgments for the topic. These (up to 20) documents were then included in the set of (typically 400) documents judged by the new assessors in 2008 (who were not told which documents had been judged before). The results of the new assessors' judgments of the training documents were summarized in Table <ref type="table" coords="34,535.02,377.52,4.98,8.74" target="#tab_9">7</ref> of last year's track overview paper <ref type="bibr" coords="34,224.99,389.48,14.61,8.74" target="#b12">[13]</ref>.</p><p>A bug was later found in how the "oldrel08" run was produced. The random sample of 10 documents was taken not just from the past relevant documents for the topic, but also from the past gray documents. This bug caused gray documents to be in "oldrel08" for 4 of the 12 topics (6 gray documents were in oldrel08 for topic #60, 1 for #79, 1 for #85, and 2 for #89). The bug in the 'relsubset' option of l07 eval.c was discovered while producing the "oldrel09" run for the 2009 Batch task and was fixed on Aug 9, 2009, in time for the 2009 task.</p><p>(The bug was that the 'relsubset' option of l07 eval.c identified relevant documents in the input qrels as any document with a non-zero judgment label. This heuristic was sufficient for correctly producing the "oldrel07" run when the option was first added in 2007, but in 2008 the training qrels started to include "gray" documents which were labeled as -1 or -2. The fix in 2009 was just to consider the labels of 1 or 2 as relevant.)</p><p>The corrected version of last year's Table <ref type="table" coords="34,269.80,532.94,4.98,8.74" target="#tab_9">7</ref> is presented herein in Table <ref type="table" coords="34,401.17,532.94,8.49,8.74" target="#tab_17">13</ref>. For this corrected table, the oldrel08 run was replaced with a subset called "oldrel08Fixed" which omits the unintended gray documents. The "Previously Judged Relevant" column shows how the new assessor in 2008 judged the (up to) 10 documents that were judged relevant when the topic was used in the 2006 or 2007 Ad Hoc task (as per the oldrel08Fixed run). The "Prev. Judged Non-relevant" column shows the same information for the 10 documents previously judged non-relevant (as per the oldnon08 run). The labels are "tot" for total judged (which was 10 except when less than 10 relevant documents were in oldrel08Fixed for the topic), "hrel" for highly relevant, "orel" for other relevant, "non" for non-relevant, and "gr" for gray. Last year's paper stated that 58% of previously judged relevant documents were judged relevant again in 2008; the corrected number is 62% (based on ((28+36)/(106-2))). As stated last year, almost half of these (44%) were judged highly relevant (note that before 2008, the "highly relevant" category was not available). Also, as stated last year, 18% of previously judged non-relevant documents were judged relevant in 2008; note that these non-relevant documents may have been rated highly by past search engines (which would Topic Previously Judged Relevant (oldrel07) Prev. Judged Non-relevant (oldnon07) have boosted their chance of being in the previous judging pool in the first place).</p><p>In the corrected version of the table, for the previously judged relevant documents, we now see full agreement for two of the topics (topics #60 and #89). For the previously judged non-relevant documents, there are 3 topics for which both assessors agreed that all 10 documents were non-relevant (topics #14, #36 and #83). It is still in the case in the corrected version of the table that there were 2 topics (#73 and #85) for which the assessor in 2008 found that more of the previously judged non-relevant documents were relevant than of the previously judged relevant documents. Last year's paper noted that "Past assessor agreement studies typically have found a lot of assessor disagreements, but generally retrieval systems are rated similarly regardless of which assessor's judgments are used <ref type="bibr" coords="35,112.34,409.15,14.61,8.74" target="#b11">[12]</ref>. We have not to date attempted to quantify whether our levels of disagreement are more or less than the norm. Note that none of these double-assessed documents were used in [the 2008] evaluation (as residual evaluation excludes previously judged documents)."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Additional Data from the 2007 Assessor Consistency Study</head><p>An analogous assessor consistency study was also conducted in the Relevance Feedback task of the TREC 2007 Legal Track, as per the oldrel07 and oldnon07 runs described in the 2007 track overview paper <ref type="bibr" coords="35,521.73,491.29,14.61,8.74" target="#b14">[15]</ref>. Although fewer topics were assessed (10 topics instead of 12), the random samples were larger (25 relevant and 25 non-relevant documents per topic instead of just 10 each).</p><p>Unfortunately, the 2007 track overview paper only reported the results for 3 of the 10 topics (in Table <ref type="table" coords="35,535.02,527.16,4.98,8.74" target="#tab_3">3</ref> of <ref type="bibr" coords="35,83.98,539.11,15.50,8.74" target="#b14">[15]</ref> as part of the Interactive task reporting that year) and it did not remark on the results. Here we report the data for all 10 topics in Table <ref type="table" coords="35,256.52,551.07,8.49,8.74" target="#tab_18">14</ref>. We find that 64% of the documents judged relevant in 2006 were again judged relevant in 2007 (from (157/(250-5))), and just 10% of the documents judged non-relevant were judged relevant in 2007. These numbers (64% and 10%) are similar to the corresponding numbers from the aforementioned 2008 study (62% and 18% respectively). Note that there were differences in the details of the studies of the two years; for example, in 2008, the assessors distinguished 'highly relevant' and 'other relevant' judgments, whereas in 2007, only the 'relevant' category was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">2009 Assessor Consistency Study</head><p>More assessor consistency data is available from the 2009 Batch Task described earlier in this paper. Instead of a fixed size random sample of past judged relevant and non-relevant documents, the 2009 results are just based on the overlap in documents selected for judging. The 2009 results include not just an oldrel09 and oldnon09 run, but also oldHrel09 and oldOrel09 runs for some topics. More details are in Section 3.10.8 of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our work with construction of a test collection based on nearly 7 million scanned documents is now complete, with relevance judgments for a total of 109 richly structured topics. Our efforts to construct a new test collection based on email have, however, only just begin. We now have relevance judgments for seven richly structured topics that can be used to support development of future systems, and we plan to judge relevance for additional topics in 2010. As noted above (Section 2.2.1), our plan for 2010 has been to develop, in collaboration with the EDRM Data Set Project [1] and ZL Technologies, a new version of the Enron collection that would be free of the known issues with the 2009 collection. We have, at the time of the writing of this overview, followed through on that plan, and can report that participants in the 2010 Legal Track are already working with the new collection.</p><p>The general design of the 2010 Legal Track has emerged from our discussions at the TREC conference in 2009. In order to make the best use of available assessment resources, we expect to focus exclusively on the email collection, using it as a basis for both the Interactive task and, as a successor to the Batch task, a newly designed Learning task. We plan to create new topics for the Interactive task. We continue to be interested in issues of evaluation measure design and test collection reusability, and we welcome suggestions for task designs that would facilitate experiments addressing those issues, and others that are of interest to track participants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,225.79,372.25,160.42,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Team-TA interaction time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="18,201.34,566.02,209.33,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interactive runs-recall and precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="19,210.46,433.67,191.09,9.65"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interactive runs-F 1 vs. TA-time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="20,175.74,323.88,260.52,8.74"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interactive runs-rate of appeal vs. improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,117.98,94.92,369.91,577.61"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table coords="10,348.65,94.92,134.78,7.01"><row><cell>Proportion of</cell><cell>Proportion of</cell></row></table><note coords="10,263.65,663.79,124.26,8.74"><p>Submissions (message-level).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,192.50,428.33,227.01,8.74"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note coords="12,232.06,428.33,187.44,8.74"><p>Stratification &amp; sampling-high-level view.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,78.84,134.18,454.31,378.06"><head>Table 4 :</head><label>4</label><figDesc>Appeal &amp; adjudication data.</figDesc><table coords="14,412.32,134.18,108.68,7.01"><row><cell>Conflicts</cell><cell>Appeals</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="16,248.59,318.91,114.82,8.74"><head>Table 5 :</head><label>5</label><figDesc>Estimated yields.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="17,165.31,444.47,281.39,9.65"><head>Table 6 :</head><label>6</label><figDesc>Post-adjudication estimates of recall, precision, and F 1 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="26,86.94,390.88,282.38,28.66"><head>Table 7 :</head><label>7</label><figDesc>Mean Set-based Scores using All Relevant documents •</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="26,86.94,410.80,320.02,28.66"><head>Table 8 :</head><label>8</label><figDesc>Mean Set-based Scores using only Highly Relevant documents •</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="26,86.94,430.73,291.66,28.66"><head>Table 9 :</head><label>9</label><figDesc>Mean Rank-based Scores using All Relevant documents •</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="26,96.91,450.65,324.31,8.74"><head>Table 10 :</head><label>10</label><figDesc>Mean Rank-based Scores using only Highly Relevant documents</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="26,472.15,678.70,67.85,8.74"><head>Table 7 :</head><label>7</label><figDesc>Table 26 of [4]). Mean set-based measures using all relevant documents (avg. 314,526.8 est. relevant documents)</figDesc><table coords="27,72.00,116.41,468.00,524.72"><row><cell>Run</cell><cell cols="2">Avg. K Recall Precision</cell><cell>F 1</cell><cell cols="3">Gray Fallout LAM</cell><cell>Avg. Num. Judged</cell></row><row><cell>watstack</cell><cell>241,550 0.192</cell><cell>0.402</cell><cell cols="2">0.214 0.014</cell><cell>0.022</cell><cell>0.169</cell><cell>(333r, 150n, 1g)</cell></row><row><cell>watlogistic</cell><cell>318,520 0.198</cell><cell>0.406</cell><cell>0.207</cell><cell>0.009</cell><cell>0.033</cell><cell>0.192</cell><cell>(333r, 179n, 2g)</cell></row><row><cell>otL09F</cell><cell>198,939 0.167</cell><cell>0.398</cell><cell>0.196</cell><cell>0.005</cell><cell>0.019</cell><cell>0.179</cell><cell>(329r, 142n, 1g)</cell></row><row><cell>otL09frwF</cell><cell>270,389 0.182</cell><cell>0.377</cell><cell>0.189</cell><cell>0.025</cell><cell>0.028</cell><cell>0.176</cell><cell>(359r, 248n, 3g)</cell></row><row><cell>watrrf</cell><cell>233,550 0.162</cell><cell>0.378</cell><cell cols="2">0.177 0.027</cell><cell>0.022</cell><cell>0.183</cell><cell>(320r, 141n, 2g)</cell></row><row><cell>uclsi</cell><cell>960,000 0.354</cell><cell>0.131</cell><cell>0.162</cell><cell>0.007</cell><cell>0.129</cell><cell>0.369</cell><cell>(346r, 524n, 6g)</cell></row><row><cell>otL09rvl</cell><cell>192,605 0.165</cell><cell>0.267</cell><cell>0.162</cell><cell>0.006</cell><cell>0.021</cell><cell>0.256</cell><cell>(344r, 284n, 5g)</cell></row><row><cell>ucedlsi</cell><cell>960,000 0.374</cell><cell>0.133</cell><cell>0.162</cell><cell>0.007</cell><cell>0.127</cell><cell>0.347</cell><cell>(358r, 552n, 7g)</cell></row><row><cell>ucscra</cell><cell>960,000 0.309</cell><cell>0.117</cell><cell>0.144</cell><cell>0.008</cell><cell>0.128</cell><cell>0.378</cell><cell>(319r, 484n, 6g)</cell></row><row><cell>fullset09</cell><cell>6,910,192 1.000</cell><cell>0.046</cell><cell>0.084</cell><cell>0.010</cell><cell>1.000</cell><cell cols="2">0.843 2275 (446r, 1806n, 23g)</cell></row><row><cell>EmcRun1</cell><cell>27,462 0.041</cell><cell>0.353</cell><cell>0.068</cell><cell>0.015</cell><cell>0.002</cell><cell>0.211</cell><cell>269 (179r, 88n, 2g)</cell></row><row><cell>refL09B</cell><cell>27,462 0.037</cell><cell>0.391</cell><cell>0.063</cell><cell>0.016</cell><cell>0.002</cell><cell>0.196</cell><cell>277 (182r, 92n, 3g)</cell></row><row><cell>oldrel09</cell><cell>643 0.002</cell><cell>0.781</cell><cell>0.004</cell><cell>0.001</cell><cell cols="2">0.000 0.095</cell><cell>74 (65r, 9n, 0g)</cell></row><row><cell>oldnon09</cell><cell>1,398 0.001</cell><cell>0.133</cell><cell>0.002</cell><cell>0.016</cell><cell>0.000</cell><cell>0.412</cell><cell>102 (28r, 73n, 2g)</cell></row><row><cell>Run</cell><cell cols="2">Avg. K h Recall Precision</cell><cell>F 1</cell><cell cols="3">Gray Fallout LAM</cell><cell>Avg. Num. Judged</cell></row><row><cell>watlogistic</cell><cell>31,852 0.178</cell><cell>0.326</cell><cell cols="2">0.190 0.000</cell><cell>0.004</cell><cell>0.125</cell><cell>(127r, 146n, 0g)</cell></row><row><cell>watstack</cell><cell>24,155 0.158</cell><cell>0.371</cell><cell>0.180</cell><cell>0.000</cell><cell>0.003</cell><cell>0.098</cell><cell>(114r, 127n, 0g)</cell></row><row><cell>watrrf</cell><cell>23,355 0.146</cell><cell>0.353</cell><cell>0.163</cell><cell>0.000</cell><cell>0.002</cell><cell>0.117</cell><cell>(108r, 127n, 0g)</cell></row><row><cell>otL09F</cell><cell>26,582 0.167</cell><cell>0.266</cell><cell>0.132</cell><cell>0.000</cell><cell>0.003</cell><cell>0.108</cell><cell>(115r, 145n, 0g)</cell></row><row><cell>otL09frwF</cell><cell>26,582 0.168</cell><cell>0.215</cell><cell>0.113</cell><cell>0.016</cell><cell>0.003</cell><cell>0.125</cell><cell>(134r, 248n, 1g)</cell></row><row><cell>otL09rvl</cell><cell>61,608 0.239</cell><cell>0.119</cell><cell>0.105</cell><cell>0.007</cell><cell>0.008</cell><cell>0.165</cell><cell>(141r, 321n, 3g)</cell></row><row><cell>oldrel09</cell><cell>643 0.065</cell><cell>0.401</cell><cell>0.079</cell><cell>0.001</cell><cell>0.000</cell><cell>0.145</cell><cell>74 (44r, 30n, 0g)</cell></row><row><cell>EmcRun1</cell><cell>27,462 0.128</cell><cell>0.109</cell><cell>0.071</cell><cell>0.016</cell><cell>0.004</cell><cell>0.171</cell><cell>269 (84r, 183n, 2g)</cell></row><row><cell>refL09B</cell><cell>27,462 0.143</cell><cell>0.149</cell><cell>0.063</cell><cell>0.016</cell><cell>0.004</cell><cell>0.169</cell><cell>277 (84r, 190n, 3g)</cell></row><row><cell>fullset09</cell><cell>6,910,192 1.000</cell><cell>0.008</cell><cell>0.016</cell><cell>0.010</cell><cell>1.000</cell><cell cols="2">0.934 2275 (175r, 2077n, 23g)</cell></row><row><cell>oldnon09</cell><cell>1,398 0.017</cell><cell>0.028</cell><cell cols="2">0.006 0.016</cell><cell>0.000</cell><cell>0.356</cell><cell>102 (6r, 95n, 2g)</cell></row><row><cell>ucscra</cell><cell>580 0.002</cell><cell>0.120</cell><cell>0.004</cell><cell>0.003</cell><cell>0.000</cell><cell>0.239</cell><cell>65 (15r, 49n, 1g)</cell></row><row><cell>ucedlsi</cell><cell>580 0.001</cell><cell>0.083</cell><cell>0.002</cell><cell>0.014</cell><cell>0.000</cell><cell>0.291</cell><cell>67 (12r, 54n, 1g)</cell></row><row><cell>uclsi</cell><cell>580 0.001</cell><cell>0.046</cell><cell>0.001</cell><cell>0.006</cell><cell>0.000</cell><cell>0.351</cell><cell>53 (4r, 48n, 1g)</cell></row><row><cell cols="8">Table 8: Mean set-based measures using only highly relevant documents (avg. 55,146.8 est. highly relevant</cell></row><row><cell>documents)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="28,73.36,288.76,465.29,293.39"><head>Table 9 :</head><label>9</label><figDesc>Mean rank-based measures using all relevant documents (avg. 314,526.8 Est. relevant documents)</figDesc><table coords="28,93.63,447.09,424.74,135.06"><row><cell>Run</cell><cell>Avg. Ret P@B</cell><cell cols="5">R@B F 1 @R h R@ret indAP GS10J</cell><cell>S1J</cell><cell>Fields</cell></row><row><cell>watstack</cell><cell cols="3">1,500,000 0.285 0.251 0.248</cell><cell>0.780</cell><cell>0.441</cell><cell cols="3">0.868 6/10 F</cell></row><row><cell>watrrf</cell><cell>1,500,000 0.272</cell><cell>0.237</cell><cell>0.246</cell><cell>0.782</cell><cell>0.439</cell><cell cols="3">0.846 6/10 F</cell></row><row><cell cols="2">watlogistic 1,500,000 0.262</cell><cell>0.225</cell><cell>0.240</cell><cell>0.693</cell><cell cols="4">0.454 0.882 6/10 F</cell></row><row><cell>otL09F</cell><cell>1,500,000 0.256</cell><cell>0.228</cell><cell>0.213</cell><cell>0.757</cell><cell>0.409</cell><cell>0.815</cell><cell cols="2">2/10 mMF</cell></row><row><cell cols="2">otL09frwF 1,500,000 0.221</cell><cell>0.230</cell><cell>0.193</cell><cell>0.719</cell><cell>0.366</cell><cell>0.868</cell><cell cols="2">5/10 brmBMF</cell></row><row><cell>otL09rvl</cell><cell>1,500,000 0.239</cell><cell>0.174</cell><cell>0.164</cell><cell>0.688</cell><cell>0.338</cell><cell>0.779</cell><cell cols="2">5/10 rmM</cell></row><row><cell>EmcRun1</cell><cell>212,795 0.109</cell><cell>0.128</cell><cell>0.098</cell><cell>0.293</cell><cell>0.197</cell><cell>0.791</cell><cell cols="2">4/10 bCrmM</cell></row><row><cell>ucscra</cell><cell>960,000 0.054</cell><cell>0.055</cell><cell>0.061</cell><cell>0.506</cell><cell>0.171</cell><cell>0.477</cell><cell cols="2">2/10 rMF</cell></row><row><cell>ucedlsi</cell><cell>960,000 0.077</cell><cell>0.056</cell><cell>0.060</cell><cell>0.575</cell><cell>0.174</cell><cell>0.253</cell><cell cols="2">0/10 rMF</cell></row><row><cell>uclsi</cell><cell>960,000 0.044</cell><cell>0.043</cell><cell>0.043</cell><cell>0.554</cell><cell>0.149</cell><cell>0.183</cell><cell cols="2">0/10 rMF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="28,72.00,595.83,468.00,20.69"><head>Table 10 :</head><label>10</label><figDesc>Mean rank-based measures using only highly relevant documents (avg. 55,146.8 est. highly relevant documents) So the picture overall looked similar if just counting highly relevant documents. Below, we just focus on the results counting "all relevant" documents, particularly as half of the topics did not actually have training examples distinguishing highly relevant from other relevant documents.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="32,72.00,641.91,468.00,40.03"><head>Table 12 :</head><label>12</label><figDesc>3.10.11  Did the LAM measure correlate with the F 1 measure?LAM is Logistic Average Misclassification, which was the main set-based measure of the TREC Spam Track<ref type="bibr" coords="32,100.36,672.28,14.61,8.74" target="#b9">[10]</ref>. (Because LAM is an error rate, a lower LAM score is preferred, in contrast to F 1 , for which a Comparison of the training and test judgment results for the reference Boolean query</figDesc><table coords="33,89.56,76.95,425.47,8.74"><row><cell>Topic</cell><cell>B</cell><cell>Est. Rel@B</cell><cell>Est. Non@B</cell><cell>Precision</cell><cell>Total Est. Rel</cell><cell>Recall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="34,72.00,275.41,468.00,20.69"><head>Table 13 :</head><label>13</label><figDesc>Consistency of previous and new judgments for the 12 topics of the TREC 2008 Legal Track Relevance Feedback task (tot=total, hrel=highly relevant, orel=other relevant, non=non-relevant, gr=gray).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="35,72.00,95.28,468.00,172.67"><head>Table 14 :</head><label>14</label><figDesc>Consistency of previous and new Judgments for the 10 topics of the TREC 2007 Legal Track Relevance Feedback task (tot=total, rel=relevant, non=non-relevant, gr=gray).</figDesc><table coords="35,93.46,95.28,409.72,135.86"><row><cell>7 (2006-A-2)</cell><cell>tot=25, rel=6, non=18, gr=1</cell><cell>tot=25, rel=0, non=25, gr=0</cell></row><row><cell>8 (2006-A-3)</cell><cell>tot=25, rel=19, non=6, gr=0</cell><cell>tot=25, rel=8, non=17, gr=0</cell></row><row><cell>13 (2006-A-9)</cell><cell>tot=25, rel=9, non=13, gr=3</cell><cell>tot=25, rel=0, non=25, gr=0</cell></row><row><cell>26 (2006-C-2)</cell><cell>tot=25, rel=14, non=11, gr=0</cell><cell>tot=25, rel=5, non=18, gr=2</cell></row><row><cell>27 (2006-C-3)</cell><cell>tot=25, rel=22, non=3, gr=0</cell><cell>tot=25, rel=0, non=25, gr=0</cell></row><row><cell>30 (2006-C-6)</cell><cell>tot=25, rel=17, non=8, gr=0</cell><cell>tot=25, rel=0, non=25, gr=0</cell></row><row><cell>34 (2006-D-1)</cell><cell>tot=25, rel=23, non=2, gr=0</cell><cell>tot=25, rel=4, non=21, gr=0</cell></row><row><cell>37 (2006-D-4)</cell><cell>tot=25, rel=19, non=6, gr=0</cell><cell>tot=25, rel=6, non=18, gr=1</cell></row><row><cell>45 (2006-E-4)</cell><cell>tot=25, rel=12, non=13, gr=0</cell><cell>tot=25, rel=0, non=23, gr=2</cell></row><row><cell>51 (2006-E-10)</cell><cell>tot=25, rel=16, non=8, gr=1</cell><cell>tot=25, rel=1, non=23, gr=1</cell></row><row><cell>Totals</cell><cell>tot=250, rel=157, non=88, gr=5</cell><cell>tot=250, rel=24, non=220, gr=6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,87.24,654.35,452.76,6.99;8,72.00,663.82,468.00,6.99;8,72.00,673.28,468.00,6.99;8,72.00,682.75,369.05,6.99"><p>Individuals from the following law schools, academic and non-academic institutions participated in the review: Capital U., Indiana U. Purdue U. Indianapolis, Loyola Law School Los Angeles, Loyola University New Orleans, Northern Kentucky U., Rutgers (SCILS) &amp; Information, U. of Missouri, U. of North Carolina, Equivalent Data, Faegre &amp; Benson, Greensfelder, Hemker &amp; Gale, Hunton &amp; Williams, Inventus, Law Offices of Russell Goodrow, and Mayer Brown.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This track would not have been possible without the efforts of a great many people. Our heartfelt thanks again go to <rs type="person">Ian Soboroff</rs> for creating and maintaining the relevance assessment system used by both the Batch and Interactive tasks this year for different document collections; to <rs type="person">Venkat Rangan</rs> for obtaining the Enron emails and converting them into more easily used forms; to the dedicated group of pro bono relevance assessors and the pro bono coordinators at participating law schools; to <rs type="person">Macyl Burke</rs> (of <rs type="affiliation">ACT Litigation Services</rs>) and <rs type="person">Brandon M. Mack</rs> (of <rs type="affiliation">BIA</rs>) for coordinating their firms' contribution to the assessment process; to <rs type="person">Tom Greiff</rs> for coordinating the contributions of the "Jerusalem team" of assessors; to <rs type="person">Christopher Boehning</rs>, <rs type="person">Ross Gotler</rs> and <rs type="person">Charlene Jones</rs>, from the law firm of Paul, Weiss, Rifkind, <rs type="person">Wharton &amp; Garrison</rs>, and to <rs type="person">Maura Grossman</rs>, from <rs type="funder">Wachtell, Lipton</rs>, <rs type="person">Rosen &amp; Katz</rs>, for their invaluable assistance in complaint drafting and topic formulation for this year's Interactive task; to our dedicated topic authorities for this year's Interactive task, including <rs type="person">Art Bieser</rs> (<rs type="affiliation">Hunton &amp; Williams</rs>), <rs type="person">Christopher Boehning</rs>, <rs type="person">Michael Roman Geske</rs> (<rs type="affiliation">Aphelion Legal Solutions</rs>), <rs type="person">Maura Grossman</rs>, <rs type="person">Howard Nicols</rs> (<rs type="affiliation">Squire, Sanders, &amp; Dempsey</rs>), <rs type="person">David Stanton</rs> (<rs type="affiliation">Pillsbury Winthrop Shaw Pittman</rs>), and <rs type="person">K. Krasnow Waterman</rs> (LawTechIntersect); and finally, to <rs type="person">Richard Braman</rs>, Executive Director of The Sedona Conference R , for his continued support of the <rs type="funder">TREC Legal Track</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Sampling &amp; Assessment Tables-Interactive Task</head><p>In this appendix, we present tables that summarize, for each of the seven Interactive topics, the results of the sampling and assessment process followed in the the 2009 exercise. Each table shows: (i) total messages in each stratum (in the full collection); (ii) total messages sampled from each stratum; (iii) total sampled messages observed to be assessable; (iv) total sampled messages observed to be assessable and relevant (preadjudication); and (v) total sampled messages observed to be assessable and relevant (post-adjudication).</p><p>It is on the basis of the data contained in these tables that we arrived at the estimates of the message-level recall, precision, and F 1 attained in each run. Each table is structured as follows. The leftmost columns represent the relevance values (R = Relevant; N = Not Relevant) from the participant submissions that define each stratum. The right-hand columns show the counts of messages in each stratum; more specifically, the columns show the following data: N = total messages in the stratum; n = total messages sampled from the stratum; a = total sampled messages observed to be assessable; r 1 = total sampled messages observed to be assessable and relevant (pre-adjudication); r 2 = total sampled messages observed to be assessable and relevant (post-adjudication).</p><p>The tables for the seven Interactive topics follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stratum</head><p>Counts (Messages)        </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="33,91.36,216.43,431.08,8.74;40,72.00,72.12,75.52,12.62;40,76.61,95.68,310.92,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="40,72.00,72.12,75.52,12.62;40,76.61,95.68,113.23,7.86">References [1] EDRM Data Set Project</title>
		<author>
			<persName coords=""><surname>Avg</surname></persName>
		</author>
		<ptr target="http://edrm.net/projects/dataset" />
		<imprint>
			<date type="published" when="1940">27462 14275.0, 15295.3 10679.2, 12047.4 0.33, 0.39 159949.1, 314526.8 0.17, 0.04</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.31,110.62,279.38,7.86;40,91.32,121.58,314.68,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="40,142.26,110.62,224.38,7.86">Legal Track-Interactive Task Topic-Specific Guidelines</title>
		<author>
			<persName coords=""><surname>Trec-</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/data/legal/08/LegalInteractiveTopicGuidelines2008.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.31,136.52,185.87,7.86;40,91.32,147.48,303.39,7.86" xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Trec-</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/LT09ComplaintJfinal.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="40,142.26,136.52,107.21,7.86">Legal Track -Complaint J</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.31,162.43,448.69,7.86;40,91.32,173.38,264.00,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="40,265.42,162.43,135.15,7.86">Batch Task (Appendix Document)</title>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="40,164.15,162.43,94.56,7.86;40,418.97,162.43,121.03,7.86;40,91.32,173.38,175.00,7.86">The Eighteenth Text REtrieval Conference</title>
		<imprint>
			<publisher>Per-Topic Scores</publisher>
		</imprint>
	</monogr>
	<note>TREC 2009) Proceedings, 2010</note>
</biblStruct>

<biblStruct coords="40,91.31,188.33,448.69,7.86;40,91.32,199.29,305.53,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="40,219.95,188.33,213.81,7.86">Legal Track, Interactive Task (Appendix Document)</title>
		<idno>Per-Topic Scores: TREC 2009</idno>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="40,457.09,188.33,82.91,7.86;40,91.32,199.29,216.54,7.86">The Eighteenth Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2009) Proceedings, 2010</note>
</biblStruct>

<biblStruct coords="40,91.31,214.23,448.69,7.86;40,91.32,225.19,264.00,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="40,252.91,214.23,139.31,7.86">Batch Task (Appendix Document)</title>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="40,146.09,214.23,98.72,7.86;40,415.15,214.23,124.85,7.86;40,91.32,225.19,175.00,7.86">The Eighteenth Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2009) Proceedings, 2010</note>
</biblStruct>

<biblStruct coords="40,91.31,240.13,448.69,7.86;40,91.32,251.09,412.25,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="40,390.09,240.13,149.91,7.86;40,91.32,251.09,67.98,7.86">Interactive Task Guidelines -TREC-2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/2008InteractiveGuidelines.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.31,266.04,448.69,7.86;40,91.32,277.00,370.80,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="40,390.09,266.04,149.91,7.86;40,91.32,277.00,67.98,7.86">Interactive Task Guidelines -TREC-2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/LT09ITGfinal.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.31,291.94,448.69,7.86;40,91.32,302.90,295.27,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="40,326.20,291.94,138.02,7.86">TREC-2006 Legal Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,485.30,291.94,54.70,7.86;40,91.32,302.90,213.20,7.86">The Fifteenth Text REtrieval Conference Proceedings (TREC 2006)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,317.84,448.68,7.86;40,91.32,328.80,448.68,7.86;40,91.32,339.76,346.27,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="40,285.53,317.84,135.87,7.86">TREC 2005 spam track overview</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,197.84,328.80,342.16,7.86;40,91.32,339.76,29.12,7.86">The Fourteenth Text Retrieval Conference (TREC 2005), volume Special Publication 500-266</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="91" to="108" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,354.70,448.68,7.86;40,91.32,365.66,448.68,7.86;40,91.32,376.62,288.13,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Maura</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Art</forename><surname>Bieser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">H</forename><surname>Boehning</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Michael Roman Geske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Nicols</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Krasnow</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Waterman</surname></persName>
		</author>
		<ptr target="http://trec-legal.umiacs.umd.edu/" />
		<title level="m" coord="40,248.68,365.66,291.32,7.86;40,91.32,376.62,63.35,7.86">Reflections of the Topic Authorities about the 2009 TREC Legal Track Interactive Task</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,391.57,448.68,7.86;40,91.32,402.53,291.97,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="40,172.64,391.57,111.24,7.86">The TREC Test Collections</title>
		<author>
			<persName coords=""><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,511.05,391.57,28.95,7.86;40,91.32,402.53,210.26,7.86">TREC: Experiment and Evaluation in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="21" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,417.47,448.68,7.86;40,91.32,428.43,302.41,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="40,398.56,417.47,141.44,7.86;40,91.32,428.43,21.14,7.86">Overview of the TREC 2008 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,132.28,428.43,175.75,7.86">The Seventeenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,443.37,448.68,7.86;40,91.32,454.33,355.47,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="40,242.76,443.37,137.58,7.86">Building digital tobacco document</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Butter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California, San Francisco Library/Center for Knowledge Management. D-Lib Magazine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,469.28,448.68,7.86;40,91.32,480.23,338.29,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="40,402.50,469.28,137.50,7.86;40,91.32,480.23,21.14,7.86">Overview of the TREC 2007 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,132.28,480.23,269.22,7.86">The Sixteenth Text Retrieval Conference (TREC 2007) Proceedings</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="40,91.32,495.18,448.68,7.86;40,91.32,506.14,448.68,7.86;40,91.32,517.10,406.19,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="40,352.14,495.18,187.87,7.86;40,91.32,506.14,259.47,7.86">Comparing Exclusionary and Investigative Approaches for Electronic Discovery using the TREC Enron Corpus</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cameron</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sofia</forename><surname>Andrianakou</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec18/papers/zlti.legal.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="40,370.21,506.14,169.79,7.86;40,97.29,517.10,178.31,7.86">TREC 2009) Proceedings, 2010. Available at</title>
		<imprint/>
	</monogr>
	<note>The Eighteenth Text REtrieval Conference</note>
</biblStruct>

<biblStruct coords="40,91.32,532.04,448.68,7.86;40,91.32,543.00,448.68,7.86;40,91.32,553.96,66.32,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="40,372.69,532.04,150.02,7.86">Assessor error in stratified evaluation</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="40,91.32,543.00,443.60,7.86">Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
