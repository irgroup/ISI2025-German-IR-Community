<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.40,114.63,277.32,12.36">Overview of the TREC 2009 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.32,148.42,105.77,10.97"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.60,148.42,69.97,10.97;1,308.64,162.46,48.12,10.97"><forename type="first">Nick</forename><forename type="middle">Craswell</forename><surname>Microsoft</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ian Soboroff NIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.40,114.63,277.32,12.36">Overview of the TREC 2009 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B681FAE2A921C4040B8A4AC99C5DADBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Overview</head><p>The TREC Web Track explores and evaluates Web retrieval technologies. Currently, the Web Track conducts experiments using the new billion-page ClueWeb09 collection <ref type="foot" coords="1,409.08,259.78,4.23,7.29" target="#foot_0">1</ref> . The TREC 2009 track is the successor to the Terabyte Retrieval Track, which ran from 2004 to 2006, and to the older Web Track, which ran from 1999 to 2003. The TREC 2009 Web Track includes both a traditional adhoc retrieval task and a new diversity task. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For example, given the query "windows", a system might return the Windows update page first, followed by the Microsoft home page, and then a news article discussing the release of Windows 7. Mixed in these results might be pages providing product information on doors and windows for homes and businesses.</p><p>The track used the new ClueWeb09 dataset as its document collection. The full collection consists of roughly 1 billion web pages, comprising approximately 25TB of uncompressed data (5TB compressed) in multiple languages. The dataset was crawled from the Web during January and February 2009. For groups who were unable to work with this full "Category A" dataset, the track accepted runs over the smaller ClueWeb09 "Category B" dataset, a subset of about 50 million English-language pages.</p><p>Topics for the track were created from the logs of a commercial search engine, with the aid of tools developed at Microsoft Research. Given a target query, these tools extracted and analyzed groups of related queries, using co-clicks and other information, to identify clusters of queries that highlight different aspects and interpretations of the target query.</p><p>These clusters were employed by NIST for topic development. Each resulting topic is structured as a representative set of subtopics, each related to a different user need. Documents were judged with respect to the subtopics, as well as with respect to the topic as a whole. For each subtopic, NIST assessors made a binary judgment as to whether or not the document satisfies the information need associated with the subtopic.</p><p>These topics were used for both the adhoc task and the diversity task. For both tasks, participants executed the original target queries over the ClueWeb09 collection. The tasks differ primarily in their evaluation measures. The adhoc task uses an estimate of mean average precision, based on overall topical relevance <ref type="bibr" coords="1,207.25,626.74,10.83,10.91" target="#b2">[3]</ref>. The diversity task uses newer measures, based on the subtopics, which explicitly consider novelty in the result list (intent aware precision <ref type="bibr" coords="1,423.24,640.30,11.56,10.91" target="#b0">[1]</ref> and α-nDCG <ref type="bibr" coords="1,504.72,640.30,11.26,10.91" target="#b3">[4]</ref>). A total of 26 groups submitted runs to the track, with many groups participating in both tasks. Table <ref type="table" coords="2,103.20,197.02,5.45,10.91" target="#tab_0">1</ref> summarizes the participation of these groups. About half the groups worked with the full collection. A few groups submitted runs over both the full (Category A) collection and the Category B collection. This report provides an overview of the track, including topic development, evaluation measures, and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Groups submitting Groups submitting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topics</head><p>NIST created and assessed new 50 topics for the task. Two example topics are provided in Figure <ref type="figure" coords="2,531.51,296.38,4.22,10.91" target="#fig_0">1</ref>. Unlike most TREC tasks, NIST initially did not release the full topics to the participants. Instead, the initial release of the topics consisted only of the query fields from the 50 topics. No other information regarding the topics was provided. While the topics were fully defined by NIST in advance of the initial release, the detailed topics were released to the participants only after all runs had been submitted.</p><p>For the purposes of the diversity task, each topic is structured into a representative set of subtopics, related to different user needs. As shown in Figure <ref type="figure" coords="2,372.24,391.30,4.22,10.91" target="#fig_0">1</ref>, topics were categorized as either "ambiguous" or "faceted". Ambiguous queries are those that have multiple distinct interpretations. We assume that a user interested in one interpretation would not be interested in the others. On the other hand, facets reflect underspecified queries, with different aspects covered by the subtopics. We assume that a user interested in one aspect may still be interested in others.</p><p>In turn, each subtopic was categorized as being either navigational ("nav") or informational ("inf"). A navigational subtopic usually has only a small number of relevant pages (often one). For these subtopics, we assume the user is seeking a page with a specific URL, such as an organization's homepage. On the other hand, an informational query may have a large number of relevant pages. For these subtopics, we assume the user is seeking information without regard to its source, provided that the source is reliable.</p><p>The subtopics are based on information extracted from the logs of a commercial search engine, and are roughly balanced in terms of popularity. When selecting the subtopics, strange and unusual interpretations and aspects were avoided as much as possible. The set of subtopics is intended to be representative, not exhaustive, with the number of subtopics per topic ranging from three to eight, with a mean of 4.9.</p><p>For the diversity task, documents were judged with respect to the subtopics. For each subtopic, NIST assessors made a binary judgment as to whether or not the document satisfies the information need associated with the subtopic. For the adhoc task, relevance is primarily judged on the basis of the description field, but if a document is relevant to any subtopic then it is usually relevant to the overall topic. However, for the diversity task, a document may not be relevant to any subtopic, even if it is relevant to the overall topic.</p><p>&lt;topic number="19" type="ambiguous"&gt; &lt;query&gt;the current&lt;/query&gt; &lt;description&gt; I'm looking for the homepage of The Current, a program on Minnesota Public Radio. &lt;/description&gt; &lt;subtopic number="1" type="nav"&gt; Take me to the homepage of The Current, a program on Minnesota Public Radio. &lt;/subtopic&gt; &lt;subtopic number="2" type="nav"&gt; I'm looking for the homepage of The Current newspaper in New Jersey. &lt;/subtopic&gt; &lt;subtopic number="3" type="nav"&gt; I want to find the homepage of The Current newspaper in Hartford. &lt;/subtopic&gt; &lt;subtopic number="4" type="nav"&gt; I want to find the homepage of The Current magazine in San Antonio. &lt;/subtopic&gt; &lt;/topic&gt; &lt;topic number="21" type="faceted"&gt; &lt;query&gt;volvo&lt;/query&gt; &lt;description&gt; I'm looking for information on Volvo cars and trucks. &lt;/description&gt; &lt;subtopic number="1" type="nav"&gt; I'm looking for Volvo's homepage. &lt;/subtopic&gt; &lt;subtopic number="2" type="inf"&gt; Find reviews of the Volvo XC90 SUV. &lt;/subtopic&gt; &lt;subtopic number="3" type="inf"&gt; Where can I find Volvo semi trucks for sale (new or used)? &lt;/subtopic&gt; &lt;subtopic number="4" type="inf"&gt; Find a Volvo dealer. &lt;/subtopic&gt; &lt;subtopic number="5" type="inf"&gt; Find an online source for Volvo parts. &lt;/subtopic&gt; &lt;/topic&gt;    All topics are expressed in English. For the TREC 2009 track, all non-English documents were judged non-relevant, even if the assessor understood the language of the document and the document would be relevant in that language. In order to reduce the influence of Web spam, assessors were instructed that pages with misleading and/or malicious content should be considered "not relevant".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topic Development Procedure</head><p>Topic development this year was guided by real search engine usage in two ways: 1) by queries sampled from search logs, and 2) through subtopic development informed by the output of a clustering algorithm. This section describes this sampling and clustering.</p><p>To guarantee having 50 queries that were useable by NIST, we sampled 200 queries from the usage logs of a commercial search engine. The 200 were sampled from a January 2009 query log, which is roughly the same time that ClueWeb documents were crawled. The sampling distribution was to prefer queries of medium popularity, on the assumption that very popular queries are too navigational, therefore less challenging, and very rare queries may contain personally identifiable information, so should not be used in experiments. This gave 200 of what we call torso queries, such as "the current" and "volvo", avoiding very head queries such as "ebay" and very tail queries such as "nick craswell phone number". We also implemented an Adult Filter, so the 200 queries were unlikely to be seeking adult content.</p><p>A clustering algorithm <ref type="bibr" coords="4,201.38,578.02,11.56,10.91" target="#b4">[5]</ref> was run on each of the 200 queries. Using the query as a seed, the algorithm first finds other queries that are likely to occur in the same session, using a large amount of aggregated session data. For example, the seed query "the current" is expanded to "current newspaper" and "the current radio" amongst others. Then the same expansion step is run again, yielding a much wider set of queries, some of which are on-topic (e.g. "current newspaper nj") and some of which stray too far off-topic (e.g. "prairie home companion").</p><p>The algorithm then induces a graph, where each node is a query from the expanded set, and edges connect two queries if they have clicks on the same URL. This co-click information may mean that there is no longer a path from the seed query to some off-topic queries (no path from "the current" to "prairie home companion"). Disconnected nodes were removed, to reduce noise.</p><p>Finally an agglomerative clustering algorithm is run on the graph. The resulting clusters were provided to NIST for use in topic development, in the form of unigram language models. The clusters may contain omissions, and may contain duplicate clusters, but can be used to ensure that subtopics have good coverage of interpretations that are "nearby in the space of user clicks and reformulations. The eight clusters in Figure <ref type="figure" coords="5,293.43,156.10,5.45,10.91" target="#fig_2">2</ref> were used to develop the subtopics for topic 19, appearing in Figure <ref type="figure" coords="5,169.83,169.66,4.22,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adhoc Task</head><p>An adhoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. The goal of an adhoc task is to return a ranking of the documents in the collection in order of decreasing probability of relevance, where the probability of relevance of a document is considered independently of other documents that appear before it in the result list. For each topic, participants submitted a ranking of the top 1,000 documents.</p><p>The process of executing the queries over the documents and generating the experimental runs was required to be entirely automatic, with no human intervention at any stage, including modifications to retrieval systems motivated by an inspection of the queries. Group were instructed not to materially modify their retrieval system between the time they downloaded the queries and the time they submitted their runs.</p><p>Each group could submit up to three runs for the adhoc task. All runs were judged by NIST assessors. Each document was judged on a four-point scale as being "highly relevant", "relevant", "not relevant", or "not relevant, but reasonable". These relevance judgments were made with respect to the description field of the topic associated with the query. The "not relevant, but reasonable" level means that the document is not relevant with respect to the description field, but may be relevant to some other reasonable interpretation of the query.</p><p>Results for the adhoc task are shown in Figures <ref type="figure" coords="5,319.46,445.18,5.45,10.91" target="#fig_2">2</ref> and<ref type="figure" coords="5,349.35,445.18,4.22,10.91" target="#fig_4">3</ref>. As with previous TREC adhoc tasks, the primary evaluation measure is mean average precision (MAP). For the results reported in the figure, MAP is estimated by the Minimal Test Collection (MTC) method <ref type="bibr" coords="5,422.64,472.18,10.92,10.91" target="#b2">[3]</ref>, which is also used by the TREC Million Query Track <ref type="bibr" coords="5,226.59,485.74,10.92,10.91" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Diversity Task</head><p>The diversity task is similar to the adhoc retrieval task, but differs in its evaluation measures and in its judging process, as described above. The goal of the diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For this task, the probability of relevance of a document is assumed to be conditioned on the documents that appear before it in the result list.</p><p>In all other respects, the diversity task is identical to the adhoc task: The same 50 topics were used. Query processing was required to be entirely automatic. Groups could submit up to three runs, all of which were judged.  <ref type="figure" coords="6,279.13,347.26,4.97,10.91">B</ref>). Estimates are calculated using the MTC method <ref type="bibr" coords="6,525.49,347.26,10.83,10.91" target="#b2">[3]</ref>. Runs are ranked by expected MAP. Only the best run from each group is included in the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measures for the Diversity Task</head><p>The diversity task used two evaluation measures: 1) α-nDCG as defined by Clarke et al. <ref type="bibr" coords="6,491.78,414.46,10.83,10.91" target="#b3">[4]</ref>, and 2) an "intent aware" version of precision, based on the work of Agrawal et al. <ref type="bibr" coords="6,436.93,428.02,10.92,10.91" target="#b0">[1]</ref>. The computation of α-nDCG exactly follows the procedure described in Clarke et al., with α = 0.5. We refer the reader to that paper for further information. For a given topic, the intent-aware measures of Agrawal et al. <ref type="bibr" coords="6,377.28,468.70,11.56,10.91" target="#b0">[1]</ref> treat each subtopic as a distinct interpretation of the associated query. Standard evaluation measures are computed separately with respect to each interpretation. A weighted average is then computed across the various interpretations to give intent-aware versions of the standard measures. Agrawal et al. allow probabilities to be attached to each interpretation. For the TREC 2009 Web track, we assume equal probabilities.</p><p>More specifically, we compute intent-aware precision at retrieval depth k using the following procedure. Assume there are M topics. Let N t , 1 ≤ t ≤ M be the number of subtopics associated with topic number t. Let j t (i, j) = 1 if the document returned for topic t at depth j is judged relevant to subtopic i of topic t; otherwise, let j t (i, j) = 0. We then define intent-aware precision at retrieval depth k as:</p><formula xml:id="formula_0" coords="6,198.00,613.79,341.93,32.12">precision-IA@k = 1 M M t=1 1 N t Nt i=1 1 k k j=1 j t (i, j)<label>(1)</label></formula><p>α-nDCG precision-IA Group Id Run Id @5 @10 @20 @5 @10 @20 MSRAsia Runs are ranked by α-nDCG@10. Only the best run from each group is included in the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Diversity Task Results</head><p>Figures 4 and 5 present the top results for the diversity task. Both α-nDCG and precision-IA are reported at retrieval depths 5, 10, and 20. Note that the run with id "uwgym" should not be viewed as an official submission by a track participant (although it does satisfy the rules for a valid submission). The run was generated by one of the track coordinators (Clarke, with the aid of graduate student Hani Khoshdel-Nikkhoo) to act as a baseline run for the track. To generate the run, the queries were submitted to one of the major commercial search engines. The results were then filtered against the URLs appearing in the Category B collection. Efforts to generate diversity met with mixed success. Overall, 18.1% of subtopics have no relevant documents. Of the 50 topics, only half have at least one relevant document for every subtopic. The evaluation script for the diversity task ignores subtopics for which there are no judged relevant documents.</p><p>Diversity and relevance are closely related. Runs that return many relevant documents tend to have higher subtopic recall. Figure <ref type="figure" coords="7,261.87,590.62,5.45,10.91" target="#fig_4">3</ref> illustrates this relationship. The figure shows a scatter plot of subtopic recall at depth 20 vs. precision at depth 20 for the Web Tack runs. For this plot, we compute precision at 20 by ignoring distinctions between subtopics. Relevance judgments for subtopics are combined by treating a document as relevant to a topic if it is relevant to any of its subtopics. We compute subtopic recall as the number of subtopics with relevant documents in the top 20 divided by the total number of subtopics with relevant documents in the collection.  For this plot only, precision at 20 is computed by combining subtopics, where we treat a document as relevant to a topic if it is relevant to any of its subtopics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Directions</head><p>The Web Track will continue for TREC 2010. Our current plans include both an adhoc task and a diversity task. In addition, we intend to introduce a Web spam task. As an aid to participating groups, we plan to distribute a preliminary spam ranking of the full Category A collection in early 2010.</p><p>Based on discussions during the conference, we intend to make a number of changes to the pooling and judging process. Several groups requested that the top documents from the diversity runs be judged using the adhoc criteria, which would aid them in making comparisons between methods. Other groups requested complete judgments of the adhoc runs to some fixed depth. We hope to accommodate these requests for TREC 2010, resources permitting.</p><p>As illustrated by Tables <ref type="table" coords="8,212.41,616.30,5.45,10.91" target="#tab_4">4</ref> and<ref type="table" coords="8,245.66,616.30,5.45,10.91" target="#tab_5">5</ref> the top Category A and Category B runs achieve similar performance. This outcome contradicts our expectation that a good Web search engine would to do better on Category A (which includes Category B as a subset) partly because it should benefit from the availability of a larger link graph and more anchor text. It is possible that, due to factors such as crawl order, Category B contains higher quality pages and less spam. The Wikipedia is</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,167.16,649.18,272.83,10.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of full TREC 2009 Web track topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,71.99,17.25,10.07"><head></head><label></label><figDesc>the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,72.36,269.50,467.34,10.91"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Output of the clustering algorithm is eight clusters, relating to the query "the current".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,72.00,409.90,468.20,10.91;8,72.00,423.46,468.05,10.91;8,72.00,437.02,286.47,10.91"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter plot of subtopic recall at depth 20 vs. precision at depth 20 for diversity runs.For this plot only, precision at 20 is computed by combining subtopics, where we treat a document as relevant to a topic if it is relevant to any of its subtopics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,112.80,87.40,386.34,73.96"><head>Table 1 :</head><label>1</label><figDesc>Participation in the TREC 2009 Web track.</figDesc><table coords="2,112.80,87.40,386.34,51.40"><row><cell></cell><cell>Category A runs</cell><cell>Category B runs</cell><cell>Total groups</cell></row><row><cell>adhoc task</cell><cell>13</cell><cell>14</cell><cell>25</cell></row><row><cell>diversity task</cell><cell>10</cell><cell>10</cell><cell>18</cell></row><row><cell>any task</cell><cell>13</cell><cell>16</cell><cell>26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,87.40,468.08,247.60"><head>Table 2 :</head><label>2</label><figDesc>Top 6 adhoc task results (Category A). Estimates are calculated using the MTC method<ref type="bibr" coords="6,525.49,191.74,10.83,10.91" target="#b2">[3]</ref>. Runs are ranked by expected MAP. Only the best run from each group is included in the ranking.</figDesc><table coords="6,97.44,87.40,416.70,247.60"><row><cell>Group Id</cell><cell>Run Id</cell><cell>Expected MAP</cell><cell>@5</cell><cell>@10</cell><cell>@20</cell></row><row><cell>Waterloo</cell><cell>watwp</cell><cell>0.043362</cell><cell cols="3">0.331699 0.347432 0.366025</cell></row><row><cell cols="2">UWaterlooMDS WatSdmrm3we</cell><cell>0.034555</cell><cell cols="3">0.155000 0.162222 0.177072</cell></row><row><cell>unimelb</cell><cell>muadibm5</cell><cell>0.033712</cell><cell cols="3">0.255000 0.286667 0.297297</cell></row><row><cell>MSRAsia</cell><cell>MSRANORM</cell><cell>0.033240</cell><cell cols="3">0.430000 0.380000 0.377338</cell></row><row><cell>UAms</cell><cell>uvamrftop</cell><cell>0.033225</cell><cell cols="3">0.445000 0.415556 0.382783</cell></row><row><cell>msrc</cell><cell>MS2</cell><cell>0.029981</cell><cell cols="3">0.425000 0.402222 0.390479</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Expected Precision</cell></row><row><cell cols="2">Group Id Run Id</cell><cell>Expected MAP</cell><cell>@5</cell><cell>@10</cell><cell>@20</cell></row><row><cell>UMD</cell><cell>UMHOOsd</cell><cell>0.047620</cell><cell cols="3">0.345772 0.399920 0.409814</cell></row><row><cell>UDel</cell><cell>udelIndDRSP</cell><cell>0.047082</cell><cell cols="3">0.277171 0.356119 0.389080</cell></row><row><cell>uogTr</cell><cell>uogTrdphCEwP</cell><cell>0.045983</cell><cell cols="3">0.541884 0.528193 0.522276</cell></row><row><cell>NEU</cell><cell>NeuLMWeb600</cell><cell>0.044242</cell><cell cols="3">0.395000 0.400567 0.406506</cell></row><row><cell>ICTNET</cell><cell>ICTNETADRun3</cell><cell>0.043297</cell><cell cols="3">0.442089 0.443574 0.442391</cell></row><row><cell>EceUdel</cell><cell>UDWAxBL</cell><cell>0.042454</cell><cell cols="3">0.334019 0.331361 0.337146</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.00,347.26,204.48,10.91"><head>Table 3 :</head><label>3</label><figDesc>Top 6 adhoc task results (Category</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.00,100.78,468.07,247.79"><head>Table 4 :</head><label>4</label><figDesc>Top 6 diversity task results (Category A). Runs are ranked by α-nDCG@10. Only the best run from each group is included in the ranking.</figDesc><table coords="7,122.64,100.78,366.54,247.79"><row><cell></cell><cell>MSRAACSF</cell><cell cols="4">0.281 0.316 0.365 0.127 0.112 0.108</cell></row><row><cell>msrc</cell><cell>MSDiv3</cell><cell cols="4">0.268 0.309 0.346 0.127 0.117 0.105</cell></row><row><cell>unimelb</cell><cell>mudvimp</cell><cell cols="4">0.220 0.241 0.268 0.091 0.073 0.061</cell></row><row><cell>THUIR</cell><cell cols="5">THUIR09FuClu 0.206 0.234 0.271 0.105 0.094 0.086</cell></row><row><cell>uogTr</cell><cell>uogTrDYScdA</cell><cell cols="4">0.159 0.191 0.233 0.074 0.077 0.089</cell></row><row><cell>utwente</cell><cell>twCSodpRBB</cell><cell cols="4">0.144 0.164 0.193 0.067 0.062 0.061</cell></row><row><cell></cell><cell></cell><cell></cell><cell>α-nDCG</cell><cell cols="2">precision-IA</cell></row><row><cell>Group Id</cell><cell>Run Id</cell><cell>@5</cell><cell>@10 @20</cell><cell>@5</cell><cell>@10 @20</cell></row><row><cell>Waterloo</cell><cell>uwgym</cell><cell cols="4">0.335 0.369 0.400 0.162 0.144 0.122</cell></row><row><cell>uogTr</cell><cell>uogTrDYCcsB</cell><cell cols="4">0.253 0.282 0.308 0.142 0.132 0.127</cell></row><row><cell>ICTNET</cell><cell>ICTNETDivR3</cell><cell cols="4">0.251 0.272 0.301 0.104 0.095 0.092</cell></row><row><cell cols="6">Amsterdam UamsDancTFb1 0.232 0.250 0.281 0.086 0.079 0.071</cell></row><row><cell>CSIUCD</cell><cell>UCDSIFTdiv</cell><cell cols="4">0.212 0.249 0.278 0.112 0.121 0.115</cell></row><row><cell cols="2">LU WUME wume1</cell><cell cols="4">0.220 0.247 0.279 0.121 0.113 0.108</cell></row><row><cell>NEU</cell><cell>NeuDiv1</cell><cell cols="4">0.215 0.243 0.278 0.126 0.131 0.134</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,72.00,360.82,467.81,10.91"><head>Table 5 :</head><label>5</label><figDesc>Top 6 diversity task results (Category B) with uwgym included for comparison (see text).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.56,661.84,451.45,8.17;1,72.00,672.88,467.90,8.17;1,72.00,683.80,89.41,8.17"><p>Further information on the ClueWeb09 collection is available at boston.lti.cs.cmu.edu/Data/clueweb09. We thank Jamie Callan, Mark Hoy, and the Language Technologies Institute at Carnegie Mellon University for creating this valuable resource.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>included in the Category B collection and may exert some influence on the results. We expect the differences between the categories to be explored by participating groups over the next year. At the present time, we plan to include both Category A and B runs in TREC 2010.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,88.92,160.66,450.92,10.91;9,88.92,174.10,451.00,10.91;9,88.92,187.66,210.53,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,449.65,160.66,90.19,10.91;9,88.92,174.10,29.85,10.91">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">Sreenivas</forename><surname>Rakesh Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,141.96,174.10,397.96,10.91;9,88.92,187.66,31.61,10.91">Proceedings of the Second ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM International Conference on Web Search and Data Mining<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.93,210.22,450.95,10.91;9,88.92,223.78,450.87,10.91;9,88.92,237.34,146.72,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,506.43,210.22,33.44,10.91;9,88.92,223.78,132.54,10.91">Million Query Track 2008 overview</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,250.44,223.78,284.59,10.91">17th Text REtrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>TREC 2008) Proceedings</note>
</biblStruct>

<biblStruct coords="9,88.93,259.78,450.99,10.91;9,88.92,273.34,450.89,10.91;9,88.92,286.90,404.33,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,345.50,259.78,194.42,10.91;9,88.92,273.34,28.53,10.91">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,141.84,273.34,397.97,10.91;9,88.92,286.90,194.56,10.91">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and development in information retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and development in information retrieval<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.93,309.46,451.25,10.91;9,88.92,323.02,451.03,10.91;9,88.92,336.58,451.01,10.91;9,88.92,350.14,341.21,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,281.09,323.02,258.86,10.91;9,88.92,336.58,17.41,10.91">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Ashkann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,128.16,336.58,411.77,10.91;9,88.92,350.14,179.95,10.91">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.93,372.58,450.99,10.91;9,88.92,386.14,451.15,10.91;9,88.92,399.70,24.63,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,342.42,372.58,197.49,10.91;9,88.92,386.14,45.55,10.91">Inferring query intent from reformulations and clicks</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,157.20,386.14,227.79,10.91">19th International World Wide Web Conference</title>
		<meeting><address><addrLine>Raleigh, North Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
