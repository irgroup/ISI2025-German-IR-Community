<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.05,137.95,289.90,15.12">Overview of the TREC 2009 Entity Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.35,170.43,79.88,10.48"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<email>k.balog@uva.nl</email>
						</author>
						<author>
							<persName coords="1,184.54,218.13,84.38,10.48"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
							<email>p.serdyukov@tudelft.nl</email>
						</author>
						<author>
							<persName coords="1,366.09,218.13,68.18,10.48"><forename type="first">Paul</forename><surname>Thomas</surname></persName>
							<email>paul.thomas@csiro.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Arjen P. de Vries CWI</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">CSIRO</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Thijs Westerveld Teezir</orgName>
								<address>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.05,137.95,289.90,15.12">Overview of the TREC 2009 Entity Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">363A07BBE325C9EF1D8D3C711DBF2703</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the entity track is to perform entity-oriented search tasks on the World Wide Web. Many user information needs would be better answered by specific entities instead of just any type of documents.</p><p>The track defines entities as "typed search results," "things," represented by their homepages on the web. Searching for entities thus corresponds to ranking these homepages. The track thereby investigates a problem quite similar to the QA list task. In this pilot year, we limited the track's scope to searches for instances of the organizations, people, and product entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related entity finding task</head><p>The first edition of the track featured one pilot task: related entity finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The document collection is the "category B" subset of the ClueWeb09 data set 1 . The collection comprises about 50 million English-language pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task</head><p>The first year of the track investigates the problem of related entity finding:</p><p>Given an input entity, by its name and homepage, the type of the target entity, as well as the nature of their relation, described in free text, find related entities that are of target type, standing in the required relation to the input entity. This task shares similarities with both expert finding (in that we need to return not "just" documents) and homepage finding (since entities are uniquely identified by their homepage). However, approaches to address this task need to generalize to multiple types of entities (beyond 1 ClueWeb09: http://boston.lti.cs.cmu.edu/Data/clueweb09/ just people) and return the homepages of multiple entities, not just one. Also, the topic defines a focal entity to which returned homepages should be related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Input</head><p>For each request (query) the following information is provided:</p><p>• Input entity, defined by its name and homepage</p><p>• Type of the target entity (person, organization, or product)</p><p>• Narrative (describing the nature of the relation in free text) This year's track limits the target entity types to three: people, organizations, and products. (Note that the input entity does not need to be limited to these three types).</p><p>An example topic is shown below: &lt;query&gt; &lt;num&gt;7&lt;/num&gt; &lt;entity_name&gt;Boeing 747&lt;/entity_name&gt; &lt;entity_URL&gt;clueweb09-en0005-75-02292&lt;/entity_URL&gt; &lt;target_entity&gt;organization&lt;/target_entity&gt; &lt;narrative&gt;Airlines that currently use Boeing 747 planes.&lt;/narrative&gt; &lt;/query&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Output</head><p>For each query, participants could return up to 100 answers (related entities). Each answer record comprises the following fields:</p><p>• (HP1..HP3) Up to 3 homepages of the entity (excluding Wikipedia pages)</p><p>• (WP) Wikipedia page of the entity • (NAME) A string answer that represents the entity concisely • (SUPPORT) Up to 10 supporting documents For each target entity (answer) at least one homepage (HP1) and at least one supporting document must be returned. The other two homepages (HP2 and HP3), the wikipedia page (WP), and the entity's name (NAME) are optional. Homepage fields (HP1..HP3) are treated as a set, i.e., the order in which these are returned is indifferent. The same entry (i.e., documents returned in the HP1..HP3 and WP fields) must not be retrieved for multiple entities in the same topic.</p><p>Returned entity names are required to be normalized as follows:</p><p>• Only the following characters are allowed: [a..z], [A..Z], [0..9],</p><p>• Accented letters need to be mapped to their plain ASCII equivalents (e.g., "á" ⇒ "a", "ü" ⇒ "u")</p><p>• Spaces need to be replaced with " "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Topics and assessments</head><p>Both topic development and relevance assessments were performed by NIST. Topic development encountered difficulties because it turned out that for many candidate topics, the "Category B" collection did not contain enough entity homepages. Trivial topics, i.e., topics for which all the related entities are linked from input entity's homepage/website or from its Wikipedia page, were avoided. For the first year of the track, 20 topics were created and assessed. Entities are not so easily defined very precisely; instead of engaging in a long discussion about the exact semantics underlying the notion of entity, we simply adopt the following working definition: A web entity is uniquely identifiable by one of its primary homepages. Real-world entities can be represented by multiple homepages; a clearly preferred one cannot always be given. As a work-around, entity resolution is addressed at evaluation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Assessment procedure</head><p>The assessment procedure consisted of two stages. In phase one, judgments were made for HP, WP, and NAME fields, individually. Then, in phase two, HPs, WPs, and NAMEs belonging to the same entity were grouped together.</p><p>Phase one. All runs were pooled down to 10 records, and for each record entry, judgments were made for the homepage (HP and WP) and the name (NAME) fields.</p><p>Homepages were judged on a three-point relevance scale: (0) non-relevant, (1) relevant ("descriptive") or (2) primary ("authoritative"). If a HP entry was the homepage for a correct entity, it was judged "primary." Likewise, if a WP entry was a correct Wikipedia page for an entity. Pages that were related without being actual homepages for the entities were judged "relevant." All other pages were judged non-relevant.</p><p>Each name returned in the record was also judged on a three-level scale: (0) incorrect, (1) inexact or (2) correct. A name was judged inexact or correct if it matched up with something else in the record, even if the record was not either primary or relevant for the topic. A name was "inexact" if it was correct but was not a complete form (had extra words or was ambiguous). Otherwise it was judged incorrect.</p><p>Phase two. Assessors matched primary pages (HP and WP) to correct names, creating a set of equivalence classes for the right answers to each topic (i.e., addressing the resolution of entities).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Qrels</head><p>In the qrels file, the fields are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>topic-entry_type docid_or_name rel class</head><p>Where topic-entry type denotes the topic ID (first half) and the field (second half), e.g., "1-HP" is the HP field for topic 1; docid or name is a document ID (for fields HP1..HP3 and WP) or a name (for field NAME); rel is {0, 1, 2} as described above; and class is an integer value, where lines with the same topic number and class correspond to the same entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Evaluation measures</head><p>The main evaluation measure we use is NDCG@R; that is, the normalized discounted cumulative gain at rank R (the number of primaries and relevants for that topic) where a record with a Run Group Type WP Ext. NDCG@R P@10 #rel #pri The top run from each group by NDCG@R, using the default evaluation setting (HP-only). The columns of the table (from left to right) are: runID, group, type of the run (automatic/manual), whether the Wikipedia subcollection received a special treatment (Yes/No), whether any external resources were used (Yes/No), NDCG@R, P@10 (fraction of records in the first 10 ranks with a primary homepage), number of relevant retrieved homepages, and number of primary retrieved homepages.</p><p>primary gets gain 2, and a record with a relevant gets gain 1. We also report on P@10, the fraction of records in the first ten ranks with a primary. Note that evaluation results are not computed using the standard trec eval tool, but a script developed specifically for the 2009 edition of the Entity track<ref type="foot" coords="4,402.78,422.23,3.97,6.12" target="#foot_0">2</ref> .</p><p>In the next section, we report the official evaluation results for the tasks. These are computed only on the basis of the homepage (HP) fields. In addition, we report on alternative evaluation scenarios, where extra credit is given for finding Wikipedia homepages and names for the related entities (see Section 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs and Results</head><p>Each group was allowed to submit up to four runs. Thirteen groups submitted a total of 41 runs; of those, 34 were automatic runs. Four groups submitted a total of 7 manual runs.</p><p>Table <ref type="table" coords="4,149.66,550.30,4.98,8.74" target="#tab_0">1</ref> shows the evaluation results for the top run from each group (ordered by NDCG@R). As we see from Table <ref type="table" coords="4,208.63,562.26,3.87,8.74" target="#tab_0">1</ref>, performance varies significantly over the participants. Interestingly, result rankings would be quite different dependent on the performance measure chosen.</p><p>The differences between P@10 and NDCG@R results show that even though teams Purdue and CAS find the same number of primary entity homepages in their top 10 results, the Purdue strategy seems better at identifying more relevant (but not primary) homepages. University of Glasgow retrieves by far the highest number of relevant entities, but other groups achieve better early precision. This could be merely a matter of re-ranking the initial results list, possibly helped by improved spam detection (but we did not investigate this in detail yet).</p><p>The complete list of all submitted runs along with the evaluation results using the default evaluation setting is presented in Table <ref type="table" coords="4,282.26,669.85,3.87,8.74">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Alternative evaluations</head><p>We consider different variations for computing the gain for each record.</p><p>HP-only (default) only homepage (HP1..3) fields are considered; a record with a primary homepage gets gain 2, with a relevant homepage gets gain 1. Names are not taken into account. (For each record the maximum gain is 2.)</p><p>HP+NAME in addition to the homepage (HP1..3) fields, NAME is also taken into account.</p><p>An extra gain of 1 is awarded if an exact name is returned along with a primary homepage.</p><p>(For each record the maximum gain is 3.)</p><p>WP-only only the Wikipedia (WP) field is considered; a record with a primary Wikipedia page gets gain 2, with a relevant Wikipedia page gets gain 1. Names are not taken into account. (For each record the maximum gain is 2.) HP+WP HP1..3 and WP fields are all considered, names are not; a record with a primary page (either homepage or Wikipedia page) gets gain 2, with a relevant page gets gain 1.</p><p>(For each record the maximum gain is 2.) HP+WP+NAME all fields are considered. An extra gain of 1 is awarded if an exact name is returned along with a primary homepage or Wikipedia page. (For each record the maximum gain is 3.)</p><p>The results of these alternative evaluation scenarios are presented in Table <ref type="table" coords="5,437.24,362.54,3.87,8.74">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The usefulness of Wikipedia</head><p>In order to study how far we can go with Wikipedia only when looking for entities, we analyzed the list of relevant entities and the list of their description pages. We found that 160 out of 198 relevant entities (≈80%) have a Wikipedia page among their primary pages, while only 108 of them have a primary web page (70 entities have both). However, not all primary Wikipedia pages could be returned by participants or judged, or not all Wikipedia pages could exist on the date when the ClueWeb collection was crawled (January/February 2009). So, we manually looked for primary Wikipedia pages for those 38 entites that had only primary web pages, using online Wikipedia (accessed in December 2009). As a result, we discovered primary Wikipedia pages for 22 entities. Those 16 entities that are not represented in Wikipedia are seemingly not notable enough, however they include all answers for 3 of 20 queries (looking for audio cds, phd students and journals).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approaches</head><p>The following are descriptions of the approach taken by different groups. These paragraphs were contributed by participants and are meant to be a road map to their papers.</p><p>Purdue We propose a hierarchical relevance retrieval model for entity ranking. In this model, three levels of relevance are examined which are document, passage and entity, respectively. The final ranking score is a linear combination of the relevance scores from the three levels. Furthermore, we exploit the structure of tables and lists to identify the target entities from them by making a joint decision on all the entities with the same attribute. To find entity homepages, we train logistic regression models for each type of entities. A set of templates and filtering rules are also used to identify target entities. <ref type="bibr" coords="5,387.84,688.74,80.40,8.74" target="#b1">(Fang et al., 2009)</ref> uogTr The uogTr group extended the Voting Model for people search to the task of finding related entities of a particular type. Their approach builds semantic relationship support for the Voting Model, by considering the co-occurrences of query terms and entities in a document as a vote for the relationship between these entities. Additionally, on top of the Voting Model, they developed a novel graph-based technique to further enhance the initial vote estimations. <ref type="bibr" coords="6,209.95,160.84,106.02,8.74" target="#b3">(McCreadie et al., 2009)</ref> CAS In our approach, a novel probabilistic model was proposed to entities finding in a Web collection. This model consists of two parts. One is the probability indicating the relation between the source entity and the candidate entities. The other is the probability indicating the relevance between the candidate entities and the topic. <ref type="bibr" coords="6,439.34,216.63,78.59,8.74" target="#b9">(Zhai et al., 2009)</ref> NiCT We aim to develop an effective method to rank entities via measuring "similarities" between input query and supporting snippets of entities. Three models are implemented to this end: The DLM calculates the probabilities of generating input query given supporting snippets of entities via language model; The RSVM ranks entities via a supervised Ranking SVM; The CSVM estimates the probabilities of input query belonging to "topics" represented by entities and their supporting snippets via SVM classifier. <ref type="bibr" coords="6,436.61,296.33,85.39,8.74;6,132.91,308.28,23.80,8.74" target="#b7">(Wu and Kashioka, 2009)</ref> UAms (Amsterdam) For the entity ranking track, we explore the effectiveness of the anchor text representation, we look at the co-citation graph, and experiment with using Wikipedia as a pivot. Two of our official runs exploit information in Wikipedia. The first run ranks all Wikipedia pages according to their match to entity name and narrative. To find primary homepages, we follow links on Wikipedia pages. The other run reranks Wikipedia pages of the first run using category information. The other two runs use an anchor text index where the queries consist of the entity name and the narrative, and co-citations of the given entity url. <ref type="bibr" coords="6,205.71,411.90,94.09,8.74" target="#b2">(Kaptein et al., 2009)</ref> TUDelft In three of four methods used to produce our runs we treated Wikipedia as the repository of entities to rank. We ranked either all Wikipedia articles, or those articles that are linked by the "primary" Wikipedia page for the query entity. Then we considered only entities that are mentioned at the given primary or at the top ranked non-Wikipedia pages from the entire collection. Additionally we filtered-out entities that belong to nonmatching classes using DBPedia, Yago, and articles infoboxes. <ref type="bibr" coords="6,411.15,491.60,110.85,8.74;6,132.91,503.55,23.80,8.74" target="#b5">(Serdyukov and de Vries, 2009)</ref> BUPTPRIS In our work, an improved two-stage retrieval model is proposed according to the task. The first stage is document retrieval, in order to get the similarity of the query and documents. The second stage is to find the relationship between documents and entities. Final scores are computed by combining previous results. We also focus on entity extraction in the second stage and the final ranking. <ref type="bibr" coords="6,364.43,571.30,84.12,8.74" target="#b6">(Wang et al., 2009)</ref> UALR CB We used Lemur tool kit version 4.10 to index the WARC format documents which were given on Red Hat Enterprise Linux machine. Then we used the queries to retrieve the named entities using Indri Query Language which was very related to the Inquery language. First we retrieved the pages related to the given queries of people or organizations and products and then we found the exact home pages for them using some keywords related to them. <ref type="bibr" coords="6,173.31,651.00,100.07,8.74" target="#b4">(Pamarthi et al., 2009)</ref> UIUC The team from University of Illinois at Urbana-Champaign focused on studying the usefulness of information extraction techniques for improving the accuracy of entity finding task. The queries were formulated as a relation query between two entities such that one of the entities is known and the goal is to find the other entity that satisfies the relation. The two-step approach of relation retrieval followed by entity finding helped explore techniques to improve entity extraction using NLP resources and corpus-based reranking based on other relations that link the entities.</p><p>UWaterloo All terms in the entity name and narrative except stopwords constitute our query terms. We retrieve the query's top-100 passages and expanded them using a sliding window size of 100. We fetch their n-grams where n = 1..10. We consider only n-grams that is a Wikipedia title. Tf-idf weight was assigned to each term in the n-gram. We now compute the ranking score for each n-gram using the sum of their term weights.</p><p>EceUdel Our general goal for the Entity track is to study how we may apply language modeling approaches and natural language processing techniques to the task. Specifically, we proposed to find supporting information based on segment retrieval, extract entities using Stanford NER tagger, and rank entities based on a previously proposed probabilistic framework. <ref type="bibr" coords="7,184.71,272.42,85.79,8.74" target="#b10">(Zheng et al., 2009)</ref> BIT Related Entity Finding by Beijing Institute of Technology employs Lemur toolkit to index and retrieve dataset stemmed by Krovetz stemmer and stopped using a standard list of 421 common terms; devised OpenEphyras Question Analyzer to construct weighted query strings; OpenEphyras NETagger to extract typed entities; OpenNLPs ME classifier to rank extracted entities homepages whose model is trained by TREC-supplied test topics; DBPedia (dump date 05/11/09) to extract product name list for identifying product entity names. <ref type="bibr" coords="7,166.17,364.07,81.37,8.74" target="#b8">(Yang et al., 2009)</ref> UAms (ISLA) We propose a probabilistic modeling approach to related entity finding. We estimate the probability of a candidate entity co-occurring with the input entity, in two ways: context-dependent and context-independent. The former uses statistical language models built from windows of text in which entities co-occur, while the latter is based on the number of documents associated with candidate and input entities. We also use Wikipedia for detecting entity name variants and type filtering. <ref type="bibr" coords="7,413.66,443.78,80.70,8.74" target="#b0">(Bron et al., 2009)</ref> 5 Summary</p><p>The first year of the entity track featured a related entity finding task. Given an input entity, the type of the target entity (person, organization, or product), and the relation, described in free text, systems had to return homepages of related entities, and, optionally, the corresponding Wikipedia page and/or the name of the entity. Topic development encountered difficulties because it turned out that for many candidate topics, the "Category B" collection did not contain enough entity homepages. For the first year of the track, 20 topics were created and assessed. Assessment took place in two stages. First, the assessors judged the returned pages. Here, the hard parts of relevance assessment are to (a) identify a correct answer and (b) distinguish a homepage from a non-homepage. Assessors were then shown a list of all pages they had judged "primary" and all names that were judged "correct". They could assign each to a pre-existing class, or create a new class.</p><p>Concerning submissions, a common take on the task was to first gather snippets for the input entity, then extract co-occurring entities from these snippets, using a named entity tagger (off-the-self or custom-made). Language modeling techniques were often employed by these approaches. Several submissions built heavily on Wikipedia; exploiting links outgoing from the entity's Wikipedia page, using it to improve named entity recognition, making use of Wikipedia categories for entity type detection, just to name a few examples. Table 2: All submitted runs by NDCG@R, using the default evaluation setting (HP-only). The columns of the table (from left to right) are: runID, group, type of the run (automatic/manual), whether the Wikipedia subcollection received a special treatment (Yes/No), whether any external resources were used (Yes/No), NDCG@R, P@10, number of relevant retrieved homepages, and number of primary retrieved homepages. Highest scores for each metric are in boldface.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,108.00,118.94,401.77,186.14"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,119.57,118.94,390.19,152.20"><row><cell>KMR1PU</cell><cell>Purdue</cell><cell>auto</cell><cell>Y</cell><cell>Y</cell><cell cols="2">0.3061 0.2350 126</cell><cell>61</cell></row><row><cell>uogTrEpr</cell><cell>uogTr</cell><cell>auto</cell><cell>N</cell><cell>N</cell><cell cols="2">0.2662 0.1200 347</cell><cell>79</cell></row><row><cell>ICTZHRun1</cell><cell>CAS</cell><cell>auto</cell><cell>N</cell><cell>N</cell><cell>0.2103 0.2350</cell><cell>80</cell><cell>70</cell></row><row><cell>NiCTm3</cell><cell>NiCT</cell><cell>auto</cell><cell>Y</cell><cell>Y</cell><cell>0.1907 0.1550</cell><cell>99</cell><cell>64</cell></row><row><cell cols="3">UAmsER09Ab1 UAms (Amsterdam) auto</cell><cell>N</cell><cell>N</cell><cell cols="2">0.1773 0.0450 198</cell><cell>19</cell></row><row><cell>tudpw</cell><cell>TUDelft</cell><cell>auto</cell><cell>Y</cell><cell>N</cell><cell cols="2">0.1351 0.0950 108</cell><cell>42</cell></row><row><cell>PRIS3</cell><cell>BUPTPRIS</cell><cell cols="2">manual N</cell><cell>N</cell><cell>0.0892 0.0150</cell><cell>48</cell><cell>3</cell></row><row><cell>UALRCB09r4</cell><cell>UALR CB</cell><cell>auto</cell><cell>N</cell><cell>N</cell><cell>0.0666 0.0200</cell><cell>15</cell><cell>4</cell></row><row><cell>UIauto</cell><cell>UIUC</cell><cell>auto</cell><cell>N</cell><cell>N</cell><cell>0.0575 0.0100</cell><cell>64</cell><cell>3</cell></row><row><cell>uwaterlooRun</cell><cell>Waterloo</cell><cell>auto</cell><cell>N</cell><cell>N</cell><cell>0.0531 0.0100</cell><cell>55</cell><cell>5</cell></row><row><cell>UdSmuTP</cell><cell>EceUdel</cell><cell>auto</cell><cell>N</cell><cell>N</cell><cell cols="2">0.0488 0.0000 102</cell><cell>10</cell></row><row><cell cols="2">BITDLDE09Run BIT</cell><cell cols="2">manual N</cell><cell>Y</cell><cell>0.0416 0.0200</cell><cell>81</cell><cell>9</cell></row><row><cell>ilpsEntBL</cell><cell>UAms (ISLA)</cell><cell>auto</cell><cell>Y</cell><cell>Y</cell><cell>0.0161 0.0000</cell><cell>30</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,123.24,688.78,182.36,6.99"><p>http://trec.nist.gov/data/entity/09/eval-entity.pl</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>(1) HP-only</p><p>(2) WP-only (3) HP+WP NDCG P@10 #rel #pri +NAME NDCG P@10 #rel #pri NDCG P@10 #rel #pri +NAME KMR1PU  To save space, we write NDCG@R as NDCG when HP/WP fields are considered; +NAME denotes NDCG@R when the NAME field is also taken into account. P10, #rel, and #pri are as before. The ordering of runs corresponds to those of Table <ref type="table" coords="10,379.19,607.66,3.87,8.74">2</ref>. Highest scores for each metric are in boldface.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,108.00,122.88,414.00,8.74;8,117.96,134.84,404.03,8.74;8,117.96,146.79,22.69,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,286.78,122.88,212.87,8.74">Related Entity Finding Based on Co-Occurance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.96,134.84,312.04,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,166.72,414.00,8.74;8,117.96,178.67,404.04,8.74;8,117.96,190.63,343.23,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,295.94,166.72,221.08,8.74;8,117.96,178.67,305.03,8.74">Exploiting the Structure of Tables and Learning Homepage Classifiers</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,443.96,178.67,78.04,8.74;8,117.96,190.63,226.57,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Entity Retrieval with Hierarchical Relevance Model</note>
</biblStruct>

<biblStruct coords="8,108.00,210.55,414.00,8.74;8,117.96,222.51,404.03,8.74;8,117.96,234.46,294.21,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,281.17,210.55,240.82,8.74;8,117.96,222.51,256.81,8.74">Result Diversity and Entity Ranking Experiments: Anchors, Links, Text and Wikipedia, University of Amsterdam</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaptein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,396.05,222.51,125.95,8.74;8,117.96,234.46,177.55,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,254.39,414.00,8.74;8,117.96,266.34,404.03,8.74;8,117.96,278.30,226.81,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,425.14,254.39,96.87,8.74;8,117.96,266.34,183.77,8.74">University of Glasgow at TREC 2009: Experiments with Terrier</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,324.98,266.34,197.02,8.74;8,117.96,278.30,110.15,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,298.22,414.00,8.74;8,117.96,310.18,404.03,8.74;8,117.96,322.13,22.69,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,274.51,298.22,230.47,8.74">A Journey in Entity Related Retrieval for TREC 2009</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pamarthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.96,310.18,312.04,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,342.06,414.00,8.74;8,117.96,354.02,404.04,8.74;8,117.96,365.97,108.79,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,253.87,342.06,268.13,8.74;8,117.96,354.02,79.38,8.74">Delft University at the TREC 2009 Entity Track: Ranking Wikipedia Entities</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,215.94,354.02,301.53,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,385.90,414.00,8.74;8,117.96,397.85,404.03,8.74;8,117.96,409.81,22.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,337.54,385.90,163.13,8.74">BUPT at TREC 2009: Entity Track</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.96,397.85,312.04,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,429.73,414.00,8.74;8,117.96,441.69,404.04,8.74;8,117.96,453.64,71.56,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,222.74,429.73,299.26,8.74;8,117.96,441.69,22.86,8.74">NiCT at TREC 2009: Employing Three Models for Entity Ranking Track</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kashioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,163.29,441.69,309.97,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,473.57,414.00,8.74;8,117.96,485.52,404.04,8.74;8,117.96,497.48,108.79,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,303.31,473.57,218.69,8.74;8,117.96,485.52,51.60,8.74">Experiments on Related Entity Finding Track at TREC 2009</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,197.45,485.52,320.02,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,517.40,414.00,8.74;8,117.96,529.36,404.04,8.74;8,117.96,541.31,226.81,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,318.69,517.40,203.32,8.74;8,117.96,529.36,186.01,8.74">A Novel Framework for Related Entities Finding: ICTNET at TREC 2009 Entity Track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,326.46,529.36,195.54,8.74;8,117.96,541.31,110.15,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,561.24,414.00,8.74;8,117.96,573.19,404.03,8.74;8,117.96,585.15,22.69,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,321.46,561.24,181.32,8.74">UDEL/SMU at TREC 2009 Entity Track</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.96,573.19,312.04,8.74">Proceedings of the Eighteenth Text REtrieval Conference (TREC 2009)</title>
		<meeting>the Eighteenth Text REtrieval Conference (TREC 2009)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
