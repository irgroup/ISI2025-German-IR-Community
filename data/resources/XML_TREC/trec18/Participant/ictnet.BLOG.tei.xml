<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.04,77.48,335.52,19.82">ICTNET at Blog Track TREC 2009</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,133.56,126.40,30.64,8.77"><forename type="first">Xueke</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,172.38,126.40,23.13,8.77"><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.87,126.40,34.13,8.77"><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.04,126.40,40.12,8.77"><forename type="first">Xiaoming</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.39,126.40,34.82,8.77"><forename type="first">Linhai</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.40,126.40,35.38,8.77"><forename type="first">Feng</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.76,126.40,38.82,8.77"><forename type="first">Zeying</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.28,124.05,44.25,11.11"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.04,77.48,335.52,19.82">ICTNET at Blog Track TREC 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A30F7A765E21B23E7A89D85C1F9A843</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in blog track of TREC2009. All runs are submitted for both two task, namely Top stories identification task and faceted blog distillation task. The "FirteX" platform was used to index and retrieval posts. As for top stories identification task, to identify important headlines, we measure the importance of headline by accumulating the BM25 relevance score with posts on the query day. We propose a graph-based iterative approach and a sub-topic detecting based approach respectively to identify diverse blog posts. As for faceted blog distillation task: we adopt a very straightforward approach and measure the topical relevance by only exploiting top ad-hoc 10000 posts. To identify facet inclination, we either train centroid classifier or compute facet inclination weights of terms to compute facet inclination score and rerank feed by combining relevance score and facet inclination score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired by more refined and complex search scenarios in the blogosphere, the Blog track 2009 has two new pilot tasks faceted blog distillation and top stories identification task. Faceted blog distillation task is beyond blog distillation task，which is defined as finding user a blog with a principle, recurring interest in given topic，by requiring participants to provide additionally facet inclination for the retrieved blogs beyond topical relevance . The facets considered for Blog track 2009 are opinionated, personal and in-depth. Top stories identification task aims to investigate the blogosphere's response to news stories as they develop and verify the usefulness of the blogosphere in real-time news identification.</p><p>In this year ICTNET group participates in blog track and submits runs for both two tasks, namely top stories identification task and faceted blog distillation, For both tasks, data preprocessing plays a important role, we need to detect valuable content blocks from post pages and discard noisy blocks. This blog track use a new collection called Blogs08 which is one order of magnitude bigger than Blogs06 and amounts to over 2TB of data, making our experiments more challenging. We use our "FirteX" platform which is developed by our lab for indexing and retrievaling preprocessed posts.</p><p>As for Top stories identification task, to detect important headlines, we treat each headline as a query and measure the importance of the headline by accumulating BM25 relevance score <ref type="bibr" coords="1,472.20,534.83,11.68,9.02">[1]</ref> with posts on the given query day. To provide supporting posts covering diverse aspects of the story, we apply a graph-based iterative algorithm for finding supporting relevant posts and propose a sub-topic detecting based method for diversity.</p><p>As for faceted blog distillation task, results show that runs exploiting only top 10000 ad-hoc relevant posts for each topic perform much better than those using all contained posts in topical blog distillation subtask. For facet identification, we either train centroid classifier or weight terms for specific facet inclination using statistical approaches. Both the classifier and weights of terms are used to compute facet inclination score measuring the extent to which the post should be treated as specific inclination of given facet and finally we rerank feed by combining relevance score and facet inclination score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Preprocessing</head><p>Data preprocessing task is manly focused on content extraction. The content extraction is to find the valuable and related parts in post pages. The layouts of this type of web pages in the Trec blog 2009 corpus vary greatly, so it is very difficult to specify a general template to feature valuable and related parts. We only design an algorithm to remove link tables, which are most common noise in web pages </p><formula xml:id="formula_0" coords="2,166.86,277.43,244.16,111.36">21 N = parent(N ); 22 w hile score(parent(N )) &lt; score(N ): 23 N = parent(N ); 24 D elete N from T ; ( ) ( ) ( ) ( ) textC nt N linkC nt N Score N textC nt N - =</formula><p>This algorithm converts each input page into a DOM tree in Step 1. Firstly, it counts the number of words (textCnt) and the number of links (linkCnt) for each terminal node in Steps 2-11. Secondly, it counts these two numbers for non-terminal nodes in Steps 12-16, and textCnt and linkCnt of each non-terminal node are the sum of the textCnt and linkCnt of all its children. It calculates text-to-link ratio scores for non-terminal nodes in Step 17. In Steps 18-24, it traverses through the DOM tree in DFS (Depth First Search) order to find link tables and remove them. The algorithm searches a link table from a link node and a link table is found when the score of parent of the link node is below a pre-defined threshold. After finding a link table, the algorithm tries to extend it in Steps 22-23. A link table which can't be extended is removed from the DOM tree in Step 24. Traversing in DFS order guarantees that deleting nodes will not impact traversing. Extending link tables from bottom up will avoid deleting useful information in the input page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Faceted blog distillation task Topical blog distillation sub-task</head><p>We produce a ranked list of the top 10000 ad-hoc relevant posts according to BM25 relevance score for each topic based on our "FirteX" platform. Two submitted runs measure the topic relevance and inclination of feeds by only exploiting posts in the list and other runs consider all contained posts. Results show the former two runs perform much better. The reason may be that since most posts are irrelevant or weakly relevant even they contains some query terms, they may play an overwhelming part, weaken the information delivered by relevant posts and make model biased to noisy information. These results are indicative that filtering out irrelevant posts to the fullest extent possible beforehand may eliminate the negative influence and boost the performance.</p><p>The topical relevance formula of the best run (run tag ICTNETBDRUN2) is as follows:</p><p>( , ) We also identify blogs' topical relevance by considering the topic relevance distribution among the timespan inspired the idea that a relevant feed should be one with recurring interest in the topic, not a bursting focus on the topic in a short time, results shows that we need improve our premature idea and maybe consider topic evolution information throughout the timespan, treating feed as temporal information sources.</p><p>Facet identification sub-task 1. For in-depth and personal facet, a centroid classifier refined by DragPushing strategy [2] is learnt, where a prototype vector (centroid) is learnt to represent each facet inclination, namely indepth and shallow. We score a post according to following formula (only take in-depth facet for example, personal facet likewise) respectively , the post is represented by VSM and term weighting method is tf-idf ,the similarity between post and prototype vector is computed with cosine measure. The score ranges from 0 to 1 and measure the confidence of judge a post as being indepth, when the score is large than 0.5 the post is identified as being indepth, otherwise shallow. We classify feeds by majority voting and score the feed as follows The above discussion is mainly about the best run (run tag ICTNETBDRUN2), while other runs differ from it in topical relevance formula or whether it uses all contained post to judge topical relevance or facet inclination.</p><formula xml:id="formula_1" coords="3,162.78,213.68,150.58,29.29">( , ) ( ) ( , )<label>( ,</label></formula><p>Here, we give the results of the 4 official runs submitted, which are listed in the table below. All these runs are title only automatic runs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,144.78,699.33,3.99,10.80;2,174.36,699.33,14.91,10.80;2,291.42,699.33,19.18,10.80;2,340.32,699.33,3.99,10.80;2,225.66,695.02,7.85,4.53;2,249.66,687.78,10.04,6.24;2,203.28,696.84,21.83,6.24;2,236.22,681.27,13.23,10.80;2,267.96,681.27,16.74,10.80;2,137.04,699.33,47.82,10.80;2,313.26,699.33,23.82,10.80;2,233.34,708.75,23.82,10.80;2,206.52,694.61,4.94,8.49;2,192.00,695.46,6.57,14.68;2,212.04,674.08,12.81,22.02;2,99.90,732.06,26.71,9.02;2,152.10,727.43,9.56,5.54;2,134.10,732.44,16.79,7.64;2,169.56,732.06,335.95,9.02;2,89.88,744.24,121.01,9.02"><head></head><label></label><figDesc>of posts which are both contained in Feed and also in top 10000 ad-hoc posts list for the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,353.76,232.16,4.00,10.81;3,287.10,220.20,20.81,6.25;3,140.52,229.32,20.76,6.25;3,242.76,238.68,20.75,6.25;3,330.90,238.68,21.55,6.25;3,236.58,213.68,51.01,10.81;3,127.02,222.74,13.32,10.81;3,168.60,222.74,6.00,10.81;3,192.18,232.16,51.03,10.81;3,280.05,232.16,51.01,10.81;3,181.68,218.87,6.58,14.70;3,271.02,228.29,6.58,14.70;3,99.90,252.12,26.71,9.02;3,152.10,257.38,20.77,6.27;3,136.02,250.79,16.53,10.85;3,178.26,252.12,2.51,9.02;3,201.66,257.34,21.61,6.24;3,185.28,250.76,16.50,10.80;3,233.58,252.12,272.12,9.02"><head></head><label></label><figDesc>for indepth ,shallow facet inclination</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,274.62,328.15,16.18,10.82;3,157.80,346.33,4.00,10.82;3,187.62,346.33,4.00,10.82;3,228.00,341.98,7.86,4.54;3,252.36,334.73,20.82,6.25;3,205.62,343.73,21.84,6.25;3,135.54,352.85,20.82,6.25;3,254.28,354.47,10.85,6.25;3,238.92,328.15,13.33,10.82;3,280.44,328.15,6.01,10.82;3,122.04,346.33,13.32,10.82;3,162.47,346.33,23.85,10.82;3,229.32,356.41,23.83,10.82;3,208.86,341.50,4.95,8.51;3,194.34,342.46,6.59,14.71;3,214.38,321.00,12.84,22.07;3,89.88,374.82,415.69,9.02;3,89.88,389.10,38.33,9.02;3,154.66,389.10,8.34,9.02;3,189.38,389.10,32.81,9.02;3,253.50,387.73,4.00,10.81;3,283.08,387.73,14.86,10.81;3,245.76,387.73,47.81,10.81;3,321.00,389.10,5.01,9.02;3,385.38,387.74,4.00,10.81;3,415.14,387.74,4.00,10.81;3,363.12,394.32,20.76,6.25;3,349.62,387.74,13.32,10.81;3,390.05,387.74,23.84,10.81;3,442.92,389.10,2.79,9.02;3,472.15,389.10,33.35,9.02;3,99.78,406.66,4.02,10.85;3,129.36,406.66,14.88,10.85;3,92.10,406.66,69.38,10.97;3,207.12,406.79,4.01,10.85;3,236.94,406.79,7.97,10.85;3,184.86,413.38,20.83,6.27;3,171.42,406.82,13.32,10.81;3,211.79,406.82,23.85,10.81;3,162.42,402.91,6.60,14.75;3,251.88,408.12,68.88,9.02;3,99.90,435.84,405.63,9.02;3,89.88,447.30,415.71,9.02;3,89.88,458.82,415.62,9.02;3,89.88,470.34,415.72,9.02;3,89.88,481.80,282.59,9.02;3,129.18,494.75,16.19,10.85;3,185.10,494.75,24.29,10.85;3,231.36,494.75,12.05,10.85;3,120.66,501.36,6.94,6.25;3,164.93,501.36,17.18,6.25;3,222.84,501.36,6.95,6.25;3,107.04,494.78,33.97,10.81;3,157.68,494.78,8.01,10.81;3,190.92,494.78,47.68,10.81;3,170.76,499.12,3.81,8.50;3,148.08,490.91,6.58,14.70;3,210.77,490.91,3.00,14.70;3,89.88,515.10,26.71,9.02;3,150.90,513.71,24.29,10.85;3,130.74,520.32,17.17,6.25;3,123.48,513.74,8.01,10.81;3,156.72,513.74,13.66,10.81;3,136.62,518.08,3.81,8.50;3,182.52,515.10,188.80,9.02;3,394.20,513.71,12.05,10.85;3,385.68,520.32,6.95,6.25;3,378.30,513.74,127.24,10.81;3,89.88,531.30,26.07,9.02;3,94.92,542.82,410.66,9.02;3,99.78,574.66,4.02,10.85;3,129.36,574.66,14.88,10.85;3,92.10,574.66,59.61,10.85;3,210.36,556.64,16.18,10.80;3,178.74,570.40,6.42,4.53;3,201.84,563.16,6.95,6.25;3,156.18,572.22,21.89,6.25;3,198.48,582.96,8.93,6.25;3,188.28,556.64,33.90,10.80;3,173.34,584.84,23.83,10.80;3,159.48,569.98,4.94,8.49;3,164.34,549.50,12.82,22.04;3,235.38,576.12,270.19,9.02;3,99.78,623.78,4.00,10.80;3,129.36,623.78,14.86,10.80;3,92.10,623.78,56.55,10.80;3,206.46,605.66,16.18,10.80;3,174.84,619.48,6.42,4.53;3,197.94,612.24,6.95,6.25;3,152.28,621.24,21.89,6.25;3,194.58,631.98,8.93,6.25;3,184.38,605.66,33.90,10.80;3,169.44,633.92,23.83,10.80;3,155.58,619.00,4.94,8.49;3,160.44,598.52,12.82,22.04;3,225.60,625.14,2.51,9.02"><head></head><label></label><figDesc>opinionated facet, for each post in the top relevant posts list mentioned above, we sum up tf-idf weight of opinionate terms, and pick the top 200 posts as pseudo-opinionated posts. We then apply the Bol term weighting model[3], which measures how informative a term is in pseudo-opinionated post set against top relevant post set, to derive topic-specific opinionate term weights. Finally, opinionate score of a the post is calculated as follows idf weight of term t in VSM of post p, and ( ) op w t is opinionate weight of term t.Finally, we rerank feeds by combining opinionate score and topical relevance according to</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Top stories identification task</head><p>The top stories identification task can be divided into two sub-tasks: first participants should identify top news stories which they think are important; second they should further provide supporting posts which are not only relevant to the news story but also cover diverse aspects of the story with less redundance. As for the first sub-task ,we devise our method based on the observation that import headlines are those concerning wide-ranging influential topics or events and thus mentioned by bloggers extensively; we treat each headline as a query and measure the importance of the headline by accumulating the BM25 relevance score with posts on given day . Specifically, for a given day, the importance of a headline can be measure by formula 1 ( , ) Score headline day = 25 ( , )</p><p>To provide supporting posts covering diverse aspects of the story, we apply two approaches: a graph-based iterative ranking algorithm and sub-topic detecting based method. The first method (denoted as Topic-PR) is inspired by idea of topic-focused text summarization; we propose a graph-based ranking algorithm which resembles topic-sensitive PageRank. The aim of the algorithm is to find most representative posts with respect to the topic. For each headline, top 100 relevant posts are retrieved from posts of following 7 days (including the query day), these posts are modeled as a graph, and an iterative algorithm is applied on the graph to score the representativeness of each post. Given headline h, the iterative formula is as follows</p><p>Where, the damping factor α is set to 0.85; M is the normalized similarity matrix of the graph where cosine measure is used to computing similarity of two posts.</p><p>Then diversity penalty strategy is then used to reinforce diversity in greedy way [4], finally 10 posts are picked as supporting posts, As for the second method (denoted as Sub-Topic) ,we perform query expansion on top 100 relevant posts and select top 50 expansion term as candidates, we treat the original headline along with a expansion term as refinement of original topic ,namely sub-topic and present each sub-topic with top 5 retrieved post with query of headline along with expansion term. However there are highly overlapping among these sub-topics which may lead to redundance. To reduce redundance, we first score sub-topic with score of corresponding expansion term in query expansion procedure, then pick 10 sub-topic with novelty in a greedy way , specifically we penalize the sub-topic highly similar with already picked sub-topic, and rerank the remaining sub-topic once one sub-topic is picked. Finally the most relevant post is retrieved for each sub-topic, by means of sub-topics detecting, we aim to find posts concerning diverse aspect in more supervised way.</p><p>Summaries for submitted runs are listed as follows，the candidate headlines are those on d, d+1 ,d-1 unless mentioned specially, here d is the query date:</p><p>1. ICTNETTSRun1 Tab1 Results for identifying important headlines Tab2 Results for identifying diverse blog posts From table1 , the results for identifying important headlines are dissatisfactory，and as the consequence, the results of second sub-task are also disappointed. The reason may lies in that measuring importance of headlines with sum of BM25 relevance with posts on given day tends to select headlines respecting very general topics which are not good candidates as top news stories. We may consider burst characteristic of headline to identify top news stories and filter headlines respecting very general topics in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>This paper reports data preprocessing method and technical scheme for two tasks in TREC 2009 Blog Track. Most methods are straightforward, and the most indicative finding is that filtering irrelevant posts beforehand may eliminate the negative influence and boost the performance in tradition blog distillation task.</p><p>In the future, we will devote to exploit the topic evolution information throughout the timespan, treating feeds as temporal information sources, to judge the topical relevance and develop more reasonable approaches for identifying facet inclination of blog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledge</head><p>This work was funded by the 973 National Basic Research Program of China under grant number 2007CB311100 and National Natural Science Foundation of China under grant number 60873245， 60933005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">References</head><p>[1] Robertson, S.E. and Walker, S.Okapi/Keenbow at TREC-8, In the Eighth Text Retrieval Conference(TREC 8), 1999, pp.151-162.</p><p>[2] Songbo Tan, Xueqi Cheng, etc.A Novel Refinement Approach for Text Categorization.CIKM2005, October 31-November 5, 2005, Bremen, Germany.</p><p>[3] G. Amati. Probabilistic models for information retrieval based on Divergence from Randomness. PhD thesis, University of <ref type="bibr" coords="5,193.52,624.89,63.14,9.02;5,89.88,636.35,277.99,9.02">Glasgow, 2003. [4] Zhang, B., Li, H., Liu, Y., Ji, L., Xi, W., Fan, W., Chen, Z., and</ref><ref type="bibr" coords="5,371.02,636.35,64.96,9.02">Ma, W.-Y. 2005</ref>. Improving web search results using affinity graph. In Proceedings of SIGIR2005. </p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
