<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,83.10,70.07,443.59,18.83;1,67.67,89.99,474.43,18.83;1,173.12,109.92,263.52,18.83">A Study of Term Proximity and Document Weighting Normalization in Pseudo Relevance Feedback -UIUC at TREC 2009 Million Query Track</title>
				<funder ref="#_VQP6KJf #_WhHvQFZ #_XNW5bAT">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,82.36,156.29,62.47,12.55"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,155.64,156.29,40.52,12.55"><forename type="first">Jing</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName coords="1,206.97,156.29,117.44,12.55"><forename type="first">V</forename><forename type="middle">G Vinod</forename><surname>Vydiswaran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.22,156.29,84.81,12.55"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,430.83,156.29,92.35,12.55"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,83.10,70.07,443.59,18.83;1,67.67,89.99,474.43,18.83;1,173.12,109.92,263.52,18.83">A Study of Term Proximity and Document Weighting Normalization in Pseudo Relevance Feedback -UIUC at TREC 2009 Million Query Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7857C5139479A0B870485E6E333D98F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report our experiments in the TREC 2009 Million Query Track. Our first line of study is on proximitybased feedback, in which we propose a positional relevance model (PRM) to exploit term proximity evidence so as to assign more weights to expansion words that are closer to query words in feedback documents. The second line of study is to improve the weighting of feedback documents in the relevance model by using a regression-based method to approximate the probability of relevance (and thus the name RegRM). In the third line of study, we test a supervised approach for query classification. Besides, we also evaluate a selective pseudo feedback strategy which stops pseudo feedback for precision-oriented queries and only uses it for recall-oriented ones.</p><p>The proposed PRM has shown clear improvements over the relevance model for pseudo feedback, suggesting that capturing the term proximity heuristic appropriately could lead to a better feedback model. RegRM performs as well as relevance model, but no noticeable improvement is observed. Unfortunately, the proposed query classification methods appear to not work well. The results also show that the proposed selective pseudo feedback may not work well, since precision-oriented queries can also benefit from pseudo feedback, though not as much as recall-oriented queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We took the opportunity of participating in TREC 2009 Million Query Track to study some novel pseudo relevance feedback algorithms on the large Web data set ClueWeb09.</p><p>Most existing feedback algorithms, e.g., <ref type="bibr" coords="1,227.26,543.39,9.72,9.41">[7,</ref><ref type="bibr" coords="1,240.47,543.39,7.16,9.41" target="#b6">6,</ref><ref type="bibr" coords="1,251.12,543.39,7.16,9.41" target="#b8">8,</ref><ref type="bibr" coords="1,261.77,543.39,7.16,9.41" target="#b3">3,</ref><ref type="bibr" coords="1,272.42,543.39,7.16,9.41" target="#b9">9,</ref><ref type="bibr" coords="1,283.07,543.39,6.48,9.41" target="#b4">4]</ref>, used a whole feedback document as a unit, without distinguishing the position of each word for term weighting. However, it is often the case that only some part of a document is useful for query expansion. This problem is especially critical for Web search, because the content of a web page is often incoherent and covers several different topics. It motivates us to select expansion terms through assigning appropriate weights to expansion terms by incorporating term position and proximity information into the feedback model, based on the intuition that words closer to query words in feedback documents often appear to be more relevant to the query.</p><p>Besides term weighting, document weighting also plays an important role in pseudo feedback as shown in our recent work <ref type="bibr" coords="1,338.79,217.13,9.21,9.41" target="#b4">[4]</ref>, in which we found one variation of relevance model (i.e., RM3) <ref type="bibr" coords="1,366.24,227.60,9.72,9.41" target="#b3">[3,</ref><ref type="bibr" coords="1,380.12,227.60,7.16,9.41" target="#b1">1]</ref> most robust due to its use of the query likelihood as a weight for each pseudo-relevant document. However, our pilot experiments also indicate that the query likelihood score does not reflect the probability of relevance well, suggesting there is still room to improve the estimation of the relevance model with better document weighting.</p><p>To address these two issues, we extend the relevance model <ref type="bibr" coords="1,316.81,300.82,9.72,9.41" target="#b3">[3]</ref> to incorporate term position and proximity information to improve term weighting, and to normalize the query likelihood score to enhance document weighting. Specifically, in the first line of study, we propose a positional relevance model (PRM), which extends our previous work <ref type="bibr" coords="1,506.25,342.66,9.72,9.41" target="#b5">[5]</ref> to exploit the evidence of term proximity in a probabilistic model for term weighting so as to assign more weights to words closer to query words in feedback documents; In our second line of study, which is an extension to our recent work <ref type="bibr" coords="1,529.02,384.50,9.21,9.41" target="#b4">[4]</ref>, we develop a regression-based method to normalize document weighting (i.e., query likelihoods) for the relevance model to make it better reflect the probability of relevance, and thus the name regularized relevance model (RegRM).</p><p>Besides the two directions discussed above, we also explore query classification in our experiments. We design a number of features which are combined using a logistic classifier for query classification. Based on the results of query classification, we also evaluate a selective pseudo feedback strategy which stops pseudo feedback for precision-oriented queries and only uses it for recall-oriented ones.</p><p>The results show that the PRM improves over the relevance model clearly and consistently for pseudo feedback, suggesting that the term proximity heuristic is useful for feedback. RegRM performs as well as relevance model, but no noticeable improvement is observed. Unfortunately, the proposed query classification methods appear to not work well. The results also show that selective pseudo feedback may not work well if the decision only depends on whether a query is recall-oriented or precision-oriented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">POSITIONAL RELEVANCE MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Positional Language Model</head><p>The key idea of positional language model (PLM) <ref type="bibr" coords="1,526.47,657.39,9.72,9.41" target="#b5">[5]</ref> is to estimate a language model for each position of a document. Specifically, we let each word at each position of a document to propagate the evidence of its occurrence to all other positions in the document so that positions closer to the word would get more share of the evidence than those far away.</p><p>The PLM at each position can then be estimated based on the propagated counts of all the words to the position as if they had appeared actually at the position with discounted counts. This new family of language models is intended to capture the content of the document at each position, which is roughly like a "soft passage" centered at this position but can potentially cover all the words in the document with less weight on words far away from the position.</p><p>Formally, the PLM at position i of document D can be estimated as:</p><formula xml:id="formula_0" coords="2,117.12,166.29,175.78,23.45">p(w|D, i) = c (w, i) w ∈V c (w , i)<label>(1)</label></formula><p>where c (w, i) is the total propagated count of term w at position i from the occurrences of w in all the positions. Following <ref type="bibr" coords="2,94.66,218.37,9.21,9.41" target="#b5">[5]</ref>, c (w, i) is estimated using the Gaussian kernel function:</p><formula xml:id="formula_1" coords="2,99.40,245.32,193.50,27.88">c (w, i) = N j=1 c(w, j) exp -(i -j) 2 2σ 2<label>(2)</label></formula><p>where i and j are absolute positions of the corresponding terms in the document, and N is the length of the document; c(w, j) is the actual count of term w at position j.</p><p>The PLM P (•|D, i) needs to be smoothed. We use Jelinek-Mercer smoothing method to smooth the PLM, which is shown to work as well as Dirichlet prior smoothing and is relatively insensitive to the setting of smoothing parameter in our experiments.</p><formula xml:id="formula_2" coords="2,91.62,373.20,197.34,9.87">p λ (w|D, i) = (1 -λ)p(w|D, i) + λp(w|B) (<label>3</label></formula><formula xml:id="formula_3" coords="2,288.96,373.20,3.92,9.41">)</formula><p>where λ is a smoothing parameter and p(w|B) is the background language model estimated using the whole collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Positional Relevance Model</head><p>Positional relevance model (PRM) extends PLM for feedback. PRM estimates the conditional probability P (w|Q) in terms of the joint probability of observing w with the query Q at every position in every feedback document. Formally,</p><formula xml:id="formula_4" coords="2,102.73,484.99,190.14,42.93">P (w|Q) ∝ P (w, Q) = D∈F |D| i=1 P (w, Q, D, i)<label>(4)</label></formula><p>where i indicates a position of document D, and F is the set of pseudo-relevant documents. We propose two methods for estimating this joint probability.</p><p>First method: given each feedback document D, we choose a position i with a probability P (i|D), and then generate word w and query Q conditioned on D and i, i.e., P (w, Q|D, i). Mathematically, we get the following derivation of the joint probability:</p><formula xml:id="formula_5" coords="2,82.22,627.29,210.66,9.41">P (w, Q, D, i) = P (D)P (i|D)P (w, Q|D, i) (5)</formula><p>where, p(D) is a general prior on documents and is often assumed to be uniform. We also do not put any prior on positions without extra knowledge, and thus we can compute P (i|D) as 1 |D| . We will further assume that the query Q and the word w in feedback documents are sampled identically and independently from a unigram distribution P (•|D, i). We get the following final estimate for the joint probability of w and Q:</p><formula xml:id="formula_6" coords="2,348.04,72.07,207.88,28.70">P (w, Q) ∝ D∈F |D| i=1 P (Q|D, i)P (w|D, i) |D| (6)</formula><p>Second method: we fix a value of Q according to some prior P (Q), and then assume the following generating process: a document D is first drawn conditioned on Q, and a position i of document D is then drawn dependent on D and Q, followed by the generation of word w. Mathematically, we get the following derivation of the joint probability:</p><formula xml:id="formula_7" coords="2,334.64,173.81,221.25,9.41">P (w, Q, D, i) ∝ P (D|Q)P (i|Q, D)P (w|Q, D, i)<label>(7)</label></formula><p>In the above equation, it is natural to assume that the sampling of word w is only conditioned on the position i and the document D and is independent of any query, and thus we obtain: P (w|Q, D, i) = P (w|D, i). We also adopt uniform prior for both documents and positions. After rewriting P (D|Q) and P (i|Q, D) by Bayes rule, we get the following final estimation for the joint probability of w and Q:</p><formula xml:id="formula_8" coords="2,322.76,268.24,233.15,23.68">P (w, Q) ∝ D∈F |D| i=1 P (Q|D) D∈F P (Q|D) P (Q|D, i) |D| i=1 P (Q|D, i) P (w|D, i) (8)</formula><p>In the above two estimation methods, for the efficiency reason, we simplify P (w|D, i) as:</p><formula xml:id="formula_9" coords="2,370.55,325.06,185.36,21.96">P (w|D, i) = 1.0 if D[i] == w 0.0 otherwise<label>(9)</label></formula><p>Note that P (Q|D, i) is the key component in equations 6 and 8. It is the query likelihood of the positional language model at position i of document D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DOCUMENT WEIGHTING NORMALIZA-TION WITH REGRESSION</head><p>In our recent work <ref type="bibr" coords="2,405.08,425.30,9.21,9.41" target="#b4">[4]</ref>, we have found that the relevance model <ref type="bibr" coords="2,343.51,435.76,9.72,9.41" target="#b3">[3]</ref> works more robust than some other feedback models mainly due to its use of the query likelihood as document weighting. However, our pilot experiments show that the query likelihood scores do not reflect the probability of relevance well.</p><p>The distribution of relevance can be approximated as follows:</p><formula xml:id="formula_10" coords="2,383.28,509.82,172.63,25.10">p(i|θ rel ) = j δ(Q j , i) i j δ(Qj, i)<label>(10)</label></formula><p>where Q j is a training query. δ(Q j , i) equals to 1 if the ith retrieved document of Q j is relevant; 0 otherwise. Here the approximated distribution of relevance is essentially the distribution of relevant documents in different ranking positions. Similarly, we can also obtain the distribution of query likelihood scores.</p><formula xml:id="formula_11" coords="2,359.69,604.65,196.22,28.22">p(i|θQL) = 1 M M j QL(Q j , i) |F | k=1 QL(Qj, k) (11)</formula><p>where QL(Q j , i) is the query likelihood score of the ith result document of Q j , and M is the number of training queries. Through visualization or power transformation, we can find an approximate "logarithm" relation between the two distributions, that is, where c = b a . We thus hypothesize that in order to get a better approximation of the distribution of relevance, we can normalize the query likelihood by learning parameter c through a simple linear regression method. And then, we can use the normalized query likelihood to replace the raw query likelihood in the relevance model. We call this method regularized relevance model (RegRM) in this paper.</p><formula xml:id="formula_12" coords="2,371.19,696.25,184.71,23.26">p(i|θ rel ) ≈ a • log p(i|θQL) + b ∝ log p(i|θ QL ) + c (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QUERY CLASSIFICATION</head><p>For the query classification task, we classify a query as either recall-oriented or precision-oriented. We use logistic classification to learn the classification models, which are trained on the TREC 04 Web track dataset. Although in this training set, queries are classified as homepage query (HP), named page query (NP), or topic distillation query (TD), both homepage queries and named page queries can be regarded as precision-oriented queries since their information needs can often be satisfied by one Web page, while topic distillation queries can be considered as recall-oriented queries.</p><p>We train two binary classifiers: the first one is to classify navigational queries from informational queries, while the second one is to classify precision-oriented informational queries from recall-oriented informational queries. During the classification process, the first classifier is used to classify all queries, after that, those queries that are judged to be informational queries are further classified into precisionoriented or recall-oriented categories by using the second classifier.</p><p>There are three types of features used in our work: preretrieval, post-retrieval and search engine generated features. Pre-retrieval features can be computed easily from the query string and document collection statistics; post-retrieval features can only be computed after a ranked list of documents has been retrieved; search engine generated features are computed based on the top ranked documents from Google for the corresponding query. Table <ref type="table" coords="3,194.67,510.94,4.61,9.41" target="#tab_1">1</ref> lists all the features we used.</p><p>We submitted three runs, each of which used a different set of features to train the corresponding classification model. Table <ref type="table" coords="3,78.49,552.78,4.61,9.41" target="#tab_0">2</ref> shows feature used in these three runs, as well as the classification performance of each run. The overall accuracy, the accuracy of precision-oriented queries, and the accuracy of recall-oriented queries are listed in columns 3, 4 and 5, respectively.</p><p>It shows that our methods appear to not work well. For the three runs we submitted, neither term dependency (with ordered/unordered terms) features nor search engine generated features help improve the performance. There are several possible reasons. First, the features may have not been normalized well. Second, the characteristics of precisionoriented queries may not be captured sufficiently (e.g., in our training data, we only take homepage and named page queries that have single-page answers as precision-oriented queries, but actually a precision-oriented query may also be answered by a combination of several pages.), leading to a worse performance on precision-oriented queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We use the Lemur toolkit (version 4.10) and Indri search engine (version 2.10)<ref type="foot" coords="3,407.71,125.46,3.66,6.28" target="#foot_0">1</ref> in our experiments. For all data sets, the preprocessing of documents and queries involves stemming with the Porter stemmer and stopwords removing using a total of 418 stopwords from the standard InQuery stoplist.</p><p>Our baseline retrieval model is the KL-divergence retrieval model <ref type="bibr" coords="3,346.02,189.50,9.21,9.41" target="#b2">[2]</ref>, and we choose the popular Dirichlet smoothing method <ref type="bibr" coords="3,366.38,199.96,14.33,9.41" target="#b10">[10]</ref> for smoothing document language models, where the smoothing parameter µ is set empirically to 1, 500 in our experiments.</p><p>We also fix the number of feedback documents to 20 and the number of terms in feedback model to 30, while the feedback interpolation coefficient for all feedback algorithms is set to 0.4 for WT2g (with query topics 401-450) and 0.5 for ClueWeb09 and Terabyte06 (with query topics 801-850), respectively.</p><p>There are two additional parameters σ and λ in the positional relevance model, we fix them to 100 and 0.3 respectively, which have been shown to perform robustly in both our preliminary and official experiments. Besides, the linear regression model used for normalizing query likelihood scores is trained on Terabyte06.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Results</head><p>We submitted five runs for the Million Query Track to test the proposed pseudo feedback algorithms, including uiuc09KL, uiuc09RegQL, uiuc09MProx, uiuc09GProx, and uiuc09Adpt. For all these runs, we just experimented with the first 1, 000 queries. These runs are described in Table <ref type="table" coords="3,498.93,420.42,3.58,9.41" target="#tab_2">3</ref>. Some runs are also involved in the query classification task as shown in Table <ref type="table" coords="3,342.14,441.34,3.58,9.41" target="#tab_0">2</ref>.</p><p>In Table <ref type="table" coords="3,364.98,451.80,3.58,9.41">4</ref>, we compare the performance of our official runs. Although the evaluation results of mtc and statMAPs are often inconsistent with each other, we can still see that in general, uiuc09GProx &gt; uiuc09MProx &gt; uiuc09RegQL &gt; uiuc09Adpt. However, the results of statMAPs show surprisingly that all the pseudo feedback algorithms lose to the baseline KL-divergence retrieval model, even though the results of mtc show otherwise.</p><p>Overall, both variations of PRM worked well: they perform comparably or better than the relevance model (i.e., RM3). Another run, uiuc09RegQL, performs similarly to the relevance model, but no noticeable improvement has been observed; one possible reason is that queries are different, and we should consider their characteristics to adaptively normalize document weighting.</p><p>Unfortunately, uiuc09Adpt fails to improve the performance. To examine if the poor performance is due to the low precision of the query classification, we generated another adaptive retrieval run Adpt+. This new run dynamically chooses uiuc09GProx/uiuc09KL for recall/precisionoriented queries based on the ground truth of query classification, but when a query is not covered by the ground truth, Adpt+ is backed off to uiuc09Adpt. The results in Table <ref type="table" coords="3,341.30,692.40,4.61,9.41">4</ref> show that Adpt+ is slightly better than uiuc09Adpt, pre-retrieval 1. maximal/average normalized df 2. maximal/average normalized cf 3. point mutual information between pairs of terms 4. query length 5. query likelihood 6. maximal/average normalized df for n ordered terms from the query 7. maximal/average normalized df for n any terms from the query post-retrieval 8. normalized maximal score difference between di and di+1 9. common document number in top three documents by content retrieval and multiple fields(anchor, url, content) retrieval 10. is it a homepage document retrieved in top three ranks by multiple field retrieval search engine 11. Is the top retrieved document from Google suggesting that recall-oriented queries indeed benefit a little more from feedback than precision-oriented queries. To further examine if it can improve performance by stopping pseudo feedback for precision-oriented queries, we generated an "ideal" run Adpt++, which does not execute pseudo feedback for precision-oriented queries according to the ground truth but does pseudo feedback for all other queries (including recall-oriented queries and unknown-type queries). It turns out that Adpt++ still loses to uiuc09GProx, which means that precision-oriented queries could also benefit from pseudo feedback, though not as much as recall-oriented queries.</p><p>Overall, selective pseudo feedback may not work well if the decision only depends on whether a query is recall-oriented or precision-oriented. In uiuc09MProx and uiuc09GProx, we employ our original implementation of PLM in <ref type="bibr" coords="4,186.10,504.06,9.72,9.41" target="#b5">[5]</ref> directly. However, one concern with that implementation is that the length of "soft" passages around the boundary of a document would be smaller than that in the middle of the document; as a result, the boundary positions tend to receive more weights. This may not raise problems in PLMs for retrieval <ref type="bibr" coords="4,213.04,556.37,9.21,9.41" target="#b5">[5]</ref>, but it could hurt PRM, where the relative weights of terms are more important. So, we decided to generate two additional runs PRM1 and PRM2 (see Table <ref type="table" coords="4,149.64,587.75,3.58,9.41" target="#tab_2">3</ref>), in which we use a fixed-length (the length of the very middle "soft" passage) for all "soft" passages in the document to estimate their corresponding positional language models.</p><p>The results of the additional runs are also presented in Table <ref type="table" coords="4,79.37,640.05,3.58,9.41">4</ref>. It is interesting to see that PRM1 and PRM2 are much better than uiuc09MProx and uiuc09GProx respectively. Overall, PRM1 performs the best on the ClueWeb09 data set, and it outperforms the regular relevance model by more than 5% in most of the cases; PRM2 also performs better than the regular relevance model. It suggests that capturing term proximity appropriately could lead to a bet-ter feedback model.</p><p>In addition, we also compare PRM1 and PRM2 with the regular relevance feedback and the baseline KL-divergence retrieval model on two other TREC data sets, i.e., WT2g and Terabyte06, using traditional evaluation criteria, such as MAP and precision at top-k documents. The results of comparison confirm our observation that both PRM1 and PRM2 outperform RM3, though PRM1 is slightly worse than PRM2, which is inconsistent with the observations on ClueWeb09 data set. More experiments are needed to understand which variation of the PRM is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In summary, we first studied proximity-based feedback, for which we propose a positional relevance model to exploit term proximity evidence so as to assign more weights to words closer to query words in pseudo-relevant documents; the second line of study was to improve the weighting of feedback documents in the relevance model by using a regression-based method to approximate the probability of relevance; thirdly, we also tested a supervised approach to query classification; in addition, we also evaluated a selective pseudo feedback strategy which skips pseudo feedback for precision-oriented queries and only uses it for recall-oriented ones.</p><p>The proposed PRM has been shown to clearly outperform the relevance model for pseudo feedback, suggesting that capturing term proximity appropriately could lead to a better feedback model; the relevance model with regressionbased document weighting (RegRM) performs as well as relevance model, but no noticeable improvement is observed; however, the proposed query classification methods appear to not work well. The results also show that selective pseudo feedback may not work well if the decision only depends on whether a query is recall-oriented or precision-oriented. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,60.81,53.70,225.09,56.93"><head>Table 2 :</head><label>2</label><figDesc>Features used in the runs</figDesc><table coords="3,60.81,53.70,225.09,36.46"><row><cell>Run</cell><cell>Features</cell><cell cols="3">All(%) P(%) R(%)</cell></row><row><cell>uiuc09Adpt</cell><cell>1,2,3,4,5,8,9,10</cell><cell>57.6</cell><cell>43.4</cell><cell>68.2</cell></row><row><cell cols="2">uiuc09ReqQL 1,2,3,4,5,6,9,10,11</cell><cell>54.9</cell><cell>53.5</cell><cell>55.9</cell></row><row><cell>uiuc09KL</cell><cell cols="2">1,2,3,4,5,6,7,8,9,10 56.7</cell><cell>31.0</cell><cell>76.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,76.29,200.95,456.99,105.25"><head>Table 1 :</head><label>1</label><figDesc>Features for Query Classification</figDesc><table coords="4,76.29,221.38,456.99,84.82"><row><cell></cell><cell>uiuc09KL</cell><cell>Baseline KL-divergence without feedback</cell></row><row><cell></cell><cell>uiuc09RegQL</cell><cell>Document weighting normalization with regression</cell></row><row><cell>Official runs</cell><cell>uiuc09MProx</cell><cell>Positional relevance model (i.i.d. sampling) + soft passage</cell></row><row><cell></cell><cell>uiuc09GProx</cell><cell>Positional relevance model (conditional sampling) + soft passage</cell></row><row><cell></cell><cell>uiuc09Adpt</cell><cell>Adaptively choose uiuc09GProx/uiuc09KL for recall/precision-oriented queries</cell></row><row><cell></cell><cell>RM3</cell><cell>Baseline relevance model [3, 1]</cell></row><row><cell>Additional runs</cell><cell>PRM1</cell><cell>Positional relevance model (i.i.d. sampling) + fixed-length passage</cell></row><row><cell></cell><cell>PRM2</cell><cell>Positional relevance model (conditional sampling) + fixed-length passage</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,237.46,317.51,134.74,9.41"><head>Table 3 :</head><label>3</label><figDesc>Description of Runs</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,321.42,709.69,122.63,9.41"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>0.<rs type="grantNumber">2608 0.2583 0.2666 0.2596 0.2576 0.2770 0.2580 0.2692 eP30 0.2592 0.2639 0.2641 0.2648 0.2626 0.2629 0.2722 0.2625 0.2633 eP100 0.2310 0.2374 0.2386 0.2383 0.2377 0.2384 0.2485 0.2383 0.2389 statMAP 0.2252 0.2122 0.2091 0.2182 0.2169 0.2158 0.2266 0.2213 0.2177</rs> statMAPs <rs type="grantNumber">sMPC10 0.2454 0.2433 0.2428 0.2420 0.2297 0.2454 0.2412 0.2363 0.2291</rs> .base <rs type="grantNumber">sMPC30 0.2829 0.2875 0.2819 0.2848 0.2906 0.2860 0.2922 0.2896 0.2760 sMPC100 0.2650 0.2574 0.2534 0.2701 0.2492 0.2575 0.2633 0.2679 0.2697 statMAP 0.2274 0.2112 0.2044 0.2087 0.2108 0.2145 0.2196 0.2177</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VQP6KJf">
					<idno type="grant-number">2608 0.2583 0.2666 0.2596 0.2576 0.2770 0.2580 0.2692 eP30 0.2592 0.2639 0.2641 0.2648 0.2626 0.2629 0.2722 0.2625 0.2633 eP100 0.2310 0.2374 0.2386 0.2383 0.2377 0.2384 0.2485 0.2383 0.2389 statMAP 0.2252 0.2122 0.2091 0.2182 0.2169 0.2158 0.2266 0.2213 0.2177</idno>
				</org>
				<org type="funding" xml:id="_WhHvQFZ">
					<idno type="grant-number">sMPC10 0.2454 0.2433 0.2428 0.2420 0.2297 0.2454 0.2412 0.2363 0.2291</idno>
				</org>
				<org type="funding" xml:id="_XNW5bAT">
					<idno type="grant-number">sMPC30 0.2829 0.2875 0.2819 0.2848 0.2906 0.2860 0.2922 0.2896 0.2760 sMPC100 0.2650 0.2574 0.2534 0.2701 0.2492 0.2575 0.2633 0.2679 0.2697 statMAP 0.2274 0.2112 0.2044 0.2087 0.2108 0.2145 0.2196 0.2177</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,58.28,381.67,96.80,12.55" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,399.36,212.16,9.41;5,72.62,409.82,198.34,9.41;5,72.62,420.28,218.29,9.41;5,72.62,430.73,197.45,9.41;5,72.62,441.20,155.57,9.41" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,189.85,430.73,80.22,9.41;5,72.62,441.20,68.49,9.41">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName coords=""><forename type="first">Nasreen</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leah</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In TREC &apos;04</note>
</biblStruct>

<biblStruct coords="5,72.62,452.66,203.19,9.41;5,72.62,463.11,219.49,9.41;5,72.62,473.58,183.57,9.41;5,72.62,484.04,58.85,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,234.48,452.66,41.34,9.41;5,72.62,463.11,219.49,9.41;5,72.62,473.58,96.06,9.41">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,186.87,473.58,40.49,9.41">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,495.49,220.19,9.41;5,72.62,505.96,214.74,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,227.36,495.49,65.45,9.41;5,72.62,505.96,64.52,9.41">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,156.12,505.96,40.49,9.41">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,517.42,205.37,9.41;5,72.62,527.87,193.13,9.41;5,72.62,538.33,219.92,9.41;5,72.62,548.80,38.84,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,218.85,517.42,59.14,9.41;5,72.62,527.87,193.13,9.41;5,72.62,538.33,115.74,9.41">A comparative study of methods for estimating query language models with pseudo feedback</title>
		<author>
			<persName coords=""><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,207.12,538.33,85.42,9.41;5,72.62,548.80,11.09,9.41">Proceedings of CIKM &apos;09</title>
		<meeting>CIKM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,560.25,186.07,9.41;5,72.62,570.71,209.78,9.41;5,72.62,581.18,101.84,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,218.85,560.25,39.84,9.41;5,72.62,570.71,165.47,9.41">Positional language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,256.30,570.71,26.09,9.41;5,72.62,581.18,11.09,9.41">SIGIR &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,592.63,192.21,9.41;5,72.62,603.09,207.73,9.41;5,72.62,613.56,167.96,9.41;5,72.62,624.02,82.39,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,72.62,603.09,143.74,9.41">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karen</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,223.68,603.09,56.67,9.41;5,72.62,613.56,163.93,9.41">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,635.47,196.28,9.41;5,72.62,645.93,187.26,9.41;5,72.62,656.40,196.80,9.41;5,72.62,666.85,160.64,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,130.82,635.47,138.08,9.41;5,72.62,645.93,32.05,9.41">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,134.84,645.93,125.03,9.41;5,72.62,656.40,192.78,9.41">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<imprint>
			<publisher>Prentice-Hall Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,72.62,678.31,216.60,9.41;5,72.62,688.78,202.15,9.41;5,72.62,699.23,167.96,9.41;5,72.62,709.69,82.39,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,211.87,678.31,77.35,9.41;5,72.62,688.78,138.23,9.41">Improving retrieval performance by relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,218.10,688.78,56.67,9.41;5,72.62,699.23,163.93,9.41">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="288" to="297" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,384.21,213.86,9.41;5,335.63,394.67,190.15,9.41;5,335.63,405.14,206.70,9.41;5,335.63,415.60,20.98,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="5,498.78,384.21,50.72,9.41;5,335.63,394.67,190.15,9.41;5,335.63,405.14,81.97,9.41">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,435.79,405.14,39.84,9.41">CIKM &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,335.63,427.05,205.92,9.41;5,335.63,437.52,218.70,9.41;5,335.63,447.98,186.62,9.41;5,335.63,458.43,58.85,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="5,498.78,427.05,42.78,9.41;5,335.63,437.52,218.70,9.41;5,335.63,447.98,99.12,9.41">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,452.94,447.98,40.49,9.41">SIGIR &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
