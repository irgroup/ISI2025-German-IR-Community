<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.20,71.96,313.29,16.59;1,200.28,91.88,209.18,16.59;1,105.60,128.11,398.53,12.90">University of Glasgow at TREC 2009: Experiments with Terrier Blog, Entity, Million Query, Relevance Feedback, and Web tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.12,157.07,91.10,10.76"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
							<email>richardm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.83,157.07,80.75,10.76"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
							<email>craigm@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.17,157.07,50.68,10.76"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
							<email>ounis@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.89,157.07,37.85,10.76"><forename type="first">Jie</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.14,157.07,99.41,10.76"><forename type="first">Rodrygo</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
							<email>rodrygo@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow Scotland</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.20,71.96,313.29,16.59;1,200.28,91.88,209.18,16.59;1,105.60,128.11,398.53,12.90">University of Glasgow at TREC 2009: Experiments with Terrier Blog, Entity, Million Query, Relevance Feedback, and Web tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6215506720EDEEBB80D1CC79115E496B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2009, we extend our Voting Model for the faceted blog distillation, top stories identification, and related entity finding tasks. Moreover, we experiment with our novel xQuAD framework for search result diversification. Besides fostering our research in multiple directions, by participating in such a wide portfolio of tracks, we further develop the indexing and retrieval capabilities of our Terrier Information Retrieval platform, to effectively and efficiently cope with a new generation of large-scale test collections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In TREC 2009, we participate in the Blog, Entity, Million Query, Relevance Feedback and Web tracks. This year, we have further developed our Terrier IR platform <ref type="bibr" coords="1,181.27,418.32,14.87,8.07" target="#b28">[29]</ref> with regards to efficiency and effectiveness for the newly introduced large-scale collections. Participation in such a wide portfolio of tracks allows us to comprehensively evaluate Terrier in a challenging environment. Our primary research directions focus on further applications for the Voting Model <ref type="bibr" coords="1,92.33,470.64,13.70,8.07" target="#b19">[20]</ref>, as well as on experimenting with our novel xQuAD framework for search result diversification <ref type="bibr" coords="1,207.61,481.08,14.12,8.07" target="#b33">[34,</ref><ref type="bibr" coords="1,224.05,481.08,10.59,8.07" target="#b34">35]</ref>.</p><p>In the faceted blog distillation task of the Blog track, we investigate how machine learning techniques can be used to address faceted blog ranking. In particular, on top of a Voting Model-based blog retrieval system, we devise a large set of features, and investigate the effectiveness of formulating the faceted blog distillation problem as a text classification or a learning-to-rank problem.</p><p>For the Blog track top news stories identification task, we identify the most important headlines for each day, by using the Voting Model. In particular, we believe that the number of blog posts mentioning a headline (aka votes) is a good indicator of the importance of each headline. However, as the blogosphere exhibits a bursty nature, we examine how to make use of the fact that important headlines can persist over a period of days. Lastly, we identify a set of novel yet relevant blog posts for each headline, by diversifying these blog posts based on temporal distance or content similarity.</p><p>In the Entity track, we extend the Voting Model to the task of finding related entities, by considering the co-occurrence of query terms and candidate entities in a document as a vote for the strength of the relationship between these entities and the query entity. In addition, we experiment with novel graph-based techniques, in order to promote entities associated to authoritative documents or documents from the same community as the query entity.</p><p>For the diversity task of the Web track, we experiment with our novel xQuAD diversification framework, based on the explicit account of the possible aspects underlying a query, in the form of sub-queries <ref type="bibr" coords="1,360.38,302.63,14.12,8.07" target="#b33">[34,</ref><ref type="bibr" coords="1,376.94,302.63,10.59,8.07" target="#b34">35]</ref>. In particular, we investigate the effectiveness of exploiting query suggestions provided by a major Web search engine as sub-queries within our proposed framework.</p><p>Lastly, in our participations in the Web track adhoc task, the Relevance Feedback track and the Million Query track, we test the effectiveness and efficiency of Terrier on the large-scale ClueWeb09 corpus. In particular, we test and further enhance our MapReducebased indexing implementation in Terrier <ref type="bibr" coords="1,467.22,375.95,14.12,8.07" target="#b26">[27,</ref><ref type="bibr" coords="1,483.78,375.95,10.59,8.07" target="#b27">28]</ref>, and deploy distributed retrieval techniques <ref type="bibr" coords="1,420.06,386.39,9.68,8.07" target="#b5">[6,</ref><ref type="bibr" coords="1,432.66,386.39,11.87,8.07" target="#b31">32]</ref> to permit efficient experimentation on this new, large-scale corpus.</p><p>The remainder of this paper is structured as follows. Section 2 describes the corpora used in our participation, along with the associated indexing and retrieval strategies we employ. Section 3 defines the models we use for retrieval and relevance feedback, and also introduces the Voting Model. Sections 4 and 5 cover our participation in the Blog track faceted blog distillation and top stories tasks, respectively. Our participation in the Entity track is discussed in Section 6. Sections 7 and 8 discuss our work in the adhoc and diversity tasks of the Web track, respectively. Sections 9 and 10 present our hypotheses and results for the Million Query and Relevance Feedback tracks, respectively. Lastly, Section 11 provides concluding remarks and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">INDEXING &amp; RETRIEVAL</head><p>The test collection for the Blog track is the new TREC Blogs08 collection, which is a crawl of the blogosphere over a 54-week period <ref type="bibr" coords="1,342.74,585.35,13.70,8.07" target="#b24">[25]</ref>. During this time, the blog posts (permalinks), feeds (RSS/Atom XML) and homepages of each blog were collected. In our participation in the Blog track, we index only the permalinks component of the collection. In particular, there are approximately 28 million documents in this component.</p><p>For the Entity, Million Query, Relevance Feedback, and Web tracks, the test collection is the new billion document TREC Clue-Web09 collection, which has an uncompressed size of 25TB. <ref type="foot" coords="1,538.08,656.88,2.99,5.38" target="#foot_0">1</ref> We index this collection in two manners. Firstly, the so-called 'category B' subset, containing 50 million English documents, and secondly all 500 million English documents (the 'category A' subset). For indexing purposes, we treat the above two collections in the same way. Using the Terrier IR platform<ref type="foot" coords="2,201.00,145.32,2.99,5.38" target="#foot_1">2</ref>  <ref type="bibr" coords="2,207.00,147.12,13.70,8.07" target="#b28">[29]</ref>, we create contentbased indices, including the document body and title. Each term is stemmed using Porter's English stemmer, and standard English stopwords are removed. In both cases, we use our distributed MapReduce indexing implementation in Terrier <ref type="bibr" coords="2,205.98,189.00,14.12,8.07" target="#b26">[27,</ref><ref type="bibr" coords="2,223.74,189.00,10.59,8.07" target="#b27">28]</ref>. The indexing process is split into many 'map' tasks over the input data, followed by one or more reduce tasks to create the final inverted index shards. In particular, Table <ref type="table" coords="2,152.50,220.32,4.48,8.07" target="#tab_0">1</ref> gives an overview of the map and reduce functions used in our implementation. Each map task takes as input a document to be indexed, and processes that document, building up a miniature inverted index in memory. When memory is exhausted, the mini-inverted index is emitted from the map task to disk, in the form of Term, PostingList tuples. Each reduce task takes as input several posting lists for a given term, and merges these into the final inverted index. Note that the number of reduce tasks defines the number of inverted index shards created. For more details and comparative experiments, see <ref type="bibr" coords="2,202.48,314.52,13.70,8.07" target="#b26">[27]</ref>.</p><p>We use a distributed version of Terrier to speed up retrieval for large corpora. In particular, we use distributed retrieval for retrieving documents from the ClueWeb09 category A corpus (500 million documents). Following <ref type="bibr" coords="2,161.76,356.40,9.68,8.07" target="#b5">[6,</ref><ref type="bibr" coords="2,175.32,356.40,10.59,8.07" target="#b31">32]</ref>, our system uses one query server to serve results from one or more document-partitioned index shards, while a centralised query broker is responsible for passing the query to each query server, and merging the results. Moreover, the process for each query follows two phases. Firstly, the query is tokenised, and each term is passed to the query servers to obtain their local statistics for the term. These local statistics are merged by the broker, so that accurate global statistics are obtained. In the second phase, the query servers score and rank their documents, making use of the global statistics. Finally, the documents from each query server are merged into a single ranking by the broker. During merging, no score normalisation is necessary, as the retrieval approach applied by each query server is identical, using exactly the same global statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODELS</head><p>The main weighting model used in our TREC 2009 participation is the DPH model, which is derived from the Divergence From Randomness (DFR) framework <ref type="bibr" coords="2,153.84,549.60,9.51,8.07" target="#b0">[1]</ref>. Using DPH, the relevance score of a document d for a query Q is given by <ref type="bibr" coords="2,195.84,560.16,9.70,8.07" target="#b1">[2]</ref>:</p><formula xml:id="formula_0" coords="2,57.96,574.28,389.43,46.67">score(d, Q) = X t∈Q qtw(1 -F ) 2 tf + 1 • `tf • log 2 (tf • avg l l N T F ) + 0.5 • log 2 (2π • tf • (1 -F ))<label>(1)</label></formula><p>where F is given by tf /l, tf is the within-document frequency, and l is the document length in tokens. avg l is the average document length in the collection, N is the number of documents in the collection, and T F is the term frequency in the collection. Note that DPH is a parameter-free model, and therefore requires no particular tuning. qtw is the query term weight and is given by qtf /qtfmax, where qtf is the query term frequency and qtfmax is the maximum query term frequency among all query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Terms Dependence in the Divergence From Randomness Framework</head><p>Taking into account the dependence and proximity of query terms in documents can increase adhoc retrieval effectiveness. To this end, we use an extension of the DFR framework that can account for the dependence of query terms in documents <ref type="bibr" coords="2,494.70,113.52,14.12,8.07" target="#b18">[19,</ref><ref type="bibr" coords="2,511.62,113.52,10.59,8.07" target="#b29">30]</ref>. In general, when using a term dependence model, the score of a document d for a query Q is given as follows:</p><formula xml:id="formula_1" coords="2,327.36,151.87,228.58,21.82">score(d, Q) = X t∈Q qtw • score(d, t) + X p∈Q 2 score(d, p)<label>(2)</label></formula><p>where score(d, t) is the score assigned to a query term t in the document d, p corresponds to a pair of query terms, and Q2 is the set that contains all possible combinations of two query terms.</p><p>In Equation ( <ref type="formula" coords="2,365.37,214.20,3.17,8.07" target="#formula_1">2</ref>), P t∈Q qtw • score(d, t) can be estimated by any DFR weighting model, such as DPH. The score(d, p) of a pair of query terms in a document is computed as follows:</p><formula xml:id="formula_2" coords="2,364.44,253.92,188.03,15.31">score(d, p) = -log 2 (Pp1) • (1 -Pp2) (<label>3</label></formula><formula xml:id="formula_3" coords="2,552.47,254.52,3.48,8.07">)</formula><p>where Pp1 is the probability that there is a document in which a pair of query terms p occurs a given number of times. Pp1 can be computed with any randomness model from the DFR framework, such as the Poisson approximation to the Binomial distribution. Pp2 corresponds to the probability of seeing the query term pair once more, after having seen it a given number of times. Pp2 can be computed using any of the after-effect models in the DFR framework. The difference between score(d, p) and score(d, t) is that the former depends on occurrences of the pair of query terms p, while the latter depends on occurrences of the query term t. This year, for obvious efficiency reasons, we applied the pBiL randomness model <ref type="bibr" coords="2,385.82,387.84,13.70,8.07" target="#b18">[19]</ref>, which does not consider the collection frequency of pairs of query terms. It is based on the binomial randomness model, and computes the score of a pair of query terms in a document as follows:</p><formula xml:id="formula_4" coords="2,342.84,433.32,213.10,53.71">score(d, p) = 1 pf + 1 " -log 2 (l -1)! + log 2 pf ! + log 2 (l -1 -pf )! -pf log 2 (pp)<label>(4)</label></formula><formula xml:id="formula_5" coords="2,433.08,485.59,95.75,18.96">-(l -1 -pf ) log 2 (p ′ p ) "</formula><p>where l is size of document d in tokens, pp = 1 l-1 , p ′ p = 1 -pp, and pf is the frequency of the tuple p, i.e., the number of windows of size ws in document d in which the tuple p occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relevance Feedback</head><p>We use a term weighting model in the context of the Relevance Feedback (RF) track, and also for pseudo-relevance feedback (PRF) and collection enrichment (CE) <ref type="bibr" coords="2,432.02,587.04,14.12,8.07" target="#b17">[18,</ref><ref type="bibr" coords="2,448.58,587.04,11.12,8.07" target="#b20">21,</ref><ref type="bibr" coords="2,462.14,587.04,11.87,8.07" target="#b30">31]</ref> in the Blog track. The central idea behind PRF is to assume that the top documents returned for a query are relevant, while in RF, a few relevant documents are known. We can then learn from these feedback documents to improve retrieval performance through query expansion or term re-weighting. In particular, we apply the Bo1 term weighting model, derived from the DFR framework <ref type="bibr" coords="2,486.52,649.80,9.51,8.07" target="#b0">[1]</ref>. This model is based upon Bose-Einstein statistics and works in a similar fashion to Rocchio's relevance feedback method <ref type="bibr" coords="2,462.90,670.68,13.70,8.07" target="#b32">[33]</ref>. In Bo1, the informativeness w(t) of a term is given by:</p><formula xml:id="formula_6" coords="2,357.84,697.68,194.63,21.19">w(t) = tfx • log 2 1 + Pn Pn + log 2 (1 + Pn) (<label>5</label></formula><formula xml:id="formula_7" coords="2,552.47,704.16,3.48,8.07">)</formula><p>where tfx is the frequency of the term t in the pseudo-relevant set, Pn is given by T F N , T F is the frequency of t in the whole collection, and N is the number of documents in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Voting Model</head><p>The Voting Model <ref type="bibr" coords="3,131.93,111.00,14.87,8.07" target="#b19">[20]</ref> addresses the task of ranking document aggregates instead of individual documents. In TREC 2009, we consider different types of aggregates for specific tasks. In the faceted blog distillation task of the Blog track, blogs are represented by aggregates of blog posts, whereas in the top stories identification task, aggregates of blog posts are used to represent the days in which these blog posts are published. Lastly, in the Entity track, entities are represented by aggregates of the documents in which they occur.</p><p>In all cases, we consider the ranking of documents with respect to the query Q, which we denote R(Q). The intuition is that a document aggregate ranking with respect to Q can be modelled as a voting process, using the retrieved documents in R(Q). Specifically, every document in R(Q) is possibly associated with one or more aggregates, and these associations act as votes for each aggregate to be relevant to Q. The votes for each aggregate are then appropriately combined to form the final ranking, taking into account the number of associated voting documents, as well as their relevance scores. Importantly, this model is extensible and general, and is not collection or topic dependent. It should be noted that, in practice, R(Q) contains only a finite number of top documents, with the size of R(Q) denoted |R(Q)|.</p><p>In <ref type="bibr" coords="3,74.08,341.16,13.70,8.07" target="#b23">[24]</ref>, we defined twelve voting techniques for aggregating votes for candidate experts within the context of the expert search task, inspired by data fusion techniques and social choice theory. In this work, we use two voting techniques, namely Votes, and exp-CombMNZ. In Votes, the score of an aggregate C with respect to a query Q is given by:</p><formula xml:id="formula_8" coords="3,92.64,410.39,200.26,15.31">scoreV otes(C, Q) = |R(Q) ∩ prof ile(C)|<label>(6)</label></formula><p>where |R(Q) ∩ prof ile(C)| is the number of documents from the profile of the aggregate C that are in the ranking R(Q).</p><p>The robust and effective expCombMNZ voting technique ranks aggregates by considering the sum of the exponential of the relevance scores of the documents associated with each aggregate. Moreover, it includes a component which takes into account the number of documents in R(Q) associated to each aggregate, hence explicitly modelling the number of votes made by the documents for each aggregate. In expCombMNZ, aggregates are scored as:</p><formula xml:id="formula_9" coords="3,77.28,529.19,215.62,35.58">score expCombM NZ (C, Q) = |R(Q) ∩ prof ile(C)| • X d ∈ R(Q)∩ prof ile(C) exp(score(d, Q)) (7)</formula><p>where score(d, Q) is the score of document d for query Q, as given by a standard weighting model, such as DPH (Equation ( <ref type="formula" coords="3,258.17,585.00,3.14,8.07" target="#formula_0">1</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BLOG TRACK: FACETED BLOG DISTILLATION TASK</head><p>In the faceted blog distillation task, the goal is to produce a ranking of blogs for a given query that have a recurrent interest in the topic of the query, and that also fulfil a required facet. In particular, three facets are considered in this task: indepth, opinionated, and shallow <ref type="bibr" coords="3,99.36,679.80,13.70,8.07" target="#b24">[25]</ref>. For each query, participants are required to provide a baseline ranking, and two rankings fulfilling the possible inclinations of the facet associated to the query. For instance, for an indepth-related query, besides a baseline ranking, participants should produce a second ranking, aimed at favouring indepth blogs, and a third ranking, aimed at favouring shallow blogs.</p><p>In TREC 2009, we deploy different machine learning techniques in order to identify blogs fulfilling a desired facet inclination, from a baseline ranking produced by the Voting Model. In particular, we investigate both traditional text classification techniques <ref type="bibr" coords="3,531.48,109.92,14.87,8.07" target="#b35">[36]</ref> as well as a state-of-the-art learning-to-rank technique <ref type="bibr" coords="3,501.78,120.36,15.16,8.07" target="#b38">[39]</ref> in order to produce targeted rankings for each inclination.</p><p>Our first approach to this task builds upon traditional text classification. By using four different classifiers, we estimate the extent to which a given blog matches the facet inclination of interest. In particular, we use the following classifiers: Naive Bayes, a decision tree learner (J48), logistic regression, and a Support Vector Machine (SVM) classifier <ref type="bibr" coords="3,414.01,193.67,13.70,8.07" target="#b9">[10]</ref>. The classifier's confidence in the classification of a blog to a particular inclination is then integrated with the baseline relevance score using FLOE <ref type="bibr" coords="3,488.08,214.55,9.51,8.07" target="#b8">[9]</ref>. In our second approach, we employ the AdaRank <ref type="bibr" coords="3,443.83,225.00,14.87,8.07" target="#b38">[39]</ref> learning-to-rank algorithm to produce a ranking model for each inclination.</p><p>To enable both approaches, we devise a set of 18 features, calculated from individual blog posts as well as entire blogs, for the facets considered in this task. For example, intuitively, long posts or sentences should reflect a more indepth blog, whereas having only a single author or having offensive words should likely constitute positive indicators of a personal blog. Additionally, for the opinionated facet, we repurpose our effective post-level opinion detection techniques <ref type="bibr" coords="3,384.96,319.19,14.12,8.07" target="#b11">[12,</ref><ref type="bibr" coords="3,402.00,319.19,10.59,8.07" target="#b12">13]</ref>, deployed in previous Blog tracks <ref type="bibr" coords="3,541.68,319.19,14.12,8.07" target="#b10">[11,</ref><ref type="bibr" coords="3,316.80,329.63,10.59,8.07" target="#b13">14]</ref>, in order to produce blog-level opinion features. Despite being motivated by our intuitions regarding specific facet inclinations, we do not restrict the use of these features to the identification of blogs fulfilling these inclinations, but instead let our deployed approaches decide whether and how to use each feature. Additionally, for our learning-to-rank approach, negated versions of all features are also considered, so as to allow the learner to decide whether a highly weighted feature should be considered a positive or a negative indicator of a particular facet inclination (for instance, is a long average sentence length a good or bad feature for an indepth blog?). Finally, both text classification and learning-to-rank approaches are trained using a few annotated examples of blogs that fulfil each facet inclination, gathered from the TREC Blogs06 collection <ref type="bibr" coords="3,504.49,455.15,13.70,8.07" target="#b21">[22]</ref>.</p><p>We submit four runs to the faceted blog distillation task, as described next and summarised in Table <ref type="table" coords="3,461.32,476.03,3.34,8.07" target="#tab_3">2</ref>. All runs use the DPH weighting model (Equation ( <ref type="formula" coords="3,418.91,486.47,3.36,8.07" target="#formula_0">1</ref>)) and the expCombMNZ voting technique (Equation ( <ref type="formula" coords="3,379.68,497.03,3.36,8.07">7</ref>)) to create an initial ranking of blogs, which are then re-ranked to match a particular facet inclination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">uogTrFBNclas uses the confidence scores provided by a naive</head><p>Bayes classifier to re-rank blogs for each facet inclination.</p><p>2. uogTrFBMclas is similar to uogTrFBNclas, except that it uses the scores provided by the best (rather than a single) of our considered classifiers on a per-facet inclination basis, according to their performance on the training data.</p><p>3. uogTrFBAlr uses the AdaRank algorithm to learn a different ranking model for each facet inclination.</p><p>4. uogTrFBHlr is similar to uogTrFBAlr, but uses intuitively set feature weights for each facet inclination, as a baseline.</p><p>Table <ref type="table" coords="3,348.24,658.91,4.48,8.07" target="#tab_2">3</ref> shows the results of our submitted runs for each of the facets of interest. Performance is given in terms of mean average precision (MAP) on a per-facet inclination basis. Additionally, the performance of our baseline ranking for each inclination is also shown. Unfortunately, we had an oversight on the configuration of this baseline, which used an extremely large |R(Q)|, markedly Indepth <ref type="bibr" coords="4,196.23,55.04,10.59,7.17" target="#b17">(18</ref>    compromising the performance of our submitted runs. Hence, in Table <ref type="table" coords="4,75.24,388.32,3.34,8.07" target="#tab_2">3</ref>, besides the performance of each run, with |R(Q)| = 20,000, we include an additional row showing its attained performance after correcting the baseline ranking, with |R(Q)| = 1,000. Additionally, in order to assess the impact of the used training data, Table <ref type="table" coords="4,271.67,419.64,4.48,8.07" target="#tab_2">3</ref> also includes a row with the performance of our runs when overfitted using the relevance assessments for this task. <ref type="foot" coords="4,214.92,438.84,2.99,5.38" target="#foot_2">3</ref>From Table <ref type="table" coords="4,106.82,451.08,3.34,8.07" target="#tab_2">3</ref>, we first observe that the performance of our submitted runs is above median across most settings. Moreover, when the corrected runs are considered, improvements in terms of baseline performance are observed across all settings. The inclinationspecific performance of these runs, in turn, increases across most settings, with the second inclination of the personal facet being the only exception. Nevertheless, even after correcting our baseline, re-ranking it in order to favour blogs fulfilling a desired facet inclination remains challenging. We hypothesise that this is partially due to the insufficient training data we had available. Indeed, when the overfitted runs are considered, a more comparable performance to that of our baseline ranking is observed for most settings. As for the deployed approaches themselves, the classification-based runs performed generally better for the first inclination of the indepth and personal facets, as well as for both inclinations of the opinionated facet, whereas our approach based on learning-to-rank was generally the best for the remaining settings. Overall, our results attest the difficulty of the task <ref type="bibr" coords="4,156.00,628.92,13.70,8.07" target="#b24">[25]</ref>, but they also show some promising directions for improvement. In particular, the availability of suitable training data should allow us to better estimate the usefulness of different features in discriminating between blogs fulfilling different facet inclinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BLOG TRACK: TOP STORIES IDENTIFICATION TASK</head><p>In the top stories identification task, the goal is to produce a set of important headlines (from an editorial perspective) and associated blog posts in relation to a day of interest. In particular, the task involves, for each query day, finding the most important headlines for that day, and then selecting ten relevant and diverse blog posts for each of those headlines <ref type="bibr" coords="4,417.98,355.80,13.70,8.07" target="#b24">[25]</ref>. We divide the problem into two distinct sub-tasks: headline ranking, the ranking of top headlines for the query day; and blog post selection, where we select a diverse set of top blog posts pertaining to a headline.</p><p>For our participation in this task, we investigate the application of the Voting Model <ref type="bibr" coords="4,392.45,408.12,14.87,8.07" target="#b19">[20]</ref> (see Section 3.3) to the headline ranking problem. For blog post selection for a given headline, we explore diversity by promoting relevant yet novel blog posts in the ranking. In particular, we explore both the textual and temporal dissimilarity between blog posts as evidence for diversification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Headline Ranking</head><p>The aim of the headline ranking sub-task is to produce a set of headlines which were deemed, from an editorial perspective, to be important on the query day dQ, using evidence from the blogosphere. Our headline ranking approach is based on the intuition that, on any day, bloggers will create posts pertaining to prominent news stories for that day. We desire to score a headline h for a given query day dQ, which we denote score(h, dQ). Our basic approach uses the Votes voting technique (Equation ( <ref type="formula" coords="4,478.31,554.27,3.36,8.07" target="#formula_8">6</ref>)) to score all headlines published on day dQ ± 1 (to account for the time difference between countries), by counting the number of blog posts mentioning the headline h on query day dQ (i.e. from the ranking of blog posts R(h)). We use DPH (Equation ( <ref type="formula" coords="4,455.17,596.15,3.36,8.07" target="#formula_0">1</ref>)) for ranking blog posts in response to a headline. As suggested in <ref type="bibr" coords="4,462.74,606.60,13.70,8.07" target="#b19">[20]</ref>, we limit the number of retrieved blog posts to |R(h)| ≤ 1000.</p><p>However, blog posts created after the query day dQ may also help to improve the accuracy of our approach. Our intuition is that news stories will often be discussed afterwards for long running, controversial or important unpredictable stories, e.g. the aftermath of a terrorist bombing. Indeed, by taking this evidence into account, we can identify those stories which maintain their interest over time, and as such can be deemed more important. In particular, <ref type="bibr" coords="4,330.62,700.67,14.87,8.07" target="#b15">[16]</ref> suggested that bursts in term distributions could last for a period of time. Hence, in the following, we define two alternative  techniques for calculating score(h, dQ), which leverage the temporal distribution of each headline h over time. In particular, these techniques accumulate vote evidence from the days preceding or following dQ, to 'boost' the score of headlines which retain their importance over multiple days.</p><p>In our first proposed temporal distribution boosting technique, N DayBoost, we linearly combine the scores for the following n days before or after day dQ, as:</p><formula xml:id="formula_10" coords="5,85.92,229.64,206.98,30.10">scoreNDayBoost(h, dQ) = d Q +n X d=d Q |R(h, d)|<label>(8)</label></formula><p>where |R(h, d)| measures the importance of headline h on day d, n is a parameter controlling the number of days before (n &lt; 0) or after (n &gt; 0) dQ to take into account, while d represents any single day. Note that this technique places equal emphasis on all days dwe expect the distribution of |R(h, d)| to peak around day dQ.</p><p>Importantly, this approach can incorporate evidence from multiple days. However, due to the linear nature of the score aggregation, all days are treated equally, when it is intuitive to think that days more distant from dQ will provide poorer evidence.</p><p>To address this, we propose a second temporal distribution boosting technique. In particular, GaussBoost is similarly based upon the intuition that important stories will run for multiple days. However, instead of judging each subsequent day equally, we weight based on the time elapsed from the day of interest dQ, using a Gaussian curve to define the magnitude of emphasis. In this way, we state a preference for stories that were most important around dQ, rather than stories which peaked some time before/after dQ:</p><formula xml:id="formula_11" coords="5,53.76,451.52,244.18,30.10">scoreGaussBoost(h, dQ) = d Q +m X d=d Q Gauss(d -dQ) • |R(h, d)| (9)</formula><p>where m is the maximum number of days before or after dQ to take into account and d -dQ is the number of days elapsed since the day of interest dQ (0 ≤ dQ ≤ m). Gauss(∆d) is the Gaussian curve value for a difference of days ∆d, as given by:</p><formula xml:id="formula_12" coords="5,101.76,535.28,191.03,23.65">Gauss(∆d) = 1 w. √ 2π • exp -(∆d) 2 (2w) 2<label>(10)</label></formula><p>where w defines the width of the Gaussian curve. A smaller w will emphasise stories closer to dQ, while a larger w will take into account stories on more distant days, up to the maximum m days. It should also be noted that the original headlines provided for this task contain many non-news entries (e.g. paid death notices, corrections, etc). We apply a small set of heuristics to the headline corpus beforehand to remove these spurious entries, on the intuition that these headlines can never be deemed important. Furthermore, as a means to counter term sparsity in the headlines, we investigate the usefulness of collection enrichment <ref type="bibr" coords="5,195.74,658.92,14.12,8.07" target="#b17">[18,</ref><ref type="bibr" coords="5,212.06,658.92,11.12,8.07" target="#b20">21,</ref><ref type="bibr" coords="5,225.38,658.92,11.87,8.07" target="#b30">31]</ref> in this domain. Indeed, expanding queries based on a higher quality, external resource has been shown to be more effective than doing so on the local collection, since blog posts are often noisy <ref type="bibr" coords="5,232.94,690.24,13.70,8.07" target="#b13">[14]</ref>. In particular, we enrich each headline from Wikipedia (as extracted from the ClueWeb09 collection) using DPH (Equation ( <ref type="formula" coords="5,222.73,711.24,3.36,8.07" target="#formula_0">1</ref> Bo1 (Equation ( <ref type="formula" coords="5,374.20,267.24,3.36,8.07" target="#formula_6">5</ref>)) to select the top 10 terms for each headline. Our submitted runs are summarised in Table <ref type="table" coords="5,461.70,277.80,3.34,8.07" target="#tab_5">4</ref>. Table <ref type="table" coords="5,348.36,288.24,4.48,8.07" target="#tab_6">5</ref> presents the mean average precision for headline ranking over our four submitted runs. From the results, we see that our baseline (uogTrTsbmmr) voting-based approach provides a strong performance of 0.1731 MAP, which is markedly higher than the median for this task. Indeed, all of our submitted runs comfortably exceed this median. Note that, for our boosting runs (uogTrTswtime and uogTrTSemmrs), we encountered a 'long' to 'int' overflow bug, which affected their performance. Once this was corrected, their performances were comparable to our baseline, as shown in the corrected column of Table <ref type="table" coords="5,427.12,382.32,3.34,8.07" target="#tab_6">5</ref>. Indeed, uogTrTswtime improved upon our baseline ranking, indicating that there is useful evidence which can be leveraged to improve the ranking performance from after the query day. Our best run was that done with collection enrichment using Wikipedia, which indicates that, indeed, term sparsity within headlines is an important factor, and deserves further investigation. Moreover, uogTrTswtime proved to be the best run at the TREC 2009 Blog top stories task <ref type="bibr" coords="5,459.71,455.64,13.70,8.07" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Blog Post Selection</head><p>The goal of the blog selection sub-task is to retrieve a set of ten blog posts for a given headline which are both relevant to this headline, and moreover cover as large a variety of the aspects of this headline as possible. Using DPH, we produce a first ranking of blog posts for each headline. However, there is also additional temporal information which can be exploited to improve upon this initial ranking. During the headline selection sub-task, our approach generates day-oriented blog post rankings for each headline -i.e. for day dQ, the top blog posts (if any) which match each retrieved headline h. We exploit this to create a second, enhanced blog post ranking, by merging some of these day-oriented blog post rankings together, keeping only the top scored results. In particular, we merge the rankings for the day of the headline, with those for the following week. In this way, we restrict the blog posts to be selected to only those in temporal proximity to the query day dQ, on the intuition that these will more likely be relevant, while still bringing potentially novel information as the story develops.</p><p>To diversify either of these two blog post rankings, we then apply one of two re-ranking techniques: diversification through textual dissimilarity and diversification using temporal dissimilarity. For textual dissimilarity, we apply the Maximal Marginal Relevance (MMR) <ref type="bibr" coords="5,347.79,711.24,10.43,8.07" target="#b6">[7]</ref> method. In particular, MMR greedily selects a docu- ment d * from the initial ranking with maximum relevance to the query (headline) and maximum dissimilarity to the previously selected documents (blog posts). The selection criterion used by the MMR algorithm is defined below:</p><formula xml:id="formula_13" coords="6,65.04,216.08,227.74,29.23">d * = arg max d i ∈R\S [λ Sim1(di, h) -(1 -λ) max d j ∈S Sim2(di, dj)]<label>(11)</label></formula><p>where R is a ranked list of blog posts, h is a headline, S is the subset of documents in R already selected, and R \ S is the set difference, i.e, the documents not yet selected. Sim1 is the similarity metric used in document retrieval (i.e. DPH), and Sim2 is the similarity between documents di and dj, which can be computed by the same metric used for Sim1 or a different one. In particular, we use the cosine distance between vector representations of the blog posts di and dj, weighted by DPH.</p><p>For temporal dissimilarity, we develop a novel time-based diversification approach, which exploits the evolution of a story over time. The intuition is that, as the story progresses, different viewpoints will be expressed and new actors will arrive. Hence, to truly provide an overview of a particular story, we hypothesise that blog posts should be selected over time. To promote a wide variety of blog posts over the course of the story, we select blog posts with increasing temporal distance from the headline time. In particular, we incrementally select blog posts published at least 6 hours apart. Our submitted runs are listed in the last column of Table <ref type="table" coords="6,257.70,432.24,3.34,8.07" target="#tab_5">4</ref>.</p><p>Our results are shown in Table <ref type="table" coords="6,180.30,442.68,3.34,8.07" target="#tab_7">6</ref>. We can see that all our results outperform the TREC median by a large margin, with our best run (uogTrTsbmmr) achieving 0.518 α-NDCG@10. Indeed, it was the best top news stories identification task run at TREC 2009 <ref type="bibr" coords="6,74.16,484.44,13.70,8.07" target="#b24">[25]</ref>. Moreover, both maximal marginal relevance (uogTrTsbmmr) and temporal diversification (uogTrTstimes) proved to be effective techniques when applied on our baseline DPH blog post ranking. In contrast, runs using our merged blog post rankings (uogTrTswtime and uogTrTSemmrs) were less effective. However, it is unclear whether their performance is due to the method itself, or to the input data from the headline ranking, which was the subject of the overflow bug mentioned earlier. In point of fact, further investigation confirmed that indeed the input data was to blame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ENTITY TRACK</head><p>In the new Entity track, the goal is to retrieve entities of a particular type (people, organisations, or products) that are somehow related to an input entity in the query <ref type="bibr" coords="6,193.60,627.47,9.51,8.07" target="#b3">[4]</ref>. Our major goal in this track was to extend our Voting Model to the task of finding related entities of the desired type. Our approach builds a semantic relationship support for the Voting Model, by considering the co-occurrences of query terms and entities within a document as a vote for the relationship between these entities and the one in the query. Additionally, on top of the Voting Model, we develop novel techniques to further enhance the initial vote estimations. In particular, we promote entities associated to authoritative documents or documents from the same community as the query entity in the hyperlink structure underlying the ClueWeb09 collection.</p><p>Firstly, in order to identify entities in the category B subset of the corpus, we resort to an efficient dictionary-based named entity recognition approach. 4 In particular, we build a large dictionary of entity names using DBPedia, 5 a structured representation of Wikipedia. Dictionary entries comprise all known aliases for each unique entity, as obtained from DBPedia (e.g., 'Barack Obama' is represented by the dictionary entries 'Barack Obama' and '44th President of the United States'). In order to differentiate between the entity types of interest in this task, DBPedia names are further categorised as people, organisations, or products, based on each entity's category description on DBPedia and several heuristics (for instance, the occurrence of the clue word 'company' is likely to identify organisations). In order to account for people that do not have a Wikipedia page, entries in the produced dictionary are complemented with common proper names derived from the US Census data. 6 After being identified, entity name occurrences in the corpus are recorded in appropriate index structures, so as to make this information efficiently available at querying time. By doing so, a rich profile is built for every unique entity, comprising the documents in which the entity occurs in the corpus.</p><p>Additionally, in order to find the correct homepages for each retrieved entity, we again resort to DBPedia. In particular, for some catalogued entities, DBPedia includes a set of associated documents, which correspond to external (i.e., non-Wikipedia) pages linked to from each entity's Wikipedia page, and are more likely to correspond to the desired homepages for that entity. For entities with no such associated documents and also for non-DBPedia entities, we simply retrieve the top scored documents from the entities' profile as their candidate homepages.</p><p>At querying time, we experiment with different approaches that refine the initial ranking of documents for a given query. Firstly, on top of the DPH weighting model (Equation ( <ref type="formula" coords="6,475.59,402.83,3.14,8.07" target="#formula_0">1</ref>)), we apply the pBiL proximity model (Equation ( <ref type="formula" coords="6,422.44,413.27,3.14,8.07" target="#formula_4">4</ref>)), in order to favour documents in which the query terms occur in close proximity. This can be particularly beneficial, as the queries in this task include named entities. Additionally, in an attempt to promote authoritative homepages at the document ranking level, we integrate a document indegree feature, computed on the hyperlink graph underlying the category B subset of the ClueWeb09 collection. Alternatively, we experiment with a state-of-the-art community detection technique <ref type="bibr" coords="6,512.54,486.47,9.59,8.07" target="#b4">[5]</ref>, in order to favour documents from the same community as those associated to the input entity. By doing so, we expect to promote the entities associated to these documents, with the intuition that these entities are more likely to be related to the input entity. On top of the document ranking produced by either of these techniques, the expCombMNZ voting technique (Equation ( <ref type="formula" coords="6,479.45,549.23,3.36,8.07">7</ref>)) is then applied to produce a ranking of entities, generating the following runs:</p><p>1. uogTrEbl is a baseline run, which applies the DPH weighting model and the expCombMNZ voting technique.</p><p>2. uogTrEpr applies the pBiL proximity model at the document ranking level, with a window size ws = 4.</p><p>3. uogTrEdi integrates the indegree feature to the document ranking using FLOE, with the settings suggested in <ref type="bibr" coords="6,525.14,639.83,9.51,8.07" target="#b8">[9]</ref>.</p><p>4. uogTrEc3 promotes entities associated to documents from the same community as those associated to the input entity. 4 http://alias-i.com/lingpipe 5 http://dbpedia.org 6 http://www.census.gov/genealogy/names/ names_files.html Table <ref type="table" coords="7,85.20,57.60,4.48,8.07">7</ref> summarises our submitted runs, while Table <ref type="table" coords="7,255.90,57.60,4.48,8.07" target="#tab_8">8</ref> presents their results in terms of normalised discounted cumulative gain (NDCG) at R, where R is the number of primary and relevant documents (i.e., homepages), precision at 10 (P@10), and the total number of relevant (rel) and primary (pri) homepages retrieved. From the results in Table <ref type="table" coords="7,159.88,198.24,3.34,8.07" target="#tab_8">8</ref>, we observe that all our runs perform well above the median of the participant groups according to both NDCG@R and P@10. Moreover, all four runs achieved by far the best performance among the TREC participants in terms of the number of relevant and primary homepages associated with the retrieved entities <ref type="bibr" coords="7,117.40,250.56,9.51,8.07" target="#b3">[4]</ref>, hence attesting the strength of our baseline approach. Additionally, the integration of the document indegree feature further improved over our strongly performing baseline in terms of P@10. Moreover, applying proximity at the document ranking level brought improvements in terms of both NDCG@R and P@10, as did our community-based boosting technique. Overall, these results not only attest the effectiveness of our Voting Model extension for this task, but also demonstrate its promise as a general framework for entity-related search tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">WEB TRACK: ADHOC TASK</head><p>In the adhoc task of the Web track, participants aimed to identify topically relevant documents on both the category B (50 million documents) and category A (500 million documents) subsets of the ClueWeb09 corpus <ref type="bibr" coords="7,126.74,510.36,9.51,8.07" target="#b7">[8]</ref>. In this task, we aimed to test our DFR models, and the Terrier IR platform on this larger corpus.</p><p>In particular, we submitted three runs to the adhoc task. Two of these were for category B, one for category A. For all runs, we applied the DPH DFR model (Equation ( <ref type="formula" coords="7,200.99,552.24,3.14,8.07" target="#formula_0">1</ref>)). In particular, the submitted runs, and an unsubmitted baseline are described below:</p><p>• uogTrdph is our unsubmitted baseline, and uses DPH only.</p><p>• uogTrdphP adds the pBiL proximity model (Equation ( <ref type="formula" coords="7,280.28,600.00,3.14,8.07" target="#formula_4">4</ref>)), with window size ws = 4, to the scores generated by DPH.</p><p>• uogTrdphA tests the simple use of anchor text, by uniformly combining scores from content and anchor text indices.</p><p>• uogTrdphCEwP uses collection enrichment (CE) <ref type="bibr" coords="7,263.78,658.92,14.12,8.07" target="#b17">[18,</ref><ref type="bibr" coords="7,281.54,658.92,11.12,8.07" target="#b20">21,</ref><ref type="bibr" coords="7,76.20,669.36,10.59,8.07" target="#b30">31]</ref>, by expanding the queries from documents retrieved only from the Wikipedia portion of ClueWeb09. The Bo1 term weighting model (Equation ( <ref type="formula" coords="7,178.55,690.24,3.36,8.07" target="#formula_6">5</ref>)) is used to weight terms in the pseudo-feedback documents. Additionally, the pBiL proximity model is also applied by this run, with ws = 4.</p><p>A summary of our submitted runs is given in Table <ref type="table" coords="7,513.30,57.60,3.34,8.07" target="#tab_10">9</ref>. Their retrieval performance is provided in Table <ref type="table" coords="7,468.44,68.16,7.41,8.07" target="#tab_11">10</ref>. For the category B runs, we note the following: our DPH weighting model baseline run (uogTrdph) performed well above median; applying collection enrichment and proximity (uogTrdphCEwP) improved upon the baseline; however, the simplistic combination of anchor text with content used by run uogTrdphA was detrimental to retrieval performance. For the category A runs, our retrieval perform ance was roughly median. On a closer inspection of our category A run, we found that it suffered from retrieving many spam Web pages. Hence, in the future, we will investigate the application of techniques to remove spam, and/or identify high quality documents.   Overall, our results in this task show the promise of the Terrier IR platform and the DFR weighting models for larger corpora, even without any training, since our participation in TREC 2009 relied solely on parameter-free models. Additionally, we believe we can enhance our retrieval performance by applying field-based weighting models (e.g. PL2F <ref type="bibr" coords="7,406.83,492.24,13.41,8.07" target="#b18">[19]</ref>), and, particularly on the A subset, developing spam detection techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">WEB TRACK: DIVERSITY TASK</head><p>The goal of the diversity task of the Web track is to produce a ranking of documents that (1) maximises the coverage and (2) reduces the redundancy of the retrieved documents with respect to the possible aspects underlying a query, in the hope that users will find at least one of these documents to be relevant to this query <ref type="bibr" coords="7,543.23,585.72,9.51,8.07" target="#b7">[8]</ref>.</p><p>In our participation in this task, we propose to explicitly take into account the possible aspects underlying a query, in the form of sub-queries <ref type="bibr" coords="7,372.98,617.04,14.12,8.07" target="#b33">[34,</ref><ref type="bibr" coords="7,391.22,617.04,10.59,8.07" target="#b34">35]</ref>. By estimating the relevance of the retrieved documents to individual sub-queries, we seek to produce a re-ranking of these documents that maximises the coverage of the aspects underlying the initial query, while reducing its redundancy with respect to already well covered aspects. In particular, we experiment with our novel framework for search result diversification, called xQuAD (eXplicit Query Aspect Diversification). Given a query Q, and an input ranking R(Q), xQuAD iteratively builds a result ranking S(Q) by selecting, at each iteration, the document d * ∈ R(Q) \ S(Q) with the highest score, as given by:</p><formula xml:id="formula_14" coords="8,65.28,69.56,227.51,26.50">d * = arg max d∈R(Q)\S(Q) r1(d, Q) X Q ′ ∈G(Q) i(Q ′ , Q)r2(d, Q ′ ) m(Q ′ , S(Q))<label>(12)</label></formula><p>where:</p><p>• r1(d, Q) is the relevance of document d with respect to the initial query Q, as estimated by any retrieval approach, such as the DPH document weighting model (Equation ( <ref type="formula" coords="8,260.55,142.68,3.14,8.07" target="#formula_0">1</ref>)),</p><p>• G(Q) is the set of sub-queries Q ′ associated to Q,</p><p>• i(Q ′ , Q) is the estimated importance of the sub-query Q ′ relatively to all sub-queries associated to Q,</p><p>• r2(d, Q ′ ) is the relevance of document d to the sub-query Q ′ , as estimated by any retrieval approach (not necessarily the same used for r1(d, Q ′ )), and</p><formula xml:id="formula_15" coords="8,67.08,242.72,59.15,16.91">• m(Q ′ , S(Q))</formula><p>estimates the amount of information satisfying the sub-query Q ′ present in the documents already selected in S(Q), as a measure of novelty.</p><p>In our experiments, the G(Q) component is based on query suggestions provided by a major Web search engine for each of the TREC 2009 Web track topics. Alternatively, we investigate a new cluster-based query expansion technique aimed at generating subqueries from the target collection itself. In particular, we cluster the top retrieved results for an initial query using the k-means algorithm <ref type="bibr" coords="8,84.14,346.20,13.70,8.07" target="#b25">[26]</ref>, and then generate different sub-queries by expanding the initial query from each individual cluster.</p><p>As for the importance component, i(Q ′ , Q), we propose a simple baseline estimation mechanism, iu(Q ′ , Q), which considers a uniform importance distribution over sub-queries:</p><formula xml:id="formula_16" coords="8,133.08,402.23,159.71,27.20">iu(Q ′ , Q) = 1 |G(Q)| ,<label>(13)</label></formula><p>where |G(Q)| is the number of sub-queries generated for query Q.</p><p>Alternatively, we experiment with biasing the diversification process towards those sub-queries likely to represent more plausible aspects of the initial query. Inspired by a state-of-the-art resource selection technique <ref type="bibr" coords="8,127.80,472.44,13.70,8.07" target="#b36">[37]</ref>, we estimate the relative importance of each generated sub-query, by considering the ranking produced for this sub-query as a sample of the documents it covers in the whole collection. In particular, we estimate the importance ic(Q ′ , Q) of the sub-query Q ′ as:</p><formula xml:id="formula_17" coords="8,53.76,528.20,241.59,37.75">ic(Q ′ , Q) = n(Q ′ ) max Q ′ i ∈G(Q) n(Q ′ i ) 1 n(Q ′ ) X d|r 2 (d,Q ′ )&gt;0 τ -j(d, Q),<label>(14)</label></formula><p>where r2(d, Q ′ ) is as described above, n(Q ′ ) is the total number of results associated with the sub-query Q ′ , n(Q ′ ) corresponds to the number of results associated to Q ′ that are among the top τ ranked results for the initial query Q, with j(d, Q) giving the ranking position of the document d with respect to Q.</p><p>Finally, the novelty component m(Q ′ , S(Q)) is estimated as the number of documents retrieved for the sub-query Q ′ that are among the already selected documents in S(Q).</p><p>In our submitted runs, we use the DPH weighting model to produce the initial baseline ranking, and also the ranking for each identified sub-query, for category A. For category B, DPH is used along with the pBiL proximity model, with a window size ws = 4. On top of the initial baseline, we experiment with the different components of our proposed framework to produce diverse rankings with</p><formula xml:id="formula_18" coords="8,335.64,53.24,201.53,36.81">Run Cat. r {1,2} (d, Q) G(Q) i(Q ′ , Q) uogTrDYScdA A DPH sWQ iu uogTrDPCQcdB B DPH+pBiL cQE iu uogTrDYCcsB B DPH+pBiL sWQ ic</formula><p>Table <ref type="table" coords="8,339.72,102.36,7.91,8.07" target="#tab_0">11</ref>: Submitted runs to the diversity task of the Web track, including the category of each run. τ = 1000 documents, resulting in the following three submitted runs, summarised in Table <ref type="table" coords="8,413.32,150.48,11.37,8.07" target="#tab_0">11:</ref> 1. uogTrDYScdA retrieves documents from the whole of Clue-Web09, and then re-ranks these using query suggestions from a major Web search engine as sub-queries (denoted sWQ), weighted by the iu importance estimator.</p><p>2. uogTrDPCQcdB investigates generating sub-queries from the collection itself, by applying the previously described cluster-based query expansion technique (denoted cQE). The Bo1 term weighting model (Equation ( <ref type="formula" coords="8,486.42,244.68,3.36,8.07" target="#formula_6">5</ref>)) is used to produce sub-queries from each cluster generated by k-means (k = 10) from a baseline ranking of 1000 documents. In our experiments, a maximum of 10 terms are expanded from the 3 highly scored documents in each cluster, so as to form a sub-query. The iu importance estimator is used once again.</p><p>3. uogTrDYCcsB uses the same sub-queries as uogTrDYScdA (i.e., sWQ), but with the importance of each sub-query estimated by our resource selection-inspired technique, ic.</p><p>Table <ref type="table" coords="8,347.52,349.31,8.92,8.07" target="#tab_13">12</ref> shows the performance of our runs in the diversity task, in terms of α normalised discounted cumulative gain (α-NDCG) and intent-aware precision (IA-P). From the table, we observe that all our runs perform well above the median of the TREC participants, for both category A and B settings, and in terms of both measures. Indeed, run uogTrDYCcsB was the best performing among all participating runs for category B, in terms of both α-NDCG and IA-P <ref type="bibr" coords="8,337.27,422.51,9.51,8.07" target="#b7">[8]</ref>. Notwithstanding, there is still scope for improvements, as demonstrated by a further analysis of the individual components underlying our framework, their own performance, and their contribution to the performance of the approach as a whole <ref type="bibr" coords="8,517.40,453.95,13.70,8.07" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Cat. α-NDCG@10 IA-P@10  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">MILLION QUERY TRACK</head><p>In the Million Query track, participants aimed to identify topically relevant documents for many queries, on both the category B (50 million documents) and category A (500 million documents) subsets of the ClueWeb09 corpus. We submitted two runs to the Million query track, to see how the performance of these runs differed from the performance of the equivalent runs submitted to the adhoc task of the Web track. The runs were:</p><p>• uogTRMQdph40 applies DPH on the content-only index (40,000 queries). This run corresponds to run uogTrdph from the adhoc task of the Web track (see <ref type="bibr" coords="8,470.90,711.24,35.61,8.07">Section 7)</ref>.</p><p>• uogTRMQdpA10 applies DPH, combining scores from body and anchor text indices (10,000 queries). This run corresponds to run uogTrdphA from the adhoc task of the Web track (see <ref type="bibr" coords="9,112.94,89.04,35.61,8.07">Section 7)</ref>.</p><p>Table <ref type="table" coords="9,85.44,108.24,8.92,8.07" target="#tab_15">13</ref> summarises the obtained retrieval performance of the runs submitted to the Million Query track. We note that the retrieval performance of both runs are markedly above the median measures, particularly on the statMAP measure. Anchor text makes no marked benefit to eMAP performance, but is detrimental to statMAP performance, mirroring our observations from Section 7.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">RELEVANCE FEEDBACK TRACK</head><p>The aim of the TREC 2009 Relevance Feedback track was to examine the aspects affecting the selection of good feedback documents. In our participation in this track, we focus on the stability of our query expansion Bo1 DFR term weighting model (Equation ( <ref type="formula" coords="9,73.32,343.20,3.36,8.07" target="#formula_6">5</ref>)) across different feedback identification strategies.</p><p>In the first phase of the track, participants submitted 5 Clue-Web09 category B documents to be assessed for each topic. Our first feedback set, ugTr.1, was created using the DLH13 model <ref type="bibr" coords="9,275.76,374.52,13.70,8.07" target="#b20">[21]</ref>. Our second feedback set, ugTr.2, was created using the DPH model (and hence corresponds to run uogTrdph from Section 7). Table <ref type="table" coords="9,283.92,395.52,8.92,8.07" target="#tab_16">14</ref> reports the P@5 performances of our submitted feedback sets, and also of several other phase 2 feedback sets. From the table, we note that the ugTr.1 compared well with the other feedback sets, while ugTr.2 was the best performing of this selection of feedback sets. In the second phase, participants submitted one run for each of the 8 feedback sets assigned to them. We used only the relevant documents from each feedback set, and ranked documents from the category B collection using the DPH document weighting model (i.e. based on the Web track adhoc task baseline uogTrdph). We used the Bo1 term weighting model (Equation ( <ref type="formula" coords="9,227.34,648.48,3.36,8.07" target="#formula_6">5</ref>)) to identify and weight the 10 most informative terms to expand the query with.</p><p>Unfortunately, our second phase relevance feedback runs encountered a bug in our mapping from 'DOCNO' to internal docid, and as a consequence, the actual used feedback documents were incorrect. Hence, in the following, we report the performance of our submitted runs, and the correct retrieval performances. reports the performance of these submitted and corrected runs, 7  for the various feedback sets. In all cases, retrieval performance was detrimentally impacted by the presence of the bug, compared to the corrected runs. Moreover, we note that Bo1 does not appear to favour the feedback sets identified by the DFR weighting models (i.e. ugTr.1 &amp; ugTr.2). Indeed, some of the less accurate feedback sets (see Table <ref type="table" coords="9,407.16,258.00,7.91,8.07" target="#tab_16">14</ref>) perform better overall. This suggests that some of these sets produce better feedback documents for Bo1 than DPH or DLH13 alone <ref type="bibr" coords="9,414.97,278.88,13.70,8.07" target="#b14">[15]</ref>. Indeed, Bo1 performed best using the hit2.1 feedback set, even though hit2.1 performed worse than ugTr.2 (hit2.1 exhibited 37% less P@5 than ugTr.2). Overall, we conclude that our participation in the Relevance Feedback track has facilitated an investigation into the aspects affecting the Bo1 term weighting model, and testified to its suitability for application using various methods for generating feedback sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">CONCLUSIONS</head><p>In TREC 2009, we participated in five tracks, namely the Blog, Entity, Million Query, Relevance Feedback and Web tracks, using our Terrier IR platform. In particular, our participation focused on new applications for the Voting Model, as well as on fresh approaches for search result diversification.</p><p>Our results for the Blog track top news stories identification task are particularly strong. In the faceted blog distillation task, a configuration oversight hindered the retrieval performance of our runs. Nevertheless, our corrected results show very good promise, but also attest that this task remains hard without suitable training data. In the Entity track, our proposed extension to the Voting Model has been shown to provide a very effective framework for tackling the related entity finding task. In the diversity task of the Web track, the new xQuAD framework shows a strong retrieval performance, with promising directions for further improvements.</p><p>Finally, with the advent of several larger test collections, we took the opportunity presented by TREC 2009 to overhaul the Terrier platform, for instance with improved MapReduce indexing, and scalable retrieval. However, a small bug in the improved system affected our submitted Relevance Feedback track runs. On the larger ClueWeb09 category A collection, our runs were affected by the presence of spam in the collection. In the future, we will endeavour to develop spam detection and document quality features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,94.80,120.44,13.26,7.17;7,137.16,120.44,36.81,7.17;7,94.80,129.80,111.36,7.17;7,94.80,138.80,132.16,7.17;7,94.80,147.68,142.71,7.17;7,94.80,156.68,156.72,7.17;7,89.28,176.28,168.08,8.07"><head>Table 7 :</head><label>7</label><figDesc>Submitted runs to the Entity track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.76,55.44,239.42,60.51"><head>Table 1 : Overview of the MapReduce functions used during indexing.</head><label>1</label><figDesc></figDesc><table coords="2,72.96,55.44,197.19,29.31"><row><cell>Stage</cell><cell>Input</cell><cell>Output</cell></row><row><cell>Map</cell><cell>Document</cell><cell>Term, PostingList</cell></row><row><cell cols="2">Reduce Term, list[PostingList]</cell><cell>Inverted index</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,91.32,246.96,366.80,70.49"><head>Table 3 : Per-facet MAP performance: submitted, corrected, and overfitted runs.</head><label>3</label><figDesc></figDesc><table coords="4,91.32,274.04,163.87,43.41"><row><cell>Run</cell><cell>Description</cell></row><row><cell>uogTrFBNclas</cell><cell>DPH+expCombMNZ+Naive</cell></row><row><cell cols="2">uogTrFBMclas DPH+expCombMNZ+BestClass</cell></row><row><cell>uogTrFBAlr</cell><cell>DPH+expCombMNZ+AdaRank</cell></row><row><cell>uogTrFBHlr</cell><cell>DPH+expCombMNZ+Human</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,53.76,329.88,239.14,18.51"><head>Table 2 : Submitted runs to the faceted blog distillation task of the Blog track.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,131.64,110.88,346.39,8.07"><head>Table 4 : Summary of submitted runs to the top stories identification task of the Blog track.</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,226.08,137.96,330.02,581.35"><head>Table 5 : Headline ranking MAP performance of our submit- ted and corrected (where applicable) runs for the top stories identification task of the Blog track.</head><label>5</label><figDesc>)) for retrieval and</figDesc><table coords="5,368.40,137.96,135.45,61.77"><row><cell>Run</cell><cell cols="2">Submitted Corrected</cell></row><row><cell>TREC best</cell><cell>0.2600</cell><cell></cell></row><row><cell>TREC median</cell><cell>0.0400</cell><cell></cell></row><row><cell>uogTrTsbmmr</cell><cell>0.1731</cell><cell>N/A</cell></row><row><cell>uogTrTswtime</cell><cell>0.0795</cell><cell>0.1812</cell></row><row><cell>uogTrTstimes</cell><cell>0.1862</cell><cell>N/A</cell></row><row><cell>uogTrTSemmrs</cell><cell>0.1186</cell><cell>0.1720</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,53.76,54.87,239.28,92.88"><head>Table 6 : Blog post selection performance of our submitted runs for the top stories identification task of the Blog track.</head><label>6</label><figDesc></figDesc><table coords="6,99.12,54.87,148.53,61.94"><row><cell>Run</cell><cell cols="2">α-NDCG@10 IA-P@10</cell></row><row><cell>TREC best</cell><cell>0.7723</cell><cell>0.2758</cell></row><row><cell>TREC median</cell><cell>0.0217</cell><cell>0.0040</cell></row><row><cell>uogTrTsbmmr</cell><cell>0.518</cell><cell>0.168</cell></row><row><cell>uogTrTswtime</cell><cell>0.297</cell><cell>0.094</cell></row><row><cell>uogTrTstimes</cell><cell>0.449</cell><cell>0.155</cell></row><row><cell>uogTrTSemmrs</cell><cell>0.371</cell><cell>0.123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,70.08,355.16,206.57,82.15"><head>Table 8 : Results of submitted runs to the Entity track.</head><label>8</label><figDesc></figDesc><table coords="7,87.72,355.16,171.09,61.77"><row><cell>Run</cell><cell cols="3">NDCG@R P@10 #rel #pri</cell></row><row><cell>TREC best</cell><cell>0.4098</cell><cell>0.3500</cell><cell></cell></row><row><cell>TREC median</cell><cell>0.0751</cell><cell>0.0050</cell><cell></cell></row><row><cell>uogTrEbl</cell><cell>0.2510</cell><cell>0.1050 344</cell><cell>75</cell></row><row><cell>uogTrEpr</cell><cell>0.2662</cell><cell>0.1200 347</cell><cell>79</cell></row><row><cell>uogTrEdi</cell><cell>0.2502</cell><cell>0.1150 343</cell><cell>74</cell></row><row><cell>uogTrEc3</cell><cell>0.2604</cell><cell>0.1200 331</cell><cell>75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="7,316.80,250.92,239.02,144.53"><head>Table 9 : Submitted and unsubmitted runs to the adhoc task of the Web track, including the category of each run.</head><label>9</label><figDesc></figDesc><table coords="7,347.04,296.60,178.46,98.85"><row><cell>B runs</cell><cell cols="3">Submitted statMAP statNDCG</cell></row><row><cell>TREC best</cell><cell></cell><cell>0.4305</cell><cell>0.6091</cell></row><row><cell>TREC median</cell><cell></cell><cell>0.1539</cell><cell>0.2956</cell></row><row><cell>uogTrdph</cell><cell>"</cell><cell>0.1970</cell><cell>0.3096</cell></row><row><cell>uogTrdphA</cell><cell></cell><cell>0.1825</cell><cell>0.3245</cell></row><row><cell>uogTrdphCEwP</cell><cell></cell><cell>0.2072</cell><cell>0.3934</cell></row><row><cell>A runs</cell><cell></cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>TREC best</cell><cell></cell><cell>0.8320</cell><cell>0.7780</cell></row><row><cell>TREC median</cell><cell></cell><cell>0.1600</cell><cell>0.1720</cell></row><row><cell>uogTrdph</cell><cell>"</cell><cell>0.0650</cell><cell>0.0969</cell></row><row><cell>uogTrdphP</cell><cell></cell><cell>0.1600</cell><cell>0.1660</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="7,316.80,407.88,239.27,18.51"><head>Table 10 : Results of submitted and unsubmitted runs to the adhoc task of the Web track.</head><label>10</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="8,316.80,556.80,239.18,18.51"><head>Table 12 : Retrieval performances of our submitted runs to the diversity task of the Web track.</head><label>12</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="9,53.76,238.56,239.22,18.51"><head>Table 13 : Summary of retrieval performance of our submitted Million Query track runs.</head><label>13</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="9,53.76,459.08,239.18,121.15"><head>Table 14 : Retrieval performances of our submitted phase 1 feedback sets, and our phase 2 allocated feedback sets for the Relevance Feedback track.</head><label>14</label><figDesc></figDesc><table coords="9,137.64,459.08,71.42,79.77"><row><cell cols="2">Feedback Set P@5</cell></row><row><cell>ugTr.1</cell><cell>0.320</cell></row><row><cell>ugTr.2</cell><cell>0.504</cell></row><row><cell>CMU.1</cell><cell>0.340</cell></row><row><cell>UMas.1</cell><cell>0.496</cell></row><row><cell>UPD.1</cell><cell>0.460</cell></row><row><cell>YUIR</cell><cell>0.252</cell></row><row><cell>hit2.1</cell><cell>0.320</cell></row><row><cell>ilps.2</cell><cell>0.368</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="9,260.31,55.04,253.62,664.27"><head>Table 15</head><label>15</label><figDesc></figDesc><table coords="9,358.32,55.04,155.61,88.29"><row><cell>Run</cell><cell cols="2">Submitted statMAP eMAP</cell><cell>Corrected statMAP</cell></row><row><cell>ugTr.CMU.1</cell><cell>0.1764</cell><cell>0.0409</cell><cell>0.2492</cell></row><row><cell>ugTr.UMas.1</cell><cell>0.1958</cell><cell>0.0421</cell><cell>0.2373</cell></row><row><cell>ugTr.UPD.1</cell><cell>0.1715</cell><cell>0.0399</cell><cell>0.2325</cell></row><row><cell>ugTr.YUIR</cell><cell>0.1900</cell><cell>0.0460</cell><cell>0.2055</cell></row><row><cell>ugTr.hit2.1</cell><cell>0.1667</cell><cell>0.0414</cell><cell>0.2578</cell></row><row><cell>ugTr.ilps.2</cell><cell>0.1756</cell><cell>0.0407</cell><cell>0.2212</cell></row><row><cell>ugTr.ugTr.1</cell><cell>0.2081</cell><cell>0.0464</cell><cell>0.2240</cell></row><row><cell>ugTr.ugTr.2</cell><cell>0.1810</cell><cell>0.0409</cell><cell>0.2349</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="9,316.80,155.64,239.04,18.51"><head>Table 15 : Retrieval performances of our allocated feedback sets for the Relevance Feedback track.</head><label>15</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.24,711.72,232.06,7.05"><p>http://boston.lti.cs.cmu.edu/Data/clueweb09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,58.32,711.72,118.78,7.05"><p>http://www.terrier.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,58.32,702.24,234.59,8.07;4,53.76,711.24,176.40,8.07"><p>Note that this training regime is not applicable for the uogTrFBHlr run, as it is independent of the used training data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Ben He</rs> for his help with the Relevance Feedback runs, and <rs type="person">Stuart Chalmers</rs> for assisting us in our TREC assessment workload this year. We are also grateful to <rs type="person">Duncan McDougall</rs> for helping us with the extraction and preparation of several features used in the Blog and Entity tracks.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,73.20,70.80,205.67,8.07;10,73.20,81.24,211.50,8.07;10,73.20,91.68,171.56,8.07" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,111.12,70.80,167.75,8.07;10,73.20,81.24,141.81,8.07">Probabilistic Models for Information Retrieval based on Divergence From Randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computing Science, Univ. of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,73.20,103.20,187.72,8.07;10,73.20,113.64,217.08,8.07;10,73.20,124.08,205.88,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,121.08,113.64,169.20,8.07;10,73.20,124.08,90.57,8.07">FUB, IASI-CNR and University of Tor Vergata at TREC 2007 Blog track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ambrosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gaibisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gambosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,179.04,124.08,96.06,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,135.60,219.45,8.07;10,73.20,146.04,207.57,8.07;10,73.20,156.48,20.00,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,222.30,135.60,70.35,8.07;10,73.20,146.04,114.01,8.07">Italian monolingual information retrieval with Prosit</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,202.80,146.04,77.97,8.07">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,167.88,196.75,8.07;10,73.20,178.44,215.82,8.07;10,73.20,188.88,100.04,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,126.86,178.44,148.55,8.07">Overview of the TREC 2009 Entity Track</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,188.88,96.06,8.07">Proceedings of TREC 2009</title>
		<meeting>TREC 2009</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,200.28,213.08,8.07;10,73.20,210.72,212.32,8.07;10,73.20,221.28,109.28,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,73.20,210.72,208.46,8.07">Fast unfolding of community hierarchies in large networks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,82.92,221.28,77.38,8.07">Proceedings of CoRR</title>
		<meeting>CoRR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,232.68,201.79,8.07;10,73.20,243.12,208.82,8.07;10,73.20,253.56,129.92,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,219.45,232.68,55.54,8.07;10,73.20,243.12,208.82,8.07;10,73.20,253.56,53.15,8.07">A case study of distributed information retrieval architectures to index one terabyte of text</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cacheda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,131.88,253.56,20.13,8.07">IP&amp;M</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,265.07,172.33,8.07;10,73.20,275.52,198.91,8.07;10,73.20,285.95,192.20,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,181.22,265.07,64.31,8.07;10,73.20,275.52,198.91,8.07;10,73.20,285.95,75.35,8.07">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,164.88,285.95,96.50,8.07">Proceedings of SIGIR 1998</title>
		<meeting>SIGIR 1998</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,297.47,208.24,8.07;10,73.20,307.92,197.68,8.07;10,73.20,318.35,44.24,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,238.78,297.47,42.66,8.07;10,73.20,307.92,128.37,8.07">Preliminary report on the TREC 2009 Web track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.20,307.92,53.68,8.07;10,73.20,318.35,40.24,8.07">Proceedings of TREC 2009</title>
		<meeting>TREC 2009</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,329.87,199.28,8.07;10,73.20,340.31,202.12,8.07;10,73.20,350.75,100.52,8.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,73.20,340.31,188.42,8.07">Relevance weighting for query independent evidence</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,350.75,96.50,8.07">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,362.15,214.88,8.07;10,73.20,372.59,197.04,8.07;10,73.20,383.15,158.12,8.07" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<title level="m" coord="10,135.46,372.59,134.78,8.07;10,73.20,383.15,108.22,8.07">The WEKA data mining software: an update. SIGKDD Explorations</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,394.55,201.05,8.07;10,73.20,404.99,212.93,8.07;10,73.20,415.43,212.11,8.07;10,73.20,425.99,20.00,8.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,73.20,404.99,212.93,8.07;10,73.20,415.43,118.42,8.07">University of Glasgow at TREC 2007: experiments in Blog and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,207.36,415.43,77.95,8.07;10,73.20,425.99,16.00,8.07">Proceedings of TREC 2007</title>
		<meeting>TREC 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,437.39,204.28,8.07;10,73.20,447.83,205.24,8.07;10,73.20,458.27,20.00,8.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,202.63,437.39,74.85,8.07;10,73.20,447.83,111.06,8.07">Ranking opinionated blog posts using OpinionFinder</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,200.28,447.83,78.16,8.07">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,469.79,196.36,8.07;10,73.20,480.23,186.62,8.07;10,73.20,490.67,100.52,8.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,225.62,469.79,43.94,8.07;10,73.20,480.23,173.64,8.07">An effective statistical approach to blog post opinion retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,490.67,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,502.19,215.47,8.07;10,73.20,512.63,215.13,8.07;10,73.20,523.07,209.29,8.07;10,73.20,533.51,100.04,8.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,73.20,512.63,215.13,8.07;10,73.20,523.07,196.46,8.07">University of Glasgow at TREC 2008: experiments in Blog, Enterprise, and Relevance Feedback tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,533.51,78.07,8.07">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,545.03,207.70,8.07;10,73.20,555.47,100.52,8.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,145.10,545.03,121.75,8.07">Finding good feedback documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,555.47,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,566.87,216.32,8.07;10,73.20,577.43,97.04,8.07" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,121.42,566.87,154.52,8.07">Bursty and hierarchical structure in streams</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,577.43,75.08,8.07">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,588.83,212.98,8.07;10,73.20,599.27,213.63,8.07;10,73.20,609.71,160.88,8.07" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,73.20,599.27,213.63,8.07;10,73.20,609.71,44.58,8.07">TREC-7 Ad-Hoc, High Precision and Filtering experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,134.04,609.71,96.06,8.07">Proceedings of TREC 1998</title>
		<meeting>TREC 1998</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,621.23,197.68,8.07;10,73.20,631.67,205.16,8.07" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,169.64,621.23,101.24,8.07;10,73.20,631.67,88.61,8.07">Improving two-stage ad-hoc retrieval for short queries</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,177.84,631.67,96.50,8.07">Proceedings of SIGIR 1998</title>
		<meeting>SIGIR 1998</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,73.20,643.19,215.92,8.07;10,73.20,653.63,217.21,8.07;10,73.20,664.07,174.47,8.07;10,73.20,674.51,100.04,8.07" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,106.59,653.63,183.81,8.07;10,73.20,664.07,161.46,8.07">University of Glasgow at TREC 2006: experiments in Terabyte and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,73.20,674.51,78.07,8.07">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,57.59,205.35,8.07;10,336.24,68.15,218.39,8.07" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="10,391.56,57.59,127.83,8.07">The Voting Model for People Search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computing Science, Univ. of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,336.24,79.55,219.64,8.07;10,336.24,89.99,202.09,8.07;10,336.24,100.43,218.96,8.07" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,518.53,79.55,37.35,8.07;10,336.24,89.99,202.09,8.07;10,336.24,100.43,103.18,8.07">University of Glasgow at TREC 2005: experiments in Terabyte and Enterprise tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.16,100.43,96.06,8.07">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,111.95,213.03,8.07;10,336.24,122.39,204.63,8.07;10,336.24,132.83,188.94,8.07;10,336.24,143.27,55.52,8.07" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,437.97,111.95,111.30,8.07;10,336.24,122.39,156.05,8.07">The TREC Blogs06 collection: creating and analysing a blog test collection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno>TR-2006-224</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Dept. of Computing Science, Univ. of Glasgow</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. report</note>
</biblStruct>

<biblStruct coords="10,336.24,154.79,207.52,8.07;10,336.24,165.23,152.84,8.07" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,437.97,154.79,105.79,8.07;10,336.24,165.23,36.53,8.07">Key blog distillation: ranking aggregates</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,388.56,165.23,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,176.63,214.12,8.07;10,336.24,187.19,181.58,8.07;10,336.24,197.63,100.72,8.07" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,437.97,176.63,112.39,8.07;10,336.24,187.19,168.53,8.07">Voting for candidates: adapting data fusion techniques for an expert search task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,197.63,78.54,8.07">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,209.03,205.27,8.07;10,336.24,219.47,197.21,8.07" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,483.74,209.03,57.76,8.07;10,336.24,219.47,81.81,8.07">Overview of the TREC 2009 Blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.32,219.47,96.13,8.07">Proceedings of TREC 2009</title>
		<meeting>TREC 2009</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,230.99,192.16,8.07;10,336.24,241.43,198.88,8.07;10,336.24,251.87,77.36,8.07" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,398.61,230.99,129.78,8.07;10,336.24,241.43,129.22,8.07">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,481.44,241.43,53.68,8.07;10,336.24,251.87,55.08,8.07">Proceedings of Berkeley SMSP</title>
		<meeting>Berkeley SMSP</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,263.39,198.87,8.07;10,336.24,273.83,213.68,8.07;10,336.24,284.27,134.71,8.07" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="10,494.96,263.39,40.14,8.07;10,336.24,273.83,156.76,8.07">Comparing distributed indexing: to MapReduce or not?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,505.55,273.83,44.36,8.07;10,336.24,284.27,41.06,8.07">Proceedings of LSDS-IR</title>
		<meeting>LSDS-IR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,295.79,211.56,8.07;10,336.24,306.23,209.64,8.07" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,494.96,295.79,52.84,8.07;10,336.24,306.23,92.60,8.07">On single-pass indexing with MapReduce</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,445.44,306.23,78.28,8.07">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,317.63,218.44,8.07;10,336.24,328.07,183.36,8.07;10,336.24,338.63,196.65,8.07;10,336.24,349.07,91.64,8.07" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="10,374.64,328.07,144.97,8.07;10,336.24,338.63,105.22,8.07">Terrier: a high performance and scalable information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,457.56,338.63,75.34,8.07;10,336.24,349.07,69.33,8.07">Proceedings of OSIR Workshop at SIGIR</title>
		<meeting>OSIR Workshop at SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,360.47,210.92,8.07;10,336.24,370.91,207.42,8.07;10,336.24,381.47,100.52,8.07" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="10,336.24,370.91,193.37,8.07">Incorporating term dependency in the DFR framework</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,381.47,96.50,8.07">Proceedings of SIGIR 2007</title>
		<meeting>SIGIR 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,392.87,205.66,8.07;10,336.24,403.31,218.36,8.07;10,336.24,413.75,53.84,8.07" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="10,440.81,392.87,101.09,8.07;10,336.24,403.31,157.80,8.07">Predicting the Usefulness of Collection Enrichment for Enterprise Search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,510.24,403.31,44.36,8.07;10,336.24,413.75,49.81,8.07">Proceedings of ICTIR 2009</title>
		<meeting>ICTIR 2009</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,425.27,217.41,8.07;10,336.24,435.71,198.03,8.07;10,336.24,446.15,193.13,8.07" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="10,463.88,425.27,89.77,8.07;10,336.24,435.71,198.03,8.07;10,336.24,446.15,64.38,8.07">University of Glasgow at TREC 2004: experiments in Web, Robust and Terabyte tracks with Terrier</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,416.04,446.15,91.27,8.07">Proceedings of the TREC</title>
		<meeting>the TREC</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,457.67,211.28,8.07;10,336.24,468.11,130.12,8.07" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="10,378.64,457.67,155.99,8.07">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,468.11,103.93,8.07">The SMART Retrieval System</title>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,479.51,190.21,8.07;10,336.24,489.95,214.64,8.07;10,336.24,500.51,98.04,8.07" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="10,336.24,489.95,201.03,8.07">Explicit search result diversification through sub-queries</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,500.51,94.04,8.07">Proceedings of ECIR 2010</title>
		<meeting>ECIR 2010</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,511.91,199.97,8.07;10,336.24,522.35,219.08,8.07;10,336.24,532.79,100.59,8.07" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="10,498.37,511.91,37.84,8.07;10,336.24,522.35,205.88,8.07">Exploiting query reformulations for Web search result diversification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L T</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,532.79,96.59,8.07">Proceedings of WWW 2010</title>
		<meeting>WWW 2010</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,544.31,177.40,8.07;10,336.24,554.75,177.56,8.07" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="10,386.28,544.31,127.36,8.07;10,336.24,554.75,49.15,8.07">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,391.32,554.75,70.69,8.07">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,566.27,207.40,8.07;10,336.24,576.71,178.62,8.07;10,336.24,587.15,98.04,8.07" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="10,387.53,566.27,156.11,8.07;10,336.24,576.71,165.64,8.07">Central-Rank-based Collection Selection in uncooperative distributed information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,587.15,94.04,8.07">Proceedings of ECIR 2007</title>
		<meeting>ECIR 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,598.55,192.18,8.07;10,336.24,609.11,210.44,8.07;10,336.24,619.55,188.64,8.07;10,336.24,629.99,126.32,8.07" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="10,336.24,619.55,175.35,8.07">OpinionFinder: a system for subjectivity analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.24,629.99,104.63,8.07">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,336.24,641.39,185.75,8.07;10,336.24,651.95,188.72,8.07" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="10,396.06,641.39,125.93,8.07;10,336.24,651.95,72.72,8.07">AdaRank: a boosting algorithm for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,424.44,651.95,96.50,8.07">Proceedings of SIGIR 2007</title>
		<meeting>SIGIR 2007</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
