<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.01,112.05,425.97,15.12">Ad Hoc and Diversity Retrieval at the University of Delaware</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,85.35,144.53,86.25,10.48"><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.44,144.53,87.08,10.48"><forename type="first">Aparna</forename><surname>Kailasam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.07,144.53,88.70,10.48"><forename type="first">Divya</forename><surname>Muppaneni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.75,144.53,63.67,10.48"><forename type="first">Lekha</forename><surname>Thota</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,450.07,144.53,76.59,10.48"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.01,112.05,425.97,15.12">Ad Hoc and Diversity Retrieval at the University of Delaware</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">85129FA597D24032B9C6C554F63C06D8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is the report on the University of Delaware Information Retrieval Lab's participation in the TREC 2009 Web and Million Query tracks. Our report on the Relevance Feedback track is in a separate document [3].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Preprocessing and Indexing</head><p>We indexed ClueWeb using the Indri retrieval engine <ref type="bibr" coords="1,309.76,289.47,9.96,8.74" target="#b5">[6]</ref>. Due to disk space constraints, we elected to use the Category B subset of 50 million English-language web pages only. We indexed the full documents. We included field information such as title, headings, and bold/italic markup, and dropped script and style tags. We did not index anchor text.</p><p>We used the Krovetz stemmer and a simple stopword list. The stopword list is inadequate for the collection, as it does not include meaningless tokens found very frequently in web pages (e.g. http). Attempts to create a more appropriate list by the submission deadline failed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Distributed Processing</head><p>We built eight separate indexes, each comprising the documents in two subdirectories of the Category B collection. Each index took roughly eight hours to build.</p><p>The eight indexes were distributed over eight dual-processor quad-core compute nodes; on each compute node, an Indri daemon process interfaced with the index. Running a query on the head node involved communicating with each of the eight compute nodes: document term statistics could be computed on nodes themselves, and collection statistics were computed by communication between the nodes. After each node scored the documents in its index, they were returned to the head node for merging and output.</p><p>With this approach, we could run queries in parallel. Though network communication is a bottleneck, caching collection statistics at each node improved performance. There is more computation involved in calculating the document term statistics (compute-bound) than the collection term statistics (mostly IObound), so running eight queries in parallel put us at roughly 40% load over all 64 cores.</p><p>By setting up our index this way, we were able to process a query in about half a second on average. Even very long, complicated queries (the result of query expansion or making use of the Indri query language, as shown in Figure <ref type="figure" coords="1,146.75,562.90,4.43,8.74">1</ref>) ran in less than a second. Thus we were able to run all 40,000 queries for the Million Query Track in a matter of hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">PageRank</head><p>We calculated PageRank <ref type="bibr" coords="1,182.13,621.13,10.52,8.74" target="#b7">[8]</ref> using the web graph information provided with the ClueWeb collection. PageRank calculation was parallelized over 64 cores using Hadoop. We used our own implementation written in python; each iteration took roughly one hour. We completed 10 iterations; while the PageRanks had not yet converged, they did not seem to be changing significantly. We stored the PageRank values in the Indri indexes as document priors, converting each to a log probability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Domain Trust</head><p>We also calculated a simple measure of "trust" in a domain using some publicly-available sendmail and content filtering whitelists and blacklists. Each time a domain appeared on any of our blacklists, we increment its blacklist count b by one. Each time it appeared on any of our whitelists, we incremented its whitelist count w by one. We then assign a trust value w+1 w+b+2 to each domain. Finally, we distribute the domain trust values to the individual web pages in that domain and normalize them so that they sum to one. We take the log and store these in the Indri indexes as document priors.</p><p>The trust is of course limited by the completeness of the blacklists and whitelists. We originally conceived of this as a "spam prior", but we found that the whitelists provided much more information than the blacklists did.</p><p>The 10 most trusted domains were: livejournal.com, homestead.com, suite101.com, live.com, ucla.edu, skyrock.com, stanford.edu, harvard.edu, ca.gov, and army.mil. Some of these are highly trusted solely because of how often they appeared on whitelists, which may actually reflect the opposite of what we intend: livejournal.com may appear on whitelists very often because it actually has more spam than other domains with equivalent numbers of pages. We have not investigated this hypothesis further. Wikipedia was the 289th-most-trusted domain, only because it appeared fewer times on whitelists than other domains-it never appeared on a blacklist.</p><p>The 10 least trusted domains were: top-site-list.com, k2free.com, 1sweethost.com, net.cn, toptenreviews.com, 8n.com, oymap.com, co.cc, com.cn, and skydrive.live.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Web and Million Query Ad Hoc Tasks</head><p>For the ad hoc tasks in the Web and Million Query tracks, we used combinations of the query-likelihood language model (QL), the Markov random field model of term dependencies (DM) <ref type="bibr" coords="2,431.69,591.29,9.96,8.74" target="#b6">[7]</ref>, and pseudo-relevance feedback with relevance models (RM) <ref type="bibr" coords="2,245.02,603.25,9.96,8.74" target="#b4">[5]</ref>. We also used two document priors: the PageRank (PR) and domain trust (SP) priors described above. Table <ref type="table" coords="2,290.21,615.20,4.98,8.74" target="#tab_0">1</ref> shows all of our ad hoc runs based on combining these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Tuning Parameters</head><p>Query likelihood has a Dirichlet smoothing parameter µ. The dependence model has three parameters reflecting the relative weight of the original query, query terms appearing in ordered phrases, and query #weight(0.75 #combine( #prior(pagerank) #weight( 0.8 #combine( obama family tree ) 0.1 #combine( #1( family tree ) #1( obama family ) #1( obama family tree ) ) 0.1 #combine( #uw8( family tree ) #uw8( obama tree ) #uw8( obama family ) #uw12( obama family tree ) ))) 0.25 #weight( 0.147 family 0.124 on 0.112 obama 0.106 tree 0.106 2008 0.102 trivia 0.079 search 0.078 1 0.077 news 0.069 comment) )</p><p>Figure <ref type="figure" coords="3,128.15,213.58,3.87,8.74">1</ref>: Automatically-generated QL+DM+RM+PR query for topic #1 "obama family tree".</p><p>terms appearing in unordered windows. The relevance model has four: number of top-ranked documents to use, number of terms to include in the model, smoothing with collection, and smoothing with original query.</p><p>To set these, we did a simple parameter sweep, running the 784 queries from the 2008 Million Query track on a GOV2 index <ref type="bibr" coords="3,154.54,280.55,9.96,8.74" target="#b0">[1]</ref>. We evaluated using the Million Query evaluation measure expected MAP (eMAP) and chose the parameters with greatest eMAP. We did not have time for a full parameter sweep, so we tried a few values that have been shown to work well in published work along with some other values further away. This gave us a very rough map of the parameter space that we can use to do more detailed tuning in the future.</p><p>Figure <ref type="figure" coords="3,117.31,340.33,4.98,8.74">1</ref> shows a typical example of our longest query type: the QL+DM+RM+PR model post-expansion. All other types are essentially formed from subtrees of this type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Web Diversity Task</head><p>We compared two basic models for diversity retrieval: a simple similarity-based pruning method and a more complicated method that probabilistically models "facets" or subtopics (FM) <ref type="bibr" coords="3,416.96,410.36,9.96,8.74" target="#b1">[2]</ref>. The subtopic models in FM can be estimated in different ways; we tried an approach based on document relevance models (RM) and one based on link graphs induced from ranked results (WG). Table <ref type="table" coords="3,387.06,434.27,4.98,8.74" target="#tab_0">1</ref> shows these three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Greedy Result Set Pruning</head><p>Greedy result set pruning is a method for pruning documents from a baseline ranking that are likely to overlap in content with other documents in the same ranking. First, the documents are ranked in order of their query-likelihood score. Second, documents that are most similar to previously ranked documents are pruned. Here, we simply step down the ranked list of documents and prune documents with similarity greater than some threshold θ, i.e at rank i, we remove any document D j , j &gt; i, with sim(D j , D i ) &gt; θ. The similarity score is cosine similarity with TF-IDF term weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Probabilistic Set-Based Approach</head><p>In the set-based approach we retrieve a set of documents that maximizes the likelihood of capturing all the facets. Consider a set of subtopics F of size m. Then the probability that all of the subtopics are captured by a set of documents D of size n is:</p><formula xml:id="formula_0" coords="3,147.94,624.24,316.12,64.87">P (F ∈ D) = m j=1 P (F j ∈ D) = m j=1 P (F j ∈ D 1 ∨ F j ∈ D 2 ∨ • • • ∨ F j ∈ D n ) = m j=1 1 - n i (1 -P (F j ∈ D)).</formula><p>Note we have assumed that facets occur in documents independently. A diversity retrieval system using this model must do three things. First, it must hypothesize a set of subtopics F . Second, for each subtopic F j , it must estimate the probability P (F j ∈ D i ) that it occurs in each document D i . Third, it must have a way to select the smallest set of documents that is most likely to contain all the subtopics.</p><p>We have two separate ways to hypothesize subtopics and estimate containment probabilities; they are described below. Given all the containment probabilities, we select the set of documents to rank by taking max i P (F j ∈ D i ) for each subtopic F j ; we then rank these in decreasing order of their original query-likelihood scores. This can be seen as an approximation to maximizing the likelihood of P (F ∈ D) <ref type="bibr" coords="4,460.34,158.84,9.96,8.74" target="#b1">[2]</ref>.</p><p>Relevance Models. A relevance model is a distribution of words P (w|R) estimated from a set of relevant or retrieved documents. We will model diversity by estimating m subtopic models P (w|F 1 )..P (w|F m ) from a set of retrieved document using the RM2 approach described by Lavrenko and Croft <ref type="bibr" coords="4,447.86,204.67,9.96,8.74" target="#b4">[5]</ref>. If we knew which documents were relevant to each subtopic, we would estimate each model as:</p><formula xml:id="formula_1" coords="4,173.67,239.32,264.66,22.28">P (w|F j ) ∝ P (w) f k ∈Fj Di∈D F j P (f K |D i )P (w|D i )P (D i )/P (w)</formula><p>where D Fj is the set of documents relevant to subtopic F j , f k are the terms in the subtopic documents, P (w) = Di∈D F j P (w|D i )P (D i ), and P (w|D i ) is a smoothed estimate. Since we do not know the relevant documents, we estimate the model from the retrieved documents. Specifically, m models are obtained from the top m retrieved documents by taking each document along with its k nearest neighbors.</p><p>Given these relevance models, we estimate containment probabilities P (F j ∈ D i ) by transforming P (D i |F j ) to a binary probability between 0.25 and 0.75 by a simple linear mapping.</p><p>Web Graph Model. Network structure can provide information about the topical content of documents: documents that have dense linkings among each other are more likely to be relevant to a subtopic than other documents outside that subgraph. In this method we are interested in finding several densely linked subsets of documents in the results of an initial query-likelihood retrieval. Each subset could potentially cover a different subtopic for a given query.</p><p>The top k initial results are represented as a directed graph G = (V, E): nodes correspond to pages and a directed edge (p, q) ∈ E correspond to the presence of link from p to q. We expand the subgraph to include all the in-links and out-links to and from the subgraph. The hubs and authorities scores are calculated using the iterative procedure described by Kleinberg <ref type="bibr" coords="4,278.09,464.81,9.96,8.74" target="#b3">[4]</ref>.</p><p>We then build m models from the set of documents from the top m -1 non-principle eigenvectors and the principle eigenvector. Subtopic models are built from the set of documents as done in the relevance modeling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ad Hoc Results</head><p>Table <ref type="table" coords="4,98.73,575.71,4.98,8.74" target="#tab_1">2</ref> shows ad hoc results (precision at 10 from trec eval and two MAP estimates) for all of our Web and MQ track runs (including diversity runs). The three measures have a fair amount of agreement: dependence modeling and the domain trust prior are useful; relevance modeling is useful in conjunction with dependence modeling but much less so on its own; the PageRank prior actively hurts results; diversity retrieval methods do not do well at ad hoc retrieval. There is one obvious outlier: the DM+RM+PR run is far overrated by eMAP compared to P10 and statMAP. Based on the individual DM, RM, and PR results, we are inclined to believe that P10 and statMAP underrated this run's performance while eMAP overrated it, and it should probably by somewhere in the middle. Some caveats about this  <ref type="table" coords="5,100.84,380.00,3.87,8.74">3</ref>: Million Query results broken out by baseline queries (that we contributed judgments to) and reusability queries (that we did not). There were 405 baseline queries and 186 reusability queries.</p><p>statMAP and P10 estimates change substantially from the first 50 queries (which averaged 262 category B judgments each) to the remaining 39,950 (which averaged 50 category B judgments each); since we did not do anything different for those queries, we must conclude that this is a function of having fewer judgments available to compute estimates. The eMAP estimates are virtually the same in the two groups. P10 results should be taken with a grain of salt, as not all documents in the top 10 were judged (this is especially true of the Million Query queries). In particular, it is strange that the diversity run udelSimPrune did the best of all runs by precision at 10, but was among the worst by MAP estimates. Nevertheless, it seems clear that the DM+RM+SP model is best among these, and that DM and SP are both generally useful for retrieval in the general web. All measures agree that the diversity runs performed poorly, which is not unexpected considering they were not designed to perform ad hoc retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Million Query Results</head><p>The Million Query track included a reusability study in which sites were held out of a subset of queries. Evaluation results over those queries may differ from evaluation results over the queries the site contributed to. Table <ref type="table" coords="5,115.17,600.13,4.98,8.74">3</ref> shows results for our five MQ runs broken out by "baseline" queries that we contributed to (405 queries total) and "reusability" queries that we did not (186 queries total). The ranking of systems changes slightly between the two, but not in a way that would refute our conclusions above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diversity Results</head><p>Figure <ref type="figure" coords="5,104.26,670.32,4.98,8.74">2</ref> shows a plot comparing our diversity results to the median for each query. In general they did not perform particularly well. The simple similarity-pruning approach was our best run by αNDCG@10. Figure <ref type="figure" coords="6,103.51,309.86,3.87,8.74">2</ref>: Diversity run results compared to the median performance (black line) for each query. Our three methods seem to have distinct strengths and weaknesses, as evidenced by that fact that they do not coincide very much.</p><p>Diversity results for all 11 runs are shown in Table <ref type="table" coords="6,288.44,365.60,3.87,8.74" target="#tab_3">4</ref>. By αNDCG@10, two of our diversity runs outperformed any of our ad hoc runs. By IA-P10, the similarity-pruning method was in the top 3-the fact that our IA-P10 performance was worse than our αNDCG@10 performance suggests that a small number of intents may have dominated, and that the best ad hoc approaches were picking up on those at least as well as the diversity approaches. Despite not performing well compared to the median for the task, generally performed well compared to ad hoc runs, suggesting that even "broken" approaches to diversity can improve on the diversity provided by traditional retrieval methods. <ref type="foot" coords="6,295.30,435.75,3.97,6.12" target="#foot_0">1</ref>It is interesting that our diversity methods did so well compared to the ad hoc methods even though they found many fewer relevant documents (as the ad hoc results show). This suggests to us that there is a great deal of redundancy in this corpus, and that novelty and diversity are therefore very worthy subjects of future research.</p><p>However, we note that few of the differences are statistically significant. Among the diversity runs, only the 50% improvement from udelFMRM (αNDCG10 0.126) to udelSimPrune (αNDCG10 0.189) is significant by a paired t-test at the α = 0.05 level. The 33% improvement from udelFMRM to udelFMWG is not significant. This is due to very high variance in per-query performance. When all 11 runs are considered together (and the p-values adjusted according to Tukey's Honest Significant Differences (HSD) procedure to ensure a family-wise false positive rate of 0.05), there is not enough evidence to truly find any of the pairwise differences significant. The same is true of the IA-P10 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Diversity for Relevance Feedback</head><p>Though our main Relevance Feedback track experiments are described in a separate paper <ref type="bibr" coords="6,489.89,613.12,9.96,8.74" target="#b2">[3]</ref>, we will briefly describe one diversity-related experiment we did for that track. One of our Phase 1 submissions-the udel.2 run-used our similarity-based pruning method to try to find five documents that were likely to be on different topics. The hypothesis is that documents that distinguish between query intents may provide better feedback. We believe we can count that hypothesis as falsified (to the extent that similarity pruning does what it's supposed to): the udel.2 judgments provided better results than other judgments only 19% of the time, compared to our udel.1 judgments that outperformed others 82% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Based on our results, we draw the following conclusions:</p><p>• standard retrieval models can work reasonably well on the general web;</p><p>• even computationally-intensive models like relevance models can be computed reasonably quickly with distributed processing;</p><p>• PageRank does not seem to be a useful feature, but some measure of domain trust does;</p><p>• ad hoc retrieval methods do not generally work well for diversity retrieval, and vice versa;</p><p>• decent diversity results do not seem to rely on finding a lot of relevant material;</p><p>• a simple similarity-pruning approach seems to give decent diversity results.</p><p>Additionally, the results seem to point to a conclusion about Million Query evaluation methods: eMAP is more robust to differing numbers of relevance judgments than statMAP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,73.56,468.00,186.41"><head>Table 1 :</head><label>1</label><figDesc>Our Web and Million Query runs: methods used, queries completed, task performed, and run name. Note that Web queries 1-50 were numbered 20001-20050 for the Million Query track, but they are the same topics.</figDesc><table coords="2,97.31,73.56,417.38,140.64"><row><cell>method</cell><cell>queries run</cell><cell>task</cell><cell>run name</cell></row><row><cell>query-likelihood (QL)</cell><cell cols="3">1-50; 20051-60000 Million Query ad hoc udelIndri</cell></row><row><cell>QL+dependence model (DM)</cell><cell cols="3">1-50; 20051-60000 Million Query ad hoc udelIndDM</cell></row><row><cell>QL+relevance model (RM)</cell><cell cols="3">1-50; 20051-60000 Million Query ad hoc udelIndRM</cell></row><row><cell>QL+PageRank prior (PR)</cell><cell cols="3">1-50; 20051-60000 Million Query ad hoc udelIndPR</cell></row><row><cell>QL+domain trust prior (SP)</cell><cell cols="3">1-50; 20051-60000 Million Query ad hoc udelIndSP</cell></row><row><cell>QL+DM+RM</cell><cell>1-50</cell><cell>Web ad hoc</cell><cell>udelIndDMRM</cell></row><row><cell>QL+DM+RM+PR</cell><cell>1-50</cell><cell>Web ad hoc</cell><cell>udelIndDRPR</cell></row><row><cell>QL+DM+RM+SP</cell><cell>1-50</cell><cell>Web ad hoc</cell><cell>udelIndDRSP</cell></row><row><cell>QL+similarity pruning</cell><cell>1-50</cell><cell>Web diversity</cell><cell>udelSimPrune</cell></row><row><cell>QL+facet model (FM) with RM</cell><cell>1-50</cell><cell>Web diversity</cell><cell>udelFMRM</cell></row><row><cell>QL+FM with web graph (WG)</cell><cell>1-50</cell><cell>Web diversity</cell><cell>udelFMWG</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,671.35,468.00,20.69"><head>Table 2 :</head><label>2</label><figDesc>table: Million Query results are averaged over all judged queries, which may not be entirely appropriate; the next section shows MQ results broken out into two separate groups. Both Ad hoc evaluation results for all ad hoc and diversity runs, sorted by eMAP on queries 1-50. As expected, diversity runs score poorly by ad hoc measures. The DM+RM+SP model seems to be best for queries 1-50, and the DM model is best over all judged queries.</figDesc><table coords="5,277.16,73.56,185.54,8.74"><row><cell>queries 1-50</cell><cell>20001-60000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,146.33,73.56,319.33,162.50"><head>Table 4 :</head><label>4</label><figDesc>Diversity evaluation results for all 11 runs sorted by αNDCG10.</figDesc><table coords="7,197.61,73.56,216.77,140.64"><row><cell>run</cell><cell>task</cell><cell cols="2">αNDCG10 IA-P10</cell></row><row><cell>udelIndRM</cell><cell>ad hoc</cell><cell>0.102</cell><cell>0.056</cell></row><row><cell>udelIndDRPR</cell><cell>ad hoc</cell><cell>0.116</cell><cell>0.049</cell></row><row><cell>udelIndri</cell><cell>ad hoc</cell><cell>0.124</cell><cell>0.061</cell></row><row><cell>udelFMRM</cell><cell>diversity</cell><cell>0.126</cell><cell>0.052</cell></row><row><cell>udelIndDM</cell><cell>ad hoc</cell><cell>0.140</cell><cell>0.073</cell></row><row><cell>udelIndSP</cell><cell>ad hoc</cell><cell>0.141</cell><cell>0.071</cell></row><row><cell>udelIndDRSP</cell><cell>ad hoc</cell><cell>0.141</cell><cell>0.086</cell></row><row><cell>udelIndPR</cell><cell>ad hoc</cell><cell>0.146</cell><cell>0.055</cell></row><row><cell cols="2">udelIndDMRM ad hoc</cell><cell>0.152</cell><cell>0.083</cell></row><row><cell>udelFMWG</cell><cell>diversity</cell><cell>0.168</cell><cell>0.060</cell></row><row><cell>udelSimPrune</cell><cell>diversity</cell><cell>0.189</cell><cell>0.081</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,87.24,667.92,452.76,6.99;6,72.00,677.38,217.05,6.99"><p>Our implementation of the FMRM model had a bug resulting from a misunderstanding of how Indri's distributed processing worked; we believe this bug explains its poor performance.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,87.50,567.94,452.50,8.74;7,87.50,579.90,231.35,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,478.27,567.94,61.73,8.74;7,87.50,579.90,87.99,8.74">Million Query Track 2008 overview</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,196.85,579.90,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.50,599.82,452.50,8.74;7,87.50,611.78,176.67,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,258.18,599.82,281.82,8.74;7,87.50,611.78,34.67,8.74">Probabilistic models of novel document ranking for faceted topic retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,142.89,611.78,89.20,8.74">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.50,631.70,452.51,8.74;7,87.50,643.66,285.37,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,484.79,631.70,55.22,8.74;7,87.50,643.66,142.05,8.74">Minimal test collections for relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aparna</forename><surname>Kailasam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Divya</forename><surname>Muppaneni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lekha</forename><surname>Thota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,250.87,643.66,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.50,663.58,452.50,8.74;7,87.50,675.54,22.69,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,154.74,663.58,220.81,8.74">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,384.09,663.58,83.38,8.74">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,75.16,452.51,8.74;8,87.50,87.11,452.50,8.74;8,87.50,99.07,90.83,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,264.40,75.16,146.97,8.74">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,437.07,75.16,102.93,8.74;8,87.50,87.11,448.66,8.74">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,118.99,452.50,8.74;8,87.50,130.95,329.48,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,253.55,118.99,286.45,8.74;8,87.50,130.95,46.85,8.74">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,142.61,130.95,176.63,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,150.87,452.51,8.74;8,87.50,162.83,452.50,8.74;8,87.50,174.78,133.32,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,248.85,150.87,194.62,8.74">A markov random field for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.25,150.87,76.75,8.74;8,87.50,162.83,452.50,8.74;8,87.50,174.78,34.28,8.74">Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval (SIGIR)</title>
		<meeting>the 28th annual international ACM SIGIR conference on research and development in information retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,87.50,194.71,452.50,8.74;8,87.50,206.67,141.91,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,401.39,194.71,138.61,8.74;8,87.50,206.67,110.98,8.74">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
