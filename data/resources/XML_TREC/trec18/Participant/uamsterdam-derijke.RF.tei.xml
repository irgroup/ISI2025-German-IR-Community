<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.89,83.76,309.95,15.48">Topical Diversity and Relevance Feedback</title>
				<funder ref="#_RAUAaZT">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_4seMXEa #_udA5DXw #_Gm8A6S7 #_Xh5K6Rs #_rjmfnTz">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_5vmSexs">
					<orgName type="full">Dutch and Flemish Governments</orgName>
				</funder>
				<funder ref="#_k3VQVsu">
					<orgName type="full">DAESO</orgName>
				</funder>
				<funder ref="#_keSxNgD">
					<orgName type="full">Dutch Ministry of Education, Culture and Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.74,116.28,58.77,10.75"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.45,116.28,42.85,10.75"><forename type="first">Jiyin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.24,116.28,97.60,10.75"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,381.79,116.28,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.89,83.76,309.95,15.48">Topical Diversity and Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9CE9D0B3F4C16D14FEC9EA8DDA116B63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of the University of Amsterdam's Intelligent Systems Lab in the relevance feedback track at TREC 2009. Our main conclusion for the relevance feedback track is that a topical diversity approach provides good feedback documents. Further, we find that our relevance feedback algorithm seems to help most when there are sufficient relevant documents available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year, the goal of the Relevance Feedback track is to evaluate how well a system can find good documents to serve as input for the relevance feedback algorithm, as well as the improvement gained by the feedback algorithm itself. There are two phases to the track. In the first phase, systems were to return two ranked lists (with a maximum of 5 documents) for each topic. In the second phase, all participating systems were given their own ranked list and a number of ranked lists from other groups from phase 1 and relevance assessments to perform relevance feedback.</p><p>For phase 1 we submitted two distinct runs. The first is based on an approach that attempts to maximize diversity, the second is based on a standard combination of term dependency and pseudo-relevance feedback. For phase 2 we adopt a four-step relevance feedback approach that generates ranked lists of documents for key terms in each judged document. We then combine each of these lists into two rankings (a relevant and a non-relevant one) which we then combine into a final ranking.</p><p>In the next section we first introduce our experimental setup. In Sections 3 and 4 we detail our approaches for phase 1 and 2, respectively, and we end with a concluding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Framework</head><p>We employ a language modeling approach to IR and rank documents by their log-likelihood of being relevant given a query. Without presenting details here, we only provide our final formula for ranking documents, and refer the reader to <ref type="bibr" coords="1,316.81,190.62,77.20,8.64" target="#b0">(Balog et al., 2008)</ref> for the steps of deriving this equation:</p><formula xml:id="formula_0" coords="1,327.76,208.48,228.16,10.37">log P(D|Q) ∝ log P(D) + ∑ t∈Q P(t|θ Q ) • log P(t|θ D ). (1)</formula><p>Here, both documents and queries are represented as multinomial distributions over terms in the vocabulary, and are referred to as document model (θ D ) and query model (θ Q ), respectively. The third component of our ranking model is the document prior (P(D)), which is assumed to be uniform, unless stated otherwise. Note that by using uniform priors, Eq. 1 gives the same ranking as scoring documents by measuring the KL-divergence between the query model θ Q and each document model θ D , in which the divergence is negated for ranking purposes <ref type="bibr" coords="1,401.75,334.57,99.76,8.64" target="#b3">(Lafferty and Zhai, 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling</head><p>Unless indicated otherwise, we smooth each document model using a Dirichlet prior:</p><formula xml:id="formula_1" coords="1,372.61,410.58,183.30,23.95">P(t|θ D ) = n(t, D) + µP(t) ∑ t n(t, D) + µ ,<label>(2)</label></formula><p>where n(t, D) indicates the count of term t in D and P(t) indicates the probability of observing t in a large background model such as the collection:</p><formula xml:id="formula_2" coords="1,369.58,482.18,186.34,22.18">P(t) = P(t|C) = ∑ D n(t, D) |C| .<label>(3)</label></formula><p>Here, µ is a hyperparameter that controls the influence of the background corpus which we set to the average document length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ClueWeb</head><p>We do not use any form of stemming and remove a conservative list of 588 stopwords. We index the headings, titles, and contents as searchable fields and do not remove any HTML tags. For our submitted runs in phase 1 we used the Category B subset of ClueWeb, while for the runs in phase 2 we used Category A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phase 1</head><p>For the first phase we generated two runs based on different approaches. The first run was inspired by our approach runID score ilps.1 0.8281 ilps.2 0.2885</p><p>Table <ref type="table" coords="2,98.76,122.99,3.88,8.64">1</ref>: Results of our submitted runs in phase 1.</p><p>to the diversity task of this year's Web track <ref type="bibr" coords="2,249.52,150.32,43.38,8.64;2,53.80,162.28,21.44,8.64" target="#b2">(He et al., 2010)</ref>, whereas the second run was a standard combination of pseudo-relevance feedback and query modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Diversity</head><p>This run (ilps.1) tries to select documents that reflect different topical facets of a given query for relevance feedback. Intuitively, a query may have different topical facets, where some are relevant while others are non-relevant. From a clustering point of view, a set of documents that are representative for different topical facets would provide more information than documents that all focus on a single topical facet, since we can easily use a "prototypical" model to represent the single-topic set of documents.</p><p>For detecting different topical facets of the documents associated with each topic, we run hierarchical clustering on the top 50 documents from an initial retrieval run. For this kind of clustering one needs to pre-define a cut-off threshold that determines the number of clusters. However, in our scenario, we are not interested in getting a perfect clustering of the documents. Instead, we only want to detect the significant topical facets contained in the documents associated with a particular query. We measure the significance of a cluster with two measures: stability and cluster quality. A cluster is stable when it repeatedly occurs given different cut-off threshold and is of high quality when it results in a high Silhouette value (a measure for the quality of a cluster <ref type="bibr" coords="2,83.88,484.37,75.45,8.64" target="#b8">(Rousseeuw, 1987)</ref>). Additionally, in order to prevent outliers from dominating the top ranked clusters, we also take the cluster size into account. We rank the clusters by combining these scores, i.e., the stability, silhouette values, and cluster size, in a heuristic way by multiplying them. We leave other, more elaborate approaches for future work. Once we have obtained a ranked list of clusters, we select the top scoring documents from each cluster as our ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo Relevance Feedback</head><p>For this run (ilps.2) we apply a standard combination of pseudo relevance feedback and structured query modeling. We first transform each query into a full-dependency query model <ref type="bibr" coords="2,108.52,651.05,104.36,8.64" target="#b7">(Metzler and Croft, 2005)</ref>. We then perform a retrieval run and select the 10 top-ranked documents. From these documents we generate relevance models (RM-1 <ref type="bibr" coords="2,60.70,686.91,107.02,8.64" target="#b4">(Lavrenko and Croft, 2001)</ref>) and keep the 50 terms with the highest probability. We use the expanded query to retrieve our final ranked set of documents. Significance is tested using the Wilcoxon signed rank test against the run without any relevance feedback information (last row). We indicate significant increases (or drops) for p &lt; .01 using (and ) and for p &lt; .05 using (and ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Table <ref type="table" coords="2,341.32,367.88,4.98,8.64">1</ref> shows the aggregate score of our submitted runs for phase 1. We observe that ilps.1 is a better source of feedback documents than most other runs, whereas the opposite is true for ilps.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Phase 2</head><p>The main goal of phase 2 is to see how well each participants' relevance feedback algorithm performs, by running them on a set of 8 baseline runs (constructed in phase 1).</p><p>Using each of the baseline runs and the relevance assessments, we need to identify new relevant documents. Participants were allowed to submit only one run and we suffice by reporting on our approach and its results. Comparing it to other approaches remains a topic for future work.</p><p>The leftmost columns of Table <ref type="table" coords="2,457.33,555.41,4.98,8.64" target="#tab_0">2</ref> lists the baseline runs that we were assigned, and the number of retrieved documents and the number of relevant documents in each run. As can be observed from this table, the information available from just the relevant documents is limited (the best run has 44% of its returned documents judged relevant). We believe that making our feedback approach dependent solely on these few documents is not a good idea and we feel we need to incorporate the non-relevant information as well to obtain the best relevance feedback results.</p><p>The general goal of a relevance feedback algorithm is to extract terms from relevant documents that distinguish them from other, non-relevant documents. One way of approaching this would be to use the non-relevant documents as a language model against which to compare the relevant documents <ref type="bibr" coords="3,84.90,69.23,69.91,8.64" target="#b6">(Meij et al., 2008)</ref>. From this comparison one could, for example, extract terms that distinguish between the relevant and non-relevant documents. Even though this is a valid approach, we feel that in the current situation this approach might not work optimally: first, the total number of judged documents is very limited (maximum of 5 documents per topic), which makes it hard to put confidence in comparing the two sets. Second, for a significant portion of the topics we have neither relevant nor non-relevant documents, and for these cases this approach would not work at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approach</head><p>Building on the observations above, we arrive at the following wish list. First, a sensible approach to feedback should make use of each individual judged document as much as possible. Second, the approach should be able to handle cases in which no relevant or no non-relevant documents are known. Finally, as mentioned before, the approach should take non-relevance into account and not depend on relevant documents only. Based on these requirements we take a four-step relevance feedback approach:</p><p>1. Extract key terms from each individual document.</p><p>2. Use the extracted terms as queries.</p><p>3. Combine the result lists from step 2 in two rankings: a relevant and a non-relevant one.</p><p>4. Combine both rankings from step 3 into a final ranking.</p><p>Below we elaborate on these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Extract key terms and run as queries</head><p>We compare each judged document to a background collection and identify key terms that distinguish this document.</p><p>As background collection we take the full collection and we select only terms that occur at least four times in the document (to avoid selecting infrequent terms and typos). The weights of the resulting terms are normalized, leaving us with a weighted representation (or "query") for each document. We use this query to retrieve a set of new documents. We now have, for each judged document, a ranked list of documents which are highly similar. Examples of two queries, one relevant and one non-relevant, are displayed without their weights in Table <ref type="table" coords="3,178.81,605.96,3.74,8.64">3</ref>. Additionally, we create a baseline ranking based on the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Construct relevant and non-relevant rankings</head><p>We then combine the ranked lists from the previous step into two separate rankings: one for the relevant documents and one for the non-relevant documents. We do so by normalizing the retrieval scores for each topic and ranking using minmax normalization <ref type="bibr" coords="3,130.15,710.82,45.91,8.64" target="#b5">(Lee, 1995)</ref> and use CombMNZ <ref type="bibr" coords="3,257.92,710.82,34.98,8.64">(Fox and</ref> Relevant greyhounds, rescuing, doberman purebred, adoption, shih, collie, rescues Non-relevant adoption, transracial, photolisting Table <ref type="table" coords="3,340.94,108.03,3.88,8.64">3</ref>: Examples of the key terms from a relevant and nonrelevant document for topic RF09-38, "dogs for adoption." <ref type="bibr" coords="3,316.81,152.01,50.43,8.64" target="#b1">Shaw, 1994)</ref> to combine the relevant rankings into one, and the non-relevant rankings into one. We are now left with two new rankings, one being a ranking of relevant documents and the other a ranking of non-relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Construct final ranking</head><p>The final ranking is then constructed from the relevant and non-relevant rankings: we simply subtract the non-relevant score for each document from its relevant score. The idea behind this step is that a document that is ranked high for many relevant documents, but is hardly ever returned for non-relevant documents, receives a high final score. Documents that are mixed, i.e., showing up in both rankings, would get ranked below these documents, and documents that are ranked high in the non-relevant ranking and are nowhere to be found in relevant rankings, drop all the way to the bottom.</p><p>The approach described above fulfills our requirements in that it (i) takes full advantage of each individual document, (ii) can handle cases where no relevant or no non-relevant information is available, and (iii) takes non-relevance into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" coords="3,341.28,470.27,4.98,8.64" target="#tab_0">2</ref> shows the result of applying our relevance feedback algorithm to our assigned input rankings from phase 1. From this table we observe that there seems to be a correlation between the number of relevant documents in the phase 1 ranking and the resulting, final performance. The last row of the table indicates the performance of our system without any relevance feedback information. We note that using relevance feedback information helps in all cases but one. Further, the improvement of applying our relevance feedback algorithm is significant for early precision in most cases. Finally, we observe that the absolute MAP values are quite low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented our approaches to the two phases of this year's TREC relevance feedback track. For phase 1 we found that an approach based on diversity outperforms a standard approach based on pseudo relevance feedback. As to phase 2, we have found that our proposed approach helps most when there are sufficiently many relevant documents in the initial ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,59.27,239.10,210.66"><head>Table 2 :</head><label>2</label><figDesc>Main results of our system for phase 2. The second and third column indicate the number of retrieved documents and the number of relevant documents for each of the baseline runs assigned to us. The rightmost columns contain the resulting performance of applying our feedback algorithm.</figDesc><table coords="2,322.79,59.27,221.27,137.99"><row><cell></cell><cell cols="2">Documents</cell><cell cols="2">IlpsRF</cell></row><row><cell>runID</cell><cell cols="2">retrieved relevant</cell><cell>MAP</cell><cell>P10</cell></row><row><cell>QUT.1</cell><cell>248</cell><cell>36</cell><cell>0.0688</cell><cell>0.1286</cell></row><row><cell>Sab.1</cell><cell>250</cell><cell>98</cell><cell>0.0581</cell><cell>0.0939</cell></row><row><cell>WatS.1</cell><cell>250</cell><cell>110</cell><cell>0.0915</cell><cell>0.2878</cell></row><row><cell>fub.1</cell><cell>250</cell><cell>81</cell><cell>0.0792</cell><cell>0.1694</cell></row><row><cell>ilps.1</cell><cell>250</cell><cell>87</cell><cell>0.0705</cell><cell>0.2020</cell></row><row><cell>ilps.2</cell><cell>250</cell><cell>92</cell><cell>0.0680</cell><cell>0.1898</cell></row><row><cell>twen.1</cell><cell>250</cell><cell>83</cell><cell>0.0776</cell><cell>0.1837</cell></row><row><cell>twen.2</cell><cell>250</cell><cell>71</cell><cell>0.0694</cell><cell>0.1816</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.0639</cell><cell>0.0959</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>This research was supported by the <rs type="funder">DAESO</rs> and <rs type="person">DuO-MAn</rs> project carried out within the <rs type="programName">STEVIN program</rs> which is funded by the <rs type="funder">Dutch and Flemish Governments</rs> under project number <rs type="grantNumber">STE-09-12</rs>, and by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">640. 001.501</rs>, <rs type="grantNumber">640.002.501</rs>, <rs type="grantNumber">612.066.512</rs>, <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.061.815</rs>, <rs type="grantNumber">640.004.802</rs>, and by the <rs type="programName">Virtual Laboratory for e-Science project</rs>, which is supported by a <rs type="grantName">BSIK grant</rs> from the <rs type="funder">Dutch Ministry of Education, Culture and Science</rs> and is part of the <rs type="programName">ICT innovation program</rs> of the <rs type="institution">Ministry of Economic Affairs</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k3VQVsu">
					<orgName type="program" subtype="full">STEVIN program</orgName>
				</org>
				<org type="funding" xml:id="_5vmSexs">
					<idno type="grant-number">STE-09-12</idno>
				</org>
				<org type="funding" xml:id="_RAUAaZT">
					<idno type="grant-number">640. 001.501</idno>
				</org>
				<org type="funding" xml:id="_4seMXEa">
					<idno type="grant-number">640.002.501</idno>
				</org>
				<org type="funding" xml:id="_udA5DXw">
					<idno type="grant-number">612.066.512</idno>
				</org>
				<org type="funding" xml:id="_Gm8A6S7">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_Xh5K6Rs">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_keSxNgD">
					<idno type="grant-number">640.004.802</idno>
					<orgName type="grant-name">BSIK grant</orgName>
					<orgName type="program" subtype="full">Virtual Laboratory for e-Science project</orgName>
				</org>
				<org type="funding" xml:id="_rjmfnTz">
					<orgName type="program" subtype="full">ICT innovation program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,75.32,274.88,66.65,12.90;4,53.80,300.56,239.10,8.64;4,63.76,312.52,229.14,8.64;4,63.76,324.29,202.40,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,268.10,300.56,24.80,8.64;4,63.76,312.52,229.14,8.64;4,63.76,324.47,113.48,8.64">A few examples go a long way: constructing query models from elaborate query formulations</title>
		<author>
			<persName coords=""><forename type="first">References</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,195.73,324.29,39.21,8.59">SIGIR &apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,344.40,239.10,8.64;4,63.76,356.17,229.14,8.82;4,63.76,368.13,229.14,8.82;4,63.76,380.26,48.70,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,197.00,344.40,95.90,8.64;4,63.76,356.35,48.02,8.64">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,135.00,356.17,157.90,8.59">The Second Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,400.19,239.10,8.64;4,63.76,412.14,229.14,8.64;4,63.76,423.92,229.14,8.82;4,63.76,435.88,229.14,8.82;4,63.76,448.01,139.31,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,223.34,412.14,69.56,8.64;4,63.76,424.10,147.62,8.64">Heuristic ranking and diversification of web documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,231.03,423.92,61.87,8.59;4,63.76,435.88,86.41,8.59">Eighteenth Text REtrieval Conference</title>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2009">2010. 2009</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,467.94,239.10,8.64;4,63.76,479.89,229.14,8.64;4,63.76,491.67,120.00,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,188.36,467.94,104.54,8.64;4,63.76,479.89,229.14,8.64;4,63.76,491.85,31.67,8.64">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,113.32,491.67,39.22,8.59">SIGIR &apos;01</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,511.77,239.10,8.64;4,63.76,523.55,142.36,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,208.65,511.77,84.25,8.64;4,63.76,523.73,53.09,8.64">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,135.69,523.55,39.21,8.59">SIGIR &apos;01</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,543.65,239.10,8.64;4,63.76,555.43,216.52,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,115.85,543.65,177.06,8.64;4,63.76,555.61,127.17,8.64">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,209.84,555.43,39.21,8.59">SIGIR &apos;95</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,575.53,239.10,8.64;4,63.76,587.49,229.14,8.64;4,63.76,599.26,229.14,8.82;4,63.76,611.22,229.14,8.82;4,63.76,623.35,80.65,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,281.28,575.53,11.62,8.64;4,63.76,587.49,229.14,8.64;4,63.76,599.44,64.45,8.64">Incorporating non-relevance information in the estimation of query models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,150.60,599.26,142.31,8.59;4,63.76,611.22,18.25,8.59">Seventeenth Text REtrieval Conference</title>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,643.28,239.10,8.64;4,63.76,655.05,203.23,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,200.53,643.28,92.37,8.64;4,63.76,655.23,114.10,8.64">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,196.55,655.05,39.21,8.59">SIGIR &apos;05</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,53.80,675.16,239.10,8.64;4,63.76,686.93,229.14,8.82;4,63.76,698.89,104.05,8.82" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,145.60,675.16,147.30,8.64;4,63.76,687.11,178.26,8.64">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,249.65,686.93,43.26,8.59;4,63.76,698.89,45.48,8.59">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
