<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.72,69.83,370.81,12.62">IIIT Hyderabad at Million Query Track TREC 2009</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.60,105.38,46.08,10.48;1,137.41,119.33,48.45,10.48"><forename type="first">Prashant</forename><surname>Ullegaddi</surname></persName>
							<email>prashant.ullegaddi@research.iiit.ac.in</email>
						</author>
						<author>
							<persName coords="1,221.56,105.38,29.27,10.48;1,221.32,119.33,29.75,10.48"><forename type="first">Sudip</forename><surname>Datta</surname></persName>
							<email>sudip.datta@research.iiit.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Vinay Pande Kushal Dave Vasudeva Varma</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Search and Information Extraction Lab</orgName>
								<orgName type="institution">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad-500032</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.72,69.83,370.81,12.62">IIIT Hyderabad at Million Query Track TREC 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E82AA2790D37D7A737126A93B1AEDC26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This was our maiden attempt at Million Query track, TREC 2009. We submitted three runs for ad-hoc retrieval task in Million Query track. We explored ad-hoc retrieval of web pages using Hadoop-a distributed infrastructure. To enhance recall, we expanded the queries using WordNet and also by combining the query with all possible subsets of tokens present in the query. To prevent query drift we experimented on giving selective boosts to different steps of expansion including giving higher boosts to sub-queries containing named entities as opposed to those that did not. In fact, this run achieved highest precision among our other runs. Using simple statistics we identified authoritative domains such as wikipedia.org, answers.com, etc and attempted to boost hits from them, while preventing them from overly biasing the results. An attempt to query classification was also made.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goals of Million Query track in TREC 2009 were 1) To explore ad-hoc retrieval and classify the queries as precision-oriented/recall-oriented queries, 2) To investigate the reusability of the relevance judgments. To evaluate ad-hoc task a set of 40K queries were provided. First 1000 queries were of the highest priority. And every system was required provide results for the first 1000 queries compulsorily. We explored ad-hoc retrieval by designing a distributed system built using Lucene 1 and Hadoop 2 which will be discussed in Section 2.</p><p>We started off by exploiting the finer details embedded in a document structure. We had extracted various fields of a page (e.g., title, bold, href, etc.).</p><p>1 http://lucene.apache.org/ 2 http://hadoop.apache.org/</p><p>We initially tried using this information in ranking the result (weighing terms in title higher, and so on). But the results were not too encouraging. Finally, we retained three fields: document-url, title and body after parsing the content.</p><p>Few experiments on initial search results showed that a lot of spam/duplicate pages were present in the data set especially in the top hits. When we checked with the contents of those pages, it was found that there was a negligible difference between the duplicate documents. Due to this, most of the results in the top hits looked similar and had similar content. To overcome this, We employed a technique called deduplication explained in Section 3.1 to remove duplicates from the result set, resulting in more diversified results. The main motivation behind this was any user would not want to see very similar (in fact the same) results in top hits. In all we tried to model our retrieval system as a web search engine.</p><p>Another area we concentrated on was query expansion. Since majority of the queries were short, a query expansion module had to be designed. We also experimented with proper nouns in query expansion. Proper nouns in a query are important than any other query terms for they seem to carry more information. In expanded query, sub-queries not containing any of proper noun terms should have lower weightage (as they are more likely to result in query drift) as opposed to ones containing one or more of these terms. E.g. consider query "University of Hyderabad", here proper noun "Hyderabad" carries more information. "Hyderabad" together with "University" makes the above query complete. Hence, here query term "Hyderabad" is an important query term and should be weighed high. We employed a simple heuritsic to identify all proper nouns in the query log as explained in Section 3.2.</p><p>Finally, we also experimented with giving additional boost (document level boost) to authoritative pages among the search results. The webgraph pro-Figure <ref type="figure" coords="2,95.95,265.25,3.87,8.74">1</ref>: Stage 1-Parsing of HTML pages on cluster-One time job vided with the dataset was not a complete graph as many outlinks were pointing to pages outside the corpus. Hence pageranking was not found to be too usefull. We needed another way to devise a way to find authoritative pages in the corpus over different domains. We used another method as discussed in Section 3.3 to find authoritative pages and boost them in the search results.</p><p>We also attempted the original query classification task of classifying queries into 4 classes -Hard Precision, Soft Precision, Soft Recall, Hard Recall. To determine the viability of finding a global consensus and possible rules that could later be used to build a classifier. Four of us independently tagged the first 500 queries, but observed large inter-annotator disagreement both in terms of rules and labels. Thus we concluded the task of classifying queries into precision and recall is very subjective and even harder to generalize than classifying queries into navigational and informational.</p><p>The rest of the paper explains in detail the approaches we followed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Design</head><p>The data set consisted of 50 million HTML web pages, with some long web pages truncated to 256KB. The data set size was just over 1TB. Indexing 1TB data set (50million documents) and handling truncated pages required a careful system design. Truncation caused a major problem in parsing the data as most of the HTML parsers were vulnerable to missing tags/malformed tags. We needed a robust We used Lucene + Hadoop combination in a similar fashion to Nutch<ref type="foot" coords="2,405.54,331.97,3.97,6.12" target="#foot_1">4</ref> architecture, for indexing using Map/Reduce <ref type="bibr" coords="2,377.77,345.50,10.52,8.74" target="#b1">[2]</ref> paradigm. Here we describe Hadoop cluster which was used to index the data set. We used a cluster of 9 nodes. Each node was a quad core machine with 4GB RAM. One node was specially designated as the master, which only took care of assigning tasks to the remaining nodes i.e., slaves. Each slave node was set to run 8 maps and 4 reduces. With this setting we could index the whole data in less than 4 hours which otherwise would have taken a day to index!</p><p>The architecture used for parsing and indexing the data is shown in figure <ref type="figure" coords="2,416.39,477.96,4.98,8.74">1</ref> and figure <ref type="figure" coords="2,469.08,477.96,3.87,8.74" target="#fig_0">2</ref>. The distributed infrastructure we employed for indexing the data consisted of two stages:</p><p>• Stage 1: Parsing-Every map read a WARC file, and parsed it with WARC reader. Once HTML pages were extracted, they were parsed with Jericho HTML parser, and saved on HDFS as a key/value pair SequenceFile where key was clueweb-id of the page, and value was the parsed content of the page . The architecture for parsing on Hadoop is shown in figure <ref type="figure" coords="2,469.04,608.34,3.87,8.74">1</ref>.</p><p>• Stage 2: Indexing-The parsed data in first stage was fed to indexer. We had 8 reduces and each used Lucene to creat 4 indexes of the data it processed. In all we had 32 indexes of the data created by eight reduces. The architecture for indexing on Hadoop is shown in figure <ref type="figure" coords="3,252.57,77.15,3.87,8.74" target="#fig_0">2</ref>.</p><p>• Stage 3: Merging-We then merged these 32 indexes into 8 indexes so that they could be searched in distributed manner by placing each of the 8 indexes on individual machines. We used RMI search to get result from each machine and then merged results on a server. In this way we could substantially reduce the search time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we present the various approaches we followed and the experiments we conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">De-duplication</head><p>The TREC web corpus contained many duplicate pages having almost the same title and content but slightly different URLs. We refer to process of handling these duplicates as De-duplication. When the search results were analyzed without de-duplication it was found that the results contained many duplicates clustered together and there was no diverseness in the top results. For Example: searching for the string "naïve bayes" gave many wikipedia pages that had the same content and similar URLs. As all these duplicate results have scores very close to each other, they come in a group in search results. This situation is not desirable in the domain of Web search as it results in large redundancy in search hits. Another ill-effect of this redundancy is that other important results fail to figure among the top hits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Duplicate Detection</head><p>As the duplicates have scores very close to each other, the search for duplicates is carried out only in the vicinity of the result for which duplicates are checked. Hence, search for duplicates is done in a window of 50, where the first result(in the group of duplicates) is kept as it has the highest score amongst all the duplicates and rest are treated as duplicates.</p><p>We used following heuristics to detect duplicates:</p><p>1. The string edit distance of the URLs of the two results was less than a threshold (was set to 10).</p><p>2. If the difference in scores of the two duplicates was less than a threshold (set to 0.001).</p><p>3. The titles of the duplicates match exactly.</p><p>The duplicates were searched in a sliding window where one result (the first) was checked for duplicate with the rest of the results in the window. There are two approaches that were followed to tackle the problem:</p><p>1. Removing duplicates: As the duplicates do not contribute anything to the result set, the duplicates are simply deleted from the result set. As the maximum number of results to be submitted per query was 1000, deleting duplicates gave chance to any relevant result outside the first 1000 results. This is the approach followed in all the runs.</p><p>2. Demoting duplicates: Another approach that can be followed is demoting the results. In this approach duplicates are penalized and are moved down the order. The process of demotion has to be in such a way that the duplicates are disperesed down the results while other hits from the top 1000 are promoted up. Descending all the duplicates by a Fibonacci sequence (2, 3, 5, 8 . . . ) is a natural choice for such an application. For example, the first duplicate is demoted by 2 positions from its current position, the next by 3, then next by 5 and so on. For some duplicates the position to which the duplicate descended can reach 1000, hence they should be promoted up from the position 1000 again by a Fibonacci sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Expansion using Lucene</head><p>We search the query terms in different fields of documents like "url", "title", "h1", "bold", "text", etc. We observed that if query terms occured in url or title of the document, then chances of document being relevant are high. Hence, we gave more boost to documents having query terms in their url and title. For rest of the fields we assigned weights in decreasing order of their importance.</p><p>For query expansion, we generated a large number of simple queries containing query terms from the original query and process them separately. After executing all the subqueries we obtained final result by combining all results after giving differential boosts to results from individual queries. Procedure for creating subqueries is stated below:</p><p>1. We assume that query term order and proximity are very important for finding relevant documents. Hence, we did exact search of query terms in the documents. Documents having exact query terms in url and titles have more weigh-tage than documents with exact terms in other fields of the document.</p><p>2. We also search query term in approximate vicinity by using window of three words. Here, we assume that query terms can occur toghether in any order within the window of three words containing other words with query terms.</p><p>3. To increase recall, we also constructed queries from all the possible subsets of tokens (except the stop words) contained in the query. As an ill-effect, this could result in large query drift. We observed that for queries containing noun terms, subqueries which do not contain any of the noun terms result in very noisy results. Further, if the queries contain 'proper nouns', giving higher weightage to results from subqueries containing these terms, results in further containment of quert drift. We used WordNet <ref type="bibr" coords="4,269.24,287.25,10.52,8.74" target="#b2">[3]</ref> to identify noun terms. Any term not contained in WordNet is likely to either be a 'proper noun' or a mis-spelling. We employed this hypothesis as a first step to identifying proper nouns. Besides, we manually short-listed proper nouns from the list of words marked as nouns by WordNet. This query expansion technique could be more effectively employed if there exist other efficient ways to identify 'proper nouns'.</p><p>To decide weightage among subqueries containing nouns we use a simple formula to provide more boost to subqueries containing more proper nouns than query containing mixture of nouns and proper noun terms. The weightage assigned to a sub-query Q sub is given by:</p><formula xml:id="formula_0" coords="4,84.46,510.14,173.51,43.47">W eightage(Q sub ) = |Q sub | |Q| • |P sub | |P | • B f where, |Q sub | is number of terms in subquery</formula><p>|Q| is total number of terms in original query</p><formula xml:id="formula_1" coords="4,84.46,573.84,230.80,39.54">|P sub | is number of pronouns in subquery |P | is total number of proper nouns in original query B f is boost set for field 'f '</formula><p>From this formula, it is clear that sub-query containing more number of terms and more number of proper nouns gets more weightage than smaller subqueries containing less proper nouns; thus naturally, original query getting maximum weightage.</p><p>For some queries we found less recall even after doing all of the above steps. For such queries, we Results returned by subquries may not be mutually independent. If a document is retrieved by multiple queries, then it is assigned highest of these scores.</p><p>Example: For a query "President Barrack Obama", final expanded query becomes: Because the fully expanded query is very large we display only a part of the expanded query. Out of three query terms only 'Obama' is tagged as proper noun in our proper noun list. Hence, subqueries containing term Obama have higher weightage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Promoting Authoritative Pages</head><p>As Web pages from authoritative domains like wikipedia.org provide reliable and less noisy information, the chances of them being relevant hits are high. Hence, we promoted the authoritative pages in the search results. To identify authoritative domains in our corpus, we generated a list of top domains from the corpus based on number of pages from that domain. We promoted authoritative pages in search results which were ranked among top 100 hits. Table <ref type="table" coords="4,318.63,631.87,4.98,8.74" target="#tab_0">1</ref> shows a few top authoritative domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The performance measures used for evaluation were Prec@N and nDCG@N, where Prec@N is precision  <ref type="table" coords="5,257.86,248.40,4.98,8.74" target="#tab_1">2</ref> shows the Prec@N values for N = 10, 30, 50 and 100 and Table <ref type="table" coords="5,90.00,272.31,4.98,8.74">3</ref> shows the nDCG@N values for N = 10, 30, 50 and 100. We submitted 3 runs for the Million Query track where each of the runs were built on top of Lucene. The salient features for each of the runs are mentioned: In iiithExpQry to enhance recall we did query expansion using WordNet, giving higher boost to subqueries containing named entity terms from the original query, to prevent query drift. It also incorporated de-duplication wherein we removed repeated hits. We did this, because we observed that a number or similar/identical hits were coming in the top results and deduplication helped in removing them. In retrospect, we realised that this severly hurt our performance as it resulted in removal of several relevant (though possibly repeated) results.</p><p>The iiithAuthPN run included features from ii-ithExpQry except query expansion using WordNet. It incorporated changes in terms of improved query expansion (my minimizing query drift using selective boosting). It also incorporated boosting of hits from authoritative domains.</p><p>The iiithAuEQ incorporated a combination of all approaches that we experimented with. Since, it resulted in the worst performance, we believe that its difficult to integrate multiple approaches to come up with a cumulative improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>We started off with the understanding of how Nutch <ref type="bibr" coords="5,62.04,662.95,10.52,8.74" target="#b0">[1]</ref> works. Nutch is an open source search tool built on top of Lucene and Hadoop. We also indexed the clueweb09 data in the same way. Our architecture which combines both Lucene and Hadoop is explained in detail in Section 2 and also depicted in Figure <ref type="figure" coords="5,543.23,236.44,4.98,8.74">1</ref> and Figure <ref type="figure" coords="5,369.44,248.40,3.87,8.74" target="#fig_0">2</ref>.</p><p>To summarize, we experimented with 1) Proper noun based query expansion, 2) Authoritative boost for hits, and 3) Deduplication to remove similar pages from the top hits. Expanding the proper noun terms helped in increasing the recall for the search results, while authoritative pages helped in promoting the top relevant results from authoritative domains. Using de-duplication resulted in the removal of relevant but redundant hits from the search results. We attribute the lower precision of all the three runs to the removal of such duplicates.</p><p>We also made an attempt at query classification task of classifying quries into precision/recall queries. The results obtained were not encouraging. We annotated the queries manually. Each query was annotated by four annotators, but the annotations differed to a greater extent. This called for some sort of personalization in query classification.</p><p>In future, we wish to work on query classification and design a more robust proper noun based query expansion. Also we want to investigate better ways of detecting authoritative pages in a data set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,330.21,263.00,206.42,8.74;2,318.63,62.04,235.35,185.85"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Stage 2-Indexing on Hadoop cluster</figDesc><graphic coords="2,318.63,62.04,235.35,185.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,318.63,277.10,209.21,8.30;4,323.86,289.06,214.44,8.30;4,323.86,301.01,167.37,8.30;4,323.86,312.97,172.60,8.30;4,323.86,324.92,183.06,8.30;4,323.86,336.88,193.52,8.30;4,323.86,348.83,162.14,8.30;4,323.86,360.79,172.60,8.30;4,323.86,372.74,115.07,8.30;4,323.86,384.70,115.07,8.30;4,323.86,396.65,94.15,8.30;4,323.86,408.61,125.53,8.30"><head>(</head><label></label><figDesc>title:"president barack obama"^3000.0 | title:"president barack obama"~3^1000.0 | title:"barack obama"^11.304348 | title:"barack obama"~3^5.652174 | title:"president obama"^13.043479 | title:"president obama"~3^6.5217395 | title:"president barack"^0.05 | title:"president barack"~3^0.05 | title:obama^4.347826 | title:obama^2.173913 | title:barack^0.5 | title:president^0.5)~0.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,318.63,72.11,229.59,131.09"><head>Table 1 :</head><label>1</label><figDesc>Top Authoritative Domains</figDesc><table coords="4,318.63,83.81,229.59,119.39"><row><cell>Authoritative Domains</cell><cell>N o. of P ages</cell></row><row><cell>en.wikipedia.org</cell><cell>5996421</cell></row><row><cell>dictionary.ref erence.com</cell><cell>34686</cell></row><row><cell>dir.yahoo.com</cell><cell>30428</cell></row><row><cell>www.aboutus.org</cell><cell>26818</cell></row><row><cell>acronyms.thef reedictionary.com</cell><cell>25849</cell></row><row><cell cols="2">expand the query by adding synonyms of query terms.</cell></row><row><cell cols="2">To get synonyms we use WordNet database.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,62.04,72.11,394.79,185.02"><head>Table 2 :</head><label>2</label><figDesc>Prec@N for our runs</figDesc><table coords="5,62.04,81.88,394.79,175.26"><row><cell>RunID</cell><cell cols="5">P rec@10 P rec@30 P rec@50 P rec@100 M AP</cell></row><row><cell>iiithAuEQ</cell><cell>0.208</cell><cell>0.188</cell><cell>0.178</cell><cell>0.189</cell><cell>0.154</cell></row><row><cell>iiithAuthP N</cell><cell>0.214</cell><cell>0.195</cell><cell>0.183</cell><cell>0.194</cell><cell>0.156</cell></row><row><cell>iiithExpQry</cell><cell>0.285</cell><cell>0.230</cell><cell>0.207</cell><cell>0.180</cell><cell>0.179</cell></row><row><cell></cell><cell cols="3">Table 3: nDCG@N for our runs</cell><cell></cell><cell></cell></row><row><cell>RunID</cell><cell cols="5">nDCG@10 nDCG@30 nDCG@50 nDCG@100</cell></row><row><cell>iiithAuEQ</cell><cell>0.260</cell><cell>0.268</cell><cell>0.279</cell><cell></cell><cell>0.290</cell></row><row><cell>iiithAuthP N</cell><cell>0.207</cell><cell>0.240</cell><cell>0.252</cell><cell></cell><cell>0.267</cell></row><row><cell>iiithExpQry</cell><cell>0.202</cell><cell>0.233</cell><cell>0.247</cell><cell></cell><cell>0.263</cell></row><row><cell cols="3">at rank N , and nDCD@N is Normalized Discounted</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cumulative Gain(NDCG) at rank N . Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,333.87,690.68,125.91,6.99"><p>http://jerichohtml.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,333.87,700.18,86.34,6.99"><p>http://www.nutch.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,334.12,566.18,214.09,8.74;5,334.12,578.13,199.23,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,481.89,566.18,66.32,8.74;5,334.12,578.13,82.82,8.74">Building nutch: Open source search</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Cutting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,425.60,578.13,25.22,8.74">Queue</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="54" to="61" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.12,598.06,214.09,8.74;5,334.12,610.01,214.09,8.74;5,334.12,621.97,144.13,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,497.54,598.06,50.67,8.74;5,334.12,610.01,214.09,8.74;5,334.12,621.97,16.61,8.74">Mapreduce: simplified data processing on large clusters. Commun</title>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,334.12,641.89,214.09,8.74;5,334.12,653.85,196.64,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,412.63,641.89,135.58,8.74;5,334.12,653.85,28.62,8.74">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,371.25,653.85,65.57,8.74">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
