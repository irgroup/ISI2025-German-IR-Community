<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.02,115.49,269.95,14.93">Northeastern University in TREC 2009</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,282.25,133.14,47.51,9.46"><forename type="first">Web</forename><surname>Track</surname></persName>
						</author>
						<author>
							<persName coords="1,142.07,162.77,75.38,10.37;1,217.45,160.77,1.41,6.99"><forename type="first">Shahzad</forename><surname>Rajput</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.16,162.77,97.65,10.37;1,325.81,160.77,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Studies Department</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.05,162.77,57.43,10.37;1,393.48,160.77,1.41,6.99"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.19,162.77,61.01,10.37;1,465.20,160.77,1.41,6.99"><forename type="first">Javed</forename><surname>Aslam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.02,115.49,269.95,14.93">Northeastern University in TREC 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08CC3338078D7D779D2C5ECE009760ED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a typical retrieval scenario a user poses a query to a retrieval system in order to satisfy an information need generated during some task the user is undertaking. Retrieval systems access an underline collection of searchable material and rank them according to some definition of relevance of the material to the users request and returns this ranked list to the user. In the case of web search where typical users express their information needs by 2-3 keywords submitted queries often time have ambiguous meanings, representing more than one information need. Given a query, a good retrieval system should be able to satisfy all possible users by ranking documents in a way that their content covers as many information needs as possible.</p><p>The primary goal of our Web Track submission is to explore whether named entity tags can be utilized to diversify the returned ranked list of documents. Our hypothesis is that each information need could be represented by a certain named entity tag (or certain combination of them). For instance, in Table 1 one can see the example query taken from the Web Track web page. The query is "physical therapists". The subtopics that correspond to this query are listed in the left column of the table. To illustrate our hypothesis, next to each subtopic, in bold, we have manually identified a possible combination of entity tags that could represent each subtopic/information need.</p><p>Further, each document relevant to the original query could also be represented by a set of named entity tags. Instead of attempting to diversify documents based on the distance of their language models over text, we explored whether it is possible to diversify them according to the distance of their language model over entity tags. Entity tags could allow a further abstraction of documents avoiding issues like language mismatch. Our methodology highly depended on two assumptions: (1) retrieval methods based on a bug-ofwords representation can retrieve many relevant documents in the top 2,000 positions, and (2) the relevant documents would be diverse enough at the first place. Then using our methodology we could abstract the representation of those documents and diversify the list based on their tag distributions.</p><p>A second goal of our Web Track submission was to develop a simple spam filter. By analyzing a small subset of the documents, selected at random from the top 2,000 documents ranked by Indri language model per query over the new ClueWeb09 collection (category B)<ref type="foot" coords="1,334.36,597.58,3.99,6.91" target="#foot_0">1</ref> , we observed that 44.5% of them were spam. A large subset of the spam documents were those that contained query terms way too many times. For this purpose, we decided to develop a simple spam filter to remove these documents from the ranked list. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Named Entity Tagger</head><p>We believe that the problem of diversity is very much related to the structural content of the document. One way to determine the structural content is by identifying the named entities in the text. There are about 70 entities that we tag, some of which are person names, countries, cities, organizations, sports, accidents, crimes, moods, wars, etc. We have build a named entity tagger for this purpose, which identifies entities by using a lookup dictionary. The dictionary is built by the consolidation and modification of the publicly available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before Named Entity Tagging</head><p>In 1993, Drew Bledsoe and Rick Mirer were the top two picks in the NFL draft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>After Named Entity Tagging</head><p>In &lt; year1 &gt; 1993 &lt; year1 &gt;, &lt; person1 &gt; DREW BLEDSOE &lt; /person1 &gt; and &lt; person2 &gt; RICK MIRER &lt; /person2 &gt; were the top two picks in the</p><formula xml:id="formula_0" coords="2,124.79,560.73,200.80,9.81">&lt; company1 &gt; NFL &lt; /company1 &gt; draft.</formula><p>Table <ref type="table" coords="2,269.01,583.94,4.24,9.46">2</ref>: Example of Tagging An example of tagged text is shown in Table <ref type="table" coords="2,299.48,611.66,4.09,9.46">2</ref>. Once the named entities have been identified, we represent each document in the DOCSET as feature vector. Each feature corresponds to the normalized frequency of each named entity in that document. An example of feature vectors can be seen in Table <ref type="table" coords="2,531.81,638.76,4.09,9.46" target="#tab_1">3</ref>. For doc1 10% of the entities belong to PersonName, 20% to Country and so on. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document PersonName Country City</head><formula xml:id="formula_1" coords="3,207.85,74.43,205.39,69.97">• • • Doc1 0.1 0.2 0.1 • • • Doc2 0.1 0.2 0.2 • • • Doc3 0.2 0.1 0.1 • • • . . . . . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Diversity using entity tags</head><p>We assume that the ranked list of documents retrieved by Indri search engine for each query contains documents related to different aspects of the query, which if identified correctly, could help us produce a diverse ranked list.</p><p>For this task, we applied two approaches: (1) we clustered the top 2,000 documents based on the distance of their tag language model, we ordered the documents in each cluster by their Indri score and used roundrobin algorithm selecting a document at a time from each one of the clusters to populate the list, and (2) we considered the returned by Indri ranked list and rearranged the ranking of documents based on a combination of their original Indri score and the distance of their entity tag and entity value language model between the documents that have already populated the diverse ranked list and the remaining documents of the original ranked list. The two methods are described in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clustering &amp; Round-Robin</head><p>According to this first method, we consider the ranked list of documents returned by Indri. We eliminate the spam documents based on the simple spam filter we will describe in Section 4. We limit ourselves to the top 2,000 "non-spam" documents. We employ the entity tagger to tag these 2,000 documents and construct the tag feature vector. We utilize the k-means algorithm to cluster documents based on the distribution of tags in each one of them. The number of clusters k was fixed to 7. The documents in each cluster were ranked based on their original Indri score. The final "diverse" ranked list was produced by fusing the ranked-list of documents from each cluster in a Round-Robin manner. The results of the submitted run, NEURRWeb300, can be seen in Tables <ref type="table" coords="3,167.16,483.49,5.45,9.46" target="#tab_2">4</ref> and<ref type="table" coords="3,193.82,483.49,4.09,9.46" target="#tab_3">5</ref> As it can be observed the clustering approach led to the worst performing algorithm among the three employed in our submission. As an initial diagnostic of what went wrong, we first examined the ranked list of documents returned by Indri to explore whether a large number of diverse relevant documents were found in the first place and then we examined whether documents in certain clusters indeed corresponded to the different query aspects (subtopics). First, we observed that only a very small subset of the Indri ranked list of 2,000 documents were in fact relevant. Further, most of these relevant documents were actually relevant to one or two query aspects. Thus our first assumption that a "bug-of-words" approach can return a large number of relevant and diverse documents at the first place was certainly not true. Regarding whether clustering over entity tags could improve diversity, we observed that there were cases, where documents relevant to a certain query aspect were clustered together and separately from documents relevant to a different query cluster. Given, however, the small number of relevant documents and the absence of diversity conclusions can only be tentative at this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Window-greedy maximization of diversity</head><p>In a second approach we again considered the ranked list of documents returned by Indri after eliminating the spam ones. We re-ranked the top 2,000 documents by some combination of the produced Indri score and a diversity measure. Each document is represented by the distribution of entity tags along with the distribution of entity values. For instance, if a document contains the word "Obama" twice and the word "Mergel" once, and these words were tagged by the "president" tag, we counted three times the entity tag "president", twice the entity value "Obama" and once the entity tag "Mergel". The counts were then transformed into a Robertson's TF -like score.</p><p>We also incorporate two weighting factors:</p><p>• entity tag weighting factor: We manual predefined an importance ratio between entity tags, e.g. tags over query terms are generally more important than other tags, or non-specific "dates" do not count at all.</p><p>• entity value position in document: For each entity value (entity) we keep 5 counts: anchor, title, bodytop, body-middle, body-bottom. These 5 categories have predefined importance weights, e.g. anchor counts 4 times more than body-bottom, etc.</p><p>The diversity measure was calculated as follows. Let's assume that we have populated the "diverse" list with a number of documents. For all the documents already in the "diverse" list, we aggregated all entity tag and entity value counts. Regarding the documents that are not in the "diverse list" already, at each round of the algorithm we only consider W of them, the top W documents based on their original Indri scores (i.e., a window of length W ). The distance between the entity tag and value distribution of each one of these W documents is computed against the aggregated distribution of entity tags and values of the documents already in the "diverse list" and the most "diverse" document is added to the ranked list. The first document listed in the "diverse" list is the highest ranked document by Indri, since there are no "prior" documents to measure diversity against.</p><p>We varied W , that is the number of documents considered to be added in the "diverse" ranked list and submitted 2 runs: NEUDiv1 with W =25, and NEUDivW75 with W =75. The results can be viewed in Tables <ref type="table" coords="4,103.39,669.46,5.45,9.46" target="#tab_2">4</ref> and<ref type="table" coords="4,130.95,669.46,4.09,9.46" target="#tab_3">5</ref>. As it can be observed this approach outperforms the clustering one. Further, a window of size 25 leads to better results than a window of size 75. The fact that a small window outperforms both a larger window and the clustering approach seems to highlight the importance of the relevance score over the importance of diversity. Given that most of the top 2,000 documents considered are non-relevant an approach that puts a large weight to diversity will lead to non-relevant documents ranked high in the "diverse" ranked list, since on average the distance between non-relevant documents is larger than the distance between relevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adhoc Spam Filter</head><p>The spam filter is based on the idea that the term frequency in a spam document does not follow the same distribution as the term frequency in a non-spam document. Essentially, our hypothesis is that the distribution of terms in non-spam documents is more random than the distribution of terms in spam documents. By measuring the randomness of the text in a document with entropy, we attempted to identify all the documents whose entropy is very close to the entropy of the general English. The latter was approximated with a training set of non-spam documents.</p><p>We selected about 100 documents, SPAMDOCSET from the DOCSET at random, which were used as a training set. All of these documents were labeled manually as spam or non-spam. We computed the entropy of the text for each of these documents. The entropy is defined as,</p><formula xml:id="formula_2" coords="5,238.42,319.13,301.58,33.71">H(W ) = n i=1 p(w i ) log 2 p(w i )<label>(1)</label></formula><p>where W is the set of all the words in the document, w i is the i th word in W and p i is the probability of w i in W. Thus, the training data is a set of l labeled examples (e 1 , y 1 ), • • • , (e l , y l ), where e i denotes the entropy of i th document and y i is its label: 1 for spam and 0 for non-spam. A linear classifier is then trained over this training data set to obtain the optimal entropy e * such that e * minimizes f (e * ), where (2)</p><p>In other words, e * minimizes the square error in the prediction of our spam filter.</p><p>For the documents in SPAMDOCSET, the value of e * was found to be 918.399. For this value of e * , 9.78% of the labeled spam documents were not identified as spam and 3.26% of the labeled non-spam documents were identified as spam. Given that this 3.26% of non-spam documents that were labeled as spam are documents ranked high in the ranked list returned by Indri with a number of terms repeated more than normal, we assumed that some of these terms could be query terms and thus these filtered documents could be relevant documents. Hence, we decided to drop the entropy threshold e * down to 600 and 300. Based on these two thresholds we submitted two runs, the NeuLMWeb600 and the NeuLMWeb300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results for TREC 2009 Web track adhoc task</head><p>In total we submitted the following three runs to TREC 2009 Web track adhoc task:</p><p>1. NeuLMWebBase: This is the ranked list returned by Lemur search engine. It is used as a baseline to compare results after removing spam.</p><p>2. NeuLMWeb300: We remove some spam documents from the ranked list returned by Lemur search engine. We use the threshold of 300 to control the spam filtering.</p><p>3. NeuLMWeb600: We remove some spam documents from the ranked list returned by Lemur search engine. We use the threshold of 600 to control the spam filtering which removes more spam than in the previous case.</p><p>The results are summarized in Table <ref type="table" coords="6,232.91,164.52,4.09,9.46" target="#tab_4">6</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,243.48,450.51,15.84,9.57;5,259.32,448.01,4.23,6.99;5,264.05,450.51,15.76,9.57;5,284.37,438.88,12.70,6.99;5,283.87,465.60,13.70,6.99;5,298.60,450.51,50.76,10.63;5,349.35,448.01,4.23,6.99;5,354.09,450.51,9.70,9.57;5,363.78,448.01,4.23,6.99"><head>f</head><label></label><figDesc>(e * ) = 100 i=1 {y i -(e i /e * )} 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,111.48,74.28,389.03,280.29"><head>Table 1 :</head><label>1</label><figDesc>Web track query and subtopics example.</figDesc><table coords="2,111.48,74.28,389.03,257.63"><row><cell>Query : physical therapist</cell><cell></cell></row><row><cell>Description : The user requires information</cell><cell></cell></row><row><cell>regarding the profession and the services it provides</cell><cell></cell></row><row><cell>Subtopics</cell><cell>Entity Tags</cell></row><row><cell>What does a physical therapist do?</cell><cell>JOBTITLE</cell></row><row><cell>Where can I find a physical therapist?</cell><cell>LOCATION/</cell></row><row><cell></cell><cell>ORGANIZATION/PERSON</cell></row><row><cell>How much does physical therapy cost per hour?</cell><cell>MONETARY UNITS/</cell></row><row><cell></cell><cell>TIME UNIT/PERSON</cell></row><row><cell>What education or training does a physical therapist require?</cell><cell>TIME/ORGANIZATION</cell></row><row><cell>Where can I obtain this training? How long does it take?</cell><cell></cell></row><row><cell>What is the American Physical Therapy Association?</cell><cell>ORGANIZATION</cell></row><row><cell>What is the URL of its Website?</cell><cell></cell></row><row><cell>How much do physical therapists earn?</cell><cell>MONETARY UNITS/</cell></row><row><cell>What is the starting salary?</cell><cell>DATE UNIT</cell></row><row><cell>What is the average salary for an experienced therapist?</cell><cell></cell></row><row><cell>What is the difference between a occupational therapist and a</cell><cell>JOBTITLE</cell></row><row><cell>physical therapist?</cell><cell></cell></row><row><cell cols="2">Information is required regarding physical therapist's assistants. ORGANIZATIONS/</cell></row><row><cell>What education do they require? How much do they make?</cell><cell>MONETARY UNITS/</cell></row><row><cell></cell><cell>JOBTITLE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,187.34,157.80,237.33,9.46"><head>Table 3 :</head><label>3</label><figDesc>Representation of Documents from DOCSET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,83.78,483.49,444.45,108.91"><head>Table 4 :</head><label>4</label><figDesc>. Results for TREC 2009 Web track diversity task (α-NDCG measures)</figDesc><table coords="3,83.78,505.48,444.45,64.05"><row><cell>Run</cell><cell cols="3">alpha-ndcg5 alpha-ndcg10 alpha-ndcg20</cell><cell># of queries with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>alpha-ndcg10 &gt; median(statMAP)</cell></row><row><cell>NEURRWeb300</cell><cell>0.133</cell><cell>0.160</cell><cell>0.189</cell><cell>17</cell></row><row><cell>NEUDiv1</cell><cell>0.215</cell><cell>0.243</cell><cell>0.278</cell><cell>24</cell></row><row><cell>NEUDivW75</cell><cell>0.207</cell><cell>0.220</cell><cell>0.250</cell><cell>22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,149.06,74.78,313.87,73.37"><head>Table 5 :</head><label>5</label><figDesc>Results for TREC 2009 Web track diversity task (IA measures)</figDesc><table coords="4,206.39,74.78,199.22,50.50"><row><cell>Run</cell><cell cols="3">IA-P5 IA-P10 IA-P20</cell></row><row><cell cols="2">NEURRWeb300 0.057</cell><cell>0.062</cell><cell>0.063</cell></row><row><cell>NEUDiv1</cell><cell>0.126</cell><cell>0.131</cell><cell>0.134</cell></row><row><cell>NEUDivW75</cell><cell>0.122</cell><cell>0.119</cell><cell>0.124</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,94.13,164.52,423.74,95.36"><head>Table 6 :</head><label>6</label><figDesc>. Results for TREC 2009 Web track adhoc task</figDesc><table coords="6,94.13,186.17,423.74,50.85"><row><cell>Run</cell><cell cols="3">eMAP (MTC) statMAP # of queries with statMAP &gt; median(statMAP)</cell></row><row><cell>NEULMWebBase</cell><cell>0.042828</cell><cell>0.1763</cell><cell>30</cell></row><row><cell>NEULMWeb300</cell><cell>0.043899</cell><cell>0.1865</cell><cell>34</cell></row><row><cell>NEULMWeb600</cell><cell>0.044242</cell><cell>0.1869</cell><cell>32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.14,648.20,327.78,7.77"><p>For the rest of the paper, we will refer to this set of approximately 100,000 documents -i.e.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="1,418.69,648.20,121.31,7.77;1,72.00,659.00,80.69,7.94"><p>2,000 documents per query for 50 queries -as DOCSET.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
