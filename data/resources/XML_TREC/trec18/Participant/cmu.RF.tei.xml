<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,57.15,72.35,495.43,16.84">Pairwise Document Classification for Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,124.13,118.05,92.78,11.06"><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Elsas</surname></persName>
							<email>jelsas@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.63,118.05,72.23,11.06"><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
							<email>pinard@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.02,118.05,66.86,11.06"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.51,118.05,102.07,11.06"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,57.15,72.35,495.43,16.84">Pairwise Document Classification for Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">95C60E70F6B103FDC6DD8A7C06D30C22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present Carnegie Mellon University's submission to the TREC 2009 Relevance Feedback Track. In this submission we take a classification approach on document pairs to using relevance feedback information. We explore using textual and non-textual document-pair features to classify unjudged documents as relevant or non-relevant, and use this prediction to re-rank a baseline document retrieval. These features include co-citation measures, URL similarities, as well as features often used in machine learning systems for document ranking such as the difference in scores assigned by the baseline retrieval system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Retrieval systems employing relevance feedback techniques typically focus on augmenting the representation of the information need in order to improve performance. This is typically done through adding or re-weighting terms in the query representation, and have been shown to be effective techniques in the past <ref type="bibr" coords="1,146.17,420.28,9.72,7.86" target="#b4">[4,</ref><ref type="bibr" coords="1,159.18,420.28,7.16,7.86" target="#b7">7,</ref><ref type="bibr" coords="1,169.63,420.28,7.17,7.86" target="#b8">8,</ref><ref type="bibr" coords="1,180.08,420.28,10.74,7.86" target="#b13">13]</ref>. These techniques, however, are typically limited to the information need representation used in the baseline retrieval system and generally don't utilize information beyond the word distributions in the feedback documents to modify the query model. This paper describes the CMU submission to the TREC 2009 Relevance Feedback Track. With this submission, our goal is to explore techniques beyond query term re-weighting and other traditional approaches to query expansion. Our approach constructs pairwise features between judged-relevant feedback documents and unjudged documents, and then applies a learned classifier to identify those unjudged documents likely to be relevant. The output of this classification is then used to re-rank an initial document ranking, favoring those documents predicted to be relevant to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM DESCRIPTION</head><p>The CMU submission system consists of four main components: baseline retrieval, document selection, relevance classification and document re-ranking. The document selection and relevance classification components of the system take a machine learning approach, using a feature space derived from document pairs. This section describes these four components in the CMU relevance feedback track submission, as well as this featurebased document-pair representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline Retrieval</head><p>For these experiments, we use Indri for our baseline ranking <ref type="foot" coords="1,329.08,218.76,3.65,5.24" target="#foot_0">1</ref> . Indri has been shown to perform well in ad-hoc retrieval tasks at TREC in previous years <ref type="bibr" coords="1,484.50,230.99,9.71,7.86" target="#b8">[8,</ref><ref type="bibr" coords="1,498.04,230.99,10.74,7.86" target="#b10">10]</ref>. For these experiments we made use of a small standard stop-word list and applied the Krovetz stemmer. We constructed fulldependence model queries from the query text <ref type="bibr" coords="1,505.63,262.37,9.20,7.86" target="#b9">[9]</ref>. Smoothing parameters were taken directly from previously published TREC configurations <ref type="foot" coords="1,428.66,281.52,3.65,5.24" target="#foot_1">2</ref> .</p><p>Initial informal experiments with pseudo-relevance feedback (PRF) with relevance models <ref type="bibr" coords="1,460.21,304.21,9.71,7.86" target="#b7">[7]</ref> indicated that traditional approaches to query expansion may be less effective on the ClueWeb09 collection due to the susceptibility of those techniques to the web-spam present in the collection. For this reason we did not use PRF in our baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Representation</head><p>We take a machine learning approach to the document selection and relevance classification components of our system. These components use a common document representation scheme, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Pairwise Representation</head><p>Our feature-based representation constructs feature vectors for each pair of documents retrieved by the baseline retrieval for a given query. Dq = {dq1, dq2, . . . , dqR} Pq = {f (dqi, dqj) | i, j ∈ {1, . . . , R}, i = j} Dq are the R documents retrieved for query q, Pq are the document pair vectors defined by f : Dq × Dq → R M , a vector feature function over document pairs:</p><formula xml:id="formula_0" coords="1,341.56,550.30,186.02,7.89">f (di, dj) = f0(di, dj), f1(di, dj), . . . , fM (di, dj)</formula><p>where each f k are instantiations of individual features derived from the document pairs. This representation allows use of some features that can be difficult to integrate into traditional retrieval systems that exclusively use term-weighting for estimating relevance. As we describe below, many of our features cannot be modeled with a bag-of-words document representation. Using a pairwise representation also allows a "query by example" approach to leveraging the feedback information. We make the assumption that relevant documents tend to be similar to each other, viz. the cluster hypothesis <ref type="bibr" coords="1,508.73,672.48,13.49,7.86" target="#b12">[12]</ref>. Thus, using pairwise features that describe document similarities (or dissimilarities), the goal of our approach is to find other relevant documents similar to those that have been judged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Features</head><p>The fourteen document-pair feature functions (f k (di, dj)) used in these experiments are described below. These features are generally intended to capture different types of similarity (or dissimilarity) between two documents. Many of these features are computed with the Jaccard coefficient, a measure of similarity of two sets of objects. The Jaccard coefficient of two sets A and B is given by:</p><formula xml:id="formula_1" coords="2,135.40,177.67,157.50,19.75">J(A, B) = |A ∩ B| |A ∪ B|<label>(1)</label></formula><p>1. Document features (a) Length: The absolute value of the difference in the lengths of di and dj.  (c) Co-citation: The Jaccard coefficient computed over the set of documents that link to di and dj.</p><p>(d) References: The Jaccard coefficient computed over the set of documents that di and dj link to. All features are normalized to have zero-mean unit-variance per query prior to training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Classification</head><p>We can use the above document pair representation scheme to train a classifier that predicts whether unjudged documents are relevant or non-relevant given some judged documents. We make the assumption that relevant documents are likely to be similar to each other, and dissimilar to nonrelevant documents with respect to the features defined in Section 2.2.2. In contrast, we make no assumption about the similarity of non-relevant documents to each other.</p><p>We train this classifier on a set of queries with known relevant and non-relevant documents. Let the set of (binary) judgements for a given training query, q be:</p><formula xml:id="formula_2" coords="2,378.94,274.26,114.84,7.86">Jq = {(dqi, rqi) | rqi ∈ {0, 1}}</formula><p>where rqi = 1 indicates the document dqi is relevant for query q, and rqi = 0 indicates the document is non-relevant.</p><p>We train a logistic regression classifier on judged document pairs, letting yqij ∈ {0, 1} indicate the class label of the pair (dqi, dqj). This training set is constructed as follows: After feedback judgements are collected, assuming some of the feedback documents are relevant, we can apply the learned classifier to predict whether or not unjudged documents are relevant or non-relevant. For each unjudged document dqj, we make a relevance prediction given all the judged relevant documents: {h(dqi, dqj) ∀ dqi s.t. rqi = 1}. This set of predictions can be combined in several ways to form a final relevance classification, for example taking the mean, minimum, or maximum value across the predictions. Preliminary experiments with the TREC 2009 Relevance Feedback Track data showed that taking the maximum prediction value across all the judged relevant documents generally yielded the best performance. Thus, we define our final prediction for an unjudged document as follows:</p><formula xml:id="formula_3" coords="2,346.82,350.54,179.10,7.86">JPq = {(f (dqi, dqj), yqij) | rqi = 1; yqij = rqj}</formula><formula xml:id="formula_4" coords="2,374.25,625.57,124.23,14.00">π(dqj) = max d qi ∈Jq ;r qi =1 h(dqi, dqj)</formula><p>This relevance prediction effectively classifies unjudged documents based on their similarity to the closest judged relevant feedback document with respect to the feature space defined above. Because of this, it is critical to collect relevance judgements on a diverse set of documents in order to maximize the chance of identifying relevant documents similar to possibly relevant but unjudged documents.</p><p>Note that judged non-relevant documents are used for training the model, but are not used at prediction time after collecting feedback judgements. Methods of using these nonrelevant feedback documents is an area for future refinement of the models presented here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Document Re-Ranking</head><p>We use the output of the above relevance classifier π to re-rank the documents retrieved with the baseline ranking algorithm. Due to the difficulty of re-scaling Indri's language modeling score and the output of a logistic regression classifier, we chose to combine scores using a rank-based voting method, Borda Count <ref type="bibr" coords="3,143.38,201.35,9.20,7.86" target="#b1">[1]</ref>. Rather than combining the scores of the baseline ranker and the logistic regression, Borda Count linearly combines the ranks of the documents from each of these components. Although this method ignores the magnitude of the confidence of the prediction output, it avoids the need to re-scale the scores to be comparable.</p><p>We use a weighted version of Borda Count in these experiments to adjust the relative influence of the baseline ranking score and the relevance prediction output. This weight is selected to maximize Mean Average Precision via a grid search on the same training data used to train the relevance classifier. For these experiments, we selected a weight of 0.3 on the relevance classifier and 0.7 on the baseline ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Document Selection</head><p>The final component of our system is the document selection system. As pointed out earlier, diversity is a critical factor underlying our document selection approach. The classification method in Section 2.3 gives a probabilistic measure of the relevance of an unjudged document paired with a judged relevant document. The final relevance score of an unjudged document is then the maximum value assigned across all the judged relevant documents for that query. Having similar judged relevant documents agree on the relevance of an unjudged document is not as effective as having agreement across a diverse committee. Thus, this is the main focus of our selection mechanism.</p><p>The most naïve approach is to select the top 5 documents for feedback. However, it is often the case that top documents are similar to each other. Learning the relevance level of similar documents might improve the ranking for additional similar documents, but it might not generalize to a larger set of documents. The diversity factor has been investigated in the active learning literature <ref type="bibr" coords="3,240.01,564.73,9.72,7.86">[5,</ref><ref type="bibr" coords="3,254.12,564.73,10.74,7.86" target="#b11">11]</ref>. It is indicated that choosing the unlabeled examples which are representative of the underlying data distribution boosts the performance. Hence, we focus in this section to select documents that are likely to be relevant and also different from each other. Specifically, we adopted a clustering framework where we cluster the unjudged documents using the Fuzzy Clustering algorithm <ref type="bibr" coords="3,140.30,637.96,9.72,7.86" target="#b2">[2,</ref><ref type="bibr" coords="3,153.08,637.96,6.48,7.86" target="#b6">6]</ref>.</p><p>The objective of fuzzy clustering is to spread out each example into various clusters. In other words, each example has a degree of belonging to clusters, rather than completely belonging a single cluster. Hence, it is a soft clustering method instead of hard clustering. For each point x, there is a corresponding coefficient indicating the degree of belonging to the k th cluster; i.e. u k (x). However, the sum of the coefficients for any given point x is equal to 1.</p><formula xml:id="formula_5" coords="3,395.22,75.58,160.70,27.03">K X k=1 u k (x) = 1∀x (2)</formula><p>Furthermore, the degree of belonging u k (x) (or the membership coefficient) is inversely related to the distance of the point to the cluster center center k :</p><formula xml:id="formula_6" coords="3,383.21,149.64,172.71,20.23">u k (x) = 1 d(center k , x)<label>(3)</label></formula><p>Hence, points further away from the center of the cluster have a lower degree of belonging than the points closer to the center. The cluster center is calculated using the mean of all points, weighted by their membership coefficients:</p><formula xml:id="formula_7" coords="3,380.87,227.46,175.05,24.18">center k = P x u k (x) f x P x u k (x) f<label>(4)</label></formula><p>where f &gt; 1 is a predefined parameter that controls the fuzzyness. For instance, increasing f leads to crisper clusterings whereas f close to 1 resembles the k-means algorithm. Finally, the fuzzy clustering tries to minimize the following objective function</p><formula xml:id="formula_8" coords="3,362.51,318.51,193.41,27.10">X k=1,...,K P i,j u k (i) f u k (j) f d(i, j) 2 P j u k (j) f<label>(5)</label></formula><p>where d(i, j) is the distance between two documents di and dj. The algorithm tries to minimize the inter-cluster similarity while minimizing the intra-cluster variance. It converges to a locally optimal solution <ref type="bibr" coords="3,433.40,387.67,9.20,7.86" target="#b2">[2]</ref>. We use the output of our trained logistic regression classifier on the document-pair features, as described above, to approximate this distance metric, d(i, j). Although this is not a proper metric in the mathematical sense, it can be used by the presented clustering algorithm and it does capture the feature-weighted similarity used in the relevance classification component of our system.</p><p>Because our re-ranking system does not use non-relevant feedback documents, we want to select documents that are likely to be relevant as well as diverse. The classification scheme described in Section 2.3 requires judged relevant documents to make predictions on the unjudged documents during testing. Initial investigation with the TREC 2008 Relevance Feedback data indicated that increasing the number of judged relevant documents is quite beneficial to the final re-ranking performance. Therefore, our aim is to identify the potentially relevant documents while maintaining a degree of diversity among them. Assuming the baseline indri ranking is well-tuned and relatively accurate, it is reasonable to consider the top documents to be judged. After we build the clusters among unjudged documents, we choose the top ranked document in each cluster to be judged. This simple method has the two characteristics we require: 1) it consists of top ranked documents that are likely to be relevant, and 2) it is a diverse set that leverages the underlying relevance distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>This section describes the experiments conducted for the TREC 2009 relevance feedback track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>The document selection and relevance classification components require training data in order to learn weights on the features described in Section 2.2.2 for use in the logistic regression relevance classifier (Section 2.3) and the clustering algorithm (Section 2.5). Because previous queries and relevance judgements do not exist on the ClueWeb09 dataset, we built our training data from previous years' TREC ad-hoc tasks using the GOV2 collection. This training set includes all relevance judgements for queries 701-850 excluding those queries with no relevant documents. The final constructed training set includes 1.8 million document pairs, with 31% positive examples (relevant/relevant pairs) and 69% negative examples (relevant/non-relevant pairs). Although these two document collections are somewhat different, the feature set described above can be generated on both collections. We make the assumption for these experiments that the feature weights learned on the GOV2 collection are similarly effective on the ClueWeb09 collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Features Weights</head><p>Sections 2.2 and 2.3 describe the pairwise document representation and how we use this representation in a logistic regression classifier to predict the relevance level of an unjudged document given a judged relevant document. It is informative to inspect the learned logistic regression weights for each of the features used in our model, as the larger magnitude weights indicates a more influential feature. Figure <ref type="figure" coords="4,53.80,352.31,4.61,7.86" target="#fig_4">1</ref> shows the absolute weights of all the features learned in the logistic regression model. We can see that the most in-  Out-link count feature is the only webgraph feature that is at all influential in the model. This feature is derived exclusively from the content of the page (just the count of anchors), rather than relations between documents in the collection. This may be an indication that the GOV2 webgraph used for training may be too sparse to effectively estimate the other webgraph features which rely on linking among documents in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned Feature Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Document selection</head><p>In this section, we analyze the quality of our document selection mechanism across queries. First, looking at the distribution of ranks in our baseline retrieval selected for judgement, we can see a strong skew towards the top-ranked documents to be selected for judgement. We also see that we do a reasonably good job of finding relevant documents not only at high ranks but also at lower ranks, though with decreasing frequency. This is especially useful since it detects the relevant documents the baseline ranker misjudged by putting in lower ranks. Incorporating such documents to the rank learner is likely to lead to improvements. To evaluate the quality of our phase-1 document selection (CMU.1), we primarily consider the fraction of other inputs that our phase-1 input performed better than, which we refer to as the score here. (This score was computed and distributed by the track organizers.) The score value is intended to measure the general quality of the selected documents across a variety of systems that use this feedback as input. A higher value indicates the documents selected by our phase 1 system tended to be more useful that document selected by other phase 1 systems. The score is calculated on a per-query basis, and we evaluate the correlation across queries with various other measures. These measures are described below:</p><p>1. Mean Rank: The mean rank in our baseline ranking of the documents selected in our phase 1 selection (CMU.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Max Rank:</head><p>The max rank in our baseline ranking of the documents selected in our phase 1 selection (CMU.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Num. Relevant:</head><p>The number of documents selected by CMU.1 judged relevant for the query.</p><p>Table <ref type="table" coords="5,89.26,127.52,4.61,7.86">1</ref> shows the mean and the standard deviation of these measures and their correlations with the score, all computed across queries. There is not a strong correlation between the score value and any of the other performance measures computed over our document selection set. Our document selection component was designed to identify documents useful for our relevance classifier and reranking components. For this reason, another appropriate method of evaluating the quality of our phase 1 input is to compare the relative improvement in phase 2 performance using our phase 1 input and other phase 1 inputs. Figure <ref type="figure" coords="5,53.80,376.44,4.61,7.86" target="#fig_1">3</ref> shows this relative improvement as a function of the total number of relevant documents selected by that phase 1 input. For each input set, we compute the statMAP on the baseline and phase 2 run excluding those documents in the input set from each evaluation (i.e. residual performance). The relative improvement of a phase-2 run over the baseline is referred to as the relative residual performance improvement and is used as our primary measure to evaluate phase-2 performance.</p><p>There is a strong correlation between the number of relevant documents selected and the relative improvement in statMAP (Pearsons's correlation of 0.926). This is likely due to our phase 2 system ignoring non-relevant feedback documents, and suggests that focusing only on relevant feedback is not always an appropriate strategy.</p><p>We also see that, although our phase 1 selection system is moderately coupled with the phase 2 re-ranking system, it doesn't yield the best relative improvement in statMAP. These results clearly indicate that for our phase 2 system, increasing the number of relevant documents selected for feedback is an effective strategy for improving performance.</p><p>Looking deeper at the robustness of our phase-2 performance as a function of feedback documents, we evaluate the relative residual performance for all input sets as we vary the wight given to the feedback documents. Figure <ref type="figure" coords="5,261.92,627.50,4.61,7.86" target="#fig_2">4</ref> shows the relative residual performance for each of our system's input sets as the weight on feedback documents varies from 0 to 1. The vertical line in this figure indicates the weight we used in our TREC submission (0.3) and the values along this vertical line correspond to those plotted in Figure <ref type="figure" coords="5,269.10,679.80,3.58,7.86" target="#fig_1">3</ref>. We can see that the weight selected based on our training data is not optimal for all of the input sets, but does represent a reasonable tradeoff across the different inputs. The best q q q q q q q 60 70 80  performing input set (CMIC.1) could have achieved almost a 13% improvement in residual statMAP had we selected a lower weight, but for most input sets the selected value is within 2% relative residual performance of the optimal weight.</p><p>Interestingly, the CMIC.1 input set, which yielded our best relative increase in statMAP, almost exclusively consists of documents from Wikipedia<ref type="foot" coords="5,454.02,445.96,3.65,5.24" target="#foot_3">4</ref> , whereas all of the other input sets consist of less than 5% Wikipedia documents. Although documents from Wikipedia may tend to be of higher general quality with less spam, these documents may be less diverse especially with regard to our link-based and URLbased document pair features. This result is somewhat contrary to the hypothesis that drove our document selection algorithm, that a diverse set of documents with respect to our feature space woud be most beneficial in final re-ranking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this year's submission to the TREC Relevance Feedback track, we took a machine learning approach to both the phase 1 (document selection) and phase 2 (document reranking) components of our system. These two systems use a shared feature space to represent pairs of documents. Our system specifically tried to leverage non-textual information such as webgraph features and URL similarity features, as well as textual features such as scores generated from different components of the baseline query. The shared representation moderately couples our selection and re-ranking systems, enabling us to select a set of documents specifically deemed to be useful for the down-stream re-ranking Feedback Document Weight Relative Change in Residual statMAP q q q q q q q q q q q MSRC.1 CMIC.  component. Initial analysis suggests that phase 1 selection algorithms that identify more relevant documents yield a higher relative increase in performance for our phase 2 re-ranking system. Although our phase 1 selection system performed well, yielding almost an 8.5% relative improvement in statMAP, higher relative improvement was achieved by several other phase 1 inputs which did not share the same feature space. For this reason, it is not clear that coupling the representation used in our phase 1 and phase 2 systems yielded a significant performance boost. Further analysis is necessary to understand the effect of coupling these two systems.</p><p>One of the goals of the phase 1 selection system was to identify a diverse set of relevant documents by clustering the top-ranked documents from the baseline retrieval. This clustering was performed in the same feature space used by the relevance classification component (Section 2.3) in an effort to couple the two systems. To evaluate the effect of this coupling, future work should assess the performance of other selection mechanisms that aim to identify diverse documents, but not necessarily within the same feature space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,64.58,253.87,65.68,7.86;2,79.70,271.47,213.21,7.89;2,95.94,281.96,196.97,7.86;2,95.94,292.42,103.33,7.86;2,79.19,306.43,213.72,7.89;2,95.94,316.92,196.97,7.86;2,95.94,327.38,95.71,7.86;2,80.21,341.39,212.70,7.89;2,95.94,351.88,196.97,7.86;2,95.94,362.34,75.71,7.86"><head></head><label></label><figDesc>URL Depth: The absolute value of the difference in the depth (number of occurrences of '/') in the URLs of di and dj. (b) URL Host: The Jaccard coefficient computed over overlapping character 4-grams in the URL hostnames of di and dj.(c) URL Path: The Jaccard coefficient computed over overlapping character 4-grams in the URL paths of di and dj.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,64.58,379.96,87.18,7.86;2,151.75,378.19,3.65,5.24;2,79.70,397.56,213.20,7.89;2,95.94,408.05,143.74,7.86;2,79.19,422.06,213.72,7.89;2,95.94,432.55,159.88,7.86"><head>3. Webgraph features 3 (</head><label>3</label><figDesc>a) In-link: The absolute value of the difference in the number of in-links to di and dj. (b) Out-link: The absolute value of the difference in the number of out-links from di and dj.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,64.58,499.17,103.33,7.86;2,79.70,516.77,213.21,7.89;2,95.94,527.25,196.97,7.86;2,79.19,541.26,213.71,7.89;2,95.94,551.75,196.97,7.86;2,95.94,562.21,78.04,7.86;2,80.21,576.22,212.69,7.89;2,95.94,586.71,196.97,7.86;2,95.94,597.17,85.70,7.86;2,79.19,611.18,213.71,7.89;2,95.94,621.67,196.97,7.86;2,95.94,632.13,162.19,7.86;2,80.20,646.14,212.70,7.89;2,95.94,656.63,196.97,7.86;2,95.94,667.09,196.97,7.86;2,95.94,677.55,24.33,7.86;2,343.80,57.61,212.12,7.89;2,358.95,68.10,196.97,7.86;2,358.95,78.56,196.96,7.86;2,358.95,89.02,51.69,7.86"><head>4 .</head><label>4</label><figDesc>Query-derived features (a) Unigram count: The absolute value of the difference in the count of query tokens in di and dj. (b) Ordered bigram count: The absolute value of the difference in the count of ordered query bigrams in di and dj. (c) Unordered bigram count: The absolute value of the difference in the count of unordered query bigrams in di and dj. (d) Unigram score: The absolute value of the difference in Indri score of the unigram component of the baseline dependence model query. (e) Ordered window score: The absolute value of the difference in Indri score of the ordered window component of the baseline dependence model query. (f) Unordered window score: The absolute value of the difference in Indri score of the unordered window component of the baseline dependence model query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="2,316.81,367.76,239.11,7.86;2,316.81,378.22,239.11,7.86;2,316.81,388.68,239.11,7.86;2,316.81,399.14,239.12,7.86;2,316.81,409.60,239.10,7.86;2,316.81,420.06,239.10,7.86;2,316.81,430.52,239.10,7.86;2,316.81,440.98,239.10,7.86;2,316.81,451.44,239.12,7.86;2,316.81,461.91,143.30,7.86"><head></head><label></label><figDesc>so that each pair of training examples has at least one judged relevant document (dqi). The judgement on the other document (dqj) indicates whether this pair is a positive or negative training example. Thus, the classifier is trained to assign a positive (1) classification to relevant/relevant document pairs, and a negative (0) classification to relevant/nonrelevant pairs. The result of this training produces a classification function h : Dq × Dq → [0, 1], where a value close to 1 indicates a positive classification, and a value close to 0 indicates a negative classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,90.33,628.42,166.04,7.89;4,53.80,648.42,239.11,7.86;4,53.80,658.88,239.11,7.86;4,53.80,669.34,239.11,7.86;4,53.80,679.80,239.11,7.86;4,53.80,690.26,239.10,7.86;4,53.80,700.70,239.11,7.89;4,53.80,711.19,239.12,7.86"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Learned Feature Weights. fluential features in our model are the URL-based features, particularly the similarity of the host name and path portions of the URL. The next most powerful features are the components of the baseline Dependence Model query -the ordered and unordered window scores assigned by Indri. The Out-link count feature is the only webgraph feature that is at all influential in the model. This feature is derived</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="4,316.81,513.11,239.11,7.89;4,316.81,523.57,148.16,7.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Rank distribution of selected documents, and judged relevant documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,316.81,302.07,239.10,7.89;5,316.81,312.53,239.11,7.89;5,316.81,323.00,239.11,7.89;5,316.81,333.46,239.11,7.89;5,316.81,343.92,165.10,7.89"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relative residual performance improvement in statMAP over our baseline vs. number of relevant documents found in the input set. Each point represents a unique input set, and our phase-1 input (CMU.1) is shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,53.80,302.07,239.11,7.89;6,53.80,312.53,199.86,7.89"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative residual statMAP for each input set as feedback document weight increases.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.42,701.50,155.06,7.47"><p>http://www.lemurproject.org/indri</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,321.42,711.83,225.53,7.47"><p>http://ciir.cs.umass.edu/ metzler/indri-tb05.tgz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,58.40,693.25,234.50,7.86;2,53.80,702.22,239.10,7.86;2,53.80,711.19,151.59,8.12"><p>All webgraph features were computed with the use of the WebGraph software package, available from http://webgraph.dsi.unimi.it/<ref type="bibr" coords="2,193.12,711.19,9.20,7.86" target="#b3">[3]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,321.42,711.83,108.07,7.47"><p>http://en.wikipedia.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,58.28,575.99,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,593.13,220.32,7.86;6,72.59,603.59,220.32,7.86;6,72.59,614.05,20.96,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,188.43,593.13,88.75,7.86">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,72.59,603.59,40.32,7.86">SIGIR &apos;01</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,625.51,182.11,7.86;6,72.59,635.97,198.52,7.86;6,72.59,646.43,150.09,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,129.62,625.51,125.08,7.86;6,72.59,635.97,120.38,7.86">Pattern Recognition with Fuzzy Objective Function Algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,657.89,205.38,7.86;6,72.59,668.35,219.44,7.86;6,72.59,678.81,135.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,166.13,657.89,111.83,7.86;6,72.59,668.35,92.27,7.86">The webgraph framework I: compression techniques</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,183.47,668.35,41.94,7.86">WWW &apos;04</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,72.59,690.26,216.60,7.86;6,72.59,700.72,208.77,7.86;6,72.59,711.19,48.33,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,221.60,690.26,67.59,7.86;6,72.59,700.72,105.45,7.86">Query expansion using random walk models</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,196.95,700.72,39.82,7.86">CIKM &apos;05</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,57.64,195.44,7.86;6,335.61,68.10,214.06,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,455.41,57.64,75.63,7.86;6,335.61,68.10,126.21,7.86">Paired sampling in density-sensitive active learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,480.24,68.10,41.71,7.86">ISAIM &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,79.55,214.07,7.86;6,335.61,90.02,173.11,7.86;6,335.61,100.48,128.97,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,475.58,79.55,74.10,7.86;6,335.61,90.02,169.09,7.86">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-03">March 1990</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,111.93,189.97,7.86;6,335.61,122.39,211.16,7.86;6,335.61,132.85,117.12,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,460.17,111.93,65.40,7.86;6,335.61,122.39,64.47,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,418.98,122.39,40.45,7.86">SIGIR &apos;01</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,144.31,157.34,7.86;6,335.61,154.77,205.56,7.86;6,335.61,165.23,192.44,7.86;6,335.61,175.69,209.03,7.86;6,335.61,186.15,187.57,7.86;6,335.61,196.62,166.87,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,377.47,144.31,115.47,7.86;6,335.61,154.77,205.56,7.86;6,335.61,165.23,192.44,7.86;6,335.61,175.69,61.15,7.86">Incorporating Relevance and Psuedo-relevance Feedback in the Markov Random Field Model: Brown at the TREC&apos;08 Relevance Feedback Track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,415.54,175.69,40.48,7.86">TREC &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Best results in track. This paper supersedes an earlier version appearing in conference&apos;s Working Notes</note>
</biblStruct>

<biblStruct coords="6,335.60,208.07,211.56,7.86;6,335.61,218.53,202.29,7.86;6,335.61,228.99,175.66,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,453.78,208.07,93.38,7.86;6,335.61,218.53,114.26,7.86">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,468.65,218.53,40.45,7.86">SIGIR &apos;05</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,240.45,212.05,7.86;6,335.61,250.91,216.71,7.86;6,335.61,261.37,111.73,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,498.53,240.45,49.13,7.86;6,335.61,250.91,216.71,7.86;6,335.61,261.37,24.84,7.86">Indri TREC Notebook 2006: Lessons learned from Three Terabyte Tracks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,379.15,261.37,40.48,7.86">TREC &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,272.83,208.27,7.86;6,335.61,283.29,201.07,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,458.66,272.83,85.22,7.86;6,335.61,283.29,53.55,7.86">Active learning using pre-clustering</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.53,283.29,38.51,7.86">ICML &apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,294.75,173.08,7.86;6,335.61,305.21,206.76,7.86" xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,419.04,294.75,89.65,7.86;6,335.61,305.21,96.09,7.86">Information Retrieval. Butterworth-Heinemann</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Newton, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,316.67,185.62,7.86;6,335.61,327.13,191.99,7.86;6,335.61,337.59,173.03,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,443.93,316.67,77.30,7.86;6,335.61,327.13,191.99,7.86;6,335.61,337.59,82.54,7.86">Improving retrieval performance by relevance feedback. Readings in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
