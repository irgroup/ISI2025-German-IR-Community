<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,102.01,468.00,16.20">Experiments with ClueWeb09: Relevance Feedback and Web Tracks</title>
				<funder>
					<orgName type="full">Amazon Web Services in Education Research Grant</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Waterloo</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.40,134.36,90.24,10.91"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<address>
									<country>University of Waterloo</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.52,134.36,105.90,10.91"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.06,134.36,103.83,10.91"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,102.01,468.00,16.20">Experiments with ClueWeb09: Relevance Feedback and Web Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0E1B440582B6296DE8A46EF1E2E05319</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report on our TREC experiments with the ClueWeb09 document collection. We participated in the relevance feedback and web tracks. While our phase 1 relevance feedback run's performance was good, our other relevance feedback and web track submissions' performances were lacking. We suspect this performance difference is caused by the Category B document subset of the ClueWeb09 collection having a higher prior probability of relevance than the rest of the collection. Future work will involve a more detailed error analysis of our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our goals for TREC this year were to get experience with the multi-terabyte ClueWeb09 document collection, and to establish baselines on the collection with various retrieval techniques. As such, we participated in the relevance feedback (RF) track as well as both the ad-hoc and diversity tasks of the Web track. Each of these tracks utilized the same 50 topics and document collection. We first briefly describe how we indexed ClueWeb09 and then explain our approaches for each of these tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Indexing ClueWeb09 English</head><p>To index ClueWeb09 English (ClueWeb09 English <ref type="bibr" coords="1,289.96,593.65,7.37,8.94" target="#b0">[1]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b1">[2]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b2">[3]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b3">[4]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b4">[5]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b5">[6]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b6">[7]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b7">[8]</ref><ref type="bibr" coords="1,297.33,593.65,3.69,8.94" target="#b8">[9]</ref><ref type="bibr" coords="1,72.00,605.61,11.62,8.94">[10]</ref>), we used Indri <ref type="bibr" coords="1,158.88,605.61,10.52,8.94" target="#b5">[6]</ref> version 4.10, which had been modified by the Lemur Project developers to index ClueWeb09. We built 30 separate indexes across 10 machines. Each machine was a modern (2-2.3 GHz CPU) 4 core machine with 8 GB of RAM and approximately 800 GB of disk storage. Building the indexes took less than 2 days using 3 index processes per machine. The CatB subset (English 1) took 31.5 hours to index. Overall time spent to index the collection was about 2 weeks of work including moving 2 TB of data across the Internet. We stemmed all terms with the Krovetz <ref type="bibr" coords="1,391.55,232.91,10.52,8.94" target="#b3">[4]</ref> stemmer and did not perform stop word removal at index time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relevance Feedback Track</head><p>The relevance feedback track was run in two phases. The first phase allowed participating sites to submit two runs. Each run was limited to have at most 5 results for each topic and participants could only use the CatB subset of the ClueWeb09 collection. NIST had the assessors completely judge the Phase 1 runs for relevance. Phase 2 of the RF track consisted of submitting a baseline (no feedback) run of 2500 results and then runs utilizing the judgments from the phase 1 runs. Each site was assigned its own phase 1 runs as well as 5 other sites' phase 1 runs to use for feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phase 1 Methods</head><p>For our phase 1 run, we followed the technique of Metzler et al. <ref type="bibr" coords="1,378.33,496.95,10.52,8.94" target="#b7">[8]</ref> that combines dependence models <ref type="bibr" coords="1,326.19,508.91,10.52,8.94" target="#b6">[7]</ref> and relevance models <ref type="bibr" coords="1,438.94,508.91,9.96,8.94" target="#b4">[5]</ref>. Table <ref type="table" coords="1,487.14,508.91,4.98,8.94" target="#tab_0">1</ref> shows our retrieval parameter settings. We call this formulation DMRM3 to represent a dependence model (DM) mixed with a relevance model (RM). The successful technique of mixing a relevance model with the original query has come to be called RM3 <ref type="bibr" coords="1,480.02,568.68,9.96,8.94" target="#b0">[1]</ref>.</p><p>In this formulation, the relevance model is typically computed from the top documents retrieved by the dependence model. For our phase 1 run, we obtained the pseudo-relevant feedback documents from the Yahoo (search.yahoo.com) and Bing (www.bing.com) web search engines. Our reasoning for this was to obtain from the smaller CatB collection a retrieval that would mimic as much as possible the assumed good retrievals of the major search engines.</p><p>For each of the topics, we retrieved the top 10 results from both Yahoo and Bing using the various APIs provided by the search engines. We also ob- For each result, we fetched the actual web page or pdf document. To easily compute a model of the documents, for each topic, we created an Indri index of the, up to, 20 documents. We stopped the collection with an in-house list of 418 stop words (see Appendix A of <ref type="bibr" coords="2,137.63,379.85,10.79,8.94" target="#b8">[9]</ref>) and we stemmed with the Krovetz stemmer <ref type="bibr" coords="2,111.93,391.81,9.96,8.94" target="#b3">[4]</ref>. After building each index, we computed a maximum likelihood estimated (MLE) collection model for each collection. In effect, we concatenated the documents and built a MLE language model. We truncated each model to the 50 terms with the highest probability. We then mixed this model with the dependence model.</p><p>We made a mistake in the building of the final DMRM3 queries. The web expansion component was weighted as part of the original query instead of mixing the DM query with the RM query. Instead of the queries being constructed as #weight( 0.3 dependence model 0.7 pseudo relevance model ), we constructed them as #weight( 0.3 #weight( 0.8 unigram model 0.1 ordered windows 0.1 unordered windows 0.7 pseudo relevance model ) ), which is the same effectively as #weight( 0.8 unigram model 0.1 ordered windows 0.1 unordered windows 0.7 pseudo relevance model ).</p><p>We retrieved the top 10 documents using these queries. We did stop word removal at query time with the above described 418 stop words. We call this run WatS.1+WatS.2.</p><p>We created two phase 1 runs from WatS.1+WatS.2. WatS.1 consisted of the rank 1-5 results and WatS.2 consisted of ranks 6-10. By doing this we were able to have this CatB run judged to a depth of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phase 2 Methods</head><p>Our phase 1 runs, like any other runs utilizing commercial web search engines, utilize the secret retrieval methods of the web search engines. As such, our phase 1 runs are non-reproducible. While it is interesting to see what is possible with such expansions, these expansions are not the equivalent of doing one's own expansion on a web sized collection.</p><p>For our phase 2 baseline, WatS.base, we again utilized a dependence model with a relevance model for pseudo relevance feedback, but this time the initial retrieval was done with the dependence model. We built the relevance model from the top 25 results. The documents in the baseline pseudo-relevance model are weighted based on the dependence model's retrieval scores as is standard for relevance models.</p><p>Our relevance feedback runs used the following phase 1 runs: fub.1, SIEL.1, Sab.1, UCSC.1, twen.1, twen.2, WatS.1, and WatS.2. If the given phase 1 run had any relevant documents, we built a model of those relevant documents and replaced the pseudorelevance model, otherwise the baseline query was used without change. We weighted the documents in the relevance model equally.</p><p>For our baseline and feedback runs, we computed a non-traditional relevance model. In our early experimentation with the ClueWeb09 collection, we observed that the relevance models of top ranked documents appeared to contain many terms that we felt might be misleading because they were particular to the source of the document and did not reflect terms relevant to the topicality of the document. In an attempt to correct for this issue, we first built a relevance model. From this model, we removed all stop words and stop word stems, and then took the 100 most probable terms and for each of those terms, we computed their pointwise KL-divergence:</p><formula xml:id="formula_0" coords="3,125.36,285.68,171.41,23.53">P (w|M R ) log P (w|M R ) P (w|C) (<label>1</label></formula><formula xml:id="formula_1" coords="3,296.77,293.04,4.24,8.94">)</formula><p>where w is the word stem, P (w|M R ) is the probability of w given the relevance model M R , and P (w|C) is the probability of w given a maximum likelihood estimated model (MLE) of the collection. From these 100 stems, we built an expansion query with the 25 stems with the highest pointwise KLdivergence. We gave each stem a weight equal to the maximum of its pointwise KL-divergence or 0.000001. This ad-hoc weighting scheme looked okay but was not based on any existing practice or theory.</p><p>We then mixed the original dependence model query with our expansion query. The dependence model had a query weight of 0.3 and the expansion model had a weight a 0.7.</p><p>Our phase 2 runs retrieved from the ClueWeb09 English [1-10] collection with the parameters of Table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussion</head><p>Our phase 1 run, WatS.1 obtained a P5 of 0.440. WatS.2 obtained a P5 of 0.592. When combined, the original run had a P10 of 0.516. We refer to the original, combined run as WatS.1+WatS.2.</p><p>Out of 30 phase 1 runs submitted by participants, WatS.2 had the highest P5 (rank 1). The rank 2 submission had a P5 of 0.504. WatS.1 had a rank of 8.</p><p>Our phase 2 runs performed significantly worse than our phase 1 run, WatS.1+WatS.2. Nevertheless, as Table <ref type="table" coords="3,131.59,677.34,4.98,8.94" target="#tab_1">2</ref> shows, our feedback runs did perform better than our phase 2 baseline, WatS.base.</p><p>The relatively poor performance of our phase 2 feedback runs could be from several causes:</p><p>1. We could have made a mistake in either the indexing or retrieval from English [1-10] vs. CatB, or there could be some sort of bug in Indri only exposed by the English [1-10] indexes.</p><p>2. The assessors could have changed their criteria of relevance when they later judged the phase 2 runs (and other tracks such as the web track).</p><p>3. The documents in CatB could have a much higher prior probability of relevance than the documents in English [1-10] minus CatB.</p><p>4. Since the phase 2 runs are judged over the residual collection, if the phase 1 runs consumed a significant fraction of the relevant documents, then phase 2 runs might perform worse than phase 1.</p><p>While at this time we do not have proof, we suspect the answer is #3 above, i.e. CatB has a higher prior probability of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Web Track: Ad-hoc</head><p>For the web ad-hoc track, we retrieved from the English [1-10] collection and employed conventional retrieval techniques to establish various baselines on the ClueWeb09 collection. Our runs were: WatSql, WatSdmrm3, and WatSdmrm3we. Table <ref type="table" coords="3,490.36,403.04,4.98,8.94" target="#tab_0">1</ref> shows the retrieval parameters.</p><p>WatSql is a simple query likelihood retrieval given the query. WatSdmrm3 is a dependence model (DM) plus a relevance model (RM3) as described in section 3.1. WatSdmrm3we used the same queries as the run described in Section 3.1 as WatS.1+WatS.2, which is a DMRM3 run with the RM3 component coming from the top results of the Yahoo and Bing search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and Discussion</head><p>Run WatSdmrm3we utilized the same queries as our combined phase 1 RF track run, WatS.1+WatS.2 (see section 3.1). The only difference between the runs is that WatSdmrm3we retrieved against English [1-10] while WatS.1+WatS.2 retrieved against CatB. The assessors judged WatSdmrm3we to at least a depth of 12 and WatS.1+WatS.2 was judged to a depth of 10. WatS.1+WatS.2 were judged as separate runs as part of phase 1 of the RF track and then WatSdmrm3we was judged at a later date as part of the web ad-hoc track.</p><p>The P10 of WatS.1+WatS.2 is 0.516. The P10 of WatSdmrm3we is 0.164. The P10 of WatSdmrm3 is 0.118. The P10 of WatSql is 0.084. Assuming no issues with Indri and no change in assessor judging from phase 1 of the relevance feedback track to the judging of the web ad-hoc track, the DMRM3 web expanded queries of both WatS.1+WatS.2 and WatS-dmrm3we perform significantly better over CatB than over English <ref type="bibr" coords="4,152.68,134.95,7.59,8.94" target="#b0">[1]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b1">[2]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b2">[3]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b3">[4]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b4">[5]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b5">[6]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b6">[7]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b7">[8]</ref><ref type="bibr" coords="4,160.26,134.95,3.79,8.94" target="#b8">[9]</ref><ref type="bibr" coords="4,164.06,134.95,11.38,8.94">[10]</ref>. Initial inspection of results shows that English [1-10] minus CatB may contain more spam than CatB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Web Track: Diversity</head><p>We submitted three runs to the web diversity track: WatSklq, WatSklfb, and WatSklfu. For each run we first obtained the 25 highest scoring point-wise KLdivergence stems from the 100 most frequent stems in the top 25 documents retrieved by a query likelihood retrieval ranking of the documents containing all of the query terms. We exclude the query stems and 418 stopwords from being part of this set. We used this set of stems in three different ways to generate our three runs. All runs were over the English <ref type="bibr" coords="4,277.22,320.72,7.93,8.94" target="#b0">[1]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b1">[2]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b2">[3]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b3">[4]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b4">[5]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b5">[6]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b6">[7]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b7">[8]</ref><ref type="bibr" coords="4,285.15,320.72,3.96,8.94" target="#b8">[9]</ref><ref type="bibr" coords="4,289.11,320.72,11.89,8.94">[10]</ref> subset. We next describe in more detail the methods used for our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>We generated 25 stems following the method explained in Section 3.2 except that we used the top 25 documents returned by a query likelihood retrieval using the query terms and requiring that all returned documents contain all query terms.</p><p>For WatSklq, we used the stems to generate 25 additional retrievals. To generate the 25 retrievals, for each of the 25 stems, we appended the stem to the topic's query to generate a new query. The query was thus of the form #filreq( #band( original query new stem ) #combine( original query new stem ) ) in the Indri query language.</p><p>To produce WatSklq, we kept the initial retrieval and these 25 lists in a queue and selected documents from them as follows. The lists are numbered from 0 to 25. List 0 is initially the retrieval of the query alone without any added stems. We repeat the following steps until we have enough results or have emptied all lists. First, we select as our current document the document at rank 1 of list 0. The current document is added to our run's results. We then move list 0 to the end of the queue. For lists at queue positions 0 to 24, we stable sort them in descending order by the rank at which we find the currently selected document. If a list does not contain the document, the document is assumed to be at rank infinity. We remove from each list the current document. When a list has no remaining documents, it is removed from the queue.</p><p>The motivation behind this method is to retrieve documents containing different relevant nuggets. The hope is that each list will have a different set of relevant documents. We prefer documents at the top of the lists and we prefer as the next list the list that is most different from the current document. Our "similarity measure" between the current document and a list is the document's rank in the list.</p><p>For WatSklfb, we used the 25 stems to expand the query. The expansion terms' relative weights were equal to their pointwise KL-divergence or 0.000001 if the pointwise KL-divergence was negative. The original query had a weight of 0.3 and the expansion terms' query was given a weight of 0.7. Our original query was of the form #filreq( #band( query ) #combine( query ) ) in the Indri query language.</p><p>For WatSklfu, we generated 25 additional retrievals in the same manner as for WatSklq. We then used a variant of reciprocal rank fusion <ref type="bibr" coords="4,448.72,294.63,10.52,8.94" target="#b2">[3]</ref> to join the original retrieval list and these 25 lists. We gave list 0 (the initial query) a weight of 0.5 and the 25 "query plus stem" runs a weight of 0.5 times the stem's pointwise kl-divergence where negative pointwise kl-divergences were given a value of 0.000001. Then documents were scored based on the reciprocal rank fusion with each list contributing to the score of the document the value:</p><formula xml:id="formula_2" coords="4,395.72,409.11,140.03,23.53">w r + 1 + k (<label>2</label></formula><formula xml:id="formula_3" coords="4,535.76,416.46,4.24,8.94">)</formula><p>where w is the weight of the list, r is the rank of the document, and k = 60. The 1 in the divisor sum appears to be a coding mistake that we overlooked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>The official measure for the diversity track is α-nDCG@10, which is a measure that evaluates a result list on its ability to satisfy multiple information needs with novel and diverse results <ref type="bibr" coords="4,476.00,567.61,9.97,8.94" target="#b1">[2]</ref>. All of our runs failed to perform well. WatSklfu, WatSklfb, and WatSklq have average α-nDCG@10's of 0.057, 0.077, and 0.090 respectively. In general, our runs either produced what might be considered an acceptable score for a topic or produced a score of zero or near zero. Our best run, WatSklq, scored at or above the median on 29 out of 50 topics. While WatSklfb did not perform well overall, it did obtain the high score for topic 44, "map of the united states." Our results may have been hurt by our poorer performance on the English [1-10] subset compared to CatB (see Section 3.3).</p><p>While indexing the ClueWeb09 collection in a reasonable amount of time requires a significant amount of hardware, it is feasible with existing retrieval systems such as Indri. By running the same queries on both the CatB subset and the larger English [1-10] subset, we believe we have discovered that CatB has a higher prior probability of relevance than the remainder of the collection. We plan in future work to examine in more detail the causes of our runs' varied performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,72.00,73.58,450.14,239.96"><head>Table 1 :</head><label>1</label><figDesc>Retrieval parameters. tained results from Google, but were unable to utilize them because of technical issues and a lack of time.</figDesc><table coords="2,89.85,73.58,45.29,8.94"><row><cell>Parameter</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.00,73.58,229.01,150.35"><head>Table 2 :</head><label>2</label><figDesc>Relevance Feedback Results. WatS.base is our baseline without feedback.</figDesc><table coords="3,138.33,73.58,96.37,116.93"><row><cell>Run</cell><cell>P10</cell></row><row><cell>WatS.base</cell><cell>0.118</cell></row><row><cell>WatS.twen.1</cell><cell>0.222</cell></row><row><cell>WatS.fub.1</cell><cell>0.233</cell></row><row><cell>WatS.SIEL.1</cell><cell>0.257</cell></row><row><cell>WatS.Sab.1</cell><cell>0.259</cell></row><row><cell>WatS.twen.2</cell><cell>0.263</cell></row><row><cell cols="2">WatS.UCSC.1 0.280</cell></row><row><cell>WatS.WatS.1</cell><cell>0.306</cell></row><row><cell>WatS.WatS.2</cell><cell>0.306</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>Special thanks to <rs type="person">David Fisher</rs> for his help with indexing ClueWeb09. <rs type="person">Iván Patarroyo</rs> and <rs type="person">Ben Lin</rs> wrote and ran the code to query the search engines and download the corresponding web pages.</p><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, in part by an <rs type="funder">Amazon Web Services in Education Research Grant</rs>, and in part by the <rs type="funder">University of Waterloo</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,87.51,445.02,213.56,8.94;5,87.50,456.98,213.57,8.94;5,87.50,468.94,213.54,8.94;5,87.50,480.89,213.52,9.34;5,87.50,492.84,213.53,9.34;5,87.50,504.80,213.52,8.94;5,87.50,516.75,143.46,8.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,87.50,468.94,191.21,8.94">UMass at TREC 2004: Novelty and HARD</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,284.21,480.89,16.81,9.34;5,87.50,492.84,147.38,9.34">The Twelth Text REtrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2003">2003. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.51,535.98,213.54,8.94;5,87.50,547.94,213.54,8.94;5,87.50,559.89,213.57,8.94;5,87.50,571.84,213.49,9.34;5,87.50,583.80,52.30,8.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,122.16,559.89,178.91,8.94;5,87.50,571.84,74.62,8.94">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,186.70,571.84,39.34,9.34">SIGIR&apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.51,603.03,213.52,8.94;5,87.50,614.98,213.55,8.94;5,87.50,626.93,213.52,9.34;5,87.50,638.89,120.42,8.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,87.50,614.98,213.55,8.94;5,87.50,626.93,144.76,8.94">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,256.75,626.93,39.34,9.34">SIGIR&apos;09</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.51,658.12,213.52,8.94;5,87.50,670.07,213.52,9.34" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,142.80,658.12,158.22,8.94;5,87.50,670.07,29.91,8.94">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,136.28,670.07,39.34,9.34">SIGIR&apos;93</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,87.51,689.29,213.52,8.94;5,87.50,701.25,213.49,9.34;5,87.50,713.21,52.30,8.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,229.53,689.29,71.51,8.94;5,87.50,701.25,71.13,8.94">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,185.59,701.25,39.34,9.34">SIGIR&apos;01</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,326.48,75.17,213.54,8.94;5,326.47,86.51,212.69,9.78" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="5,366.71,75.17,173.31,8.94;5,326.47,87.12,28.52,8.94">Lemur Toolkit for Language Modeling and IR</title>
		<author>
			<persName coords=""><surname>Lemur</surname></persName>
		</author>
		<ptr target="http://www.lemurproject.org/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,326.48,107.05,213.56,8.94;5,326.47,119.01,213.52,9.34;5,326.47,130.96,120.43,8.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,459.54,107.05,80.50,8.94;5,326.47,119.01,146.55,8.94">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,495.74,119.01,39.34,9.34">SIGIR&apos;05</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,326.49,150.89,213.57,8.94;5,326.47,162.84,213.53,8.94;5,326.47,174.80,213.55,8.94;5,326.47,186.76,123.28,9.34" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,357.09,162.84,182.91,8.94;5,326.47,174.80,195.73,8.94">UMass robust 2005 notebook: Using mixtures of relevance models for query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,326.47,186.76,92.66,9.34">TREC 2005 Notebook</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,326.49,206.68,213.53,9.34;5,326.47,218.63,213.53,9.34;5,326.47,230.59,190.77,8.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="5,400.40,206.68,139.62,9.34;5,326.47,218.63,148.19,9.34">Evaluation of Find-Similar with Simulation and Network Analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
