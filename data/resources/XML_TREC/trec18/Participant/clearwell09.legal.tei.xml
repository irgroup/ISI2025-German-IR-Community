<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.62,76.38,424.62,16.65">Clearwell Systems at TREC 2009 Legal Interactive</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.22,98.92,81.55,11.10"><forename type="first">Venkat</forename><surname>Rangan</surname></persName>
							<email>venkat.rangan@clearwellsystems.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Clearwell Systems</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.07,98.92,66.19,11.10"><forename type="first">Maojin</forename><surname>Jiang</surname></persName>
							<email>maojin@clearwellsystems.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Clearwell Systems Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.62,76.38,424.62,16.65">Clearwell Systems at TREC 2009 Legal Interactive</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A49F1AF3A97D5AF0402EB8118B484673</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The TREC Legal Track 2009 features an Interactive Task that is designed to replicate real-world challenges in producing a collection of responsive documents from a large collection of documents. The task required us to produce responsive documents from any of the seven topics, which are production requests. Clearwell Systems incorporated novel methods for producing a responsive collection using a combination of automated sampling, evaluation of the samples, and using the samples as input into a blind relevance feedback engine. The algorithms applied use an automatic correlation covariance matrix for automatic evaluation of the samples and, using the correlation coefficient, determine whether the process of blind feedback converges to a highly correlated set of responsive documents. The number of iterations of sampling, the K-value for blind feedback, along with the final convergence threshold are monitored. The F-measure results of this are compared across the three different Interactive Topics that Clearwell participated in, for discussions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In TREC 2009 Legal Track, Clearwell participated in the Interactive Task and worked on in total three topic requests, including topic 201 (prepay transaction), topic 202 (FAS 140/125 compliance) and topic 205 (energy load).</p><p>The Interactive Task is motivated by the real world litigation process and aims at modeling the real-world conditions in which e-discovery is pursued by law firms or other e-discovery companies <ref type="bibr" coords="1,97.70,461.20,9.78,8.10" target="#b6">[6]</ref>. From the previous Legal Track task and the current e-discovery practices, such a legal search embodied in the Interactive Task is an iterative process that requires collaboration among members of a team consisting of people from different areas <ref type="bibr" coords="1,75.74,503.44,9.93,8.10" target="#b4">[4,</ref><ref type="bibr" coords="1,89.06,503.44,6.62,8.10">7]</ref>, a well-thought-out process <ref type="bibr" coords="1,204.17,503.44,9.92,8.10" target="#b1">[1,</ref><ref type="bibr" coords="1,217.49,503.44,7.56,8.10" target="#b5">5]</ref> and lastly but not the least importantly a powerful software system that allows and automates such collaboration and process as much as possible, in addition to providing state-of-the-art search techniques.</p><p>As a result, success of such an e-discovery task does not merely rely on advanced search techniques. It also calls for well-planned and close collaboration of searchers, especially with senior litigators, which are the roles that a Topic Authority (TA) plays in the Interactive Task. It is a senior litigator, or a TA, who defines the scope of responsiveness that each team as a searcher should have to replicate at the time when it performs searches to produce relevant documents to meet a topic request. Thus, it is our interest both to explore state-of-the-art search techniques and to leverage the advanced case management function of Clearwell e-discovery systems for the Legal Interactive Task. In this paper, we will address our work in both directions.</p><p>The remainder of this paper is organized as follows. In the next section, we give a brief description of the data set used for the Interactive Task. In section 3, we introduce the Clearwell ediscovery system that we used throughout the execution of the three interactive tasks. Then, in section 4, we address our approaches to indexing, search, advanced analysis, case management, review and production and TA communication, after which we give our results in section 5. Lastly, we give the conclusion and discuss possible future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA COLLECTION</head><p>This year's test collection for the 2009 Interactive Task is Enron Collection, which consists of 569,034 unique email messages, in the native format of Microsoft ".msg" files, together with attachments, which are of a variety of file types, such as PDF files and JPEG images, etc. Including attachments and duplicate messages, it amounts to over two million documents. The collection also provides these email messages in plain text version, which contains only extracted text from messages and attachments. This collection is generated by Clearwell Systems from the collection that was originally released by the Federal Energy Regulatory Commission (FERC) and acquired by Clearwell from Aspen Systems. Further cleanup process was performed by University of Maryland.</p><p>Together with the collection are a mock complaint and a list of seven topic requests, out of which we were assigned topic 201, 202 and 205. Note that in the mock complaint, the imaginary major defendant is named Volteron Corp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ESA -CLEARWELL E-DISCOVERY PLATFORM</head><p>Throughout the execution of the three topic request production, we used the Clearwell enterprise-class e-discovery platform to index and process the messages and attachments, perform searches, manage collection and search results as cases, generate samples of search results for review and assessment, and finally export search results for production. In addition, the sampling engine of the platform was enhanced to automatically sample both the retrieved and un-retrieved collections, and use the samples to measure dispersion of the sampled documents from the mean vector of the document feature vectors of the top N results from the original search. If the dispersion was beyond a certain established threshold, the samples are analyzed to automatically identify additional salient search terms for the next iteration of search. We then observed the convergence of this iterative search. We also evaluated a human judgment of samples and compared the results from the manual search query augmentation against the automated search query augmentation.</p><p>In this section, we discuss the key features of the Clearwell ediscovery system that are used for the Interactive Task. In the remaining part of this paper, we will use ESA to refer to this system. As depicted in Figure <ref type="figure" coords="2,152.90,232.55,3.41,8.10" target="#fig_0">1</ref>, ESA unifies the following key capabilities in its powerful case management, including processing, analysis, search and cull-down, and review and production, running as a Web application, easily accessible from a modern Web browser such as Internet Explorer.</p><p>Case management allows collaboration among multiple users to work on the same or different projects within or across multiple cases. In a case, different data sources can be indexed and analyzed for various e-discovery tasks. Users can be assigned different roles to access the complete or different parts of the case data, performing searches, reviewing, tagging and finally producing desired data for e-discovery production.</p><p>Processing provides capability to identify, index and analyze over 400 different document types. Particularly, it is able to read emails to identify and extract various regions, such as sender, recipient, subject and attachments, which facilitate various analysis functions. In addition, it also applies linguistic techniques such as sentence structure, part of speech information, word sense disambiguation to detect and label noun phrases in textual data, using commercial application from Basis Technologies<ref type="foot" coords="2,260.93,443.81,3.00,5.40" target="#foot_0">1</ref> . As we know, an inherent property of email is its high redundancy in that multiple copies of the same message exist due to the fact that the same message often has more than one recipient. To handle this in order to improve the quality of search results, ESA also provides intelligent de-duplication step to detect such redundancy.</p><p>Analysis adopts proprietary, patent-pending algorithms known as "Dynamic Content Analysis TM " to create the "Clearwell Master Index TM ", which is much more sophisticated and powerful than a traditional full-text index. A message stream often weaves naturally into discussion threads, which group together the initial message, replies, carbon copies, forwards and near duplicates. By tracing a thread, one can quickly identify all the participants, and determine who knew what and when, which proves invaluable when performing early case assessments. Another analysis feature is topic clustering, which automatically groups messages based on topics they discuss by incorporating a text clustering algorithm derived from K-means hard clustering <ref type="bibr" coords="2,204.92,631.63,9.85,8.10" target="#b8">[8]</ref>. One last important analytic feature is to clean up, merge and optimize various data structures previously generated so that queries can be efficiently executed, also consisting of three types of analyses, namely 1) "people analytics", which allows a user to access a list of top custodians for a search or monitor communications between regulated and non-regulated divisions within a company; 2) "file analytics", which allows investigators to easily determine everyone who possesses or has sent or received a file of interest and allows reviewers to review a file once instead of multiple times; and 3) "term analytics", which analyzes noun phrases to help users uncover secret project names and code words that may be relevant to a case or investigation by applying Natural Language Processing techniques (NLP).</p><p>Search and cull-down offers a suite of state-of-the-art search techniques through a user-friendly interface. Especially tailored to email search, it supports searches based on senders, recipients, direction, subject, attachment names and types, and date range, etc. As we know, emails often demonstrate a free writing style, which means senders who write messages do not pay too much attention to spelling or grammar. To cope with this, ESA supports wildcard, fuzzy and proximity searches as well. Another useful search feature is facilities for interactive query expansion (IQE) <ref type="bibr" coords="2,317.93,281.30,10.46,8.10" target="#b9">[9,</ref><ref type="bibr" coords="2,328.39,281.30,10.46,8.10" target="#b10">10]</ref>, packaged as a "search preview" (an illustration of this feature is shown in Figure <ref type="figure" coords="2,415.74,291.86,3.47,8.10" target="#fig_2">3</ref>, Section 4.3). This provides visibility into matching keyword variations for wildcard and stemming searches prior to running a search. Users can see all variations of keywords used in search with their frequencies in the case data, which makes it possible for users to selectively include relevant variations or exclude false positive variations from their search query, thereby adding relevant documents and removing irrelevant documents from search results. In order to assess the quality of a search, a popular method is to make a sample of search results for assessment. This can be done within ESA by either manually selecting documents or by automatic and random selection, at a user's discretion.</p><p>Review and production provides multiple viewing modes giving users the flexibility to view header, snippet or document detail. In addition to linear review of individual documents, ESA also allows users to view documents as a group based on discussion threads or topic clusters, significantly increasing the review throughput. In addition, ESA allows case administrators to bulk tag documents into review sets and assign them to an individual for review. Case reviewers can review documents in their favorite reviewing modes and easily view and update tags for documents. Lastly, ESA allows results to be produced in multiple production formats (e.g. CSV or XML) that may meet different needs for different cases or tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODOLOGY AND TASK EXECUTION</head><p>This section describes the methods and the process we used during the execution of three interactive tasks. A diagram of the execution is depicted in the Figure <ref type="figure" coords="2,446.90,613.51,3.40,8.10" target="#fig_1">2</ref>. Simply speaking, a searcher constructs a search based on his or her understanding of the information needed behind a topic request, performs the search, then communicates with a TA about the effectiveness of the current search (e.g. by asking a TA to judge responsiveness of a document, etc.). In addition, search results are randomly sampled for internal assessment. Then, by reviewing both the TA"s feedback and judgment of the current search and the internal assessment result, the searcher will adjust or correct his or her understanding of the topic request and construct a new search, which essentially repeats the above steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Iterations</head><p>In our execution of the tasks, six iterations were performed and tracked. Most of the iterations had a duration of one week. The first iteration spanned over a month after the task guideline and court complaint were released by TREC and until an orientation call was given for a topic. The last iteration lasted over one week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Processing</head><p>All messages and attachments in their native formats were processed as follows. No word was removed (this enables us to do some pattern-based search; for example, we can search and find out noun phases following "such as" in the collection). Terms were stemmed after tokenization. Further, all terms were lowercased and indexed. In addition, different regions of an email were identified and indexed separately, such as "subject", "sender", "recipient", timestamps, attachment names, etc., which facilitated search by these regions later. Thanks to the capability of supporting over 400 different file types in ESA in native formats, an individual attachment was detected with its original file name in each message and its textual content was able to be correctly parsed and tokenized, which maintained the integrity of a parent message and its attachment(s).</p><p>Beyond simple tokenization that is often done on unstructured plain text and indexing, we also performed three types of postprocessing "analyses" as discussed in section 3 previously. By these steps, structures of emails were analyzed and retained such that the related messages that are replies or forwards of the same messages were grouped together as a discussion thread. The messages were also grouped by participants so that we can easily and efficiently find out who sent what messages to whom and, from the timestamps, when they were sent. Duplicate messages were detected and labeled. Topics were discovered by applying NLP algorithms. Finally, data storages were optimized for efficient searches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Search</head><p>First, we take advantage of the powerful yet flexible Boolean search features provided in ESA. This year"s submission is based on "document-level" responsiveness. That is, each individual attachment and each parent message are considered separate individual documents. So, to produce any relevant document, we need to restrict a search to either the parent message or to the attachments to locate the occurrence of relevant information. In ESA, we can easily achieve this by searching by regions, including "subject line", "sender", "recipient", "attachment file name", "quoted text in a message", "message body without quoted text" and attachments, among other things.</p><p>Another important feature we used in ESA is called "search preview" as aforementioned. Using it, we can determine if we need to search by using variations of a search word in order to get an estimate of how many unique emails and how many unique files may contain the word and its variations. This is especially useful in handling free writing styles in email settings since it is not rare that neither grammar nor spelling is strictly followed in writing messages. For example, Figure <ref type="figure" coords="3,464.62,185.99,4.50,8.10" target="#fig_2">3</ref> shows the variations of the words "yosemite" and "prepay".</p><p>As we know, a message is often forwarded or replied to more than once. Such follow-up messages together with the original message form a natural discussion thread. Although sometimes arguable, it is often true that if a message is relevant, its follow-up messages can be also deemed relevant. As a result, in our work, once we found a relevant parent message, we also included the messages in the same discussion as relevant. In the future, we will pursue discriminative power at a finer granularity based on this. For example, if a follow-up message does not quote the original message, and does not contain any new text that is relevant, the message will not be considered relevant. In e-discovery, especially in searching for documents for civil litigation use in courts, the information need is often so complex that it is hard to describe as keyword searches at first sight. For example, topic 202 request asks searchers to find "all documents or communications that describe, discuss, refer to, report on, or relate to the Company's engagement in transactions that the Company characterized as compliant with FAS 140 (or its predecessor FAS 125)". Although some documents will mention FAS 140 or FAS 125, one would expect that most responsive documents will not mention these words at all. As a result, to find these "hard" documents that talk about either FAS 140 (or FAS 125) compliance or in-compliance but do not mention any of these words such as "FAS", "140", "125", or "financial accounting statement", we need to understand the concepts conveyed in FAS 140 or FAS 125 and their relationships, in order to find these "hard" documents.</p><p>To deal with such difficulties, two auxiliary search methods are used. One is collocation (for both topic 201 and 202) and the other is concept search (solely for topic 202).</p><p>Based on collocation, we observed that even when we do not have a profound understanding of concepts to be searched, we can take advantage of what is collocated with known entities of given concepts to find new entities. In essence, what we used is based on bootstrapping information extraction that was proposed in the "KnowItAll" system <ref type="bibr" coords="4,134.77,95.30,9.80,8.18" target="#b3">[3]</ref>. In contrast to the proposal, in our work,we only performed one iteration by searching for certain patterns. For example, based on known "Hawaii 125" deals, we found other deals that might be highly relevant to topic 202 by searching for noun phrases close to "Hawaii 125 series", including "McGarret C, D, G, F(Riva)(New) H(Braveheart) (New) and Danno B(Alchemy)". For another instance, based on known financing vehicles, such as "Osprey", we found other similar vehicles by searching for the following patterns, "financing vehicles include NP (, NPs)" (or "such as NP (, NPs)") appears within a small context window of "Osprey".</p><p>Concept search is leverages the fact that semantically correlated terms often occur together. The application of such correlation is first adopted in Latent Semantic Analysis (LSA or LSI standing for Latent Semantic Indexing) <ref type="bibr" coords="4,170.66,249.23,9.79,8.10" target="#b2">[2]</ref>. To capture the concepts and their relationships conveyed in FAS 140 or 125, we also used FAS 140 summary and each individual paragraph (including paragraphs 2, 5, 6, 8, 9c, 17d, 17f, 27, 35 and 36, as mentioned by the topic 202 TA) in a concept search to find documents that might mention the compliance or in-compliance with that specific requirement in the summary and each paragraph in the statement. After giving samples of these concept search results to the TA and collecting his judgments, we only used the top relevant results found by using paragraphs 5, 6, 8, 9c, 27 and 36 in the end.</p><p>We also performed blind relevance feedback (BRF). By returning the top salient terms in the top relevant documents, we did find some clues which may lead to finding more responsive documents; for example, one such term found in this way is CSFB for topic 201, which refers to "Credit Suisse First Boston", involved in some circular prepays with Volteron. But overall, these terms were already known to us through other methods and the number of these useful terms found is small. So, we did not use BRF as a means to find more responsive documents.</p><p>In the last iteration, by summarizing the effective searches from previous iterations, we ended up with a long list of Boolean queries for each topic (for topic 202, in addition to Boolean queries, we also have concept searches).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Management: Sampling, Reviewing and Production</head><p>As mentioned in the Introduction section, an interactive task naturally progresses into an iterative process, which demands management of search and search results from different searchers and in different stages, allowing for sampling, collaborated review and production.</p><p>With this in mind, we created a case in ESA for the collection in the native formats. The case was carefully named with a description to indicate what it is about and the location of its data source. In the case, exactly one project was created for each topic for the duration of each iteration. Within a project, searchers were allowed to create tags to label different search methods. After a search was done, the documents found were labeled with the tag of the corresponding search used.</p><p>To review the effectiveness of each search method, the following process was followed to allow reviewers to review a sample set of results: first, an administrator created a list of users for search result reviewing and assessment, and for each user, a project was also created; next, the administrator created a randomly sampled set from search results to be assessed and added it to each reviewer"s project; the administrator also created a global multiple-valued tag to allow each reviewer to tag a document under review as "Responsive", "Not Responsive" or "Not Sure". In this way, by collecting different values of the global tag, we were able to know how many results were assessed as responsive, not responsive or not sure.</p><p>As discussed earlier, messages in the collection were processed to recover discussion threads. ESA provides a convenient visualization of messages in a discussion thread for reviewing. For example, Figure <ref type="figure" coords="4,379.75,205.07,4.50,8.10" target="#fig_3">4</ref> (a) illustrates the first 10 out of 15 discussions detected from all messages with "mahonia" in their subject lines. Figure <ref type="figure" coords="4,345.55,226.19,4.50,8.10" target="#fig_3">4</ref> (b) shows an expanded structure of four participating messages by selecting the discussion "Mahonia Series X Bond". In the figure, the participating messages are labeled by their senders and are shown in the left panel. The content of the currently selected message is shown in the right panel, which is further divided into two panes: the lower one displays the forwarded text and the upper displays the new text. Note that the search term "mahonia" is also highlighted.</p><p>Having documents tagged with different names, ESA allows a user to combine them in various ways to obtain a desired merged set, e.g. including documents labeled with one specific tag or subtracting documents labeled with another one from the final set. By managing search and search results in this way, good searches and positive results can be carried over from a previous iteration to a latter one while allowing updates. In the last iteration, similarly, a batch of searches was performed, documents found were tagged and a final set was generated by merging results of different searches. Lastly, for production, we used ESA to export a list of the final set which shows the original document ID associated with each result in the collection by including those messages in the same discussion threads as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Automatic Sampling</head><p>We also implemented an automatic sampling evaluation system that enabled us to evaluate convergence of the search process. Given a set of searches that were validated by human review, our retrieval process sampled the non-retrieved collection, and evaluated each individual document"s similarity to other documents in the retrieved set. This similarity measurement was based on Noun Phrases extracted from the text of emails and attachments. Previous studies <ref type="bibr" coords="5,179.30,275.42,15.28,8.10" target="#b11">[11,</ref><ref type="bibr" coords="5,194.57,275.42,11.46,8.10" target="#b12">12,</ref><ref type="bibr" coords="5,206.03,275.42,11.46,8.10" target="#b13">13]</ref> indicate that the information content from noun phrases is sufficient to describe the features of a document. We compute a scored feature vector for each document, based on its frequency of occurrence in various regions of text of emails. Also, we selected the top 20 noun phrases, as the information gain from the lower ranked noun phrases is small. This feature vector from the sample document is verified for similarity using the cosine distance formula, to identify whether the document from the sample is close to any of the documents of the retrieved set. The number of documents that are part of the non-retrieved set that is greater than a threshold cutoff in similarity represents missed documents that would reduce the recall rate. Given the overall goal of achieving a high recall, we then analyzed the documents with high similarity for additional noun phrases that must be used to for the next iteration of the search. This constitutes a single iteration of search relevance feedback.</p><p>To evaluate convergence of subsequent iterations, we measured the number of documents that were in the missed pool. We expect convergence if the information gain in the new feedback loop is less than previous iterations, and if the additional documents identified are below a certain threshold document count.</p><p>We represent each document by its feature vector, V=</p><formula xml:id="formula_0" coords="5,267.83,517.20,23.07,12.70">N i v , 0 </formula><p>where each i v represents a noun phrase in a score-ordered list of noun phrases extracted from that document. Each sampled document has a feature vector, v S which is then measured for similarity against a featured vector that represents the entire retrieved set. The retrieved set feature vector similarity was measured using two different methods: i) merged feature vector evaluation and ii) document-by-document feature vector similarity evaluation.</p><p>The merged feature vector comparison first combines the top score-ordered documents from the retrieved set and merges their feature vectors. For this study, we placed a k cutoff value of 2000 retrieved documents, whose feature vectors are then merged per the following formula. Each feature vector entry, i v is represented as a tuple i i s t , , where i t is the raw term frequency and i s is the score for the term. The merging of feature vectors into a combined feature vector retains the term frequency of all the vectors and normalizes the score by the total number of terms in the feature vector.</p><p>The similarity of the sampled document and the merged feature vector is based on the cosine measurement, computed using the following.</p><formula xml:id="formula_1" coords="5,319.97,203.29,178.62,60.44">           M i i i N i i i N i i i C C V V C V Similarity 0 0 0</formula><p>This assumes that the document"s vector i V has [0, N] noun phrases and the merged feature vector has [0, M] words each with frequency i c . For the specific noun phrase i t , the corresponding word"s frequency in the cluster feature vector is k c . If the document feature word does not appear in the cluster feature vector, this word contributes zero to the dot product.</p><p>For document-by-document feature vector evaluation, we compute each pair-wise similarity and note the number of pairs where the similarity exceeds a certain threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5. Automatic Sampling Iterations</head><p>Sampling evaluations were designed to constrain the error and confidence levels to standard practice. In our evaluations, we used a confidence measure of 95%, so that the sampling error is within % 5  of the estimated value. This resulted in us evaluating 1537 sample documents for a coverage of one sigma around the mean of the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">TA communication</head><p>In real world litigation, a leading attorney defines the scope of document production. In the interactive task, for each topic, a Topic Authority (TA) plays the same role.</p><p>Clearly, the scope of the information needed for each topic is beyond the surface meaning of the topic description. As a result, to draw a clear picture of what is requested to produce, all three TAs gave an orientation call at the beginning of the task execution and we participated in all of them. These initial orientations proved to be very helpful to correct our initial understanding of topic requests. For example, by description, topic 205 seemingly asks for all documents that talk about volume(s) or geographic location(s) of energy loads. As a result, before we attended the orientation, we tended to produce any document in which some energy products of Volteron Corp. (the imaginary defendant) are discussed in volume, e.g. 10 MMBTU natural gas or 10 MWh electricity 2 . However, this is incorrect. In the orientation, the TA strengthens that it is the overlap of paragraph 21e of the mock complaint with the the 2 Natural gas and electricity are energy products of Volteron. description of the topic that defines the scope of production. From paragraph 21e and other paragraphs in the mock complaint, we know that Volteron participated in manipulation of the U.S. energy market in which false volume(s) and geographic location(s) are involved to illegally raise prices of their energy products. With this new restriction, for example, it immediately put the production under attack, since the production contains a document that simply summarizes the volume of natural gas provided for a European country in a specific year. As a result, this reminds us (as a searcher) that we should not isolate a topic request from the complaint whenever we determine whether a document is responsive or not. In addition to putting a restriction on documents speaking of volumes, the orientation also revealed that we needed to produce those messages and attachments that relate to Volteron"s manipulation in various ways, directly or indirectly. One such example is that Volteron sells a certain volume of electricity to another state and then immediately buys it back to give a false impression of a lack of energy in the home state and to ask for a congestion fee from the state"s regulatory organization.</p><p>We communicated with TAs of topic 201 and 202 mostly by emails and with TA of topic 205 mostly by phone. In the first contact, we exchanged thoughts for both logistics and some general production questions, such as whether a follow-up message should be produced given that the message it follows is responsive. Then, throughout the execution of a task, we gave each TA our findings with our rationale and asked for his judgments. Based on his feedback, we made adjustment to our understandings and search methods.</p><p>In the end, we ended up with spending 4 hours, 5 hours and 8 hours in consulting TAs of topic 201, 202 and 205 respectively.</p><p>Although the interaction with three TAs is helpful, we found some confusion regarding the overlap of the role of the interactive task teams and that of a TA. Here is why we have such confusion. For example, in the topic 205 task, participating teams are asked to find all "circular prepay transactions" in the collection. Our assumption is that the TA will help all teams to understand what a "circular prepay transaction" means and then it is each team"s task to find any such prepays appearing in the collection. However, right before the orientation call, the TA distributed a 192-page document that lists many details of 11 circular prepay transactions. In our opinion, we believe these 11 prepays, together with other circular prepays that might appear in the collection, are what participating teams should look for. So, any information related to such prepays should remain un-disclosed until the submission is closed and will then be used as facts to judge the final results of teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We submitted 16112, 20937 and 292540 results for topic 201, 202 and 205 respectively, including both messages and attachments, at document-level.</p><p>During generation of the final results, we found the following issue. In the collection, there is a list which shows the identifiers of unique messages, which are called master copies. Another list shows messages that are duplicates of each master copy. Ruled by TREC, only the master copy messages and their attachments will be used in the final evaluation of interactive tasks. In our work, we used all the messages and thus their attachments in the collection for all three topics and found responsive messages and their attachments listed as duplicates as well. However, many such duplicates are actually different from their master copies (or their attachments) in content (probably due to some de-dup errors during the time the duplicate list was prepared). As a result, they should be produced to reflect such differences. But unfortunately, they will be excluded from evaluation because they are deemed as duplicates by TREC.</p><p>Because of this, we made the following adjustment for these duplicate messages and attachments whose master copies were not produced in the final result for a topic: 1) take the identifier of each such message, replace it with the identifier of the master copy; and 2) take the identifier of the message part of such an attachment, replace it with the identifier of the master copy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Measurements</head><p>The following shows the evaluation results of our submission. An important goal that we established was to give preference to Recall at the expense of Precision. Our goal was to identify the highest number of responsive documents using an iterative procedure. In that context, we completed 18 iterations of various searches. Each iteration was driven by the following items for refinement. e) Additional messages that enclosed the same or similar attachments.</p><p>f) Automatic sampling process to identify sampled messages that matched a significant number of documents. A threshold of 80% similarity was required for a document to be similar. g) We also had an internal team of assessors evaluate a small sample of documents from the un-retrieved collection. Given that human review was expensive, we limited this to 50 documents per reviewer, with a total of 400 documents reviewed.</p><p>h) Small number of samples reviewed by Topic Authority.</p><p>The following progression of iterations illustrates the information gain for each iteration of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6: Retrieval Effectiveness</head><p>Note that some number of documents is duplicated during subsequent searches, and this causes additional new documents to be smaller resulting in less new gain in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sampling Distribution</head><p>An important consideration in determining a conclusion was whether there would likely be any new document gain from additional searches. While it is possible to manually review a sample of documents, we wanted to approach this using an automated sampling process, given the large number of iterations. An observation on manual review has been that each new iteration carries with it a new review cost, which is often substantial. One major innovation in our research is to identify the stopping point when we expect no new improvement in retrieval effectiveness for the cost.</p><p>To achieve this we sampled at 1537 samples (95% confidence for % The above sample distribution illustrates the number of documents from the sample of un-retrieved documents that had a similarity to the merged feature vector of the top 2000 retrieved results. As can be seen, we see a general drop in sample match count at higher levels of similarity. Also, it was observed that at lower levels of similarity, commonly occurring terms such as "load", "gas", and "enron" tended to contribute similarity. On the higher similarity buckets, we found certain highly relevant terms that could be used for new searches. As an example, the term "Yosemite" was found in samples that matched against results from the term "Mahonia".</p><p>In addition to the distribution of samples, we measured individual matches between samples from the un-retrieved set against the retrieved documents. This is a measure of individual documentby-document matching of sample documents against retrieved documents. As can be seen, we found very few sample documents from the un-retrieved collection that matched documents in the retrieved collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion of Results</head><p>The TREC 2009 Legal Track exercise revealed certain process related items that appeared to impact the results. The Legal Track process involves an initial assessments of samples, where stratified samples are drawn from a population that had no team submissions, population that had a single team and multiple team submissions. These initial assessments are distributed to the participating teams for appeals of the assessments, which are then adjudicated by the Topic Authority. The appeal and adjudication phase has the potential to change the results from initial assessments in fairly substantial ways, when there are reversals on the initial appeals. One of the considerations is the rate of reversal of the initial appeals and its impact on adjudicated results. With a very large reversal rate, one consideration is its impact on assessments when not all the initial assessments were appealed.</p><p>As an example, for those teams that appealed based on random selection of a subset of the samples, projecting the reversal into the unappealed population would in fact produce a more accurate reflection of that team"s ability to retrieve relevant documents. Thus, the Legal Track Interactive Task changed from a pure information request into a review of first pass assessments exercise. Teams that invested review resources during this phase of the project benefited the greatest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of Appeal and Adjudication Phase on Clearwell Results</head><p>As a participating team, Clearwell chose to appeal only 2% of the sample assessments, and therefore did not fully benefit from the appeals and adjudication phase. The impact of low appeal rate is shown in the change below, from an initial assessment of F1 estimate, which while improving from (0.088, 0.272 and 0.455) to (0.299, 0.619 and 0.434), the improvement was not as much as for other teams that chose to appeal the assessment in larger numbers. As an example, the remaining 558 appeals in Topic 202 changed the Team X results from 0.251 to 0.764, illustrating the impact of a more thorough review for generating appeals. The final adjudicated results compared with other participating teams is shown below. As noted, while all teams improved their estimated F measures, some teams benefited more than others, based on the extent of appeals. One potential way to manage this discrepancy is to limit the number of appeals each team can submit. Another way to extend the results from the appeal subset to the entire population. Assuming random selection of appeals, and the sample population reflecting a homogeneous mix, the following changes are anticipated. However, the overall impact of successful appeal of unappealed samples is impossible to predict. It is possible that a successful appeal from the rest of the unappealed population would reverse the R assessment of another team, causing their precision component to drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Topic 205 Anomalies</head><p>Clearwell noted that all 27 of their appeals to reverse Non-Responsive documents to Responsive were rejected. Upon reviewing the guidelines and the appeals, it is unclear why these appeals were rejected. Especially unclear is why identical document content, but near-duplicate of the appealed documents were assessed in one case as relevant, and in another case as not relevant.</p><p>Additionally, given the Topic 205 was a broad information request, and we were given guidance to consider the information request as broadly as possible, it came as a surprise to see the initial submission and appeals reverse this directive. This seems to indicate some high-level misunderstanding of the information request either by the Participating Team or the Topic Authority. Not having the ability to clarify this, especially in the context of very large reversals has the potential to leave the final assessments in question.</p><p>Another complication with Topic 205 was that this topic was about energy loads and manipulation of energy loads, but several thousand documents had a "Load Two" extraneous text added to email subject line. This text was to tag emails as a loading batch, but that addition complicated the search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we reported the execution of three interactive tasks and results. We especially demonstrated how the Clearwell E-Discovery System can be fully explored to benefit e-discovery applications such as a search in civil litigation as simulated by an interactive task in Legal Track, from providing powerful and state-of-the-art search methods to managing the iterative search, review and production process executed and collaborated among human searchers. An important contribution is the incorporation of automatic sampling and using sampling as a way to supplement human review for determining the effectiveness of each iteration of search. Another technique we will explore is reference resolution. As we know, in messages, authors often use acronyms, code words (sometimes even coined words that cannot be found in any extant dictionary), or pronouns to refer to what was discussed in previous messages. If a message does not contain any explicit occurrence of relevance but does contain such references which point to relevant information elsewhere, we should produce the message as responsive. In addition, noticing this year"s interactive task will generate a collection of responsive documents for each topic, we also plan to investigate how supervised learning can be used to produce more accurate results by using this collection as training data. Lastly, it is easy to see that all emails form a social network naturally by connections (i.e. a person is a node and an edge connects the sender node to the recipient node(s)). Further, different overlay networks can be constructed by subject lines, by topics, by locations, or by time range, etc. In such social networks, each edge can be assigned a weight bearing certain semantics; for example, the similarity value by the topics of messages exchanged between peer nodes. One would expect that responsive messages might exhibit certain patterns in these social networks at a specific time point or over time. So, we also plan to do social network analysis to complement other methods in the interactive tasks in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,93.26,201.59,161.57,8.10;2,72.00,72.00,225.35,121.10"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Clearwell E-Discovery Platform.</figDesc><graphic coords="2,72.00,72.00,225.35,121.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,86.06,219.71,176.23,8.10;3,54.10,99.10,239.80,112.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Interactive Task Execution Diagram</figDesc><graphic coords="3,54.10,99.10,239.80,112.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,337.27,456.34,201.31,8.04;3,317.90,333.00,240.00,114.70"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Search Preview of 'yosemite' and 'prepay'.</figDesc><graphic coords="3,317.90,333.00,240.00,114.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,99.50,89.03,148.99,8.10"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Discussion Visualization Tool</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,72.02,443.92,221.89,8.10;7,90.02,454.48,203.97,8.10;7,90.02,465.04,173.85,8.10;7,72.02,479.56,135.01,8.10;7,72.02,494.08,222.13,8.10;7,90.02,504.64,98.83,8.10;7,72.02,519.16,222.03,8.10;7,90.02,529.72,115.80,8.10"><head>a)</head><label></label><figDesc>Additional search terms suggested by the ESA Search Preview, which is based on surface features of keywords such as stemming, wildcards, fuzzy matches etc. b) Additional Conceptual Matches. c) Related Terms, as defined by other terms that co-occur with previous search terms. d) Additional messages from Discussion Threads and email conversation connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,91.82,193.67,164.41,8.10;9,54.00,72.00,240.00,115.20"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Clearwell Initial Assessment (F1)</figDesc><graphic coords="9,54.00,72.00,240.00,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,87.86,433.00,172.45,8.10;9,54.00,280.90,240.10,145.60"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Clearwell Adjudicated Results (F1)</figDesc><graphic coords="9,54.00,280.90,240.10,145.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,59.40,296.32,191.46,95.21"><head>Table 1 : Adjudicated Results</head><label>1</label><figDesc></figDesc><table coords="7,59.40,296.32,191.46,69.48"><row><cell></cell><cell>Submitted</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Topic</cell><cell>Count</cell><cell>F1</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>201</cell><cell cols="2">16112 0.299</cell><cell>0.489</cell><cell>0.215</cell></row><row><cell>202</cell><cell cols="2">20937 0.619</cell><cell>0.579</cell><cell>0.664</cell></row><row><cell>205</cell><cell cols="2">292540 0.410</cell><cell>0.673</cell><cell>0.321</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,317.93,495.82,256.31,209.66"><head>Table 2 : Topic 201 Sample Distribution</head><label>2</label><figDesc></figDesc><table coords="7,317.93,495.82,256.31,209.66"><row><cell></cell><cell>5</cell><cell cols="7">of error estimate) and identified whether new samples</cell></row><row><cell cols="8">with high similarity added any new interesting search terms.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Results</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell cols="2">0.5 0.6</cell></row><row><cell cols="2">Q1</cell><cell>535</cell><cell>974</cell><cell cols="2">382 142</cell><cell>35</cell><cell>4</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Q2</cell><cell cols="2">661 1042</cell><cell>397</cell><cell>62</cell><cell>36</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Q3</cell><cell cols="2">255 1144</cell><cell>304</cell><cell>25</cell><cell>58</cell><cell>5</cell><cell>1</cell><cell>0</cell></row><row><cell cols="2">Q4</cell><cell cols="2">71 1292</cell><cell>195</cell><cell>34</cell><cell>10</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell cols="2">Q5</cell><cell>606</cell><cell>939</cell><cell cols="2">326 211</cell><cell>60</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Q6</cell><cell>24909</cell><cell>935</cell><cell cols="2">410 123</cell><cell>42</cell><cell>26</cell><cell>1</cell><cell>0</cell></row><row><cell cols="2">Q7</cell><cell>1685</cell><cell>926</cell><cell>517</cell><cell>77</cell><cell>11</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell cols="2">Q8</cell><cell>2399</cell><cell>882</cell><cell cols="2">461 169</cell><cell>18</cell><cell>2</cell><cell>5</cell><cell>0</cell></row><row><cell cols="2">Q9</cell><cell cols="2">628 1000</cell><cell>477</cell><cell>46</cell><cell>10</cell><cell>4</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Q10</cell><cell cols="2">26 1311</cell><cell>160</cell><cell>55</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,61.80,443.52,224.59,138.71"><head>Table 3 : Topic 201 Sample Misses</head><label>3</label><figDesc></figDesc><table coords="8,61.80,443.52,224.59,113.01"><row><cell></cell><cell></cell><cell></cell><cell>Matching</cell><cell>Miss</cell></row><row><cell></cell><cell cols="2">Results Misses</cell><cell>Misses</cell><cell>Estimate</cell></row><row><cell>Q1</cell><cell>535</cell><cell>2</cell><cell>17</cell><cell>2750</cell></row><row><cell>Q6</cell><cell>24909</cell><cell>6</cell><cell>111</cell><cell>8251</cell></row><row><cell>Q10</cell><cell>26</cell><cell>2</cell><cell>2</cell><cell>2750</cell></row><row><cell>Q11</cell><cell>1032</cell><cell>1</cell><cell>1</cell><cell>1375</cell></row><row><cell>Q12</cell><cell>1907</cell><cell>1</cell><cell>8</cell><cell>1375</cell></row><row><cell>Q13</cell><cell>42524</cell><cell>6</cell><cell>14</cell><cell>8251</cell></row><row><cell>Q17</cell><cell>8488</cell><cell>1</cell><cell>2</cell><cell>1375</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,317.93,109.91,239.93,128.58"><head>Table 4 : Topic Appeal Effectiveness</head><label>4</label><figDesc>Following are the rates of reversals, as indicated by the TREC 2009 Coordinators.</figDesc><table coords="8,320.21,152.59,235.63,60.21"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Reversal</cell></row><row><cell>Topic</cell><cell cols="3">Samples Appeals Success</cell><cell>%</cell></row><row><cell>201</cell><cell>6956</cell><cell>497</cell><cell>464</cell><cell>93.4</cell></row><row><cell>202</cell><cell>7435</cell><cell>708</cell><cell>584</cell><cell>82.5</cell></row><row><cell>205</cell><cell>6367</cell><cell>967</cell><cell>932</cell><cell>96.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,320.57,449.64,234.78,88.67"><head>Table 5 : Clearwell Appeal Rates</head><label>5</label><figDesc></figDesc><table coords="8,320.57,449.64,234.78,62.97"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Appeals</cell></row><row><cell></cell><cell>Appeals NNN</cell><cell>R</cell><cell></cell><cell>Rate %</cell></row><row><cell>201</cell><cell>139</cell><cell>100</cell><cell>39</cell><cell>1.99</cell></row><row><cell>202</cell><cell>150</cell><cell>110</cell><cell>40</cell><cell>2.01</cell></row><row><cell>205</cell><cell>140</cell><cell>113</cell><cell>27</cell><cell>2.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,63.72,543.96,220.62,88.70"><head>Table 6 : F Estimate Adjustent</head><label>6</label><figDesc></figDesc><table coords="9,63.72,543.96,220.62,62.97"><row><cell></cell><cell>Original F Estimate</cell><cell>Adjustment Ratio</cell><cell>Adjusted F Estimate</cell></row><row><cell>201</cell><cell>0.299</cell><cell>1.387</cell><cell>0.414</cell></row><row><cell>202</cell><cell>0.619</cell><cell>1.268</cell><cell>0.785</cell></row><row><cell>205</cell><cell>0.434</cell><cell>1.169</cell><cell>0.507</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,63.12,697.27,230.81,8.10"><p>Basis Technologies, Rosette Base Linguistics for English,</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,58.50,75.32,92.29,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.02,88.43,205.80,8.10;10,72.02,98.99,203.50,8.10;10,72.02,109.55,211.26,8.10;10,72.02,120.11,213.89,8.10;10,72.02,130.67,215.52,8.10;10,72.02,141.23,58.41,8.10" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Braman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">J</forename><surname>Withers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">Y</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">L</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,94.82,109.55,188.46,8.10;10,72.02,120.11,213.89,8.10;10,72.02,130.67,43.53,8.10">The Sedona Conference Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery</title>
		<imprint>
			<date type="published" when="2007-08">2007. Aug. 2007</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="189" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.02,155.75,221.13,8.10;10,72.02,166.31,207.01,8.10;10,72.02,176.87,218.13,8.10;10,72.02,187.43,72.33,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,139.34,166.31,135.98,8.10">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,72.02,176.87,202.95,8.10">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.02,201.95,204.11,8.10;10,72.02,212.51,211.93,8.10;10,72.02,223.07,200.45,8.10;10,72.02,233.63,219.20,8.10;10,72.02,244.19,202.37,8.10;10,72.02,254.78,120.21,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,234.65,223.07,37.82,8.10;10,72.02,233.63,206.18,8.10">Web-scale information extraction in KnowItAll (preliminary results)</title>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.02,244.19,202.37,8.10;10,72.02,254.78,63.85,8.10">Proceedings of the 13th International Conference on the World Wide Web</title>
		<meeting>the 13th International Conference on the World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.02,269.30,203.76,8.10;10,72.02,279.86,219.83,8.10;10,72.02,290.42,221.62,8.10;10,72.02,300.98,99.09,8.10" xml:id="b4">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Brassil</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Esq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Rugani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Misti</forename><surname>Reinhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teresa</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.02,290.42,163.80,8.10">TREC 2008 Legal Interactive: User Modeling</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.02,315.50,198.11,8.10;10,72.02,326.06,194.59,8.10;10,72.02,336.62,77.97,8.10" xml:id="b5">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,156.96,315.50,113.18,8.10;10,72.02,326.06,194.59,8.10;10,72.02,336.62,22.55,8.10">Search of the Perfect Search: A project closes in on a protocol to improve e-discovery results</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,74.25,201.33,8.10;10,335.95,84.83,206.41,8.10;10,335.95,95.39,68.73,8.10" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tominson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<title level="m" coord="10,415.99,84.83,126.37,8.10;10,335.95,95.39,45.94,8.10">Overview of the TREC 2008 Legal Track TREC</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,109.91,209.52,8.10;10,335.95,120.47,176.89,8.10;10,335.95,131.03,185.40,8.10" xml:id="b7">
	<analytic>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi-Ling</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.95,120.47,176.89,8.10;10,335.95,131.03,133.43,8.10">Pitt@TREC08: An Initial Study of Collaborative Information Behavior in E-Discovery</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,145.55,192.92,8.10;10,335.95,156.11,135.24,8.10" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,419.71,145.55,79.05,8.10">Clustering Algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,170.63,218.72,8.10;10,335.95,181.16,196.31,7.67;10,335.95,191.24,192.06,7.67;10,335.95,201.32,101.73,7.67" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,380.31,170.63,131.34,8.10">Towards interactive query expansion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,518.02,170.96,36.65,7.67;10,335.95,181.16,196.31,7.67;10,335.95,191.24,192.06,7.67;10,335.95,201.32,28.26,7.67">SIGIR &apos;88: Proceedings of the 11th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,215.39,191.52,8.10;10,335.95,225.95,215.28,8.10;10,335.95,236.51,206.69,8.10;10,335.95,247.07,153.36,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,335.95,225.95,211.16,8.10">Efficient interactive query expansion with CompleteSearch</title>
		<author>
			<persName coords=""><forename type="first">Holger</forename><forename type="middle">;</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Debapriyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.95,236.51,206.69,8.10;10,335.95,247.07,148.89,8.10">CIKM&apos;07 : Proceedings of the 2007 ACM Conference on Information and Knowledge Management</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,261.62,198.74,8.10;10,335.95,272.18,213.61,8.10;10,335.95,282.74,221.59,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,474.19,261.62,60.50,8.10;10,335.95,272.18,108.32,8.10">Using phrases as features in email classification</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chung</forename><surname>Keung Poon</surname></persName>
		</author>
		<ptr target="www.elsevier.com/locate/jss" />
	</analytic>
	<monogr>
		<title level="j" coord="10,449.86,272.18,99.69,8.10;10,335.95,282.74,30.68,8.10">The Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1036" to="1045" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,297.26,201.70,8.10;10,335.95,307.82,217.57,8.10;10,335.95,318.38,198.35,8.10;10,335.95,328.94,71.08,8.10" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,508.53,297.26,29.11,8.10;10,335.95,307.82,214.11,8.10">A Query Expansion Algorithm based on Phrases Semantic Similarity</title>
		<author>
			<persName coords=""><forename type="first">Yongli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<pubPlace>Beijing</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science and Technology, Beihang University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.95,343.46,221.88,8.10;10,335.95,354.02,195.39,8.10" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De Loupy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Beze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-F</forename><surname>Marteau</surname></persName>
		</author>
		<title level="m" coord="10,535.34,343.46,22.49,8.10;10,335.95,354.02,195.39,8.10">Query Expansion and Classification of Retrieved Documents</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
