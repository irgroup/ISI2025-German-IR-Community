<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.43,164.90,272.40,16.56;1,169.96,186.82,271.33,16.56">BRAT: A Random Walk through the Semantic Spaces of the Blogosphere</title>
				<funder>
					<orgName type="full">Charles Tijus</orgName>
				</funder>
				<funder ref="#_ymbmAzr">
					<orgName type="full">Thalès</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.27,225.94,66.99,11.50"><forename type="first">Adil</forename><forename type="middle">El</forename><surname>Ghali</surname></persName>
							<email>elghali@lutin-userlab.fr</email>
							<affiliation key="aff0">
								<orgName type="department">CHArt -Lutin</orgName>
								<orgName type="institution">Université</orgName>
								<address>
									<addrLine>Paris 8 * 2 rue</addrLine>
									<postCode>Liberté -93200</postCode>
									<settlement>Saint Denis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.58,225.94,60.54,11.50"><forename type="first">Yann</forename><surname>Vigile</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CHArt -Lutin</orgName>
								<orgName type="institution">Université</orgName>
								<address>
									<addrLine>Paris 8 * 2 rue</addrLine>
									<postCode>Liberté -93200</postCode>
									<settlement>Saint Denis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.11,225.94,45.87,11.50;1,266.57,239.50,29.12,11.06"><forename type="first">Hoareau</forename><surname>Team</surname></persName>
							<email>hoareau@lutin-userlab.fr</email>
							<affiliation key="aff0">
								<orgName type="department">CHArt -Lutin</orgName>
								<orgName type="institution">Université</orgName>
								<address>
									<addrLine>Paris 8 * 2 rue</addrLine>
									<postCode>Liberté -93200</postCode>
									<settlement>Saint Denis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.43,164.90,272.40,16.56;1,169.96,186.82,271.33,16.56">BRAT: A Random Walk through the Semantic Spaces of the Blogosphere</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ADB8E4D23AB2A46471CC62643625E761</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic spaces, such as the Latent Semantic Analysis (LSA), Hyperspace Analog to Language (HAL) or Random Indexing (RI), offer convenient methods to represent semantic relations between words and concepts, abstracted from a distribution of documents. The distribution of documents determines the local co-occurrence pattern between words all over the corpus and, then, determines the semantic abstracted from the local distribution. Such methods are sensitive to the statistical properties on the distribution of words over documents. For instance, the semantic on the word table abstracted from a scientific corpus or a general corpus may be different. In the first case, since table may occur in the context of table of correlation or table of results, it would be considered to be associated to the word correlation whereas in the second case, because it may co-occur with kitchen or living-room, it would rather be considered as similar to chair. Nevertheless, the formal relation bearing the properties of the distribution of word's co-occurence and the final semantic produced by Semantic space methods have not been described until now. In the case of a mixed "scientific and general" corpus, what makes that the semantic of table became more similar to chair than Speerman and vice-versa?</p><p>We approached the Top-stories task of the Blog-Track'09 using a system named Blogosphere Random Analysis using Texts (BRAT) composed of two layers. The first layer distributes and represents blogs posts' in different semantic spaces built using Random Indexing. The second layer is an algorithm of retrieval that have the aim of navigate in the semantic space via a ramdom walk. BRAT have been constructed under two main working hypothesis that we considered important for dealing with the semantic of the blogosphere: the notion of semantic identity and the notion of semantic pollution.</p><p>The article is organized as follows. In a first part, we shortly overview the methods and properties of semantic spaces models. The notions of semantic identity and semantic pollution are described in general together with their practical implication within the Top-stories task. In the second part, the BRAT system is described. The third part gives an overview of the performances of BRAT for the Top-stories task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The cognition of Blog Mining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Spaces</head><p>Word Vectors are a family of models that represent semantic similarity between words in function of the textual environment in which those words appear. The words co-occurence distribution is collected, analyzed and transformed into a semantic space, in which words or concepts are represented as vectors in a high-dimension vector space. LSA <ref type="bibr" coords="2,344.33,325.49,128.86,9.58" target="#b2">[Landauer and Dumais, 1997]</ref>, Hyper Analog to Language <ref type="bibr" coords="2,261.03,337.44,111.98,9.58" target="#b3">[Lund and Burgess, 1996]</ref> and Random Indexing <ref type="bibr" coords="2,133.77,349.40,92.24,9.58" target="#b2">[Kanerva et al., 2000]</ref> are some exemplars of Word Vectors. Those models are based on the Harris <ref type="bibr" coords="2,223.88,361.35,59.81,9.58" target="#b1">[Harris, 1968]</ref> distributional hypothesis, which states that words that appear in similar context have similar meanings. The definition of the unit of context is a common issue to all of those models, even if it is of different nature depending of the models. For example, LSA build a worddocument matrix, in which each cell a ij holds the frequency of a specific word i in a specific unit of context j. HAL defines a floating window of n words that scrolls each word of the corpus. Then build a word-word matrix, in which each cell a ij contains the frequency a word i co-occurs with a word j for the considerate floating window. Different mathematical/statistical methods to abstract the meaning of concepts are applied on the distribution of frequencies stored in the word-document or word-word matrix. The first purpose of those mathematical processing is to abstract the central tendency of frequencies variations and to eliminating what can be considerate like "noise" caused by the part of specific use of language associated to each person or author. LSA uses a general method of linear decomposition of a matrix into principal independent components, which is called the Singular Value Decomposition (SVD). HAL reduces the expense of computational complexity by retaining a small number of principal components of the co-occurence matrix. Vectorial representations are used for the storage and the manipulation of concepts meaning. At the end of the process, similarity between two words may be calculated using different methods. A classical method is to calculate the value of the cosine of the angle between two vectors corresponding to a words or a group of words to approximate their semantic similarity. Another equivalent method is the pondered Euclidian distance.</p><p>In sum, Word Vectors inputs are a distribution of textual episodes defined as unit of context. The distribution of words co-occurence is matched with the distribution of textual episodes in which they appear. Word Vectors outputs are concepts that emerged from this distribution's matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic identity and pollution</head><p>As we started to introduce above, within the frame of semantic space methods, the semantic produced for a given word depends of the distribution of the other words that co-occur with it. It makes that no semantic of any words is given ex nihilo, ie pre-existing without (i) a learning process realized on (ii) a distribution of contexts or episodes (ie, unit of experience). The final semantic associated to a word have an identity that have been forged along the process of learning that is realized by SVD for LSA or the accumulation for RI. The semantic identity for a given word such as table changes in function of the corpus in appears within.</p><p>The notion of semantic identity addresses not only the scale of words but also the scale of the semantic space it-self. A semantic space have a particular identity that is given by the distribution of word's co-occurence that the space is composed by. The notion of semantic identity is circular because it reflects the circularity of the distributional hypothesis, which semantic spaces are based upon.</p><p>The notion of semantic identity does not produce something very new for researchers familiar with semantic spaces and the notion may appear somehow trivial if it did not allow to highlight a second notion that we will call the semantic pollution. In the previous example of a mixed "scientific and general" semantic space, the semantic identity of table is as much forged by the semantic related to science as by the semantic related to everyday life. In a general semantic space, if a word is similar to table, one can make the reasonable assumption that this word is not so far similar to kitchen or house. In a mixed "scientific and general" space, such an assumption became not so much reasonable, because the semantic of table have been some kind of polluted by the scientific part of the corpus. One can argue that this semantic pollution is nothing more than polysemy. It is true for the case of the word table because it is a polysemous word, but the pollution of the identity of the word table have and effect of pollution of the identity of words that it have co-occur together such as correlation, Speerman, kitchen, house, etc. Those words are not polysemous words but their semantic identity would be polluted too. Because of table, words such as correlation may possibly be not so far from living-room in term of semantic similarity. One again the semantic pollution addresses to the scale of word but also to the scale of the space for the same reason of circularity described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Application to Blogs Mining</head><p>The notion of semantic identity and semantic pollution are the two main ideas that are underlying our approach of the analysis of the blogosphere. In our view, the blogosphere is a cognitive system that produces textual information that expresses people's views and ideas concerning views and ideas of others. For the Top-stories task of the Blog-Track of the TREC'09, the goals were (i) to detect the headlines of the New York Times that had produced exchanges in the blogosphere and (ii) for each of these headline, to propose some related blogs.</p><p>Considering the notion of semantic identity, we assume that the events of a given actuality produce some specific exchanges that are different of the exchanges produced relatively to the events of another actuality. Therefore, there is an advantage in splitting exchange in period of time in the aim of extracting the semantic identity associated with the actuality that have produce those exchanges.</p><p>Within the frame of semantic space models, the textual exchanges that are produced during a specific period of time constructs a semantic identity that is related to this particular actuality. Hence, in the first part of the process, semantic spaces are build from posts and commentaries written in a given period of time.</p><p>Nevertheless, even in choosing documents that have been produced during the same period of time, there is a large part of the selected exchanges that are not related to the headline of the New York Time. Those "not related texts" participated in the construction of the semantic identity corresponding to each semantic space, but they also pollute these semantic in the manner described above. The retrieval algorithm tries to navigate in a semantic space taking into account the degree of semantic pollution in the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BRAT</head><p>The Basic idea behind our work is that if we provide any efficient and easy way to navigate in a semantic space containing both blogs posts and headlines, then we can retrieve for each headline the relevant blogs posts by walking randomly in the semantic space. However we have to cope with the semantic pollution of the space.</p><p>The principle underlying the algorithm is to consider a representation of the "semantic identity of the day" as the sum of all document vectors corresponding to the given day. Taking into account that this representation might be strongly affected by a large amount of irrelevant documents, from the perspective of the top stories of the day. We defined a procedure that computes each document's similarity with both the "semantic identity of the day" and each of the headlines. In addition, for each headline, we rank a number of posts using a random walk through the semantic space. The procedure is stopped when satisfying a set of conditions that will be developed beyond.</p><p>Practically, for each topic and after a pre-processing phase, Random indexing <ref type="bibr" coords="4,151.89,627.32,70.32,9.58" target="#b4">[Sahlgren, 2006]</ref> was used to built a semantic space containing the blog posts, as well as the headlines, in a window around the date of the topic. This geometric representation of meanings of the episodes (posts and headlines) is then crawled using a random-walk-like algorithm to find the closest posts for each headline. The ranking of the headlines takes into account the number of steps needed to find n relevant posts for a headline, together with the density of posts around the headline, as well as the average similarity between each headline and its associated posts. For each headline, the posts are ranked with regard to their similarity with the headline. Let us describe these steps with some more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Blog08 data pre-processing</head><p>The Blog08 collection was made by crawling the blogosphere during more than year. The data were provided as-is: without any cleaning and the content of the blogs posts' where stored in a pseudo-XML format<ref type="foot" coords="5,375.03,257.56,3.49,6.71" target="#foot_0">1</ref> which is unfortunetly not very well suited to blogs data. The first not-very-interesting-butnecessary step was to split the permalinks files and organize them by posting date (instead of crawling date).</p><p>We also took the opportunity during this step to clean the posts from the parts that we consider useless such us: CSS and Javascript. but also to extract some general meta-data about posts and some structure informations of the blogosphere such us the inter-comments network.</p><p>The last step of the preparation of the data was to detect the languages of the blogs, in order to keep only english blogs. We use a language categorization library<ref type="foot" coords="5,163.47,377.11,3.49,6.71" target="#foot_1">2</ref> that implements the algorithms described in <ref type="bibr" coords="5,363.16,378.70,118.52,9.58" target="#b0">[Cavnar and Trenkle, 1994]</ref> to categorize texts using n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic space construction</head><p>The Semantic space method we use in the context of the Blog-Track'09 is Random Indexing (RI), which is not a typical method in the family of Semantic space methods. Particularities of RI are that (i) it does not create co-occurrence matrix (but it is possible if needed) and (ii) it does not need heavy statistical treatments like SVD for LSA. Contrary to the other Word Vector models, RI is based on random projection, a method that approximate statistics cooccurences, and allows to scale to huge number of documents. The construction of a semantic space with RI is as follows:</p><p>• Create a matrix A(d x N ), containing Index vectors, where d is the number of documents or contexts and N , the number of dimensions (N &gt; 1000) decided by the experimenter. Index vectors are sparse and randomly generated. They consist in small numbers +1 and -1 and thousands of 0.</p><p>• Create a matrix B(t x N ), containing term vectors, where t is the number of different terms in the corpus. Set all vectors with null values to start the semantic space construction.</p><p>• Scan each document of the corpus. Each time a term τ appears in a document δ, accumulate the randomly generated δ-index vector to the τ -term vector.</p><p>At the end of the process, term vectors that appeared in similar contexts have accumulated similar index vectors. There is a training cycle option in the model. When the scan has been computed for all documents, the matrix B is charged for all term vectors. Then a matrix A (d x N ), with d = d can be computed with the output of term vectors. The number of training cycle is a parameter in the model. The training process improves the quality of the Semantic space. The RI model has performed in TOEFL synonymy test <ref type="bibr" coords="6,133.77,254.94,151.63,9.58" target="#b2">[Kanerva et al., 2000, Karlgren and</ref><ref type="bibr" coords="6,287.89,254.94,67.00,9.58" target="#b2">Sahlgren, 2001]</ref> as well as in text categorization <ref type="bibr" coords="6,153.64,266.87,118.31,9.61" target="#b5">[Sahlgren and C öster, 2004]</ref>.</p><p>For each topic (a date D) a semantic space SS D is built relying on the Semantic Vectors<ref type="foot" coords="6,198.51,289.23,3.49,6.71" target="#foot_2">3</ref> library <ref type="bibr" coords="6,237.12,290.81,126.41,9.58" target="#b6">[Widdows and Ferraro, 2008]</ref>. The semantic space contains two kinds of episodic documents: (i) all the headlines in a window<ref type="foot" coords="6,444.20,301.18,3.49,6.71" target="#foot_3">4</ref> [D-1, D + 1], (ii) all the english posts<ref type="foot" coords="6,269.80,313.14,3.49,6.71" target="#foot_4">5</ref> in a window [D -1, D + 3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A random walk in the semantic space</head><p>Once the semantic space SS D of a day D constructed, we use a random-walklike algorithm to navigate in the space in order to retrieve for each headline n related blog posts.</p><p>We call a prototype for a category of a set of documents (blog posts or headlines), a pseudo document represented in the semantic space by the sum of all the vectors in the set. For instance, the prototype of all the headlines is a pseudo document P H represented by the vector:</p><formula xml:id="formula_0" coords="6,279.76,457.90,197.72,20.14">P H = h∈H h (1)</formula><p>where H is the set containing all the headlines of SS D . Given a headline h i ∈ SS D and η ∈ N, we call η-neighbourhood of h i w.r.t a prototype P , the set of blogs posts defined as follow:</p><formula xml:id="formula_1" coords="6,190.91,533.35,286.57,22.31">η -neighbourhood(h i , P ) = {b j |d(b j , h i ) &lt; d(P, h i ) η }<label>(2)</label></formula><p>where d(d i , d j ) is an euclidien distance in the semantic space between the vectors d i and d j .</p><p>In order to retrieve the n related blog posts for the headline h i , we choose a threshold m &gt; n, we walk randomly through the set B containing all the blog posts of SS D until founding m candidates posts in the η-neighbourhood of h i w.r.t the prototype P H of all the headlines. If we found m candidates posts, we define the score p i of the headline h i as the number of steps we walked in B. If the number of founded blog posts m &lt; m then the score p i of h i is defined as</p><formula xml:id="formula_2" coords="7,265.29,161.83,212.19,9.65">p i = card(B) -m<label>(3)</label></formula><p>In each set B i containing the founded blog posts for h i , we keep the min(n, m ) closest blog posts to h i as the related posts. And the headlines are ranked in ascending order of p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The submitted runs implement different hypothesis concerning the organisation of the knowledge in semantic space build from the blogosphere. The runs correspond to different values of η used for the random walk algoritm.</p><p>In the runs ri2049rw3 corresponds to an application of the algorithm with η = 3 in a 2049 dimensions space.</p><p>The run ri1025rw5432 corresponds to an adaptative algorithm using the same principle, and where the results of the random walk with η = 5, 4, 3, 2 are combined.</p><p>The run ri1025rw5h2b uses a similar algorithm but with little modification of the definition neighbourhood. The used neighbourhood is the intersection of the 5-neighbourhood w.r.t to P H and the 2-neighbourhood w.r.t to P B .</p><p>The run ri1025rw2b corresponds to an application of the algorithm with the 2-neighbourhood w.r. The obtained results are summerized in Table <ref type="table" coords="7,359.47,536.90,4.98,9.58" target="#tab_0">1</ref> and Figure <ref type="figure" coords="7,420.70,536.90,3.74,9.58" target="#fig_0">1</ref>. The good performance of the adaptative run (ri1025rw5432) and the even better performance of the double constraint run (ri1025rw5h2b) constitutes good arguments in favour of the validity of the notion of semantic identity and semantic pollution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The original contribution of our work is to propose a simple and efficient algorithm to navigate in a semantic space in which the semantic of blog posts is supposed to be strongly polluted because of a number of irrelevant posts. The principle underlying the algorithm is to consider a representation of the "semantic identity of the day" as the sum of all document vectors corresponding to the given day. Taking into account that this representation might be strongly affected by a large amount of irrelevant documents, from the perspective of the top stories of the day. We defined a procedure that computes each document's similarity with both the "semantic identity of the day" and each of the headlines. In addition, for each headline, we rank a number of posts using a random walk through the semantic space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,133.77,639.80,343.71,9.58;8,133.77,651.76,28.51,9.58;8,133.77,379.80,385.20,234.90"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Retrieved relevant posts and R-Precision for the 4 runs with reference values</figDesc><graphic coords="8,133.77,379.80,385.20,234.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,133.77,408.11,343.71,117.12"><head>Table 1 :</head><label>1</label><figDesc>t to P B . Comparaison of R-Precision and Number of Relevant Retrieved Headlines with the Median values</figDesc><table coords="7,139.75,431.24,334.41,59.83"><row><cell>Run ID</cell><cell cols="2">R-Precision ≥ Median % of Retrieved Relevant Headlines</cell></row><row><cell>ri1025rw2b</cell><cell>32</cell><cell>24%</cell></row><row><cell>ri1025rw5432</cell><cell>32</cell><cell>24%</cell></row><row><cell>ri1025rw5h2b</cell><cell>34</cell><cell>24%</cell></row><row><cell>ri2049rw3</cell><cell>26</cell><cell>20%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,148.12,648.13,329.36,7.67;5,133.77,657.59,190.84,7.67"><p>The files of the Blog08 collection are not in a well formed XML format, and the preparation of the data was a very time and resource consuming task</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,148.12,667.33,113.81,7.67"><p>http://olivo.net/software/lc4j/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,148.12,647.77,161.40,7.67"><p>http://code.google.com/p/semanticvectors/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,148.12,657.34,299.39,7.92"><p>Except for the run ri2049rw3 where only the headlines of the day D were considered.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,148.12,667.33,324.34,7.67"><p>We choose to consider as an episode the document containing a blog post and its comments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work would have never been possible without the support of <rs type="funder">Charles Tijus</rs> and the <rs type="institution">Lutin Lab</rs>. We especially want to thank the Lutin members <rs type="person">Zakia Ikhlef</rs>, <rs type="person">Rebecca Djuric</rs>, <rs type="person">Daniel Hromada</rs>, <rs type="person">Olivier Floucat</rs>, and <rs type="person">Franc ¸oise Richard</rs> for their valuable support.</p><p>We are also grateful to the members of the <rs type="projectName">DOXA</rs> project and the <rs type="institution">Cap Digital Business Cluster</rs>, especially <rs type="person">Thibaut Ehrette</rs>, <rs type="person">Jacques Bibal</rs>, <rs type="person">Catherine Gouttas</rs> from <rs type="funder">Thalès</rs>, <rs type="programName">Patrick Constant and Guillaume Logerot from Pertimm</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ymbmAzr">
					<orgName type="project" subtype="full">DOXA</orgName>
					<orgName type="program" subtype="full">Patrick Constant and Guillaume Logerot from Pertimm</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.87,397.38,338.61,9.58;9,145.39,409.21,332.09,9.71;9,145.39,421.16,292.48,9.71" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,439.31,397.38,38.17,9.58;9,145.39,409.34,108.28,9.58">N-grambased text categorization</title>
		<author>
			<persName coords=""><forename type="first">Trenkle ;</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,286.10,409.21,191.38,9.50;9,145.39,421.16,220.73,9.50">Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,133.77,441.09,343.71,9.71;9,145.39,453.17,87.79,9.58" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,274.05,441.09,148.43,9.50">Mathematical Structures of Language</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harris ; Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968. 1968</date>
			<publisher>John Wiley and Son</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,133.77,473.10,343.71,9.58;9,145.39,485.05,332.09,9.58;9,145.39,496.88,332.10,9.71;9,145.39,508.83,245.19,9.71;9,133.77,528.89,343.71,9.58;9,145.39,540.85,332.09,9.58;9,145.39,552.67,287.09,9.71;9,133.77,572.73,343.71,9.58;9,145.39,584.68,332.09,9.58;9,145.39,596.51,332.09,9.71;9,145.39,608.59,66.41,9.58" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,456.73,473.10,20.75,9.58;9,145.39,485.05,259.11,9.58;9,453.95,528.89,23.53,9.58;9,145.39,540.85,108.02,9.58;9,453.06,572.73,24.42,9.58;9,145.39,584.68,332.09,9.58;9,145.39,596.64,23.14,9.58">A Solution to Plato&apos;s Problem: The Latent Semantic Analysis Theory of Acquisition</title>
		<author>
			<persName coords=""><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.20,496.88,239.28,9.50;9,145.39,508.83,59.72,9.50;9,145.39,552.67,156.91,9.50">Proceedings of the 22nd Annual Conference of the Cognitive Science Society</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Uesaka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</editor>
		<meeting>the 22nd Annual Conference of the Cognitive Science Society<address><addrLine>Mahwah; Stanford</addrLine></address></meeting>
		<imprint>
			<publisher>CSLI Publications</publisher>
			<date type="published" when="1997">2000. 2000. 2001. 2001. 1997</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="211" to="240" />
		</imprint>
	</monogr>
	<note>Random Indexing of Text Samples for Latent Semantic Analysis. Induction and Representation of Knowledge</note>
</biblStruct>

<biblStruct coords="9,133.77,628.52,343.71,9.58;9,145.39,640.34,332.09,9.71;9,145.39,652.30,208.30,9.71" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,404.62,628.52,72.86,9.58;9,145.39,640.47,245.00,9.58">Producing highdimensional semantic space from lexical co-occurence</title>
		<author>
			<persName coords=""><forename type="first">Burgess</forename><forename type="middle">;</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,405.06,640.34,72.42,9.50;9,145.39,652.30,139.96,9.50">Behavior research methods, instruments &amp; computers</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.77,127.86,343.71,9.71;10,145.39,139.82,332.09,9.50;10,145.39,151.77,332.10,9.71;10,145.39,163.86,97.38,9.58" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,307.85,127.86,169.62,9.50;10,145.39,139.82,332.09,9.50;10,145.39,151.77,140.43,9.50">The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren ; Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Linguistics Stockholm University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,133.77,183.76,343.71,9.61;10,145.39,195.74,332.09,9.58;10,145.39,207.56,332.09,9.71;10,145.39,219.52,332.09,9.71;10,145.39,231.60,120.02,9.58" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,416.97,183.78,60.51,9.58;10,145.39,195.74,332.09,9.58;10,145.39,207.69,48.05,9.58">Using bag-ofconcepts to improve the performance of support vector machines in text categorization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Öster ; Sahlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.19,207.56,264.29,9.50;10,145.39,219.52,107.42,9.50">COLING &apos;04: Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">487</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,133.77,251.53,343.71,9.58;10,145.39,263.48,332.09,9.58;10,145.39,275.31,332.10,9.71;10,145.39,287.26,179.61,9.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,437.33,251.53,40.15,9.58;10,145.39,263.48,332.09,9.58;10,145.39,275.44,76.00,9.58">Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application</title>
		<author>
			<persName coords=""><surname>Widdows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferraro ; Widdows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ferraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.97,275.31,233.51,9.50;10,145.39,287.26,174.94,9.50">Proceeding of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>eeding of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
