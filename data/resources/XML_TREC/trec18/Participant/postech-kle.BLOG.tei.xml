<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.24,116.00,308.87,12.91;1,266.63,133.93,82.10,12.91">POSTECH at TREC 2009 Blog Track: Top Stories Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.73,171.56,35.58,8.97"><forename type="first">Yeha</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Electrical and Computer Engineering Pohang</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<addrLine>San 31, Hyoja-Dong, Nam-Gu</addrLine>
									<postCode>790-784</postCode>
									<settlement>Pohang</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.17,171.56,64.91,8.97"><forename type="first">Hun-Young</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Electrical and Computer Engineering Pohang</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<addrLine>San 31, Hyoja-Dong, Nam-Gu</addrLine>
									<postCode>790-784</postCode>
									<settlement>Pohang</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.84,171.56,57.69,8.97"><forename type="first">Woosang</forename><surname>Song</surname></persName>
							<email>woosang@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Electrical and Computer Engineering Pohang</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<addrLine>San 31, Hyoja-Dong, Nam-Gu</addrLine>
									<postCode>790-784</postCode>
									<settlement>Pohang</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.50,171.56,66.11,8.97"><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
							<email>jhlee@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Electrical and Computer Engineering Pohang</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<addrLine>San 31, Hyoja-Dong, Nam-Gu</addrLine>
									<postCode>790-784</postCode>
									<settlement>Pohang</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.24,116.00,308.87,12.91;1,266.63,133.93,82.10,12.91">POSTECH at TREC 2009 Blog Track: Top Stories Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB9109D4DFD07E9D5EE89DD8A102221C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in the TREC 2009 Blog Track. Our system consists of the query likelihood component and the news headline prior component, based on the language model framework. For the query likelihood, we propose several approaches to estimate the query language model and the news headline language model. We also suggest two approaches to choose the 10 supporting relevant posts: Feed-Based Selection and Cluster-Based Selection. Furthermore, we propose two criteria to estimate the news headline prior for a given day. Experimental results show that using the prior significantly improves the performance of the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Blog track explores information seeking behavior in the blogosphere. Compared with previous Blog track, the Blog track 2009 aims to investigate more refined and complex search scenarios in the blogosphere. In TREC 2009, the Blog track has two main tasks: Faceted Blog Distillation Task and Top Stories Identification Task.</p><p>Among two tasks, we participate in the Top Stories Identification Task. The Top Stories Identification Task is a new pilot search task addressing the news dimension in the blogosphere. Query of this task is a date "query". For a given date query, a system should rank news headlines according to their importance on the given day. Furthermore, for each headline, the system should provide 10 supporting blog posts which are relevant to and discuss the news story headline. To achieve this, the system will be provided with news headline corpus and the Blogs08 corpus, and could use external resources.</p><p>For the Top Stories Identification Task, our approach consists of two steps: the preprocessing step and the news headline ranking step. In the preprocessing step, HTML tags and non-relevant contents, provided by blog providers such as site description and menus, are removed. In the news headline ranking step, we make two language models, the date query language model and the headline language model, and estimate the probability that each news headline will be the top story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preprocessing Step</head><p>TREC Blog08 collection contains permalinks, feed files and blog homepages. We only used the permalink pages for the top stories identification task. The permalinks are en-coded by HTML, and there are many different styles of permalinks. Beside the relevant textual parts, the permalinks contain many non-topical or non-relevant contents such as HTML tags, advertisements, site descriptions, and menus.</p><p>The non-relevant contents consist of many different types of blog templates which may be provided from commercial blog service venders. We used the DiffPost algorithm <ref type="bibr" coords="2,158.84,178.96,10.79,8.97" target="#b4">[5,</ref><ref type="bibr" coords="2,171.30,178.96,8.29,8.97" target="#b6">7]</ref> to deal with the non-relevant contents.</p><p>To preprocess the Blog08 corpus, we firstly discarded all HTML tags, and applied DiffPost algorithm to remove non-relevant contents. DiffPost segments each document into lines using the carriage return as a separator. DiffPost tries to compare sets of lines, and then regards the intersection of sets as the non-content information.</p><p>For example, let P i and P j be blog posts within the same blog feed. Let S i and S j be the sets of lines correspond to P i and P j , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NoisyIn f ormation(P</head><formula xml:id="formula_0" coords="2,318.28,272.69,162.31,10.57">i , P j ) = S i ∩ S j (1)</formula><p>We discarded non-relevant contents through the set difference between a document and noisy-information. Finally, we removed stopwords from the content results of the DiffPost algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">News Headline Ranking Step</head><p>The Top Stories Identification Task aims to find important news headlines for a given date query. Let H be news headline and Q be a date query. To estimate the importance of news headline on a date query, we used a language model framework, widely used for many information retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(H|Q) ∝ P(Q|H)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Likelihood</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(H)</head><note type="other">Headline Prior</note><p>(2)</p><p>That is, we assume that for a given date query the importance of news headlines can be estimated using the probability that a query language model generates each news headline. We evaluate each component using only Blog08 corpus and news headline corpus without resorting to any external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Query Likelihood</head><p>For the query likelihood, we should estimate two language models, the Query Language Model (QLM) and the News Headline Language Model (NHLM). Both language models are estimated using the contents of blog posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Query Language Model</head><p>We gather blog posts between -3 and +7 days for a given query (date query). We believe that if a news headline is important on a given day, many blog posts relevant to the news topic were posted near the day.</p><p>The gathered documents contain not only the information relevant to the news topic but also background information or non-relevant topics. We assume that the documents are generated by a mixture model of the QLM and the collection language model. Let QD = {d 1 , d 2 , • • • , d n } be the documents between -3 and +7 days for a given query.</p><formula xml:id="formula_1" coords="3,195.79,170.61,284.80,23.98">P(QD) = ∏ i ∏ w ((1 -λ)P(w|θ QLM ) + λP(w|θ C )) c(w;d i )<label>(3)</label></formula><p>where θ QLM is a query language model, P(w|θ C ) = ct f w |C| : ct f w is the number of times term w occurred in the entire collection, |C| is the length of the collection, c(w; d i ) is the count of a word w in a document d i and λ is a weighting parameter <ref type="foot" coords="3,418.26,230.02,3.69,6.64" target="#foot_0">1</ref> .</p><p>Then, we can estimate θ QLM using the EM algorithm <ref type="bibr" coords="3,377.49,243.83,10.58,8.97" target="#b0">[1]</ref>. The EM updates for P(w|λ QLM ) are as follows:</p><formula xml:id="formula_2" coords="3,227.66,271.98,252.92,26.74">t n w = (1 -λ)P n (w|θ QLM ) (1 -λ)P n (w|θ QLM ) + λP n (w|θ C )<label>(4)</label></formula><formula xml:id="formula_3" coords="3,235.10,300.59,245.49,29.33">P n+1 (w|θ QLM ) = ∑ n j=1 c(w; d j )t n w ∑ i ∑ n j=1 c(w; d j )t n w i (5)</formula><p>The News Headline Language Model To learn the NHLM, for each news headline, we first retrieved blog posts relevant to its topic. We evaluate the relevance between a news headline H and a blog post d using the KL-divergence language model <ref type="bibr" coords="3,448.29,370.57,11.62,8.97" target="#b3">[4]</ref> with Dirichlet smoothing <ref type="bibr" coords="3,217.23,382.52,10.58,8.97" target="#b7">[8]</ref>. : µ d is a smoothing parameter<ref type="foot" coords="3,315.97,447.55,3.69,6.64" target="#foot_1">2</ref> . We gather only blog posts between -7 and +28 days for a given date query among the search results. We then choose 10 supporting relevant posts. When selecting 10 supporting posts, we want them to capture diverse aspects or opinions relevant to the news story. To achieve this, we propose two different approaches.</p><formula xml:id="formula_4" coords="3,225.13,408.05,23.20,8.97">Score</formula><p>One is the Feed-Based Selection (FBS) that selects supporting relevant posts based on feed information which belongs to them. This approach selects 10 supporting relevant posts from as various blog feeds as it possible. To this end, we choose at most two blog posts from each blog feed according to their relevance scores from Eq. 6.</p><p>The other is the Cluster-Based Selection (CBS) which first groups search results into 10 clusters and chooses one representative document from each cluster. To cluster search results, we use K-Medoid clustering algorithm <ref type="bibr" coords="3,353.17,582.17,10.58,8.97" target="#b2">[3]</ref>, and J-Divergence <ref type="bibr" coords="3,442.78,582.17,11.62,8.97" target="#b5">[6]</ref> as the distance function.</p><formula xml:id="formula_5" coords="3,186.17,610.21,294.42,25.64">J(d i ||d J ) = ∑ w p(w|θ i )log p(w|θ i ) p(w|θ j ) + ∑ w p(w|θ j )log p(w|θ j ) p(w|θ i )<label>(7)</label></formula><p>We estimate the NHLM using the maximum likelihood estimate of the 10 supporting relevant posts and the Dirichlet smoothing <ref type="bibr" coords="4,321.06,130.95,10.58,8.97" target="#b7">[8]</ref>.</p><p>Let θ NHLM and θ C be the NHLM and the collection language model, respectively. Let SD be a set of the 10 supporting relevant posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(w|θ</head><formula xml:id="formula_6" coords="4,254.81,174.89,225.78,23.54">NHLM ) = c(w; SD) + µP(w|θ C ) |SD| + µ (8)</formula><p>where c(w; SD) is the count of a word w in the document set SD and µ is a smoothing parameter 3 .</p><p>The Score Function To evaluate the query likelihood, we use KL-divergence language model <ref type="bibr" coords="4,162.16,264.12,11.62,8.97" target="#b3">[4]</ref> to rank news headlines in response to a given date query.</p><p>Let Score QLH (Q, H) be the relevance score of the news headline H with respect to a given date query Q.</p><formula xml:id="formula_7" coords="4,182.85,304.50,297.73,23.61">Score QLH (Q, H) = ∑ w P(w|θ QLM )logP(w|θ NHLM ) + const(Q) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The News Headline Prior</head><p>We propose two criteria to estimate the news headline prior that it will be a top story for a given day. We consider these criteria as the priors of a news headline in that they are independent of the query language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Temporal Profiling</head><p>The Temporal Profiling criterion uses time information of blog posts relevant to each news headline. We made a temporal profile of each news headline using a temporal profiling approach that Diaz and Jones <ref type="bibr" coords="4,407.23,444.82,11.62,8.97" target="#b1">[2]</ref> proposed. The temporal profile of the news headline H is defined as follows:</p><formula xml:id="formula_8" coords="4,235.12,477.43,241.31,26.22">P(t|H) = ∑ d∈R P(t|d) P(H|d) ∑ d ′ ∈R P(H|d ′ ) (<label>10</label></formula><formula xml:id="formula_9" coords="4,476.44,484.18,4.15,8.97">)</formula><p>where R is the document set which contains top 500 blog posts among the search results from Eq. 6, and P(H|d) is approximated using Score(H, d), and</p><formula xml:id="formula_10" coords="4,213.96,547.61,184.83,20.92">P(t|d) = 1 if t is equal to the document date 0 otherwise</formula><p>We then smooth the temporal profile P(t|H) using the background model as follows:</p><formula xml:id="formula_11" coords="4,234.92,600.39,241.52,12.98">P(t|H) = (1 -α)P(t|H) + αP(t|C) (<label>11</label></formula><formula xml:id="formula_12" coords="4,476.44,603.65,4.15,8.97">)</formula><p>where α is a smoothing parameter and P(t|C) = 1 |D| ∑ d∈C P(t|d): |D| is the total number of documents in the entire collection. This temporal profile is defined at each single day. However, the blog posts about the important news story may occur over a period of several days. Therefore, we smooth the temporal profile model with the model for adjacent days. Let Score T P (H) be the value of a news headline estimated using the temporal profile of the news headline.</p><formula xml:id="formula_13" coords="5,233.41,177.69,243.03,26.43">Score T P (Q, H) = 1 |φ| ∑ φ i P(t + φ i |H) (<label>12</label></formula><formula xml:id="formula_14" coords="5,476.44,184.44,4.15,8.97">)</formula><p>where φ indicates the period <ref type="foot" coords="5,249.79,218.18,3.69,6.64" target="#foot_2">4</ref> .</p><p>The Term Importance The Term Importance criterion uses term information of each news headline. We believe that respective terms have a different importance for a given date query. If a headline consists of more important terms, it is more likely to be a top story. For example, a headline consisted of common words or stopwords may not a top story.</p><p>To estimate the term-based evidence, we first extracted all NP phrases from each news headline using Stanford Parser <ref type="foot" coords="5,284.71,328.95,3.69,6.64" target="#foot_3">5</ref> . We then gathered all n-gram (n ≤ 3) from NP phrases. We evaluate the importance of the n-gram terms using two heuristic measures.</p><p>One is term frequency measure that is naive, but widely used in the many IR tasks. Intuitively, if a term occurs frequently at a query date, it is more likely to be important. Let nt be n-gram term and T F(nt, Q) be a term frequency measure for a date query Q.</p><formula xml:id="formula_15" coords="5,244.58,404.44,231.86,9.96">T F(nt, Q) = log(1 + c(nt; Q)) (<label>13</label></formula><formula xml:id="formula_16" coords="5,476.44,405.11,4.15,8.97">)</formula><p>where c(nt; Q) means the count of a term nt within the news headlines that are issued at the same date with a given date query Q.</p><p>The other is distribution within the news headline corpus. We believe that if a term occurrence is concentrated at a query date, it is more likely to be important. Let DI(nt, Q) be the measure of the term distribution for a date query Q. We define DI(nt, Q) as follows:</p><formula xml:id="formula_17" coords="5,254.40,515.35,226.18,24.13">DI(nt, Q) = c(nt; Q) ∑ t∈T c(nt;t) (14)</formula><p>where T indicate a set of days corresponding to timespan covered by the news headline corpus.</p><p>Let Score T I (Q, H) be the value of a news headline evaluated based on the termbased evidence. </p><formula xml:id="formula_18" coords="5,215.69,613.91,264.89,15.16">Score T I (Q, H) = max nt (T F(nt, Q)×DI(nt, Q)) (15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Ranking Fuction</head><p>From above steps, we can get three scores related to the importance of the news headline. To rank the news headlines, we should combine the score of the query-likelihood with two measures for the headline prior. However, each score has different scale. Therefore, a value of each score is scaled from 0 to 1. Finally, our ranking function is as follows:</p><formula xml:id="formula_19" coords="6,180.98,393.71,299.61,28.10">Score(H, Q) = (1 -β 1 )Score QLH (Q, H) + β 1 {(1 -β 2 )Score T P (Q, H) + β 2 Score T I (Q, H)} (16)</formula><p>where β 1 is the weighting parameter that controls the importance between the query likelihood and the headline prior components, and β 2 controls the weight between two evidences for the headline prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>In the Top Stories Identification Task, we submitted 4 runs as follows:</p><p>1. KLEFeed chooses 10 supporting blog posts using the FBS to estimate the NHLM, and does not use the news headline prior, that is, β 1 = 0. 2. KLECluster chooses 10 supporting blog posts using the CBS to estimate the NHLM, and does not use the news headline prior. 3. KLEFeedPrior chooses 10 supporting blog posts using the FBS to learn the NHLM, and uses the news headline prior with β 1 = 0.7, β 2 = 0.2. 4. KLEClusPrior chooses 10 supporting blog posts using the CBS to learn the NHLM, and uses the news headline prior with β 1 = 0.7, β 2 = 0.2.</p><p>Table <ref type="table" coords="6,174.08,644.17,4.98,8.97" target="#tab_1">1</ref> and 2 shows the performances of our runs based on the headlines relevance judgments only and based on the blog-post level evaluation, respectively.</p><p>We have described our participation in the TREC 2009 Blog Track. For Top Stories Identification Task, we presented two components: the query likelihood and the news headline prior, based on the language model framework. To evaluate the query likelihood, we estimated the query language model and the news headline language model using the contents of blog posts. We also proposed two methods to choose the 10 supporting blog posts for each news headline: FBS and CBS. Furthermore, we suggested two criteria to estimate the news headline prior. One is Temporal Profiling criterion that uses time information of blog posts relevant to each headline. The other is Term Importance criterion that uses term information of a news headline. Experimental results show that using the news headline prior significantly improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This work was supported in part by MKE &amp; IITA through IT Leading R&amp;D Support Project and also in part by the BK 21 Project in 2009.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,170.49,117.78,274.38,139.97"><head>Table 1 .</head><label>1</label><figDesc>The performances based on the headlines relevance judgments only</figDesc><table coords="6,207.60,117.78,200.16,139.97"><row><cell></cell><cell>Run</cell><cell>MAP P@10</cell></row><row><cell></cell><cell cols="2">KLEFeed 0.0132 0.0345</cell></row><row><cell cols="3">KLECluster 0.0182 0.0600</cell></row><row><cell cols="3">KLEFeedPrior 0.1548 0.2800</cell></row><row><cell cols="3">KLEClusPrior 0.1605 0.2964</cell></row><row><cell>Run</cell><cell cols="2">NDCG-alpha@10 intent-aware P@10</cell></row><row><cell>KLEFeed</cell><cell>0.066</cell><cell>0.023</cell></row><row><cell>KLECluster</cell><cell>0.098</cell><cell>0.037</cell></row><row><cell>KLEFeedPrior</cell><cell>0.504</cell><cell>0.162</cell></row><row><cell>KLEClusPrior</cell><cell>0.409</cell><cell>0.117</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,188.22,260.55,238.90,8.07"><head>Table 2 .</head><label>2</label><figDesc>The performances based on the blog-post level evaluation</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,642.57,96.06,11.68"><p>In our runs, we set λ = 0.7</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.20,106.44,9.89"><p>In our runs, we set µ d = 1000</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,144.73,642.57,261.98,11.68"><p>In our runs, the period φ is between -3 and +7 days from the query day Q</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,144.73,656.80,172.85,8.07"><p>http://nlp.stanford.edu/software/lex-parser.shtml</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.13,373.72,342.46,8.07;7,146.48,384.68,263.57,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,284.63,373.72,195.96,8.07;7,146.48,384.68,33.40,8.07">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,199.02,384.68,141.27,8.07">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="issue">B</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,395.64,342.47,8.07;7,146.48,406.60,334.11,8.07;7,146.48,417.56,206.77,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,221.98,395.64,213.02,8.07">Using temporal profiles of queries for precision prediction</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,457.18,395.64,23.41,8.07;7,146.48,406.60,334.11,8.07;7,146.48,417.56,130.49,8.07">SIGIR &apos;04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,428.51,342.46,8.07;7,146.48,439.47,142.47,8.07" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,264.48,428.51,212.65,8.07">Finding groups in data: an introduction to cluster analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,450.44,342.46,8.07;7,146.48,461.39,334.12,8.07;7,146.48,472.35,318.80,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,227.63,450.44,252.96,8.07;7,146.48,461.39,72.58,8.07">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,236.09,461.39,244.50,8.07;7,146.48,472.35,233.56,8.07">SIGIR &apos;01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,483.31,342.46,8.07;7,146.48,494.27,244.08,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,382.21,483.31,98.38,8.07;7,146.48,494.27,98.22,8.07">Kle at trec 2008 blog track: Blog post and feed retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,263.30,494.27,97.07,8.07">Proceedings of TREC 2008</title>
		<meeting>TREC 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,505.23,342.46,8.07;7,146.48,516.19,110.07,8.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,175.68,505.23,184.35,8.07">Divergence measures based on the shannon entropy</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,367.52,505.23,113.07,8.07;7,146.48,516.19,39.10,8.07">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,527.14,342.46,8.07;7,146.48,538.11,334.11,8.07;7,146.48,549.07,195.73,8.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,301.27,527.14,179.33,8.07;7,146.48,538.11,198.03,8.07">Diffpost: Filtering non-relevant content based on content difference between two consecutive blog posts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,453.20,538.11,27.38,8.07;7,146.48,549.07,95.64,8.07">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5478</biblScope>
			<biblScope unit="page" from="791" to="795" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,560.02,342.46,8.07;7,146.48,570.98,212.03,8.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,224.08,560.02,256.51,8.07;7,146.48,570.98,44.69,8.07">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,197.56,570.98,79.52,8.07">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
