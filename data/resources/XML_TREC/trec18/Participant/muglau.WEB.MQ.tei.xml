<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.68,123.76,379.90,18.08;1,165.68,145.67,263.91,18.08">IRRA at TREC 2009: Index Term Weighting based on Divergence From Independence Model</title>
				<funder ref="#_Fetpnkg">
					<orgName type="full">DFI</orgName>
				</funder>
				<funder ref="#_TRTdCmn">
					<orgName type="full">The Scientific and Technological Research Council of Turkey</orgName>
				</funder>
				<funder>
					<orgName type="full">TUBITAK</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,111.52,181.57,77.60,9.41"><forename type="first">Bekir</forename><forename type="middle">Taner</forename><surname>Dinçer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Mugla University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.55,179.31,54.96,11.67"><forename type="first">İlker</forename><surname>Kocabaş</surname></persName>
							<email>ilker.kocobas@ege.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department">International Computer Inst</orgName>
								<orgName type="institution">Ege University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.63,181.57,69.01,9.41"><forename type="first">Bahar</forename><surname>Karaoglan</surname></persName>
							<email>bahar.karaoglan@ege.edu.tr</email>
							<affiliation key="aff2">
								<orgName type="department">International Computer Inst</orgName>
								<orgName type="institution">Ege University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.68,123.76,379.90,18.08;1,165.68,145.67,263.91,18.08">IRRA at TREC 2009: Index Term Weighting based on Divergence From Independence Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9890FBFCB8AC10A88517C47E164A8884</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>IRRA (IR-Ra) group participated in the 2009 Web track (both adhoc task and diversity task) and the Million Query track. In this year, the major concern is to examine the effectiveness of a novel, nonparametric index term weighting model, divergence from independence (DFI).</p><p>The notion of independence, which is the notion behind the well-known statistical exploratory data analysis technique called the correspondence analysis <ref type="bibr" coords="1,303.29,332.33,75.93,10.46" target="#b4">(Greenacre, 1984;</ref><ref type="bibr" coords="1,381.98,332.33,56.85,10.46" target="#b7">Jambu, 1991)</ref>, can be adapted to the index term weighting problem. In this respect, it can be thought of as a qualitative description of the importance of terms for documents, in which they appear, importance in the sense of contribution to the information contents of documents relative to other terms. According to the independence notion, if the ratios of the frequencies of two different terms are the same across documents, they are independent from documents. For example, each Web page contains a pair of "html" and a pair of "body" tags, so that the ratio of frequencies of these tags is the same across all Web pages, indicating that the "html" and "body" tags are independent from Web pages. They are used by design, irrespective of the information contents of Web pages. On the other hand, some tags, such as "image", "table", which are also independent from Web pages, may occur less or more in some pages than the expected frequencies suggested by the independence model; so, their associated frequency ratios may not be the same for all Web pages. However, it is reasonable to expect that, if the pages are not about the tags' usage, such as a "HTML Handbook", frequencies of those tags should not be significantly different from their expected frequencies: they should be close to the expectation, i.e., in a parametric point of view, their observed frequencies on individual documents should be attributed to chance fluctuation. Although this tag example is helpful in exemplifying the use of independence notion, it is obvious that the tags are artificial, and so, governed by some rules completely different from the rules of a spoken language. Nonetheless, some words, like the ones in a common "stopwords list", appear in documents, not because of their contribution to the information contents of documents, but because of the grammatical rules. On this account, such words can be modeled as if they were tags, because they are independent from documents in the same manner. Their observed frequencies in individual documents is expected to fluctuate around their frequencies expected under independence, as in the case of tags. Content bearing words are, therefore, the words whose frequencies highly diverge from the frequencies expected under independence. The results of the TREC experiments about IRRA runs show that the independence notion promises a natural basis for quantifying the categorical relationships between the terms and the documents.</p><p>The TERRIER retrieval platform <ref type="bibr" coords="1,232.45,631.21,81.41,10.46" target="#b9">(Ounis et al., 2007)</ref> is used to index and search the ClueWeb09-T09B<ref type="foot" coords="1,523.70,630.14,3.97,7.32" target="#foot_0">1</ref> data set, a subset of about 50 million Web pages in English <ref type="bibr" coords="1,328.04,643.16,156.92,10.46">(TREC 2009 "Category B" data set)</ref>. During indexing and searching, terms are stemmed and a particular set of stop words<ref type="foot" coords="1,411.79,654.05,3.97,7.32" target="#foot_1">2</ref> are eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Organization</head><p>In statistics, the raw input to multivariate data analysis is usually a r×c (r rows by c columns) rectangular array of real numbers called a data matrix given in Figure <ref type="figure" coords="2,329.73,121.85,3.87,10.46" target="#fig_0">1</ref>.  The data matrix represents the observations made on r objects each of which is characterized with respect to c variables. The observed values of those variables may represent the measurement of a quantity or a numerical code for a classification scheme. The objects may be an individual or a unit, and hence the variables will be the attribute, characteristics, response or item of these objects. This multidimensional data are represented by r × c matrix X. The elements of X represent the observed values and denoted by x ij where i = 1, 2, . . . , r and j = 1, 2, . . . , c; the marginal total of row i is denoted by x i. and is given by x i. = c j=1 x ij ; similarly x .j denotes the marginal total of column j and is given by x .j = r i=1 x ij ; finally, x .. denotes the grand total and is given by x .. = r i=1 c j=1 x ij . Any given set of documents can be formed into a r × c data matrix whose columns represent documents, rows represent terms, and cells contain the frequency of each term in the associated documents.</p><p>3 Divergence From Independence -DFI Given a term-by-document data matrix X, the notion of independence can simply be explained by odds ratios. The ratio x ij /x j measures the odds of being in row i relative to row given column j. The ratio x ik /x k measures the odds of being in row i relative to row given column k. The odds ratio is the ratio of the two sets of odds and is given by</p><formula xml:id="formula_0" coords="2,281.48,493.29,31.66,24.94">x ij /x j x ik /x k</formula><p>The odds ratio is necessarily 1, if the independence assumption holds. Under independence, the odds of being in row i relative to row do not depend on the column.</p><p>DFI is closely related to the divergence from randomness (DFR) model introduced by <ref type="bibr" coords="2,475.67,547.16,47.60,10.46;2,72.00,559.12,96.69,10.46" target="#b0">Amati and Van Rijsbergen (2002)</ref>, but they are different in that, in DFR, it is assumed that the important terms of a given document are those words whose frequencies diverge from the frequencies suggested by a basic randomness model, such as Poisson, Hyper-Geometric, Bose-Einstein etc, whereas in DFI, it is assumed that the important terms of a given document are those words whose frequencies diverge from the frequencies suggested by the independence model. <ref type="bibr" coords="2,86.94,618.90,72.97,10.46">Harter (1975a,b)</ref> is the first researcher who uses the Poisson distribution for weighting index terms. In essence, the notion behind DFR, as well as DFI, is the notion behind the Harter's approach. In the Harter's approach, words are classified into two groups, namely the "speciality words" and the "nonspeciality words". Speciality words, or the content bearing words, are the words that occur densely in an "elite set" of documents, whose informative contents are actually related to the meanings of that words, whereas nonspeciality words are those words whose frequencies distribute on documents, randomly. In this point of view, speciality words should differ from the nonspeciality words in distribution on a collection of documents. Harter argues that both the speciality and the nonspecialty words follows a Poisson distribution, but with different means, λ 1 and λ 2 , respectively, where λ 1 &gt; λ 2 . According to the DFR model, speciality words are those words whose within document frequencies diverge from the frequencies suggested by the basic randomness model, whereas nonspeciality words are the words that follow the basic randomness model. According to the DFI model, speciality words are those words whose within document frequencies depend on the documents, whereas nonspeciality words are those words whose within document frequencies are independent from the documents. The difference is that, DFI replaces the notion of randomness with the notion of independence. In DFI, amount of divergence from independence is measured as chi-square distance. The fact that the Pearson's chi-square statistic is of the nonparametric type <ref type="bibr" coords="3,178.80,147.85,69.73,10.46" target="#b1">(Conover, 1999)</ref> suggests that the proposed index term weighting model is the nonparametric counterpart of the model introduced by Harter. The major advantage of a nonparametric index term weighting model is that, it does not require a hypothesis about the functional form of the term frequency distributions on document population, such as Poisson distribution: that is, to decide whether a particular term is independent from a given document, the proposed model does not require any external reference<ref type="foot" coords="3,167.84,206.55,3.97,7.32" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weighting Models based on DFI</head><p>DFI quantifies the categorical relationships between the terms and the documents, and hence, it basically corresponds to the TF component of the well-known TF×IDF weighting scheme <ref type="bibr" coords="3,430.86,265.72,92.42,10.46;3,72.00,277.67,22.14,10.46" target="#b12">(Salton and Buckley, 1988)</ref>, where TF stands for the term frequency and IDF stands for the inverse document frequency. In contrast to TF, IDF is a collection dependent factor, which identifies the terms that concentrates in a few documents of the collection. <ref type="bibr" coords="3,243.22,301.58,118.08,10.46" target="#b12">Salton and Buckley (1988)</ref> state that "the term discrimination considerations suggest that . . . the best terms should have high term frequencies but low overall collection frequencies". According to the DFI notion, this statement can be such that ". . . the best terms should have high DFI scores but low overall collection frequencies". A trivial weighting model based on the TF×IDF scheme can therefore be given as</p><formula xml:id="formula_1" coords="3,252.27,373.32,90.23,11.35">w ij = DF I ij × IDF i</formula><p>The amount of divergence from independence, DF I ij of a given term t i (i = 1, 2, . . . , r) in a particular document d j (j = 1, 2, . . . , c) is measured as the difference of the frequency of the term in that document (x ij ) from the expected frequency (e ij ) suggested by the independence model, given by e ij = x i.</p><p>x .j x ..</p><p>where x i. corresponds to the total frequency of t i in collection (i.e. x i. = c j=1 x ij ), and x .j corresponds to the length of document d j (i.e., x .j = r i=1 x ij ). The intuition behind expected frequencies is as follows. Under independence, marginal (total) frequencies of terms, x i. 's should be distributed on documents, proportionally to the length of documents (x .j /x .. ), i.e., j x .j = x.. and so j x .j /x .. = 1; thus j e ij = x i. . For any term t i and document d j , the amount of divergence from independence, DF I ij can be measured as chi-square distance, given by</p><formula xml:id="formula_2" coords="3,251.57,542.56,90.44,26.00">DF I ij = (x ij -e ij ) 2 e ij</formula><p>Notice that, in this formulation, it is not possible to distinguish whether the frequency of a particular term is below or above the expected frequency, i.e., it is not possible to determine the direction of interaction between terms and documents. On the other hand, it is a fact that terms whose frequencies are above the expected frequencies are the terms that have a categorical relationship with the documents, in a positive sense, whereas terms whose frequencies are below the expected frequencies are the terms that have also a categorical relationship with the associated documents, but in a negative sense. A term weighting method is supposed to identify documents that have a positive categorical relationships with the given query terms; thus, it is necessary to make the direction of interaction explicit, as given by</p><formula xml:id="formula_3" coords="3,250.48,685.04,92.22,24.93">DF I ij = x ij -e ij √ e ij</formula><p>In practice, for a given term t i , its associated weight w ij with document d j should be taken as 0, when DF I ij ≤ 0. In consequence, given a query q k (k = 1, 2, . . .) with p terms, score (s kj ) of a given document d j can be calculated as</p><formula xml:id="formula_4" coords="4,245.31,131.47,103.75,32.01">s(q k , d j ) = p i x ik × w ij</formula><p>where x ik is the frequency of term t i in query q k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DFI Formulae used in IRRA runs</head><p>The basic DFI formula used in IRRA runs is given by</p><formula xml:id="formula_5" coords="4,233.16,242.21,285.87,24.93">DF I ij = log 2 x ij -e ij √ e ij , (<label>1</label></formula><formula xml:id="formula_6" coords="4,519.03,248.95,4.24,10.46">)</formula><p>Use of a log-transformation is attractive for two reasons. First, it provides a multiplicative model for document scoring (i.e., s = log(w 1 ) + log(w 2 ) = log(w 1 × w 2 )), instead of an additive model (i.e., s = w 1 + w 2 ). Given a query q k with p terms, in the additive model, scores of two different documents can be the same, when the sets of term weights associated with individual documents sum up to the same total, while in contrast, in the multiplicative model, different sets of weights associated with different documents might result in different scores. In other words, given a query q k with p terms, for every document d j , an additive model produces the same document score with different sets of term weights, satisfying the equation given by w 1j + w 2j + . . . + w pj = C, while a multiplicative model produces the same document score with different sets of term weights, satisfying the equation given by</p><formula xml:id="formula_7" coords="4,72.00,383.27,118.08,11.35">w 1j × w 2j × • • • × w pj = C.</formula><p>The multiplicative model can therefore provide more discrimination power than the additive model. Second, it is a fact that it is possible to obtain a multiplicative model of document scoring by simply multiplying the term weights (i.e., s(q k , d j ) =</p><p>x ik × w ij ), but in this time, the benefit of using a power transformation is lost. As in the case of DFI, if the symmetry of the main body of the data around the center is desired but skewness in the tails is relatively unimportant, then a log-transformation should be used.</p><p>Notice that the DFI formula given in Equation 1 also applies a power transformation to the expected frequencies ( √ e ij ) in the denominator, namely a square-root transformation which provides the symmetry in the tails of the distribution, i.e., providing the symmetry in the tails of the expected frequency distributions of terms in individual documents. In consequence, the following DFI formulae can also be considered:</p><formula xml:id="formula_8" coords="4,233.16,529.75,285.87,53.04">DF I ij = x ij -e ij e ij , DF I ij = log 2 x ij -e ij e ij , (<label>2</label></formula><formula xml:id="formula_9" coords="4,519.03,564.60,4.24,10.46">)</formula><p>However it should be noted that these formulae do not measure the divergence from independence in the true sense; hence, they actually suffer from the lack of a well-defined theoretical basis, though the results of the experiments performed on the past TREC collections shown that DFI formula given in Equation 2 is the superior on long queries (i.e., Title + Description + Narrower).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The IDF component</head><p>IRRA runs, which were submitted to all TREC tracks, employ an IDF formulation derived based on the DFI. However, for the IDF component, the following standard IDF formulations could also be used:</p><formula xml:id="formula_10" coords="4,171.46,716.22,252.36,41.25">IDF 1 = log 2 (c/c i ) IDF 2 = log 2 ((c -c i + 0.5)/(c i + 0.5)), (BM25 IDF) IDF 3 = log 2 ((c + 1)/(c i + 0.5)), (DFR IDF)</formula><p>where c is the total number of documents in the collection and c i is the number of documents that contain term t i . IDF 1 is attributed to <ref type="bibr" coords="5,242.11,100.03,53.31,10.46" target="#b8">Jones (1972)</ref>; IDF 2 is attributed to <ref type="bibr" coords="5,400.31,100.03,99.34,10.46" target="#b11">Robertson et al. (1981)</ref>, and <ref type="bibr" coords="5,72.00,111.99,127.29,10.46" target="#b10">Robertson and Walker (1994)</ref>. Empirical results suggest that the optimal weighting model is the model that includes IDF 2 coupled with the DFI formula given in the Equation <ref type="formula" coords="5,402.40,123.94,3.87,10.46" target="#formula_5">1</ref>. Nevertheless, other IDF formulae could also be used. Empirical results revealed that the differences in contribution to the overall retrieval performance between the IDF formulae are negligible, compared to the differences between the DFI formulae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Run Descriptions</head><p>IRRA runs are of the content-only type: none of the information in Web pages, like Meta information, Link information, etc., was distinguished and utilized. In addition, during searching, none of the common supplementary methods, such as parsing rules, phrase processing, query processing, query expansion, relevance feedback, thesaurus, WordNets is used. IRRA runs are pure, out-of-the-box DFI runs.</p><p>irra1a : TF component is a derivation of the DFI formula given in Equation <ref type="formula" coords="5,416.18,272.36,4.98,10.46" target="#formula_5">1</ref>and IDF component is also a DFI based formula developed as an alternative to the existing IDF formulae.</p><p>irra2a : DFI formula given in Equation <ref type="formula" coords="5,251.93,304.24,4.98,10.46" target="#formula_8">2</ref>and IDF used in "irra1a".</p><p>irra3a : DFI formula given in Equation <ref type="formula" coords="5,251.93,324.16,4.98,10.46" target="#formula_5">1</ref>and IDF used in "irra1a".</p><p>These are the runs that were submitted to the Web track adhoc task. The runs submitted to the diversity task are based on the same strategies used in adhoc task, with a difference. For diversity task, the result sets are filtered such that they consist of at most two Web pages from the same host (URL), and labeled as "irra1d", "irra2d" and "irra3d", respectively. For MQ track, both strategies are used, and the runs are labeled as "irra1mqa", "irra1mqd", "irra2mqa", "irra2mqd", and "irra3mqd', where "a" and "d" indicate that the associated run employs adhoc strategy and diversity strategy, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this year, the major concern is to verify that the DFI is a valid basis for weighting index terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adhoc Task</head><p>The estimated performance scores of adhoc runs over 49 topics<ref type="foot" coords="5,341.94,505.83,3.97,7.32" target="#foot_3">4</ref> are given in the Table <ref type="table" coords="5,445.40,506.90,4.98,10.46" target="#tab_1">1</ref> and<ref type="table" coords="5,472.02,506.90,47.38,10.46" target="#tab_2">the Table 2</ref> It appears that the runs "irra1a" and "irra3a" are compatible in performance. Although they show relatively higher performance than "irra2a", both "irra1a" and "irra3a" seems to be the median runs on the average. However, the observed statAP scores of "irra1a" on topic 12 and 26 are the highest scores that could be achieved by the submitted runs. Similarly, "irra2a" has the best scores at topic 34 and 45; "irra3a" has the best score at topic 12. Observed statAP scores of "irra1a" are above the median scores of 24 topics; "irra2a" is above median on 14 topics; "irra3a" is above median on 26 topics. On the other hand, "irra2a" is also said to be compatible in performance with the other IRRA runs, based on the Prec30 measure. MTC results, which is given in Table <ref type="table" coords="6,441.31,171.76,3.87,10.46" target="#tab_2">2</ref>, also lead to the similar conclusions. Based on the MTC statistics, relatively low confidence value between "irra1a" and "irra3a" (P (∆M AP &lt; 0) = 0.7670) indicates high uncertainty in the final ranking of these two runs. This also suggests that they are compatible in performance. However, it should be noted that, based on eP5 (expected precision at 5 documents), "irra2a" is the best run, and that its performance very stable across different measurement depths. Its eP5, eP10, eP15, eP20, eP30 and eP100 scores are approximately equal, while the performance scores of others tend to increase by measurement depth.</p><p>It is a fact that the average performance is a valid criterion to decide that one system is better than other. But it is also a fact that making decisions solely based on this criterion may well be misleading. High average performance is the necessary, but not the sufficient condition for being an effective IR system. A univariate data analysis employs univariate statistics, and a univariate statistic, such as raw averaging, may oversimplify, even hide the performance differences between retrieval strategies. In contrast to univariate data analysis, multivariate data analysis is concerned with the joint nature of measurements, and it refers to analyze the data in a manner that takes into account the relationships among measurements. It is, therefore, better if a univariate analysis is supported by the multivariate data analysis techniques, before making the final decision about the effectiveness of the retrieval strategies under consideration.</p><p>To visually inspect the mutual performance relationships among different retrieval strategies across a given set of topics, one can use the principal components analysis (PCA), a statistical, multivariate, exploratory data analysis technique. How PCA is applied to the results of retrieval experiments can be found in the work of <ref type="bibr" coords="6,167.19,410.87,60.21,10.46" target="#b3">Dinçer (2007)</ref>  <ref type="foot" coords="6,227.40,409.79,3.97,7.32" target="#foot_4">5</ref> . PCA is basically a dimension reduction technique. Since higher dimensional spaces are difficult to inspect, it becomes necessary to reduce them into lower dimensions. Roughly speaking, in a test collection with many topics, groups of topics are often performed similarly by the retrieval systems. One reason for this is that more than one topic might be measuring the same driving characteristic governing the behavior of the retrieval systems. In many systems there are only a few such driving forces. But an abundance of instrumentation enables us to measure dozens of topics. When this happens, you can take advantage of this redundancy of information. Thereby, you can simplify the problem by replacing a group of topics with a single new "meta topic". PCA is simply a method for achieving this simplification. In PCA, principal components correspond to those "meta topics". Each principal component is a linear combination of the original topics. All the principal components are orthogonal to each other; so, opposed to the original topics, there is no redundant information. The first principal component is a single axis in space. When you project each measured performance score on that axis, the resulting values form a new meta topic. And the variance of this meta topic is the maximum among all possible choices of the first axis. The second principal component is another axis in space, perpendicular to the first. Projecting the performance scores on this axis generates another new meta topic. The variance of this meta topic is the maximum among all possible choices of this second axis. Thus, the first two principal components together can be used to define two orthogonal dimensions, and hence can be used to visualize the mutual performance relationships between considered retrieval systems in a two dimensional representation, by means of a scatter plot, which accounts for the major part of the total performance variations observed on the original topics. The scatter plot of the component scores of IRRA runs and the loadings of 49 topics on the first and the second principal axes is given in Figure <ref type="figure" coords="6,152.23,661.92,3.87,10.46" target="#fig_1">2</ref>. There are three extra points in the plot labeled as "APWorst", "APMedian" and "APBest", in addition to the IRRA runs. The point "APWorst" can, for instance, be thought of as a run that performs each topic with a statAP score equal to the statAP score of the worst run.</p><p>Interpretation of a PCA biplot is simple. Runs, which are cumulated at the same location in the plot, are the runs that show relatively close average performances over the same set (subset) of topics. Groups of runs, which are compatible in performance, may scatter to different locations in the plot. Subsets of topics associated with those locations vary and can be determined by the loadings of topics on the associated principal components. Origin of the given PCA plot represents the average performance over all topics, and the gradient levels of observed performance scores are depicted towards the corners of the plot. The first principal component accounts for the 89.01% and the second principal component accounts for the 8.92% of the total performance variation observed on all topics; thus, the given PCA plot explains about 99% of the total performance variation among the considered runs across the 49 adhoc topics. In the given PCA plot, first principal component is positively related to almost all topics; hence, the first principal axis acts as an index variable, and agrees with the statMAP measure. On the other hand, second principal component contrasts the performance of runs on two subsets of topics.</p><p>The first principal component is dominated by the topics, including 46, 2, 21, 33, and 45; thus, the runs that are effective on these topics tend to have high scores on the first principle axis, and the ineffective runs tend to have low scores. On the other hand, the second principal component contrasts the performance scores observed on topics, including 10, 42, 6, 23, and 9, with the performance scores observed on topics, including <ref type="bibr" coords="7,115.12,498.91,91.88,10.46">46, 33, 22, 45, and 12</ref>. This means that the high component scores along the second principal axis indicate high values of effectiveness on the former topics, and low values of effectiveness on the later topic. Conversely, the low component scores indicate high values of effectiveness on the later topics, and low values of effectiveness on the former topics. However in particular to the case at hand, interpretation should slightly be different, because there can be no run that could perform a topic better than the "APBest" run. That is to say, on the former topics, the observed differences in performance between the "APBest" run and the IRRA runs are higher than the corresponding differences observed on the former topics. For example, on topic 10, the observed statAP score of "APBest" is 0.8994, and the best score amongst others is of "irra2a", with 0.1604. But on topic 33, the observed score of "APBest" is 0.5224, and the observed score of "irra3a" is 0.4736. In this respect, it can be said that the contrast between the former topics and the later topics is related to the difference between the "APBest" run and the IRRA runs. On the former topics (i.e., the topics located on the top half panel of the plot), magnitude of difference is high, while on the later topics (i.e., the topics located on the bottom half panel of the plot), it is low. That's why all topics on which the IRRA runs have the best score are located in the bottom half panel of the plot. But notice that, except for the topic 45, all of those topics are located on the left-bottom panel of the plot. With respect to the first principle component, this means that the magnitudes of the scores observed on that topics are low in general, compared to the magnitudes of the scores observed the topics located on the right-bottom panel of the plot.</p><p>In PCA biplots, the proximities between runs and topics can only be interpreted through the principal components. That is to say, the closeness of two objects in a PCA biplot is meaningful only if the objects are of the same kind. Cross-proximities between a run and a topic has no direct interpretation, topics and runs can only be related through principal components. In the given PCA biplot, the topics that dominate the first principle component tend to have high scores on the first principal axis, proportional to the weights assigned by the first eigenvector, obtained by the spectral decomposition of the data matrix in use. For example, topic 19 is the topic that has the lowest score along the first principal axis, and so, it is the topic that is farthest from the "APBest" run. But "APBest" is still the run that has the highest score on this topic. Position of topic 19 indicates that the magnitude of the performance scores observed on topic 19 is lower than the magnitude of the performance scores observed on the topics that have high scores on the first principal axis, such as topic 46. This means that the effect of topic 19 on the average performance of the considered runs is relatively less than the effect of topic 46. It can, therefore, be said that the average performance scores of the considered runs are mostly determined by the topics that have high scores on the first principle axis. This suggests that the performance ranking of the considered runs could remain constant, even if the most of the topics located on the left-half panel of the plot are ignored (starting from the left most topic).</p><p>Recall that "irra1a" and "irra3a" are the median runs, on the average. This is clearly depicted by the given PCA plot: they are located close to the "APMedian". But, as seen, "irra2a" is also located close to the "APMedian". This suggests that, for the influencing topics, the performance differences between the IRRA runs are negligible, when they are compared to the performance differences observed among "APWorst", "APMedian" and "APBest" runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of the Performance Profiles of IRRA runs</head><p>Retrieval effectiveness has many dimensions, and the average performance is just one of those dimensions. Performance profile of a retrieval strategy is also an important dimension of retrieval effectiveness, and refers to the distribution of its total (average) performance on topics. It is a fact that, given a set of topics, a particular level of total performance, say A, can be obtained by infinitely may sets of topic scores, satisfying the equation A = x 1 + x 2 + . . . + x c , where x j is the performance score observed on topic j (j = 1, 2, . . . , c). This suggests that two particular runs may have the same average performance over a set of topics, but their observed performance scores on individual topics can be different, as long as the scores of each run sum up to the same total over all topics. In contrast, two runs may have different average performance scores, while they show the same performance profile across the given set of topics. In this case, per topic performance scores of runs must be the multiple of each other on every topic. For instance, suppose that the total performance of a run (r 1 ) is A 1 . Then the total performance of the other run (r 2 ) must be A 2 = A 1 (for ≥ 0):</p><formula xml:id="formula_11" coords="8,202.24,500.97,190.80,26.31">A 1 = x 1 + x 2 + . . . + x c , A 2 = • A 1 = • x 1 + • x 2 + . . . + • x c .</formula><p>This suggest that, whatever the functional difference between r 1 and r 2 is, it evenly contributes to the performance of r 2 on every topic. Moreover, in relation to the performance profile of retrieval system, we can consider a research question, which may arises as "Is it possible to define an ideal performance profile for a given retrieval system?". To define the ideal performance profiles of retrieval systems and to compare their observed performance profiles, a statistical, multivariate, exploratory data analysis technique, called the correspondence analysis (CA), can be used. How CA is applied to the results of retrieval experiments can be found in the work of Dinçer 6 . The independence notion/model, which is used in the DFI based index term weighting methods, is also the notion behind the CA. According to the independence model, a retrieval system is said to be independent from topics, if the observed performance of that system on each topic is equal to the performance suggested by the independence model. For any retrieval strategy, to be independent from topics is important, because, given a test collection with a particular set of topics, if the observed performance of a particular retrieval strategy is independent from the topics, then it is reasonable to expect that a similar performance would also be observed on different sets of topics 7 . The performance suggested by the independence model vary for each pair of system and topic, according to both the total performance of the given system (i.e., the system sum score over all topics) and the total performance score of the given topic (i.e., the topic sum score over all systems). Let x i. = c j=1 x ij and x .j = r i=1 x ij denote, respectively, the total performance score of system i (i = 1, 2, . . . , r) and the total performance score of topic j (j = 1, 2, . . . , c), where x ij is the performance score of system i observed on topic j. Then the performance expected under independence for system i on topic j is given by</p><formula xml:id="formula_12" coords="9,72.00,156.55,281.28,44.04">e ij = x i. • x j. x .. = x .i x .. • x .j where x .. = r i=1 c j=1</formula><p>x ij is the grand total. In consequence, system i is said to be independent from topic j, if the performance score of system i on topic j, x ij is equal to e ij , the performance suggested by the independence model. It immediately follows that, under independence, x i. , the sum of observed scores of system i over all topics is expected to distribute on topics, according to the proportion of the total performance scores of topics, x .j /x .. . This distribution is referred to as the performance profile expected under independence, and can be thought of as the ideal performance profile. In here, the proportion of the sum of performance scores observed on a particular topic to the sum of performance scores observed on all topics is the estimate of the population performance proportion of that topic, and hence, defines a conditional performance standard for every system. The independence notion can also be interpreted for topics, symmetrically. Under independence, x .j , the sum of observed scores on topic j over all systems is expected to distribute on systems, according to the proportion of the total performance scores of systems, x i. /x .. (i.e., the equation given in brackets).</p><p>To a certain extent, CA is similar to the PCA. But in fact, they are completely different techniques that can be used to visualize different aspects of the same set of data. Two systems that have different average performance scores can show the same performance profile across a given set of topics; so, opposed to the PCA plots, they can be located at the same position in the CA plots. Conversely, two systems that have the same average performance can show different performance profiles; so, they can be located apart from each other in the CA plots. Moreover, CA accounts for only the gains and the losses in a retrieval system's performance, relative to the performance expected from that system under independence (i.e., the difference of the observed performance profiles of retrieval systems from their performance profiles expected under independence); it does not account for the magnitudes of the performance scores. In this respect, a particular retrieval system that has a low average performance can gain (or lose) performance on some topics, more than the gains (or the losses) of other systems that have high average performance in general. As a result, CA is unique in that, it enables us to explore the interdependencies between the retrieval systems and the topics, by identifying the systems that response to some topics more than the response expected under independence, and simultaneously, the topics that are responded by some systems more than the response expected under independence.</p><p>The CA biplot of the IRRA runs and the adhoc topics is given in Figure <ref type="figure" coords="9,422.77,510.54,3.87,10.46" target="#fig_2">3</ref>. Origin of the given CA biplot represents the independence: it is the point of no divergence from the performance profile expected under independence. In other words, the retrieval systems and the topics that are located close to the origin are independent from topics and retrieval systems, respectively. In consequence, the systems that depend on some topics, and the topics that depend on some systems are located far from the origin. For the systems and the topics that are located far from the origin, systems with similar observed performance profiles are located close to each other in the plot. In the same way, topics with similar observed performance profiles are located close to each other, too. On the other hand, the direction of divergence from independence vary system from system, topic from topic. On a subset of topics, the observed performance scores of a particular system may be below the performance scores suggested by the independence model, while on the rest, they may be equal to or above the performance scores expected under independence. In CA biplots, a retrieval system, which departs from independence, is located close to the topics, on which its observed performance scores are above the performance scores expected under independence, while in contrast, it is located far from the topics, on which its performance scores are below the performance scores expected under independence.</p><p>It appears that each IRRA run has a different performance profile across the adhoc topics, and gains (or loses) performance on different topics. Along the first principle axis, "irra2a" and the other IRRA runs are contrasted, and this contrast seems to be related mostly to the contrast between two subsets of topics, which can be characterized by the topic 34 and the topic 8. Along the second dimension, it appears that only the "irra1a" and "irra3a" are contrasted, i.e., "irra2a" is located on the horizontal (dashed) line crossing the origin. The first principal axis explains about the 84%, and the second principal axis explains about the 16% of the total performance deviations of IRRA runs from independence. This suggests that the difference between the performance profiles of "irra2a" and the others is higher than the difference between the performance profiles of "irra1a" and "irra3a". Recall that "irra2a" has low average performance, relative to the other IRRA runs. However, when the topics that are located close to the "irra2a" are examined in detail, it is apparent that the performance scores observed on that topics are relatively lower than the performance scores observed on the contrasted topics, in magnitude. For example, on topic 44, the statAP scores of "irra1a", "irra2a" and "irra3a" are 0.0189, 0.0662, and 0.0173, respectively; similarly, on topic 10, they are 0.0497, 0.1604, and 0.0504. However, on topic 24, they are 0.4753, 0.0635, and 0.4753. In other words, the number of topics, on which "irra2a" gains performance, is relatively more than the number of topics, on which the other IRRA runs gains, but the effects of that topics on the average performance is relatively lower than the effects of others, because of the differences in magnitudes.</p><p>On the other hand, the given CA plot also reveals that the performance of "irra2a" depends on the topics in use, more than the performance of other IRRA runs: along the first principal axis, "irra2a" is more distant from the origin than "irra1a" and "irra3a". In this respect, "irra1a" seems to be the best run amongst the IRRA runs. Its performance shows a considerable divergence from the performance expected under independence, only on the topic 26, where the scores of "irra1a", "irra2a" and "irra3a" are 0.2352, 0.0527, and 0.1652, respectively. Although "irra1a" has also the best score at topic 12, it appears that this scores is an expected one for it, because topic 12 is located close to the origin. But this case is valid not only for the "irra1a", but also for the "irra2a" and "irra3a". The observed performance scores of "irra2a" and "irra3a" on topic 12 are also the expected performance scores from them under independence. On topic 12, the scores of "irra1a", "irra2a" and "irra3a" are 0.2729, 0.2180, and 0.2729, respectively.</p><p>As a result, the considered DFI formulae are good at different subsets (possibly, different types) of topics, or in other words, effectiveness of each DFI formula depends on a subset of topics, which is different from the subsets of topics that the effectiveness of other formulae depend on. The DFI formula used in "irra1a" is actually a derivation of the DFI formula used in "irra3a". If topic 26 is ignored, it can be said that the derivation used in "irra1a" is beneficial, because this derivation approaches the performance profile of "irra3a" to the performance profile expected under independence (i.e., the ideal performance profile). Performance of "irra1a" is, in this respect, more predictable than the performance of "irra3a". As a result, if a DFI formula has to be singled out, the DFI formula used in "irra1a" should be preferred to the other DFI formulae. depth increases, they spread towards different directions. The performances of "irra1a" and "irra3mqd" peak at 30 document as 0.2985 and 0.2767, while the performances of others consistently decrease. Up to the 100 document, the ranking of IRRA runs nearly remains unchanged. In general, these results agree with the adhoc results with respect to the relative performance ranking of IRRA runs. As seen, "irra3a" was not submitted to the MQ track. The major reason is that the MQ track is limited to 5 runs for each participating group. But the performance of "irra3mqa" can be estimated from the performance of "irra1mqa". "irra1a" and "irra3a" are known to be compatible in average performance; they only differ in performance profiles. If it was submitted, its performance should have been close to (or slightly higher at 30 and 50 documents than) the observed performance of "irra1mqa", as also suggested by the observed similarity in performance between the "irra1mqd" and "irra3mqd".</p><p>In detail, "irra1mqa" is above the median scores of 210 topics out of 310 (68%), while "irra2a" is above median for 147 topic (47%). "irra1mqa" has the best scores on 14 topics, while "irra2a" has the best scores on 16 topics; they coincide on only one topic (topic 20556). The statAP score of "irra1mqa" is zero for 18 topics, and the statAP score of "irra1mqa" is zero for 23 topics. On the other hand, for the IRRA runs that use the diversity strategy, "irra1mqd", "irra2mqd" and "irra3mqd" are above the median scores of 159 topics (51%), 123 topics (40%), and 176 topics (57%), respectively. They have the best statAP scores, respectively, for 9, 14, and 12 topics. The PCA biplot of the components scores of IRRA runs and the loadings of 310 topics on the first and the second principal components are given in Figure <ref type="figure" coords="12,103.95,303.27,3.87,10.46" target="#fig_3">5</ref>, which provides more information about the mutual performance relationships between them across 310 MQ topics. In general, this PCA plot agree with the PCA plot given in Figure <ref type="figure" coords="12,380.33,596.16,3.87,10.46" target="#fig_1">2</ref>: IRRA runs are scattered close to the median. In particular, the average performance of "irra1mqa" seems to be better than the other IRRA runs: it is the right most run along the first principal axis. The labeled topics are the topics that dominate the first principal component, and so, they are the topics that have the highest influence on the final performance ranking of IRRA runs; for example, topic 557 is the topic where the highest difference in performance between the "APBest" and the IRRA runs is observed, while in contrast, topic 183 is the topic where the lowest difference in performance between the "APBest" and the IRRA runs is observed. The CA biplot of the IRRA runs is given in Figure <ref type="figure" coords="12,298.12,679.84,3.87,10.46" target="#fig_4">6</ref>.</p><p>As seen in the given CA biplot, along the first principal axis, "irra1mqX" ("irra3mqX") and "irra2mqX" are contrasted. This contrast indicates that the DFI formulae used in "irra1" and "irra2" produce two different retrieval strategies that are sensitive to different subsets (types) of topics. Along the second principal axis, the strategies used in adhoc runs and diversity runs are contrasted, and again, this contrast suggests that these two strategies are also sensitive to different subsets of topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>IRRA group participated in the 2009 Web track adhoc task and the diversity task, as well as the Million Query (MQ) track. In this year, the major concern is to examine the effectiveness of a novel, nonparametric index term weighting method based on the divergence from independence (DFI). All submitted IRRA runs are of the content-only type: none of the information in Web pages, such as Meta information, Link information, etc., was distinguished and used. In addition, during searching, none of the common supplementary methods, such as parsing rules, phrase processing, query processing, query expansion, relevance feedback, thesaurus, WordNets, etc., was used. IRRA runs are pure, out-of-the-box DFI runs. In brief, three possible DFI formulae are considered, and used in IRRA runs, as the TF component of the well-known TF×IDF weighting scheme. "irra1" uses a derivation of the DFI formula given in Equation 1 and a new IDF formula developed on the basis of the DFI, as an alternative to the existing IDF formulae. "irra2" and "irra3" use the same IDF formula with "irra1", and they employ the DFI formulae given in Equation 2 and Equation 1, respectively. In summary, empirical results show that "irra1" and "irra3" are compatible in performance on the average, and better than "irra2".</p><p>In the adhoc task, "irra1" has the best statAP scores on topic 12 and 26. Similarly, "irra2" has the best scores on topic 34 and 45; "irra3" has the best score on topic 12. "irra1" are above the median on 24 topics; "irra2a" are above the median on 14 topics; "irra3a" are above the median on 26 topics. The results of the analysis of the performance profiles of IRRA runs reveal that, to a certain degree, performances of considered DFI formulae depend on topics. In other words, effectiveness of each DFI formula depends on the types topics in use. On the other hand, it can also be argued that the derivation used in "irra1" can be a remedy, in this respect. It can correct the performance profile of "irra3" and approach it to the ideal performance profile; thereby, the performance of "irra1" might be more predictable (or stable) than the performance of other IRRA runs.</p><p>The runs submitted to the diversity task are the runs that are submitted to the adhoc task, with only one difference. For the diversity task, the result sets are filtered, such that they consist of at most two Web pages from the same host (URL). Based on the α-nDCG@10 measure, "irra2" outperforms other IRRA runs, whereas the results based on IA-P@10 indicates that they are compatible in performance. The diversity task results suggest that "irra2a" is better than other IRRA runs in returning diverse results to the considered topics. The DFI formula given in Equation 2 is qualitatively different from the other DFI formulae. It can, therefore, satisfy diverse information needs of users. Together with the adhoc task results, diversity results actually suggest that a better index term weighting could be possible, if the two DFI formulae can somehow be fused in the same host system.</p><p>MQ track results agree with the adhoc results about the relative ranking of IRRA runs, but they differ in that, the average performance of IRRA runs might be underestimated in magnitude with insufficient number of topics. In MQ track, "irra1" is above the median for 210 topics out of 310 (68%), and "irra2" is above median for 147 topic (47%). "irra1" has the best scores on 14 topics (4.5%), and "irra2" has the best scores on 16 topics (5%): they coincide on only one topic (i.e., topic 20556). The number of topics used in the MQ track is approximately the 6 times of the number of topics used in the Web adhoc task. It is worth noting that, for the topics on which the IRRA runs have the best scores, the number of MQ topics are about the 7 times of the number of adhoc topics, e.g., "irra1" has the best scores on 2 adhoc topics and has the best scores on 14 MQ topics. The MQ results of the analysis of the performance profiles of IRRA runs agree with the adhoc results. Both indicate that "irra1" and "irra2" are two different retrieval strategies that are sensitive to different subsets (probably, different types) of topics.</p><p>In summary, the results of the TREC 2009 experiments verify that the independence notion can provide a simple, but powerful basis for weighting index terms, so that it promises a new direction in the IR research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,188.16,249.33,218.96,10.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The data matrix for a set of documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,72.00,287.76,451.29,10.46;7,72.00,299.72,451.28,10.46;7,72.00,311.67,330.16,10.46"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PCA biplot of IRRA adhoc runs and 49 TREC topics based on the statAP scores. Component scores of runs and the loadings of topics are standardized so that the component scores plot of runs and the component loadings plot of topics can be superimposed on one another.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,121.83,287.76,351.61,10.46"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CA biplot of IRRA runs and adhoc topics based on the statAP scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,72.00,562.23,451.28,10.46;12,72.00,574.18,43.28,10.46"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The PCA biplot of component scores of the IRRA MQ base runs, based on the statAP estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,111.61,311.22,372.06,10.46"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The CA biplot of the IRRA MQ base runs, based on the statAP estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,127.92,506.90,395.36,180.66"><head>Table 1 :</head><label>1</label><figDesc>. Estimated performance scores of IRRA adhoc runs based on statAP.</figDesc><table coords="5,148.45,528.91,298.37,158.64"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">statAP Estimates</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell>nDCG</cell><cell cols="2">R-Prec</cell><cell cols="2">Prec30</cell><cell>SD(AP)</cell></row><row><cell></cell><cell cols="2">irra1a</cell><cell>0.1530</cell><cell>0.2924</cell><cell cols="2">0.2399</cell><cell cols="2">0.3316</cell><cell>0.006855</cell></row><row><cell></cell><cell cols="2">irra2a</cell><cell>0.1100</cell><cell>0.2290</cell><cell cols="2">0.1798</cell><cell cols="2">0.3182</cell><cell>0.007478</cell></row><row><cell></cell><cell cols="2">irra3a</cell><cell>0.1557</cell><cell>0.2954</cell><cell cols="2">0.2467</cell><cell cols="2">0.3321</cell><cell>0.006851</cell></row><row><cell></cell><cell>best</cell><cell></cell><cell>0.4392</cell><cell>0.6215</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">median</cell><cell>0.1570</cell><cell>0.3016</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">worst</cell><cell>0.0042</cell><cell>0.0202</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">MTC Estimates</cell><cell></cell></row><row><cell>run</cell><cell>eMAP</cell><cell cols="2">eR-Prec</cell><cell>eP5</cell><cell>eP10</cell><cell></cell><cell>eP15</cell><cell></cell><cell>eP20</cell><cell>eP30</cell><cell>eP100</cell></row><row><cell>irra1a</cell><cell>0.0375</cell><cell cols="2">0.0968</cell><cell>0.2167</cell><cell>0.2780</cell><cell cols="2">0.3017</cell><cell cols="2">0.3122</cell><cell>0.3401</cell><cell>0.3366</cell></row><row><cell>irra2a</cell><cell>0.0274</cell><cell cols="2">0.0860</cell><cell>0.2811</cell><cell>0.2756</cell><cell cols="2">0.2865</cell><cell cols="2">0.2793</cell><cell>0.2846</cell><cell>0.2865</cell></row><row><cell>irra3a</cell><cell>0.0379</cell><cell cols="2">0.0971</cell><cell>0.2117</cell><cell>0.2810</cell><cell cols="2">0.3027</cell><cell cols="2">0.3197</cell><cell>0.3399</cell><cell>0.3420</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,131.20,707.92,332.89,10.46"><head>Table 2 :</head><label>2</label><figDesc>Estimated performance scores of IRRA adhoc runs based on MTC.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,87.24,738.29,172.68,8.37"><p>http://boston.lti.cs.cmu.edu/Data/clueweb09/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,87.24,747.80,318.71,8.37"><p>The set of stop words bundled in TERRIER + all numbers + number-word mixtures.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,87.24,719.42,436.14,8.37;3,72.00,728.88,451.39,8.37;3,72.00,738.34,451.40,8.37;3,72.00,747.80,379.73,8.37"><p>DFR is also qualified as a nonparametric model in the work of Amanti and van Rijsbergen. But they mentioned that, in IR, the term "nonparametric" has a different meaning than in statistics (personal contact). In IR, nonparametric means to "parameter-free" models, and parameter-free models are meant to be models that do not contain parameters that are learned from relevance feedback. On this account, DFI model is a nonparametric model in both sense.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,87.24,727.19,301.15,8.37"><p>For topic 20, none of the participating runs had returned any relevant document.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,87.24,740.96,263.00,8.37"><p>Interested readers may obtain the MATLAB M file for PCA by e-mail.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,87.24,724.48,267.40,8.37"><p>To the interested readers, I can send MATLAB M file for CA by e-mail.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="8,87.24,733.98,436.14,8.37;8,72.00,743.44,245.07,8.37"><p>Of course, provided that the topic set in use is a true representative of the topic population, i.e., only if the topic set is large enough and a proper random sample from the population.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>Index term weighting by <rs type="funder">DFI</rs> is developed under the project titled "<rs type="projectName">Design of A Statistical Information Retrieval System</rs>", and supported by <rs type="funder">TUBITAK</rs>, <rs type="funder">The Scientific and Technological Research Council of Turkey</rs>, with Project No:<rs type="grantNumber">107E192</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Fetpnkg">
					<orgName type="project" subtype="full">Design of A Statistical Information Retrieval System</orgName>
				</org>
				<org type="funding" xml:id="_TRTdCmn">
					<idno type="grant-number">107E192</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Diversity Task</head><p>The estimated performance scores of IRRA diversity runs are given in the Table <ref type="table" coords="11,425.84,106.46,3.87,10.46">3</ref>  Based on the α-nDCG@10 measure, it appears that "irra2d" outperforms other IRRA runs, whereas the results based on IA-P@10 indicates that they are compatible in performance. These results do not agree with the adhoc results in general. In particular, they differ about the performance of "irra2d". Recall that DFI formula used in "irra2d" is the DFI formula used in "irra2a". Therefore, these results actually indicate that "irra2a" is better than other IRRA runs in returning diverse results to the considered topics. This means that the DFI formula given in Equation 2 is qualitatively different from the other DFI formulae, in that it can satisfy more diverse information needs of users, than the other IRRA runs. Together with the adhoc results, diversity results suggest that, first, a better index term weighting formula could be possible, if the DFI formula used in "irra1a" is somehow fused with the DFI formula used in "irra2a". Second, the average performance of "irra2a" seems to be relatively unstable/uncertain; so, the performance ranking of IRRA runs could change. In this respect, MQ track results may provide more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Million Query Track</head><p>Performance summaries of the IRRA runs that were submitted to the MQ track are given in Figure <ref type="figure" coords="11,515.53,404.59,3.87,10.46">4</ref>. The given performance summaries are of the base type of evaluations over 310 topics, where judgment pools include the documents sampled from the IRRA runs. MQ results obtained over 310 (valid) topics suggest that the average performances of IRRA runs are higher than the corresponding average performances observed on 49 adhoc topics; for example, the average performance of "irra1mqa" over 310 topics is 0.1926, and approximately 25% higher than its average performance (0.1530) over 49 adhoc topics. The performance scores measured by the R-Prec measure agree with the MAP measure about the ranking of IRRA runs. On the other hand, average precision scores measured at 10 documents (i.e., P @10) indicates that all IRRA runs are almost compatible in performance, and centered around the statAP score of 0.2600. However, as the measurement</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,72.00,388.89,451.29,10.46;14,81.96,400.85,441.31,10.46;14,81.96,412.80,189.61,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,236.35,388.89,286.94,10.46;14,81.96,400.85,139.02,10.46">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582416</idno>
		<ptr target="http://doi.acm.org/10.1145/582415.582416" />
	</analytic>
	<monogr>
		<title level="j" coord="14,230.94,400.85,97.63,10.46">ACM Trans. Inf. Syst</title>
		<idno type="ISSN">1046-8188</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,432.72,316.69,10.46;14,388.69,431.66,3.70,7.32;14,393.10,432.72,130.17,10.46;14,81.96,444.68,35.98,10.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="14,144.26,432.72,148.83,10.46">Practical Nonparametric Statistics</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Conover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>3 r d edition</note>
</biblStruct>

<biblStruct coords="14,72.00,464.60,451.29,10.46;14,81.96,476.57,441.31,10.46;14,81.96,488.52,36.27,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,135.64,464.60,387.64,10.46;14,81.96,476.57,267.80,10.46">Correspondence analysis for the evaluation of the results of information retrieval experiments: Comparison of the performance profiles of IR systems</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Dinçer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,359.37,476.57,141.97,10.46">Journal of Information Retrieval</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="14,76.91,508.45,446.37,10.46;14,81.96,520.40,441.32,10.46;14,81.96,532.35,157.56,10.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,134.75,508.45,286.41,10.46">Statistical principal components analysis for retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dinçer</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.v58:4</idno>
		<ptr target="http://dx.doi.org/10.1002/asi.v58:4" />
	</analytic>
	<monogr>
		<title level="j" coord="14,430.44,508.45,92.84,10.46;14,81.96,520.40,235.15,10.46">Journal of the American Society for Information Science and Technology</title>
		<idno type="ISSN">1532-2882</idno>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="560" to="574" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,552.28,451.27,10.46;14,81.96,564.23,79.56,10.46" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,138.52,552.28,228.79,10.46">Theory and Applications of Correspondence Analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Greenacre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,584.16,451.28,10.46;14,81.96,596.11,441.31,10.46;14,81.96,608.07,68.63,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,131.71,584.16,391.57,10.46;14,81.96,596.11,169.12,10.46">A probabilistic approach to automatic keyword indexing. Part I: On the distribution of specialty words in a technical literature</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,258.85,596.11,244.30,10.46">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="197" to="216" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,628.00,451.29,10.46;14,81.96,639.95,412.31,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,128.47,628.00,394.82,10.46;14,81.96,639.95,67.33,10.46">A probabilistic approach to automatic keyword indexing. Part II: An algorithm for probabilistic indexing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,158.11,639.95,246.57,10.46">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="280" to="289" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,659.88,397.52,10.46" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,123.60,659.88,189.34,10.46">Exploratory and Multivariate Data Analysis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jambu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,679.80,451.28,10.46;14,81.96,691.76,151.06,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,131.10,679.80,337.17,10.46">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,478.00,679.80,45.27,10.46;14,81.96,691.76,63.70,10.46">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,72.00,711.68,451.27,10.46;14,81.96,723.64,441.33,10.46;14,81.96,735.59,54.00,10.46" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="14,343.59,711.68,179.69,10.46;14,81.96,723.64,251.58,10.46">Research directions in Terrier. Novatica/UPGRADE Special Issue on Web Information Access</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<editor>Ricardo Baeza-Yates et al.</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Invited Paper</note>
</biblStruct>

<biblStruct coords="15,72.00,88.08,451.28,10.46;15,81.96,100.03,441.31,10.46;15,81.96,111.99,441.33,10.46;15,81.96,123.94,45.97,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,210.94,88.08,312.34,10.46;15,81.96,100.03,75.80,10.46">Some simple approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,177.47,100.03,345.81,10.46;15,81.96,111.99,228.92,10.46">Proceedings of the Seventeenth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the Seventeenth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,143.86,451.29,10.46;15,81.96,155.82,441.31,10.46;15,81.96,167.77,249.42,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,316.54,143.86,202.45,10.46">Probabilistic models of indexing and searching</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,383.64,155.82,135.11,10.46">Information Retrieval Research</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Williams</surname></persName>
		</editor>
		<meeting><address><addrLine>Butterworths, Oxford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="35" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,72.00,187.70,451.28,10.46;15,81.96,199.66,220.42,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,200.67,187.70,243.93,10.46">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,471.19,187.70,52.09,10.46;15,81.96,199.66,120.98,10.46">Information Processing and Management</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
