<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.20,81.12,245.60,12.62;1,152.71,97.06,306.58,12.62">UMass Amherst and UT Austin @ The TREC 2009 Relevance Feedback Track</title>
				<funder ref="#_87fSTmQ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,87.65,137.16,126.23,10.52"><forename type="first">Marc-Allen</forename><surname>Cartright</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.33,137.16,76.03,10.52"><forename type="first">Jangwon</forename><surname>Seo</surname></persName>
							<email>jangwon@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.12,137.16,89.76,10.52"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
							<email>mlease@austin.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.20,81.12,245.60,12.62;1,152.71,97.06,306.58,12.62">UMass Amherst and UT Austin @ The TREC 2009 Relevance Feedback Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B08E7033B0A2095476DC2BAB1412A4AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new supervised method for estimating term-based retrieval models and apply it to weight expansion terms from relevance feedback. While previous work on supervised feedback [Cao et al., 2008] demonstrated significantly improved retrieval accuracy over standard unsupervised approaches <ref type="bibr" coords="1,194.63,318.91,58.88,7.86" target="#b8">[Lavrenko and</ref> Croft,  2001, Zhai and<ref type="bibr" coords="1,154.71,328.87,57.80,7.86" target="#b19">Lafferty, 2001]</ref>, feedback terms were assumed to be independent in order to reduce training time. In contrast, we adapt the AdaRank learning algorithm <ref type="bibr" coords="1,231.32,358.76,50.47,7.86;1,89.01,368.72,20.99,7.86" target="#b18">[Xu and Li, 2007]</ref> to simultaneously estimate parameterization of all feedback terms. While not evaluated here, the method can be more generally applied for joint estimation of both query and feedback terms. To apply our method to a large web collection, we also investigate use of sampling to reduce feature extraction time while maintaining robust learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Term-based models have a long and distinguished history in information retrieval, spanning vector-space <ref type="bibr" coords="1,162.23,509.31,131.92,9.57" target="#b15">[Salton and Buckley, 1987]</ref>, probabilistic <ref type="bibr" coords="1,133.60,522.86,119.19,9.57" target="#b16">[Sparck Jones et al., 2000]</ref>, and language modeling approaches <ref type="bibr" coords="1,211.58,536.41,87.22,9.57;1,72.00,549.96,18.59,9.57" target="#b14">[Ponte and Croft, 1998</ref>]. While such models are remabarkably expressive in the range of possible document rankings they can represent, their practical accuracy depends heavily on effective estimation. A wide variety of different term weighting schemes have been proposed over the years based on handtuning <ref type="bibr" coords="1,105.75,631.25,123.43,9.57" target="#b15">[Salton and Buckley, 1987]</ref>, unsupervised learning <ref type="bibr" coords="1,113.26,644.80,76.91,9.57" target="#b11">[Mei et al., 2007]</ref>, and supervised learning <ref type="bibr" coords="1,92.58,658.35,121.96,9.57" target="#b5">[Fuhr and Buckley, 1991</ref><ref type="bibr" coords="1,214.54,658.35,84.26,9.57;1,72.00,671.90,53.31,9.57" target="#b2">, Bendersky and Croft, 2008</ref><ref type="bibr" coords="1,125.31,671.90,95.64,9.57" target="#b9">, Lease et al., 2009</ref><ref type="bibr" coords="1,220.94,671.90,77.86,9.57;1,72.00,685.45,73.06,9.57" target="#b7">, Kumaran and Carvalho, 2009]</ref>. While previous work in query expansion has traditionally focused on unsupervised approaches <ref type="bibr" coords="1,170.93,712.55,67.89,9.57" target="#b8">[Lavrenko and</ref><ref type="bibr" coords="1,242.28,712.55,56.52,9.57;1,313.20,237.51,42.95,9.57">Croft, 2001, Zhai and</ref><ref type="bibr" coords="1,360.29,237.51,67.87,9.57" target="#b19">Lafferty, 2001]</ref>, recent work in supervised learning has also invesigated this scenario and shown that supervision can be beneficially applied here as well <ref type="bibr" coords="1,413.11,278.16,81.51,9.57" target="#b3">[Cao et al., 2008]</ref>. We describe a new such approach for supervised learning of expansion term weights.</p><p>Previous work in estimating term weights has generally relied on simplifying independence assumptions in order to achieve more tractable training. For example, <ref type="bibr" coords="1,490.36,359.86,49.65,9.57;1,313.20,373.41,82.33,9.57" target="#b2">Bendersky and Croft [2008]</ref> predict a "key concept" for each query and produce a weighted combination using classifier confidence of each independent prediction. <ref type="bibr" coords="1,373.18,414.06,82.38,9.57" target="#b3">Cao et al. [2008]</ref> similarly predict "good" terms and create a weighted combination in the same way. While <ref type="bibr" coords="1,453.63,441.16,86.36,9.57" target="#b9">Lease et al. [2009]</ref> leverage full parameterizations in estimating expected term weights (i.e. their training data), term weights are still predicted independently. <ref type="bibr" coords="1,313.20,495.35,106.68,9.57" target="#b10">Lin and Murray [2005]</ref> evaluate different combinations of expansion terms but assume a fixed weight of each term in the combination. In contrast, our learning procedure directly estimates full model parameterization over all terms.</p><p>As in <ref type="bibr" coords="1,352.44,563.51,83.50,9.57" target="#b9">Lease et al. [2009]</ref>, we adopt a model of indirect parameterization: we define some arbitrary feature space correlated with term weights and then generate term weights via a linear model over those features. In this study, we adopt the features proposed by <ref type="bibr" coords="1,462.16,631.25,77.84,9.57" target="#b3">Cao et al. [2008]</ref> and learn feature weights via an adaptation of the AdaRank algorithm <ref type="bibr" coords="1,448.90,658.35,86.46,9.57" target="#b18">[Xu and Li, 2007]</ref>. While AdaRank as proposed uses direct parameterization, we introduce a level of indirection in training: candidate parameterizations of the feature space are not used to rank documents but to generate term weights. Given those term weights, documents can then be ranked, and evaluation of this ranking leads to a new update of parameters. AdaRank is attractive in that it facilitates direct optimization of an arbitrary retrieval metric, thus avoiding metric divergence issues associated with minimizing discordant pairs <ref type="bibr" coords="2,141.39,170.35,79.54,9.57" target="#b6">[Joachims, 2002]</ref> or other surrogate metrics. While <ref type="bibr" coords="2,179.40,183.90,89.86,9.57" target="#b9">Lease et al. [2009]</ref>'s use of efficient regression enables fast iteration in feature design and scalability to a growing feature space, such regression minimizes squared error as a surrogate for retrieval accuracy. We avoid metric divergence entirely while maintaining tractable computation.</p><p>Per our participation in the Relevance Feedback track at the 2009 Text REtreival Conference (TREC)<ref type="foot" coords="2,137.39,303.89,4.23,6.99" target="#foot_0">1</ref> , we evaluated our approach on the newly crawled ClueWeb09 Dataset<ref type="foot" coords="2,253.89,317.44,4.23,6.99" target="#foot_1">2</ref> . In particular, we use the "Category B" data which includes over 425 million unique URLs and 30GB of uncompressed text. We encountered two primary challenges in applying our approach to this collection: 1) performing supervised learning without existing relevance judgements, and 2) achieving tractable feature extraction (since some features drew on novel collection-wide statistics). For model training, parameters were estimated on the smaller wt10g web collection and then ported to the larger ClueWeb09; a similar approach was applied in early TREC Terabyte Tracks <ref type="bibr" coords="2,138.53,495.53,96.38,9.57" target="#b13">[Metzler et al., 2006</ref>] before relevance judgements were available for the GOV2 collection<ref type="foot" coords="2,116.85,520.68,4.23,6.99" target="#foot_2">3</ref> . To accelerate feature extraction, we investigated strategies for subsampling the collection while preserving collection properties in the sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model, Features, and Sampling</head><p>This section describes the model and features used in our work. As in <ref type="bibr" coords="2,204.31,619.28,90.08,9.57" target="#b9">Lease et al. [2009]</ref>, there are in fact two distinct models and sets of features we apply which must be distinguished. To rank documents, we adopt standard unigram language modeling <ref type="bibr" coords="2,169.30,673.47,111.61,9.57" target="#b14">[Ponte and Croft, 1998]</ref>, in which the feature space consists of the term vocabulary and the model is parameterized by the term weights. To generate these term weights, however, another model is used: a linear model defined over an arbitrary feature space. It is this model and features to which we will focus subsequent discussion. Since a goal of our study is to compare our new estimation method to that used in previous work <ref type="bibr" coords="2,441.82,183.90,77.93,9.57" target="#b3">[Cao et al., 2008]</ref>, we adopt the same feature set used in that earlier study with only minimal differences as noted.</p><p>Cao et al. define five feature templates that are each instantiated on 1) the feedback documents and 2) the entire collection. For example, the first feature template yields φ 0 over the feedback set of documents F and φ 1 over the entire collection C. For the first three feature templates, "term distributions", "co-occurrence with single query term", and "co-occurrence with pairs query terms" we used them exactly as originally defined <ref type="bibr" coords="2,400.34,346.61,81.63,9.57" target="#b3">[Cao et al., 2008]</ref>. We revise the final two feature templates as follows. We denote an expansion term by e and the query by Q = q 0 q 1 . . . q k . Each feature template is defined in terms of F ; the corresponding odd-numbered feature is similarly defined over C.</p><p>Weighted Term Proximity (φ 6 (e)) While Cao et al. used minimum distance as the distance function, we instead use average weighted distance from the expansion term to any of the co-occurring query terms. log q i ∈Q D∈F C W (q i , e|D) * dist(q i , e|D)</p><formula xml:id="formula_0" coords="2,386.12,523.21,108.38,13.05">q i ∈Q D∈F C w (q i , e|D)</formula><p>where dist(q i , e|D) is the average number of terms between q i and e in D.</p><p>Document Frequency for query terms and the expansion term together (φ 8 (e)) This feature provides information about the frequency of the expansion term occurring with the entire query in the set of the documents. As in <ref type="bibr" coords="2,327.07,644.80,83.02,9.57" target="#b3">[Cao et al., 2008]</ref>, 0.5 is used as a smoothing factor. The corresponding feature in <ref type="bibr" coords="2,518.18,658.35,21.82,9.57;2,313.20,671.90,54.80,9.57" target="#b3">[Cao et al., 2008]</ref> appears to use the floor function after adding the smoothing factor, which seems to obviate the purpose of smoothing. Our interpretation is to instead apply the floor after the log operation, effectively binning the feature values.</p><formula xml:id="formula_1" coords="3,76.85,100.48,225.07,10.63">log (|{D ∈ F | ∀q i ∈ Q : q i ∈ D ∧ e ∈ D}| + 0.5)</formula><p>One challenge in applying Cao et al.'s method to very large document collections like ClueWeb09 is that feature extraction for collection-wide statistics can be quite slow. To accelerate this process, we employ uniform sampling to approximate collection statistics. Sample size was selected to be roughly the size of the wt10g collection to support efficienct collection while maintaining robust statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Estimation</head><p>Our work on supervised estimation of feedback term weights was inspired by <ref type="bibr" coords="3,216.70,292.37,82.10,9.57;3,72.00,305.92,26.49,9.57">Cao et al.'s work [2008]</ref>, and we begin this section by reviewing their approach. Following this we describe our approach, and how we adapted the AdaRank algorithm <ref type="bibr" coords="3,123.38,346.57,88.45,9.57" target="#b18">[Xu and Li, 2007]</ref> for this purpose. We also discuss technical challenges encountered and our strategies for addressing them.</p><p>Cao et al. distinguish three types of expansion terms: good, neutral, and bad. These categories are defined by the impact each has on retrieval accuracy when its terms are used to expand the query. Expanding by good terms improves retrieval accuracy, expanding by neutral terms has no effect, and expanding by bad terms hurts accuracy. To label each term's category, it is added to the original query with a small fixed weight, and then the query is run and evaluated. By following this process, training data is created for learning a supervised binary classifier (neutral and bad terms are combined into one group). Expansion is then performed by independently predicting whether or not each term is good, then weighting that term by the classifier's confidence of its goodness. When integrated into the new query via either relevance modeling <ref type="bibr" coords="3,118.13,631.25,125.88,9.57" target="#b8">[Lavrenko and Croft, 2001]</ref> or the mixture model feedback <ref type="bibr" coords="3,174.87,644.80,119.29,9.57" target="#b19">[Zhai and Lafferty, 2001]</ref>, superior accuracy is achieved in comparison to what either achieves using unsupervised estimation. Note that: 1) the goodness of each term in their study is determined independently, 2) this is based on a single trial with a fixed weight, and 3) their learning procedure maximizes classification accuracy rather than retrieval accuracy.</p><p>Inspired by their success, we investigate an alternative learning procedure by which retrieval accuracy is directly maximized and the interaction between feedback terms is directly modeled. As in <ref type="bibr" coords="3,340.54,156.80,82.01,9.57" target="#b9">Lease et al. [2009]</ref>, term weights are generated by a linear model defined over an arbitrary feature space. Given a candidate parameterization of feature weights, our meta-training algorithm is as follows:</p><p>1. generate term weights 2. perform retrieval 3. measure accuracy 4. update feature weights based on retrieval accuracy A variety of learning algorithms could be used with this scheme, such as simulated annealing, genetic algorithm, etc. Since each iteration involves running the query, which can be computaionally expensive as query and collection sizes increase, we desire an efficient learning algorithm minimizing the number of such iterations required. Learning to rank algorithms designed for ranking problems are particularly suitable. However, pairwise preference-based learning to rank algorithms are less desirable because we need term-based models rather than document-based models. For example, while <ref type="bibr" coords="3,313.20,509.31,145.11,9.57" target="#b7">[Kumaran and Carvalho, 2009]</ref> addressed supervised term selection using a pairwise preferencebased learning to rank algorithm, they used much smaller number of terms to select compared to our case. We chose AdaRank <ref type="bibr" coords="3,420.62,577.06,83.73,9.57" target="#b18">[Xu and Li, 2007]</ref> for the following reasons. It directly optimizes retrieval performance metrics, thus avoiding metric divergence. This allows document ranking to be performed via a traditional term-based retrieval model. AdaRank's simplicity also lends itself easily to customization for our particular training setting. While AdaRank was designed for parameter estimation in learning to rank models, our learning scenario instead introduces a new wrinkle via adding a layer of indirection:</p><formula xml:id="formula_2" coords="4,131.51,72.46,105.70,12.75">Input: S = {(q i , e i )} m</formula><p>i=1 where e i is a set of expanded terms for query q i Initialize P 1</p><formula xml:id="formula_3" coords="4,131.51,87.98,348.97,91.78">(i) = 1/m For t = 1 : T -Create weak expander h t = h ĵ by ĵ = max j m i=1 P t (i)E(R(h j (q i , e i ))) -Choose α t by α t = 1 2 ln P m i=1 Pt(i)[1+E(R(ht(q i ,e i )))] P m i=1 Pt(i)[1-E(R(ht(q i ,e i )))] -Create expander f t by f t = t k=1 α k h k -Update P t+1 by P t+1 (i) = exp[-E(R(ft(q i ,e i )))] P m j=1 exp[-E(R(ft(q j ,e j )))]</formula><p>End For Output expander model: One point of note is that AdaRank assumes that features positively correlate with retrieval accuracy and can only learn positive weights. However, one of our important features is actually negatively correlated, modeling discrepancy between feedback-set features and collection features. Consequently, we use (1 -φ) instead of φ for the collection features.</p><formula xml:id="formula_4" coords="4,72.00,195.35,192.24,32.57">f T Figure 1: AdaRankT Algorithm. E</formula><p>A practical challenge we encountered with AdaRank is as follows. Imagine AdaRank picks up a weak ranker based on feature φ i . AdaRank decreases the weight of queries for which the selected weak ranker shows good performance. In other words, training weight distribution P is updated based on the search results by the weak ranker. Ideally, at the next round, AdaRank can be expected to select one of weak rankers based on a feature other than φ i . However, φ i can be dominant enough to be selected again. Then, the ranking model still depends on only a single weak ranker and the same results are returned. Therefore, P is not updated and AdaRank cannot choose other weak rankers forever.</p><p>Since AdaRankT follows AdaRank except for some modifications, we cannot avoid this problem. Instead, we tweak the algorithm as follows:</p><p>1. If the same feature is selected for the first two rounds, enqueue the strong feature and learn with only weak features.</p><p>2. Repeat (1) until any dominant feature does not appear 3. Dequeue and add back a removed feature to the remaining weak feature set. Then, learn with the set and preset the learned model into an initial model for the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Repeat (3) until all features are added back</head><p>In our experiments, feature φ 1 was the dominant feature and always selected for the first two rounds. After the tweak, as a final model, we recovered meaningful feature weights for four features. φ 1 remained the most dominant while φ 2 , φ 3 , and φ 9 provided some significant contribution. This learned model achieved better performance compared to using only φ 1 . Although previous work did not indicate the relative importance of the various features used <ref type="bibr" coords="5,276.99,407.09,21.82,9.57;5,72.00,420.64,52.52,9.57" target="#b3">[Cao et al., 2008]</ref>, we were surprised to find only a few of the features extracted were actually assigned any weight during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Relevance Feedback Track</head><p>Our study was conducted in the context of the 2009 TREC Relevance Feedback (RF) Track, which explores the interaction of different feedback document selection strategies with different algorithms for incorporating such feedback. The RF Track involved two distinct phases. In Phase 1, participating teams identified five documents per query to be assessed by NIST for relevance. If teams had multiple selection strategies, up to two sets of five documents per query could be submitted. In Phase 2, teams evaluated retrieval accuracy under their respective systems using different feedback document sets.</p><p>For Phase 1, we ranked documents according to two baseline systems:</p><p>• A run based on the Query Likelihood (QL)</p><p>unigram model <ref type="bibr" coords="5,169.30,712.55,112.01,9.57" target="#b14">[Ponte and Croft, 1998]</ref> • A run based on the Markov Random Field Model (MRF) <ref type="bibr" coords="5,409.90,89.06,125.46,9.57" target="#b12">[Metzler and Croft, 2005]</ref>.</p><p>Weights for term, ordered, and unordered components were set to 0.80, .015, and 0.05, respectively. Due to time constraints, we did not have time to tune the weights.</p><p>To select documents for assessment, we used the following algorithm to select documents which 1) exhibited large disagreement between the two runs and 2) are less certain of being relevant and thus more likely to provide useful information via assessment.</p><p>Suppose we have ranked lists A and B.</p><p>1. Choose up to 5 best ranked documents in list B such that each document d fulfills the following criteria:</p><formula xml:id="formula_5" coords="5,321.55,315.72,189.18,47.37">• d does not appear in A, or • d's rank in A is worse than in B 2.</formula><p>If there are less than 5 documents found using the previous step, fill the remaining space with any document d from B that fulfills the following criteria:</p><p>• d's rank is worse than rank 5, and • d has equal rank in both A and B 3. Finally, if there are still less than 5 documents found using the previous steps, fill the remaining space with any document d such that:</p><p>• d's rank is worse than rank 5, and • d is not already in the list Using this algorithm, we submitted two sets of feedback documents for Phase 1. The UMas.1 feedback set was produced with list A coming from the QL run and B from the MRF run. The UMas.2 feedback set used the MRF run for list A and the QL run for list B.</p><p>For phase 2, we employed our supervised learning method given different input sets of feedback documents produced by RF track participants in Phase 1. We used only those documents assessed as relevant; we leave weighting of feedback terms from non-relevant documents for future work.</p><p>As mentioned earlier, since the ClueWeb09 collection used in the RF track does not have existing relevance judgements, we train our model on the wt10g collection instead and port the learned parameters to perform retrieval on ClueWeb09. As such, one source of potential error in our experiments is mismatch between train and test collections. The wt10g collection contains 1.7 million pages and 140,470 relevance judgments for 150 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>This section presents results and analysis of our retrieval experiments. Retrieval accuracy on the ClueWeb09 (Category B) collection was evaluated for eight runs: a baseline run (the MRF run used in Phase 1 with additional pseudo-relevance feedback performed <ref type="bibr" coords="6,254.98,316.92,43.81,9.57;6,72.00,330.47,76.15,9.57" target="#b8">Lavrenko and Croft [2001]</ref>) and seven feedback runs based on different feedback sets from the track participants. For each feedback set, we performed query expansion using our learned AdaRankT model and then ran a mixture of MRF and expanded queries to be comparable to the base run. Table <ref type="table" coords="6,126.06,411.77,5.45,9.57" target="#tab_1">1</ref> shows the 7 assigned feedback sets and the number of topics which include positive feedback, i.e. feedback sets containing at least one relevant document. For topics with no relevant document, the baseline ranking was used.</p><p>We performed retrieval experiments using Indri <ref type="bibr" coords="6,89.64,494.41,109.44,9.57" target="#b17">[Strohman et al., 2005]</ref>. Figure <ref type="figure" coords="6,245.58,494.41,5.45,9.57">3</ref> shows an example expanded query expressed in the Indri query language. Table <ref type="table" coords="6,200.78,521.51,5.45,9.57" target="#tab_3">3</ref> shows retrieval accuracy achieved with each of the seven sets of feedback documents. To provide some intuition as to what the expanded queries look like, Table <ref type="table" coords="6,90.89,575.71,5.45,9.57" target="#tab_2">2</ref> shows examples of the top ten weighted expansion terms selected by our method when using the ilps.1 feedback set. Expansion sets generally appear to be semantically cohesive.</p><p>A question that arose during analysis is whether having a feedback set with more relevant documents (or documents considered to be "more relevant" would correlate with an improvement in performance. Figure <ref type="figure" coords="6,242.22,685.45,5.45,9.57" target="#fig_0">2</ref> shows the results of our analysis, where the topics are ordered by increasing correlation coefficient. The first two sets of correlation coefficients (marked by "gr") use the graded relevance scores, therefore a feedback set can score as high as 10 (5 documents at relevance level 2). The second two sets of coefficients use flat relevance scores, meaning feedback set scores could only go as high as 5. All combinations of evaluation metric and relevance scoring behave similarly, having a fairly even spread across the spectrum of possible values. Nevertheless, when using the graded relevance which emphasizes the quality of feedback documents, more topics have positive correlation coefficients. Results suggest that at low sample sizes (i.e. 5 or less), our feedback method relies heavily on the quality of the feedback documents and less on their quantity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Efficiency Analysis</head><p>Distributing the Indri index over a cluster of 10 CPUs, our longest run took approximately 3 hours.</p><p>Table <ref type="table" coords="6,407.96,344.58,5.45,9.57" target="#tab_5">5</ref> shows the average run time when generating each feature from the ClueWeb English 1 collection. Note that while the feedback set is several orders of magnitude smaller than the full collection, some of the collection-scale features were on average generated faster than their feedback set counterparts. This is due to the availability of collectionlevel statistics for each term provided by Indri <ref type="bibr" coords="6,329.91,466.52,106.67,9.57" target="#b17">[Strohman et al., 2005]</ref>, whereas some of the feedback set statistics (e.g. number of occurrences of a term in a particular set of documents) had to be calculated on the fly. The generation times for φ 8 and φ 9 where significantly larger than the other features. This was due to our implementation effectively performing a set intersection of the posting lists of the different terms. While this may have been less efficient for the feedback-scale calculation, it was the fastest way we were aware of to calculate the feature on the whole collection. Some of our implementation could be optimized or effectively approximated, however we leave this task to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Our Phase 1 runs involved only the Query Likelihood Model and MRF Model as source runs. We wanted to perform a more thorough   <ref type="bibr" coords="8,242.73,406.72,113.26,9.57" target="#b4">[Carterette et al., 2006]</ref> and StatAP <ref type="bibr" coords="8,421.75,406.72,118.25,9.57" target="#b1">[Aslam and Pavlu, 2007]</ref> scores achieved by various participant runs using our selected set of Phase 1 feedback documents.</p><p>A two-tailed t-test was used to test for statistically significant differences between all pairs of runs for both metrics. Only two differences were significant, both for the eMAP metric only: YUIR.  <ref type="bibr" coords="9,109.22,466.95,122.76,9.57" target="#b8">Lavrenko and Croft [2001]</ref>, and possibly using the MRF Model, followed by expansion using Relevance Models. The motivation behind using multiple models when searching for feedback documents would be to help characterize the query -a search engine may want to treat a query differently if two models that emphasize different aspects of a query (such as querylikelihood and MRF) return different result lists, versus another query that has a smaller perturbation between the two methods. We considered only positive feedback instances for now. However, this may not achieve full potential from relevance feedback because documents which are classified as irrelevant can provide more clear guidelines to distinguish bad terms from good terms or neutral terms. Therefore, in future we also plan to investigate effective ways of integrating negative feedback in-stances in our framework.</p><p>Although our experiments seemed to favor a subset of the entire set of features available, we agree with previous work that expansion terms require a richer representation for proper selection. We would like to explore other features that may help discriminate the useful set of feedback terms from the neutral or hurtful terms contained the the feedback set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,72.00,500.61,468.00,9.57;7,72.00,514.15,468.00,9.57;7,72.00,527.70,468.00,9.57;7,118.80,110.12,374.42,374.42"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Correlation coefficients between number of feedback documents in a set and AdaRankT's performance based on that feedback set, ordered by increasing coefficient. When computing the coefficient for each topic, feedback sets which do not contain any relevant document were excluded.</figDesc><graphic coords="7,118.80,110.12,374.42,374.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,218.35,468.00,503.77"><head></head><label></label><figDesc>is a retrieval evaluation measure function and R is a retrieval algorithm, e.g., the query-likelihood or Markov Random Field (MRF) model.</figDesc><table coords="4,72.00,265.51,226.81,420.51"><row><cell>parameters are not used to rank documents but</cell></row><row><cell>to generate term weights which are themselves</cell></row><row><cell>used to rank documents. Consequently, we re-</cell></row><row><cell>vise the training algorithm as illustrated in Fig-</cell></row><row><cell>ure 1 and refer to this procedure as AdaRankT</cell></row><row><cell>(i.e., for term-based modeling).</cell></row><row><cell>AdaRankT can be distinguished from</cell></row><row><cell>AdaRank as follows:</cell></row><row><cell>1. Our weak expander h k generates expanded</cell></row><row><cell>queries using term-based feature φ k . That</cell></row><row><cell>is, we construct a structured query with ex-</cell></row><row><cell>panded term candidates using the feature</cell></row><row><cell>values as their term weights as shown in</cell></row><row><cell>Figure 3. However, since using all term can-</cell></row><row><cell>didates for a query sounds infeasible, we</cell></row><row><cell>select only top M terms of all candidates</cell></row><row><cell>by the feature values. In this work, we set</cell></row><row><cell>M = 100. This expander is evaulated by</cell></row><row><cell>retrieval results of running a conventional</cell></row><row><cell>retrieval algorithm (e.g., query likelihood</cell></row><row><cell>model or markov random field model) with</cell></row><row><cell>the expanded query.</cell></row><row><cell>2. For weak expander selection, we see which</cell></row><row><cell>weak expander performs best under wieght</cell></row><row><cell>distribution P t-1 by running all expanded</cell></row><row><cell>queries based on each term-based features.</cell></row><row><cell>Since this expensive process is repeated ev-</cell></row><row><cell>ery round, it can save time to keep search</cell></row><row><cell>results by each weak ranker before running</cell></row><row><cell>the AdaRankT algorithm.</cell></row></table><note coords="4,80.35,699.00,218.45,9.57;4,93.82,712.55,204.99,9.57;4,335.02,265.51,204.98,9.57;4,335.02,279.06,204.98,9.57;4,335.02,292.61,204.48,10.63;4,335.02,306.15,204.98,9.57;4,335.02,319.70,204.98,10.63;4,335.02,333.25,70.66,9.57;4,321.55,354.49,218.44,9.57;4,335.02,368.04,204.99,9.57;4,335.02,381.59,204.98,9.57;4,335.02,395.14,173.27,9.57"><p>3. We do not update a ranking model that is a linear model of weak rankers but a term weight model which is a linear model of term-based features. We compute a weight α t based on a result by a weak expander h t in the same manner as AdaRank. However, α t plays a role of weights for term-based feature k used. 4. According to the above modifications, training data weight P is updated based on ranking results by queries expanded by linear term weight model f at round t.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,72.00,623.93,468.00,60.06"><head>Table 1 :</head><label>1</label><figDesc>Our assigned feedback sets and the number of topics containing at least one relevant document.</figDesc><table coords="7,79.82,623.93,452.36,23.51"><row><cell>feedback</cell><cell cols="7">ilps.1 PRIS.1 UCSC.2 ugTr.1 UMas.1 UMas.2 YUIR.2</cell></row><row><cell>#topics with pos. feedback</cell><cell>31</cell><cell>33</cell><cell>39</cell><cell>32</cell><cell>23</cell><cell>26</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,263.58,468.00,129.11"><head>Table 2 :</head><label>2</label><figDesc>Top 10 expanded terms generated by our supervised term weighting model. The query terms in expanded queries are excluded.</figDesc><table coords="8,81.15,355.62,449.70,37.06"><row><cell cols="8">Metric PRIS.1 UCSC.2 UMas.1 UMas.2 YUIR.2 ilps.1 ugTr.1 Min</cell><cell>Max Avg</cell></row><row><cell>eMAP</cell><cell>.0490</cell><cell>.0477</cell><cell>.0493</cell><cell>.0478</cell><cell>.0500</cell><cell>.0500 .0468</cell><cell cols="2">.0468 .0500 .0486</cell></row><row><cell cols="2">StatAP .2249</cell><cell>.2279</cell><cell>.2294</cell><cell>.2197</cell><cell>.2236</cell><cell>.2311 .2175</cell><cell cols="2">.2175 .2311 .2249</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,72.00,406.72,166.53,9.57"><head>Table 3 :</head><label>3</label><figDesc>Expected MAP (eMAP)  </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,72.00,447.37,468.00,238.38"><head>Table 4 :</head><label>4</label><figDesc>Each set of feedback documents was used in multiple particpant runs. In comparison to other feedback sets used by those participants, how often did the given feedback set yield better or worse performance? The score is defined over both metrics as the number of better runs divided the total number of runs. With UMas.2 feedback documents, exactly half the runs did better for each metric. Runs using UMas.1 feedback documents performed slightly better than average, with better StatAP accuracy compensating for slightly worse eMAP accuracy.#weight( 0.5 #weight(0.8 #combine(air travel information) 0.15 #combine(#1(air travel) #1(travel information) ) 0.05 #combine(#uw8(air travel) #uw8(travel information) ) ) 0.5 #weight( 0.697082 accommodate 0.684322 caribbean 0.690662 cruise 0.686319 destinate 0.690842 fly 0.700626 lease 0.689636 premier 0.690763 resort 0.689994 route 0.702285 safe 0.696152 schedule 0.690543 tourism 0.686859 transportation 0.687628 vacation 0.691785 wine ..... ) )Figure3: Example of an expanded query using Indri query language</figDesc><table coords="8,533.87,447.37,6.14,9.57"><row><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,72.00,403.42,226.80,73.10"><head>Table 5 :</head><label>5</label><figDesc>Average run times in feature extraction comparison of the results returned by performing pseudo-relevance feedback using Relevance Models</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.59,692.58,78.15,7.86"><p>http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.59,703.23,183.27,7.86"><p>http://boston.lti.cs.cmu.edu/Data/clueweb09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,88.59,713.88,152.99,7.86"><p>http://ir.dcs.gla.ac.uk/test collections</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantName">PIRE Grant</rs> No <rs type="grantNumber">OISE-0530118</rs> and the <rs type="institution">Center for Intelligent Information Retrieval</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_87fSTmQ">
					<idno type="grant-number">OISE-0530118</idno>
					<orgName type="grant-name">PIRE Grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,313.20,467.76,226.80,7.86;9,324.11,477.72,215.89,7.86;9,324.11,487.68,215.89,7.86;9,324.11,497.64,215.89,7.86;9,324.11,507.61,215.89,7.86;9,324.11,517.57,58.87,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,349.58,477.72,190.42,7.86;9,324.11,487.68,118.81,7.86">Document selection methodologies for efficient and effective learning-to-rank</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Savev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,466.27,487.68,73.74,7.86;9,324.11,497.64,215.89,7.86;9,324.11,507.61,187.93,7.86">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,531.78,226.80,7.86;9,324.11,541.74,215.89,7.86;9,324.11,551.70,122.79,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,429.29,531.78,110.71,7.86;9,324.11,541.74,139.95,7.86">A practical sampling strategy for efficient retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Northeastern University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="9,313.20,565.91,226.80,7.86;9,324.11,575.87,215.89,7.86;9,324.11,585.83,215.89,7.86;9,324.11,595.80,215.89,7.86;9,324.11,605.76,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,438.22,565.91,101.78,7.86;9,324.11,575.87,71.41,7.86">Discovering key concepts in verbose queries</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,416.60,575.87,123.40,7.86;9,324.11,585.83,215.89,7.86;9,324.11,595.80,148.00,7.86">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,619.97,226.80,7.86;9,324.11,629.93,215.89,7.86;9,324.11,639.89,215.89,7.86;9,324.11,649.85,215.89,7.86;9,324.11,659.82,215.89,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,504.16,619.97,35.85,7.86;9,324.11,629.93,211.77,7.86">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,335.37,639.89,204.63,7.86;9,324.11,649.85,215.89,7.86;9,324.11,659.82,125.92,7.86">SIGIR &apos;08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,674.02,226.80,7.86;9,324.11,683.99,215.89,7.86;9,324.11,693.95,215.89,7.86;9,324.11,703.91,215.89,7.86;9,324.11,713.88,84.02,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,488.52,674.02,51.48,7.86;9,324.11,683.99,136.52,7.86">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,482.72,683.99,57.27,7.86;9,324.11,693.95,215.89,7.86;9,324.11,703.91,212.34,7.86">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,76.83,226.80,7.86;10,82.91,86.80,215.89,7.86;10,82.91,96.76,165.45,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,183.90,76.83,114.90,7.86;10,82.91,86.80,118.80,7.86">A probabilistic learning approach for document indexing</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,209.68,86.80,89.11,7.86;10,82.91,96.76,80.42,7.86">ACM Transactions on Information systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="245" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,110.83,226.80,7.86;10,82.91,120.79,215.89,7.86;10,82.91,130.75,215.89,7.86;10,82.91,140.71,184.33,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,136.48,110.83,162.32,7.86;10,82.91,120.79,53.67,7.86">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.02,120.79,132.78,7.86;10,82.91,130.75,215.89,7.86;10,82.91,140.71,92.91,7.86">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,154.78,226.80,7.86;10,82.91,164.74,215.89,7.86;10,82.91,174.71,215.89,7.86;10,82.91,184.67,215.89,7.86;10,82.91,194.63,58.87,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,207.78,154.78,91.03,7.86;10,82.91,164.74,121.13,7.86">Reducing long queries using query quality predictors</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,225.77,164.74,73.03,7.86;10,82.91,174.71,215.89,7.86;10,82.91,184.67,187.93,7.86">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,208.70,226.80,7.86;10,82.91,218.66,215.89,7.86;10,82.91,228.62,215.89,7.86;10,82.91,238.59,215.89,7.86;10,82.91,248.55,215.89,7.86;10,82.91,258.51,205.56,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,195.58,208.70,103.22,7.86;10,82.91,218.66,26.15,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.383972</idno>
		<ptr target="http://doi.acm.org/10.1145/383952.383972" />
	</analytic>
	<monogr>
		<title level="m" coord="10,128.44,218.66,170.36,7.86;10,82.91,228.62,215.89,7.86;10,82.91,238.59,148.00,7.86">SIGIR &apos;01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,272.58,226.80,7.86;10,82.91,282.54,215.89,7.86;10,82.91,292.50,215.89,7.86;10,82.91,302.47,215.89,7.86;10,82.91,312.43,172.22,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,256.05,272.58,42.74,7.86;10,82.91,282.54,215.89,7.86;10,82.91,292.50,45.95,7.86">Regression rank: Learning to meet the opportunity of descriptive queries</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,153.48,292.50,145.32,7.86;10,82.91,302.47,215.89,7.86;10,82.91,312.43,85.91,7.86">ECIR &apos;09: Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="90" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,326.49,226.80,7.86;10,82.91,336.46,215.89,7.86;10,82.91,346.42,215.89,7.86;10,82.91,356.38,215.89,7.86;10,82.91,366.34,120.26,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,165.83,326.49,132.97,7.86;10,82.91,336.46,152.45,7.86">Assessing the term independence assumption in blind relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.92,336.46,45.88,7.86;10,82.91,346.42,215.89,7.86;10,82.91,356.38,215.89,7.86;10,82.91,366.34,24.37,7.86">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">636</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,380.41,226.80,7.86;10,82.91,390.37,215.89,7.86;10,82.91,400.34,215.89,7.86;10,82.91,410.30,215.89,7.86;10,82.91,420.26,141.64,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,197.46,380.41,101.34,7.86;10,82.91,390.37,173.84,7.86">A study of Poisson query generation model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,281.15,390.37,17.65,7.86;10,82.91,400.34,215.89,7.86;10,82.91,410.30,215.89,7.86;10,82.91,420.26,51.00,7.86">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,434.33,226.80,7.86;10,82.91,444.29,215.89,7.86;10,82.91,454.25,215.89,7.86;10,82.91,464.22,215.89,7.86;10,82.91,474.18,215.89,7.86;10,82.91,484.14,215.89,7.86;10,82.91,494.10,164.87,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,201.50,434.33,97.30,7.86;10,82.91,444.29,119.11,7.86">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/1076034.1076115</idno>
		<ptr target="http://doi.acm.org/10.1145/1076034.1076115" />
	</analytic>
	<monogr>
		<title level="m" coord="10,228.13,444.29,70.68,7.86;10,82.91,454.25,215.89,7.86;10,82.91,464.22,215.89,7.86;10,82.91,474.18,75.96,7.86">SIGIR &apos;05: Proceedings of the 28th annual international ACM SI-GIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,508.17,226.80,7.86;10,82.91,518.13,215.89,7.86;10,82.91,528.10,215.89,7.86;10,82.91,538.06,20.99,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,279.07,508.17,19.73,7.86;10,82.91,518.13,124.93,7.86">Indri at TREC 2005: Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.16,518.13,71.64,7.86;10,82.91,528.10,211.70,7.86">Proceedings of the Fourteenth Text REtrieval Conference (TREC 2005)</title>
		<meeting>the Fourteenth Text REtrieval Conference (TREC 2005)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,552.12,226.80,7.86;10,82.91,562.09,215.89,7.86;10,82.91,572.05,215.89,7.86;10,82.91,582.01,215.89,7.86;10,82.91,591.98,84.02,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,210.53,552.12,88.27,7.86;10,82.91,562.09,135.65,7.86">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,241.28,562.09,57.53,7.86;10,82.91,572.05,215.89,7.86;10,82.91,582.01,212.34,7.86">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,606.04,226.80,7.86;10,82.91,616.00,215.89,7.86;10,82.91,625.97,99.42,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,187.03,606.04,111.77,7.86;10,82.91,616.00,106.27,7.86">Term weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<idno>97-881</idno>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,72.00,640.03,226.80,7.86;10,82.91,650.00,215.89,7.86;10,82.91,659.96,215.89,7.86;10,82.91,669.92,209.45,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,267.02,640.03,31.78,7.86;10,82.91,650.00,215.89,7.86;10,82.91,659.96,174.95,7.86">A probabilistic model of information retrieval: development and comparative experiments (parts i and ii)</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,263.14,659.96,35.66,7.86;10,82.91,669.92,130.83,7.86">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="779" to="840" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,683.99,226.80,7.86;10,82.91,693.95,215.89,7.86;10,82.91,703.91,215.89,7.86;10,82.91,713.88,215.89,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,82.91,693.95,215.89,7.86;10,82.91,703.91,46.97,7.86">Indri: a language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,223.30,703.91,75.51,7.86;10,82.91,713.88,188.42,7.86">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,313.20,76.83,226.80,7.86;10,324.11,86.80,215.89,7.86;10,324.11,96.76,215.89,7.86;10,324.11,106.72,214.96,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,385.41,76.83,154.59,7.86;10,324.11,86.80,74.27,7.86">Adarank: a boosting algorithm for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,417.40,86.80,122.60,7.86;10,324.11,96.76,215.89,7.86;10,324.11,106.72,146.38,7.86">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">398</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,313.20,120.67,226.80,7.86;10,324.11,130.63,215.89,7.86;10,324.11,140.60,215.89,7.86;10,324.11,150.56,215.89,7.86;10,324.11,160.52,20.99,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,421.18,120.67,118.83,7.86;10,324.11,130.63,212.33,7.86">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.01,140.60,203.98,7.86;10,324.11,150.56,146.83,7.86">Proceedings of the 10th conference on Information and knowledge management (CIKM)</title>
		<meeting>the 10th conference on Information and knowledge management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
