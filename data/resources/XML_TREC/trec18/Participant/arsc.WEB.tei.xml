<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,100.52,283.40,12.23">Lucene for n-grams using the ClueWeb collection</title>
				<funder>
					<orgName type="full">HPC</orgName>
				</funder>
				<funder>
					<orgName type="full">Arctic Region Supercomputing Center</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,144.04,79.82,9.61"><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center</orgName>
								<orgName type="institution">University of Alaska Fairbanks</orgName>
								<address>
									<postCode>99775</postCode>
									<settlement>Fairbanks</settlement>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,160.14,144.04,52.52,9.61"><forename type="first">Chris</forename><surname>Fallen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center</orgName>
								<orgName type="institution">University of Alaska Fairbanks</orgName>
								<address>
									<postCode>99775</postCode>
									<settlement>Fairbanks</settlement>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.90,144.04,75.99,9.61"><forename type="first">Kylie</forename><surname>Mccormick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Arctic Region Supercomputing Center</orgName>
								<orgName type="institution">University of Alaska Fairbanks</orgName>
								<address>
									<postCode>99775</postCode>
									<settlement>Fairbanks</settlement>
									<region>AK</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,100.52,283.40,12.23">Lucene for n-grams using the ClueWeb collection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35B10A5E241E22DB944536714EAE7B30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Lucene for n-grams using the ClueWeb Collection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ARSC team made modifications to the Apache Lucene engine to accommodate "go words," taken from the Google Gigaword vocabulary of n-grams. Indexing the Category "B" subset of the ClueWeb collection was accomplished by a divide and conquer method, working across the separate ClueWeb subsets for 1, 2 and 3-grams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview and Prior work</head><p>Phrase searching-or imposing an order on query terms-has traditionally been an expensive IR task. One approach is to use sophisticated algorithms at query time to analyze the sequence of a given term relative to nearby terms in the text, where term locations were stored at indexing time. Another method is to index the document collection relative to a large set of phrase tokens, rather than single terms. For the TREC 2009 Web track, we indexed the ClueWeb09 Category B document collection utilizing a "go list" vocabulary (as opposed to a "stop list") of 1-, 2-, and 3-gram phrase tokens extracted from the Google NGram data set. We made small modifications to Lucene to facilitate use of the go list, in both the indexer and the query processor. We found that query processing was quite fast, with queries being processed in well under 2 seconds each. Lucene indexing time increased approximately 3X for the three passes (with 1-, 2-, 3-grams), and index files were larger due to the larger number of indexed n-grams. The additional cost paid to index using ngram tokens allowed for phrase searching to occur as quickly as regular single term searching.</p><p>Prior TREC systems that we built for the Million Query (MQ) <ref type="bibr" coords="1,356.34,531.25,12.27,9.81" target="#b2">[2,</ref><ref type="bibr" coords="1,371.17,531.25,9.92,9.81" target="#b5">5]</ref> and Terabyte (TB) <ref type="bibr" coords="1,472.51,531.25,12.17,9.81" target="#b4">[4,</ref><ref type="bibr" coords="1,487.17,531.25,9.92,9.81" target="#b3">3]</ref> tracks were models of large-scale distributed or grid information retrieval (IR) systems, made from the GOV2 corpus distributed over various clusters from the Arctic Region Supercomputing Center (ARSC). Effective and efficient distributed IR is more difficult than monolithic IR, but it allows for easier fine-grained access control of document collections while maintaining search and discover capability.</p><p>Incorporating phrase search in distributed IR systems is useful, not just for the traditional retrieval task performed by each search node, but potentially also for intelligently restricting the number of collections searched. The search portal component of our 2008 MQ track system <ref type="bibr" coords="1,452.76,659.89,12.05,9.81" target="#b5">[5]</ref>, for example, used a set of term-document co-occurrence matrices constructed from approximately 1000 document collections and a fixed 400,000 term vocabulary from the SCOWL wordlist <ref type="bibr" coords="1,470.90,689.59,12.05,9.81" target="#b1">[1]</ref>, in order to restrict the number of collections queried using an eigensystem approach adapted from Latent Semantic Indexing (LSI) techniques. Incorporating phrases into the master vocabulary of the portal collection-selection algorithm is a natural extension of that technique.</p><p>Our 2009 Web Track system provides search for a set of document collections where each collection has been indexed relative to a fixed phrase vocabulary that has been previously constructed and filtered. The vocabulary may, or may not, be shared with multi-search portals that could use this information to rank or otherwise restrict either the set of collections to be searched or the set of documents to be retrieved from each remote collection. As with our previous TREC systems, the ARSC Web Track phrase-search application described here is built on a modified version of the Lucene Toolkit. Thus, similar phrase-search functionality could be incorporated with existing and well-tested search APIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Component Description</head><p>In this paper, we define a token to be any number of one or more characters, numbers, or punctuations (including whitespace) accepted as a single 'word' in the vocabulary. A term refers here to any number of one or more characters, numbers, or punctuations (excluding whitespace) within a token that is separated by white space. In other words, an n-gram token has n terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lucene</head><p>Lucene is an open-source tool from the Apache project that contains a high-performance search engine library written in Java <ref type="bibr" coords="2,211.58,360.91,12.04,9.81" target="#b6">[6]</ref>. Lucene has a simple API with complex options for building and maintaining search engines with scalable, high-performance indexing and searching. Lucene only processes plain text documents, so HTML and other markup languages must be removed prior to indexing.</p><p>Lucene discovers document tokens by using a TokenStream from each document. Each token is defined by the chosen Tokenizer object, which populates the TokenStream by breaking up the full plain text input of the document. TokenFilters are used to modify, remove, or even add tokens to the TokenStream.</p><p>The Google Gigaword collection (described below) provided us with a "go-list" of word tokens that we wanted to index, rather than a "stop-list" of word tokens that we did not want to index. We used the Lucene WhitespaceTokenizer to populate the TokenStream from any given document, and then applied our GoWordFilter that ignored all unrecognized terms.</p><p>The first index (see section 3) we built for the Web track was used during out official Web track run and utilized only unigram tokens. We also indexed the data using phrases, or n-grams, from the Google Gigaword collection, and this work was used for subsequent non-official runs. For the ngram index and search task, we use Sebastian Kirsch's NGramFilter for Lucene to populate the TokenStream with all possible n-grams from the document before applying the GoWordFilter, which ignores any unknown n-grams in the TokenStream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Google Gigaword</head><p>The Google Gigaword collection is comprised of unigram, bigram, trigram, fourgram, and fivegram tokens found and normalized from approximately one trillion word tokens extracted from publically accessible web pages <ref type="foot" coords="3,219.48,75.21,3.89,6.27" target="#foot_0">1</ref> . While only English-text web pages were candidates, some non-English texts and words exist in the Gigaword collection. The collection also included the token occurrence frequencies across these documents. The tokens were normalized by length and character formatting as well as occurrence value. If a unigram token appeared at least 200 times, or an n-gram token appeared at least 40 times, the token was kept in the Gigaword collection. This collection also maintained cases; so "the," "The," and "THE" were all considered separate unigrams. The same goes for punctuation; there is a distinction between the tokens "word!" and "word." These two particular distinctions were not relevant to our indexing, so we preprocessed the token files by removing tokens considered redundant. Tokens that had unrecognized characters or were comprised of only punctuation marks were removed. However, tokens that contained punctuation marks were retained, except in the case of an end mark (?, !, ., ;, ,) at the end of a word or an unpaired parenthesis.</p><p>The final alteration of the tokens in the Gigaword set was collapsing all like Date Tokens. Since dates can be represented in many forms but be the same date (i.e. 15 May 2009 and 05/15/09), a simple date filter was used to identify possible date tokens. If the token parsed into a date (containing at least a month and either a year or a day, but possibly all three), it was changed into unified date token (formatted: YYYY-MM-DD or YYYY-MM or MM-DD). The date filter works on both unigrams and n-grams, and it can modify an n-gram partially if only part of the n-gram contains a date value (i.e. Superbowl Sunday Feb. 1, 2009 would become Superbowl Sunday 2009-02-01).</p><p>The preprocessing (dubbed "cleaning") of the Category "B" ClueWeb data took approximately 1705 CPU hours to compute. This computation also provided files that did not contain the occurrence values of each token, which is reflected in the file sizes reported in Table <ref type="table" coords="3,412.95,431.35,4.16,9.81">1</ref>. The large difference between the fourgram and the fivegram collection sizes may be attributed to the fact that any rejected term found within an n-gram token would cause the entire token to be discarded. The same factors that would discard a unigram, described above, would cause a term to be discarded, except that a term within an n-gram token could be a length of one character; whereas, unigram tokens needed to have at least two characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Count</head><p>Original Size (GB)  <ref type="table" coords="3,104.22,649.78,5.08,8.77">1</ref> Google Gigaword Tokens and file sizes before and after cleaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARSC's Midnight Supercomputer</head><p>We utilized several computational systems to accomplish the work described here. For processing the ClueWeb dataset and running Lucene with the Google Gigaword "go-list," the primary system used was ARSC's "Midnight."</p><p>Midnight is a supercomputer cluster built from Sun Microsystems servers. With 2312 processors and a theoretical peak performance of 12.8 teraFLOPs (TF), Midnight consists of 358 SunFire x2200M2 "small" nodes (each with two dual-core Opteron processors) and 55 SunFire x4600 "large" nodes (each with eight dual-core Opteron processors). Compute nodes provide 4GB of memory per processor core. Connected via InfiniBand, programs on Midnight have rapid access to 140 raw TB of high performance storage on a Lustre filesystem. Midnight runs the SuSE Linux operating system.</p><p>We ran most jobs on the "large" nodes of Midnight, each with 64GB of memory. They are connected to a high-performance shared workspace, have 250GB of local non-shared temporary storage, and access to petabyte-scale archival storage. For the most part, we used the high-performance storage to store the ClueWeb collection and other input data, then the local on-node disk to build incremental indexes. Indexes were then copied out to the high-performance storage at the end of each indexing job. Multiple jobs, each with multiple Lucene threads, were needed to complete our indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Indexing procedure</head><p>Before the ClueWeb data collection was indexed, we pre-processed the WARC files from the collection into plain text documents associated with the corresponding TREC IDs. This preprocessing stripped HTML and markup tags from each document, leaving behind meta-data (such as alt values from image tags) and some scripting values from languages like Javascript. Since the Gigaword collection contained primarily English words, we only indexed the English data from ClueWeb09. As Table <ref type="table" coords="4,173.63,464.77,6.08,9.81" target="#tab_2">2</ref> shows, the reduced plain text files were approximately 21% the size of the uncompressed, WARC-formatted files with markup tags.</p><p>The size of the data set made even a single, monolithic indexing sweep undesirable. Lucene offers index merging, which takes multiple indexes and merges them into one, which can then be optimized for searching. We used this feature both on the small scale and the large scale. First, on the small scale, we implemented multithreading indexers to create a set of small indexes built in parallel during the program's run; then, the final set of the programs merged them into a single, final index for this run. On the large scale, we ran this program once per every 70 -100 reduced files, depending on the exact file size. When all reduced files were indexed, we merged the final indexes of these program runs to make a single index.</p><p>The other benefit of merging was flexibility with the choice of vocabulary. We were able to index the reduced documents first with only unigrams, then with only bigrams, and then finally merge the two super-indexes together. This method was used for all sizes of grams to reduce the amount of memory used by Lucene, which enabled us to have more threads and a shorter token-load time, which sped up the process of indexing. This divide and conquer scheme also enabled us to retain the smaller, sub-indices from each program run, while providing us with a monolithic index to search. These sub-indices can be used for later research, possibly in relation to our previous work in Multisearch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Run description</head><p>The official ARSC09 submitted run used unigrams over only a fraction-about 25%-of the Class B data set. We subsequently indexed unigrams up to trigrams (with fourgrams and fivegrams pending), for later comparison. The searching was exceptionally fast, with only one query taking long enough-one second-for our timer to measure it. Table <ref type="table" coords="5,397.31,316.09,6.08,9.81">3</ref> gives a list of select queries, the number of found documents in the Class B data set and the time it took to run the query.</p><p>Unigram runs simply search the index with the modified query terms, which undergo the same modifications as the indexed data, as described in section 3. Unigrams were run over both the Class B dataset and later over the full English data set of ClueWeb. N-gram runs had two run types: exact and verbose. Exact n-gram runs modified the query such that the largest n-gram token in the query would be considered a single token. For example, the query little red riding hood cookies would recognize only two tokens: little red riding hood and cookies. Essentially, this is exact phrase matching.</p><p>Verbose n-gram runs modified the query by populating it with all valid renditions of its terms as ngram tokens and removing any tokens that are not listed in the modified Gigaword vocabulary. The  <ref type="table" coords="5,104.22,518.44,5.08,8.77">3</ref> Selected queries and their results and runtimes over the Class B data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>original query terms are also preserved. For example, the query little red riding hood cookies would become the search tokens little red, little red riding hood, hood cookies, little, red, riding, hood, and cookies. Essentially, this is query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Preliminary results</head><p>Initial IR performance summary results from the ARSC09 Web track system were based only on the subset of Category "B" that was indexed in time for the submission deadline. Relative to both the estimated Average Precision (AP) and Normalized Discounted Cumulative Gain (nDCG) <ref type="bibr" coords="6,481.76,177.49,13.77,9.81">[7]</ref> performance measures, our Web track system scored below the TREC median on 90% of the topics and tied the TREC worst score on 60% of the topics. Further analysis is needed to identify system deficiencies; however, we indexed only 25% of the Category B data set before submitting the official results and this is likely a significant contributing factor in our sub-median IR performance. A baseline run with a stock Lucene-based system would also help to determine whether the phrase index and search technique improved or degraded Lucene IR performance.</p><p>Four topics-numbers 4, 23, 28, and 29-are especially interesting because they scored above the median on one or both IR performance measures (see Table <ref type="table" coords="6,354.97,306.13,4.17,9.81">4</ref>). Our nDCG score on topic 29, ps 2 games, was the best of the TREC participants. Topics 4 and 28, toilet and inuyasha, respectively, scored above the TREC median relative to nDCG but tied the TREC worst relative to AP. Likewise, topic 23, yahoo, scored above the TREC median relative to AP but tied the TREC worst relative to nDCG. It is interesting to note that only one of the exceptional topics is a phrase token containing more than a single term. In particular, the topic ps 2 games likely performed well because each of the terms was contained in the vocabulary yet the number 2 was probably discarded by many other TREC participant systems during indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary and future work</head><p>ARSC is continuing with several different approaches to experimentation in information retrieval. Along with other TREC Web track participants, we will be looking into characteristics of the ClueWeb collection, and seeking to understand performance on topics. Our research with a "goword" list, described here, has promise for more easily enabling indexing and searching on phrases. We anticipate that, with further tuning, this will be a major advantage for TREC's Web-style queries. Such queries only have a few words, and therefore we can match the entire query as a phrase.  <ref type="table" coords="6,104.22,526.96,5.08,8.77">4</ref> IR performance of the ARSC Web Track system on topics scoring above the TREC participant median relative to the AP or nDCG performance measures. terms). Modifying TF/IDF or pivoted weighting schemes to account for phrases, versus single terms, is a goal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,79.20,508.60,415.52,51.49"><head>Compressed WARC Files (TB) Uncompressed WARC Files (TB) Plain Text Reduced Files (TB)</head><label></label><figDesc></figDesc><table coords="4,79.20,538.42,385.30,21.67"><row><cell>Class B Dataset</cell><cell>0.22</cell><cell>1.5</cell><cell>0.31</cell></row><row><cell>Full English Dataset</cell><cell>2.0</cell><cell>13.2</cell><cell>2.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,79.20,566.26,213.96,8.77"><head>Table 2</head><label>2</label><figDesc>ClueWeb09 English Dataset WARC file sizes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,72.00,439.00,470.50,261.90"><head></head><label></label><figDesc>To date, we believe Lucene has not yet been modified to give differential weights to phrases (versus</figDesc><table coords="6,79.20,439.00,463.30,96.73"><row><cell></cell><cell></cell><cell>AP</cell><cell></cell><cell></cell><cell></cell><cell cols="2">nDCG</cell><cell></cell></row><row><cell>Query</cell><cell>ARSC</cell><cell>TREC best</cell><cell>TREC median</cell><cell>TREC worst</cell><cell>ARSC</cell><cell>TREC best</cell><cell>TREC median</cell><cell>TREC worst</cell></row><row><cell>wt09-04: toilet</cell><cell>0.0141</cell><cell>0.2177</cell><cell>0.0902</cell><cell>0.0005</cell><cell>0.1170</cell><cell>0.4098</cell><cell>0.1082</cell><cell>0.0000</cell></row><row><cell>wt09-23: yahoo</cell><cell>0.0292</cell><cell>0.5492</cell><cell>0.0106</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.3271</cell><cell>0.0493</cell><cell>0.0000</cell></row><row><cell>wt09-28: inuyasha</cell><cell>0.0081</cell><cell>0.7384</cell><cell>0.3735</cell><cell>0.0081</cell><cell>0.3370</cell><cell>0.6450</cell><cell>0.3175</cell><cell>0.0422</cell></row><row><cell>wt09-29: ps 2 games</cell><cell>0.0081</cell><cell>0.0918</cell><cell>0.0065</cell><cell>0.0001</cell><cell>0.1782</cell><cell>0.1782</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,79.08,698.88,409.49,8.95;3,72.00,711.06,302.61,8.95"><p>This collection is available from the Linguistic Data Consortium as "Web 1T 5-gram version 1" at www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We are also continuing with our investigations into large-scale eigensystems [8], and included some detail on work to date in our TREC 2009 poster. Using the PETSc and SLEPc packages, we have performed full eigensystems analysis (singular value decomposition) on sparse matrices including over 600,000 terms [9]. Description of the properties of these eigensystems, and their utility for information retrieval, will be the focus of a forthcoming paper.</p><p>Research described at past years of TREC is also ongoing. This includes our efforts on grid computing, federated search, and unified relevance ranking from discrete collections.</p></div>
<div><head>Acknowledgements</head><p>This work was supported in part by a grant of <rs type="funder">HPC</rs> resources from the <rs type="funder">Arctic Region Supercomputing Center</rs> and the <rs type="institution">DoD High Performance Computing Modernization Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,77.04,239.72,72.77,11.39" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,256.75,194.41,9.81" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,166.50,256.75,101.97,9.81">Kevin&apos;s Word List Page</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.03,269.65,431.48,9.81;7,108.00,282.55,388.28,9.81;7,108.00,295.45,71.30,9.81" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,241.68,269.65,297.83,9.81;7,108.00,282.55,193.27,9.81">Collection Selection Based on Historical Performance for Efficient Processing, 16th Text REtrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<imprint>
			<publisher>NIST Special Publications</publisher>
			<pubPlace>Gaithersburg, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.03,308.29,417.42,9.81;7,108.00,321.19,250.09,9.81" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,241.68,308.29,283.76,9.81;7,108.00,321.19,12.35,9.81">Distributed Web Search Efficiency by Truncating Results, JCDL &apos;07</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ACM</publisher>
			<pubPlace>Vancouver, British Columbia, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.03,334.09,399.07,9.81;7,108.00,346.99,372.89,9.81;7,108.00,359.90,229.44,9.81" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,241.68,334.09,265.42,9.81;7,108.00,346.99,306.86,9.81">Partitioning the Gov2 Corpus by Internet Domain Name: A Resultset Merging Experiment, The 15th Text REtrieval Converence</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>NIST Special Publications</publisher>
			<pubPlace>Gaithersburg, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.03,372.79,426.77,9.81;7,108.00,385.69,310.01,9.81;7,108.00,398.59,265.09,9.81" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,311.16,372.79,223.64,9.81;7,108.00,385.69,279.77,9.81">Distributed multisearch and resource selction for the TREC Million Query Track, 17th Text REtrieval Conference</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mccormick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>NIST Special Publications</publisher>
			<pubPlace>Gaithersburg, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.04,411.49,349.99,9.81" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<title level="m" coord="7,244.86,411.49,73.26,9.81">Lucene in Action</title>
		<imprint>
			<publisher>Manning Publications</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,424.39,423.26,9.81;7,108.00,437.29,155.35,9.81" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,243.60,424.39,226.60,9.81">Cumulated gainbased evaluation of IR techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,477.31,424.39,53.97,9.81;7,108.00,437.29,37.63,9.81">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.03,450.19,414.00,9.81;7,108.00,463.09,276.96,9.81" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,237.42,450.19,284.60,9.81;7,108.00,463.10,109.55,9.81">Instability of RelevanceRanked Results Using Latent Semantic Indexing for Web Search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kettani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,224.82,463.10,39.71,9.81">HICCS</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="2009">2009</date>
			<pubPlace>Honolulu, Hawaii</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.03,475.99,394.67,9.81;7,108.00,488.89,418.00,9.81;7,108.00,501.73,26.56,9.81" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,297.42,475.99,205.28,9.81;7,108.00,488.89,159.90,9.81">Large Scale Sparse Matrix Analysis for Term Document Occurrences in Web Text</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Newby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Styers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Fallen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,300.48,488.89,133.32,9.81">Dynamics of Complex Systems</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<pubPlace>Fairbanks, Alaska</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
