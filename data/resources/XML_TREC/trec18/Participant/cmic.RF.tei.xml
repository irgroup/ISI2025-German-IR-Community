<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,178.85,74.84,254.28,10.80">CMIC@TREC-2009: Relevance Feedback Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,229.97,88.24,72.66,9.94"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
							<email>kareemd@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Cairo Microsoft Innovation Center (CMIC) Bldg. B115</orgName>
								<address>
									<addrLine>Smart Village, Km. 28 Cairo-Alexandria Desert Rd</addrLine>
									<settlement>Abou Rawash</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.31,88.24,71.70,9.94"><forename type="first">Ahmed</forename><surname>El-Deeb</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cairo Microsoft Innovation Center (CMIC) Bldg. B115</orgName>
								<address>
									<addrLine>Smart Village, Km. 28 Cairo-Alexandria Desert Rd</addrLine>
									<settlement>Abou Rawash</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,178.85,74.84,254.28,10.80">CMIC@TREC-2009: Relevance Feedback Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">15065127E72E1A587C394421E3140A05</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes CMIC's submissions to the TREC'09 relevance feedback track. In the phase 1 runs we submitted, we experimented with two different techniques to produce 5 documents to be judged by the user in the initial feedback step, namely using knowledge bases and clustering. Both techniques attempt to topically diversify these 5 documents as much as possible in an effort to maximize the probability that they contain at least 1 relevant document. The basic premise is that if a query has n diverse interpretations, then diversifying results and picking the top 5 most likely interpretations would maximize the probability that a user would be interested in at least one interpretation. In phase 2 runs, which involved the use of the feedback attained from phase 1 judgments, we attempted to use positive and negative judgments in weighing the terms to be used for subsequent feedback. .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Phase 1 of the runs involved nominating 5 documents to a user for which the user would provide relevance judgments. In the second phase, these judgments are used for relevance feedback. In nominating the 5 documents, it is essential to present users with some relevant documents exemplars, where having at least one relevant document is better than having none. We opted to eliminate the worst case scenario, where none of the documents that a user is judging is relevant. To this end, we attempted to topically diversify the documents to be judged by the user to increase the probability that at least 1 of the 5 document is relevant, at the possible expense of decreasing the number of relevant documents in these 5 documents. The basic premise is that if a query has n diverse interpretations I n = {i 1 , i 2 , â€¦ i n } (ex. Jaguar: cat, car, OS, etc.) with P j = Prob(interest_to_user|i j ), where sum(P j | j = 1 .. n) = 1, then picking one example of each of the top 5 most likely interpretations would maximize the probability that a user would be interested in one interpretation. To achieve this kind of diversity we tried two different techniques for diversification: one relied on a knowledge base, namely Wikipedia, and the other on cluster analysis.</p><p>For phase 2 submissions involving feedback, we employed a fairly simple equation to expand the queries based on the probability of the existence of a certain term in a relevant document versus the probability of its existence in an irrelevant document.</p><p>The rest of the paper is organized as follows: section 2 surveys issues relating to results diversification; section 3 describes experimental setup; section 4 reports on submissions results; and section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diversification:</head><p>Though the work on results diversification is relatively scant, a few methods have been suggested to diversify search results. Carbonell and Goldstein <ref type="bibr" coords="2,318.31,130.72,12.91,9.94" target="#b1">[2]</ref> suggested the so-called Maximal Marginal Relevance (MMR) which attempts to reduce redundancy while maintaining relevance. MMR combines query relevance with information-novelty as follows <ref type="bibr" coords="2,306.19,159.76,11.93,9.94" target="#b1">[2]</ref>:</p><formula xml:id="formula_0" coords="2,72.02,186.67,339.71,12.80">ğ‘€ğ‘€ğ‘… â‰ ğ´ğ‘Ÿğ‘” max ğ·ğ‘– ğœ– ğ‘…\ğ‘† ğœ†(ğ‘†ğ‘–ğ‘š1 ğ· ğ‘– , ğ‘„ -1 -ğœ† max ğ· ğ‘— âˆˆğ‘† ğ‘†ğ‘–ğ‘š2(ğ· ğ‘– , ğ· ğ‘— )) (1)</formula><p>Where Q is the query, D i is the i th document in the ad-hoc retrieval ranked list, Sim 1 (D i , Q) is the similarity between Q and D i , max Sim 2 (D i , D j ) is the maximal similarity between D i and all documents D j where j ranges between 1 and i -1, and ï¬ is a weighting factor that is less than 1 and gives varying weights to Sim 1 and max Sim 2 to favor similarity to query or dissimilarity to previously seen documents. MMR favors documents that are most similar to the query while penalizing documents that contain redundant information. The newly computed MMR for each document is used to re-rank search results, hopefully selecting non-redundant relevant documents.</p><p>Chen and Karger <ref type="bibr" coords="2,151.22,326.95,12.78,9.94" target="#b2">[3]</ref> argued against the optimality of the Probability Ranking Principal, stating that -in a probabilistic context, one should directly optimize for the expected value of the metric of interestâ€–. Most web search engines optimize for metrics such as DCG and NDCG, which often hurt diversity. To achieve diversity, they used a greedy algorithm to optimize for a specific objective, namely finding at least one relevant document, integrating diversity into their ranking formula. What is noteworthy in their work is their treatise on the applicability of different evaluation metrics such as search length, MRR, %no, which measures one document sufficiency, and k-call, which is k document sufficiency. <ref type="bibr" coords="2,189.29,438.81,18.43,9.94" target="#b19">[20]</ref> explored re-ranking on the client side, to efficiently incorporate personalization with diversification, and they achieved diversification (or disambiguation) by augmenting a query with its most common reformulations, which were acquired from web search engine query logs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Radlinski and Dumais</head><p>Agrawal et al. <ref type="bibr" coords="2,139.01,492.45,12.91,9.94" target="#b0">[1]</ref> assumed the existence of a taxonomy of information to achieve diversification. They mapped both queries and documents to one or more entries in the taxonomy and query results were diversified to cover different entries in the taxonomy. They also proposed some interesting generalizations to standard IR metrics, like MAP, MRR, and NDCG to explicitly account for diversification.</p><p>Zhai et al. <ref type="bibr" coords="2,124.58,575.13,18.30,9.94" target="#b24">[25]</ref> proposed a framework for evaluating algorithms for subtopic retrieval in an effort to account for the intrinsic difficulty of a query, as well as the coverage of subtopics. They also did some work on generalizing evaluation metrics and introduced so-called S-recall and S-precision. In another work, Zhai and Lafferty <ref type="bibr" coords="2,181.49,618.84,18.30,9.94" target="#b25">[26]</ref> proposed a risk minimization framework that attempts to minimize a certain loss function that represents the user's dissatisfaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering:</head><p>Cluster analysis is an unsupervised machine learning technique that attempts to find clusters of related ndimensional objects within a data collection using different objectives or criteria. Partitioning algorithms such as k-means <ref type="bibr" coords="2,149.78,702.84,18.43,9.94" target="#b9">[10]</ref> attempts to optimize an objective function to form clusters around centers/means. K-medoids <ref type="bibr" coords="3,124.58,74.42,11.70,9.94" target="#b6">[7]</ref>, PAM <ref type="bibr" coords="3,171.02,74.42,18.43,9.94" target="#b12">[13]</ref> and CLARANS <ref type="bibr" coords="3,267.29,74.42,18.30,9.94" target="#b18">[19]</ref> are related techniques that substitute cluster means with medoids or representative data objects. Hierarchical algorithms such as single, average, and complete linkage produce dendrograms which provide clustering at several possible numbers of clusters. Other approaches include Grid-based algorithms such as DenClue <ref type="bibr" coords="3,367.27,118.12,12.80,9.94" target="#b8">[9]</ref> and STING <ref type="bibr" coords="3,449.14,118.12,16.99,9.94" target="#b22">[23]</ref>, Density-based algorithms such as DBSCAN <ref type="bibr" coords="3,217.01,132.64,11.79,9.94" target="#b4">[5]</ref>, Graph-based algorithms such as Chameleon <ref type="bibr" coords="3,451.54,132.64,16.90,9.94" target="#b11">[12]</ref>, and distancerelatedness-based algorithms such as Mitosis <ref type="bibr" coords="3,285.05,147.16,16.91,9.94" target="#b23">[24]</ref>. Partitional algorithms that are k-means like and average or complete linkage are only able to detect clusters of globular/hyper-spherical shapes. However, single linkage algorithms are known to find elongated shaped clusters, but are greatly affected by outliers. Density based algorithms as DBSCAN, tend to find clusters of arbitrary shapes and identify outliers, and more recent algorithms such as Chameleon and Mitosis find clusters of arbitrary shapes and arbitrary densities <ref type="bibr" coords="3,114.26,219.88,16.90,9.94" target="#b23">[24]</ref>. Some of the popular types of clustering include partitional k-means, hierarchical singlelink, and density-based DBSCAN clustering, which are O(nkd), O(n 2 ), and O(nlogn) respectively, where n is the number of documents and for k-means k is the number of clusters and d is the number of iterations <ref type="bibr" coords="3,116.30,263.59,17.55,9.94" target="#b17">[18]</ref> <ref type="bibr" coords="3,133.85,263.59,17.55,9.94" target="#b21">[22]</ref>. K-means proceeds through the following steps: k documents are picked randomly to form centroids of clusters, each document is assigned to the closest centroid, the center of each cluster is chosen as the new centroid, and the process iterates until the algorithm converges. K-means requires k to be specified a priori, the resulting clusters are globular in shape, and the choice of initial centroids may change the assignment of documents to clusters. Bisecting k-means is a variant of the popular k-means clustering algorithm in which a document set is split into two clusters using the generic k-means algorithm and then some (or all) of the resulting clusters of elements are iteratively split into two until the desired k clusters are formed. Although bisecting k-means is slower than k-means clustering, bisecting kmeans is insensitive to the choice of initial centroids.</p><p>Hierarchical clustering organizes documents in a tree like structure called a dendrogram, where each document is assumed to be a singleton cluster, and then clusters are merged successively in descending similarity until all documents are merged into a large cluster at the root of the dendrogram. The merge process can be applied successively until a desired number of clusters is reached.</p><p>DBSCAN <ref type="bibr" coords="3,119.54,472.65,12.91,9.94" target="#b4">[5]</ref> is a density based clustering technique in which an initial set of -coreâ€– elements, which are elements that have a minimum number of M neighbors that fall within ï¥ distance away, are used to form the seeds of clusters, and then these seed clusters are allowed take-in more points or clusters within ï¥ distance of any of their member elements (conflating clusters if need be). Some of the advantages of DBSCAN include: clusters can be arbitrary shaped, unlike k-means clustering which produces globular clusters, -coreâ€– elements in a cluster are found automatically, and unlike partitional or hierarchical techniques not all elements belong to clusters, because elements that are further than ï¥ away from other elements are deemed as outliers. The major disadvantage of DBSCAN is that the values of M and ï¥ need to be determined a priori. A distance metric or a similarity measure is used to measure proximity between objects in a clustering algorithm. Some popular similarity measures include cosine similarity and TF-IDF weighing.</p><p>As for the use of clustering in IR, subsequent to Van Rijsbergen cluster hypothesis <ref type="bibr" coords="3,461.02,646.56,18.43,9.94" target="#b10">[11]</ref> and Salton's suggestion to use clustering in IR <ref type="bibr" coords="3,223.49,661.08,16.99,9.94" target="#b20">[21]</ref>, much work has been done on applying clustering to IR <ref type="bibr" coords="3,494.62,661.08,16.99,9.94" target="#b17">[18]</ref>. Van Rijsbergen's cluster hypothesis states that -closely associated documents tend to be relevant to the same requestâ€– <ref type="bibr" coords="3,113.06,690.24,16.99,9.94" target="#b10">[11]</ref>. Attempts were made to exploit this hypothesis in various ways. As examples, post hoc clustering of retrieval results has been used to improve retrieval effectiveness <ref type="bibr" coords="3,449.50,704.76,12.74,9.94" target="#b7">[8]</ref> <ref type="bibr" coords="3,462.24,704.76,16.98,9.94" target="#b16">[17]</ref>, to improve presentation <ref type="bibr" coords="4,132.74,74.42,16.90,9.94" target="#b15">[16]</ref>, blind relevance feedback <ref type="bibr" coords="4,271.97,74.42,16.90,9.94" target="#b13">[14]</ref>, and non-blind relevance feedback <ref type="bibr" coords="4,450.58,74.42,16.99,9.94" target="#b14">[15]</ref>. This list is by no means comprehensive, but gives samples of the four main directions in which the cluster analysis was used in IR. In Lee et al. <ref type="bibr" coords="4,188.45,103.48,16.90,9.94" target="#b13">[14]</ref>, single link clustering was successfully used to identify core topics of a query, which are identified as dense clusters, for which -dominantâ€– documents were used for blind relevance feedback. Leuski and Allen <ref type="bibr" coords="4,247.13,132.64,18.30,9.94" target="#b15">[16]</ref> used hierarchical clustering as part of an interactive retrieval system in which documents that cluster together would appear together and clusters are clearly demarked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup Phase 1</head><p>For the first stage, we experimented with two different techniques to produce 5 documents to be judged by the user. Both techniques attempt to topically diversify the 5 documents to be presented to the user as much as possible in an effort to maximize the probability that these documents contain at least 1 relevant document.</p><p>The first diversification technique (employed in run CMIC.1) utilized Wikipedia with the assumption that Wikipedia articles are naturally diverse, i.e. no two articles cover exactly the same topic. We issued all the queries against Bing, a web search engine, while restricting results to Wikipedia (ex. -jaguar site:en.wikipedia.orgâ€–). The RF track collection includes a 2008 snapshot of Wikipedia. We nominated the top 5 results to show to the user, automatically excluding Wikipedia articles that point to non-article content such as images and discussion pages.</p><p>The second diversification technique (employed in run CMIC.2) relied on DBSCAN <ref type="bibr" coords="4,467.74,392.95,12.91,9.94" target="#b4">[5]</ref> -a density based clustering techniqueto cluster top 100 results for each query. The IR group at Microsoft Research, Cambridge, kindly provided us with 2,500 search results for each query <ref type="bibr" coords="4,436.90,422.01,11.70,9.94" target="#b3">[4]</ref>. These results were obtained by searching Category B of the ClueWeb09 collection using the OKAPI-BM25 weighting formula. One of the main challenges in this track was to effectively search the 50 million documents in the collection. This was done using a distributed grep-like function, implemented on Microsoft's Dryad framework, to select all documents containing the query terms and then to compute appropriate weights for ranking.</p><p>In our application of DBSCAN, all the terms in documents were tokenized, stemmed using Porter stemmer, and stopwords were removed. Distance between documents was computed as (1 -cosine similarity). Clustering was performed using parameter values M equal to 4 and ï¥ equal to 0.65. We picked these specific parameters using extensive side experiments on the TREC 2001 and TREC 2002 filtering collection, which includes a set of approximately 880,000 documents from Reuters, 184 topics, and associated relevance judgments. The highest ranked document in each cluster, excluding outliers (singleton clusters), was picked to represent the cluster. Subsequently, the highest ranked 5 documents representing clusters were nominated to be shown to the users. If the number of clusters was less than 5, the remaining documents were picked from the highest ranked outliers. Aside from being easy to implement and having an agreeable time complexity, DBSCAN has many relevant advantages including its capacity to form arbitrarily shaped clusters and to automatically detect outliers. We did some previous experiments that suggested that the use of DBSCAN for this purpose is more effective than that of kmeans and bisecting k-means. These experiments also showed the favorable effect of detecting outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase 2</head><p>For phase 2, we attempted to re-rank the top 2,500 results from searching using the original queries. The re-ranking was done by indexing these top 2,500 documents using Indri and searching the index using expansion terms from judged documents (original query terms were excluded because they were used to produce the initial results and all documents generally contained the original query terms). For expansion terms, we attempted to make use of both positive and negative judgments in weighing the terms to be used in expanding queries. To do so, we used a variant of Acc2 feature selection metric referred to by Forman <ref type="bibr" coords="5,109.58,177.64,11.79,9.94" target="#b5">[6]</ref>. Forman originally used Acc2among other metrics-as a metric for feature selection in text classification tasks. In his paper, he compared several such metrics for precision and F-measure with Acc2 being one of the best feature selection metrics and one of the easiest to implement. We use Acc2 in a different sense however: to determine the weights to assign to each term while expanding the query. The weight W for a certain term t is calculated as follows:</p><formula xml:id="formula_1" coords="5,72.02,260.24,304.08,43.28">W(t) = P(t|pos) -P(t|neg) = # ğ‘œğ‘“ ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ ğ‘‘ğ‘œğ‘ğ‘  ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘” ğ‘¡ # ğ‘œğ‘“ ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘¡ ğ‘‘ğ‘œğ‘ğ‘  - # of non -relevant docs containg t # of non -relevant docs</formula><p>In original formula used by Forman, Acc2 was the absolute value of W(t). Only terms with positive weights were used to re-rank the initial set of results of 2,500 documents by searching them using Indri.</p><p>The weights W(t) were used in weighting query terms in Indri (using Indri's #wsum operator).</p><p>CMIC was assigned 8 different phase 1 runs to use for expansion, namely: CMIC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>To ascertain the effectiveness of our phase 1 results, we used mean reciprocal rank (MRR) and precision at 5 (P@5) as the measures of quality. The rationale for picking MRR is based on the assumption that including at least 1 relevant document in the feedback set is better than having none. Thus, MRR would hopefully correlate with the probability of having at least one such document, as the higher the rank of relevant documents the greater the probability and having no relevant documents would result in an MRR of 0. The rationale for using P@5 stems from the assumption that having more relevant documents in the feedback set would yield better feedback results. Figure <ref type="figure" coords="5,324.16,527.61,5.52,9.94">1</ref> shows the results of CMIC (marked) compared to the results of the rest of the groups for phase 1 sorted by descending MRR values.</p><p>Figure <ref type="figure" coords="6,233.78,241.00,4.14,9.94">1</ref>. Phase 1 results using MRR and P@5</p><p>Using MRR as the metric of effectiveness, CMIC.1 and CMIC.2 appeared in positions 2 and 13 respectively compared to the other submissions. P@5 did not correlate perfectly with MRR. Using P@5, CMIC.1 and CMIC.2 appeared in positions 7 and 11 respectively compared to the other submissions.</p><p>It is worth noting at this point that many <ref type="bibr" coords="6,259.49,319.27,12.97,9.94" target="#b0">[1]</ref>[2][3] consider using such traditional metrics such as MRR and P@5 insufficient to judge diversification results as it is not the goal of diversification to optimize for them. Thus, some efforts went into devising modified metrics that account for diversity <ref type="bibr" coords="6,461.79,348.31,24.64,9.94">[1][3]</ref>.</p><p>Table <ref type="table" coords="6,101.40,372.91,5.52,9.94" target="#tab_1">1</ref> reports the official results phase 1 runs, where score is the ratio of runs that are better to the number of runs that are better and worse accumulated over all measures and groups using the overall average in the feedback step. For each the metrics for each of the runs, the 1 st and 2 nd numbers indicate the number of runs where the run did worse or better than respectively. As for the official phase 2 results, Table <ref type="table" coords="7,253.63,74.42,5.52,9.94">2</ref> reports expected mean average precision (emap) and statistical average precision (stAP) scores for all our phase 2 submissions sorted by emap. Unfortunately, we did not have access to the scores for submissions of other groups, which does not allow us to compare to other groups. The only information that was provided to us indicate that emap scores across all groups ranged between 0.0168 and 0.0536, and stAP scores ranged between 0.0434 and 0.2638. Using all the above metrics, CMIC.1 which involved restricting phase 1 results to Wikipedia only did better than CMIC.2, which relied on density-based clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We conclude that the use of diversification can benefit many retrieval scenarios especially when we would like to minimize the probability of a user not finding any relevant documents, hence it can be used for feedback tasks. The results suggest that relying on knowledge basesnamely Wikipedia in this work can be more effective than unsupervised approaches such as cluster analysis. Although diversification has the potential of decreasing the number of relevant documents in the diversified results (5 in the case of phase 1 results), it also has the effect of increasing the probability of finding at least one relevant document, which would improve relevance feedback. We tried to make use of non-relevant documents in our query expansion scheme. Due to the fact that we don't have the full relevance judgments, we cannot ascertain the effect of accounting for relevant as well as non-relevant documents in relevance feedback, however, we hope that this can be more effective than using only relevant documents for feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.02,371.35,468.22,24.46"><head></head><label></label><figDesc>1, CMIC.2, ilps.1, MSRC.1, udel.1, udel.2, ugTr.2, and UMas.2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,99.09,77.44,415.78,590.38"><head>Table 1 .</head><label>1</label><figDesc>Official phase 1 results</figDesc><table coords="6,99.09,77.44,415.78,590.38"><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MRR</cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>P@5</cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WatS.2</cell><cell>CMIC.1</cell><cell>UCSC.1</cell><cell>UCSC.2</cell><cell>ugTr.2</cell><cell>UMas.1</cell><cell>UPD.1</cell><cell>Sab.1</cell><cell>udel.1</cell><cell>fub.1</cell><cell>hit2.1</cell><cell>PRIS.1</cell><cell>CMIC.2</cell><cell>WatS.1</cell><cell>ilps.2</cell><cell>twen.1</cell><cell>ugTr.1</cell><cell>MSRC.1</cell><cell>hit2.2</cell><cell>UMas.2</cell><cell>udel.2</cell><cell>ilps.1</cell><cell>CMU.1</cell><cell>twen.2</cell><cell>MSRC.2</cell><cell>SIEL.1</cell><cell>YUIR.1</cell><cell>FDU.1</cell><cell>YUIR.2</cell><cell>QUT.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topic</cell><cell></cell><cell cols="2">Emap</cell><cell></cell><cell cols="2">Map</cell><cell></cell><cell></cell><cell cols="2">P@10</cell><cell></cell><cell></cell><cell cols="2">stAP</cell><cell></cell><cell cols="2">Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">CMIC.1 6 27</cell><cell></cell><cell cols="2">0 0</cell><cell></cell><cell></cell><cell cols="2">0 0</cell><cell></cell><cell></cell><cell>9 24</cell><cell></cell><cell></cell><cell cols="3">0.7727</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CMIC.2 8 5</cell><cell></cell><cell></cell><cell cols="2">5 9</cell><cell></cell><cell></cell><cell cols="2">4 10</cell><cell></cell><cell></cell><cell>9 4</cell><cell></cell><cell></cell><cell cols="3">0.5185</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="13">Table 2. Official phase 2 results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">run ID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">emap</cell><cell></cell><cell></cell><cell cols="2">stAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.1582</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CMIC.CMIC.1</cell><cell></cell><cell cols="3">0.0340</cell><cell></cell><cell></cell><cell cols="3">0.1511</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CMIC.ugTr.2</cell><cell></cell><cell></cell><cell cols="3">0.0318</cell><cell></cell><cell></cell><cell cols="3">0.1520</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CMIC.ilps.1</cell><cell></cell><cell></cell><cell cols="3">0.0314</cell><cell></cell><cell></cell><cell cols="3">0.1600</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CMIC.UMas.2</cell><cell></cell><cell cols="3">0.0312</cell><cell></cell><cell></cell><cell cols="3">0.1409</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CMIC.udel.1</cell><cell></cell><cell></cell><cell cols="3">0.0299</cell><cell></cell><cell></cell><cell cols="3">0.1363</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CMIC.udel.2</cell><cell></cell><cell></cell><cell cols="3">0.0285</cell><cell></cell><cell></cell><cell cols="3">0.1216</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CMIC.CMIC.2</cell><cell></cell><cell cols="3">0.0284</cell><cell></cell><cell></cell><cell cols="3">0.1293</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">CMIC.MSRC.1</cell><cell></cell><cell cols="3">0.0284</cell><cell></cell><cell></cell><cell cols="3">0.1331</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,76.16,432.09,463.61,9.94;7,90.02,444.69,449.80,9.94;7,90.02,457.41,101.19,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,345.95,432.09,119.17,9.94">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,473.03,432.09,66.75,9.94;7,90.02,444.69,355.84,9.94">Proceedings of the Second ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM International Conference on Web Search and Data Mining<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,470.01,463.94,9.94;7,90.02,482.61,267.05,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,227.78,470.01,312.32,9.94;7,90.02,482.61,111.44,9.94">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>SIGIR-1998</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,495.33,463.60,9.94;7,90.02,507.93,356.36,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,234.03,495.33,305.73,9.94;7,90.02,507.93,45.44,9.94">Less is more: probabilistic models for retrieving fewer relevant documents</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,143.27,507.93,51.87,9.94">SIGIR-2006</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,520.54,463.60,10.05;7,90.02,533.14,257.21,10.04" xml:id="b3">
	<analytic>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,482.95,520.54,56.81,10.04;7,90.02,533.14,159.26,10.04">TREC 2009: Web and Relevance Feedback Track</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,545.85,463.87,9.94;7,90.02,558.57,267.89,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,303.34,545.85,236.69,9.94;7,90.02,558.57,145.11,9.94">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,242.13,558.57,25.65,9.94">KDD</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,571.06,463.62,10.05;7,90.02,583.89,211.47,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,153.61,571.06,366.11,10.04">An extensive empirical study of feature selection metrics for text classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,523.82,571.06,15.96,10.04;7,90.02,583.89,79.36,9.94">â€– J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1305" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,596.52,463.87,9.94;7,90.02,609.12,240.72,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,298.90,596.52,241.14,9.94;7,90.02,609.12,106.58,9.94">The Elements of Statistical Learning. Data Mining, Inference and Prediction</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,621.84,463.61,9.94;7,90.02,634.44,294.89,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,251.17,621.84,288.60,9.94;7,90.02,634.44,27.57,9.94">Reexamining the cluster hypothesis: scatter/gather on retrieval results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,124.29,634.44,54.29,9.94">SIGIR-1996</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,76.16,647.16,463.63,9.94;7,90.02,659.76,235.02,9.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,222.40,647.16,317.39,9.94;7,90.02,659.76,21.69,9.94">An efficient approach to clustering in large multimedia databases with noise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Knowledge Discovery and Data Mining</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,81.22,672.48,444.92,9.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,236.27,672.48,108.43,9.94">Data clustering: a review</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Flyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,352.15,672.48,109.71,9.94">ACM Computer Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,81.22,685.08,458.60,9.94;7,90.02,697.68,300.65,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,274.38,685.08,261.50,9.94">The use of hierarchic clustering in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,90.02,697.68,149.08,9.94">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="1971-12">Dec. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,74.42,458.57,9.94;8,90.02,87.04,242.81,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,275.37,74.42,264.42,9.94;8,90.02,87.04,80.01,9.94">CHAMELEON: a hierarchical clustering algorithm using dynamic modeling</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,177.72,87.04,44.08,9.94">Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,99.76,458.50,9.94;8,90.02,112.36,83.84,9.94" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="8,235.50,99.76,267.24,9.94">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,125.08,458.94,9.94;8,90.02,137.68,137.25,9.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,279.35,125.08,260.82,9.94;8,90.02,137.68,37.75,9.94">A cluster-based resampling method for pseudo-relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,138.01,137.68,54.25,9.94">SIGIR-2008</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,150.28,458.54,9.94;8,90.02,162.89,220.85,10.04" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,199.95,150.28,335.40,9.94">Improving Interactive Retrieval by Combining Ranked Lists and Clustering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.02,162.89,118.14,10.04">Proceedings of RIAO&apos;2000</title>
		<meeting>RIAO&apos;2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="665" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,175.60,458.67,9.94;8,90.02,188.32,291.02,9.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,198.67,175.60,321.48,9.94">Interactive Information Retrieval Using Clustering and Spatial Proximity</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,530.90,175.60,9.00,9.94;8,90.02,188.32,199.90,9.94">In User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="259" to="288" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,200.92,458.95,9.94;8,90.02,213.52,197.79,9.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,209.69,200.92,211.05,9.94">Cluster-based retrieval using language models</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,430.85,200.92,52.08,9.94">SIGIR-2004</title>
		<meeting><address><addrLine>Sheffield, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,226.24,458.74,9.94;8,90.02,238.84,110.09,9.94" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,313.71,226.24,168.46,9.94">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,251.59,458.55,9.94;8,90.02,264.19,195.98,9.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,181.26,251.59,295.89,9.94">CLARANS: a method for clustering objects for spatial data mining</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,485.09,251.59,54.68,9.94;8,90.02,264.19,79.41,9.94">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1003" to="1016" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,276.91,458.97,9.94;8,90.02,289.51,267.89,9.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,221.92,276.91,278.21,9.94">Improving personalized web search using result diversification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,507.49,276.91,32.70,9.94;8,90.02,289.51,22.08,9.94">SIGIR-2006</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="691" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,302.11,407.00,9.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="8,137.18,302.11,220.63,9.94">Automatic Information Organization and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>McGraw Hill Text</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,314.83,458.70,9.94;8,90.02,327.32,261.24,10.04" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,295.11,314.83,223.67,9.94">A comparison of document clustering techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.02,327.32,221.32,10.04">Proc. of the KDD&apos;2000 Workshop on Text Mining</title>
		<meeting>of the KDD&apos;2000 Workshop on Text Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,340.15,458.56,9.94;8,90.02,352.75,406.40,9.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,259.84,340.15,279.94,9.94;8,90.02,352.75,28.99,9.94">STING: a statistical information grid approach to spatial data mining</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muntz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,126.72,352.75,274.10,9.94">International Conference on Very Large Data Bases VLDB&apos;97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,365.35,458.93,9.94;8,90.02,378.07,449.82,9.94;8,90.02,390.67,50.40,9.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="8,305.33,365.35,234.83,9.94;8,90.02,378.07,253.68,9.94">A distance-relatedness dynamic model for clustering high dimensional data of arbitrary shapes and densities</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Yousri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Ismail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,352.52,378.07,87.65,9.94">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1193" to="1209" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,403.39,458.54,9.94;8,90.02,415.99,229.13,9.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="8,212.70,403.39,265.21,9.94">A risk minimization framework for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,487.37,403.39,52.40,9.94;8,90.02,415.99,105.13,9.94">Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="31" to="55" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,81.22,428.61,458.60,9.94;8,90.02,441.33,272.45,9.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="8,290.26,428.61,249.57,9.94;8,90.02,441.33,126.23,9.94">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.95,441.33,51.87,9.94">SIGIR-2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
