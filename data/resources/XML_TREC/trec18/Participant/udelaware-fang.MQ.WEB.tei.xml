<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.62,135.32,330.01,15.13;1,140.50,157.24,330.24,15.13;1,226.54,179.16,158.17,15.13">Axiomatic Approaches to Information Retrieval -University of Delaware at TREC 2009 Million Query and Web Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.47,211.54,52.48,10.51"><forename type="first">Wei</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hui Fang Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.62,135.32,330.01,15.13;1,140.50,157.24,330.24,15.13;1,226.54,179.16,158.17,15.13">Axiomatic Approaches to Information Retrieval -University of Delaware at TREC 2009 Million Query and Web Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BF24EA4E19807AEFAC49F134233F53BA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report our experiments in TREC 2009 Million Query track and Adhoc task of Web track. Our goal is to evaluate the effectiveness of axiomatic retrieval models on the large data collection. Axiomatic approaches to information retrieval have been recently proposed and studied. The basic idea is to search for retrieval functions that can satisfy all the reasonable retrieval constraints. Previous studies showed that the derived basic axiomatic retrieval functions are less sensitive to the parameters than the other state of the art retrieval functions with comparable optimal performance. In this paper, we focus on evaluating the effectiveness of the basic axiomatic retrieval functions as well as the semantic term matching based query expansion strategy. Experiment results of the two tracks demonstrate the effectiveness of the axiomatic retrieval models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The InfoLab from the ECE department at the University of Delaware participated in both the million query track and the ad hoc task of Web track to evaluate the effectiveness of axiomatic retrieval models.</p><p>Axiomatic retrieval models have recently proposed and studied <ref type="bibr" coords="1,392.97,480.84,10.79,8.76" target="#b0">[1,</ref><ref type="bibr" coords="1,406.94,480.84,7.47,8.76" target="#b1">2,</ref><ref type="bibr" coords="1,417.58,480.84,7.19,8.76" target="#b2">3]</ref>. The basic idea is to search for retrieval functions that satisfy all of the reasonable retrieval functions. Fang and Zhai <ref type="bibr" coords="1,157.02,504.75,11.62,8.76" target="#b1">[2]</ref> derived several basic axiomatic retrieval functions and showed that they are less sensitive to the parameter setting than other existing retrieval functions with comparable optimal performance. To further improve the retrieval performance, the semantic term matching based query expansion method has also been proposed <ref type="bibr" coords="1,345.67,540.62,11.62,8.76" target="#b2">[3]</ref> in the axiomatic retrieval framework. In particular, the semantic similarity between two terms are measured with the mutual information computed over a carefully constructed working set. And the weights of semantically related terms are regulated by a set of reasonable semantic term matching constraints. It has been shown that the proposed semantic term matching method is effective to improve retrieval performance. As a query expansion method, it works equally well as the mixture language model feedback method. It is thus interesting to evaluate such a semantic term matching method in the context of the million query track.</p><p>The retrieval performance depends closely on the choice of the working set used to compute mutual information, since the working set directly affects the quality of the semantically related terms. However, it remains unclear whether the proposed semantic term matching is generally effective for all kinds of working sets and what are the root causes of the worse performance for some working sets. To better understand the above two questions, we tested the method by selecting semantically related terms from three different working sets: (1) the working set constructed from the test collection itself, (2) the working set constructed from the Web search engine snippets; and (3) the working set constructed from the Wikipedia collection. We use the basic retrieval function as well as the expansion methods based on the three working sets in the official runs. Experiment results of MQ 09 show that the Web based method outperforms the other two and the Wikipedia based method may hinder the performance.</p><p>The paper is organized as follows. In Section 2, we explain the general idea of the axiomatic approach and semantic terms matching method. In Section 3, we describe the implementation detail of our system. The experiment results are reported and analyzed in Section 4 and the conclusions are made in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Methods</head><p>In this section, we will first explain the basic idea of the axiomatic approach and then describe the semantic term matching method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Ideas of Axiomatic Retrieval Models</head><p>Previous work <ref type="bibr" coords="2,179.62,308.96,10.79,8.76" target="#b0">[1,</ref><ref type="bibr" coords="2,193.32,308.96,8.30,8.76" target="#b1">2]</ref> has proposed and studied the axiomatic retrieval models, where the relevance is modeled directly with retrieval constraints. With the assumption that retrieval functions satisfying all the reasonable retrieval constraints would perform well empirically, the basic idea of axiomatic retrieval models is to search for retrieval functions that can satisfy all the retrieval constraints. Previous study derived several basic axiomatic retrieval functions <ref type="bibr" coords="2,478.32,356.78,10.58,8.76" target="#b1">[2]</ref>.</p><p>Our preliminary experiments on the data collection of TREC 2008 Million Query track showed that the F2-LOG function <ref type="bibr" coords="2,224.15,380.69,11.62,8.76" target="#b1">[2]</ref> performed best, so we use this retrieval function as the baseline retrieval function in our work. The retrieval function of F2-LOG is shown as follows:</p><formula xml:id="formula_0" coords="2,193.75,406.09,298.68,31.24">S (Q, D) = ∑ t∈Q∩D C(t, Q) × C(t, D) C(t, D) + s + s×|D| avdl × ln N + 1 d f (t)<label>(1)</label></formula><p>where Q is the query, D is the document, C(t, Q) is the term count of term t in Q, |D| is the document length, avdl is the average document length, N is the total number of documents and d f (t) is the document frequency of t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Term Matching Method</head><p>The basic axiomatic retrieval functions rely on syntactic term matching, which can not bridge the vocabulary gaps between documents and queries. To overcome this limitation, the semantic term matching method was proposed in the axiomatic retrieval framework <ref type="bibr" coords="2,435.82,543.48,10.58,8.76" target="#b2">[3]</ref>. In particular, it allows us to incorporate the semantic similarity between query terms and terms in the document into the axiomatic retrieval functions. The method relies on three semantic term matching constraints to balance the importance of the semantic related terms and the original query terms. After incorporating the semantic term matching, the retrieval scores of a single term document {t} for query Q can be computed based on the following functions.</p><formula xml:id="formula_1" coords="2,171.90,616.36,320.53,32.96">S (Q, t) = ∑ q∈Q s(q, t) |Q| , where s(q, t) = { ω(q) t = q ω(q) × β × s(q,t) s(q,q) t q (2)</formula><p>where t is a term in the document, q is a term in query Q, ω(q) is the idf of q and β is the parameter that controls how much we trust the semantically related terms. s(q, t) is the semantic similarity between q and t. Note that the retrieval scores of documents with more than one terms can be derived based on the above function as well as the document growth function discussed in the previous studies <ref type="bibr" coords="3,251.13,110.23,10.79,8.76" target="#b1">[2,</ref><ref type="bibr" coords="3,264.41,110.23,7.19,8.76" target="#b2">3]</ref>. The semantic similarity between terms, i.e., s(q, t) is computed with the mutual information <ref type="bibr" coords="3,118.82,134.14,10.79,8.76" target="#b3">[4]</ref>:</p><formula xml:id="formula_2" coords="3,179.09,139.92,313.34,31.24">s(q, t) = I(X q , X t |W) = ∑ X q ,X t ∈{0,1} p(X q , X t |W)log p(X q , X t |W) p(X q |W)p(X t |W)<label>(3)</label></formula><p>where X q and X t are two binary random variables that denote the presence/absence of query term q and term t in the document. W is the working set to compute the mutual information. We will discuss how to construct the working set in the next section.</p><p>Previous study <ref type="bibr" coords="3,195.70,215.22,11.62,8.76" target="#b2">[3]</ref> proved that the semantic retrieval function can be implemented by expanding the query with the semantic related terms and retrieve documents with basic axiomatic functions, such as F2-LOG. The weight of the expanded term can be set based on S (Q, t) shown in Equation <ref type="formula" coords="3,167.53,251.08,3.74,8.76">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation Details</head><p>We now describe the implementation details of our methods.</p><p>1. Constructing working set to compute term similarity. Equation 3 computes the semantic similarity between terms based on a working set. Previous study <ref type="bibr" coords="3,441.13,340.85,11.62,8.76" target="#b2">[3]</ref> proposed a strategy to construct a better working set based on a corpora In particular, from the corpora, the working set of a query needs to include R relevant documents and N × R random documents. In our experiments, we set R to 20 and N to 19 based on the results reported in the previous study. Note that the R relevant documents can be selected from the top R ranked documents based on the retrieval results over the corpora.</p><p>2. Expanding query with semantically related terms. For each query, we compute the semantic similarity between query term and terms in the working set based on Equation <ref type="formula" coords="3,143.73,444.46,3.74,8.76" target="#formula_2">3</ref>. Top K similar terms are selected for every query term. All the similar terms from a query are combined to generate the expanding term candidates for the query. The similarity between each term candidate and the whole query is computed based on Equation <ref type="formula" coords="3,143.73,480.32,3.74,8.76">2</ref>. The M most similar terms are added to the query with S (Q, t) as their weights. K is set to 1000 and M is set to 20 in the experiments.</p><p>3. Retrieving documents with expanded queries. We can rank documents using F2-LOG retrieval function with the expanded queries.</p><p>In step 1, the semantic similarity among terms are computed based on a working set. Different working sets would give different expanded query terms. To compare the effectiveness of different working sets, we propose to use the following three working sets in the experiments.</p><p>• Collection-based working set: We can use the document collection itself, i.e., category B collection in MQ09, as the corpora to construct the working set. All the expanded terms will come from the document collection.</p><p>• Wikipedia-based working set: We can also use the Wikipedia pages, i.e., a subset of category B collection, as the corpora to construct the working set. Again, all the expanded terms will come from a subset of the document collection. Wikipedia pages are manually written and compiled, they contain knowledge contributed by different people. Intuitively, the quality of a Wikipedia page should be higher than a random web page. Compared with the collection-based working set, this working set might be able to contribute more high quality semantically related terms if there exist Wikipedia pages that are relevant to a query.</p><p>• Web-based working set: Another possible solution is to construct the working set based on the data from Web. In particular, we submit queries to a leading Web search engine and construct the working sets based on the top 100 returned snippets. Clearly, the data indexed by the web search engine should be much larger then the category B collection. Thus, we expect this working set would contribute more semantically related terms that can not be discovered in the document collection.</p><p>4 Experiment results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results of Official Runs</head><p>We submitted five runs to the million query track and three runs to the ad hoc task of Web track. Here are the descriptions of the submitted runs.</p><p>1. UDMQAxBL/UDWAxBL: These are our baseline methods. F2-LOG function is used as the retrieval function. UDMQAxBL is the run submitted to MQ track and UDWAxBL is the run submitted to Adhoc task of Web track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">UDMQAxBLlink:</head><p>We use both the anchor text and document content to rank documents. F2-LOG function is used as the retrieval function.</p><p>3. UDMQAxQE/UDWAxQE: This run uses the semantic term matching method, and the semantically related terms are selected from the collection-based working set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>UDMQAxQEWP: This run uses the semantic term matching method, and the semantically related terms are selected from the Wikipedia-based working set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>UDMQAxWeb/UDWAxWeb: This run uses the semantic term matching method, and the semantically related terms are selected from the Web-based working set.</p><p>The official runs are evaluated with both statMAP and MTC measures <ref type="bibr" coords="4,410.24,485.67,10.58,8.76" target="#b4">[5]</ref>. In the tracks, the default value of β in Equation 2 of semantic matching method is 1.5 according to the training result on Robust04 collection.</p><p>Table <ref type="table" coords="4,158.74,521.54,4.98,8.76" target="#tab_0">1</ref> summaried the results of our five official runs in MQ track. The eMAP.base and statMAP.base in the table denoted the eMAP and statMAP value for queries that our runs contributed to. The eMAP.reuse and statMAP.reuse denoted the eMAP and statMAP for queries that our runs were held out from. We can see that the semantic term matching method can significantly improve the performance. In all evaluations of Table <ref type="table" coords="4,390.42,569.36,3.74,8.76" target="#tab_0">1</ref>, the semantic matching with web collection performed best. The semantic matching with Wikipedia often performed worse than the baseline. The reason is that it can introduce many noise into the original query if there is no related Wikipedia page for the query.</p><p>Table <ref type="table" coords="4,157.96,617.18,4.98,8.76" target="#tab_1">2</ref> listed the eMAP values of MTC evaluation and statMAP values in official reported results of Web track. The basic axiomatic function F2-LOG had the highest eMAP value and query expansion with web collection had the highest statMAP value in our three runs. Note that the queries used in Web track is a subset of those used in MQ track, it is interesting to see that the performance comparison among different methods are not consistent with what we observed in MQ results.  <ref type="table" coords="5,250.59,310.16,3.74,8.76" target="#tab_0">1</ref>, we can make two interesting observations. First, although AXQEWp hurted the performance in most bases, it can improve the retrieval performance over the baseline for hard queries. Second, AXQEWeb can significantly improve the retrieval performance in most cases, but it can not improve the performance for easy queries. Clearly, it suggests that if we could predict query categories correctly, we might be able to get better performance by using different retrieval strategies for different query categories. Table <ref type="table" coords="5,159.49,381.89,4.98,8.76" target="#tab_3">4</ref> showed the top 10 expanded terms of the semantic term matching methods in different query categories. In the query 20215, the AP values of the original query, AxQE, AxQEWp and AxQEWeb are 0.296, 0.206, 0.01, 0.313. We can see that the AxQEWeb method found a lot of semantic related terms, such as abuse, false, lawsuit, overbil, etc., to expand the query. The AxQE and AxQEWP selected some semantic related terms, such as abus, inspector and claim, but they also included a lot of noisy terms, such as beneficiary, byrd, and dingell. These noisy terms made the expanded query to perform worse than the original query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We evaluate the axiomatic retrieval models in the context of the million query track and the ad hoc task of Web track. Both the basic axiomatic retrieval function and the semantic term matching strategy have been shown to be effective based on the results. Moreover, we compare the effectiveness of three working sets which are used to compute the semantic similarities among terms. Different working sets would lead to different expanded terms. Experiment results show that the Web-based working set is the most effective one, while the Wikipediabased working set is the least effective one. Our analysis suggests that the worse performance of Wikipedia-based working set is probably caused by the noises introduced by the Wikipedia when there is no wiki pages related to the given query.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,148.19,105.08,312.38,70.10"><head>Table 1 :</head><label>1</label><figDesc>Results of official runs for Million Query track</figDesc><table coords="5,148.19,116.88,312.38,58.31"><row><cell></cell><cell>AxBL</cell><cell cols="4">AxBLlink AxQE AxQEWp AxQEWeb</cell></row><row><cell>eMAP.base</cell><cell>0.08111</cell><cell>0.05890</cell><cell cols="2">0.09637 0.082188</cell><cell>0.1148</cell></row><row><cell>eMAP.reuse</cell><cell>0.07142</cell><cell>0.05449</cell><cell cols="2">0.07316 0.061861</cell><cell>0.09238</cell></row><row><cell>statMAP.base</cell><cell>0.25498</cell><cell>0.19178</cell><cell>0.20536</cell><cell>0.13347</cell><cell>0.27147</cell></row><row><cell cols="2">statMAP.reuse 0.22081</cell><cell>0.15353</cell><cell cols="2">0.13453 0.087670</cell><cell>0.22976</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,118.82,199.12,373.61,119.80"><head>Table 2 :</head><label>2</label><figDesc>Results of our official runs in Adhoc task of Web trackWe now evaluate the performance of all the five runs in MQ based on query categories. Table3reportd the performance based on different query categories. Although the results were similar to what we observed from Table</figDesc><table coords="5,118.82,208.86,276.51,67.77"><row><cell></cell><cell>AxBL</cell><cell cols="2">AxQE AxQEWeb</cell></row><row><cell>eMAP</cell><cell cols="2">0.04245 0.02401</cell><cell>0.03838</cell></row><row><cell cols="3">statMAP 0.17583 0.13329</cell><cell>0.19986</cell></row><row><cell cols="3">4.2 Results for different query categories</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,126.29,596.19,358.67,96.80"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison for different query categories (measured with statMAP)</figDesc><table coords="5,156.36,607.99,296.05,85.01"><row><cell></cell><cell>AxBL</cell><cell cols="4">AxBLlink AxQE AxQEWp AxQEWeb</cell></row><row><cell>EASY</cell><cell>0.44879</cell><cell>0.32769</cell><cell>0.35647</cell><cell>0.19644</cell><cell>0.43225</cell></row><row><cell cols="2">MEDIUM 0.17732</cell><cell>0.15060</cell><cell>0.12315</cell><cell>0.11231</cell><cell>0.21652</cell></row><row><cell>HARD</cell><cell>0.03835</cell><cell>0.04882</cell><cell>0.02786</cell><cell>0.05242</cell><cell>0.10254</cell></row><row><cell></cell><cell>AxBL</cell><cell cols="4">AxBLlink AxQE AxQEWp AxQEWeb</cell></row><row><cell>PREC</cell><cell>0.22435</cell><cell>0.16076</cell><cell>0.15830</cell><cell>0.11162</cell><cell>0.25343</cell></row><row><cell cols="2">RECALL 0.23868</cell><cell>0.19817</cell><cell>0.18954</cell><cell>0.13323</cell><cell>0.26402</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,124.80,105.08,423.82,189.25"><head>Table 4 :</head><label>4</label><figDesc>Expanded queries in different query categories</figDesc><table coords="6,124.80,116.88,423.82,177.46"><row><cell></cell><cell>Origin</cell><cell></cell><cell>AxQE</cell><cell></cell><cell>AxQEWp</cell><cell></cell><cell></cell><cell>AxQEWeb</cell></row><row><cell>EASY</cell><cell cols="2">query 20643:</cell><cell cols="2">aia, architecure, as-</cell><cell>american,</cell><cell></cell><cell>building,</cell><cell>ahm,</cell><cell>architecture,</cell></row><row><cell></cell><cell>architects</cell><cell>in</cell><cell>sociate,</cell><cell>building,</cell><cell cols="2">connecticut,</cell><cell>con-</cell><cell>construct,</cell><cell>design,</cell></row><row><cell></cell><cell>new jersey</cell><cell></cell><cell cols="2">construct, englewood,</cell><cell cols="3">struct, design, dome,</cell><cell>institute,</cell><cell>license,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>firm,</cell><cell>flemington,</cell><cell cols="3">firm, georgia, illinoi,</cell><cell>lopatcong, millburn,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">hoboken, interior</cell><cell>indiana</cell><cell></cell><cell></cell><cell>nj, stryker</cell></row><row><cell cols="3">MEDIUM query 20215:</cell><cell cols="2">abuse, agency, bene-</cell><cell cols="2">amendment,</cell><cell>byrd,</cell><cell>abuse, attempt, cohen,</cell></row><row><cell></cell><cell cols="2">report medicare</cell><cell cols="2">ficiary, bill, care, de-</cell><cell cols="3">care, claim, deficit,</cell><cell>false, fraudulent, law-</cell></row><row><cell></cell><cell>fraud</cell><cell></cell><cell cols="2">partment, fraudulent,</cell><cell>dingell,</cell><cell></cell><cell>federal,</cell><cell>suit, medicaid, medi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">health, inspector, med-</cell><cell>health,</cell><cell cols="2">healthcare,</cell><cell>care, million, over-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>icaid</cell><cell></cell><cell cols="2">intermediary</cell><cell></cell><cell>billed</cell></row><row><cell>HARD</cell><cell cols="2">query 20101:</cell><cell>chart,</cell><cell>deployment,</cell><cell cols="3">care, career, civilian,</cell><cell>education,</cell><cell>invite,</cell></row><row><cell></cell><cell cols="2">join job corps</cell><cell cols="2">duty, enlist, ethics, hu-</cell><cell cols="3">cloth, employ, force,</cell><cell>largest, nation, recruit,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mor, igoogl, insignia,</cell><cell cols="3">home, ii, labor, occu-</cell><cell>resident, train, visit,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">legislative, marine</cell><cell>pation</cell><cell></cell><cell></cell><cell>vocation, web</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,140.40,412.20,352.04,8.76;6,140.40,424.15,113.17,8.76" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,260.65,412.20,196.57,8.76">A formal study of information retrieval heuristics</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,474.61,412.29,17.82,8.55;6,140.40,424.24,83.74,8.55">Proceedings of SIGIR-04</title>
		<meeting>SIGIR-04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,140.40,444.08,352.03,8.76;6,140.40,456.03,127.66,8.76" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,225.75,444.08,252.55,8.76">An exploration of axiomatic approaches to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,140.40,456.12,98.24,8.55">Proceedings of SIGIR-05</title>
		<meeting>SIGIR-05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,140.40,475.96,352.03,8.76;6,140.40,487.92,179.46,8.76" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,226.39,475.96,266.04,8.76;6,140.40,487.92,34.67,8.76">Semantic Term Matching in Axiomatic Approaches to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,192.20,488.01,98.23,8.55">Proceedings of SIGIR-06</title>
		<meeting>SIGIR-06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,140.40,507.84,199.95,8.76" xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,226.84,507.84,84.76,8.76">Information Retrieval</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,140.40,527.77,352.04,8.76;6,140.40,539.72,137.91,8.76" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,342.77,527.77,145.15,8.76">Million Query Track 2009 Overview</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.19,539.81,97.20,8.55">Proceedings of TREC-09</title>
		<meeting>TREC-09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
