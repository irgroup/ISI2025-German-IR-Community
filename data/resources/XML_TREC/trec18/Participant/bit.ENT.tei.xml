<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.28,149.88,260.02,9.02">Experiments on Related Entity Finding Track at TREC 2009</title>
				<funder ref="#_qQgcjNb">
					<orgName type="full">Chinese National Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.88,161.22,44.20,9.02"><forename type="first">Qing</forename><surname>Yang</surname></persName>
							<email>yangqing2005@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.08,161.22,40.15,9.02"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.63,161.22,59.83,9.02"><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
							<email>cxzhang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.62,161.22,58.09,9.02"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
							<email>zniu@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.28,149.88,260.02,9.02">Experiments on Related Entity Finding Track at TREC 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2DE6D53748E26A4F7B1B01ED4FC35D39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our goal in participating in the TREC 2009 Entity Track is to study whether QA list technique can help improve accuracy of the entity finding task. Also, we take a looking for homepage finding to identify homepages of an entity by training a maximum entropy classifier and a logistic regression models for three types of entity respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">System Overview</head><p>We complete our experimental system architecture as pipeline architecture by devising from OpenEphyra's framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This is Beijing Institute of Technology's first year participating in TREC. For related entity finding track, we mainly focus on employing pipeline architecture to model this track, indexing and retrieving by indri and making use of OpenNLP's ME classifier to identify extracted entities homepages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Entity Finding Task</head><p>The related entity finding task is new to be proposed by NIST this year. This task is defined as the following:</p><p>Given an input entity, by its name and homepage, the type of the target entity, as well as the nature of their relation, described in free text, find related entities that are of target type, standing in the required relation to the input entity.</p><p>This task shares similarities with both expert finding (in that we need to return not "just" documents) and homepage finding (since entities are uniquely identified by their homepage). However, approaches to address this task need to generalize to multiple types of entities (beyond just people) and return the homepages of multiple entities, not just one. Also, the topic defines a focal entity to which returned homepages should be related. <ref type="bibr" coords="1,245.54,540.55,11.71,9.02">[1]</ref> Fig. <ref type="figure" coords="2,207.42,366.87,3.38,8.10">1</ref>. The Related Entity Finding System Architecture.</p><p>We outline the retrieval framework as above. From TREC-supplied query topics, we first analyze the narrative of every query topic and extract keywords and terms. Second we employ BagofWordsGenerator and QueryReformationGenerator to rewrite query strings. Then we send query strings to the indri search engine, and get results. The granularity of results is focused text snippet rather than document. From the focused text snippets, we employ some OpenNLP components and Stanford's parser to identify target typed named entities. By counting number of occurrences in focused text snippets at sentence level, we rank entities by the reverse order of occurrences. We get the top 150 entities and post the top 150 entities' name to indri search engine respectively. Using Maximum entropy classifier to score the returned web pages as the related entity's homepage candidate, we rank the entity's homepage candidates by the scores in reverse order. Finally, we rerank those ranked entities by just filtering out those entities which have no homepages whose scores are above threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Query Topic Parsing</head><p>As for a QA, Ephyra spends much effort to analyze question syntactically and semantically. To identify answer type, it employs machine learning scheme to train answer patterns and identify answer types. This tricky phase is not necessary for the related entity track because the target entity type is explicitly given. Also, OpenEphyra employs wordnet to expand query terms. From the expanded query string, it generates some irrelevant terms. Then we remove those irrelevant words manually to avoid topic drift. As for the explicit entity name, we just add it to the query string without expanding it. To take the first query topic for an example, OpenEphyra will generate two query strings. One is focus words i.e. blackberry Carriers makes phones, weighted score 1.0. The query to send to indri is as following:</p><p>#combine [passage100:50](blackberry Carriers makes phones). The passage length and increment size are set 100 and 50 separately experimentally. Variable lengths of window size will generate different results. It is challenging to decide the reasoned window size. The passage length constructed a context for locating related entities. It represents the proximity between the source entity and the target related entity.</p><p>The other is expanded terms, i.e. blackberry (Carriers OR toter OR bearer) makes (phones OR telephone OR "telephone set"). Obviously, toter and bearer are synonymous with carrier in wordnet. But it not suitable for this query topic. We just remove them manually. The converted query string is such as the following:</p><p>#combine [passage100:50](blackberry Carriers #or(phones telephone "telephone set")).</p><p>This query string has weighted score 1.5. The weight score is addresses as the degree how the generated query string matches the narrative of the query topic. It can be considered as a degree of proximity between target entity and input entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Identification</head><p>In this task, the type of target entity is restricted in three types: person, organization and product. Generally speaking, the first two types are easier to identify from focused snippets. However, for the product type, it is rather difficult to be identified correctly. To deal with this issue, we resort to wikipedia online knowledge database whose pages always have a category label. We made a hardworking to find that those introductions, productions, products, games, software, hardware etc. category labels are almost classified into product type. It helps us to extract 43,393 product names. Also, by using the same method, we extracted 18,181 organization names and 118,002 person names. In this experimental system, we employ OpenEphyra's NETagger to complete named entity identification. OpenEphyra's NETagger combines model-based, patternbased and list-based named entity taggers. It is natural for us to add a product list to hope to improve product identification performance. As for the other easier to identify named entity types resort to StanfordNeTagger.</p><p>As for StanfordNeTagger, we load ner-eng-ie.crf-3-all2006-distsim.ser.gz serialized model which can label: PERSON, ORGANIZATION, and LOCATION entities. The model is trained on data from CoNLL, MUC6, MUC7, and ACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Entity Candidates Ranking</head><p>It is well known that the search results ranking is not necessary responding to those extracted entities ranking. We apply the following formula in a probabilistic model to rank related entity candidates. Result, which represents the relevancy of searched result (in this context its granularity is passage) to the search string. N refers to Number of redundancy of the focused snippets which reside the same entity. α , β , γ are the coefficient respectively. In this experimental setting, it is simply to set all the coefficients 1. Besides that, the result's score are formalized to 1 if the result is in the top N. You may notice that the effect of the result ranking is taken into consideration implicitly for we extract entities from the result passages from top N. Those which are not in the top N are ignored definitely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">ME-Based Entity Homepages Classifier</head><p>We model the entity homepage identifying as a binary-class classifying problem. Our aim is to set the probability to represent the likelihood of one URL is a homepage rather than to make a binary decision -yes or no. [4] proposed a machine learning approaches to predict the correct homepage in response to a user's homepage finding query. He generates a binary decision tree to predict whether a URL is a homepage URL or not. Obviously, it is more suitable to employ probability than binary decision in this task. Table . 2.1 demonstrate those attributes. The ME model then is applied to the results returned by the mixture of context language models retrieval system, in hopes that we can filter out most of the irrelevant web pages in these returned webpage lists.</p><p>Additionally, we normalize URL by using BasicURLNormalizer which is extracted from nutch-0.9 before we extract type of URL. As for variants of entity name, we analyze homepage_en.nt from DBPedia and get the following rule to generate variants of the specified entity name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table. 2.2 Variant form rules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Replace blank space with "_","+","%20|","" respectively 2.</p><p>Replace "'s" with "s", "", respectively 3.</p><p>Number of characters in Abbreviation is equal or greater then three 4.</p><p>Concatenate the first character of every word in the specified entity name including stopwords or excluding stopwords respectively. 5.</p><p>For two words in the specified entity name, combine first three to five characters of each word to generate abbreviations.</p><p>By using these variants of the specified entity name, it gets 99.9% in finding entity names in their homepages. It represents that web designers will always naming their homepage' URL from related entity names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.6.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.7.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regression Model for Homepage Finding</head><p>As for ME classifier, it is not easy to interpret the generated model from training materials. We leverage a logistic regression model for homepage finding also. To compare the effective performance the two models, the result is explained in the section 2.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Homepage Finding</head><p>The procedure to identify the extracted entities' homepages involves two phrases. In the first phrase, we generated a Max entropy homepage classifying model to predict the probability of a URL is the specified entity's homepage. The second phrase is to employ a mixture of context language models, which can easily be expressed in the Indri query language to find which web page is most relevant to the specified entity. For example, for the entity name "BlackBerry", the following query will be constructed:</p><p>#wsum(5.0 #1(BlackBerry).(title) 3.0 #1(BlackBerry).(anchor) 1.0 #1(BlackBerry)) After send the constructed query to Indri, we get so many homepage candidates. Then, we employ beforehand generated homepage model to predict the probability of whether a webpage is the specified entity name's homepage or not. For the task is just to return three URL as an entity name at most, we rank these homepage candidates according to the predicted probability in descending order and select the top 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.8.</head><p>2.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.10.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Entity Reranking</head><p>In this phrase, we have already extracted related entities and their homepages. We take a simple approach to rerank the entity list by filtering whose homepages' probability scores are all below 0.5 which represent that the entity has no homepage at all. Naturally, by definition, every entity will have a homepage at least. Then the entities which are considered as no homepages will be dropped off. Of course, this decision depends on the precision of the homepage ME classifier and the coverage of the used corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We ran our index builds and our queries on an IBM 366 server and data are located on SCSI disk made in RAID5. For conveniently handling we divide the index in six sections, which occupy 649G disk space totally. Index size is the total size of the index on disk including both the inverted file and compressed collection. All indexed documents are 50,220,423. For the slow index speed, we did not index anchor text but just title and heading fields. Documents are stemmed with the Krovetz stemmer and stopped using a standard list of 421 common terms. The metadata indexed include docno and url.</p><p>We used the full collection and simply handled all documents as HTML documents. That is, we did not resort to any special treatment of document types, nor did we exploit the internal document structure that may be present; instead, we represented all documents as plain text. We did not take special consideration on those wikipedia documents in the corpus.</p><p>Supporting documents When we extract context from a document we also store its document id and the entity the context belongs too. We return up to 3 supporting documents from this set for each entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions</head><p>The official test set contained only 20 queries. Given the facts that this is a new task, a new collection, and it supplies a relatively small number of topics, evaluation will primarily focus on analysis of the results and runs on a per-topic basis, rather than on average measures. The normalized discounted cumulative gain (nDCG) is an important measure in the official results. There are many methods to evaluate the system. For example, we can evaluate the system by the relevance of the normal homepages, Wikipedia homepages or names of entities. Table <ref type="table" coords="6,375.31,639.43,5.01,9.02">1</ref> lists the evaluation results of our two runs(we only submit BITDLDE09Run)that are evaluated with different output fields. NAME denotes the names of result entities, HP denotes the normal homepages of result entities and WP denotes the Wikipedia pages of result entities. The official results are the values in the first row. They evaluated the runs according to the relevance of the primary homepages of entity and did not take the Wikipedia homepages or entity names into consideration. In other rows, we combine different output fields to test the change of performance. The results shows that the performance of each run improves when consider Wikipedia homepages in evaluation. The reason is that the Wikipedia pages are of high quality and make it easy for system to find the homepage. On the other hand, using entity names to evaluate decreases the performance. It shows that directly finding the entity names is more difficult than returning the whole web pages. We make a statistic for retrieval performance without discard any homepage for identified entities. It shows that the retrieval ratio is low (0.3024 for all relevant documents and 0.0898 primary homepages). Obviously, it needs to improve query formations to raise recall greatly. The low recall ratio makes a great effect for the low performance for the next step for homepage finding.</p><p>In future work, there are a number of things for us to explore. First, we will explore more efficient way to automatically construct queries to improve great recall. It may base the assumption described in Clarke: query terms are likely to appear in close proximity to each other within relevant documents. This technique is expressed by Donald Metzler etc. in TREC 2004 in the terabyte track which is used to evaluate Indri search engine. <ref type="bibr" coords="7,208.82,469.33,13.70,9.02" target="#b1">[2]</ref>Second is on homepage finding for a specified named entity. <ref type="bibr" coords="7,124.74,480.85,11.71,9.02">[3]</ref> found a prior based on the type of the URL to be a very effective source of information. Our Max Entropy Classifier will take the different distribution for different type entities into consideration. Third, we will employ language model to construct entity model from the selected homepage candidates to improve recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,396.54,652.87,5.37,10.89;3,337.26,652.87,59.85,10.89;3,288.72,652.87,33.62,10.89;3,235.98,652.87,34.96,10.89;3,204.42,652.87,12.11,10.89;3,332.40,648.28,4.98,15.66;3,281.76,648.28,6.63,15.66;3,228.30,648.28,7.63,15.66;3,324.06,648.97,6.63,14.82;3,272.64,648.97,6.63,14.82;3,219.84,648.97,6.63,14.82;3,136.08,668.52,334.55,9.02;3,124.74,679.98,345.84,9.02"><head></head><label></label><figDesc>Query String which represents the fitness of the generated query string to express the nature of related entity with the source entity. R refers to the search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,142.20,379.95,311.12,306.22"><head>Table . 2.1 Attributes vector</head><label>.</label><figDesc></figDesc><table coords="4,142.20,408.42,311.12,277.76"><row><cell>URL length</cell><cell>the number of characters in the URL</cell></row><row><cell>URL depth</cell><cell>the number of slashes in the UR</cell></row><row><cell>URL type</cell><cell>four types of URL: root, subroot, path, file. he</cell></row><row><cell></cell><cell>type of URL which is proposed by</cell></row><row><cell></cell><cell>UTwente/TNO in TREC-2001 homepage</cell></row><row><cell></cell><cell>finding track</cell></row><row><cell>Entity in URL</cell><cell>whether the specified entity name is present in</cell></row><row><cell></cell><cell>the URL</cell></row><row><cell>Variants of entity in URL</cell><cell>manually devise many variants of the entity</cell></row><row><cell></cell><cell>name which are likely to be used by web</cell></row><row><cell></cell><cell>designers and decide whether one of those</cell></row><row><cell></cell><cell>variants exists in the URL</cell></row><row><cell>Position type</cell><cell>the position type refers to the above defined</cell></row><row><cell></cell><cell>types of URL. The position type represents the</cell></row><row><cell></cell><cell>entity name or its variants exists which part in</cell></row><row><cell></cell><cell>the URL</cell></row><row><cell>Entity in page title</cell><cell>whether entity name or its variants exist in</cell></row><row><cell></cell><cell>page's title</cell></row><row><cell>Keyword</cell><cell>whether page's title contains with a keyword;</cell></row><row><cell></cell><cell>these keywords are "official", "home",</cell></row><row><cell></cell><cell>"homepage"</cell></row><row><cell>Length of title</cell><cell>number of characters in title</cell></row><row><cell>Occurrence of entity in</cell><cell>the number of entity name occurrence in the</cell></row><row><cell>title</cell><cell>title</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,125.22,253.53,344.90,85.90"><head>Table 2 .3: nDCG and P@10 results of our runs with different output fields in Entity track</head><label>2</label><figDesc></figDesc><table coords="7,147.36,270.48,297.35,68.96"><row><cell>RunID</cell><cell cols="2">BITDLDE09Run</cell><cell cols="2">LogisticRegressionRun</cell></row><row><cell></cell><cell>nDCG</cell><cell>P@10</cell><cell>nDCG</cell><cell>P@10</cell></row><row><cell>HP</cell><cell>0.0416</cell><cell>0.0200</cell><cell>0.0499</cell><cell>0.0400</cell></row><row><cell>HP+NAME</cell><cell>0.0379</cell><cell>0.0200</cell><cell>0.0471</cell><cell>0.0400</cell></row><row><cell>HP+WIKI</cell><cell>0.0705</cell><cell>0.1250</cell><cell>0.0895</cell><cell>0.1150</cell></row><row><cell>HP+NAME+WIKI</cell><cell>0.0731</cell><cell>0.1250</cell><cell>0.0879</cell><cell>0.1150</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported by the grant from <rs type="funder">Chinese National Natural Science Foundation</rs> (No: <rs type="grantNumber">60705022</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qQgcjNb">
					<idno type="grant-number">60705022</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,129.24,587.90,72.78,10.80" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,145.74,645.75,324.90,8.10;7,145.74,656.08,61.22,8.10" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,391.66,645.75,78.97,8.10;7,145.74,656.08,53.51,8.10">Indri at TREC 2004: Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Howard Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.24,149.43,320.36,8.10;8,145.74,159.76,324.80,8.10;8,145.74,170.08,312.26,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,318.14,149.43,152.46,8.10;8,145.74,159.76,62.46,8.10">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,224.70,159.76,245.83,8.10;8,145.74,170.08,285.78,8.10">Proceedings of the Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</title>
		<meeting>the Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
