<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.79,112.05,338.41,15.12">Minimal Test Collections for Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,85.35,144.53,74.74,10.48"><forename type="first">Ben</forename><surname>Carterette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,169.08,144.53,86.25,10.48"><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.17,144.53,87.08,10.48"><forename type="first">Aparna</forename><surname>Kailasam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.80,144.53,88.70,10.48"><forename type="first">Divya</forename><surname>Muppaneni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,460.47,144.53,66.18,10.48"><forename type="first">Lekha</forename><surname>Thota</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.79,112.05,338.41,15.12">Minimal Test Collections for Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B674F5D92FA1321F9EE7D3EA57115C14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Information Retrieval Lab at the University of Delaware participated in the Relevance Feedback track at TREC 2009. We used only the Category B subset of the ClueWeb collection; our preprocessing and indexing steps are described in our paper on ad hoc and diversity runs <ref type="bibr" coords="1,382.44,234.70,14.61,8.74" target="#b9">[10]</ref>.</p><p>The second year of the Relevance Feedback track focused on selection of documents for feedback. Our hypothesis is that documents that are good at distinguishing systems in terms of their effectiveness by mean average precision will also be good documents for relevance feedback. Thus we have applied the document selection algorithm MTC (Minimal Test Collections) developed by Carterette et al. [6,<ref type="bibr" coords="1,446.92,282.53,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,457.53,282.53,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="1,468.13,282.53,7.75,8.74" target="#b4">5]</ref> that is used in the Million Query Track [2,<ref type="bibr" coords="1,195.61,294.48,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="1,206.77,294.48,7.75,8.74" target="#b7">8]</ref> for selecting documents to be judged to find the right ranking of systems. Our approach can therefore be described as "MTC for Relevance Feedback".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">MTC Overview</head><p>MTC is a greedy algorithm for selecting documents to be judged. It takes as input a set of relevance judgments (possibly empty) and a set of ranked lists of documents for a query or set of queries; as output it produces a set of "importance weights" for each unique document ranked for each query. The weights reflect the utility of the document for ranking the input lists by their mean average precision. After a document or set of documents has been judged, the algorithm can be run again with those judgments and the same input runs; the weights in computes will then be based on any existing judgments as well as the runs. Note that the weights do not necessarily have anything to do with relevance; any correlation between the weights and document relevance is unintended. In fact, since judgments of nonrelevance often say more about the difference in MAP between two systems than judgments of relevance, it is more likely that the weights negatively correlate to relevance.</p><p>MTC judgments can be used with trec eval, making the assumption that unjudged documents are not relevant. This is not optimal, however; instead, the MTC method uses the judgments to fit a classifier, which it then uses to predict the relevance of unjudged documents. These predicted relevance judgments are then used to calculate expected values of MAP; the maximum-likelihood ranking of systems is the one by expected MAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MTC for Relevance Feedback</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phase 1: Selecting Documents for Feedback</head><p>The traditional approach to relevance feedback is that a user provides feedback on some of the top documents retrieved by some system; that system then uses that feedback to rerank documents (often by expanding the original query). The key difference for the MTC approach is that there are a set of systems from which a few documents are selected to ask for feedback. After receiving feedback, the documents will be reranked.</p><p>Therefore the first step in applying MTC is to generate several different ranked lists for each query. Using the Indri retrieval system, we applied the following methods, each generating a different ranked list:</p><p>1. basic query-likelihood language modeling with Dirichlet smoothing; 2. the Markov Random Field (MRF) model of Metzler &amp; Croft <ref type="bibr" coords="2,364.32,75.16,14.61,8.74" target="#b12">[13]</ref>;</p><p>3. pseudo-relevance feedback with external query expansion (top 10 documents retrieved by Google for the same query) <ref type="bibr" coords="2,170.60,106.14,14.61,8.74" target="#b10">[11]</ref>;</p><p>4. maximum marginal relevance ranking <ref type="bibr" coords="2,264.14,125.16,9.96,8.74" target="#b2">[3]</ref>;</p><p>5. greedy similarity-based pruning, as described by Carterette &amp; Chandar <ref type="bibr" coords="2,412.75,144.19,10.52,8.74" target="#b6">[7]</ref> (also used in the diversity task for the Web track <ref type="bibr" coords="2,198.85,156.14,14.76,8.74" target="#b9">[10]</ref>);</p><p>6. various automatically-generated queries using Indri operators like #uwN, #odN, #weight;</p><p>7. weighting the appearance of query terms in title and heading fields higher than the body field.</p><p>In all, 11 ranked lists for each query were used as input to MTC. Note that the pseudo-feedback and other re-ranking approaches were used to generate some of these; our RF track experiments are completely separate from such fully-automatic approaches. MTC produced a weight for each unique document retrieved by at least one of the 11 runs. The top 5 highest-weighted documents were selected to be judged in Phase 1 of the track (the udel.1 Phase 1 submission; our udel.2 submission did not use MTC and is described in our paper on ad hoc and diversity <ref type="bibr" coords="2,516.42,272.09,14.76,8.74" target="#b9">[10]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phase 2: Query Expansion and Re-ranking</head><p>Within the MTC framework, there are several possible ways to use the Phase 1 judgments for relevance feedback:</p><p>1. use them to evaluate the input lists, then choose the top-performing list as the final ranking; 2. use them to train a relevance classifier, then use the predictions of relevance from that classifier to rank documents in decreasing order of relevance, with this ranking being the final ranking shown to the user;</p><p>3. use them to train a relevance classifier, then use the predictions of relevance from that classifier to expand the original query; re-rank documents using the expanded query.</p><p>The structure of the RF track required that we choose one of these. Based on some limited experiments with previous years' TREC data, we chose the third approach for our official submissions. In Section 3 we will evaluate all three, so we will describe each in more detail here. Each of these approaches can be applied to any set of feedback judgments. We used each approach for each set of Phase 1 judgments we received.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Selecting the Top-Performing Ranking</head><p>This is a very straightforward application of MTC: use the judgments and the MTC evaluation to rank the input systems, then choose the best one as the final ranking. In some sense this is using relevance feedback to select among possible models/ranking algorithms rather than perform any query expansion or reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Ranking Documents by Probability of Relevance</head><p>As described above, MTC requires probabilities of relevance to evaluate systems. We could use these probabilities to directly rank documents. If our classifier is any good, the new ranking should perform well compared to any input ranking. In practice, MTC does not actually require good estimates of relevance in order to produce good rankings of systems (they only need be "good enough", and the evaluation will tolerate a lot of error in these estimates), and therefore we cannot necessarily expect the classifier to be very good.</p><p>Our classifier is a logistic linear model. We use features extracted from the input runs reflecting how well they were able to rank the judged documents; the procedure is described in detail by Carterette <ref type="bibr" coords="2,494.66,683.31,9.96,8.74" target="#b3">[4]</ref>. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q query Phase 1 eval shows performance over all queries. The horizontal line is at 50% performance, where udel.1 is the median Phase 1 set. We were better than the median in 58% of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Using Probabilities of Relevance for Query Expansion</head><p>Instead of ranking documents directly by probabilities of relevance, knowing that they are quite errorful, we can instead use them to do some query expansion. In some sense this results in averaging over a large number of documents and thus if our classifier is "good enough" it may provide a decent expanded query. We used the same classifiers and features as in the previous section to get probabilities of relevance for unjudged documents. Documents were ranked by these probabilities (filling in judgments from Phase 1 if known); this ranking was then used to estimate a relevance model as described by Lavrenko &amp; Croft <ref type="bibr" coords="3,509.93,382.75,14.61,8.74" target="#b11">[12]</ref>. A relevance model gives a probability to each term in the top ranked documents; these probabilities are based on the term frequencies, collection frequencies, and probabilities of relevance of those documents. This model is then used to re-rank the collection to produce a final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phase 1: Document Selection</head><p>The official Phase 1 evaluation is based on comparing a set of Phase 1 judgments to other Phase 1 judgments used as input to the same Phase 2 approach. For example, in Phase 2 we used Phase 1 judgments SIEL.1, Sab.1, UCSC.1, WatS.1, fub.1, twen.1, udel.1, and udel.2. The effectiveness of udel.1 is the ratio of the number of these Phase 1 sets that resulted in worse Phase 2 results than udel.1 to the total number of Phase 2 results (removing ties).</p><p>Our Phase 1 submission udel.1 based on MTC selection from 11 (automatic) input runs outperformed 82.14% of the other Phase 1 submissions used for the same Phase 2 approach (aggregating over different Phase 2 approaches and averaging over queries). Performance on individual queries is shown in Figure <ref type="figure" coords="3,522.39,577.45,3.87,8.74">1</ref>.</p><p>In light of the fact that MTC's document weights are not intentionally correlated to relevance, it may be worth looking at the proportion of Phase 1 documents found to be relevant. In our case, 39.6% of the documents selected by MTC were judged relevant. This was by no means the greatest of any Phase 1 submission; we ranked 9th among all Phase 1 submission by this measure. Of course, the goal of MTC is not to select relevant documents, but to select documents that are good at distinguishing between systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phase 2: Relevance Feedback</head><p>We were assigned eight Phase 1 sets to apply our relevance feedback approaches to: SIEL.1, Sab.1, UCSC.1, WatS.1, fub.1, twen.1, udel.1, and udel.2. For each of these sets, our official Phase 2 submission was based on the third approach described in Section 2.2: train a relevance classifier using the judgments in the set, use that classifier to predict the relevance of the unjudged documents, and use those predictions (along with any judgments in the set) to estimate a relevance model (a weighted expanded query). The final ranking is then based on ranking the collection to the relevance model query. As a baseline (denoted base), we used traditional pseudo-feedback relevance modeling based on an initial ranking by query-likelihood. Since we used the category B subset, the official evaluation measures are statMAP (a low-bias estimate of MAP based on a sample of documents) and expected MAP (described above). Table <ref type="table" coords="4,459.11,360.25,4.98,8.74" target="#tab_1">1</ref> shows results for our official submissions, comparing our feedback approach with different Phase 1 inputs. Though the two measures disagree on the ranking, they agree that our udel.1 submission provided better Phase 2 results than any other set, suggesting that MTC selection is the best way to select documents if an MTC-like approach is to be used in reranking (though it is unlikely these differences are significant). Note, however, that by the official eMAP scores, none of the Phase 1 sets outperformed blind feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional Results</head><p>Here we present some additional analysis and results outside of the official track results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Confidence in Pairwise Differences</head><p>The eMAP evaluation can be used to estimate the degree of confidence in pairwise differences between systems. These confidence scores can give us some idea of how "definite" the ranking is: low confidences (near 0.5) indicate that more relevance judgments could possibly cause the two systems to swap; high confidences (near 1.0) indicate that the systems are unlikely to swap even with more judgments. Table <ref type="table" coords="4,535.01,556.93,4.98,8.74" target="#tab_2">2</ref> shows the confidences in the difference in eMAP for all pairs of runs.</p><p>While many of the adjacent pairs in the ranking have a fairly good chance of swapping, the baseline is unlikely to swap with any other ranking with more relevant judgments. At best, udel.1 could swap with base to become the best ranking with about 13% probability.</p><p>Because the runs have been residualized (Phase 1 judgments removed), these confidence scores should be taken with a grain of salt. Nevertheless, they provide a rough guide to interpreting the eMAP scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alternative Approaches to Feedback</head><p>In section 2.2 we described three possible approaches to relevance feedback that fit within the general MTC framework. Table <ref type="table" coords="5,153.86,265.99,4.98,8.74" target="#tab_3">3</ref> shows a comparison of these different approaches with the different Phase 1 sets used as input. Note that the best input was the external expansion run in all but two cases; the only reason the evaluation results are different is slight differences in which Phase 1 documents were removed before evaluation. Though the ranking of Phase 1 sets by statMAP and eMAP is roughly the same in both the probability ranking and RM methods, the RM method achieved much better results. We conclude from this that RM does have the ability to "improve" the probability ranking, partly by taking into account the original query, and partly by averaging over all documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our results suggest that a small number of documents selected by MTC are better for training a relevance model than any other selection process we compared to. This may have to do with the active-learning "feel" of MTC. Of course, in this case we would have been better off simply using MTC to select the best input run, but because (a) that run is a web expansion run and is thus capitalizing on years of research by Google without providing any insight into retrieval, and (b) that would not have been interesting from a research perspective, we did not do that. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,112.34,468.00,145.52"><head></head><label></label><figDesc>Figure 1: udel.1 performance on each of the first 50 queries and over all queries. The vertical dashed line</figDesc><table coords="3,102.99,112.34,393.47,101.71"><row><cell>0.8</cell><cell></cell></row><row><cell>0.6</cell><cell></cell></row><row><cell>0.4</cell><cell></cell></row><row><cell>0.2</cell><cell></cell></row><row><cell>8 34 9 37 3 39 25 30 41 45 14 17 46 22 10 38 29 21 5 36 19 50 44 32 35 48 18 47 13 2 26 1 27 40 12 4</cell><cell>7 28 33 11 24 31 16 15 43 all 6 42 23 49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,73.96,468.00,151.10"><head>Table 1 :</head><label>1</label><figDesc>Official Phase 2 evaluation results for each Phase 1 input (sorted by statMAP). Bold text indicates the best result in the column.</figDesc><table coords="4,233.36,73.96,145.29,117.01"><row><cell cols="3">Phase 1 set statMAP eMAP</cell></row><row><cell>Sab.1</cell><cell>0.1092</cell><cell>0.0328</cell></row><row><cell>SIEL.1</cell><cell>0.1311</cell><cell>0.0355</cell></row><row><cell>WatS.1</cell><cell>0.1387</cell><cell>0.0367</cell></row><row><cell>twen.1</cell><cell>0.1443</cell><cell>0.0383</cell></row><row><cell>udel.2</cell><cell>0.1480</cell><cell>0.0350</cell></row><row><cell>base</cell><cell>0.1689</cell><cell>0.0421</cell></row><row><cell>fub.1</cell><cell>0.1702</cell><cell>0.0377</cell></row><row><cell>UCSC.1</cell><cell>0.1720</cell><cell>0.0382</cell></row><row><cell>udel.1</cell><cell>0.1762</cell><cell>0.0393</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.00,73.96,468.00,138.59"><head>Table 2 :</head><label>2</label><figDesc>Confidences between Phase 2 runs with different Phase 1 input sets. Each value is the probability that the corresponding two runs would not swap in the ranking if more relevance judgments were available.</figDesc><table coords="5,120.67,73.96,370.66,105.06"><row><cell>run</cell><cell cols="6">udel.2 SIEL.1 WatS.1 fub.1 UCSC.1 twen.1 udel.1</cell><cell>base</cell></row><row><cell>Sab.1</cell><cell>0.8078</cell><cell>0.9216</cell><cell>0.9349 0.9737</cell><cell>0.9789</cell><cell>0.9836</cell><cell cols="2">0.9939 0.9998</cell></row><row><cell>udel.2</cell><cell></cell><cell>0.5638</cell><cell>0.7813 0.8376</cell><cell>0.8783</cell><cell>0.9594</cell><cell cols="2">0.9685 0.9930</cell></row><row><cell>SIEL.1</cell><cell></cell><cell></cell><cell>0.6766 0.8128</cell><cell>0.8425</cell><cell>0.8601</cell><cell cols="2">0.9257 0.9905</cell></row><row><cell>WatS.1</cell><cell></cell><cell></cell><cell>0.6458</cell><cell>0.6975</cell><cell>0.7895</cell><cell cols="2">0.8505 0.9696</cell></row><row><cell>fub.1</cell><cell></cell><cell></cell><cell></cell><cell>0.5892</cell><cell>0.5800</cell><cell cols="2">0.7563 0.9177</cell></row><row><cell>UCSC.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5239</cell><cell cols="2">0.7082 0.8960</cell></row><row><cell>twen.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.6539 0.9112</cell></row><row><cell>udel.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8278</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.00,73.96,468.00,273.29"><head>Table 3 :</head><label>3</label><figDesc>Different feedback approaches using different Phase 1 inputs. Each cell contains statMAP and eMAP for the pair. The last column contains our official submission results. Bold text indicates the best statMAP and eMAP result in each column.</figDesc><table coords="6,187.49,73.96,237.03,227.52"><row><cell cols="4">Phase 1 set best input prob ranking prob + RM</cell></row><row><cell>Sab.1</cell><cell>0.2285</cell><cell>0.0666</cell><cell>0.1092</cell></row><row><cell></cell><cell>0.0473</cell><cell>0.0118</cell><cell>0.0328</cell></row><row><cell>SIEL.1</cell><cell>0.2300</cell><cell>0.0737</cell><cell>0.1311</cell></row><row><cell></cell><cell>0.0476</cell><cell>0.0126</cell><cell>0.0355</cell></row><row><cell>WatS.1</cell><cell>0.2251</cell><cell>0.1180</cell><cell>0.1387</cell></row><row><cell></cell><cell>0.0475</cell><cell>0.0137</cell><cell>0.0367</cell></row><row><cell>twen.1</cell><cell>0.1044</cell><cell>0.1129</cell><cell>0.1443</cell></row><row><cell></cell><cell>0.0135</cell><cell>0.0136</cell><cell>0.0383</cell></row><row><cell>udel.2</cell><cell>0.0983</cell><cell>0.1193</cell><cell>0.1480</cell></row><row><cell></cell><cell>0.0131</cell><cell>0.0141</cell><cell>0.0350</cell></row><row><cell>base</cell><cell>0.1689</cell><cell>0.1689</cell><cell>0.1689</cell></row><row><cell></cell><cell>0.0421</cell><cell>0.0421</cell><cell>0.0421</cell></row><row><cell>fub.1</cell><cell>0.2260</cell><cell>0.1201</cell><cell>0.1702</cell></row><row><cell></cell><cell>0.0473</cell><cell>0.0146</cell><cell>0.0377</cell></row><row><cell>UCSC.1</cell><cell>0.2228</cell><cell>0.1407</cell><cell>0.1720</cell></row><row><cell></cell><cell>0.0467</cell><cell>0.0147</cell><cell>0.0382</cell></row><row><cell>udel.1</cell><cell>0.2218</cell><cell>0.1441</cell><cell>0.1762</cell></row><row><cell></cell><cell>0.0470</cell><cell>0.0148</cell><cell>0.0393</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,92.48,507.03,447.52,8.74;5,92.48,518.99,231.35,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,478.58,507.03,61.42,8.74;5,92.48,518.99,87.99,8.74">Million Query Track 2008 overview</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,201.84,518.99,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.48,538.91,447.52,8.74;5,92.48,550.87,295.58,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,294.01,538.91,246.00,8.74;5,92.48,550.87,152.23,8.74">Vrigil Pavlu, Blagovest Dachev, and Evangelos Kanoulas. Million Query Track 2007 overview</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,266.07,550.87,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.48,570.79,447.52,8.74;5,92.48,582.75,351.10,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,259.03,570.79,280.97,8.74;5,92.48,582.75,139.24,8.74">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,253.48,582.75,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.48,602.67,415.04,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,164.89,602.67,199.48,8.74">Robust test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,385.55,602.67,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,92.48,622.60,447.52,8.74;5,92.48,634.55,175.01,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,166.03,622.60,290.50,8.74">Low-Cost and Robust Evaluation of Information Retrieval Systems</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>University of Massachusetts Amherst</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="5,92.48,654.48,447.52,8.74;5,92.48,666.43,226.90,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,347.37,654.48,192.63,8.74;5,92.48,666.43,15.94,8.74">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramesh</forename><forename type="middle">K</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,129.27,666.43,90.79,8.74">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,368.40,447.52,8.74;6,92.48,380.36,176.67,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,261.11,368.40,278.89,8.74;6,92.48,380.36,34.67,8.74">Probabilistic models of novel document ranking for faceted topic retrieval</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,147.87,380.36,89.20,8.74">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,400.28,447.52,8.74;6,92.48,412.24,134.45,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,383.06,400.28,152.46,8.74">Million Query Track 2009 overview</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,104.93,412.24,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,432.16,447.52,8.74;6,92.48,444.12,189.41,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,271.61,432.16,247.39,8.74">Hypothesis testing with incomplete relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,92.48,444.12,89.20,8.74">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="643" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,464.04,447.52,8.74;6,92.48,476.00,345.56,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,489.97,464.04,50.03,8.74;6,92.48,476.00,202.21,8.74">Ad hoc and diversity retrieval at the university of delaware</title>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aparna</forename><surname>Kailasam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Divya</forename><surname>Muppaneni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lekha</forename><surname>Thota</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,316.05,476.00,89.77,8.74">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,495.92,447.52,8.74;6,92.48,507.88,447.52,8.74;6,92.48,519.84,256.42,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,253.32,495.92,286.69,8.74;6,92.48,507.88,31.29,8.74">Improving the estimation of relevance models using large external corpora</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,150.77,507.88,389.23,8.74;6,92.48,519.84,158.43,8.74">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,539.76,447.52,8.74;6,92.48,551.72,447.52,8.74;6,92.48,563.67,90.83,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,267.13,539.76,146.14,8.74">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,437.85,539.76,102.15,8.74;6,92.48,551.72,443.68,8.74">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.48,583.60,447.52,8.74;6,92.48,595.55,447.52,8.74;6,92.48,607.51,172.53,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,263.55,583.60,202.59,8.74">A markov random field for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,490.34,583.60,49.65,8.74;6,92.48,595.55,447.52,8.74;6,92.48,607.51,73.48,8.74">Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval (SIGIR)</title>
		<meeting>the 28th annual international ACM SIGIR conference on research and development in information retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
