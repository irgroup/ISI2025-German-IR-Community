<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.89,83.76,355.95,15.48">Related Entity Finding Based on Co-Occurrence</title>
				<funder ref="#_TgsuYEW #_PhB2nDn #_4UDM3ZR #_9XzyyhK #_mA8cvqh">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_7vEF9zM">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_vqZruE8 #_qbnP7Et">
					<orgName type="full">Dutch and Flemish Governments</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.12,116.28,56.34,10.75"><forename type="first">Marc</forename><surname>Bron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.41,116.28,80.04,10.75"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.41,116.28,90.19,10.75"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.89,83.76,355.95,15.48">Related Entity Finding Based on Co-Occurrence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F51B271129CE2927DBC6342CCFF5935</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on experiments for the Related Entity Finding task in which we focus on only using Wikipedia as a target corpus in which to identify (related) entitities. Our approach is based on co-occurrences between the source entity and potential target entities. We observe improvements in performance when a context-independent co-occurrence model is combined with contextdependent co-occurrence models in which we stress the importance of the expected relation between source and target entity. Applying type filtering yields further improvements results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The start of a new track usually means the introduction of a new task-in this case, related entity finding (REF)-to be solved in the absence of training data and a standard system design. In approaching such a task, a sensible strategy is to start with a general system design and subsequently extend and refine it. We investigate an approach based on cooccurrences of potential target entities with the source entity given in the topic statement. We consider two variants: a purely co-occurrence based model and a combination of this with a context dependent model that takes documents (in which both entities co-occur) in consideration as context. On top of this we experiment with applying a type filtering component. Our overal system design has the following components:</p><p>• Named entity recognition For the homepage finding part of the task we focus on the pipeline design; we decide on methods to use for named entity recognition (NER), named entity normalization (NEN), and homepage finding as well as how to combine these with a co-occurrence and type filtering component. As the components are mutually dependent and the evaluation is end to end, there is a risk of noise accumulating throughout the system, resulting in poor performance. So for the optional Wikipedia field we employ a different strategy and focus on the co-occurrence component, while minimizing the influence of other components in two ways: (i) NER and NEN are handled by considering Wikipedia as a repository of (normalized) known entities and (ii) homepage finding is handled by mapping entities to Wikipedia pages.</p><p>Our TREC 2009 submissions were plagued by a number of bugs. The homepage part of our runs achieves disappointing results. An analysis reveals two causes. First, a standard tagger is unsuitable for NER as it is too liberal in accepting strings as entities, thus polluting the set of candidate entities. Second, the homepage finding task is a difficult problem and our ad hoc solution (cf. Section 2.4.2) turns out to be unsuitable. As there is no value in analyzing these results any further, we leave this part as is and instead discuss our runs only considering the Wikipedia field, i.e., only using Wikipedia as the target corpus in which to identify relavant entities.</p><p>We find that considering only Wikipedia pages as entities overcomes the NER and homepage finding weaknesses in the REF pipeline. Through analysis of the co-occurrence component we find that combining the pure co-occurrence and the context dependent model improves over a pure cooccurrence model alone, and that type filtering further improves these results.</p><p>In this paper we report on the repaired runs, only using Wikipedia as the target corpus. We describe our approach in Section 2, our results in Section 3, and conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>We formulate the entity ranking problem as follows. The goal is to rank candidate entities (e) according to P(e|E, T, R), where E is the source entity, T is the target type, and R is the relation described in the narrative.</p><p>Instead of estimating this probability directly, we use Bayes' rule and reformulate it into: P(e|E, T, R) = P(E, T, R|e) • P(e) P(E, T, R) .</p><p>(1)</p><p>Next, we drop the denominator as it does not influence the ranking of entities, and derive our final ranking formula as follows:</p><formula xml:id="formula_0" coords="2,93.11,92.32,199.79,87.60">P(E, T, R|e) • P(e) = P(E, R|e) • P(T |e) • P(e) (2) = P(E, R, e) • P(T |e) = P(R|E, e) • P(E, e) • P(T |e) = P(R|E, e) • P(e|E) • P(E) • P(T |e) (3) rank = P(R|E, e) • P(e|E) • P(T |e)<label>(4)</label></formula><p>In ( <ref type="formula" coords="2,68.89,194.69,3.87,8.64">2</ref>) we assume that the type is independent of the source entity E and the relation R. Next, we rewrite P(E, R|e) to P(R|E, e) so that it expresses the probability that relation R is generated by the two (co-occurring) entities (e and E).</p><p>Finally, we rewrite P(E, e) to P(e|E)•P(E) in (3) as the latter is a more convenient form for estimation, and we drop P(E) in (4) as it does not influence the ranking (for a fixed source entity E). Given equation ( <ref type="formula" coords="2,163.29,278.37,3.87,8.64" target="#formula_0">4</ref>) we are left with the following components:</p><p>• P(e|E): pure co-occurrence model,</p><p>• P(R|E, e): context dependent model, and</p><p>• P(T |e): type filtering.</p><p>In the following sections we describe our estimation methods for these components. In Section 2.4 we give a short overview of the other components of the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pure co-occurrence model</head><p>We use this component to express the strength of associations between the source entity and candidates, without considering the nature of their relation. We use pointwise mutual information as an estimate for P(e|E):</p><formula xml:id="formula_1" coords="2,122.71,502.26,100.10,23.81">P(e|E) = PMI(e, E) ∑ e PMI(e , E)</formula><p>and PMI(e, E) is defined as follows:</p><formula xml:id="formula_2" coords="2,116.76,562.66,109.23,22.18">PMI(e, E) = log c(e, E) c(e) • c(E)</formula><p>,</p><p>where c(e, E) is the number of documents in which e and E co-occur and c(e) is the number of documents in which e occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context-dependent model</head><p>In this component we model the relations between the source entity and candidate target entities. We represent the relation between a pair of entities by a co-occurrence language model (θ Ee ), a distribution over terms taken from documents in which the source and candidate entity co-occur. By assuming independence between the terms in the relation R we arrive at the following estimate for this component:</p><formula xml:id="formula_3" coords="2,352.29,99.85,203.63,20.53">P(R|E, e) = P(R|θ Ee ) = ∏ t∈R P(t|θ Ee ) n(t,R) ,<label>(5)</label></formula><p>where n(t, R) is the number of times t occurs in R. To estimate the co-occurrence language model θ Ee we aggregate term probabilities from documents in which the two entities co-occur:</p><formula xml:id="formula_4" coords="2,372.85,186.46,183.06,26.09">P(t|θ Ee ) = 1 |D Ee | ∑ d∈D Ee P(t|θ d ),<label>(6)</label></formula><p>where D Ee denotes the set of documents in which E and e co-occur and |D Ee | the number of these documents. P(t|θ d ) is the probability of term t within the language model of document d:</p><formula xml:id="formula_5" coords="2,380.73,269.16,175.19,24.28">P(t|θ d ) = n(t, d) + µ • P(t) ∑ t n(t , d) + µ ,<label>(7)</label></formula><p>where n(t, d) is the number of times t appears in document d, P(t) is the collection language model, and µ is the Dirichlet smoothing parameter, set to the average document length in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Type detection</head><p>The final component is used to filter entities by type. In order to perform type filtering we exploit the Wikipedia category structure; we map each of the (source) entity types (T ∈ {PER, ORG, PROD}) to a top category (cat(T )), e.g., "living people" and we create a similar mapping for entities to categories (cat(e)). With these two mappings we estimate P(T |e) as follows:</p><formula xml:id="formula_6" coords="2,353.02,475.41,160.51,22.66">P(T |e) = 1 if cat(e) ∩ cat L n (T ) = / 0 0 otherwise.</formula><p>We also perform category expansion for entity types by adding direct child categories to each level and write cat L n (T ), where L n is the chosen level of expansion. For example the second level L 2 contains the top categories (of level L 1 ) and all direct child categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The rest of the pipeline</head><p>The remaining components of the REF pipeline, i.e., named entity recognition and normalization as well as homepage and Wikipedia page finding, are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Entity Recognition and Normalization</head><p>On Clueweb Category B we use the Stanford named entity tagger to recognize entities <ref type="bibr" coords="2,427.57,686.91,77.45,8.64">(Finkel et al., 2005)</ref>. The tagger recognizes 4 entity types: person, organization, location, and miscellaneous.</p><p>On Wikipedia we handle named entity recognition by only considering anchor texts from links within Wikipedia as entity occurrences. We obtain an entity's name by removing the Wikipedia prefix from the anchor URL.</p><p>For NEN we map URLs to a single entity variant. Here we make use of Wikipedia redirects that map common alternative spellings or references (e.g., "Schumacher," "Schumi" and "M. Schumacher") to the "canonical form" of an entity ("Michael Schumacher").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Homepage and Wikipage finding</head><p>Once we have obtained a ranked list of entity names, we submit a query "official homepage of ENTITY " for each to obtain a list of documents. To determine if a document is a homepage we use edit distance between a documents URL and the entity name and use the highest scoring documents as homepages.</p><p>For matching entities to Wikipedia pages we use the anchor URL and return the corresponding target destination; the entity's Wikipedia page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The runs we focus on are centered around the co-occurrence component; ilpsEntBL and ilpsEntem. In our original runs the Wikipedia fields were not included, due to a bug in our code. As our focus is now solely on Wikipedia, we have generated new runs and replaced all homepage (HP) fields by a dummy document ID. We also continue experiments with the level of category expansion for our type filtering component and vary the levels from no filtering (L 0 ) to L 2 , L 4 and L 6 . In order to compare our runs we use the number of primary Wikipedia pages (pri ret), where primary means the encyclopedic entry of an entity, normalized discounted cumulative gain (nDCG), precision at 10 (P@10) and the number of relevant Wikipedia pages (rel ret).</p><p>ilpsEntBL combines the pure co-occurrence model with Table <ref type="table" coords="3,350.12,435.86,4.98,8.64" target="#tab_0">1</ref> shows the results for the Wikipedia only runs. We observe that the model that combines context and pure cooccurrence outperforms the pure co-occurrence model in all runs. The influence of different levels of type filtering on the pure co-occurrence model shows a clear trend; less expansion improves results. In the combined model the differences are smaller, suggesting that context reduces the number of non relevant entities of the wrong type in the top of the ranking. Figure <ref type="figure" coords="3,366.83,531.50,4.98,8.64" target="#fig_1">1</ref> shows the difference between the number of primary pages found by each of the models per topic (filtering level 4). A positive value indicates that more Wikipedia pages are found when the models are combined. We observe that only on topic 10 less primary pages are found, on 7 topics using context increases that number and on 13 topics context does not influence the number of primary Wikipedia pages found.</p><p>Our context dependent model finds reasonable numbers of primary pages. The P@R and nDCG R scores, however, are low. Topic 17 (i.e., E: "The food network", R: "Chefs with a show on the food network" and T: "person") is a good example of a topic that achieves good recall and poor P@10 and nDCG scores. Table <ref type="table" coords="3,400.26,686.91,4.98,8.64" target="#tab_1">2</ref> shows the top 10 entities returned for topic 17 and their frequencies. We observe that the frequencies of the top 5 entities returned by both models are very </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In our participation this year we set out to design a related entity finding system and to investigate the applicability of co-occurrence based models to the REF task. For our main homepage finding run we focused on identifying and assembling components into a REF system. The NER tool and homepage finding method, however, turned out to be unsuitable and resulted in disappointing results. The availability of this years topics as training set will facilitate developing a more robust REF system and should help eliminate issues of this kind in the future.</p><p>For our Wikipedia runs we eliminated interfering components as much as possible. We removed noise introduced by NER by only considering anchor URLs as entities and homepage finding by mapping entities to Wikipedia pages. This allowed us to focus on the co-occurrence and type filtering components of our system. We found that using PMI for the pure co-occurrence model produces a bias towards infrequent entities, suggesting the need for other estimation methods. When the pure co-occurrence model is combined with contextual information results improve on all runs and on all but one topic. This suggests that context is either of use for REF or does not influence the result.</p><p>Our P@10 and nDCG R scores are low, a fact caused by the use of PMI in our pure co-occurrence model. In future work we plan to investigate other estimation methods for this model and to construct a more effective REF pipeline by evaluating various methods and tools for the NER and homepage finding components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,63.76,568.16,121.75,8.96;1,63.76,583.96,199.05,8.96;1,63.76,599.76,184.67,8.96;1,63.76,615.56,63.68,8.96;1,63.76,631.36,88.83,8.96"><head>•</head><label></label><figDesc>Named entity normalization • (Context-independent) co-occurrence modeling • Context-dependent co-occurrence modeling • Type filtering • Home page finding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,316.81,253.99,239.10,8.64;3,316.81,265.95,239.10,8.64;3,316.81,277.90,239.10,8.64;3,316.81,289.86,239.10,8.64;3,316.81,301.81,102.38,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Difference in the number of Wikipedia pages (pri ret) found by the pure co-occurrence model and the combination with the context dependent model. A positive value indicates that more Wikipedia pages are found when the models are combined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,57.71,483.55,231.28,125.38"><head>Table 1 :</head><label>1</label><figDesc>Total score for each of our Wikipedia based runs.</figDesc><table coords="3,59.78,499.66,216.86,109.28"><row><cell>runID</cell><cell>nDCG R</cell><cell>P10</cell><cell cols="2">pri ret rel ret</cell></row><row><cell>ilpsEntBL L0</cell><cell>0.0204</cell><cell>0.0100</cell><cell>11</cell><cell>23</cell></row><row><cell>ilpsEntBL L2</cell><cell>0.0325</cell><cell>0.0350</cell><cell>44</cell><cell>2</cell></row><row><cell>ilpsEntBL L4</cell><cell>0.0266</cell><cell>0.0300</cell><cell>35</cell><cell>3</cell></row><row><cell>ilpsEntBL L6</cell><cell>0.0227</cell><cell>0.0100</cell><cell>29</cell><cell>6</cell></row><row><cell>ilpsEntem L0</cell><cell>0.0657</cell><cell>0.0650</cell><cell>58</cell><cell>1</cell></row><row><cell>ilpsEntem L2</cell><cell>0.0616</cell><cell>0.0650</cell><cell>69</cell><cell>14</cell></row><row><cell>ilpsEntem L4</cell><cell>0.0540</cell><cell>0.0550</cell><cell>64</cell><cell>6</cell></row><row><cell>ilpsEntem L6</cell><cell>0.0575</cell><cell>0.0600</cell><cell>68</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,53.80,59.27,239.10,407.86"><head>Table 2 :</head><label>2</label><figDesc>Entities returned for topic 17 by the pure cooccurrence model (top) and the context dependent model (bottom). Relevant entities are indicated in bold.</figDesc><table coords="4,53.80,59.27,239.10,407.86"><row><cell cols="2">Pure co-occurrence</cell><cell></cell></row><row><cell>Rank</cell><cell>Entity name</cell><cell>Frequency</cell></row><row><cell>1</cell><cell>Wayne Harley Brachman</cell><cell>5</cell></row><row><cell>2</cell><cell>Kerry Vincent</cell><cell>1</cell></row><row><cell>3</cell><cell>Jacqui Maloufa</cell><cell>5</cell></row><row><cell>4</cell><cell>Glenn Lindgren</cell><cell>3</cell></row><row><cell>5</cell><cell>Geof Manthorne</cell><cell>2</cell></row><row><cell cols="2">Context dependent</cell><cell></cell></row><row><cell>Rank</cell><cell>Entity name</cell><cell>Frequency</cell></row><row><cell>1</cell><cell>Gennaro Contaldo</cell><cell>10</cell></row><row><cell>2</cell><cell>Asako Kishi</cell><cell>18</cell></row><row><cell>3</cell><cell>Yutaka Ishinabe</cell><cell>13</cell></row><row><cell>4</cell><cell>Alpana Singh</cell><cell>15</cell></row><row><cell>5</cell><cell>Masahiko Kobe</cell><cell>16</cell></row><row><cell>34</cell><cell>Anne Burrell</cell><cell>16</cell></row><row><cell>53</cell><cell>Robert Irvine</cell><cell>63</cell></row><row><cell>75</cell><cell>Tyler Florence</cell><cell>83</cell></row><row><cell>82</cell><cell>Cat Cora</cell><cell>99</cell></row><row><cell>87</cell><cell>Michael Symon</cell><cell>80</cell></row><row><cell cols="3">low. On the other hand, the relevant entities (indicated in</cell></row><row><cell cols="3">bold face) are more frequent and also ranked lower. It turns</cell></row><row><cell cols="3">out that the use of PMI in our pure co-occurrence model cre-</cell></row><row><cell cols="3">ates a bias towards entities that occur less frequent. This</cell></row><row><cell cols="3">is an inherent property of PMI as is noted in Manning and</cell></row><row><cell cols="3">Schuetze (1999) and indicates that we need to consider al-</cell></row><row><cell cols="3">ternative co-occurence statistics to obtain high precision on</cell></row><row><cell cols="2">the REF task.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p>This research was supported by the DAESO and DuOMAn projects carried out within the <rs type="programName">STEVIN programme</rs> which is funded by the <rs type="funder">Dutch and Flemish Governments</rs> (http: //www.stevin-tst.org) under project number <rs type="grantNumber">STE-05-24</rs> and <rs type="grantNumber">STE-09-12</rs>, and by the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">640. 001.-501</rs>, <rs type="grantNumber">640.002.501</rs>, <rs type="grantNumber">612.066.512</rs>, <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.061.815</rs>, <rs type="grantNumber">640.004.802</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vqZruE8">
					<idno type="grant-number">STE-05-24</idno>
					<orgName type="program" subtype="full">STEVIN programme</orgName>
				</org>
				<org type="funding" xml:id="_qbnP7Et">
					<idno type="grant-number">STE-09-12</idno>
				</org>
				<org type="funding" xml:id="_7vEF9zM">
					<idno type="grant-number">640. 001.-501</idno>
				</org>
				<org type="funding" xml:id="_TgsuYEW">
					<idno type="grant-number">640.002.501</idno>
				</org>
				<org type="funding" xml:id="_PhB2nDn">
					<idno type="grant-number">612.066.512</idno>
				</org>
				<org type="funding" xml:id="_4UDM3ZR">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_9XzyyhK">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_mA8cvqh">
					<idno type="grant-number">640.004.802</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">References</head><p>Finkel, J. R., <ref type="bibr" coords="4,368.69,372.29,142.24,8.64">Grenager, T., and Manning, C. (2005</ref> </p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
