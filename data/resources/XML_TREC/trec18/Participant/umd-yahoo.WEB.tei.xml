<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.41,72.35,284.90,16.84;1,130.73,92.27,348.25,16.84">Of Ivory and Smurfs: Loxodontan MapReduce Experiments for Web Search</title>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder ref="#_578VVxq #_sgyRzTb">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Academic Cloud Computing Initiative</orgName>
					<orgName type="abbreviated">ACCI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.80,137.97,52.12,11.06"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.21,137.97,77.87,11.06"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
							<email>metzler@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.80,137.97,76.02,11.06"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
							<email>telsayed@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,410.21,137.97,63.31,11.06"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.41,72.35,284.90,16.84;1,130.73,92.27,348.25,16.84">Of Ivory and Smurfs: Loxodontan MapReduce Experiments for Web Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D64E5DA574752EB7CD6B4ABB0A80977</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes Ivory, an attempt to build a distributed retrieval system around the open-source Hadoop implementation of MapReduce. We focus on three noteworthy aspects of our work: a retrieval architecture built directly on the Hadoop Distributed File System (HDFS), a scalable Map-Reduce algorithm for inverted indexing, and webpage classification to enhance retrieval effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>It is commonly acknowledged that web-scale collections have outgrown the capabilities of individual machines, necessitating the use of clusters to tackle basic problems in information retrieval. Although search engine and other internet companies have long recognized and adapted to this fact, the academic community is just beginning to transition from single-machine to cluster-based systems. One previous impediment to progress was the availability of data: the largest collections available to researchers could be comfortably indexed on a typical server-class machine, obviating the need for clusters. The release of the 25 terabyte, one billion page ClueWeb09 collection, however, has forced researchers to think more seriously about cluster-based distributed retrieval solutions. This is a good sign, as it will propel the field forward.</p><p>Distributed computations are inherently difficult to organize, manage, and reason about. With traditional programming models such as MPI, the developer must explicitly handle a range of system-level details, ranging from synchronization to data distribution to fault tolerance. Recently, Map-Reduce <ref type="bibr" coords="1,85.43,533.35,9.72,7.86" target="#b5">[5]</ref> has emerged as an attractive alternative: its functional abstraction provides an easy-to-understand model for designing scalable and distributed algorithms.</p><p>MapReduce builds on the observation that many information processing tasks have the same basic structure: a computation is applied over a large number of records (e.g., web pages) to generate partial results, which are then aggregated in some fashion. Taking inspiration from higher-order functions in functional programming, MapReduce provides an abstraction for programmer-defined "mappers" (that specify the per-record computation) and "reducers" (that specify result aggregation). Key-value pairs form the processing primitives. The mapper is applied to every input key-value pair to generate an arbitrary number of intermediate key-value pairs. The reducer is applied to all values associated with the same intermediate key to generate an arbitrary number of final key-value pairs as output.</p><p>Under this framework, a programmer needs only to pro-vide implementations of the mapper and reducer. On top of a distributed file system <ref type="bibr" coords="1,415.78,220.53,9.20,7.86" target="#b6">[6]</ref>, the execution framework transparently handles all other aspects of execution on clusters ranging from a few to a few thousand cores. It is responsible, among other things, for scheduling (moving code to data), handling faults, and the large distributed sorting and shuffling problem between the map and reduce phases whereby intermediate key-value pairs must be grouped by key.</p><p>Hadoop,<ref type="foot" coords="1,359.51,291.98,3.65,5.24" target="#foot_0">1</ref> the open-source implementation of MapReduce, has gained immense popularity as an accessible, cost-effective framework for processing large datasets. <ref type="foot" coords="1,482.95,312.91,3.65,5.24" target="#foot_1">2</ref> This paper describes an attempt to build a distributed retrieval system around the Hadoop ecosystem. Retrieval systems designed to run on single machines make certain assumptions about characteristics of system resources (latency, bandwidth, capacity) and relationships between them. We used this opportunity to rethink some of these assumptions in a distributed environment, as the first step in building a scalable information retrieval toolkit for the future.</p><p>The system we have developed is called Ivory, which integrates Metzler's SMRF (Search using Markov Random Fields) retrieval engine <ref type="bibr" coords="1,415.04,429.74,14.31,7.86" target="#b14">[14,</ref><ref type="bibr" coords="1,433.62,429.74,10.73,7.86" target="#b13">13]</ref>. <ref type="foot" coords="1,447.92,427.97,3.65,5.24" target="#foot_2">3</ref> Ivory has been released under an open source license and can be freely downloaded from the web. This paper discusses three noteworthy aspects of our work: a retrieval architecture built directly on HDFS (Section 2), a scalable MapReduce algorithm for inverted indexing (Section 3), and post-processing of results to suppress adult content, spam, and low quality pages (Section 4). Experimental results are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RETRIEVAL ARCHITECTURE</head><p>Given a user query, retrieval involves fetching postings lists corresponding to query terms and computing querydocument scores according to the specified retrieval model. The postings list for each query term must be traversed, in a manner determined by the organization of the index and the query evaluation strategy. Beyond collections of a certain size, it is not practical to store the entire index on a single machine. The standard distributed solution is a broker-mediated, documentpartitioned retrieval architecture, illustrated in Figure <ref type="figure" coords="2,285.74,219.63,3.58,7.86" target="#fig_0">1</ref>. The entire document collection is divided into a number of partitions (sometimes called "shards"), and indexes are built for each partition separately; a server is responsible for searching each index, independent of the others. The interactions between a search client and the partition servers are mediated by the broker. In the standard query-response cycle, the client issues a query to the broker, which then distributes the query to all partition servers in parallel. Each server computes a ranked list on its assigned document partition independently, and the results are passed back to the broker. The broker merges the results and returns the final ranked list to the client. Although this "vertical" document partitioning strategy is often used in conjunction with a "horizontal" tiered partitioning strategy (i.e., by document quality), we do not consider that additional complexity in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Distributed Environment</head><p>In the early stages of our project, we noticed a fundamental mismatch between the standard document-partitioned retrieval architecture and characteristics of the MapReduce environment.</p><p>First, consider the problem of query evaluation on each individual partition. MapReduce, which was designed for batch processing, is not appropriate for this task. In Hadoop, it can take tens of seconds for mappers to even launch, since tasks must be queued at the jobtracker before they can be assigned to individual workers. Furthermore, the current design of Hadoop limits the rate at which new map tasks can be spawned. For the sub-second query latency expected by searchers today, there is no obvious way to implement workable retrieval algorithms in MapReduce.</p><p>Moreover, the MapReduce software ecosystem presents additional challenges for real-time retrieval algorithms. An integral component of MapReduce is the underlying distributed file system (DFS), which was designed around a number of assumptions about the workload. Since it is assumed that MapReduce jobs perform batch-oriented processing of large datasets, the distributed file system was optimized for high sustained throughput and not low-latency random access.</p><p>The DFS employs a simple master-slave architecture and stores files in fixed-size blocks. The master (called the namenode in HDFS) stores metadata and namespace mappings to data blocks, which are themselves stored on the local disks of the slaves (called datanodes in HDFS). The master only communicates metadata; data transfer occurs directly be-tween the application client and the relevant datanode. To the extent possible, the MapReduce scheduler starts map tasks on the machines that hold the data block to be processed, thus guaranteeing high sustained throughput since the task reads from local disk.</p><p>This design makes it difficult to achieve low-latency random access to DFS data from an arbitrary application client (e.g., a partition server). To access a random position in a file (e.g., looking up a postings list), the client must first contact the namenode to locate the relevant data block. Then, the client must contact the appropriate datanode to obtain the requested data. In addition to a disk seek on the datanode, the entire process involves round-trip communications with multiple machines and data transfer over the network. This problem cannot be solved by simply running the application client on the datanode that has the block stored locally. The distributed file system, by design, spreads data blocks across nodes in the cluster (to ensure reliability, to provide better locality, etc.), and therefore, for even moderately-large files, no single datanode will hold the entirety of a file's contents.</p><p>The design of the distributed file system is directly at odds with the requirements for query evaluation, since lowlatency random access to postings is necessary. Even though MapReduce provides a nice framework for building inverted indexes, the above discussion suggests that the DFS makes a poor storage substrate for retrieval. This is indeed the conventional wisdom.</p><p>The typical solution to this problem is to employ a separate architecture for retrieval. Once indexes have been built using Hadoop and written out to HDFS, they are then copied over to another cluster (onto standard POSIX file systems) to support retrieval. Typically, this involves copying individual partition indexes onto the local disk of the corresponding partition server. An example is Katta, which is a system for managing distributed Lucene indexes. <ref type="foot" coords="2,529.77,422.00,3.65,5.24" target="#foot_3">4</ref> This solution, while certainly workable, suffers from two major drawbacks, discussed below.</p><p>First, this solution requires the maintenance of two separate architectures: one for batch processing and another for real-time querying. This also requires splitting hardware resources, making it difficult to bring all available capacity to bear on a large problem. Although it is possible for the same physical machines to serve "double duty", such a setup may have unpredictable performance effects as multiple processes are competing for the same cores, memory, disk, and network. Furthermore, maintaining independent architectures will inevitably require keeping multiple copies of the data. For example, the collection needs to reside in HDFS to support indexing, but a separate copy may be needed on the retrieval cluster so that users can examine results.</p><p>Second, the two-architectures solution results in a complex workflow that necessitates copying large indexes over the network, thus complicating data management. Such a setup requires a good mechanism for versioning and metadata control, because duplicate data may be residing on independent systems at any given time. Workflow management is notoriously difficult in a rapidly-evolving research environment. Furthermore, the non-trivial latencies involved in copying indexes over the network to local disks is not conducive to the rapid turnaround times needed for IR experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenging Conventional Wisdom</head><p>In developing the Ivory system, we decided to challenge conventional wisdom and explore whether it was indeed feasible to "run" query evaluation algorithms directly on HDFSstored indexes. In addition, we wondered whether it was possible to use the same Hadoop cluster for both batchoriented processing (e.g., indexing) and for real-time services (e.g., retrieval).</p><p>Despite the discussion above, there were two additional observations that led us to believe that such an architecture was at least worth trying. The first bit of evidence comes from BigTable <ref type="bibr" coords="3,114.74,355.52,9.20,7.86" target="#b4">[4]</ref>, which is a sparse, distributed, persistent multidimensional sorted map built on top of the Google File System. BigTable is used for a number of production services with low latency requirements (e.g., Google Earth). Although very different from the distributed retrieval architecture we explore here, BigTable demonstrates that there is no principled reason why DFS latencies cannot be hidden by higher-level applications. The second bit of evidence comes from physical cluster architecture-as it turns out, bandwidth between a machine and the disks of any other racklocal machine is surprisingly competitive to the bandwidth of local disks (since for the most part, rack-level switches are not oversubscribed internally). A recent monograph by Barroso and Hölzle <ref type="bibr" coords="3,134.14,491.51,9.71,7.86" target="#b2">[2]</ref> discusses these observations in more detail. Operationally, this means that reading data off the disk of another machine on the same rack isn't much slower than reading data off the local disk.</p><p>The retrieval component in Ivory comes from Metzler's SMRF (Search using Markov Random Fields) engine, which was used in a number of previous studies examining the effectiveness of Markov Random Fields for information retrieval, but has not been available as open-source software until now. The major modification to the previous implementation was to fetch postings directly from HDFS instead of local disk. This is shown in Figure <ref type="figure" coords="3,204.00,606.58,3.58,7.86" target="#fig_2">2</ref>, which focuses on an individual partition server. As is standard in most retrieval engines, the vocabulary is held in memory. With frontcoding, this is relatively easy to accomplish, even for large collections. The vocabulary holds byte offsets into HDFSstored index files that correspond to locations of postings lists. The fetching of a postings list involves first contacting the namenode for the block location, and then contacting the datanode itself for the actual data-which is no different from any other HDFS read.</p><p>Within a Hadoop cluster environment, we still need to address the issue of how partition servers and the broker are initialized-given that the only point of contact between a client and the Hadoop cluster is the jobtracker. The solution we devised involves embedding servers in MapReduce jobs (albeit degenerate ones in most cases). Partition servers can be spawned as a MapReduce job that runs mappers but no reducers. Embedded inside each mapper is a server that handles queries over a TCP connection and accesses postings directly on HDFS (as described above). To start multiple partition servers, we create a Map-Reduce job that maps over a configuration file specifying the locations of the partition indexes. By appropriately configuring the job, a number of mappers equal to the number of document partitions is spawned. Each mapper reads in the location of the partition index, initializes a query engine, and then launches into an infinite service loop waiting for incoming TCP connections. The Hadoop execution framework is in essence co-opted into serving as a simple scheduler. However, we have little control over which cluster nodes the mappers are launched on. Fortunately, this situation is easy to rectify: when each mapper launches, it first writes its host information into a known DFS location. After all the partition servers have been initialized, the broker can be launched as a 1-mapper/0-reducer MapReduce job, reading the host information of all the partition servers and completing the distributed broker architecture.</p><p>Our solution addresses many of the issues with the twoarchitectures solution discussed in Section 2.1. Instead of maintaining a Hadoop cluster for indexing and another cluster for retrieval, we can accomplish both within a homogeneous environment. This allows us to better utilize available hardware resources and simplifies data management and workflow. The potential downside is, of course, degraded query performance due to reading postings remotely. Section 5.1 reports the performance of this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INVERTED INDEXING</head><p>Dean and Ghemawat's original paper <ref type="bibr" coords="3,473.85,453.27,9.71,7.86" target="#b5">[5]</ref> showed that Map-Reduce was designed from the very beginning with inverted indexing as an application. Although very little space was devoted to describing the algorithm, it is relatively straightforward to fill in the missing details: this basic MapReduce algorithm for inverted indexing is shown in Figure <ref type="figure" coords="3,530.25,505.57,3.58,7.86" target="#fig_3">3</ref>. Input to the mappers consists of document numbers<ref type="foot" coords="3,523.66,514.27,3.65,5.24" target="#foot_4">5</ref> (keys) paired with the document content (values). Inside the mapper, each document is tokenized, stemmed, and filtered for stopwords. Terms are processed sequentially to build a histogram of term frequencies (implemented as an associative array). The algorithm then iterates over all terms: for each, a posting consisting of the document number and the term frequency is created (denoted by n, H{t} in the pseudocode). The mapper then emits an intermediate key-value pair with the term as the key and the posting as the value. In this simple case, the payload of each posting contains only the tf, but this can easily be augmented with term position information to build positional indexes.</p><p>In the sort and shuffle phase, the MapReduce runtime performs a large, distributed "group by" of the postings by term. Without any additional effort by the programmer, the execution framework brings together all postings associated Emit(term t, postings P ) with the same term. This tremendously simplifies the task of the reducer, which gathers the postings and writes them to disk. The reducer begins by initializing an empty list and then appends all postings associated with the same term (key) to the list. The postings are then sorted (depending on type of index, by document number or term frequency) and written to disk (appropriately compressed). The MapReduce programming model provides a very concise expression of the inverted indexing algorithm, and can be implemented in a couple of dozen lines of code in Hadoop. Such an implementation can be successfully completed as a programming assignment in a computer science course for advanced undergraduates and first-year graduate students <ref type="bibr" coords="4,79.10,397.36,9.71,7.86" target="#b7">[7,</ref><ref type="bibr" coords="4,93.07,397.36,6.48,7.86" target="#b9">9]</ref>, which illustrates the simplicity of the the algorithm. In a traditional indexer (i.e., not implemented in MapReduce), significant attention must be devoted to the task of grouping postings by term, given constraints imposed by memory and disk (that memory capacity is limited, disk seeks are slow, sequential operations are preferred, etc.). In MapReduce, the programmer does not need to worry about any of these issues-the heavy lifting of grouping postings is handled by the execution framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scalable MapReduce Indexing Algorithm</head><p>There is, however, a significant bottleneck in the basic MapReduce algorithm for inverted indexing: it assumes that there is sufficient memory to hold all postings associated with the same term. Since the MapReduce execution framework makes no guarantees about the ordering of values associated with the same key, the reducer must first buffer all postings and then perform an in-memory sort before the postings can be written out to disk.</p><p>Since Ivory builds document-sorted indexes, we restrict our attention to the problem of sorting postings by ascending document number. Since the execution framework guarantees that keys arrive at each reducer in sorted order, one way to overcome the scalability bottleneck is to let the Map-Reduce runtime do the sorting. Instead of emitting keyvalue pairs of the following type: Emit(term t, postings P ) In other words, the key is a tuple containing the term and the document number, and the value is the term frequency. We need to redefine the sort order so that keys are sorted first by term t, and then by docno n. Additionally, we need a custom partitioner to ensure that all tuples with the same term are shuffled to the same reducer. With these two changes, the MapReduce execution framework ensures that the postings arrive in the correct order. This, combined with reducers preserving state across multiple keys, allows compressed postings to be written with minimal memory usage. The revised MapReduce inverted indexing algorithm is shown in Figure <ref type="figure" coords="4,384.71,449.67,3.58,7.86" target="#fig_4">4</ref>. The mapper remains unchanged for the most part, other than differences in the intermediate keyvalue pairs. The reducer contains two additional methods: Initialize, which is called before keys are processed, and Close, which is called after the final key is processed. The Reduce method is called for each key (i.e., t, n ), and by design, there will only be one value associated with each key. For each key-value pair, a posting can be directly added to the postings list. Since the postings are guaranteed to arrive in the correct order, they can be incrementally encoded in compressed form-thus ensuring a small memory footprint. Finally, when all postings associated with the same term have been processed (i.e., t = tprev), the entire postings list is written out to HDFS. The final postings list must be written out in the Close method.</p><formula xml:id="formula_0" coords="4,76.21,676.04,31.49,7.86">(term t,</formula><p>In our algorithm, the key space is partitioned by term; that is, all keys with the same term are sent to the same reducer. Since in Hadoop each reducer writes its output in a separate file on HDFS, our final index will be split across r files, where r is the number of reducers. In another Map-Reduce pass over these files, we construct a postings forward index to store the byte offset position of each postings list. This is used during retrieval to fetch postings that correspond to query terms. There is no need to consolidate the r files, since the postings forward index can keep track of which file a term's postings list is found in.</p><p>Three more details complete the description of Ivory's MapReduce indexing algorithm: positional information, document length data, and parameter setting for Golomb compression. First, positional indexes can be built by simply replacing the intermediate value f (term frequency) with an array of term positions; otherwise, no additional modifications are needed to the algorithm.</p><p>Second, since almost all retrieval models take into account document length, this information needs to be computed. Although it is straightforward to express this computation as another MapReduce job, this task can actually be folded into the inverted indexing process. When processing the terms in each document, the document length is known, and can be written out as "side data" directly to HDFS. We take advantage of the ability for a mapper to hold state across the processing of multiple documents in the following manner: an in-memory associative array is created to store document lengths, which is populated as each document is processed. When the mapper finishes processing input records, document lengths are written out to HDFS (i.e., in the Close method). Thus, document length data ends up in m different files, where m is the number of mappers; these files are then consolidated into a more compact representation.</p><p>Finally, parameters must be appropriately set for compression of the postings lists. The prescribed best practice is to use Golomb compression on first order document number differences (i.e., d-gaps) <ref type="bibr" coords="5,152.54,329.62,14.32,7.86" target="#b16">[16,</ref><ref type="bibr" coords="5,170.15,329.62,10.74,7.86" target="#b17">17]</ref>. The difficulty, however, is that Golomb compression requires two parameters: the size of the document collection and the number of postings for a particular postings list (i.e., df). The first is easy to obtain and can be passed into the reducer as a constant. The df of a term, however, is not known until all the postings have been processed-and unfortunately, the parameter must be known before postings are encoded. A two-pass solution that involves first buffering the postings (in memory) would suffer from the memory bottleneck we've been trying to avoid in the first place.</p><p>To get around this problem, we need to somehow inform the reducer of a term's df before any of its postings arrive. The solution is to have the mapper emit special keys of the form t, * to communicate partial document frequencies. This is accomplished in a manner similar to the computation of document lengths. The mapper holds an in-memory associative array that keeps track of how many documents a term has been observed in (i.e., the local document frequency of the term for the subset of documents processed by the mapper). Once the mapper has processed all input records, special keys of the form t, * are emitted with the partial df as the value.</p><p>To ensure that these special keys arrive first, we define the sort order of the tuple so that the special symbol * precedes all documents. Thus, for each term, the reducer will first encounter a series of t, * keys, representing partial dfs originating from each mapper. Summing all these partial contributions will yield the term's df, which can then be used to set the Golomb compression parameter. This allows the postings to be encoded in one pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Merging Results Across Partitions</head><p>The broker in a distributed document-partitioned architecture is responsible for merging results from each of the partition servers. We explored two separate algorithms for accomplishing this.</p><p>The first approach, which we call the independent fusion strategy, is to view results merging as a federated search problem, treating each partition as an independent collection. This approach simplifies index construction, but makes document scores across partitions difficult to compare directly. To correct for this, raw scores are normalized, per partition, using the z-score transformation as follows <ref type="bibr" coords="5,532.14,120.40,9.20,7.86">[8]</ref>:</p><formula xml:id="formula_1" coords="5,402.75,135.74,66.90,10.13">S * = (S -µo)/σ</formula><p>where S is the raw score, µo is the sample mean of the raw scores, σ 2 is the sample variance, and S * is the normalized score. The normalized scores are now considered samples of a standard normal distribution. The broker returns a combined ranked list by sorting all of the returned documents from all partitions based on their normalized scores.</p><p>The other strategy for merging results is called global statistics, which involves distributing global collection statistics to each of the partition indexes. First, each of the partition indexes are built independently. Then, a MapReduce job maps over all the partition indexes to compute global statistics (the global df and cf for each term and the size of the entire collection). Finally, global statistics are propagated back to each partition index. This is also accomplished with MapReduce: we map over each postings list, and inside each mapper the global statistics are loaded into memory. A new version of the index is written with the updated statistics (no reducers are required). This simple process is repeated for each partition. Given that MapReduce can take advantage of the aggregate disk throughput of multiple machines, these MapReduce jobs are surprisingly fast.</p><p>The advantage of the global statistics approach is that document scores generated in each partition are exactly the same as document scores in a single global index that spans all partitions-at least for the retrieval models used in our experiments (bm25 and query-likelihood). Thus, no additional score manipulation is necessary, and the broker simply resorts results from the partition servers and returns the final reranked list to the client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Alternative Algorithm Designs</head><p>Our inverted indexing algorithm in MapReduce represents a single point in the design space of possible approaches to the task. We discuss alternatives here, which primarily vary in the extent to which they take advantage of the large distributed group and sort operations built into the MapReduce execution framework.</p><p>Given an existing single-machine indexer, one simple way to take advantage of MapReduce is to leverage reducers to merge indexes built on local disk. This might proceed as follows: an existing indexer is embedded inside the mapper, and mappers are applied over the entire document collection. Each indexer operates independently and builds an index on local disk for the documents it encounters (i.e., index construction may involve multiple flushes to local disk and on-disk merge sorts outside of MapReduce). Once the local indexes have been built, compressed postings are emitted as values, keyed by the term. In the reducer, postings from each locally-built index are merged and written out as the final index. We did not pursue this option since it seemed like an incremental improvement over known indexing algorithms, and instead opted to develop an indexer from scratch to more fully explore the MapReduce programming model.</p><p>Another relatively straightforward adaptation of a single-machine indexer is demonstrated by Nutch. <ref type="foot" coords="6,230.93,55.87,3.65,5.24" target="#foot_5">6</ref> Its algorithm processes documents in the map phase, and emits pairs consisting of docids and analyzed document contents. The sort and shuffle phase in MapReduce is used essentially for document partitioning, and the reducers build each individual index independently. In this approach, the number of reducers specifies the number of partitions-which limits the degree of parallelization that can be achieved. Next, reconsider our critique of Dean and Ghemawat's MapReduce algorithm shown in Figure <ref type="figure" coords="6,223.15,151.78,3.58,7.86" target="#fig_3">3</ref>. Although we pointed out the scalability bottleneck associated with sorting the postings in the reducer, in actuality, there is no principled reason why this needs to be an in-memory sort. Instead, one could implement a multi-pass on-disk merge sort within the reducer. However, this is exactly what the MapReduce execution framework does in the sort and shuffle phase, so it makes sense to offload the processing.</p><p>Finally, we note that independently and roughly concurrently, McCreadie et al. <ref type="bibr" coords="6,148.94,245.93,14.31,7.86" target="#b12">[12]</ref> proposed a MapReduce inverted indexing algorithm based on emitting partial postings lists. The reducer receives partial postings lists and merges them into final postings lists.</p><p>Abstractly, inverted indexing can be viewed as a massive group and sort of individual postings. MapReduce indexing algorithms vary in what component performs these operations: the mappers and reducers, the execution framework, or a combination of both. In the first approach, the developer must shoulder at least some of the burden of grouping and sorting key-value pairs, but can take advantage of application-specific optimizations (e.g., efficient δ compression schemes). The downside, however, is added code complexity and potential scalability bottlenecks that may not be apparent. We have taken the second approach, and completely offloaded the grouping and sorting operations onto the MapReduce execution framework. Although this does not allow us to take advantage of application-specific optimizations, it does significantly simplify code. Moreover, scalability is ensured since we are taking advantage of mechanisms built directly into the programming model. Nevertheless, there is likely to be a middle ground (the third option) that balances simplicity and efficiency-which seems like a promising direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ADULT, SPAM, AND QUALITY</head><p>Given the large size of the ClueWeb09 collection, we hypothesized that traditional retrieval models would return a large amount of spam, adult material, and generally low quality documents that would severely degrade retrieval effectiveness. However, to properly test our hypothesis, we would need highly accurate spam, adult, and document quality classifiers or predictors. Rather than build classifiers ourselves, we used Yahoo!'s proprietary adult, spam, and document quality classifiers to post-process the ranked lists produced using Ivory.</p><p>Due to their proprietary nature, we are unable to provide the exact details of how these classifiers work, other than to say that they are machine-learned models that make use of many features and were trained using a very large amount of manually labeled data. We normalized the output of these classifiers to provide a score between 0 and 1, with 0 denoting not spam / not adult / low quality and 1 denoting spam / adult / high quality. <ref type="foot" coords="6,400.16,55.87,3.65,5.24" target="#foot_6">7</ref> As a reference, Qi and Davison <ref type="bibr" coords="6,541.61,57.64,14.31,7.86" target="#b15">[15]</ref> provide a recent survey on web page classification.</p><p>Given the lack of proper training data on the ClueWeb09 collection, we utilized the output of the classifiers in a simple, heuristic manner. We assumed that spam and adult documents would never be judged relevant, so we used the spam and adult classifiers to filter such documents from the result set. Furthermore, we used the output of the document quality classifier to adjust the original document scores assigned by Ivory. Results were rescored as follows: We considered two different settings for these parameters. The first, which we call conservative, corresponds to τa = 0.9, τs = 0.9, αq = 0.1. The second, which we call moderate and uses τa = 0.75, τs = 0.75, αq = 0.25. These settings were manually chosen after some preliminary experiments on a small development set of queries. To ensure that we return 1000 documents per query, we post-processed the top 2000 ranked documents.</p><formula xml:id="formula_2" coords="6,316.81,177.79,68.67,13.24">S (Q, D) = S(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>Experiments were run on a cluster provided by Google and managed by IBM, shared among a few universities as part of NSF's CLuE (Cluster Exploratory) Program and the Google/IBM Academic Cloud Computing Initiative. The cluster used in our experiments contained 99 physical nodes; each node has two single-core processors (2.8 GHz), 4 GB memory, and two 400 GB hard drives. The entire software stack (down to the operating system) was virtualized; each physical node runs one virtual machine hosting Linux. Experiments used Java 1.6 and Hadoop version 0.20.1.</p><p>Since more detailed specifications of the cluster machines were not available, we decided to informally run our own performance benchmarks. An individual cluster node achieved a composite score of 442 on NIST's SciMark 2.0 benchmark,<ref type="foot" coords="6,340.11,536.65,3.65,5.24" target="#foot_7">8</ref> averaged over 3 trials. For comparison, a laptop with a 2.6 GHz Core 2 Duo (T7800) processor 9 and 2 GB of RAM scored 494 on the same test (once again, averaged over three trials). SciMark consists of five computational kernels: FFT, Gauss-Seidel relaxation, Sparse matrix-multiply, Monte Carlo integration, and dense LU factorization. Note that this benchmark is primarily used to measure the performance of scientific and engineering applications, so the focus is on processor speed (which is only one component of overall performance). However, Lin <ref type="bibr" coords="6,482.01,632.56,14.32,7.86" target="#b10">[10]</ref> reported that on a brute-force task involving repeated computation of dot products, each cluster node was significantly slower than the same laptop. While it is true that our applications are primarily IO-bound and not processor-bound, we suspect that the cluster consists of previous-generation machines. Performance figures presented below should be interpreted with this important caveat. The 99-node cluster contained 198 cores, which, with current dual-processor quad-core configurations, could fit into 25 machines-a far more modest cluster with today's technology, not to mention that modern processors would be substantially faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Efficiency</head><p>On the 99-node cluster, indexing time for the first English segment of the ClueWeb09 collection (∼50 million pages) was 145 minutes (averaged over three trials; the fastest and slowest running times differed by less than 10 minutes). The size of the full positional index was around 66 GB.</p><p>On the retrieval end, we compared the performance of two variants of our query engine: one that reads indexes from local disk, and one that reads indexes from HDFS (the architecture discussed in Section 2.2). Both conditions utilized a single processor core on the cluster, and therefore performance differences can be attributed to the different methods of postings access. Average time per query (across three trials) is shown in Table <ref type="table" coords="7,178.54,407.82,3.58,7.86" target="#tab_2">1</ref>, for both queries from this year's web track (50 queries) and the 2004 robust track (100 queries) on the index built from the first English segment of ClueWeb09. We compared bm25 and query-likelihood, and in each case fetched 2000 hits.</p><p>These performance results were surprising in that reading postings from local disk was actually slower than reading postings over HDFS. One benefit of HDFS is the ability to read postings corresponding to different query terms in parallel, since they may involve accessing different datanodes. Reading multiple postings in parallel doesn't make much sense in a single machine environment unless there are multiple disks, and even then, it requires the retrieval engine to model that fact explicitly. In contrast, parallel reads are transparently handled by the HDFS API. The HDFS experiments also benefited from caching, which makes repeated access of postings faster (for common query terms, and also across multiple experimental runs). Although HDFS itself does not provide caching, since it resides on top of Linux, caching is performed transparently at the OS level-we can take advantage of the aggregate Linux buffer caches of all HDFS datanodes "for free". For this reason, the HDFS results are perhaps overly optimistic; more experiments are required to tease apart the various factors that influence performance.</p><p>Nevertheless, results show that our distributed architecture is not only feasible, but may provide additional performance advantages over separate batch and real-time architectures. In addition, we expect random access latencies to improve over time as developers continue to improve HDFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head><p>Model P@5 P@10 UMHOObm25GS bm25 (global) 0.1040 0.1420 UMHOObm25IF bm25 (fusion) 0.1240 0.1640 UMHOOqlGS QL (global) 0.0920 0.1180 UMHOOqlIF QL (fusion) 0.0800 0.1080 Table <ref type="table" coords="7,346.46,120.38,4.12,7.89">2</ref>: Official retrieval effectiveness for baseline category A submissions based on trec eval. Results of post-processing are shown in Table <ref type="table" coords="7,494.97,141.31,4.12,7.89" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness</head><p>Our official category A submissions were divided into two types: baseline runs and post-processed runs. The baselines examined four conditions: {bm25, query-likelihood}×{global statistics, independent fusion} (the latter describes the results merging strategies outlined in Section 3.2). The English portion of the ClueWeb09 collection was divided into ten different segments, each of which formed a partition in our architecture. For bm25, we used k1 = 0.5 and b = 0.3. For query likelihood, we used Dirichlet smoothing with µ = 1000. Official results for baseline runs based on trec eval are shown in Table <ref type="table" coords="7,382.84,292.75,3.58,7.86">2</ref>. We were quite surprised that the independent fusion approach was more effective than the global statistics approach; this may be due to a bug, since the task of propagating global statistics back to the individual partition indexes introduced an additional layer of complexity. However, see additional discussions below.</p><p>The post-processed runs used the filtering and reranking strategy described in Section 4; official results based on trec eval are shown in Table <ref type="table" coords="7,457.65,376.44,3.58,7.86" target="#tab_3">3</ref>. The second column of the table shows which of the baseline runs were postprocessed. Spam, adult, and quality scores were found in Yahoo!'s metadata store for approximately 95% of the URLs retrieved by the baseline runs. Note that the scores were computed over the version of the document at the time of run submission, which may differ from the crawled version in the ClueWeb09 collection.</p><p>Based on the Wilcoxon signed-rank test, we observe large and statistically-significant improvements (p &lt; 0.01) in P@5 and P@10 for yhooumd09BGC and yhooumd09BGM, postprocessed versions of the baseline bm25 (with global statistics) run. Furthermore, moderate filtering was found to be more effective than conservative filtering. Moderate filtering was 10% better than conservative filtering for P@5 and 6% better for P@10 (both n.s.). Table <ref type="table" coords="7,457.58,533.35,4.61,7.86" target="#tab_5">4</ref> shows the queries from the yhooumd09BGM run that were the most improved, in terms of absolute P@10, as the result of post-processing. In almost every case, these queries initially retrieved no relevant items in the top 10, but found 7 or more after postprocessing.</p><p>Somewhat surprisingly, the improvements observed for the yhooumd09BFM run were not statistically significant. One possible explanation for this is that the baseline system (i.e., UMHOObm25IF) retrieved many non-relevant documents that also happened to not be spam, adult, or low quality, thereby nullifying the effect of the filtering and reranking. Another possible explanation is that many of the ztransformed scores were close to zero, which caused our document quality score adjustments to have a negligible effect on the ranking.</p><p>Given the success of this simple, heuristic strategy, it is likely that a more formal learning to rank approach could ID Base Setting P@5 P@10 yhooumd09BFM UMHOObm25IF: bm25 (fusion) Moderate 0.1520 (+23%) 0.1640 (0%) yhooumd09BGC UMHOObm25GS: bm25 (global) Conservative 0.3880 (+273%)* 0.3820 (+169%)* yhooumd09BGM UMHOObm25GS: bm25 (global) Moderate 0.4280 (+312%)* 0.4040 (+185%)* result in even better retrieval effectiveness <ref type="bibr" coords="8,233.59,449.67,13.50,7.86" target="#b11">[11]</ref>. It would have been difficult to take such an approach this year, given the lack of training data on the ClueWeb09 collection, but it should be possible, at least to some extent, for future tasks that make use of the data. Results for category B runs are shown in Table <ref type="table" coords="8,259.61,501.97,3.58,7.86" target="#tab_4">5</ref>, based both on the statistical evaluation (StatMAP) method <ref type="bibr" coords="8,266.21,512.43,9.72,7.86" target="#b1">[1]</ref> and the Minimal Test Collection (MTC) method <ref type="bibr" coords="8,239.17,522.89,9.20,7.86" target="#b3">[3]</ref>. The first two models used features based on single term occurrences (bm25 and query-likelihood), while UMHOOsd combined term-dependence features such as ordered and unordered phrases with individual term occurrences using the Markov Random Field (MRF) retrieval framework. The single-term, ordered, and unordered clique types used in the MRF were assigned weights of 0.82, 0.09, 0.09, respectively. In order to consider retrieval efficiency, in run UMHOOsdp we pruned cliques based on idf: if a term's idf was less than 0.12 then cliques containing the term were pruned. Although the pruning threshold of 0.12 is relatively conservative for web-scale collections, we did see a drop in query evaluation time compared to the full MRF model, without a significant impact on effectiveness. Our simple pruning technique was performed at query time and hence could be adapted to query-dependent characteristics.</p><p>Details of significance testing comparing the two MRF models and the two baseline models are also shown in Ta-0.0%  ble 5. The pruned and unpruned MRF models were statistically indistinguishable, but both MRF models were significantly better than both baseline models for many metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Category A vs. Category B Quality</head><p>In addition to using the Yahoo! classifiers to improve retrieval effectiveness, we also used them to compare the quality of the category A and category B document sets. In our first experiment, we compared the spam density of documents retrieved from category A and category B using the 50 queries. Spam density is defined as the percentage of results returned, up to a certain rank depth, that is filtered as spam. Figure <ref type="figure" coords="8,392.00,627.50,4.61,7.86" target="#fig_6">5</ref> plots the spam density as a function of rank depth for both sets of documents (category A run with bm25, global statistics vs. category B run with bm25). First, the plot clearly shows that category A has a much higher spam density than category B across all ranks. This is not unexpected, as the ClueWeb09 collection represented a best-first crawl, so the larger category A document set contained documents that were lower in quality. Another interesting characteristic of the plot is that the spam den-  sity tends to be the highest at the top ranks and decreases farther down the ranked list. This suggests that traditional information retrieval models such as bm25 are highly susceptible to spam and that spammers are very good at getting their documents ranked highly when ranking is based on text alone. Table <ref type="table" coords="9,143.92,495.97,4.61,7.86" target="#tab_7">6</ref> (top) shows the individual queries with the highest spam density for categories A and B (up to 2000 hits). The query-by-query analysis shows overlap in the spammable queries and reaffirms that spam is much more prevalent in category A. We also measured the adult density, which is the percentage of results, to a fixed rank depth, that is filtered as adult. While the spam density for certain queries was often very high (up to 20%), the adult densities were significantly lower, which is either an artifact of the data collection (low adult coverage) or of the queries themselves. Table <ref type="table" coords="9,247.64,600.58,4.61,7.86" target="#tab_7">6</ref> (bottom) shows the 5 queries with the highest adult density for the two document sets (up to 2000 hits). Somewhat interestingly, category B tends to have larger adult densities than category A, which would indicate that category B may contain a larger fraction of adult pages than category A or that the adult pages are simply more 'retrievable' in category B. While most of the high adult density queries contain terms that may lead to adult results, it was surprising to see the rather innocuous query "the current" make the list.</p><p>The difference in document quality may also explain why the independent fusion approach to results merging was more effective than global statistics. If the average quality, spam density, and adult density of each segment of the ClueWeb09 collection were equal, then one would expect the use of global statistics to be more effective. On the other hand, if there is a high variance in quality across the segments, then independent fusion will rank the best documents from each partition highly, some of which will be higher quality (i.e., those returned from the high quality segment) than others. For example, consider an index with just two segments, where segment X is full of spam and segment Y has no spam. In addition, suppose spam documents rank very highly for certain queries. In this case, global statistics may return mostly documents from segment X, while independent fusion will return a mixture of documents from X and Y, and therefore have better retrieval effectiveness. Finally, we compare the average quality of the results retrieved for the two document sets. Figure <ref type="figure" coords="9,482.48,621.50,4.61,7.86" target="#fig_7">6</ref> plots the average quality as a function of rank depth for category A (bm25, global statistics) on the left and category B (bm25) on the right; higher is better. Note that the vertical axes are on the same scale, so points on the two plots can be meaningfully compared. Trendlines are added to the plots for illustrative purposes to aid in comparison. These plots show results to depth 2000, which, as we described earlier, is the number of baseline results we used for filtering and reranking. These plots show that the results retrieved from category B are consistently higher quality than the results retrieved from category A. The other point to notice is that the category B trendline is relatively flat, indicating almost constant document quality across all depths, while the category A trendline is more quadratic, increasing until around depth 1000 and then decreasing. The shape of the category A curve can be explained, in part, by Figure <ref type="figure" coords="10,187.44,130.86,3.58,7.86" target="#fig_6">5</ref>, which shows that spam density is higher early in the ranked list. Since spam plays a role in determining document quality, it is natural for the average quality curve to be inversely related to the spam density curve in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The transition from single-machine to cluster-based architectures in information retrieval research is inevitable, and the availability of the ClueWeb09 collection propels the academic community in the right direction. This development provides an opportunity to reexamine many aspects of information retrieval in a distributed processing environment for web-scale collections. In Ivory, we have explored three such aspects: an HDFS-based retrieval architecture, scalable indexing algorithms with MapReduce, and webpage classification. There is, of course, much more work to be done.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,148.51,239.11,7.89;2,53.80,158.97,204.92,7.89"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of a simple broker-mediated, document-partitioned retrieval architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,53.80,167.07,239.11,7.89;3,53.80,177.53,239.10,7.89;3,53.80,188.00,239.10,7.89;3,53.80,198.46,180.41,7.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of Ivory's distributed architecture that involves reading postings directly from HDFS (data transfer shown as solid lines; metadata communication shown as dotted lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,53.80,222.90,239.11,7.89;4,53.80,233.36,136.62,7.89"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pseudo-code of the simple inverted indexing algorithm in MapReduce.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,316.81,277.36,239.11,7.89;4,316.81,287.82,239.11,7.89;4,316.81,298.28,167.61,7.89"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pseudo-code of a scalable inverted indexing algorithm in MapReduce (slightly simplified from the actual algorithm in Ivory).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,385.48,177.79,53.03,7.86;6,438.51,176.02,8.09,5.24;6,456.74,177.79,98.47,7.86;6,403.55,188.25,16.38,7.86;6,486.03,188.25,40.38,7.86;6,316.81,206.75,239.11,7.86;6,316.81,217.21,239.11,7.86;6,316.81,227.67,239.29,7.86;6,316.81,238.13,239.10,7.86;6,316.81,248.59,239.11,7.86;6,316.81,259.05,239.10,7.86;6,316.81,269.51,92.87,7.86"><head></head><label></label><figDesc>Q, D) • fq(D) αq fa(D) &lt; τa ∧ fs(D) &lt; τs -∞ otherwise where S (Q, D) is the new score, S(Q, D) is the original score, fs(D) is the spam classifier score, fa(D) is the adult classifier score, fq(D) is the document quality classifier score, τs is the spam threshold, τa is the adult threshold, and αq is quality score adjustment factor. The free parameters are τs, τa, and αq: different settings will lead to different degrees of filtering and reranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,316.81,452.12,239.11,7.89;8,316.81,462.58,232.83,7.89"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Spam density as a function of rank depth for category A (circle) and category B (diamond).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,53.80,413.50,502.12,7.89"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average result quality vs. rank depth for category A (left) and category B (right); higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,53.80,55.49,239.11,94.20"><head>Table 1 :</head><label>1</label><figDesc>Average per-query running time on the first segment of ClueWeb09, comparing indexes stored on HDFS with indexes stored on local disk.</figDesc><table coords="7,103.16,55.49,140.39,52.49"><row><cell>Queries</cell><cell cols="3">Model HDFS local</cell></row><row><cell cols="2">Robust04 bm25</cell><cell>5.45s</cell><cell>8.25s</cell></row><row><cell cols="2">Robust04 QL</cell><cell>6.65s</cell><cell>10.0s</cell></row><row><cell>Web09</cell><cell>bm25</cell><cell>4.73s</cell><cell>6.65s</cell></row><row><cell>Web09</cell><cell>QL</cell><cell>5.60s</cell><cell>7.42s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,53.80,110.46,504.29,104.43"><head>Table 3 :</head><label>3</label><figDesc>Official retrieval effectiveness for post-processed category A runs based on trec eval (relative gains shown in parentheses). A single asterisk denotes a statistically-significant difference according the Wilcoxon signed-rank test at the p &lt; 0.01 level. • 0.3023 • 0.4272 • 0.3885 • 0.0476 • 0.1068 • 0.3458 • 0.3999 • UMHOOsdp MRF pruned 0.2138 • 0.2993 • 0.4251 • 0.3860 0.0476 • 0.1068 • 0.3436 • 0.3991 •</figDesc><table coords="8,59.18,151.91,489.79,52.52"><row><cell></cell><cell></cell><cell cols="2">StatMAP Method</cell><cell></cell><cell></cell><cell cols="2">MTC Method</cell><cell></cell><cell></cell></row><row><cell>ID</cell><cell>Model</cell><cell>MAP</cell><cell>MRP</cell><cell>MP@30</cell><cell cols="2">MnDCG eMAP</cell><cell>eRprec</cell><cell>eP5</cell><cell>eP10</cell></row><row><cell cols="2">UMHOObm25B bm25</cell><cell>0.2037</cell><cell>0.2848</cell><cell>0.3967</cell><cell>0.3718</cell><cell>0.0461</cell><cell>0.1048</cell><cell>0.3496</cell><cell>0.3849</cell></row><row><cell>UMHOOqlB</cell><cell>QL</cell><cell>0.1874</cell><cell>0.2761</cell><cell>0.3779</cell><cell>0.3416</cell><cell>0.0436</cell><cell>0.1027</cell><cell>0.2810</cell><cell>0.3395</cell></row><row><cell>UMHOOsd</cell><cell>MRF</cell><cell>0.2142</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,53.80,227.78,502.13,163.70"><head>Table 5 :</head><label>5</label><figDesc>Retrieval effectiveness for category B runs. Comparing the two MRF models to the two baseline models: indicates significantly better than bm25 (p &lt; 0.05), indicates n.s.; • indicates significantly better than QL (p &lt; 0.05), • indicates n.s. (all significance tests performed with the Wilcoxon signed-rank test).</figDesc><table coords="8,104.05,276.72,138.60,114.76"><row><cell>Query</cell><cell cols="2">Before After</cell></row><row><cell>diversity</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>inuyasha</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>atari</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>dogs for adoption</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>dinosaurs</cell><cell>0.0</cell><cell>0.9</cell></row><row><cell>espn sports</cell><cell>0.1</cell><cell>0.9</cell></row><row><cell>euclid</cell><cell>0.0</cell><cell>0.8</cell></row><row><cell>appraisals</cell><cell>0.0</cell><cell>0.7</cell></row><row><cell>hoboken</cell><cell>0.0</cell><cell>0.7</cell></row><row><cell>the secret garden</cell><cell>0.0</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,53.80,404.38,239.11,18.35"><head>Table 4 :</head><label>4</label><figDesc>Queries most improved as the result of post-processing in terms of P@10.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,117.01,55.47,375.70,169.48"><head>Table 6 :</head><label>6</label><figDesc>Queries with highest density of spam (top) and adult content (bottom).</figDesc><table coords="9,152.06,55.47,305.61,148.16"><row><cell cols="2">Spam</cell></row><row><cell>Category A</cell><cell>Category B</cell></row><row><cell>appraisals (20.0%)</cell><cell>air travel information (10.0%)</cell></row><row><cell>poker tournaments (19.5%)</cell><cell>cheap internet (6.7%)</cell></row><row><cell>elliptical trainer (13.6%)</cell><cell>website design hosting (6.3%)</cell></row><row><cell>used car parts (12.7%)</cell><cell>cell phones (4.9%)</cell></row><row><cell>cell phones (12.4%)</cell><cell>poker tournaments (4.7%)</cell></row><row><cell cols="2">Adult</cell></row><row><cell>Category A</cell><cell>Category B</cell></row><row><cell>french lick resort and casino (0.25%)</cell><cell>the current (1.85%)</cell></row><row><cell>toilet (0.15%)</cell><cell>toilet (0.45%)</cell></row><row><cell>cheap internet (0.15%)</cell><cell>french lick resort and casino (0.30%)</cell></row><row><cell>inuyasha (0.15%)</cell><cell>inuyasha (0.25%)</cell></row><row><cell>the secret garden (0.15%)</cell><cell>the secret garden (0.25%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,321.42,609.83,108.14,7.86"><p>http://hadoop.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,321.42,620.16,234.50,7.86;1,316.81,629.12,239.11,7.86;1,316.81,638.09,239.11,7.86;1,316.81,647.06,239.11,7.86;1,316.81,656.02,239.11,7.86;1,316.81,664.99,93.73,7.86"><p>To be precise, MapReduce is used to refer to the programming model in general, while Hadoop refers to the specific open-source implementation. Along the same lines, the distributed file system (DFS) is used to refer to the underlying storage substrate in general, while GFS<ref type="bibr" coords="1,482.96,656.02,9.71,7.86" target="#b6">[6]</ref> and HDFS are used to refer to specific</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,413.60,664.99,68.84,7.86;1,317.27,673.55,3.65,5.24;1,321.42,675.32,234.50,7.86;1,316.81,684.29,239.11,7.86;1,316.81,693.25,239.11,7.86;1,316.81,702.22,239.11,7.86;1,316.81,711.19,55.53,7.86"><p>implementations.<ref type="bibr" coords="1,317.27,673.55,3.65,5.24" target="#b3">3</ref> In the Maryland tradition of whimsical titles for TREC papers: The mascot for Hadoop is an elephant, and African elephants belong to the genus Loxodonta. And yes, it is clear that Hadoop is an African elephant and not of the Asian variety.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,321.42,711.19,116.93,7.86"><p>http://katta.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,321.42,702.22,234.50,7.86;3,316.81,711.19,239.11,7.86"><p>We assume that documents are sequentially numbered from 1 to n, where n is the number of documents in the collection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,58.40,711.19,130.91,7.86"><p>http://lucene.apache.org/nutch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,321.42,681.56,234.51,7.86;6,316.81,690.52,49.13,7.86"><p>Note that the scales are reversed for quality, compared to spam/adult.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,321.42,700.86,128.46,7.86"><p>http://math.nist.gov/scimark2/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">NSF</rs> under award <rs type="grantNumber">IIS-0836560</rs> and <rs type="grantNumber">IIS-0916043</rs>; <rs type="person">Google</rs> and <rs type="funder">IBM</rs>, via the <rs type="funder">Academic Cloud Computing Initiative (ACCI)</rs>. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors' and do not necessarily reflect those of the sponsors. The first author is grateful to <rs type="person">Esther and Kiri</rs> for their loving support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_578VVxq">
					<idno type="grant-number">IIS-0836560</idno>
				</org>
				<org type="funding" xml:id="_sgyRzTb">
					<idno type="grant-number">IIS-0916043</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,58.28,422.30,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.59,435.22,206.69,7.86;10,72.59,445.68,205.81,7.86;10,72.59,456.14,184.46,7.86;10,72.59,466.60,208.82,7.86;10,72.59,477.06,20.96,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,127.48,445.68,135.27,7.86">Million query track 2008 overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,72.59,456.14,184.46,7.86;10,72.59,466.60,101.69,7.86">Proceedings of the Seventeenth Text REtrieval Conference (TREC 2007)</title>
		<meeting>the Seventeenth Text REtrieval Conference (TREC 2007)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.59,488.52,201.25,7.86;10,72.59,498.98,178.30,7.86;10,72.59,509.44,194.16,7.86;10,72.59,519.90,68.25,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,191.56,488.52,82.28,7.86;10,72.59,498.98,178.30,7.86;10,72.59,509.44,106.57,7.86">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hölzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.59,531.36,207.79,7.86;10,72.59,541.82,216.80,7.86;10,72.59,552.28,185.38,7.86;10,72.59,562.74,179.60,7.86;10,72.59,573.20,211.55,7.86;10,72.59,583.66,107.42,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,246.88,531.36,33.50,7.86;10,72.59,541.82,152.41,7.86">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,243.53,541.82,45.86,7.86;10,72.59,552.28,185.38,7.86;10,72.59,562.74,179.60,7.86;10,72.59,573.20,144.44,7.86">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006)</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006)<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.59,595.12,216.95,7.86;10,72.59,605.58,197.30,7.86;10,72.59,616.04,215.40,7.86;10,72.59,626.50,215.68,7.86;10,72.59,636.96,197.94,7.86;10,72.59,647.42,204.27,7.86;10,72.59,657.89,20.96,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,119.62,616.04,168.37,7.86;10,72.59,626.50,60.94,7.86">Bigtable: a distributed storage system for structured data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,152.18,626.50,136.10,7.86;10,72.59,636.96,197.94,7.86;10,72.59,647.42,50.69,7.86">Proceedings of the 7th Symposium on Operating System Design and Implementation (OSDI 2006)</title>
		<meeting>the 7th Symposium on Operating System Design and Implementation (OSDI 2006)<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.59,669.34,205.52,7.86;10,72.59,679.80,219.95,7.86;10,72.59,690.26,196.04,7.86;10,72.59,700.73,201.76,7.86;10,72.59,711.19,109.38,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,184.14,669.34,93.97,7.86;10,72.59,679.80,129.81,7.86">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.61,679.80,71.93,7.86;10,72.59,690.26,196.04,7.86;10,72.59,700.73,116.76,7.86">Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI 2004)</title>
		<meeting>the 6th Symposium on Operating System Design and Implementation (OSDI 2004)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,57.64,194.06,7.86;10,335.61,68.10,212.45,7.86;10,335.61,78.56,209.90,7.86;10,335.61,89.02,216.89,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,513.80,57.64,15.87,7.86;10,335.61,68.10,76.29,7.86">The Google File System</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.95,68.10,117.10,7.86;10,335.61,78.56,209.90,7.86;10,335.61,89.02,20.92,7.86">Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP 2003)</title>
		<meeting>the 19th ACM Symposium on Operating Systems Principles (SOSP 2003)<address><addrLine>Bolton Landing, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,100.48,197.34,7.86;10,335.61,110.94,211.19,7.86;10,335.61,121.40,219.35,7.86;10,335.61,131.86,210.26,7.86;10,335.61,142.32,134.37,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,335.61,110.94,195.71,7.86">Cluster computing for Web-scale data processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Michels-Slettvet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bisciglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.61,121.40,219.35,7.86;10,335.61,131.86,180.97,7.86">Proceedings of the 39th ACM Technical Symposium on Computer Science Education (SIGCSE 2008)</title>
		<meeting>the 39th ACM Technical Symposium on Computer Science Education (SIGCSE 2008)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,153.78,195.34,7.86;10,335.61,164.24,186.18,7.86;10,335.61,174.70,189.11,7.86;10,335.61,185.16,209.43,7.86;10,335.61,195.62,142.19,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,480.13,153.78,50.81,7.86;10,335.61,164.24,170.76,7.86;10,402.48,174.70,122.24,7.86;10,335.61,185.16,151.54,7.86">Topic Detection and Tracking: Event-based Information Organization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sista</surname></persName>
		</author>
		<editor>J. Allan</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="67" to="84" />
		</imprint>
	</monogr>
	<note>Probabilistic approaches to topic detection and tracking</note>
</biblStruct>

<biblStruct coords="10,335.60,207.08,213.58,7.86;10,335.61,217.54,212.26,7.86;10,335.61,228.00,218.11,7.86;10,335.61,238.46,214.12,7.86;10,335.61,248.92,91.83,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,365.00,207.08,184.18,7.86;10,335.61,217.54,120.76,7.86">Exploring large-data issues in the curriculum: A case study with MapReduce</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,475.93,217.54,71.93,7.86;10,335.61,228.00,218.11,7.86;10,335.61,238.46,43.36,7.86;10,452.41,238.46,39.31,7.86">Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics</title>
		<meeting>the Third Workshop on Issues in Teaching Computational Linguistics<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
	<note>ACL 2008</note>
</biblStruct>

<biblStruct coords="10,335.60,260.38,220.32,7.86;10,335.61,270.84,214.22,7.86;10,335.61,281.30,207.27,7.86;10,335.61,291.76,208.98,7.86;10,335.61,302.22,211.55,7.86;10,335.61,312.68,117.47,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,364.84,260.38,191.08,7.86;10,335.61,270.84,198.11,7.86">Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.61,281.30,207.27,7.86;10,335.61,291.76,208.98,7.86;10,335.61,302.22,144.43,7.86">Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2009)</title>
		<meeting>the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2009)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,324.14,214.39,7.86;10,335.61,334.60,201.53,7.86;10,335.61,345.06,42.94,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,379.45,324.14,166.98,7.86">Learning to rank for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,335.61,334.60,197.70,7.86">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,356.52,202.67,7.86;10,335.61,366.98,207.59,7.86;10,335.61,377.44,177.20,7.86;10,335.61,387.90,193.92,7.86;10,335.61,398.36,184.43,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,335.61,366.98,207.59,7.86;10,335.61,377.44,17.64,7.86">Comparing distributed indexing: To MapReduce or not?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M C</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,368.83,377.44,143.98,7.86;10,335.61,387.90,193.92,7.86;10,335.61,398.36,35.73,7.86">Proceedings of the 7th Workshop on Large-Scale Distributed Systems for Information Retrieval</title>
		<meeting>the 7th Workshop on Large-Scale Distributed Systems for Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>LSDS-IR&apos;09</note>
</biblStruct>

<biblStruct coords="10,335.60,409.82,188.85,7.86;10,335.61,420.28,203.14,7.86;10,335.61,430.74,205.64,7.86;10,335.61,441.20,61.18,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,384.35,409.82,140.11,7.86;10,335.61,420.28,203.14,7.86;10,335.61,430.74,34.46,7.86">Beyond Bags of Words: Effectively Modeling Dependence and Features in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,335.60,452.66,212.32,7.86;10,335.61,463.12,204.98,7.86;10,335.61,473.58,220.06,7.86;10,335.61,484.04,209.22,7.86;10,335.61,494.50,214.88,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,453.78,452.66,94.14,7.86;10,335.61,463.12,114.26,7.86">A Markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,468.65,463.12,71.93,7.86;10,335.61,473.58,220.06,7.86;10,335.61,484.04,209.22,7.86;10,335.61,494.50,54.06,7.86">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,505.96,201.48,7.86;10,335.61,516.42,207.99,7.86;10,335.61,526.88,47.54,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,441.37,505.96,95.72,7.86;10,335.61,516.42,95.89,7.86">Web page classification: Features and algorithms</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,438.65,516.42,100.79,7.86">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,538.33,203.27,7.86;10,335.61,548.80,217.03,7.86;10,335.61,559.26,219.07,7.86;10,335.61,569.72,66.23,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,499.60,538.33,39.28,7.86;10,335.61,548.80,217.03,7.86;10,335.61,559.26,26.18,7.86">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishing</publisher>
			<pubPlace>San Francisco, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,335.60,581.17,211.25,7.86;10,335.61,591.63,211.32,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,433.89,581.17,112.96,7.86;10,335.61,591.63,27.79,7.86">Inverted files for text search engines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM Computing Surveys</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
