<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.44,76.31,243.56,12.58;1,133.62,93.77,342.01,12.58">PRIS at 2009 Relevance Feedback track: Experiments in Language Model for Relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.40,134.41,19.43,9.16"><forename type="first">Si</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.49,134.41,50.85,9.16"><forename type="first">Xinsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.93,134.41,44.66,9.16"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.09,134.41,53.86,9.16"><forename type="first">Sanyuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.64,134.41,50.41,9.16"><forename type="first">Guang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.67,134.41,35.26,9.16"><forename type="first">Jun</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.44,76.31,243.56,12.58;1,133.62,93.77,342.01,12.58">PRIS at 2009 Relevance Feedback track: Experiments in Language Model for Relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B525AB0BE8A91D853A1481F27DC76871</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes BUPT (pris) participation in Relevance Feedback Track 2009. The track has two phrases. In the first phrase, 5 documents are submitted based on the results of the k-means. In the second phrase, language model is used to relevance feedback for query expansion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year, the Relevance Feedback Track has two tasks <ref type="bibr" coords="1,353.07,368.41,11.14,9.16">[1]</ref>. In the first phase, participants should determine up to 5 documents for each topic that they desire judged. In the second phase, the results of phase 1 will be used as judged document RF input for phase 2 of the track.</p><p>The PRIS-RF system is submitted by Pattern Recognition and Intelligent System Lab at Beijing University of Posts and Telecommunications. In the first phrase, clustering algorithm is employed to get the center documents. In the second phrase, relevance feedback algorithm is used. Our system adopts language model based on weekly semi-supervised machine learning for query expansion. According to only a bit of labeled documents, clustering algorithm and bootstrapping method are used. In the first stage, the 5 given documents are regarded as the center documents. The more documents can be labeled based on k-nearest neighbors (K-NN) clustering algorithm. At second stage, language model is used in the labeled documents for query expansion. Then based on bootstrapping method, the two staged are iterated until the relevance retrieval ranking list is stable. The basic ad-hoc retrieval platform is based on the Indri Retrieval Toolkit <ref type="bibr" coords="1,431.75,555.61,11.15,9.16">[2]</ref>.</p><p>The remainder of this paper is organized as follows. In section 2, a briefly system overview is presented. Section 3 introduces the topic retrieval part. Section 4 describes the relevance feedback system. Evaluation results are shown in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The framework of the PRIS-RF system is shown in Figure <ref type="figure" coords="1,349.07,664.81,3.92,9.16">1</ref>. The preprocessing part is designed to extract content of the permalink HTML pages, and some rules are set to process abbreviations. We only use the permalink HTML pages for retrieval. These HTML pages are parsed and texts are reserved. The hyper-links, scripts, style information in the web pages and all html tags are discarded. The topic retrieval part based on the Indri Retrieval Toolkit tries structured search on the document-level retrieval. Then we get the baseline of topic relevance ranking list. The feedback is carried out based on the baseline. Language model is the main model in feedback algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 System Framework 3 Topic Relevance Retrieval</head><p>In this part, Indri is used to build index and search. Structured search is contained in the query language. The title field of the wed page is built into index. And some query languages are used in the baseline query. The following is an example of the baseline query. &lt;query&gt; &lt;number&gt;18&lt;/number&gt; &lt;text&gt;#5 (wedding budget calculator).(title) wedding budget calculator&lt;/text&gt; &lt;/query&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Language Model for Relevance Feedback</head><p>The main issues in relevance feedback are how to select relevant documents from the retrieved documents, and how to select expansion terms. Here we deal with the problem of selecting better expansion terms. The problem in traditional relevance feedback obtaining a set of expansion terms from the relevance retrieved documents that may have low precision. If a method can select better expansion terms from the relevance documents, it can almost certainly improve the effect of retrieval. The main process we have done is described in Algorithm 1.</p><p>Algorithm 1: The PRIS-RF algorithm INPUT: 5 labeled documents OUTPUT: a group of expansion terms 1 . Find the top k nearest neighbors to the 5 labeled documents to construct a labeled collection. 2 . Based on the Language Model, the expansion terms are extracted from the labeled collection which is got by the first step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Clustering Model</head><p>5 labeled documents can be got for each query. But it's too few to get the expansion term for the topic. So clustering is employed to get more relevance documents from the retrieved documents based on those labeled documents. The clustering model follows the hypothesis that the documents which are near to the labeled documents have the same label as the labeled documents. So the k-nearest neighbors (K-NN) clustering method <ref type="bibr" coords="3,340.35,154.63,12.28,9.16" target="#b0">[3]</ref> is adopted to find the relevance documents. In the KNN, each labeled document plays a central role. Each document is represented by VSM. The similarity is calculated between the labeled and non-labeled documents in order to delete the duplicate documents and label the unlabeled documents. The top n documents are labeled and used to extract the expansion terms.</p><p>The top n documents cluster a relevance class and another non-relevance class. Because the 5 labeled documents contains relevance documents and non-relevance documents. Since some cross-documents appear in the relevance class and non-relevance class, there will be some noise generated. So the expansion terms extracted from those cross-documents must be inaccurate. To eliminate the noise, we have to remove the cross-documents from the relevance class and non-relevance class. After the eliminated process, we got the pure relevance class and non-relevance class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Language Model</head><p>The query is treated as a random event generated according to a probability distribution by the language model method to IR, developed by Ponte and Croft. Here, he made a simply assumption that the user of an IR system will have an idea of a prototypical document in which he or she is interested and will choose query terms likely to occur in documents similar to that prototype. Viewed this way, one can then estimate a model of the term generation probabilities for the query terms for each document. And then he can rank the documents according to the probability of generating the query. This language model is our improved language model's foundation. In his language modeling approach the probability of generating the query terms for each document can be generate. So our improved language model change the query terms for each document to each terms in the relevance class and non-relevance class which generated on the cluster model. Then we can get the probability of each term in those two classes. Each term is ranked by the probability .We extract the top n terms as the expansion terms. The detail instruction is as follows.</p><p>In the language model approach to IR, each document and each term of documents are ranked according to the estimate of producing the terms according to the language model. So to get the probability of terms generation is the first step. The terms generation probability p(Q|M d ), is the probability of producing the terms given the language model of document d. This probability will be estimated starting with the maximum likelihood estimate of the probability of term t in document d:</p><formula xml:id="formula_0" coords="3,169.86,656.59,289.01,34.97">( , ) ˆ( | ) t d m l d d t f P t M d l = (1)</formula><p>tf <ref type="bibr" coords="3,95.88,704.23,11.88,6.32">(t,d)</ref> is the raw term frequency of term t in document d and dl d is the total number of tokens in document d. A simplifying assumption will be made. Assume that given a particular language model, the terms occur independently. Based on the assumption, the maximum likelihood estimator gives rise to the ranking formula ∏ t ∈ D P ml (t|M d ) for each document.</p><p>But there is an insufficient data problem <ref type="bibr" coords="4,275.00,76.63,12.28,9.16" target="#b1">[4]</ref> for the reliable estimation of maximum likelihood. The insufficient data problem is that some documents miss one or more of the query terms. But we do not wish to give a probability of zero for those documents. Because doing so, a document missing even one of the queries would not be retrieved. To solve the problem of insufficient data, we need an estimate of a larger amount of data. That estimate is the mean probability estimate of t in documents containing it:</p><formula xml:id="formula_1" coords="4,140.88,164.94,313.91,45.07">( ) ˆ( | ) ˆ( ) t d m l d d a v g t P t M P t d f ∈ = ∑ (<label>2</label></formula><formula xml:id="formula_2" coords="4,454.79,185.83,4.08,9.16">)</formula><p>df t is the document frequency of t. This is a more robust statistic in the sense that we have a lot more data from which to estimate it, but another problem appears. Each document containing t drawn from the same language model cannot be assumed, and so some risk is contained in using the mean to estimate p(t|M d ). Furthermore, if we use the average number of the probability of the term, the distinction between documents with different term frequencies will be ignored. In order to minimize the risk, the mean will be used to moderate the maximum likelihood estimator by combining the two estimates using the geometric distribution as follows:</p><formula xml:id="formula_3" coords="4,124.86,331.15,329.94,32.69">, , 1 .0 ˆ( ) ( ) (1 .0 ) (1 .0 ) t d tf t t d t t f R f f = × + + (<label>3</label></formula><formula xml:id="formula_4" coords="4,454.79,341.83,4.08,9.16">)</formula><p>f t is the mean term frequency of term t in documents where t occurs normalized by document length.</p><p>Using the geometric distribution has several reasons. In the first place, the mean of the distribution is equal to which is the mean probability of occurrence. Secondly, the variance of this distribution is larger than the mean. Finally, this function is defined in terms of only the mean and the tf so it can be computed without adding to the space overhead of the index and in minimal time.</p><p>So the estimate of the probability of producing the query for a given document model as follows:</p><p>, ,</p><formula xml:id="formula_5" coords="4,106.50,511.79,348.30,57.14">ˆ(1.0 ) ( , ) ( , ) ( ) , 0 ˆ( | ) t d t d R R ml avg t d d t t D P t d P t if tf P D M cf otherwise cs - ∈ ⎧ × &gt; ⎪ = ⎨ ⎪ ⎩ ∏ (<label>4</label></formula><formula xml:id="formula_6" coords="4,454.79,536.83,4.08,9.16">)</formula><p>cf t is the count of term t in the relevance class and non-relevance class . cs is the total number of tokens in those two classes. This function is computed for the probability of each terms in the documents are ranked accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Iteration</head><p>After this selection, the top n terms of the ranked list are chosen as final expansion terms. The final expanded query is combined with the original query using linear interpolation, weighted by parameter λ. Then we repeat the algorithm 1. This process constructs an iteration process. The combining parameterλ is set to be 0.8 firstly. Then this parameter decrease to 0.2 with the 0.2 decrement rate as the time of iteration increasing. When the top 2000 documents in the retrieval result do not change, we think the iteration process becomes stable. All process finished.</p><p>There are 8 results we submitted, one is the PRIS-RF baseline, and the other 7 results are got from feedback relevance model based on phrase 1 results. The whole experiments we have done are based on category B. The score of the baseline is 0.4833. The official evaluation results of the submitted 7 runs are listed in the following tables. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,170.23,415.16,56.72"><head>Table 1</head><label>1</label><figDesc>Relevance Feedback results PRIS.hit2.2 PRIS.ilps.1 PRIS.PRIS.1 PRIS.Sab.1 PRIS.SIEL.1 PRIS.twen.1 PRIS.UCSC.1</figDesc><table coords="5,90.00,202.77,402.05,24.18"><row><cell>emap</cell><cell>0.0328633</cell><cell>0.0330224</cell><cell>0.0352531</cell><cell>0.0325163</cell><cell>0.0313367</cell><cell>0.0307429</cell><cell>0.0325286</cell></row><row><cell>stAP</cell><cell>0.161541</cell><cell>0.172208</cell><cell>0.153682</cell><cell>0.178729</cell><cell>0.147595</cell><cell>0.139995</cell><cell>0.133959</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,109.22,312.61,396.13,9.16;5,90.00,328.21,415.36,9.16;5,90.00,343.81,228.67,9.16" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,316.05,312.61,189.31,9.16;5,90.00,328.21,119.15,9.16">A Cluster-Based Resampling Method for Pseudo-Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">Kyung</forename><surname>Song Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,217.40,328.21,287.96,9.16;5,90.00,343.81,172.66,9.16">The 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,104.75,359.41,283.92,9.16" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<title level="m" coord="5,194.50,359.41,144.58,9.16">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="74" to="172" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
