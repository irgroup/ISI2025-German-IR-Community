<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.44,164.71,320.45,15.55;1,224.76,186.55,161.77,15.55">University of Twente @ TREC 2009: Indexing half a billion web pages</title>
				<funder>
					<orgName type="full">MultimediaN</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,211.92,219.08,71.52,10.86"><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
							<email>c.hauff@cs.utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.10,219.08,85.31,10.86"><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
							<email>hiemstra@cs.utwente.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.44,164.71,320.45,15.55;1,224.76,186.55,161.77,15.55">University of Twente @ TREC 2009: Indexing half a billion web pages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08900BC8C3D7BC3463C2D879CF484E16</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The University of Twente participated in three tasks of TREC 2009: the adhoc task, the diversity task and the relevance feedback task. All experiments are performed on the English part of ClueWeb09.</p><p>We describe our approach to tuning our retrieval system in absence of training data in Section 3. We describe the use of categories and a query log for diversifying search results in Section 4. Section 5 describes preliminary results for the relevance feedback task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ClueWeb09</head><p>The English part of ClueWeb09 (ClueWeb09 English 1 to ClueWeb09 English 10) was indexed with the Lemur Toolkit, version 4.9. We did not attempt to index the non-English part. The documents were Krovetz stemmed. The indexed metadata fields are title, url and heading (html tags H1 to H4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anchor texts</head><p>We extracted the anchor text and indexed it separately. Due to time constraints, we were only able to match the anchor text within each of the 10 sub corpora, that is anchor text found in a document of English 2 linking to a document in English 5 was not considered. This resulted in anchor texts for 297 million docmentes, which is almost 59 % of the English part of ClueWeb09. Full anchor text extraction, which we ran after the official submission, resulted in anchor texts for about 87 % of the English web pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spam Detection</head><p>A collection crawled from the Web is likely to contain a considerable amount of spam pages. Many approaches exist to identify spam pages based on page content, hyperlink structure, URL form, the similarity between pages of a single host and combinations of those features <ref type="bibr" coords="2,308.90,181.97,10.45,9.62" target="#b0">[1,</ref><ref type="bibr" coords="2,322.46,181.97,7.81,9.62" target="#b1">2,</ref><ref type="bibr" coords="2,333.26,181.97,7.81,9.62" target="#b2">3,</ref><ref type="bibr" coords="2,344.07,181.97,7.05,9.62" target="#b3">4]</ref>. For the TREC experiments we implemented a basic spam detection mechanism, that relies on page content, page title features and URL form.</p><p>In a first step, we determined the number of different hostnames in the corpus. In total, we found more than 18 million hostnames among the English part of ClueWeb09, many with 1 or 2 pages only. While 47.78% of hosts contain 1 page only, together those pages amount to only 1.71% of all pages in the corpus. A total of 99.58% of hosts contain less than 1000 pages, and 96.75% of hosts have less than 100 pages. We concentrated our efforts of detecting spam on the small percentage of hosts with 100 or more pages in the English part of the ClueWeb09 corpus. The total number of those hosts is 589, 343. Together, they make up 75.02% of all pages in the corpus. Each of the hosts with 100 or more pages is classified as either a spam host or a no-spam host. This result is then used to postprocess the retrieval results -all documents belonging to a host classified as spam are filtered out and lower ranked results are used to fill the gap. When a document belongs to a host that was not classified it is also filtered out.</p><p>Problematic is the lack of training data for ClueWeb09. We relied on the judgments of the Web Spam Challenge in 2006 and 2007 <ref type="foot" coords="2,396.24,396.03,3.97,4.77" target="#foot_0">1</ref> , where a corpus of documents (UK domain) was labelled as spam/no-spam on the host level. When combining the training and test data provided, we find 296 spam hosts in ClueWeb09 with ten or more pages. If the minimum page count per host is set to 100 pages, we find 136 spam hosts. To boost the number of positive (i.e. spam) training examples, we manually jugded sampled hostnames from the ClueWeb corpus and found another 75 spam hosts, thus in total we have 371 positive spam examples with 10 or more documents. During the judging process we also identified 442 negative (no-spam) training examples.</p><p>The document features we rely on, include the document length, the title length, the ratio between plain text and html text, the ratio between plain text and anchor text, the number of digits in a URL, etc. For hosts with more than 100 pages in the corpus, we sample 100 pages and determine a total of 28 features for each sampled document. Then, as host features we rely on the mean and the standard deviation of each feature, resulting in a total of 56 features for each host. Given the features and the training examples we use Weka and its C4.5 implementation (called J48) to derive a decision tree. Two variations are tested, namely J48 and a cost-sensitive variety of J48 that punishes the error spam host classified as no spam 5 times as much as vice versa. The latter classifier is thus more strict, we will denote it CS5.</p><p>To evaluate both algorithms, we performed 10-fold cross-validation. For the J48 approach, 78.23% of the instances are classified correctly.   <ref type="table" coords="3,171.13,289.61,3.90,9.62" target="#tab_1">1</ref>. We tested both approaches in our submitted runs. Given the trained decision tree, all hosts with 100 or more pages in the corpus were classified as either spam or no-spam. For the J48 approach, out of 589,343 hosts, 440,043 (74.66%) hosts were found to be no-spam, 25.34% were identified as spam. Conversely, for CS5, 351,307 (59.61%) were labeled as no-spam, while 40.39% were identified as spam hosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adhoc Track</head><p>In the adhoc task, we rely on an automatic evaluation approach proposed by Soboroff et al. <ref type="bibr" coords="3,201.37,416.09,9.91,9.62" target="#b5">[6]</ref>, called Random Sampling (RS) to select the best retrieval strategy from a mix of retrieval approaches. We derive a number of retrieval runs, based on different retrieval methods such as Okapi, Language modeling, but also based on different indexed fields, e.g. the title field, the anchor text index, etc. Then, RS is employed to estimate which of those systems performs best. In previous work, RS was found to perform well to find the poor and average systems, while severely misjudging the best systems. We found in experiments with more diverse and more recent data sets, that this is only a significant problem in cases where manual systems are included in the pool of systems.</p><p>Apart from estimating the best retrieval method, we also face the issue of merging the results. The indices are on 10 different machines, thus at one point we need to merge the ranked lists of results. In Figure <ref type="figure" coords="3,387.49,559.61,4.98,9.62" target="#fig_0">1</ref> the overall system is depicted. Each query is sent to the 10 indices C 1 to C 10 and m different retrieval methods are applied, resulting in m different result lists. Then, spam detection is applied to each ranked list and those documents, determined to be spam are filtered out and removed from the list. We deal with each set of result lists separately, RS is applied to estimate which of those is the best performing one. Then, the 10 result lists are merged to form the final result list.</p><p>Merging was tested in three flavours: (i) normalizing of retrieval scores by ZMUV (zero mean, unit variance) and merging of the results based on the normalized scores, (ii) round robin merging, by first taking the top result of each index, then the second of each index and so on to possibly diversify the results and (iii) to merge the unnormalized scores.</p><p>In order for RS to be useful, there should be a relatively large number of retrieval runs to rank, but also the runs should differ in their results. For instance, it may not be very useful to derive 20 Okapi runs, that only differ in their parameter settings. We attempted to derive a variety of retrieval runs. As basic retrieval approach we rely on Dirichlet smoothed language modeling. We derived 28 runs for each of the 10 corpora including runs based on document title, anchor text, non-uniform document priors, pseudo-relevance feedback, etc.</p><p>Submitted as official runs were the runs listed in Table <ref type="table" coords="4,400.46,407.93,3.90,9.62">2</ref>. For twCSrs9N the results are filtered according to CS5 and ZMUV merging is employed. For run twCSrsR merging was performed in a round robin manner. Merging without normalization and J48 spam detection was tested in run twJ48rsU. Round robin merging results in the best mean average precision, but merging without normalization results in better early precision. runID map bpref P@5 P@100 eMAP eP@5 eP@100 twCSrs9N 0.0297 0.1480 0.2080 0.0758 0.0217 0.2250 0.2421 twCSrsR 0.0403 0.1700 0.2000 0.0960 0.0263 0.2200 0.2570 twJ48rsU 0.0284 0.1211 0.2760 0.0742 0.0200 0.2800 0.2501 Table <ref type="table" coords="4,236.64,550.73,3.90,9.62">2</ref>: Overview of submitted adhoc runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Web Diversity Track</head><p>In the diversity task, we rely on categories and a query log to determine topic aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Category &amp; query log information</head><p>We derived a list of 98 categories from the Open Directory Project (ODP),<ref type="foot" coords="5,473.04,144.99,3.97,4.77" target="#foot_1">2</ref> mostly consisting of single terms only, such as education, animal, etc. Then, we relied on Indri's Boolean AND operator, #band, to force the retrieval system to only return documents containing all query terms and the category term. This step was performed for each combination of query and category term(s). A ranking of category terms was determined by sorting them according to retrieval score of the top retrieved document. The five categories with the highest retrieval score were kept. For instance, for the query solar panel (wt09-45) and the subcorpus English 1, the top five categories found are product, company, environment, video and research.</p><p>As an alternative source of information, we used a query log consisting of over 21 million queries <ref type="bibr" coords="5,237.85,277.61,9.91,9.62" target="#b4">[5]</ref>. To respect the anonymity of the users, we used a stripped version of the query log that only contains the actual queries issued in random order (and none of the other metadata, such as user id and clicks). Each query was located in the log and the terms that co-occur with the query most often were determined. For instance, the query dinosaurs (wt09-14) occurred in the log as dinosaurs (194 times), dinosaurs com (12), walking with dinosaurs (9), www dinosaurs com (7), the dinosaurs (7), dinosaurs tv show <ref type="bibr" coords="5,417.60,349.37,11.62,9.62" target="#b5">(6)</ref>, dinosaurs pictures <ref type="bibr" coords="5,171.48,361.37,11.62,9.62" target="#b5">(6)</ref>, types of dinosaurs <ref type="bibr" coords="5,272.40,361.37,11.62,9.62" target="#b5">(6)</ref>, dinosaurs for kids <ref type="bibr" coords="5,372.84,361.37,12.76,9.62" target="#b5">(6)</ref> and dinosaurs the tv show.</p><p>From each of the 50 queries, a number of new queries are derived, containing the original query string and one of the top five category terms and the query log terms respectively. Retrieval was performed for each query; subsequently those retrieval results were merged. Here, we performed round robin merging, as we do indeed want to cover different categories. Score normalization with subsequent merging might not bring about the desired results, if a particular aspect of a query has too low scores. This process is repeated for each corpus English 1 to English 10. In a final step, the results over the 10 corpora are merged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>The two runs officially submitted are described in Table <ref type="table" coords="5,379.34,527.09,3.90,9.62" target="#tab_2">3</ref>. Spam detection was again used as a preprocessing step, the result merging occurred either in a round robin fashion (RBB) over the 10 subcorpora or according to normalized scores (RNB). runID α-ndcg@5 α-ndcg@10 α-ndcg@20 IA-P@5 IA-P@10 IA-P@20 twCSodpRNB </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Relevance Feedback Track</head><p>The Relevance Feedback track consists of two phases. In the first phase, for each query, five documents are identified that are judged by an assessor. In the second phase then, the knowledge gained of the (non-) relevance of those documents shall be exploited. For both phases the Category B data set was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Phase 1</head><p>We relied on Indri's query language for a baseline run with combinations of keywords, phrases and unordered windows of size 8 and 12. The top five documents, from different domains, retrieved per query were used. Thus, the only filtering performed was according to URL in order to avoid returning five very similar documents from the same domain. This run is twen.1. Of the top five documents of the 50 queries, fourteen had not a single relevant document among them while three had not a single non-relevant document among them. Thirteen queries had at least one highly relevant document.</p><p>The second submitted run twen.2 is a keyword only Dirichlet smoothed language modeling run (µ = 1500), without domain filtering applied. Sixteen queries had not a single relevant document in the top five documents, two queries had not a single non-relevant document and twelve queries had at least one highly relevant document among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Phase 2</head><p>Given a number d = {10, 50, 100, 250, 500} of top retrieved documents from the baseline retrieval approach (LM with Dirchlet smoothing: µ = 1500), the t = {5, 10, 20, 50, 100} most discriminative terms in them are determined. For each parameter combination the expanded query is formed by adding the t most discriminative terms to the original query. Retrieval is performed, yielding runs r 1 to r t×d .</p><p>Given the relevance judgments of five documents for each query, the quality of each r i is determined in an adhoc fashion. The relevance of a document is either 2 (highly relevant), 1 (relevant) or 0 (not relevant). For each relevance judgment, the rank of the document in the run's top 1000 retrieved documents is determined. If the docid is not found, a value of 0 is assigned, if it is a highly relevant document, its score is 2 × (1000rank), if it is relevant (1000rank) and if it is not relevant -0.5 × (1000rank). A run that does not contain any of the five judged documents is considered to be better than a run that contains one or more non-relevant documents but no relevant one.</p><p>The run of the t × d runs that has the highest quality for a given set of relevance judgments is the run used for that particular query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>Spam detection (CS5) was performed as a post-processing step. Reranking by document length was switched on for twen.ugTr.1, twen.twen.1 and twen.UCSC.2. The results of the submitted runs are described in Table <ref type="table" coords="7,381.38,170.09,3.90,9.62" target="#tab_3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This draft report presents perliminary results for the TREC adhoc task, the diversity task, and the relevance feedback task. We investigated, amongst others, unsupervised tuning of our system and the use of categories and query log information for diversifying search results. Additional experimental results will be described in the final version of this paper, showing if the approaches were indeed successful.</p><p>To deal with the new ClueWeb09 collection, we needed to aquire at least two skills that are unnessesary for tests on previous TREC collections. First, the data, even if only the English part is used, cannot be indexed on a single machine, requiring some cluster computing skills. Second, web spam seems to be a much bigger problem in ClueWeb09 than in previous (web) TREC collections, requiring some spam detection skills.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,210.24,254.93,190.84,9.62"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Retrieval chain of the adhoc task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,133.80,223.85,343.83,75.38"><head>Table 1 :</head><label>1</label><figDesc>Confusion matrices of spam versus no-spam examples based on 10-fold cross-validation. yielded 76.51% correctly classified. Though CS5 leads to less correctly classified examples, it filters out more spam hosts as can be seen in the confusion matrices of Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,139.80,598.61,334.74,45.26"><head>Table 3 :</head><label>3</label><figDesc>Overview of submitted TREC runs to diversity task.</figDesc><table coords="5,139.80,598.61,334.74,21.98"><row><cell></cell><cell>0.123</cell><cell>0.145</cell><cell>0.167</cell><cell>0.062</cell><cell>0.063</cell><cell>0.056</cell></row><row><cell>twCSodpRBB</cell><cell>0.144</cell><cell>0.164</cell><cell>0.193</cell><cell>0.067</cell><cell>0.062</cell><cell>0.061</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,133.80,170.09,343.82,165.98"><head>Table 4 :</head><label>4</label><figDesc>. Each run is created by selecting the best (according to the given relevance judgments) expanded query run out of 25 possible ones. TREC runs submitted to the relevance feedback task.</figDesc><table coords="7,236.76,216.83,137.94,95.84"><row><cell>runID</cell><cell>emap stAP</cell></row><row><cell>twen.twen.1</cell><cell>0.0317 0.1528</cell></row><row><cell>twen.twen.2</cell><cell>0.0306 0.1344</cell></row><row><cell>twen.ilps.1</cell><cell>0.0369 0.1481</cell></row><row><cell>twen.FDU.1</cell><cell>0.0311 0.1415</cell></row><row><cell cols="2">twen.YUIR.2 0.0398 0.1384</cell></row><row><cell>twen.ugTr.1</cell><cell>0.0327 0.1297</cell></row><row><cell cols="2">twen.UCSC.2 0.0343 0.1294</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,149.04,667.29,92.39,7.30"><p>http://webspam.lip6.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,149.04,666.97,81.00,7.60"><p>http://www.dmoz.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The research was funded in part by <rs type="funder">MultimediaN</rs>. We are greatful to <rs type="person">Yahoo Research, Barcelona</rs>, for contributing to our Hadoop cluster.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,149.28,647.81,328.39,9.62;7,149.28,659.69,328.17,9.62;8,149.28,127.73,328.29,9.62;8,149.28,139.73,53.89,9.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,434.52,647.81,43.15,9.62;7,149.28,659.69,114.43,9.62">Query-log mining for detecting spam</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Corsi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,288.60,659.69,188.85,9.18;8,149.28,127.73,294.91,9.18">AIRWeb &apos;08: Proceedings of the 4th inter-national workshop on adversarial information retrieval on the web</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,149.29,159.65,328.31,9.62;8,149.28,171.53,328.27,9.62;8,149.28,183.53,328.22,9.18;8,149.28,195.53,328.45,9.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,452.28,159.65,25.32,9.62;8,149.28,171.53,272.95,9.62">Know your neighbors: web spam detection using the web topology</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.28,171.53,28.27,9.18;8,149.28,183.53,328.22,9.18;8,149.28,195.53,230.18,9.18">SIGIR &apos;07: Proceedings of the 30th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,149.28,215.45,328.20,9.62;8,149.28,227.33,328.17,9.62;8,149.28,239.33,328.29,9.62;8,149.28,251.33,43.93,9.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,330.88,215.45,146.61,9.62;8,149.28,227.33,215.14,9.62">Spam, damn spam, and statistics: using statistical analysis to locate spam web pages</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,384.24,227.33,93.21,9.18;8,149.28,239.33,295.33,9.18">WebDB &apos;04: Proceedings of the 7th International Workshop on the Web and Databases</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,149.28,271.25,328.21,9.62;8,149.28,283.13,328.13,9.62;8,149.28,295.13,285.49,9.62" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,388.85,271.25,88.65,9.62;8,149.28,283.13,137.99,9.62">Detecting spam web pages through content analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ntoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.40,283.13,163.01,9.18;8,149.28,295.13,195.55,9.18">WWW &apos;06: Proceedings of the 15th international conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,149.28,315.05,328.17,9.62;8,149.28,326.93,327.93,9.18;8,149.28,338.93,62.05,9.62" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,346.48,315.05,84.75,9.62">A picture of search</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Torgeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,456.36,315.05,21.09,9.18;8,149.28,326.93,327.93,9.18;8,149.28,338.93,31.40,9.18">Info-Scale 06: Proc. of the 1st international conference on Scalable information systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,149.28,358.85,328.22,9.62;8,149.28,370.85,328.17,9.62;8,149.28,382.73,327.92,9.18;8,149.28,394.73,122.77,9.62" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,327.04,358.85,150.47,9.62;8,149.28,370.85,85.49,9.62">Ranking retrieval systems without relevance judgments</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,255.48,370.85,221.97,9.18;8,149.28,382.73,327.92,9.18;8,149.28,394.73,34.46,9.18">SIGIR &apos;01: Proceedings of the 24th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
