<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,56.69,96.55,345.78,15.29;1,56.69,118.47,444.48,15.29">Machine Learning for Information Retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,56.69,153.34,116.02,12.74"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.22,153.34,81.39,12.74"><forename type="first">Mona</forename><surname>Mojdeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,56.69,96.55,345.78,15.29;1,56.69,118.47,444.48,15.29">Machine Learning for Information Retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">40605C1331C06316F8F665AD5FC8824A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the TREC 2009, we exhaustively classified every document in each corpus, using machine learning methods that had previously been shown to work well for email spam <ref type="bibr" coords="1,330.27,255.83,10.52,8.73" target="#b8">[9,</ref><ref type="bibr" coords="1,344.71,255.83,7.01,8.73" target="#b2">3]</ref>. We treated each document as a sequence of bytes, with no tokenization or parsing of tags or meta-information. This approach was used exclusively for the adhoc web, diversity and relevance feedback tasks, as well as to the batch legal task: the ClueWeb09 and Tobacco collections were processed end-to-end and never indexed. We did the interactive legal task in two phases: first, we used interactive search and judging to find a large and diverse set of training examples; then we used active learning process, similar to what we used for the other tasks, to find find more relevant documents. Finally, we fitted a censored (i.e. truncated) mixed normal distribution to estimate recall and the cutoff to optimize F 1 , the principal effectiveness measure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Processing ClueWeb09 for Web and Relevance Feedback</head><p>We used all the English documents in the full (category A) ClueWeb09 collection. The four distribution drives were mounted on a standard PC with Intel E7400 2.80GHz dual-core processor, 4GB RAM. Decompressing the 12TB of data using gzip requires about 12 hours using both cores; the learning method (for 50 topics in parallel) adds about 6 hours to this time. That is, the score for every document in the collection with respect to every topic is computed in about 18 hours.</p><p>To achieve this processing speed, it was necessary to simplify and streamline the learning method. We explain the simplified method along with the more "heavyweight" method from which it was derived in Section 5. To test and tune our approach we first used previous TREC adhoc and web collections. We then composed 67 queries that anticipated the TREC 2009 Web queries as well as we were able, and processed them on the ClueWeb09 corpus (Table <ref type="table" coords="1,87.23,497.90,4.98,8.73">1</ref> on page 1). A cursory examination of the results indicated that, while our learning method was competitive on previous TREC collections, <ref type="bibr" coords="1,193.17,509.85,22.69,8.73">when</ref>   We anticipated that Wikipedia (which is a subset of the ClueWeb09 collection) would yield higher-quality results, but low recall, missing some topics entirely. For this reason, we did two runs: one fetched the top 10,000 documents for each topic from the entire collection; one fetched the top 10,000 Wikipedia articles for each topic. The Wikipedia results were used for pseudo-relevance feedback, but the collection was not processed again. Instead we used machine learning to re-rank the 20,000 documents per topic, and submitted the top 1000.</p><p>For the relevance feedback track, we used much the same method, but used the supplied feedback documents instead of pseudo-relevance feedback. For the Web diversity task, we re-ranked the 20,000 using a naive Bayes classifier designed to exclude duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Processing Tobacco for Batch Legal</head><p>The batch legal task used some 8M documents from the tobacco collection. We ran three spam filters over every document as if it were spam: our on-line logistic regression filter from TREC 2007 <ref type="bibr" coords="2,426.78,482.81,9.96,8.73" target="#b2">[3]</ref>; a naive Bayes spam filter modeled after Graham and Robinson <ref type="bibr" coords="2,226.56,494.77,10.52,8.73" target="#b6">[7,</ref><ref type="bibr" coords="2,241.32,494.77,11.62,8.73" target="#b10">11]</ref>; an on-line version of the BM25 relevance feedback method; batch logistic regression, as implemented by the Liblinear package. 1 The results were calibrated to log-odds using 2-fold cross validation and fused using logistic regression <ref type="bibr" coords="2,276.99,518.68,10.52,8.73" target="#b8">[9]</ref> and, for a different run, reciprocal rank fusion <ref type="bibr" coords="2,492.78,518.68,9.96,8.73" target="#b3">[4]</ref>. The cutoff value of k was chosen so as to optimize F 1 using two-fold cross validation. (In retrospect, we see that this was a mistake, as the training data grossly underestimates the number of relevant documents.) Our third and final run used batch logistic regression trained on the entire training set (as opposed to half, for two-fold cross validation). No calibration was done, and cutoff value k was determined using 2-fold cross validation with the same method. The cutoff value for "highly relevant" was set arbitrarily to the value 0.1k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Processing Enron for Interactive Legal</head><p>The interactive legal task used a new version of the Enron collection with about 800k "documents." During the course of doing the task, it became apparent that about half the documents were incorrectly processed duplicates of others in the collection (which were correctly processed). The upshot is that there are about 250K unique messages, and about 100K unique attachments, many of which are vacuous. We were assigned 4 of the 8 topics: 201 (Prepay transactions), 202 (FAS 140/125 transactions), 203 (Financial forecasts), and 207 (Football).</p><p>Our approach consisted of two phases: interactive search and judging, and interactive learning. Our interactive search and judging used essentially the same tools and approach as we used in TREC 6 <ref type="bibr" coords="2,461.84,700.96,10.52,8.73" target="#b5">[6,</ref><ref type="bibr" coords="2,476.89,700.96,12.73,8.73" target="#b12">13]</ref> to prepare an Fig. <ref type="figure" coords="3,223.34,398.89,3.87,8.85">2</ref>: Interactive search and judging interface independent set of qrels for the adhoc task. We used the Wumpus search engine<ref type="foot" coords="3,405.42,429.21,3.97,6.16" target="#foot_0">2</ref> and a custom html interface that showed hits-in-context and radio buttons for adjudication (Figure <ref type="figure" coords="3,348.64,442.79,4.98,8.73">2</ref> on page 3). Available for reference were links to the full text of the document and to the full email message containing the document, including attachments in their native format. The resulting qrels were used to train an on-line logistic regression spam filter.</p><p>The logistic regression spam filter yields an estimate of the log-odds that each document is relevant. We constructed a very efficient user interface to review documents selected by this relevance score. The primary approach was to examine unjudged documents in decreasing order of score, skipping previously adjudicated documents. Each document was rendered as text and the reviewer hit a single key ("s" for relevant; "h" for not relevant, see Figure <ref type="figure" coords="3,550.33,514.52,4.98,8.73" target="#fig_1">3</ref> on page 4) to adjudicate the document and move on to the next record. About 50,000 documents were reviewed, at an average rate of 20 documents per minute (3 seconds per document). We also examined documents in different orders; in particular, we examined documents with high scores that were marked "not relevant" and documents with low scores that were marked "relevant". From time to time we recomputed the scores by running training the filter on the augmented relevance assessments. From time to time we revisited the interactive search and judging system, to augment or correct the relevance assessments as new information came to light.</p><p>Our original intent was to judge the top-ranked documents in each run, to estimate the density of relevant documents as a function of releance score, and to select the appropriate cutoff to optimize F 1 . However, when we estimated the density of relevant documents we found that it was feasible to perform manual adjudication well beyond the optimal cutoff. The upshot is that we reviewed every document that we submitted as relevant for each of the topics, and the number of relevant documents we found agrees well with our statistical estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning Details</head><p>The general learning approach (from <ref type="bibr" coords="3,222.24,696.81,9.96,8.73" target="#b0">[1]</ref>, review copy available on request) was the same for all tasks, with some differences in detail. We describe the approach to feature engineering taken for each task, followed by particular </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Engineering</head><p>Each document was treated as a sequence of bytes without regard to markup or other meta-information. Only the first 30K bytes of each message were used. From previous email and Web spam experiments <ref type="bibr" coords="4,465.13,437.52,10.52,8.73" target="#b1">[2,</ref><ref type="bibr" coords="4,479.13,437.52,7.01,8.73" target="#b2">3]</ref>, we had reason to believe that truncating each document would improve both efficiency and effectiveness. We used binary features derived from the occurrence or non-occurrence of particular substrings in the text. For the Legal Track (and some phases of the Web track) these substrings were overlapping byte 4-grams. As there are potentially 2 32 distinct byte 4-grams, we used hashing to reduce the dimensionality of the feature space to the order of 10 8 , facilitating the use of a simple array in place of a dictionary data structure. The collisions that result from this dimensionality reduction have negligible impact on classification performance.</p><p>For the primary ClueWeb09 retrieval phase, we processed the query and document text (on the fly) to convert upper to lower case, and to treat all strings of non-alphabetic characters as a single space character. We considered all substrings of the query text to be features, and constructed a finite-state machine to recognize all such features in the WARC-format source. From a random sample of 250,000 documents, we selected only those features with a document frequency of less than 0.5. A new finite state machine was constructed to recognize this reduced feature set, and run on the WARC representation of the entire corpus. Based on pilot experiments with previous TREC collections, we departed somewhat from the use of pure binary features: we used an exponential term frequency weight, and also considered the position of the feature relative to the beginning of the document. The same approach was applied both to the entire (English) collection and to the Wikipedia subset.</p><p>For the second phase of the Web adhoc and Relevance Feedback tasks, we used binary 4-grams as described above. For the Web diversity task, we used binary "word" features instead of byte 4-grams, and clustered results according to the k most discriminative words, for k = 1, 3, 5. Our three diversity runs (one for each value of k) consisted of only the top-ranked document in each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning Methods</head><p>Logistic regression was used as the principal learning method for all tasks. For the most part we used an online gradient descent approach adapted from spam filtering. In adaptive training mode, this method processes the documents in sequence, and applies one gradient descent optimization step for each message processed. In classification mode, it is a generalized linear classifier that estimates the log-odds of relevance. Recall that each feature is binary, and so a message is represented as a vector X consisting of 10 8 zeroes and ones. The vector is sparse, containing at most 32,000 (and usually nowhere near) 32,000 non-zero entries which can be processed efficiently. The classifier consists of a weight vector β such that</p><formula xml:id="formula_0" coords="5,224.17,128.05,326.89,22.30">score = β • X ≈ log Pr[relevant] Pr[not relevant] . (<label>1</label></formula><formula xml:id="formula_1" coords="5,551.06,134.79,4.24,8.73">)</formula><p>The gradient descent update rule is very simple:</p><formula xml:id="formula_2" coords="5,238.35,181.03,312.72,22.30">β ← β + (c - 1 1 + e -score ) • X , (<label>2</label></formula><formula xml:id="formula_3" coords="5,551.06,187.77,4.24,8.73">)</formula><p>where c = 1 if the document is relevant; otherwise c = 0. Note that</p><formula xml:id="formula_4" coords="5,354.91,212.88,107.49,13.52">1 1+e -score ≈ Pr[relevant].</formula><p>Because there are in general far fewer relevant than non-relevant documents, we equalized the number by training on a random relevant document immediately after training each non-relevant document.</p><p>For the Web tasks, we constructed synthetic training examples as follows. The query itself was taken to be the sole positive example, and 250,000 randomly selected documents were used as negative examples. The resulting classifier was then applied to the entire corpus. The same approach was used for the Wikipedia subset, with negative training examples selected from only that subset. During classification, a priority queue was used to record the doc-ids of the top-scoring 10,000 documents for each query. A second pass was used to fetch these documents for further processing.</p><p>For pseudo-relevance feedback, we selected the top several documents from the Wikipedia run, and used them as positive examples. We did not include the original query as an example. We used randomly selected Wikipedia documents (not necessarily from the run) as negative examples. We trained a naive Bayes classifier on these examples, and applied it to the 20,000 Wikipedia and non-Wikipedia results. The top-scoring 1000 documents per topic were returned. We used naive Bayes instead of logistic regression because of its noise-tolerance properties <ref type="bibr" coords="5,56.69,382.18,14.61,8.73" target="#b11">[12]</ref>, as we expect that some of our pseudo-relevant examples are in fact non-relevant. Feature selection was done on a document-by-document basis: for each document 60 features were used: the 30 with the largest score, and the 30 with the smallest score. Document-by-document feature selection, suggested by Graham <ref type="bibr" coords="5,460.94,406.09,9.96,8.73" target="#b6">[7]</ref>, is commonly used in email spam filtering. The naive Bayes classifier is the same generalized linear classifier as generated by logistic regression (equation 1), but with</p><formula xml:id="formula_5" coords="5,85.72,450.92,465.35,23.22">β i = log |{relevant documents with X i = 1}| + |{(relevant documents with X i = 0}| + -log |{nonrelevant documents with X i = 1}| + |{(nonrelevant documents with X i = 0}| + . (<label>3</label></formula><formula xml:id="formula_6" coords="5,551.06,457.67,4.24,8.73">)</formula><p>The set of documents used for pseudo-relevance feedback in the Web adhoc task was determined by classification score: all documents whose score different by less than 1 from that of the top-ranked document were used as positive examples. The difference of 1 is somewhat, but not entirely, arbitrary. It corresponds to an odds ratio of about 1.4, which is in the range of what we commonly think of as a "substantive" difference. Intuitively, if there is not much to choose among the top documents, we use many, but if the top documents are distinctive, we use few.</p><p>For the Relevance Feedback track, we use the adjudicated-relevant documents as positive training examples, and add the adjudicated-non-relevant documents to the randomly selected negative examples. It is not appropriate to use only the adjudicated non-relevant examples, as they come from a population that is far from representative. But they are important to tilt the classifier away from other documents like them. As the baseline for the relevance track, we used the two top-ranked documents as positive examples for pseudo-relevance feedback.</p><p>For the Legal batch task, we used an ensemble of learning methods. We used the supplied qrels as positive and negative examples, and augmented the negative examples with 20,000 documents randomly selected from the corpus. In addition to the logistic regression and naive Bayes methods mentioned above, we used L 2 -regularized (batch) logistic regression, and also an adaptive version of the BM25 and Robertson's relevance feedback method <ref type="bibr" coords="5,56.69,652.11,15.50,8.73" target="#b9">[10]</ref> to select 20 feedback terms. In order to combine the results of the four methods it is necessary to calibrate their scores. While our on-line logistic regression method yields a log-odds estimate, it may be overfitted to the training examples. L 2 -regularized logistic regression mitigates overfitting by limiting the magnitude of β, with the result that the score ceases to be a log-odds estimate. Naive Bayes is well-known to yield poor estimates, and BM25 makes no estimate at all -its score purports to be useful only for ranking. To calibrate the four sets of scores, we perform 2-fold cross validation. The corpus is randomly partitioned into two equal halves, and each classifier is trained on one half and applied to the other, yielding a score s d for every document d. s d is converted to a log-odds </p><formula xml:id="formula_7" coords="6,546.82,196.79,8.49,8.73">)<label>4</label></formula><p>These estimates are averaged to yield an overall log-odds estimate, which is used to rank the final result for our primary submission. For our secondary submission, we combined the uncalibrated scores from 2-fold cross validation using reciprocal rank fusion <ref type="bibr" coords="6,183.58,247.41,9.96,8.73" target="#b3">[4]</ref>. For our tertiary submission, we used L 2 -regularized logistic regression, without cross-validation. That is, the training examples consisted of all the qrels, plus a sample of negative examples from the whole collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Interactive Discovery</head><p>We used two interactive tools for the interactive legal task: a search-based system and an active-learning-based system. The search system -outlined above -was used primarily at the outset, to explore the dimensions of the topic and to find as many categories of relevant documents as possible. For example, for topic 201, the task was to find documents related to circular prepay transactions. The search system was used to identify particular transactions, to investigate whether they were or were not circular, and to identify the people and sorts of documents related to the transactions. A simple user interface allowed the searcher to record the relevance of each document, and to select or elide previously-judged ones. The learning system was used to identify more potentially relevant documents, to review those documents, and to record relevance assessments for them.</p><p>For the most part, searching was done before learning. But the search system was also used when necessary to investigate (and record) the relevance of documents uncovered by learning. And the learning process was repeated many times, each time using the recorded assessments as training examples and capturing more assessments via the user interface.</p><p>The technology of the search system is unexceptional and has been documented elsewhere <ref type="bibr" coords="6,478.89,465.50,9.96,8.73" target="#b4">[5]</ref>. The learning component was exactly on-line logistic regression with two-fold cross validation, as used for the batch task. On our standard PC platform, the learning system took about 18 minutes to classify the 800,000 documents in the collection. Since we were working on four topics, it was a simple matter to review one topic while the classifier was running on another.</p><p>Our original plan was to use these interactive tools to identify as many relevant documents as possible with reasonable effort, and to augment these documents with top-scoring documents so as to improve recall, thus optimizing the F 1 effectiveness measure. To determine how many such documents to include, it is necessary to estimate precision and recall at all possible cutoff points, and pick the best. To do so, we fitted the maximum likelihood normal distribution to the scores of the relevant documents that we found when judging to a particular cutoff, and used the area under the curve to estimate the number of documents beyond the cutoff. This method agreed remarkably well with the total number of documents we actually found, which gives us some reason to believe it is accurate. Somewhat disappointingly, the result indicated that the optimal strategy was to include no unassessed documents, as any improvement in recall would be more than offset by a degradation in precision. The estimated number of relevant documents, optimal submission cutoff, and actual number of examined and judged-relevant documents are shown in table 2. At the time of writing, official estimates for the number of relevant documents were not available.</p><p>We state some assumptions underlying the method to estimate recall. First, learning methods are, of course, blind to the definition of relevance, and serve only to identify documents "like the training examples." So when we estimate recall, we are really estimating how many documents "like the training examples" there are. If there happen to be some entirely dissimilar documents that are relevant, they will not be counted. These documents might be in a different language, or perhaps in a different format such as an image or multimedia. We may only assume that we have done due diligence in our initial search, so we have reason to believe that an insubstantial Tab. 3: Web Track Ad hoc results. P@10 results for all Category A ad hoc submissions are reproduced here. watwp is our Wikipedia-only run; watprf is our pseudo-relevance feedback run, using the top watwp results as seeds; watrrfw is the combination of the two using reciprocal rank fusion.</p><p>number of dissimilar documents exist. This assumption is not unique to our efforts; it is tacit in the TREC pooling method, and also methods based on sampling like those used to evaluate the Legal track results.Second, we assume that the distribution of log-odds scores for relevant documents is normal. This assumption is generally true for natural phenomena, and appears to hold for the scores from our classifier as well. Figure <ref type="figure" coords="7,451.69,559.44,4.98,8.73">4</ref> illustrates the process of fitting a Gaussian to the top-scoring documents that are judged relevant. Estimating precision is more problematic. Based on our previous experience, we thought it is unlikely that the precision of our human assessment is greater than 0.7, notwithstanding our use of the topic authorities. For topic 203 in particular, we did not think we were able to acquire a firm enough grasp on the notion of a "responsive" document to predict precision greater than 0.5. Statistical estimation is of little help, as we judged every document that was submitted as relevant. The end result hinges on our agreement with the official adjudication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Table <ref type="table" coords="7,84.55,681.95,4.98,8.73">3</ref> shows precision at cutoff 10 for all Category A ad hoc runs, reproduced from the appendices. Our runs, prefixed by wat (not Wat), show that the method is effective. Our diversity results were unremarkable.  <ref type="table" coords="8,98.84,407.14,4.98,8.73">5</ref> shows recall, precision and F 1 for our four Legal Track interactive efforts. The first three columns show our predictions for these measures at the document level. The centre three columns show the the document-level measures computed from the official adjudicated results. The right three columns show message-level measurements. With the exception of topic 203, our recall predictions were consistently optimistic, while our precision predictions were pessimistic. Pessimism outweighed optimism with the net effect that our F 1 predictions were all pessimistic but, with the exception of 203, reasonably accurate. Our submission for 203 was best characterized as a "Hail Mary" play, one that appears to have been successful.</p><p>Our Legal Track batch efforts achieved the best overall F 1 @k for both relevant and highly relevant documents. However, we argue that the actual numbers -0.214 and 0.190 respectively -are essentially meaningless, due to assessment error. The reader is referred to the track overview <ref type="bibr" coords="8,326.85,514.73,10.52,8.73" target="#b7">[8]</ref> for further details, and a comparison of our batch results with those of the 2008 interactive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>One of the questions raised with regard to the ClueWeb09 collection was: how difficult will it be to index the collection? We circumvented this question by applying a sequential content-based classifier to the entire corpus. For 50 or even 500 topics, we suggest this approach is far cheaper than indexing. Furthermore, sequential processing facilitates exhaustive searching of large archives, as occasioned by legal discovery and information archaeology tasks. The methods we have developed are essentially domain independent. The finite-state classifier we used for ClueWeb09 could apply to any corpus; the methods of interactive search and judging and active learning require a certain amount of domain knowledge. We advocate further research into measuring and minimizing the amount of interaction necessary to acquire the necessary domain-specific information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,201.99,292.49,208.02,8.85"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Low-quality result for "star wars" query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,231.07,347.34,149.85,8.85;4,106.55,56.69,398.91,222.92"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Minimalist review interface</figDesc><graphic coords="4,106.55,56.69,398.91,222.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,106.55,56.69,398.90,327.65"><head></head><label></label><figDesc></figDesc><graphic coords="3,106.55,56.69,398.90,327.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,62.67,509.85,527.24,215.84"><head></head><label></label><figDesc>run on ClueWeb09 it yielded almost entirely spam or low-quality "junk" web</figDesc><table coords="1,62.67,540.08,527.24,164.15"><row><cell>star wars</cell><cell>money</cell><cell>gates</cell><cell>vacuum</cell><cell>whip</cell></row><row><cell>star wars sdi</cell><cell>money pink floyd</cell><cell>gates fences</cell><cell>vacuum cleaner</cell><cell>whip egg</cell></row><row><cell>star wars luke</cell><cell>money beatles</cell><cell>gates steve</cell><cell>high vacuum</cell><cell>whip crop</cell></row><row><cell>sdi</cell><cell>money spinal tap</cell><cell>windows</cell><cell>vacuum</cell><cell>whip topping</cell></row><row><cell cols="2">selective disseminatinon spinal tap</cell><cell>windows doors</cell><cell>stream</cell><cell>party whip</cell></row><row><cell>spock</cell><cell cols="2">spinal tap procedure windows os</cell><cell>stream process</cell><cell>whip it</cell></row><row><cell>spock kirk</cell><cell>spinal tap lyrics</cell><cell>apple</cell><cell>stream creek</cell><cell>bull whip</cell></row><row><cell>spock benjamin</cell><cell>jaguar</cell><cell>apple records</cell><cell cols="2">stream education WHIP 1350 AM</cell></row><row><cell>obama</cell><cell>jaguar xj</cell><cell>apple computer</cell><cell>honda stream</cell><cell>whip flagellate</cell></row><row><cell>obama japan</cell><cell>jaguar cat</cell><cell>macintosh</cell><cell>fish</cell><cell>chain whip</cell></row><row><cell>barack obama</cell><cell>jaguar fender</cell><cell cols="2">macintosh apple fishing</cell><cell>The Whip</cell></row><row><cell>capital</cell><cell>fender</cell><cell cols="2">apple macintosh go fish</cell><cell>whip antenna</cell></row><row><cell>capital city</cell><cell>fender bender</cell><cell>dead poets</cell><cell>fish episodes</cell><cell>WHIP walks hits inning pitched</cell></row><row><cell>capital assets</cell><cell>fender gibson</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="1,190.24,716.84,231.52,8.85"><p>Tab. 1: Pilot Queries composed prior to TREC 2009.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,56.69,66.12,498.61,388.98"><head></head><label></label><figDesc>Fitting a Gaussian distribution. The left panel shows the scores of all judged-relevant documents for topic 201 vs. the maximum likelihood Gaussian distribution. Typically, it is feasible to judge only the top-scoring documents, resulting in a left censored distribution, shown in the right panel. The maximum likelihood fit use used to estimate the number of relevant unjudged documents.</figDesc><table coords="7,56.69,66.12,475.62,388.98"><row><cell></cell><cell cols="5">Gaussian (Normal) vs. Score Distribution</cell><cell></cell><cell cols="8">Gaussian (Normal) vs. Censored Score Distribution</cell></row><row><cell cols="2">Gaussian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Gaussian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20%</cell><cell>Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell>Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-2</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>-2</cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell cols="5">Log-Odds Score from Classiffier</cell><cell></cell><cell></cell><cell cols="6">Log-Odds Score (Left-Censored at 3.0)</cell></row><row><cell cols="2">Fig. 4: Run</cell><cell></cell><cell>P@10</cell><cell></cell><cell>Run</cell><cell></cell><cell cols="2">P@10</cell><cell>Run</cell><cell></cell><cell></cell><cell>P@10</cell><cell></cell></row><row><cell></cell><cell>uvamrftop</cell><cell></cell><cell cols="2">0.4100</cell><cell>watprf</cell><cell></cell><cell cols="5">0.3360 WatSdmrm3we</cell><cell>0.1640</cell><cell></cell></row><row><cell></cell><cell>MS2</cell><cell></cell><cell cols="2">0.4060</cell><cell cols="2">muadimp</cell><cell cols="2">0.3006</cell><cell cols="3">UMHOObm25IF</cell><cell>0.1640</cell><cell></cell></row><row><cell></cell><cell cols="4">yhooumd09BGM 0.4040</cell><cell cols="2">Sab9wtBf1</cell><cell cols="2">0.2880</cell><cell cols="2">pkuSewmTp</cell><cell></cell><cell>0.1480</cell><cell></cell></row><row><cell></cell><cell>MSRAC</cell><cell></cell><cell cols="2">0.4000</cell><cell cols="2">muadibm5</cell><cell cols="2">0.2788</cell><cell cols="2">pkuStruct</cell><cell></cell><cell>0.1460</cell><cell></cell></row><row><cell></cell><cell cols="4">yhooumd09BGC 0.3840</cell><cell cols="2">Sab9wtBf2</cell><cell cols="2">0.2620</cell><cell cols="4">UMHOObm25GS 0.1420</cell><cell></cell></row><row><cell></cell><cell>watrrfw</cell><cell></cell><cell cols="4">0.3760 twJ48rsU</cell><cell cols="2">0.2380</cell><cell cols="2">WatSdmrm3</cell><cell></cell><cell>0.1180</cell><cell></cell></row><row><cell></cell><cell cols="2">THUIR09An</cell><cell cols="2">0.3740</cell><cell cols="2">Sab9wtBase</cell><cell cols="2">0.2260</cell><cell cols="3">UMHOOqlGS</cell><cell>0.1180</cell><cell></cell></row><row><cell></cell><cell cols="2">MSRANORM</cell><cell cols="2">0.3700</cell><cell cols="2">THUIR09LuTA</cell><cell cols="2">0.2100</cell><cell cols="2">pkuLink</cell><cell></cell><cell>0.1160</cell><cell></cell></row><row><cell></cell><cell cols="2">THUIR09TxAn</cell><cell cols="2">0.3640</cell><cell cols="2">twCSrs9N</cell><cell cols="2">0.2080</cell><cell>uvaee</cell><cell></cell><cell></cell><cell>0.1100</cell><cell></cell></row><row><cell></cell><cell>MSRAAF</cell><cell></cell><cell cols="2">0.3540</cell><cell cols="2">twCSrsR</cell><cell cols="2">0.1800</cell><cell cols="2">UMHOOqlIF</cell><cell></cell><cell>0.1080</cell><cell></cell></row><row><cell></cell><cell>MS1</cell><cell></cell><cell cols="2">0.3540</cell><cell cols="2">uogTrdphP</cell><cell cols="2">0.1680</cell><cell cols="2">uvamrf</cell><cell></cell><cell>0.0940</cell><cell></cell></row><row><cell></cell><cell cols="2">muadanchor</cell><cell cols="2">0.3519</cell><cell cols="4">yhooumd09BFM 0.1640</cell><cell cols="2">WatSql</cell><cell></cell><cell>0.0840</cell><cell></cell></row><row><cell></cell><cell>watwp</cell><cell></cell><cell cols="2">0.3516</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,56.69,705.86,498.61,20.68"><head>Table 4</head><label>4</label><figDesc>shows precision at cutoff 10 for all Category A relevance feedback runs. While the design of the task makes pairwise comparisons difficult, it appears our feedback method is competitive with that of Sabir, and superior Relevance Feedback results. P@10 results for all Category A phase 2 submissions are reproduced here. Our runs have the prefix WAT2.</figDesc><table coords="8,56.69,58.27,441.02,357.60"><row><cell>Run</cell><cell></cell><cell>P@10</cell><cell>Run</cell><cell></cell><cell></cell><cell>P@10</cell><cell>Run</cell><cell>P@10</cell></row><row><cell cols="2">WAT2.WatS.2</cell><cell cols="3">0.5900 CMIC.CMIC.2</cell><cell></cell><cell>0.3900</cell><cell>WatS.UCSC.1</cell><cell>0.3000</cell></row><row><cell cols="2">Sab9RF.udel.1</cell><cell>0.5340</cell><cell cols="2">CMIC.ilps.1</cell><cell></cell><cell>0.3880</cell><cell>WatS.twen.2</cell><cell>0.2960</cell></row><row><cell cols="2">Sab9RF.CMIC.2</cell><cell>0.5280</cell><cell cols="2">CMIC.ugTr.2</cell><cell></cell><cell>0.3860</cell><cell>WatS.SIEL.1</cell><cell>0.2680</cell></row><row><cell cols="6">WAT2.UCSC.1 0.5120 WAT2.UPD.1</cell><cell>0.3800 WAT2.YUIR.1 0.2540</cell></row><row><cell cols="2">Sab9RF.WatS.2</cell><cell>0.5100</cell><cell cols="2">CMIC.UMas.2</cell><cell></cell><cell>0.3580</cell><cell>WatS.twen.1</cell><cell>0.2520</cell></row><row><cell cols="2">WAT2.udel.1</cell><cell cols="3">0.5040 CMIC.MSRC.1</cell><cell></cell><cell>0.3580</cell><cell>WatS.fub.1</cell><cell>0.2420</cell></row><row><cell cols="2">Sab9RF.Sab.1</cell><cell>0.4880</cell><cell cols="2">WatS.WatS.2</cell><cell></cell><cell>0.3520</cell><cell>IlpsRF.ilps.2</cell><cell>0.2380</cell></row><row><cell cols="2">Sab9RF.UCSC.2</cell><cell>0.4800</cell><cell cols="3">Sab9RF.YUIR.1</cell><cell>0.3480</cell><cell>IlpsRF.ilps.1</cell><cell>0.2340</cell></row><row><cell cols="5">WAT2.CMIC.2 0.4560 WatS.WatS.1</cell><cell></cell><cell>0.3400</cell><cell>IlpsRF.twen.1</cell><cell>0.2220</cell></row><row><cell cols="2">CMIC.CMIC.1</cell><cell>0.4420</cell><cell cols="4">WAT2.MSRC.2 0.3300 IlpsRF.twen.2</cell><cell>0.2160</cell></row><row><cell cols="2">MSRC.CMU.1</cell><cell>0.4360</cell><cell cols="2">IlpsRF.WatS.1</cell><cell></cell><cell>0.3280</cell><cell>IlpsRF.fub.1</cell><cell>0.1800</cell></row><row><cell cols="2">Sab9RF.hit2.2</cell><cell>0.4180</cell><cell cols="2">CMIC.udel.2</cell><cell></cell><cell>0.3240</cell><cell>IlpsRF.QUT.1</cell><cell>0.1360</cell></row><row><cell cols="3">Sab9RF.MSRC.2 0.4080</cell><cell cols="2">WAT2.hit2.2</cell><cell></cell><cell>0.3100 IlpsRF.Sab.1</cell><cell>0.1020</cell></row><row><cell cols="2">CMIC.udel.1</cell><cell>0.3960</cell><cell cols="2">WatS.Sab.1</cell><cell></cell><cell>0.3020</cell></row><row><cell cols="4">Tab. 4: Predicted (doc.)</cell><cell cols="3">Actual (doc.)</cell><cell>Actual (msg.)</cell></row><row><cell cols="4">Topic Recall Prec. F1</cell><cell cols="3">Recall Prec. F1</cell><cell>Recall Prec. F1</cell></row><row><cell>201</cell><cell>0.9</cell><cell>0.7</cell><cell cols="2">0.787 0.843</cell><cell cols="2">0.911 0.876 0.778</cell><cell>0.912 0.840</cell></row><row><cell>202</cell><cell>0.9</cell><cell>0.7</cell><cell cols="2">0.787 0.844</cell><cell cols="2">0.903 0.872 0.673</cell><cell>0.884 0.764</cell></row><row><cell>203</cell><cell>0.5</cell><cell>0.3</cell><cell cols="2">0.375 0.860</cell><cell cols="2">0.610 0.714 0.865</cell><cell>0.692 0.769</cell></row><row><cell>207</cell><cell>0.95</cell><cell>0.9</cell><cell cols="2">0.924 0.896</cell><cell cols="2">0.967 0.930 0.761</cell><cell>0.907 0.828</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Tab. 5: Legal Interactive</cell></row><row><cell>to the rest.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,71.94,728.30,122.79,6.56"><p>http://www.wumpus-search.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,77.17,711.87,478.14,8.80;8,77.17,723.82,197.66,8.80" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Buettcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Clarles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
		<title level="m" coord="8,372.67,711.87,182.63,8.80;8,77.17,723.82,113.62,8.80">Information Retrieval: Implementing and Evaluating Search Engines</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,59.86,278.90,8.73" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,172.76,59.86,153.03,8.73">Content-based Web spam detection</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,79.72,478.14,8.80;9,77.17,91.67,301.65,8.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,178.57,79.79,288.31,8.73">University of waterloo participation in the trec 2007 spam track</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,492.54,79.72,62.77,8.80;9,77.17,91.67,154.38,8.80">Sixteenth Text REtrieval Conference (TREC-2007)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,111.67,478.14,8.73;9,77.17,123.55,478.14,8.80;9,77.17,135.51,464.70,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,357.34,111.67,197.97,8.73;9,77.17,123.62,158.80,8.73">Reciprocal rank fusion outperforms Condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,255.67,123.55,299.64,8.80;9,77.17,135.51,282.05,8.80">SIGIR &apos;09: Proceedings of the 32nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,155.50,478.14,8.73;9,77.17,167.39,478.13,8.80;9,77.17,179.34,203.50,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,390.04,155.50,165.27,8.73;9,77.17,167.46,31.54,8.73">Efficient construction of large test collections</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,127.09,167.39,428.21,8.80;9,77.17,179.34,105.20,8.80">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,199.34,478.13,8.73;9,77.17,211.22,92.78,8.80;9,225.07,211.22,147.78,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,407.54,199.34,147.76,8.73;9,77.17,211.29,43.69,8.73">Efficient construction of large tst collections</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,141.70,211.22,28.26,8.80;9,225.07,211.22,18.51,8.80">SIGIR 1998</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,231.22,388.50,8.73" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://www.paulgraham.com/better.html" />
		<title level="m" coord="9,142.99,231.22,103.96,8.73">Better bayesian filtering</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,251.14,478.13,8.73;9,77.17,263.03,430.43,8.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,404.27,251.14,151.04,8.73;9,77.17,263.10,22.86,8.73">Overview of the TREC-2009 Legal Track</title>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Hedin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,121.47,263.03,244.05,8.80">Proceedings of the Eighteenth Text REtrieval Conference</title>
		<meeting>the Eighteenth Text REtrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,282.95,478.13,8.80;9,77.17,294.91,293.44,8.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,279.43,283.02,110.46,8.73">On-line spam filter fusion</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,411.74,282.95,143.56,8.80;9,77.17,294.91,227.30,8.80">29th ACM SIGIR Conference on Research and Development on Information Retrieval</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,314.83,478.13,8.80;9,77.17,326.86,22.69,8.73" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,156.69,314.90,168.08,8.73">On term selection for query expansion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,335.59,314.83,113.21,8.80">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990-12">December 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,346.72,409.07,8.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,138.86,346.79,188.06,8.73">A statistical approach to the spam problem</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,336.11,346.72,59.83,8.80">Linux Journal</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,366.64,478.14,8.80;9,77.17,378.67,22.69,8.73" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="9,239.75,366.71,232.21,8.73">Filtering spam in the presence of noisy user feedback</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Tufts University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,77.17,398.52,478.14,8.80;9,77.17,410.48,218.72,8.80" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,147.58,398.59,347.64,8.73">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,503.22,398.52,52.09,8.80;9,77.17,410.48,120.98,8.80">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
