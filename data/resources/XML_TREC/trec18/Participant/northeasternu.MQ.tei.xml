<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.02,115.49,269.95,14.93;1,260.74,133.14,90.51,9.46">Northeastern University in TREC 2009 Million Query Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.53,162.77,97.65,10.37;1,217.18,160.77,1.88,6.99"><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Studies Department</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.41,162.77,47.84,10.37;1,275.26,160.77,1.41,6.99"><forename type="first">Keshi</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.97,162.77,57.43,10.37;1,343.40,160.77,1.41,6.99"><forename type="first">Virgil</forename><surname>Pavlu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.11,162.77,61.91,10.37;1,416.03,160.77,1.41,6.99"><forename type="first">Stefan</forename><surname>Savev</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.74,162.77,61.01,10.37;1,487.74,160.77,1.41,6.99"><forename type="first">Javed</forename><surname>Aslam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.02,115.49,269.95,14.93;1,260.74,133.14,90.51,9.46">Northeastern University in TREC 2009 Million Query Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07D59A6C9FDE3137B7DF53F8FDBF7E3C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ranking is a central problem in information retrieval. Modern search engines, especially those designed for the World Wide Web, commonly analyze and combine hundreds of features extracted from the submitted query and underlying documents in order to assess the relative relevance of a document to a given query and thus rank the underlying collection. The sheer size of this problem has led to the development of learningto-rank (LTR) algorithms that can automate the construction of such ranking functions: Given a training set of (feature vector, relevance) pairs, a machine learning procedure learns how to combine the query and document features in such a way so as to effectively assess the relevance of any document to any query and thus rank a collection in response to a user input.</p><p>Much thought and research has been placed on the development of sophisticated learning-to-rank algorithms. However, relatively little research has been conducted on the construction of appropriate learningto-rank data sets nor on the effect of these data sets on the ability of a learning-to-rank algorithm to "learn" effectively.</p><p>Given that the IR technology is ubiquitous in a vast variety of contexts and environments it is not unreasonable to assume that searchable material (corpora) and user information needs will radically vary from one retrieval environment to another. Theoretically, ranking functions should be trained over collections with similar characteristics as the collections they will be deployed in. However, the ability to construct different ranking functions for different retrieval environments is limited by the cost of constructing such customized training collections. Thus, the question that naturally arises is whether training on a collection of certain characteristics can still lead to an effective ranking function over collections of different characteristics. To answer this question we trained our ranking functions (by employing SVM) over two different collections, (a) the Million Query 2008 (MQ08) collection (GOV2 corpus and queries with at least one click on documents in the .gov domain), and (b) a Bing generated collection (described in Section 2.1) and employed the constructed ranking function over the Million Query 2009 (MQ09) collection (ClueWeb09 corpus and general web queries).</p><p>Furthermore, even within a certain retrieval environment (represented by a given collection) different queries may have radically different characteristics and thus different features may better capture the notion of relevance. For instance, in the case of precision-oriented queries, such as homepage/namepage finding, the url of a document or its popularity may be more indicative of the document relevance than the document text itself, while for informational queries the document url maybe less indicative than its text. Most of the existing learning-to-rank approaches train a single ranking function to handle all queries. Hence, the question that arises is whether training a different ranking function for each one of these different query categories and using the appropriate ranking function when a user submits a new query may lead to better retrieval performance than employing a single ranker for all queries.</p><p>Geng et al. <ref type="bibr" coords="2,140.18,102.96,12.72,9.46" target="#b3">[4]</ref> proposed query-dependent ranking. According to their approach when a user submits a query the K Nearest Neighbor (KNN) method is employed to identify the closest queries to the submitted one and a ranker is trained over these K queries. This query-dependent ranker is then used to rank documents with respect to the submitted query. The KNN query-dependent ranker model was then compared against the single ranker model (baseline) and a query classification ranker model. According to the latter model different rankers were constructed for homepage finding, namepage finding and topic distillation. The KNN approach slightly outperformed the other two (with an nDCG difference of approximately 0.01-0.02).</p><p>Web queries however may be classified based on a variety of different criteria, such as query hardness, user intent, ambiguity, popularity (frequency in a query log), spatial or temporal characteristics, length, click-through rates, verbosity (i.e. a natural language query versus a set of keywords). In this work we explore whether constructing different ranking functions for different query intent and query hardness leads to better retrieval results than constructing a single ranker for all queries. However, given that we trained over the MQ08 collection, our ability to answer this question highly depends on the robustness of the learningto-rank algorithm when trained and tested over different collections (that is, it highly depends on the answer to the first question posed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We indexed the GOV2 collection and the ClueWeb09 (category B) collection using the Indri Search Engine from the Lemur Toolkit <ref type="bibr" coords="2,177.90,366.15,11.59,9.46" target="#b0">[1]</ref>. The constructed indexes contained five fields: document, title, heading, anchor text, and url; this allowed extracting features from all these different fields.</p><p>Based on the Indri indexes, we extracted a total of 57 LETOR4.0 -like features for each one of the querydocument pairs. We did not utilize the available LETOR4.0 MQ08 training set <ref type="bibr" coords="2,432.71,406.80,12.72,9.46" target="#b1">[2]</ref> so that the extracted features from ClueWeb09 would be comparable to the ones from MQ08. A summary of the features can be seen in Table <ref type="table" coords="2,132.02,433.89,4.09,9.46" target="#tab_0">1</ref>. Text features were extracted for all 5 fields.</p><p>The MQ08 data set (training data) consists of 784 queries from the Million Query 2008 collection. 403 queries had 8 documents labeled with relevance judgments, 204 queries had 16 documents labeled, 102 queries had 32 documents labeled, 50 queries had 64 documents labeled and 25 queries had 128 documents labeled. The corpus of the MQ08 collection is the GOV2 corpus and the queries in the collection had at least one click on documents in the .gov collection. Given the computational complexity of feature extraction and training features were extracted only from the union of (a) top 1,000 documents ranked by Indri language model <ref type="bibr" coords="2,101.57,528.74,12.72,9.46" target="#b0">[1]</ref> over the document text, (b) the top 500 documents ranked by Indri over the anchor text, and (c) the top 500 documents ranked by Indri over the url of the documents. A state-of-the-art SVM learning-to-rank algorithm <ref type="bibr" coords="2,117.15,555.84,12.72,9.46" target="#b4">[5]</ref> was employed to construct the ranking functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training and testing over collections with different characteristics</head><p>Our first goal was to explore whether training over a collection with characteristics different than the collection the ranking function will be deployed in can still lead to effective retrieval.</p><p>We trained the SVM over the MQ08 data set and then used the resulting ranking function (NeuSvmBase) to re-rank the 2,000 documents (described above) per query in the new MQ09 collection.</p><p>There are two striking differences between the MQ08 and the MQ09 collections. (1) the MQ08 collection is on a .gov corpus with .gov related queries, while the MQ09 collection is a general Web collection,  <ref type="formula" coords="3,94.72,361.23,4.24,9.46">2</ref>) the MQ08 is a spam-free collection while the MQ09 is not.</p><p>In order to be able to separate the conflated effects of these two different characteristics of the two collections, we constructed a second ranker (NeuSvmStefan). We again used the SVM ranking algorithm but instead of training over the MQ08 collection we first obtained queries from the query log of a commercial search engine (different than the MQ09 queries), then we submitted these queries to Bing and finally trained over the intersection of the top 100 documents returned by Bing and the documents included in the ClueWeb09 category B crawl. Since there were no relevance judgments for the returned by Bing documents we used the reverse of the document ranks. This Bing-generated training collection has the same characteristics as the MQ09 collection except that it is (effectively) spam-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query-dependent rankings</head><p>The second question we attempted to answer was whether constructing separate ranking function for different query categories can lead to more effective retrieval than constructing a single ranker. The query characteristics we used to categorize queries were query intent (i.e. precision-oriented queries vs. recalloriented queries) and query hardness (i.e. hard vs. easy queries).</p><p>The training data set was the MQ08 data set. Regarding the query intent we manually classified the 784 queries in the MQ08 collection. The Average Average Precision (AAP) based on the Average Precision scores of the participating runs in the MQ08 track was utilized to classify queries into hard and easy.</p><p>Then, we trained three sets of ranking functions. The first set consisted of two rankers, one over the precision-oriented queries and one over the recall-oriented ones (NeuSvmPR). The second set consisted again of two rankers, one over the hard and one over the easy queries (NeuSvmHE), while the last set consisted of four rankers, a ranker for each one of the four combinations, i.e. precision-oriented and hard, recall-oriented and hard, etc. (NeuSvmPRHE). Fold1 Fold2 Fold3 Fold4 Fold5 MAP 0.283 0.294 0.325 0.300 0.278 Table <ref type="table" coords="4,151.22,110.79,4.24,9.46">2</ref>: 5-fold cross validation on all judged documents of MQ09 using regression For MQ09 queries, in order to determine what ranker to use, we had to estimate the intent (precision vs. recall) and the difficulty (hard vs. easy):</p><p>• Precision vs. Recall: We constructed an SVM classifier, trained it over MQ08 queries using as features the average feature values of the relevant document for each query with the manual labels. We applied the classifier over the MQ09 queries using as features the average features of the top 100 documents (pseudo-relevant).</p><p>• Hard vs. Easy: We used the Jensen-Shannon distance of the document rankings generated by a number of ranking functions (e.g. BM25, Language Models etc.) on the MQ09 collection <ref type="bibr" coords="4,456.89,259.18,11.59,9.46" target="#b2">[3]</ref>.</p><p>In both classifications a hard decision of whether a new query is precision-oriented or recall-oriented or whether it is hard or easy was made. Then the appropriate ranker was used to re-rank the top 2,000 documents originally ranked by the Indri language model as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>The baseline run of the single ranker trained over the Million Query 2008 collection (NeuSvmBase) resulted in an estimated Mean Average Precision (MAP) of 0.089, a particularly low score. To test the correctness of our SVM ranking algorithm and the correctness of the feature extractor we extracted features from the Million Query 2008 collection, and performed a five-fold cross validation. The retrieval performance achieved was at least as good as the LETOR 4.0 baselines. Thus, these results indicate that training over a collection of given characteristics cannot always lead to an effective ranking function when the function is deployed to rank documents in a collection of radically different characteristics.</p><p>Our second run of the single ranker trained over the Bing-generated collection (NeuSvmStefan) performed slightly worse than the baseline achieving an MAP score of 0.084. As mentioned earlier the Binggenerated training collection has similar characteristics to the Million Query 2009 collection except that it is spam-free. Therefore, a reasonable assumption is that training over a spam-free collection and using the ranker over a collection that includes spam leads to low retrieval effectiveness. We manually inspected the top documents of our baseline ranker and observed that although the original by the language model ranking included many relevant documents at the top positions the learning-to-rank algorithm boosted spam documents towards the top position of the re-ranked list. An interesting question that arises here is how much the effectiveness of our rankings could improve by simply removing all the returned spam documents. Further, a future direction of research would be to compare the retrieval effectiveness of rankers trained over collections that includes spam and rankers trained over spam-free collections with anti-spamming applied over the results of both rankers.</p><p>After obtaining the query categories and judgments form TREC MQ09, we trained and tested our ranking function over the MQ09 data by performed a 5-fold cross validation. When testing our ranking function we only considered the judged documents in a LETOR-like manner and thus the performance of our ranker is not comparable to the ones reported by TREC for our submitted rankers. However, the results for each fold can be viewed in Given the failure of the baseline learning-to-rank algorithm to learn an effective for the Million Query 2009 collection ranking function, we could not answer our second question of whether query-dependent rankings over predefined query categories can lead to significant improvements when compared with a single ranker for all queries. The estimated MAP scores were all slightly lower than the baseline but any conclusions would be misleading.</p><p>However, we conducted a similar experiment over the MQ09 collection after the qrels and the query categories were released by the Track. We built training and testing data sets based on the different user intent (i.e. one precision-oriented query data set and one recall-oriented query data set). There were in a total of 176 precision-oriented and 230 recall-oriented queries labeled by NIST assessors. We trained two different ranking functions over the two training sets and tested the ranking functions both against the precision-and against the recall-oriented testing sets. The results (shown in Table <ref type="table" coords="5,428.12,409.32,4.54,9.46">3</ref>) illustrate that a ranking function trained over precision-oriented queries outperforms a ranking function trained over recall-oriented queries when the testing set consists of precision-oriented queries only, while the opposite is true in the case of a recall-oriented query test set, as expected. A further observation is that recall-oriented queries appear to be more useful for training than precision-oriented ones, since the performance difference between the two rankers is large when the test set consists of recall-oriented queries but very small when it consists of precision-oriented ones. This however needs further investigation since the set of recall-oriented queries was much larger that the set of precision-oriented queries which could also have affected the effectiveness of the trained ranking functions.</p><p>Finally, the results of our classification can be viewed in Tables <ref type="table" coords="5,379.64,531.26,5.45,9.46" target="#tab_1">4</ref> and<ref type="table" coords="5,408.43,531.26,4.09,9.46" target="#tab_2">5</ref>. The query intent classifier seemed to biased towards recall-oriented queries by classifying 411 out of 600 queries as recall-oriented. Out of those 411 predicted recall queries 235 where assessed as recall queries by the judges. The Jensen-Shannon methodology to classify queries based on their hardness does not seem to have done well either. Given that Jensen-Shannon has been shown to predict query hardness well when it is applied over rankings by TREC runs this may indicate that applying the same methodology over rankings by basic ranking functions (e.g. BM25, LM) does not lead to equally good predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,73.96,468.00,296.73"><head>Table 1 :</head><label>1</label><figDesc>Features extracted from the MQ08 (GOV2), the Bing generated and the MQ09 (ClueWeb09 category B) collections.</figDesc><table coords="3,181.54,73.96,248.93,227.47"><row><cell></cell><cell>Text Features</cell></row><row><cell>T1.</cell><cell>length</cell></row><row><cell>T2.</cell><cell>TF</cell></row><row><cell>T3.</cell><cell>IDF</cell></row><row><cell>T4.</cell><cell>TF*IDF</cell></row><row><cell>T5.</cell><cell>normalized TF</cell></row><row><cell>T6.</cell><cell>Robertson's TF</cell></row><row><cell>T7.</cell><cell>Robertson's IDF</cell></row><row><cell>T8.</cell><cell>BM25</cell></row><row><cell>T9.</cell><cell>Language Model (Laplace Smoothing)</cell></row><row><cell cols="2">T10. Language Model (Dirichlet Smoothing)</cell></row><row><cell cols="2">T11. Language Model (JM Smoothing)</cell></row><row><cell></cell><cell>Web Features</cell></row><row><cell cols="2">W1. number of incoming links</cell></row><row><cell cols="2">W2. number of incoming links from different domains</cell></row></table><note coords="3,72.00,361.23,22.72,9.46"><p>and (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,168.37,683.01,262.67,9.46"><head>Table 4 :</head><label>4</label><figDesc>Table 2 and the mean average precision achieved was 0.296. Hard vs. Easy Queries</figDesc><table coords="5,72.00,74.38,468.00,149.55"><row><cell></cell><cell></cell><cell>Train</cell><cell>Test</cell><cell>MAP</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell cols="2">Precision 0.37</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Precision Precision 0.38</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>0.49</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>Recall</cell><cell>0.52</cell><cell></cell></row><row><cell cols="7">Table 3: Training &amp; Testing on different query categories over all judged documents of MQ09 using regres-</cell></row><row><cell>sion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Easy Medium Hard</cell><cell></cell><cell></cell><cell cols="2">Precision Recall</cell></row><row><cell>predicted Hard 115</cell><cell>91</cell><cell>63</cell><cell></cell><cell>predicted Precision</cell><cell>81</cell><cell>108</cell></row><row><cell>predicted Easy 107</cell><cell>103</cell><cell>121</cell><cell></cell><cell>predicted Recall</cell><cell>176</cell><cell>235</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,373.47,236.94,158.85,9.46"><head>Table 5 :</head><label>5</label><figDesc>Precision vs Recall Queries</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,90.17,672.51,210.28,9.46" xml:id="b0">
	<monogr>
		<ptr target="http://www.lemurproject.org" />
		<title level="m" coord="5,90.17,672.51,75.19,9.46">The lemur toolkit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.17,75.86,318.44,9.46" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Letor</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/en-us/um/beijing/projects/letor/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.17,98.37,449.82,9.46;6,90.17,111.73,312.89,9.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,197.46,98.37,342.53,9.46;6,90.17,111.92,73.84,9.46">Query hardness estimation using Jensen-Shannon divergence among multiple scoring functions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,172.28,111.73,152.34,9.40">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4425</biblScope>
			<date type="published" when="2007">198. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.17,134.44,449.82,9.46;6,90.17,147.79,449.83,9.66;6,90.17,161.34,291.24,9.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,356.07,134.44,183.93,9.46;6,90.17,147.99,36.90,9.46">Query dependent ranking using k-nearest neighbor</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.99,147.79,388.01,9.40;6,90.17,161.34,158.92,9.40">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.17,183.85,449.83,9.66;6,90.17,197.40,364.67,9.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,150.27,184.05,156.40,9.46">Training linear SVMs in linear time</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,328.36,183.85,211.64,9.40;6,90.17,197.40,257.66,9.40">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">226</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
