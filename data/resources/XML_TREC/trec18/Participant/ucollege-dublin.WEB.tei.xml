<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.93,116.95,295.50,12.62">UCD SIFT in the TREC 2009 Web Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.77,154.88,50.41,8.74"><forename type="first">David</forename><surname>Lillis</surname></persName>
							<email>david.lillis@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,195.71,154.88,61.63,8.74"><forename type="first">Fergus</forename><surname>Toolan</surname></persName>
							<email>fergus.toolan@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.89,154.88,37.34,8.74"><forename type="first">Ling</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Software School</orgName>
								<address>
									<country>Fudan University Shanghai China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.78,154.88,68.48,8.74"><forename type="first">Rem</forename><forename type="middle">W</forename><surname>Collier</surname></persName>
							<email>rem.collier@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,414.15,154.88,61.97,8.74"><forename type="first">John</forename><surname>Dunnion</surname></persName>
							<email>john.dunnion@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Informatics</orgName>
								<orgName type="institution">University College Dublin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.93,116.95,295.50,12.62">UCD SIFT in the TREC 2009 Web Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A7FB797F083A039D62F1C1771E30821</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The SIFT (SIFT Information Fusion Techniques) group in UCD is dedicated to researching Data Fusion in Information Retrieval. This area of research involves the merging of multiple sets of results into a single result set that is presented to the user. As a means of evaluating the effectiveness of this work, the group entered Category B of the TREC 2009 Web Track. This paper discusses the strategies and experiments employed by the UCD SIFT group in entering the TREC Web Track 2009. This involved the use of freely-available Information Retrieval tools to provide inputs to the data fusion process, with the aim of contrasting with more sophisticated systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first year of the SIFT (SIFT Information Fusion Techniques) Project's participation in the TREC Web Track. As the first entry, it was decided to enter Category B, as the current computing resources of the group would have made participation in Category A difficult. The principal aim of the SIFT group is to develop data fusion algorithms that combine the outputs of multiple Information Retrieval systems or algorithms in order to produce a single result-set that is of a superior quality.</p><p>The approach taken to the TREC Web Track was to focus on a number of these fusion algorithms that have been developed within the group, three of which were used for the three runs that were submitted. This means that, in contrast to a typical TREC Web Track entry, the focus was not on the evaluation of novel IR systems or algorithms, but on techniques to combine these.</p><p>The paper is organised as follows: Section 2 briefly discusses the area of Data Fusion. The specific algorithms used in the TREC experiments are discussed in greater detail in Sections 3, 4 and 5. The experiments themselves are described in 6. Finally some evaluation is contained in Section 7, in addition to outlining further experiments that will provide a more informative evaluation.</p><p>Data Fusion refers to a procedure whereby a variety of result sets produced by a number of distinct Information Retrieval systems are merged into a single result that is then presented to the user <ref type="bibr" coords="2,286.22,168.59,9.96,8.74" target="#b0">[1]</ref>. It specifically refers to a situation where these input systems retrieve documents from the same document collection, so that an identical set of documents is available for retrieval by each.</p><p>A typical data fusion algorithm will attempt to exploit one or more of the "effects" identified in <ref type="bibr" coords="2,229.93,216.57,9.96,8.74" target="#b1">[2]</ref>. The "Chorus Effect" argues that documents retrieved by multiple input systems are more likely to be relevant and so a fusion algorithm should boost documents on whose relevance the systems agree. The "Skimming Effect" is based on the observation that relevant documents are most frequently to be found in early positions in result sets, and so by "skimming" the top documents from each result set (or attaching greater weight to these), a fusion technique should boost its performance. Finally, the "Dark Horse Effect" occurs when an input system produces unusually accurate or unusually inaccurate results. This latter effect is appealed to far less frequently than the others, due to the inherently difficult nature of identifying when such an event has occurred.</p><p>Traditionally, approaches to data fusion have tended to fall broadly into two categories. Rank-based algorithms make use of the position a document occupies in a result set in calculating the final ranking. Such algorithms include approaches based on interleaving <ref type="bibr" coords="2,277.46,372.14,10.52,8.74" target="#b2">[3]</ref> and voting-based techniques <ref type="bibr" coords="2,413.45,372.14,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,425.63,372.14,7.01,8.74" target="#b4">5]</ref>. The other major category is algorithms that makes use of the score used by the input systems to rank the documents. This includes linear combination models <ref type="bibr" coords="2,460.67,396.05,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,472.84,396.05,7.75,8.74" target="#b5">6]</ref> and the widely-used CombSum and CombMNZ algorithms <ref type="bibr" coords="2,394.21,408.01,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,406.38,408.01,7.01,8.74" target="#b7">8]</ref>.</p><p>More recently, some researchers have focussed on making use of probabilities to rank fused result sets. Such approaches include Bayes-fuse <ref type="bibr" coords="2,408.42,432.08,9.96,8.74" target="#b0">[1]</ref>, ProbFuse <ref type="bibr" coords="2,470.08,432.08,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,134.77,444.03,11.62,8.74" target="#b9">10]</ref>, SegFuse <ref type="bibr" coords="2,191.18,444.03,15.50,8.74" target="#b10">[11]</ref> and SlideFuse <ref type="bibr" coords="2,272.95,444.03,14.61,8.74" target="#b11">[12]</ref>. Two of these algorithms, namely ProbFuse and SlideFuse, are the techniques used in the experiments described in this paper. They are discussed in more detail in Sections 3 and 4 respectively.</p><p>A more thorough discussion of existing Data Fusion techniques can be found in <ref type="bibr" coords="2,146.39,492.01,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ProbFuse</head><p>ProbFuse is a probabilistic data fusion algorithm that makes use of training data to estimate the probability that a document returned in a particular segment of a result set is relevant. It is discussed in detail in <ref type="bibr" coords="2,353.99,573.27,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,366.16,573.27,11.62,8.74" target="#b9">10]</ref>, where experiments on various TREC datasets demonstrate superior performance when compared with the popular CombMNZ data fusion algorithm <ref type="bibr" coords="2,337.73,597.18,9.96,8.74" target="#b6">[7]</ref>.</p><p>The ProbFuse algorithm consists of two distinct phases. Firstly, a training phase is required so as to construct a model of the probabilities that particular documents are relevant. Once this has been done, a Fusion Phase performs the actual data fusion, thereby generating the final result sets to be returned to the user. These two phases are discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Phase</head><p>Because ProbFuse depends on probability to rank documents in the fused result set, it is necessary to estimate the probability of relevance for particular documents. In this algorithm, this is done by examining the result sets generated in response to a number of training topics. This approach relies on the assumption that the performance of a particular input system on such training topics is representative of its quality when faced with future topics.</p><p>ProbFuse requires that training queries be run on each system that provides result sets for fusion. By doing this, a separate set of probabilities can be built up for each. Thus, those systems that tended to produce superior results in the training phase will be boosted above the others in the fusion phase.</p><p>The model of probability used by ProbFuse requires that each result set be divided into x segments of equal size, where possible. In some cases when the result set is not evenly divisible by x, some rounding is necessary. The division of a simple 12-document result set into segments is illustrated in Figure <ref type="figure" coords="3,446.90,297.84,3.87,8.74" target="#fig_0">1</ref>. Here, the leftmost result set is seen to be divided into two segments, with half of the documents appearing in each. Subsequent examples show increasing values for x, resulting in greater numbers of segments being created.</p><p>The object of the training phase of ProbFuse is to ascribe probabilities to each segment that will represent the likelihood that a document appearing in that segment will be relevant to any given topic. As an example, in the case where x = 2, the probability of a document being relevant when returned by the system in question will depend on whether it appears in the first or the second half of the returned result set. The division of result sets into segments, rather than depending on actual rankings, allows for variations in the lengths of the result sets being used.</p><p>Mathematically, the training phase of ProbFuse is represented as follows: in a training set of T topics, P (d k |m), the probability that a document d returned in segment k is relevant, given that it has been returned by retrieval model m, is given by:</p><formula xml:id="formula_0" coords="3,259.47,499.08,221.12,25.97">P (d k |m) = T t=1 R k,t K T (1)</formula><p>where R k,t is the number of documents in segment k that are judged to be relevant to topic t, and K is the total number of documents in segment k.</p><p>For each input system, it is necessary to calculate a probability of relevance for every segment k in the range 1..x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion Phase</head><p>Once a set of probabilities has been calculated for each system being used for fusion, the fusion phase may take place. In this phase, each document is examined and its position in each of the result sets to be fused is noted. Depending on the segment the document is returned in, each system may then contribute towards The ranking score S d for each document d is given by</p><formula xml:id="formula_1" coords="4,266.49,521.84,209.86,30.20">S d = M m=1 P (d k |m) k (<label>2</label></formula><formula xml:id="formula_2" coords="4,476.35,532.26,4.24,8.74">)</formula><p>where M is the number of retrieval models being used, P (d k |m) is the probability of relevance for a document d k that has been returned in segment k in retrieval model m, and k is the segment that d appears in (1 for the first segment, 2 for the second, etc.). For any technique that does not return document d in its result set at all, P (d k |m) = 0, so as to ensure that documents do not receive any boost to their ranking scores from techniques that do not return them as being judged relevant.</p><p>Once S d has been calculated for each document, the documents are then merged into the final result set, sorted in descending order of S d .</p><p>SlideFuse is an alternative probabilistic fusion algorithm that operates on a similar premise to ProbFuse. It also calculates ranking scores based on the probability that a document is relevant, where this probability is estimated by examining past performance in response to training topics. However, this probability distribution is calculated in a different way to that of ProbFuse. In previous work, SlideFuse has been shown to achieve superior performance to ProbFuse on the TREC2004 Web Track, when measured using both the MAP and bpref evaluation metrics <ref type="bibr" coords="5,190.20,227.89,14.61,8.74" target="#b11">[12]</ref>.</p><p>As with ProbFuse, SlideFuse consists of two distinct phases: a Training Phase and a Fusion Phase. These are discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Phase</head><p>Unlike the training phase used in ProbFuse, SlideFuse does not divide result sets into segments. Instead, it attempts to approximate the probability of relevance at each position in a result set. However, this is not a trivial task for a number of reasons. Principal amongst these is the problem of incomplete relevance judgments amongst data sets that could realistically be used for training purposes. This can result in a situation where the probability of relevance in a particular position may appear to be zero as a result of failing to return any judged relevant documents in that position. This however can result in a jagged probability distribution, as illustrated in Figure <ref type="figure" coords="5,326.17,397.91,3.87,8.74" target="#fig_1">2</ref>. For this reason, an extra stage is required in the fusion phase that smooths these probabilities into a more realistic probability distribution.</p><p>Formally, P (d p |s), the probability that a document d returned in position p of a result set is relevant, given that is has been returned by input system s is given by</p><formula xml:id="formula_3" coords="5,257.61,478.41,222.98,25.32">P (d p |s) = t∈Tp R dp,t T p<label>(3)</label></formula><p>where T p is the set of all training topics for which at least p documents were returned by the input system and R dp,t is the relevance of the document d p to topic t (1 if the document is relevant, 0 if not). This is calculated for each input system to be used in the fusion phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fusion Phase</head><p>As noted above, using the probability at each position tends to lead to jagged probability distributions, due principally to the presence of unjudged documents in each result set that are assumed not to be relevant. In order to achieve more useful probability values, we construct a window around each position, so as to make use of relevance information about near neighbours when assigning probabilities to individual ranks.</p><p>The start and end points (a and b respectively) of the sliding window surrounding each result set position p are given by</p><formula xml:id="formula_4" coords="6,243.93,153.27,236.66,63.10">a = p -w p -w &gt;= 0 0 p -w &lt; 0 (4) b = p + w p + w &lt; N N -1 p + w &gt;= N (<label>5</label></formula><formula xml:id="formula_5" coords="6,476.35,198.76,4.24,8.74">)</formula><p>where w is a parameter that indicates how many positions on either side of p should be included in the window and N is the total number of documents in the result set. In effect, the above definitions of a and b ensure that the window cannot begin before the first document in the result set and also cannot extend beyond the last document.</p><p>Having defined the limits of the window surrounding each position in the result set, it is necessary to assign a probability score to each position based on the contents of its window. This is done as follows: P (d p,w |s), the probability of relevance of document d in position p using a window size of w documents either side of p, given that it has been returned by input system s is given by</p><formula xml:id="formula_6" coords="6,250.61,351.10,229.99,25.41">P (d p,w |s) = b i=a P (d i |s) b -a + 1 (6)</formula><p>The use of the window results in the smoother probability distribution shown in Figure <ref type="figure" coords="6,175.65,393.03,3.87,8.74" target="#fig_1">2</ref>. It also diminishes the likelihood of a rank being assigned a probability score of zero, even where relevant documents have occurred in surrounding areas. The final stage of the fusion phase is to assign a ranking score to each document that will be used to order the fused result set at the end of the process. R d , the final ranking score given to document d is given by</p><formula xml:id="formula_7" coords="7,266.36,167.81,209.98,20.06">R d = s∈S P (d p,w |s) (<label>7</label></formula><formula xml:id="formula_8" coords="7,476.34,167.81,4.24,8.74">)</formula><p>where S is the set of all input systems used and p is the position in which document d was returned by input system s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Interpolated SlideFuse</head><p>Interpolated SlideFuse is a slight variation on the regular SlideFuse algorithm outlined above. Despite the smoothing of the achieved by the introduction of the sliding window surrounding each rank, the resultant probability distributions do not always reflect the general observation of the Skimming Effect that relevant documents are more likely to occur in early positions in a result set. Figure <ref type="figure" coords="7,453.68,302.71,4.98,8.74" target="#fig_1">2</ref> is an example of such a situation, where the probability for the windows surrounding ranks 25-35 tend to be higher than those in the earlier 10-20 positions.</p><p>For this reason, an additional interpolation step is added to the existing SlideFuse algorithm when the probability associated with each window is calculated. Rather than merely considering the probabilities associated with ranks in the immediate surrounding ranks, care is also taken so as to ensure a downwardsloping curve. The probability assigned to each window is the maximum probability of the window itself and all other windows occurring further down the result set. Thus, for a result set of 1000 documents, the probability associated with window number 100 is the maximum probability found between window 100 and window 1000, inclusive. An in-depth study of the effects of this change has not yet been carried out, and its use in the TREC 2009 Web Track is its first proper use in an experimental context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">TREC 2009 Experiments</head><p>On account of the resources available to the group, it was decided to enter Category B, which consists of an adhoc task and a diversity task. The dataset used is a subset of English-only pages from the ClueWeb09 dataset, containing approximately 50,000,000 web pages.</p><p>In designing the experiments for the Web Track, a number of significant issues arose and had to be addressed. In particular, these were:</p><p>-Inputs: All data fusion algorithms require at least two result sets in order to operate. In order to utilise the data fusion techniques outlined above, it was necessary to generate numerous result sets for each topic. -Training Data: One limitation of probabilistic data fusion algorithms is the need for training data. This meant that in addition to using several systems to generate results, it was also necessary to have a historical record of their effectiveness for training purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Inputs</head><p>The Terrier (TERabyte RetrIEveR) is an open-source search engine developed in the University of Glasgow and released under the Mozilla Public License <ref type="bibr" coords="8,462.33,150.61,14.61,8.74" target="#b12">[13]</ref>.</p><p>Terrier is specifically designed to be capable of handling large-scale document collections, in the order of terabytes. This, allied with the fact that it offers implementations of a variety of document ranking models, made it an attractive choice for providing the inputs to the fusion process.</p><p>To identify which of the available models were to be used in the Web Track experiments, each was run on the WT10G dataset using TREC-10 topics 501-550 and the MAP scores were compared. As a result of this, the four models with the best MAP scores on these topics were selected to generate the inputs for the fusion experiment. There were BB2, IFB2, In expB2 and InL2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training Phase</head><p>From the previous Sections, it can be seen that each of the data fusion algorithms used requires training data, so that the probability of relevance can be estimated. When training, it is essential that the systems running the training queries are the same as those used to ultimately generate the result sets for fusion.</p><p>For each of the chosen document weighting models, TREC-10 topics 501-550 (title only) were run on the WT10G dataset. This resulted in 50 training result sets being generated from each input system, with probabilities being calculated during a training phase for each algorithm, as outlined in the sections above.</p><p>For ProbFuse, it is necessary at this stage to specify x, the number of segments into which each result set is divided. Here, x was set to 25: a value that has produced satisfactory empirical results in past experiments <ref type="bibr" coords="8,413.13,425.05,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Fusion Phase</head><p>In the fusion phase, each of the four selected models was used to generate a result set for each of the topics. In generating these result sets, only the standard stopword removal and stemming (using Porter's algorithm <ref type="bibr" coords="8,385.47,496.25,15.50,8.74" target="#b13">[14]</ref>) were used. Query Expansion, Relevance Feedback and other such techniques were not used.</p><p>In this phase, it was necessary to set the size of the window to be used in SlideFuse and Interpolated SlideFuse. For this experiment, this was set to 5, following previous experiments in <ref type="bibr" coords="8,283.81,544.07,14.61,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation and Future Work</head><p>A summary of the evaluation results is contained in Tables <ref type="table" coords="8,411.47,597.34,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="8,442.38,597.34,3.87,8.74" target="#tab_1">2</ref>. These indicate the comparative evaluation scores of the SlideFuse (UCDSIFTslide), ProbFuse (UCDSIFTprob) and Interpolated SlideFuse (UCDSIFTinter) when compared with the other participants. In total, Category B had 34 submissions.</p><p>Table <ref type="table" coords="8,178.45,645.16,4.98,8.74" target="#tab_0">1</ref> shows the percentile ranks for each of the algorithms used when evaluated using the MTC sampling methods <ref type="bibr" coords="8,332.68,657.11,14.61,8.74" target="#b14">[15]</ref>. Here, it can be seen that the performance of each algorithm does not achieve a high rank for overall evaluation metrics such as eMAP and Rprec. However, the ranks are much higher in each case for metrics that measure precision in early positions. Each algorithm achieves its highest rank for either P5 or P10, with deteriorating performance further down result sets. Table <ref type="table" coords="9,177.12,300.41,4.98,8.74" target="#tab_1">2</ref> shows similar data for StatAP measures <ref type="bibr" coords="9,363.34,300.41,14.61,8.74" target="#b15">[16]</ref>. Here again, MAP and Rprec ranks are not high. Once more, however, the rankings for precision in early positions (P30 in this instance) are substantially higher. Despite the low overall rankings, the more competitive early precision results are encouraging given the tendency of users to only examine a small number of results at the top of a result set <ref type="bibr" coords="9,197.72,360.19,14.61,8.74" target="#b16">[17]</ref>. The principal area of interest for further evaluation is in the runs submitted by other groups for the TREC-2009 Web Track, in both Categories A and B. These represent sophisticated, state-of-the-art systems that employ a variety of approaches to the task and form a diverse range of inputs that can be used in a data fusion process. Also, since only standard document-ranking models were used in our initial experiments, the performance of the runs submitted by other groups is likely to be higher than for those generated by Terrier. We are especially interested in evaluating the effectiveness of our data fusion approach on these inputs and making comparisons with the runs we ourselves have submitted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,196.80,430.49,221.76,7.89;4,148.28,116.83,318.80,298.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Segmenting a result set for different values of x</figDesc><graphic coords="4,148.28,116.83,318.80,298.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,211.14,646.06,193.07,7.89;6,150.48,434.49,314.40,196.80"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Probability Distribution using SlideFuse</figDesc><graphic coords="6,150.48,434.49,314.40,196.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,188.76,198.45,237.84,62.04"><head>Table 1 .</head><label>1</label><figDesc>MTC measures: percentile rank (34 participants)</figDesc><table coords="9,195.13,219.35,225.09,41.14"><row><cell></cell><cell cols="3">eMAP Rprec P5 P10 P15 P20 P30 P100</cell></row><row><cell>UCDSIFTslide</cell><cell>8</cell><cell>8 76 79 67 58 47</cell><cell>8</cell></row><row><cell>UCDSIFTprob</cell><cell>11</cell><cell>11 41 50 47 32 20</cell><cell>14</cell></row><row><cell>UCDSIFTinter</cell><cell>5</cell><cell>5 82 76 64 55 41</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,184.60,393.32,246.16,62.04"><head>Table 2 .</head><label>2</label><figDesc>StatAP measures: percentile rank (34 participants)</figDesc><table coords="9,227.56,414.22,160.24,41.14"><row><cell></cell><cell cols="3">MAP Rprec P30 NDCG</cell></row><row><cell>UCDSIFTslide</cell><cell>5</cell><cell>5 79</cell><cell>32</cell></row><row><cell>UCDSIFTprob</cell><cell>8</cell><cell>8 52</cell><cell>26</cell></row><row><cell>UCDSIFTinter</cell><cell>11</cell><cell>11 82</cell><cell>29</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,646.84,337.64,7.86;9,151.52,657.79,329.07,7.86;10,151.52,120.67,329.07,7.86;10,151.52,131.63,280.39,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,269.63,646.84,210.97,7.86;9,151.52,657.79,202.41,7.86">Bayes optimal metasearch: a probabilistic model for combining the results of multiple retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,376.80,657.79,103.79,7.86;10,151.52,120.67,329.07,7.86;10,151.52,131.63,125.78,7.86">SIGIR {&apos;}00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="379" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,142.59,337.63,7.86;10,151.52,153.52,127.56,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,265.47,142.59,172.25,7.86">Fusion Via a Linear Combination of Scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,445.75,142.59,34.83,7.86;10,151.52,153.55,55.19,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,164.51,337.64,7.86;10,151.52,175.46,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,352.79,164.51,123.24,7.86">The Collection Fusion Problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.38,175.46,251.41,7.86">Proceedings of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,186.42,337.64,7.86;10,151.52,197.38,329.07,7.86;10,151.52,208.34,306.74,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,272.54,186.42,90.48,7.86">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,388.42,186.42,92.17,7.86;10,151.52,197.38,329.07,7.86;10,151.52,208.34,151.81,7.86">SIGIR {&apos;}01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,219.30,337.63,7.86;10,151.52,230.26,329.07,7.86;10,151.52,241.22,249.10,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,271.26,219.30,159.95,7.86">Condorcet fusion for improved retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.00,219.30,25.59,7.86;10,151.52,230.26,329.07,7.86;10,151.52,241.22,93.19,7.86">CIKM {&apos;}02: Proceedings of the eleventh international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,252.18,337.64,7.86;10,151.52,263.14,329.07,7.86;10,151.52,274.09,329.07,7.86;10,151.52,285.05,93.17,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,290.06,252.18,190.53,7.86;10,151.52,263.14,33.97,7.86">Searching distributed collections with inference networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,212.03,263.14,268.56,7.86;10,151.52,274.09,281.84,7.86">SIGIR {&apos;}95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,296.01,337.64,7.86;10,151.52,306.97,329.07,7.86;10,151.52,317.93,226.43,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,246.96,296.01,135.83,7.86">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<idno>Publication 500-215.</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,405.77,296.01,74.83,7.86;10,151.52,306.97,171.51,7.86">Proceedings of the 2nd Text REtrieval Conference (TREC-2)</title>
		<meeting>the 2nd Text REtrieval Conference (TREC-2)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology Special</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,328.86,337.64,7.89;10,151.52,339.85,32.25,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,197.46,328.89,172.82,7.86">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,380.91,328.89,55.85,7.86">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,350.81,337.64,7.86;10,151.52,361.77,329.07,7.86;10,151.52,372.73,329.07,7.86;10,151.52,383.68,60.92,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,336.10,350.81,144.50,7.86;10,151.52,361.77,61.10,7.86">ProbFuse: A Probabilistic Approach to Data Fusion</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,239.28,361.77,241.31,7.86;10,151.52,372.73,258.74,7.86">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,394.64,337.98,7.86;10,151.52,405.60,236.56,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,334.81,394.64,145.79,7.86;10,151.52,405.60,83.51,7.86">Probabilistic Data Fusion on a Large Document Collection</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,243.03,405.60,116.38,7.86">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,416.56,337.98,7.86;10,151.52,427.49,215.50,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,213.26,416.56,263.02,7.86">Segmentation of Search Engine Results for Effective Data-Fusion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,427.52,138.71,7.86">Advances in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4425</biblScope>
			<date type="published" when="2007-04">April 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,438.48,337.98,7.86;10,151.52,449.44,329.07,7.86;10,151.52,460.40,329.07,7.86;10,151.52,471.36,165.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,335.08,438.48,145.51,7.86;10,151.52,449.44,94.05,7.86">Extending Probabilistic Data Fusion Using Sliding Windows</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lillis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Toolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.42,449.44,206.18,7.86;10,151.52,460.40,134.22,7.86">Proceedings of the 30th European Conference on Information Retrieval (ECIR &apos;08</title>
		<title level="s" coord="10,366.28,460.40,114.31,7.86;10,151.52,471.36,30.50,7.86">Lecture Notes in Computer Science.</title>
		<meeting>the 30th European Conference on Information Retrieval (ECIR &apos;08<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4956</biblScope>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,482.31,337.98,7.86;10,151.52,493.27,329.07,7.86;10,151.52,504.23,211.87,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,453.14,482.31,27.46,7.86;10,151.52,493.27,119.64,7.86">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,294.53,493.27,186.06,7.86;10,151.52,504.23,144.03,7.86">Proceedings of the 27th European Conference on Information Retrieval (ECIR 05)</title>
		<meeting>the 27th European Conference on Information Retrieval (ECIR 05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,515.19,265.33,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,208.89,515.19,130.20,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,526.15,337.98,7.86;10,151.52,537.11,329.07,7.86;10,151.52,548.07,64.92,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,328.43,526.15,152.16,7.86;10,151.52,537.11,39.80,7.86">Minimal test collections for retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,198.26,537.11,282.33,7.86;10,151.52,548.07,36.25,7.86">Annual ACM Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,559.03,337.97,7.86;10,151.52,569.99,20.48,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,249.77,559.03,230.82,7.86;10,151.52,569.99,20.48,7.86">A practical sampling strategy for efficient retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,580.94,337.97,7.86;10,151.52,591.90,186.08,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,379.26,580.94,101.33,7.86;10,151.52,591.90,83.11,7.86">Analysis of a Very Large AltaVista Query Log</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Moricz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
