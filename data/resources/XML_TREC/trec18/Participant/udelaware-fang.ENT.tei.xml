<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.76,164.85,287.78,15.11">UDEL/SMU at TREC 2009 Entity Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.56,203.29,54.62,10.48"><forename type="first">Wei</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.07,203.29,90.82,10.48"><forename type="first">Swapna</forename><surname>Gottipati</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.76,203.29,53.01,10.48"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.64,203.29,47.33,10.48"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.76,164.85,287.78,15.11">UDEL/SMU at TREC 2009 Entity Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF39527668B4E6DC4CA30087A387CE85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report our methods and experiment results from the collaborative participation of the InfoLab group from University of Delaware and the school of Information Systems from Singapore Management University in the TREC 2009 Entity track. Our general goal is to study how we may apply language modeling approaches and natural language processing techniques to the task. Specifically, we proposed to find supporting information based on segment retrieval, to extract entities using Stanford NER tagger, and to rank entities based on a previously proposed probabilistic framework for expert finding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The InfoLab from University of Delaware and Singapore Management University collaborated in the TREC 2009 Entity track. The task is to find a list of target entities and their homepages, given a query entity, its relations with the target entities, and the specific entity type of the target entities <ref type="bibr" coords="1,379.40,518.58,9.97,8.74" target="#b0">[1]</ref>. Our general goal is to study how we may apply language modeling approaches and natural language processing techniques to the task. Specifically, we focus on the following four challenges: <ref type="bibr" coords="1,185.78,554.44,12.73,8.74" target="#b0">(1)</ref> how to find supporting information that is useful to extract a target entity according to a query; <ref type="bibr" coords="1,295.62,566.40,12.73,8.74" target="#b1">(2)</ref> how to extract entities based on the type specified in the query; (3) how to rank entities based on the supporting information; and (4) how to locate the homepages of the target entities.</p><p>First, to capture the intuition that the supporting information for relations between two entities often spans a short distance, we proposed to segment the documents into fixed-length paragraphs, and ranked the paragraphs based on the relevance to the query using KL divergence retrieval method. Second, to extract the target entities, we used an off-the-shelf NER tagger (the Stanford NER tagger) to find person and organization entities. For product entities, we Figure <ref type="figure" coords="2,242.43,134.88,3.88,8.74">1</ref>: design of our Entity track system used a set of heuristic rules to extract phrases that are likely to be target entities. We extracted the candidates of target entities from the relevant paragraphs that contain the topic entity and the relation narrative keywords, as retrieved in the previous step. Third, to rank the target entity candidates, we adapted the general language modeling framework for expert finding proposed in a previous study <ref type="bibr" coords="2,160.79,397.22,9.97,8.74" target="#b3">[4]</ref>. It has been shown that the proposed methods perform well on TREC Enterprise track. Thus, it is interesting to evaluate the proposed framework in the context of the entity track. Finally, we perform standard retrieval to locate the homepages of the target entities.</p><p>The paper is organized as follows. We propose a general strategy for entity retrieval in Section 2, and describe implementation details in Section 3. We then report the experiment results in Section 4 and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A General Strategy for Entity Track</head><p>Queries used in the entity track are structured. The queries specify the type of target entities, a query entity, URL of the query entity, the type of target entities and the relation between the query entity and target entities. To retrieve the target entities and their homepages based on the queries, we propose a general strategy for entity retrieval aiming to solve four challenges: (1) how to find supporting information that is useful to extract a target entity according to a query; (2) how to extract entities based on the type specified in the query; (3) how to rank entities based on the supporting information; and (4) how to locate the homepages of the target entities.</p><p>Figure <ref type="figure" coords="2,180.21,631.31,4.98,8.74">1</ref> shows the system architecture of the proposed general strategy. It can be divided into four steps:</p><p>1. Retrieving supporting information: Given a query entity and a rela-tion specified in a query, it is necessary to retrieve the supporting information that can be used to extract target entity. One possible solution is to retrieve information relevant to both the query entity and the specified relation. The underlying assumption is that if the supporting documents contain are related to both the query entity and the relation, they would be likely to contain target entity candidates, i.e., the entities related to the query entity in the specified way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Extracting candidate entities:</head><p>Given the supporting information, the next step is to extract candidate entities based on the type specified in the queries. Named Entity Recognition (NER) classifier can be applied to extract entity candidates based on the specified type.</p><p>3. Ranking candidate entities: After extracting the candidate entities from supporting information, we need to rank the candidates based on how likely they are target entities based on the query. It is necessary to use the supporting information as a bridge to connect the target entities and the information specified in the queries. Note that the problem of ranking entities is very similar to the problem of expert finding <ref type="bibr" coords="3,435.07,335.19,9.97,8.74" target="#b1">[2]</ref>, whose goal is to find persons with expertise specified in a query. Thus, we could adapt the methods proposed for expert finding problem to solve the entity ranking problem.</p><p>4. Finding homepages of the entities: For every candidate, we need to find its corresponding homepage. This step is essentially similar to the named page finding task in the previous Web tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Methods</head><p>We proposed a general strategy for entity track in the previous section. We now discuss our proposed methods for each of the four steps in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieving Supporting Information</head><p>The first step is to retrieve supporting information that contains the target entities based on the information specified in the queries. We propose to construct keyword queries based on the narrative fields of the original queries, and then use the constructed queries to retrieve supporting information. The underlying assumption is that the supporting information would be likely to contain the target entities if it contains both the query entity and the relations between the query entity and target entities, One decision we need to make in the process is which text unit we should retrieve as the supporting information. On the one hand, the text unit should be long enough to include both the query entity and target entity as well as their relations. On the other hand, the text unit should not be too long because longer text units may contain more noisy entities that are not the ones relevant to the queries. For example, query 6 in the entity track is "organizations that award Nobel prizes." The Wikipedia page of Nobel Prize discusses not only organizations that award the prize but also the organizations that mint the prize medal. The supporting information would include both the organizations awarding Nobel Prizes and the organizations minting the medal if the text unit is too long. To balance the tradeoff, we use fixed length segments, i.e., 50 words or 100 words, as the units for the supporting information in this work. To improve the efficiency, we first use the narrative field of the query to retrieve 1000 documents for each query. Each retrieved document is split into segments. Finally, the segments are ranked by the query and the top-ranked segments are selected as the supporting information.</p><p>The retrieval function used in this step is the Dirichlet prior retrieval method <ref type="bibr" coords="4,475.89,259.47,9.97,8.74" target="#b2">[3]</ref>:</p><formula xml:id="formula_0" coords="4,166.79,277.00,306.45,30.28">S(q, d) = ∑ w∈q∩d c(w, q) × ln(1 + c(w, d) µ × p(w|C) ) + |q| × ln( µ |d| + µ ) (<label>1</label></formula><formula xml:id="formula_1" coords="4,473.24,287.13,4.24,8.74">)</formula><p>where q is the query, d is the document or segment, S(q, d) is the score of d in q, w is the term that occurs both in q and d, c(w, q) is the term frequency of w in the query, C is the collection of the d, p(w|C) is the probability of the term given the collection language model, |d| is the length of d, |q| is the length of q, µ is the parameter and set to 1500 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Extraction</head><p>This step extracts the entity candidates from the supporting segments. Entity Track focuses on three types of target entities, namely, person, organization, and product. For person and organization, we directly apply the Stanford NER tagger to extract named entities of the specified type from the top-ranked segments retrieved for each topic. Stanford NER Classifier <ref type="bibr" coords="4,345.04,461.82,10.52,8.74" target="#b4">[5]</ref> is a named entity recognizer that can label the following types of entities: PERSON, ORGANIZATION, and LOCATION. It uses the Conditional Random Field (CRF) sequential model to identify named entity tokens in sentences. The CRF classifier is trained on data from CoNLL, MUC6, MUC7, and ACE. For product entities, however, Stanford NER tagger is not able to recognize them, because the annotated training corpora (CoNLL, MUC and ACE) do not contain product entities. In fact, we are not aware of any corpus that has product entities annotated. To solve this problem, initially we planned to treat proper nouns as candidate product entities and then exclude proper nouns that are tagged as persons, organizations and locations by the Stanford NER tagger. However, after some preliminary experiments, we found that true product entities could also be tagged as person, organization or location by the Stanford NER tagger, while treating all proper nouns as candidate product entities produced too much noise. In the end, we decided to use the named entities tagged by the Stanford NER tagger as either person, organization or location as candidate product entities.</p><p>After observing the extracted entities, we find that some different names of the query entity are recognized as the possible target entities and their existence will make a negative impact to the next step. Therefore, we define the following heuristics to reduce the noise: the supersets of the query entity terms are removed from the candidate list. Our assumption is that the names of the query entity may have different variations and the entity candidates that contain all the terms of the query entity are probably alternative names of the query entity. For example, entity candidate "new blackberry" is not a target entity, i.e., "carriers that blackberry makes phones for". Instead, it is clearly more related to the query entity "blackberry" than the target entity. This heuristic can delete the irrelevant candidates. However, it may also delete some relevant entities. For example, for query "Campuses of Indiana University," the entity candidate "Indiana University East" is deleted because it contains both terms in the query entity "Indiana University," but it is indeed one of the target entities. Clearly, it remains unclear what is a good strategy to filter out noisy candidates, and we leave this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Ranking</head><p>This step ranks the candidate entities based on the query and supporting information. The main challenge is how to utilize the supporting information to rank the candidate entities based on how likely they are relevant to the query. The problem of entity ranking is very similar to expert finding <ref type="bibr" coords="5,415.29,365.53,9.97,8.74" target="#b1">[2]</ref>, since both problems need to rank entities or experts based on the supporting information. Previous work <ref type="bibr" coords="5,199.56,389.44,10.52,8.74" target="#b3">[4]</ref> proposed two generative models for expert finding, i.e., candidate generation model and topic generation model. In this work, we focus on apply these two models to rank entities based on the supporting information. We now briefly review these two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Candidate generation model</head><p>The candidate generation model assumes that the candidate entity is generated by the model of its relevant topics. Formally, it computes the ranking scores of candidate entities as follows <ref type="bibr" coords="5,258.75,493.50,9.97,8.74" target="#b3">[4]</ref>:</p><formula xml:id="formula_2" coords="5,215.82,507.27,261.66,30.20">f (e, q) = ∑ s∈S p(e|s, R = 1) × p(q|s, R = 1)<label>(2)</label></formula><p>where e is a candidate entity, q is a query, R is a binary variable denoting the relevance (0 denotes irrelevant and 1 denotes relevant), s is a supporting segment and S is the collection of all the segments. The probability p(e|s, R = 1) and p(q|s, R = 1) can be computed with the Dirichlet prior retrieval function as shown in Equation <ref type="bibr" coords="5,258.41,596.53,11.63,8.74" target="#b0">(1)</ref>. The candidate generation model utilize the co-occurrences of entity e and query q in the supporting segments to compute the relevance score of entity e. Each supporting segment contributes to the score of e by the probabilities that it is relevant to q and that it mentions e. The more relevant the segment is to both t and e, the more it can contribute to the score of e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Topic generation model</head><p>The topic generation model assumes that the topic is generated by the probabilistic model of the relevant entities. The candidate entities can be ranked based on the function shown as follows <ref type="bibr" coords="6,306.70,170.26,9.97,8.74" target="#b3">[4]</ref>:</p><formula xml:id="formula_3" coords="6,164.44,185.00,313.04,30.20">f (e, q) = P (e|S) × ∑ s∈S (p(q|s, R = 1) × p(e|s, R = 1) ∑ s ′ ∈S p(e|s ′ , R = 1) )<label>(3)</label></formula><p>where P (e|S) is the prior probability of candidate entity e in the segment collection S and β is a parameter whose value is set to be 6 according to the values reported in the previous study <ref type="bibr" coords="6,300.07,248.11,9.97,8.74" target="#b3">[4]</ref>. The entity prior could be either uniform or computed based on the entity occurrences, such as count(e,S) count(e,S)+β , where count(e, S) is the count of candidate entity e in the segment collection S.</p><p>There are two main differences between the candidate generation and topic generation models in Equation ( <ref type="formula" coords="6,271.80,299.21,4.24,8.74" target="#formula_2">2</ref>) and ( <ref type="formula" coords="6,305.27,299.21,3.88,8.74" target="#formula_3">3</ref>). The first one is that topic generation model can integrate the entity prior, i.e., P (e|S) into the formula. The other one is that topic generation model can normalize the weight of each entity candidate by the sum of its probability in all segments, which is similar with the idea of idf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Homepage Finding</head><p>The last step of Entity track is to find the homepages of the result entities. In this work, we simply use every entity as a keyword query and retrieve the documents using Dirichlet prior methods as shown in Equation ( <ref type="formula" coords="6,413.85,416.74,3.88,8.74" target="#formula_0">1</ref>). The Entity track requires that one document can only be returned as the homepage of one entity in the same topic. Therefore, we keep the document as the homepage of the entity where it has highest relevance score if the document has been returned as homepage of multiple entities.</p><p>The Entity track also requires the system to return the supporting documents for each entity. For each entity, we take the sum of the scores of all segments in one document as the score of that document and return the documents with highest scores as the supporting documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Official runs</head><p>We submitted the following four official runs in the entity track. They mainly differ in three aspects: (1) whether candidate or topic generation model is used for entity ranking; (2) whether the supporting segments are based on 50 or 100 words; (3) whether a non-uniform prior is used in the topic generation model. The description of the official runs are summarized as follows.</p><p>1. UdSmuCM50: This run uses 50 words as supporting segments and uses the candidate generation model to rank candidate entities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In this section, we show the results of our systems and analyze the effectiveness of each step. There was a bug in our submitted runs. In the submitted runs, we retrieved 1000 documents by each result entity. The same document may be returned in different entities. We kept the document in the entity where it had the highest relevance score and deleted the duplication from the returned documents of all other entities in the same topic. However, each entity only needs to return three normal homepages and one wikipedia homepage. We only need to avoid the same document to occur in the top 3 normal documents and top 1 wikipedia document, instead of all returned documents, of different entities. After fixing the bug, we only delete duplication from all top 3 normal documents and top 1 wikipedia document of all the entities. That is why the results in this paper are different from the official reported results. The normalized discounted cumulative gain at rank R (NDCG@R) is the main measure of the official results. Table <ref type="table" coords="7,322.76,492.99,4.98,8.74" target="#tab_0">1</ref> listed the NDCG@R, the number of relevant retrieved entities and the number of primary retrieved entities in our four runs. In segments with 100 words, the topic generation model with non-uniform prior performed best. The reason is that it uses the occurrences of entities in relevant information to measure the quality of the entities and that can help to select target entities from the entity candidates. For the segments in different length, the segments of length 50 performed better than those of length 100 in candidate generation model. The reason is that the small length will add more strict proximity constraints into the model and the entities co-occuring with the query in the same segment are more likely to be discussing the same topic.</p><p>In the following experiments, we analyze the effectiveness of each part by evaluating the entity names selected in each step. The method used in the following experiments is the topic generation model with non-uniform prior because it performed best in all the generative models. First of all, we evaluate how many percentage of target entity names were selected in each step. The recall values of target entities in the supporting information, entity candidates and result entities were 85.5%, 67.5% and 37.5% respectively. It showed that the quality of supporting information was good since less than 15 % of target entities were missed. But there were some problems in entity extraction step and entity ranking step because the recall dramatically decreased in these two steps.</p><p>After analyzing the results of entity extraction, we found that there were mainly three kinds of mistakes in the extracted entities. First, some entities were identified in the wrong types. For example, the organization Bowmore was extracted as the location. Second, the NER classifier made mistakes in identifying the boundary of the entities. For example, the Philadelphia Athletics Philadelphia White stockings Philadelphia Centennials was extracted as one entity. Third, some entities were entirely missed. The main reason is that their first letter is non-capitalized.</p><p>In the entity ranking step, the main problem was that we used the same segments collection to extract entity candidates and rank entities. The entity extraction and entity ranking steps were both based on the relevant segments. Therefore, all the extracted entities would have high scores in entity ranking and it was difficult to select the target entities. Table <ref type="table" coords="8,372.09,444.67,4.98,8.74" target="#tab_1">2</ref> showed the NDCG@R values of our systems when using different collections in entity ranking. Seg are the systems using segments collection in entity ranking, which are our submitted runs. Doc ret uses the documents in entity ranking and the documents are retrieved by the entity candidates from the document collection of the documents returned by all the original queries. Doc collection uses the documents retrieved by the entity candidates from the Category B collection. The results showed that the Doc collection performed best because the documents are only related to the entities but are not neccessarily related to the original query. The score of the noisy entities would decrease since their relevant documents would have lower relevance score with the original query. Therefore, it can distinguish the target entities from the noisy entities. Another interesting observation is that the topic generation model with non-uniform prior performed worse than the topic generation model with uniform prior in Doc collection. The reason is that the returned documents by entity candidates may be non-relevant to the original query and we cannot measure the quality of the entities by their occurrences in these documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This is the first year of the entity track. It requires the system to find the names, homepages and supporting document of the related entities. To solve this problem, we propose a general strategy with four steps, i.e., finding supporting information, extracting entities, ranking entities and finding homepages. Specifically, we use segment retrieval to find supporting information, apply Stanford NER classifier to extract candidate entities from the supporting information, adapt two generative models (i.e., candidate generation model and topic generation model) for expert finding to rank the candidate entities, and then use the language modeling approaches to find the homepages. The results show that the topic generation model performs better than the candidate generation model in the entity ranking step and short segments can perform better than long segments because they can combine more strict proximity constraints into the models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="2,133.77,154.62,360.93,149.77"><head></head><label></label><figDesc></figDesc><graphic coords="2,133.77,154.62,360.93,149.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,142.02,134.88,335.55,165.16"><head>Table 1 :</head><label>1</label><figDesc>Results of our submitted runs This run uses 100 words as supporting segments and uses the candidate generation model to rank candidate entities. 3. UdSmuTP: This run uses 100 words as supporting segments and uses the topic generation model with non-uniform prior to rank candidate entities. 4. UdSmuTU: This run uses 100 words as supporting segments and uses the topic generation model with uniform prior to rank candidate entities.</figDesc><table coords="7,142.02,144.62,323.90,79.70"><row><cell cols="5">UdSmuCM50 UdSmuCM UdSmuTU UdSmuTP</cell></row><row><cell>#rel</cell><cell>191</cell><cell>192</cell><cell>189</cell><cell>186</cell></row><row><cell>#pri</cell><cell>8</cell><cell>10</cell><cell>11</cell><cell>9</cell></row><row><cell>NDCG@R</cell><cell>0.0809</cell><cell>0.0698</cell><cell>0.0611</cell><cell>0.0729</cell></row><row><cell>2. UdSmuCM:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,140.78,134.88,329.75,57.50"><head>Table 2 :</head><label>2</label><figDesc>NDCG@R of our runs using different collections for entity ranking</figDesc><table coords="8,173.53,146.55,260.87,45.83"><row><cell></cell><cell cols="3">UdSmuCM UdSmuTU UdSmuTP</cell></row><row><cell>Seg</cell><cell>0.0506</cell><cell>0.051</cell><cell>0.0643</cell></row><row><cell>Doc ret</cell><cell>0.0727</cell><cell>0.0778</cell><cell>0.0894</cell></row><row><cell>Doc collection</cell><cell>0.0920</cell><cell>0.1118</cell><cell>0.1049</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,154.24,348.01,323.31,8.74;9,154.25,359.97,77.84,8.74" xml:id="b0">
	<monogr>
		<ptr target="http://ilps.science.uva.nl/trec-entity/guidelines/" />
		<title level="m" coord="9,154.24,348.01,178.96,8.74">Guideline of TREC 2009 Entity Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,154.24,379.89,323.28,8.74;9,154.25,391.85,222.22,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,360.22,379.89,117.30,8.74;9,154.25,391.85,66.56,8.74">Overview of the trec 2006 enterprise track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.73,391.85,104.13,8.74">Proceedings of TREC-06</title>
		<meeting>TREC-06</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,154.24,411.78,323.29,8.74;9,154.25,423.73,323.23,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,256.94,411.78,220.59,8.74;9,154.25,423.73,168.04,8.74">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,341.81,423.73,104.70,8.74">Proceedings of SIGIR-01</title>
		<meeting>SIGIR-01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,154.24,443.66,323.24,8.74;9,154.25,455.61,283.16,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,246.27,443.66,163.43,8.74">Probabilistic models for expert finding</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,427.82,443.66,49.67,8.74;9,154.25,455.61,253.01,8.74">Proceedings of the 29th European Conference on Information Retrieval</title>
		<meeting>the 29th European Conference on Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,154.24,475.54,323.32,8.74;9,154.25,487.49,323.28,8.74;9,154.25,499.45,323.24,8.74;9,154.25,511.40,323.25,8.74;9,154.25,523.36,75.76,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,369.81,475.54,107.75,8.74;9,154.25,487.49,323.28,8.74;9,154.25,499.45,15.95,8.74">An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition</title>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,188.69,499.45,288.80,8.74;9,154.25,511.40,323.25,8.74;9,154.25,523.36,45.60,8.74">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
