<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.39,171.90,339.47,18.18;1,168.33,196.80,273.58,18.18">TREC Blog and TREC Chem: A View from the Corn Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.63,236.67,88.95,12.58"><forename type="first">Yelena</forename><surname>Mejova</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa Iowa City</orgName>
								<address>
									<postCode>52242</postCode>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.98,236.67,84.67,12.58"><roleName>Viet</roleName><forename type="first">Ha</forename><surname>Thuc</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa Iowa City</orgName>
								<address>
									<postCode>52242</postCode>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.03,236.67,83.00,12.58"><forename type="first">Steven</forename><surname>Foster</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Global News Intelligence Montpelier</orgName>
								<orgName type="institution">The University of Iowa Team</orgName>
								<address>
									<postCode>05667</postCode>
									<region>VT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,134.48,254.60,115.26,12.58"><forename type="first">Christopher</forename><surname>Harris</surname></persName>
						</author>
						<author>
							<persName coords="1,264.09,254.60,65.05,12.58"><forename type="first">Bob</forename><surname>Arens</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Iowa Iowa City</orgName>
								<address>
									<postCode>52242</postCode>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.52,254.60,118.90,12.58"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName coords="1,244.27,290.47,127.47,12.58"><forename type="first">Informatics</forename><surname>Program</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.39,171.90,339.47,18.18;1,168.33,196.80,273.58,18.18">TREC Blog and TREC Chem: A View from the Corn Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D56B1F2FBB0568CADEC6EF802879DAA8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>participated in the blog track and the chemistry track of TREC-2009. This is our first year participating in the blog track as well as the chemistry track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLOG Track</head><p>This year the Blog Track contained two tasks: Top Stories Identification and Faceted Blog Distillation Tasks. Our submissions for both tasks are described below. In this, our first entry into the blog track, we explore various strategies (latent Dirichlet relevance model, URL based ranking, query expansion etc.) for both tasks. We first indexed the blog data with Lucene and identified occurrences of Headline URLs in the permalink documents (which included the content of the posts as well as the side bars of the web pages). Text windows (+/-800 characters including HTML code) surrounding the occurrences were harvested. The four runs submitted for the first task and the two for the second are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1: Top Stories Identification Task</head><p>The goal of this task is, given a unit of time (e.g. date), the system needs to identify the top news stories and provide a list of relevant blog posts discussing each news story. The ranked list of blog posts should have a diverse nature, covering different/diverse aspects or opinions of the news story<ref type="foot" coords="2,136.61,196.97,4.23,6.99" target="#foot_0">1</ref> . Our system (IowaS) uses strategies built around two sub goals:</p><p>1. Rank headlines for a query date and 2. Rank relevant posts for top headlines.</p><p>We submitted four runs for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Headline Ranking</head><p>Runs 1 and 2 are identical in ranking headlines, they differ in how they rank posts for a given headline. Exploring the idea that if a headline URL appears in a post then this indicates the post's relevance to the headline, we rank headlines by their URL frequencies in the blog collection. Sometimes it is the case that fewer than 100 headlines have URLs, then we randomly choose the rest of the headlines for submission. The distribution of the number of URLs citing a headline had a very long tail (see Fig <ref type="figure" coords="2,381.02,390.13,9.11,10.48">1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: URL citation distribution for headlines</head><p>Because of the time differences between various time zones, a three day window was taken around the given query date. Comparatively few URL citations appear in posts within the 3-day window while many occur in followup posts or in permalinks dated before the 3-day window. On inspection, we found these predated URL references occurring in dynamicallygenerated sidebars; because the blogpost harvesting date was in most cases many months after the permalink date, a relevant headline citation could be found in the active sidebar of a permalink dated before the headline's publication date.</p><p>We noted that these non-contemporaneous URL citations are often very persistent throughout the feed, implying the "pinned" headlines were intended by the blog's author to be relevant to the blog's general topic orientation. We intend to further examine whether these citations indicate blog content which can be useful for language modeling and query expansion.</p><p>For Runs 1 and 2, URL citation ranks were created from the entire set of permalinks including those outside the 3-day window. Subsequently (after publication of the qrels) we compared runs 1 and 2 to results obtained using URL citations only from permlinks dated within the 3-day window. From Table <ref type="table" coords="3,143.24,273.05,4.55,10.48" target="#tab_0">1</ref>, MAP and R-prec are .0867 and .1596 respectively in runs 1 and 2. Comparative results are MAP = 0.0868 and R-prec = .1599 when calculated with the 3-day permalink restriction.</p><p>In runs 3 and 4, text windows of +/-800 characters (including HTML tags) around the URL occurrences were extracted and used as pseudo-relevant documents for the corresponding headlines. We explore the use of our latent Dirichlet relevance model <ref type="bibr" coords="3,247.86,344.78,12.36,10.48" target="#b1">[2]</ref> to estimate a language model (LM) for each headline from these pseudo-relevant documents. In run 3, posts are ranked by a measure indicating intensity of discussion. This measure is computed as the cosine similarity between the headline language model and the content of each post in the 3-day window. Note that because of the time differences between various time zones, a three day window was taken around the given query date.</p><p>Run 4 combines headline URL frequencies and the headline intensity measure. We rank headlines by the posterior probability:</p><formula xml:id="formula_0" coords="3,172.24,464.33,327.16,10.48">p(headline|posts) = p(headline) * p(posts|headline)<label>(1)</label></formula><p>Here, prior probability p(headline) is proportional to the headline URL frequencies as in Runs 1 and 2. The likelihood is estimated by the similarity between the headline language models and the content of the posts as in Run 3. Table <ref type="table" coords="3,161.39,637.55,5.85,10.48" target="#tab_0">1</ref> shows the results of the four runs submitted. The last column shows the median performance over all the groups that participated in the Blog Headline Track. Though the numbers seem low overall, our system performs well above median for both MAP and R-precision measures. Performance scores are higher for runs 3 and 4; these runs returned more relevant documents -572 instead of 559. Also there are no appreciable differences between runs 3 and 4. Thus including consideration of URL frequency while using the LM intensity measure does not offer an added advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blog Post Ranking</head><p>For the second part of task 1, the retrieval and ranking of blog posts for specific headlines, we used Lucene<ref type="foot" coords="4,271.39,238.60,4.23,6.99" target="#foot_1">2</ref> to index the documents (using permalinks only). After several trial runs using the headline text only (with an internally generated training set), it became apparent that headline terms are not optimal for query design. Most of the headlines are "attention grabbers", and many don't contain any important keywords related to the article itself ("Out With The Old, In With The New", "From Cult Figures to a Band of the Moment", or "Twists and Turns, Finish Line in Sight"). Thus we explore query expansion.</p><p>For query expansion too we utilize the latent Dirichlet relevance model built from the text windows surrounding headline URL appearances in blog posts. Terms ranked at the top by the model are used in conjunction with phrases extracted from the original headline title to build a query. The phrases were extracted by dividing the title text using stop words and punctuation as separators. The original headline text was weighted more than the expansion terms. For example, here is a query for headline NYTimes-20080511-0032 "DITORIAL; Rethinking Ethanol": ''EDITORIAL Rethinking Ethanol''^10 OR ''editorial''^5 ''rethinking ethanol''^5 OR editorial^2.51 rethinking^1.32 ethanol^1.37 OR tag^4.0 rel^3.92 energyoutlook.blogspot.com^3.84 oil^3.81 label^3.59 search^2.83 global^2.32 ethanol^1.61 limit^1.61 nymex^1.61</p><p>Here, the quoted title has the most weight, then follow the phrases, then each (non-stopword) term in the title, and then ten expansion terms extracted by the latent Dirichlet relevance model, with weights determined by the model. In run 1 the top retrieved 10 posts are returned for each headline. In runs 2, 3, and 4 the rankings were adjusted to boost posts containing the headline URL toward the top.</p><p>Table <ref type="table" coords="4,161.71,610.43,5.85,10.48" target="#tab_1">2</ref> summarizes the strategies for the four runs submitted to Blog Track. The best performance was achieved with Run 4 (see Table <ref type="table" coords="4,455.49,622.38,5.85,10.48" target="#tab_0">1</ref> for it's task 1 results) with MAP score of 0.0882 and R-prec of 0.1606.  The post ranking results can be seen in Table <ref type="table" coords="5,362.77,464.37,4.55,10.48" target="#tab_2">3</ref>. The legend to the above table is as follows: [X,Y,Z] where X is number of queries for which our Run gave better results than median performance for the query; Y same as median and Z worse than median. The one headline that our system consistently underperformed was NYTimes-20090120-0009, and underperformed some of the time for NYTimes-20080830-0069 and NYTimes-20080830-0044. Further study of this phenomenon is needed. On the whole, our system performs as good as or better than the TREC median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 2: Faceted Blog Distillation Task</head><p>For the Faceted Blog Distillation Task we submitted two retrieval runs using the same Lucene index of blog posts.</p><p>In run 1 queries were composed using the &lt;query&gt; field and up to 10 terms extracted from the &lt;narrative&gt; field using TFIDF as the term ranking measure. Two hundred posts were retrieved and analyzed according to the querys facets. The top 100 satisfying a facet forms our submitted result set. Three facets were explored: opinionated vs. factual, in-depth vs. shallow, and personal vs. official.</p><p>For the opinionated vs. factual facet a Lingpipe<ref type="foot" coords="6,370.71,151.88,4.23,6.99" target="#foot_2">3</ref> classifier was used. The classifier was trained on 5000 "objective" and 5000 "subjective" sentences drawn from the Internet Movie Database (IMDB) archive<ref type="foot" coords="6,410.69,175.79,4.23,6.99" target="#foot_3">4</ref> and the Rotten Tomatoes customer reviews<ref type="foot" coords="6,253.77,189.25,4.23,6.99" target="#foot_4">5</ref> . Each post was classified as subjective or objective. Each set of posts were then returned preserving the search engine ranking.</p><p>For the in-depth vs. shallow facet the length of the posts was used (excluding stop words). The intuition is that in-depth discussion will produce longer documents than a shallow one. This hypothesis was examined manually with our training data, and post length was determined to be one of the surest ways to identify this facet. Again the set of posts were returned preserving the search engine ranking.</p><p>Finally for the personal vs. official facet we used the number of personal pronouns to rank posts. Here, we counted the occurrences of personal pronouns such as I, mine, my, etc. in the top 200 posts returned by the search engine. We then found the median number of personal pronouns and returned all posts above the median as personal and below -official, while conserving the original search engine's ranking.</p><p>In run 2 the same run 1 queries were first used to identify the top ranked 50 posts for each headline. Again using our latent Dirichlet relevance model to add ten expansion terms to the original query. The expanded query was searched against the Lucene index to retrieve again two hundred posts. These are then analyzed for facets using the same methods as in run 1. Table <ref type="table" coords="6,160.36,546.80,5.85,10.48" target="#tab_3">4</ref> shows the results for our two runs. The runs were evaluated first without looking at the facet (the "none" columns for each run), then looking at the first (i.e. opinionated, in-depth, and personal), and then looking at the second (factual, shallow, and official). A median score for all TREC submissions appears in the last column. The break down of the results by facet is shown in Table <ref type="table" coords="6,232.27,606.58,4.55,10.48" target="#tab_4">5</ref>. The legend to the above table is as follows: [X,Y,Z] where X is number of queries for which our Run gave better results than median performance for the query; Y same as median and Z worse than median. On average we see a performance improvement between Run 1 and Run 2. For all facets our system performs better on the R-prec scores than MAP compared to the median runs. In the case of in-depth vs. shallow, our system performs better on the in-depth facet than the shallow. Thus, our strategies may favored one facet and not the other, which suggests separate approaches for each of the facet's values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closing Remarks</head><p>As a first year of participation in TREC, it has been a time to explore the tasks and the approaches possible. The new dataset distributed by the University of Glasgow<ref type="foot" coords="7,204.43,520.41,4.23,6.99" target="#foot_5">6</ref> brings new opportunities and challenges. It took several weeks to index the permalink documents using a cluster of 14 machines. Further analysis is needed to determine the distribution of languages in the dataset, the relationship of the posting, commenting, and crawling dates, etc. Also, further study needs to be done on the nature of both tasks: are headlines sufficient for news story retrieval? what precisely does relevancy mean in the context of news publishing? how can blog community be leveraged to determine what stories are really important, and to whom? We hope to address these and other questions in the coming Blog Track years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chemistry Track Strategy Overview</head><p>For our first year in the Chemistry track we chose to focus on Task 2: the Prior Art task. We were supplied with a dataset consisting of more than 100,000 chemical patents in XML format issued by the USPTO and EPO. We were also given 1,000 chemistry-related query patents and asked to return a list of up to 1,000 patents that could potentially invalidate a given query patent. We were not able to use the 'References Cited' field of the query patent -our task was to recreate this list with the provided patent dataset. For computationally-expensive submissions, participants also had the option of providing runs using only the first 100 query patents. Our team made one submission using all 1,000 query patents and two using the first 100 query patents.</p><p>Our initial intuition was that the claims section of a patent was central for invalidity searches, since claims are arguably the most important and most scrutinized part of a patent. Thus we began by producing two separate indexes using Lucene: one with patent claims alone; the other with the Title, Description, Abstract, and Classification Code portions (we use the acronym 'TDAC' to refer to this index). As claims are often nested within each other we first 'un-nest' them so they would each stand as an independent pseudo 'document' for indexing and retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Training of the Patent Dataset</head><p>A training set of 15 EPO patents was provided to all participants. We noticed a majority of query patents were issued by the USPTO; hence we created a second training set of 15 randomly-selected US patents to train on as well (we only include those patents not in the 1,000-query test set). Indeed we found this second training set to be a better reflection of the results received from our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description of Each Run</head><p>For our first run (UIowaS09PA1), we determined that each patent's claims should be run as individual queries against the Claims index and a separate query built from the Title, Description, Abstract, and Classification Code (TDAC) fields was run on the second index. While retrieving against the Claims index, in cases where multiple claims from the same patent appeared in the retrieved list, we took the most favorable, i.e., best rank/score for that patent. The two sets of results were merged through summarizing functions. We used our training queries to experiment with the relative weights to apply to the different summary functions. Table <ref type="table" coords="8,325.82,658.32,5.85,10.48" target="#tab_5">6</ref> shows the results on the first 100 queries using the trained function. We determined the best function to be one that weighted the score from the TDAC index as 10 times more important than the score returned by the claims index. This list of ranked patents was unduplicated where necessary. A key part of our retrieval strategy for this first run was to limit the retrieved patents to those with a priority date preceding the query patent's priority date as a threshold.</p><p>Our second and third runs were more experimental in nature and were run against the first 100 query patents. The second run (UIowaS09PA2) used only the primary classification information from the TDAC index to retrieve those patents with matching primary classifications. Priority dates were not used and they were ranked by ascending patent number. Our third run (UIowaS09PA3) was a refinement of our second run. Specifically, we worked on the assumption that patent numbers reflected a temporal sequence and only those candidate patent numbers lower than the target patent number were included. Thus, we made a better approximation of priority dates using this sequence at the expense of returning fewer patents per query.</p><p>A fourth run, also run against the first 100 queries, was not completed prior to the submission deadline. However, we believe it is interesting as we apply a technique that we had applied to a smaller dataset in previous research <ref type="bibr" coords="9,155.34,356.74,12.36,10.48" target="#b0">[1]</ref> and it shows promise for future examination. We apply this technique by first creating a machine-readable representation of the hierarchical IPC classification structure and then calculating a similarity measure between the primary classification code for each of the query patents and each of the 1,000 retrieved patents. We then re-rank these 1,000 retrieved patents for each of query patent by this similarity score. Using the first 100 queries in Run 1 as a baseline, this boosting technique demonstrated improvement on some key metrics (i.e., MAP increased by 49%). See Table <ref type="table" coords="9,417.63,440.42,5.85,10.48" target="#tab_6">7</ref> for the results for each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In the first table below, we examine the effects of the ratio of weights on the two indexes (Claims and TDAC) across four measures: mean average precision (MAP), binary preference (bpref), recall after 100 retrieved documents (recall-100), and normalized discounted cumulative gain (ndcg). Although we initially thought that a retrieval method with a heavier weighting on Claims (vs. TDAC) would perform better, our results demonstrate this is not the case. For Run 1 we used the TDAC:Claims ratio of 10:1, which provided slightly better results than the other ratios we examined.</p><p>In Table <ref type="table" coords="9,173.61,602.36,4.55,10.48" target="#tab_6">7</ref>, we show key metrics from each of our submitted runs (Runs 1-3) and one post-TREC run (Run 4) on these same metrics. Run 4 used the list of retrieved patents from Run 1 and had a boosting technique applied to rerank them, improving the observed results across all four examined metrics. There are two medians provided: one for submissions by all participants across all 1,000 queries, and another for submissions by all participants across the first 100 queries (the smaller dataset). Both Run 1 and Run 4 were above the median for all four metrics evaluated. Runs 2 and 3 performed relatively poorly compared with Runs 1 and 4, and poorly compared with the medians scores for these four metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closing Remarks</head><p>With MAP scores under 0.10 and ndcg scores under 0.25, the ability to find possible prior art violators is understood to be non-trivial -we see there is room for improvement in chemical patent search. As with the Blog track, we have considered a number of different techniques but have not yet implemented them. Much of our processing was done after indexing, which allowed us to examine the effects of different techniques quickly, but was reliant upon our underlying indexing strategy. In the coming months, we plan to explore the role of classification codes in more detail and examine each component of our TDAC index independently to determine which patent elements best comprise an effective index. We enjoyed participating in this track and look forward to participating in future TREC Chemistry tracks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,147.35,550.84,315.56,54.33"><head>Table 1 :</head><label>1</label><figDesc>Headline Ranking Results</figDesc><table coords="3,147.35,576.86,315.56,28.31"><row><cell>Measure</cell><cell>Run 1</cell><cell>Run 2</cell><cell>Run 3</cell><cell>Run 4</cell><cell>TREC Median (all submissions)</cell></row><row><cell>MAP</cell><cell>0.0867</cell><cell>0.0867</cell><cell>0.0880</cell><cell>0.0882</cell><cell>0.0445</cell></row><row><cell>R-prec</cell><cell>0.1596</cell><cell>0.1596</cell><cell>0.1601</cell><cell>0.1606</cell><cell>0.1075</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,165.61,135.90,279.03,112.31"><head>Table 2 :</head><label>2</label><figDesc>Blog post ranking strategies</figDesc><table coords="5,165.61,161.92,279.03,86.29"><row><cell>Run</cell><cell>Headline Ranking</cell><cell>Post Ranking</cell></row><row><cell>Run 1</cell><cell>URL Ranking</cell><cell>Retrieval with expansion terms</cell></row><row><cell></cell><cell></cell><cell>+ phrases</cell></row><row><cell>Run 2</cell><cell>URL Ranking</cell><cell>Retrieval with expansion terms</cell></row><row><cell></cell><cell></cell><cell>+ phrases + URL boosting</cell></row><row><cell>Run 3</cell><cell>Headline intensity ranking</cell><cell>retrieval with expansion terms</cell></row><row><cell></cell><cell></cell><cell>+ phrases + URL boosting</cell></row><row><cell>Run 4</cell><cell>Combination of URL and</cell><cell>retrieval with expansion terms</cell></row><row><cell></cell><cell>headline intensity ranking</cell><cell>+ phrases + URL boosting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,192.29,284.91,225.68,141.10"><head>Table 3 :</head><label>3</label><figDesc>Blog post ranking results</figDesc><table coords="5,192.29,310.90,225.68,115.11"><row><cell></cell><cell cols="2">Mean Scores Across All Headlines</cell></row><row><cell></cell><cell>alpha-ndcg@10</cell><cell>IA-P@10</cell></row><row><cell>Run 1</cell><cell>0.341</cell><cell>0.099</cell></row><row><cell>Run 2</cell><cell>0.322</cell><cell>0.094</cell></row><row><cell>Run 3</cell><cell>0.328</cell><cell>0.097</cell></row><row><cell>Run 4</cell><cell>0.328</cell><cell>0.097</cell></row><row><cell cols="3">Performance Difference Compared to TREC Median</cell></row><row><cell></cell><cell>alpha-ndcg@10</cell><cell>IA-P@10</cell></row><row><cell>Run 1</cell><cell>[181,74,3]</cell><cell>[177,79,2]</cell></row><row><cell>Run 2</cell><cell>[171,86,1]</cell><cell>[169,87,2]</cell></row><row><cell>Run 3</cell><cell>[175,82,1]</cell><cell>[173,83,2]</cell></row><row><cell>Run 4</cell><cell>[175,82,1]</cell><cell>[173,83,2]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,148.44,452.55,313.36,64.19"><head>Table 4 :</head><label>4</label><figDesc>Summary Faceted Blog Distillation Task Results</figDesc><table coords="6,148.44,478.57,313.36,38.17"><row><cell></cell><cell></cell><cell>Run 1</cell><cell></cell><cell></cell><cell>Run 2</cell><cell></cell><cell>TREC Median</cell></row><row><cell cols="2">Measure none</cell><cell>first</cell><cell>second</cell><cell>none</cell><cell>first</cell><cell>second</cell><cell>(all submissions)</cell></row><row><cell>MAP</cell><cell>0.07</cell><cell>0.0390</cell><cell>0.0262</cell><cell cols="2">0.0785 0.0467</cell><cell>0.0439</cell><cell>0.1265</cell></row><row><cell>R-prec</cell><cell>0.13</cell><cell>0.0394</cell><cell>0.0401</cell><cell cols="2">0.1368 0.0483</cell><cell>0.0662</cell><cell>0.1867</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,159.73,135.90,290.79,173.88"><head>Table 5 :</head><label>5</label><figDesc>Faceted Blog Distillation Task Results by Facet</figDesc><table coords="7,207.64,161.89,194.99,147.88"><row><cell></cell><cell cols="3">Opinionated Vs Factual</cell><cell></cell></row><row><cell></cell><cell cols="2">Run 1</cell><cell cols="2">Run 2</cell></row><row><cell>Measure</cell><cell>opinion.</cell><cell>factual</cell><cell>opinion.</cell><cell>factual</cell></row><row><cell>MAP</cell><cell>[7,1,5]</cell><cell>[7,2,4]</cell><cell>[7,0,6]</cell><cell>[5,2,6]</cell></row><row><cell>R-prec</cell><cell>[6,6,1]</cell><cell>[2,9,2]</cell><cell>[5,6,2]</cell><cell>[1,10,2]</cell></row><row><cell></cell><cell cols="3">Personal Vs Official</cell><cell></cell></row><row><cell></cell><cell cols="2">Run 1</cell><cell cols="2">Run 2</cell></row><row><cell>Measure</cell><cell>personal</cell><cell>official</cell><cell>personal</cell><cell>official</cell></row><row><cell>MAP</cell><cell>[6,1,1]</cell><cell>[5,1,2]</cell><cell>[5,2,1]</cell><cell>[5,1,2]</cell></row><row><cell>R-prec</cell><cell>[4,3,1]</cell><cell>[1,6,1]</cell><cell>[4,3,1]</cell><cell>[1,5,2]</cell></row><row><cell></cell><cell cols="3">In-depth Vs Shallow</cell><cell></cell></row><row><cell></cell><cell cols="2">Run 1</cell><cell cols="2">Run 2</cell></row><row><cell>Measure</cell><cell>in-depth</cell><cell>shallow</cell><cell>in-depth</cell><cell>shallow</cell></row><row><cell>MAP</cell><cell>[9,4,5]</cell><cell>[13,3,2]</cell><cell>[7,4,7]</cell><cell>[13,2,3]</cell></row><row><cell>R-prec</cell><cell>[4,11,3]</cell><cell>[5,13,0]</cell><cell>[4,9,5]</cell><cell>[3,14,1]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,118.42,135.90,373.41,141.50"><head>Table 6 :</head><label>6</label><figDesc>Determining the Ratio of Weights Between Summary Functions</figDesc><table coords="10,178.76,161.92,252.74,115.48"><row><cell>TDAC:Claims Index Ratio</cell><cell>MAP</cell><cell>bpref</cell><cell>recall-100</cell><cell>ndcg</cell></row><row><cell>1:100</cell><cell cols="2">0.0203 0.1494</cell><cell>0.0621</cell><cell>0.0886</cell></row><row><cell>1:50</cell><cell cols="2">0.0206 0.1588</cell><cell>0.0643</cell><cell>0.0956</cell></row><row><cell>1:10</cell><cell cols="2">0.0250 0.1630</cell><cell>0.0754</cell><cell>0.1034</cell></row><row><cell>1:5</cell><cell cols="2">0.0267 0.1658</cell><cell>0.0839</cell><cell>0.1071</cell></row><row><cell>1:2</cell><cell cols="2">0.0313 0.1802</cell><cell>0.1067</cell><cell>0.1214</cell></row><row><cell>1:1</cell><cell cols="2">0.0348 0.1880</cell><cell>0.1134</cell><cell>0.1304</cell></row><row><cell>2:1</cell><cell cols="2">0.0375 0.2207</cell><cell>0.1256</cell><cell>0.1451</cell></row><row><cell>5:1</cell><cell cols="2">0.0469 0.4028</cell><cell>0.1962</cell><cell>0.2159</cell></row><row><cell>10:1</cell><cell cols="2">0.0485 0.4207</cell><cell>0.1888</cell><cell>0.2245</cell></row><row><cell>50:1</cell><cell cols="2">0.0479 0.4161</cell><cell>0.1412</cell><cell>0.2226</cell></row><row><cell>100:1</cell><cell cols="2">0.0466 0.4033</cell><cell>0.1379</cell><cell>0.2118</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,153.62,314.10,303.02,99.72"><head>Table 7 :</head><label>7</label><figDesc>Run Results</figDesc><table coords="10,153.62,337.79,303.02,76.03"><row><cell>Runs</cell><cell>Query Size</cell><cell>MAP</cell><cell>bpref</cell><cell>recall-100</cell><cell>ndcg</cell></row><row><cell>Run 1</cell><cell>1000</cell><cell cols="2">0.0683 0.4066</cell><cell>0.1851</cell><cell>0.2643</cell></row><row><cell>Run 1</cell><cell>100</cell><cell cols="2">0.0485 0.4207</cell><cell>0.1888</cell><cell>0.2245</cell></row><row><cell>Run 2</cell><cell>100</cell><cell cols="2">0.0049 0.1457</cell><cell>0.0368</cell><cell>0.0616</cell></row><row><cell>Run 3</cell><cell>100</cell><cell cols="2">0.0066 0.1092</cell><cell>0.0447</cell><cell>0.0542</cell></row><row><cell>Run 4</cell><cell>100</cell><cell cols="2">0.1017 0.4401</cell><cell>0.1924</cell><cell>0.2813</cell></row><row><cell>TREC 1000 query median</cell><cell>1000</cell><cell cols="2">0.0279 0.3614</cell><cell>0.0594</cell><cell>0.1639</cell></row><row><cell>TREC 100 query median</cell><cell>100</cell><cell cols="2">0.0229 0.3950</cell><cell>0.0654</cell><cell>0.1525</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,128.79,665.44,184.46,8.74"><p>http://ir.dcs.gla.ac.uk/wiki/TREC-BLOG</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,128.79,657.81,112.38,8.74"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,128.79,627.07,121.82,8.74"><p>http://alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,128.79,639.02,103.22,8.74"><p>http://www.imdb.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,128.79,650.98,147.03,8.74"><p>http://www.rottentomatoes.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="7,128.79,640.48,241.83,8.74"><p>http://ir.dcs.gla.ac.uk/test collections/blogs08info.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,134.92,153.38,364.47,10.48;11,134.92,165.34,364.49,10.48;11,134.92,177.29,313.51,10.48" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,369.16,153.38,130.23,10.48;11,134.92,165.34,165.43,10.48">On the Role of Classification in Patent Invalidity Searches</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,322.29,165.34,177.12,10.48;11,134.92,177.29,271.73,10.48">Proceedings of the 2nd Interational Workshop on Patent Information Retrieval (PaIR&apos;09)</title>
		<meeting>the 2nd Interational Workshop on Patent Information Retrieval (PaIR&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,134.92,199.21,364.48,10.48;11,134.92,211.17,300.33,10.48" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,276.57,199.21,222.83,10.48;11,134.92,211.17,45.09,10.48">A Latent Dirichlet Framework for Relevance Modeling</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ha-Thuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,204.17,211.17,188.46,10.48">Proceedings of the 5th AIRS (LNCS)</title>
		<meeting>the 5th AIRS (LNCS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
