<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.96,115.96,321.45,12.62;1,172.38,133.89,270.59,12.62">Facet Classification of Blogs: Know-Center at the TREC 2009 Blog Distillation Task</title>
				<funder>
					<orgName type="full">State of Styria</orgName>
				</funder>
				<funder ref="#_x3vb3um">
					<orgName type="full">Austrian Ministry of Transport, Innovation and Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Ministry of Economics and Labor</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.04,171.56,58.20,8.74"><forename type="first">Elisabeth</forename><surname>Lex</surname></persName>
							<email>elex@know-center.at</email>
							<affiliation key="aff0">
								<orgName type="department">Know-Center Graz</orgName>
								<address>
									<addrLine>Inffeldgasse 21a</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.23,171.56,77.05,8.74"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Know-Center Graz</orgName>
								<address>
									<addrLine>Inffeldgasse 21a</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.42,171.56,76.90,8.74"><forename type="first">Andreas</forename><surname>Juffinger</surname></persName>
							<email>ajuffinger@know-center.at</email>
							<affiliation key="aff0">
								<orgName type="department">Know-Center Graz</orgName>
								<address>
									<addrLine>Inffeldgasse 21a</addrLine>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.96,115.96,321.45,12.62;1,172.38,133.89,270.59,12.62">Facet Classification of Blogs: Know-Center at the TREC 2009 Blog Distillation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">382BC2FE276A1FDA2297A88CF19FCD55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we outline our experiments carried out at the TREC 2009 Blog Distillation Task. Our system is based on a plain text index extracted from the XML feeds of the TREC Blogs08 dataset. This index was used to retrieve candidate blogs for the given topics. The resulting blogs were classified using a Support Vector Machine that was trained on a manually labelled subset of the TREC Blogs08 dataset. Our experiments included three runs on different features: firstly on nouns, secondly on stylometric properties, and thirdly on punctuation statistics. The facet identification based on our approach was successful, although a significant number of candidate blogs were not retrieved at all.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>On the web, a huge amount of blogs is available. However, the quality of these blogs is questionable in respect to quality because almost no mechanisms exist to validate the content before it is posted online <ref type="bibr" coords="1,351.67,423.45,9.96,8.74" target="#b0">[1]</ref>. In this year's TREC blog track, specifically the blog distillation task, exactly this problem is addressed. The goal of the track is to rank candidate blogs that are relevant to a topic or query and to assign the retrieved blogs to a set of facets that represent different quality aspects <ref type="foot" coords="1,202.95,469.70,3.97,6.12" target="#foot_0">1</ref> .</p><p>We consider this as a two-step procedure: (i) the first step is to retrieve blogs that are relevant to a topic or query, respectively. This is an Information Retrieval (IR) process. The second step (ii) is to assign particular facets of interest to the retrieved blogs. For the second step, we applied supervised classification on a manually labelled subset of the TREC Blogs08 dataset. Supervised classification has been successfully applied to text classification <ref type="bibr" coords="1,352.04,543.00,10.52,8.74" target="#b1">[2]</ref> and facet classification like for instance emotion classification <ref type="bibr" coords="1,282.99,554.96,9.96,8.74" target="#b2">[3]</ref>. We trained a classifier on a subset of blogs randomly taken from the TREC Blogs08 dataset<ref type="foot" coords="1,349.83,565.34,3.97,6.12" target="#foot_1">2</ref> and labelled them manually according to the facets. The classifier was then used to categorise the blogs into the facets.</p><p>The rest of this paper is structured as follows: Section 2 outlines the collection and the preprocessing of the data, Section 3 describes the methods and runs, in Section 4, the results are given, and Section 5 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection and Preprocessing</head><p>The experiments for the blog distillation task are carried out on the Blogs08 dataset. The Blogs08 dataset has 28.488.767 blog posts from 1.303.520 blog feeds <ref type="foot" coords="2,156.13,182.88,3.97,6.12" target="#foot_2">3</ref> . It samples the blogosphere from January 2008 to February 2009.</p><p>For the first step, the retrieval task, we vectorised and indexed 606.939 different blogs out of 1.303.520 blogs. From our indexed blogs, we retrieved 3.476 million different blog feeds. Our search index size was 41 GB, and the size of the whole repository with vectors for all features was 220 GB. Previous experiences have shown that an analysis has to be performed on blog post level <ref type="bibr" coords="2,435.74,247.76,9.96,8.74" target="#b5">[6]</ref>. To extract the blog posts, we exploited the permalinks.</p><p>For the second step, the classification task, we manually annotated 83 blogs by the facets given in the TREC blog task. These 83 blogs were selected randomly to guarantee an independent sample. Also, due to the random selection, we assume that the label distribution reflects the real blog world although the number of blogs is not high. As mentioned before, we assigned the facets on blog level. However, or analysis was done on blog post level, so if the whole blog was assigned e.g. the facet "opinionated", we assigned this label to all its blog posts. This resulted in 12870 annotated blog posts. Since we did not check all the labelling of all 12870 blog posts, there might be some mislabelled items. We then transformed the unstructured blog posts in a structured form and thus prepared the blog posts for our text mining unit. Note that we first filtered the blogs by language in order to retrieve only English blogs. For this, we exploited a language guesser developed at our institution that is based on n-grams and the Apache Nutch project <ref type="foot" coords="2,230.56,440.99,3.97,6.12" target="#foot_3">4</ref> .From the remaining structured blog posts, we extracted nouns, sentences, and punctuation features using an information extraction tool developed at our institution based on OpenNLP<ref type="foot" coords="2,348.26,464.90,3.97,6.12" target="#foot_4">5</ref> . For each set of features, we created a vector and stored these vectors in our index.</p><p>Our search framework was based on Apache Lucene which is an open source search engine that is able to index several gigabytes of documents at reasonable time. Also, complex searches can be performed quickly on the documents. Besides, Lucene maintains a relevance ranking scoring which is applicable to find the best matches to queries. We used this relevance score for ranking, as described later. To sum up, our resulting training set consists of 12870 blog posts. We trained our classifier on the annotated blog posts and also classified the blogs on blog post level. To assign the facets to the whole blogs, we performed a majority voting over the posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevance Ranking</head><p>In order to derive a final relevance ranking, we normalised the Lucene score so that it lies between 0 and 1 by dividing it by the maximum score which is shown in Equation <ref type="formula" coords="3,189.42,161.04,3.87,8.74" target="#formula_0">1</ref>.</p><formula xml:id="formula_0" coords="3,237.63,184.95,242.96,8.74">luceneScore = score/maxScore<label>(1)</label></formula><p>Then, we calculated the final relevance score whereas we took into consideration both the Lucene score and the classifier confidence for the particular facet. The formula for the final score is shown in Equation <ref type="formula" coords="3,366.23,225.44,3.87,8.74">2</ref>.</p><formula xml:id="formula_1" coords="3,174.90,249.35,305.70,8.74">f inalScore = α * lucenceScore + (1 -α) * f acetConf idence (2)</formula><p>3 Methods and Runs</p><p>Four the retrieval task, we used Apache Lucene as search engine, as mentioned before. With Lucene, we searched our blog index for the 50 topics and ranked and sorted the top 100 blogs according to our relevance ranking scheme (see Equation <ref type="formula" coords="3,178.84,337.42,3.87,8.74">2</ref>). We then classified the top 100 blogs into the given facets using the trained classifier. As a classification algorithm, we applied a Support Vector Machine based on LibLinear <ref type="bibr" coords="3,261.87,361.33,10.52,8.74" target="#b3">[4]</ref> with standard parameterisation. We carried out three runs whereas each run is characterised by the features used in this run:</p><p>1. Nounfull NF : For this run, we used a feature space only with nouns annotated by the OpenNLP library<ref type="foot" coords="3,285.15,414.85,3.97,6.12" target="#foot_5">6</ref> . 2. Punctfull PF : In this run, we created a feature space only with punctuation features. Punctuation features are for instance the amount of double or triple punctuations. 3. Sentencefull SF : For this run, we used a feature space with stylometric properties based on sentence statistics. Examples for such features are the average number of words per sentence, the average number of unique POS tags per sentence, the ratio of lower case characters to upper case characters, the number of adjectives or adverbs per tokens.</p><p>Clearly, a number of different features can be used for text classification problems, as described in <ref type="bibr" coords="3,244.83,542.80,9.96,8.74" target="#b6">[7]</ref>. In our feature selection strategy,we considered both lexical and stylometric features. Nouns belong to the category of lexical features and generally cover topics. Usually, a classification based on nouns yields good results, at least in our experience. Punctuation features and features based on sentence statistics are stylometric features. In general, stylistic variations are described by stylometric features and therefore such features are often used for authorship identification <ref type="bibr" coords="3,244.30,614.53,9.96,8.74" target="#b4">[5]</ref>. In our experiments we applied stylometric features to guarantee topic-independence. Our goal was to avoid that the classifier learns topics instead of characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>With our methodology, we achieved the following results:</p><p>Of the 50 topics, we were able to retrieve 39 topic which had at least one relevant blog for each side of the facet (e.g., one relevant opinionated blog and one relevant factual blog). Therefore, all our results relate to these 39 topics. Note that we did not manage to vectorize and index the whole Blogs08 dataset <ref type="foot" coords="4,473.36,194.18,3.97,6.12" target="#foot_6">7</ref> . Therefore, relevant blogs were missing in our index which is reflected in poor results for certain topics. In Table <ref type="table" coords="4,291.91,219.67,3.87,8.74" target="#tab_0">1</ref>, the results for all runs are summarised. For each run, three rankings of the blogs were performed. The first, "none", denotes a ranking with no facet value applied and serves therefore as baseline. The second "first" correspond to the first value of a facet ("opiniated", "indepth", "personal") and the third "second" represent the second value of a facet ("factual", "shallow", "official").  <ref type="table" coords="4,202.96,468.07,4.98,8.74" target="#tab_0">1</ref> one can see that the obtained MAP values for all rankings are quite low. In general, MAP corresponds to the average precision and emphasises returning more relevant documents earlier. Due to the fact that we were only able to index less than half of the data, our retrieval was suboptimal. More specifically, in many cases, we did not retrieve enough relevant blogs for a topic and this lead to quite low MAP values. For instance, in run NF with the second facet enabled, only 69 blogs out of 331 relevant blogs were retrieved and consequently, the MAP value for this run was low, as shown in Table <ref type="table" coords="4,334.85,551.76,3.87,8.74">2</ref>.</p><p>In cases where our retrieval was successful which means we obtained enough relevant blogs for a topic, also good MAP results were achieved. An example of this is outlined in Table <ref type="table" coords="4,237.05,588.46,3.87,8.74">3</ref>, which holds the results for the first facet for topic 1101.</p><p>Another example where the classification was successful was topic 1103, as shown in Table <ref type="table" coords="4,204.01,625.17,3.87,8.74" target="#tab_1">4</ref>. Compared to the summary statistics for the first facet "opinionated" , which is given in Table <ref type="table" coords="5,209.77,384.94,3.87,8.74">5</ref>, one can see, that for this topic 1103, our results were above average for the runs "punctfull" and "sentencefull".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>MAP1. MAP2 MAP3 R-Prec1. R-Prec2 0.4286 0.1667 0.0000 0.4286 0.2857 Table <ref type="table" coords="5,216.40,451.34,4.13,7.89">5</ref>. SENTENCE: Results for first facet for topic 1103</p><p>In some cases, our relevance ranking was not ideal as is reflected in low values for R-precision. In later experiments on the Blogs08 dataset where we also used the annotated blogs for training, we were able to achieve a classification accuracy of 0.75 for nouns only. In other experiments on other features e.g. stems, we reached an accuracy of 0.91 on blog post level. A deeper analysis of the results of the facet classification in this work revealed that for the first facets personal, opinionated and indepth, we achieved a classifier accuracy of 80%. Summing up, if we improve our retrieval and ranking, we can definitely improve our results. Also, later experiments revealed that if we use features like stems, an improvement of 15-30% of the classification accuracy can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we described the efforts and results for our participation in the TREC blog track 2009. Our system is based a plain text index extracted from the XML feeds only. We successfully indexed 680k of 1.3 Mio blogs and this index was used to retrieve candidate blogs for the given topics. From the top 2500 result blog entries, the top 100 blogs were identified according to the accumulated relevance score of the particular blog entries. The resulting blogs were classified using a Support Vector Machine trained on a manually labelled subset of the TREC Blogs08 dataset. Three runs were conducted, one based on nouns, one based on stylometric properties, and one based on punctuation statistics. The facet identification based on this approach was successful, although a significant number of candidate blogs were not retrieved at all. A deeper analysis of our facet classification results revealed that for the first facets personal, opinionated and indepth, we achieved a classifier accuracy of 80%. Our best run was based on sentence statistics for the facet "Personal": we achieved the fifth best results of all groups with this run. The results for the facet "Opinionated" were our second best results, also based on sentence statistics. Note that sentence statistics are a very simple feature. Clearly this is a very encouraging finding, since apparently very simple stylometric features allow to address certain quality aspects of blogs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,149.71,315.04,242.91,161.78"><head>Table 1 .</head><label>1</label><figDesc>Results for all facets for all runsFrom Table</figDesc><table coords="4,229.79,315.04,155.79,106.89"><row><cell cols="2">RUN Facet MAP R-Precision P@10</cell></row><row><cell>NF</cell><cell>none 0.0624 0.0980 0.1410</cell></row><row><cell>NF</cell><cell>first 0.0538 0.0725 0.0769</cell></row><row><cell cols="2">NF second 0.0231 0.0346 0.0256</cell></row><row><cell>PF</cell><cell>none 0.0624 0.0980 0.1410</cell></row><row><cell>PF</cell><cell>first 0.0691 0.0846 0.0692</cell></row><row><cell cols="2">PF second 0.0227 0.0339 0.0308</cell></row><row><cell>SF</cell><cell>none 0.0624 0.0980 0.1410</cell></row><row><cell>SF</cell><cell>first 0.0559 0.0717 0.0667</cell></row><row><cell cols="2">SF second 0.0302 0.0334 0.0436</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,187.50,129.14,240.37,210.37"><head>Table 4 .</head><label>4</label><figDesc>SENTENCE: Results for first facet for topic 1103</figDesc><table coords="5,187.50,129.14,240.37,199.46"><row><cell cols="2">numQueries</cell><cell>39</cell></row><row><cell cols="2">numRetrieved</cell><cell>3801</cell></row><row><cell cols="2">numRelevant</cell><cell>331</cell></row><row><cell cols="3">numRelevantRetrieved 69</cell></row><row><cell>map</cell><cell></cell><cell>0.0231</cell></row><row><cell cols="3">Table 2. NF: Results for second facet -all</cell></row><row><cell cols="3">TOPIC RUN MAP R-Precision P@10</cell></row><row><cell>1101</cell><cell cols="2">NF 0.2590 0.4000 0.5000</cell></row><row><cell>1101</cell><cell cols="2">PF 0.2989 0.4571 0.6000</cell></row><row><cell>1101</cell><cell cols="2">SF 0.2221 0.4143 0.3000</cell></row><row><cell cols="3">Table 3. SENTENCE: Results for first facet for topic 1101</cell></row><row><cell cols="3">TOPIC RUN MAP R-Precision P@10</cell></row><row><cell>1103</cell><cell cols="2">NF 0.0525 0.1429 0.1000</cell></row><row><cell>1103</cell><cell cols="2">PF 0.2857 0.2857 0.2000</cell></row><row><cell>1103</cell><cell cols="2">SF 0.2381 0.2857 0.2000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,645.84,170.62,7.86"><p>http://ir.dcs.gla.ac.uk/wiki/TREC-BLOG</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,656.80,223.69,7.86"><p>http://ir.dcs.gla.ac.uk/test collections/blogs08info.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,634.88,223.69,7.86"><p>http://ir.dcs.gla.ac.uk/test collections/blogs08info.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,645.84,131.09,7.86"><p>http://lucene.apache.org/nutch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,144.73,656.80,128.36,7.86"><p>http://opennlp.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,144.73,656.80,123.75,7.86"><p>http://opennlp.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,144.73,645.84,335.86,7.86;4,144.73,656.80,71.96,7.86"><p>Unfortunately, the lucene index broke a few days before the deadline and we had to re-index the data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The Know-Center is funded within the <rs type="programName">Austrian COMET Program -Competence Centers for Excellent Technologies -</rs>under the auspices of the <rs type="funder">Austrian Ministry of Transport, Innovation and Technology</rs>, the <rs type="funder">Austrian Ministry of Economics and Labor</rs> and by the <rs type="funder">State of Styria</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_x3vb3um">
					<orgName type="program" subtype="full">Austrian COMET Program -Competence Centers for Excellent Technologies -</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,445.45,342.24,7.86;6,146.91,456.41,333.68,7.86;6,146.91,467.37,56.19,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,336.63,445.45,143.96,7.86;6,146.91,456.41,256.30,7.86">Findin high-quality content in social media with application to community-based question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-D</forename><forename type="middle">D G A</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,423.73,456.41,56.86,7.86;6,146.91,467.37,25.70,7.86">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,478.33,342.25,7.86;6,146.91,489.29,333.68,7.86;6,146.91,500.25,281.73,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,380.61,489.29,99.98,7.86;6,146.91,500.25,253.88,7.86">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Viii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Viii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Viii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Informatik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Informatik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Informatik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Informatik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Informatik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,511.21,342.25,7.86;6,146.91,522.16,333.67,7.86;6,146.91,533.12,93.24,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,347.14,511.21,133.45,7.86;6,146.91,522.16,48.96,7.86">Emotion classification using web blog corpora</title>
		<author>
			<persName coords=""><forename type="first">H.-H</forename><forename type="middle">C</forename><surname>Changhua Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Hsin-Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,215.84,522.16,264.75,7.86;6,146.91,533.12,65.46,7.86">Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence</title>
		<meeting>the IEEE/WIC/ACM International Conference on Web Intelligence</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,544.08,342.25,7.86;6,146.91,555.04,265.22,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,363.25,544.08,117.34,7.86;6,146.91,555.04,75.41,7.86">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,229.97,555.04,153.92,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,566.00,342.25,7.86;6,146.91,576.96,131.20,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,187.48,566.00,254.25,7.86">Quantitative authorship attribution: An evaluation of techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Grieve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.24,566.00,32.36,7.86;6,146.91,576.96,102.50,7.86">Literary and Linguistic Computing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,587.92,342.24,7.86;6,146.91,598.88,333.67,7.86;6,146.91,609.84,150.86,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,302.15,587.92,178.44,7.86;6,146.91,598.88,28.23,7.86">Blog credibility ranking by exploiting verified content</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Juffinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,193.34,598.88,287.25,7.86;6,146.91,609.84,41.32,7.86">WICOW &apos;09: Proceedings of the 3rd workshop on Information credibility on the web</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,620.79,342.24,7.86;6,146.91,631.75,298.59,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,270.94,620.79,209.65,7.86;6,146.91,631.75,83.42,7.86">Recognizing text genres with simple metrics using discriminant analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,249.73,631.75,93.84,7.86">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1071" to="1075" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
