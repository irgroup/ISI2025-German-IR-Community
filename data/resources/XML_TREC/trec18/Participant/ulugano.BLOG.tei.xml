<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.08,85.30,393.55,16.84">University of Lugano at TREC 2009 Blog Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,102.42,131.00,79.25,11.06"><forename type="first">Mostafa</forename><surname>Keikha</surname></persName>
							<email>mostafa.keikha@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.63,131.00,69.61,11.06"><forename type="first">Mark</forename><surname>Carman</surname></persName>
							<email>mark.carman@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.15,131.00,84.77,11.06"><forename type="first">Robert</forename><surname>Gwadera</surname></persName>
							<email>robert.gwadera@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.68,131.00,70.70,11.06"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
							<email>shima.gerani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,444.29,131.00,57.09,11.06"><forename type="first">Ilya</forename><surname>Markov</surname></persName>
							<email>ilya.markov@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,156.72,143.31,84.04,11.06"><forename type="first">Giacomo</forename><surname>Inches</surname></persName>
							<email>giacomo.inches@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.56,143.31,99.72,11.06"><forename type="first">Az</forename><forename type="middle">Azrinudin</forename><surname>Alidin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.87,143.31,77.14,11.06"><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
							<email>fabio.crestani@usi.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Lugano</orgName>
								<orgName type="institution">University of Lugano</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.08,85.30,393.55,16.84">University of Lugano at TREC 2009 Blog Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0CF2D4C0D8A087D404AD50F2D9BF8837</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on the University of Lugano's participation in the Blog track of TREC 2009. In particular we describe our system for performing blog distillation, faceted search and top stories identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recently, user generated data is growing rapidly and becoming one of the most important source of information in the web. This data has a lot of information to be processed like opinion, experience,etc which can be useful in many applications. Forums, mailing lists, on-line discussions, community question answering sites and social networks like facebook are some of these data resources that have attracted researchers lately.</p><p>Blogosphere (the collection of blogs on the web) is one of the main source of information in this category. Millions of people write about their experience and opinion in their blogs everyday, and this provides a huge amount of information to be processed. Due to the importance of this information, TREC (Text REtrieval Conference) has started a new track for blog analysis including opinion detection, polarity mining and blog distillation <ref type="bibr" coords="1,168.34,469.48,9.72,7.86" target="#b7">[7,</ref><ref type="bibr" coords="1,181.12,469.48,10.74,7.86" target="#b11">11]</ref>.</p><p>In the remainder of this paper we will explain our approach in faceted blog distillation in section 2. Our approach to top stories identification is explained in section 3. We provide conclusions in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BOG DISTILLATION</head><p>Blog distillation is the problem of retrieving relevant blogs (as a collection of posts) to a given query. The blog distillation task has been approached from many different points of view. In <ref type="bibr" coords="1,105.66,581.01,9.20,7.86" target="#b3">[3]</ref>, the authors view it as ad-hoc search and consider each blog as a long document created by concatenating all postings together. Other researchers treat it as the resource ranking problem in federated search <ref type="bibr" coords="1,255.20,612.39,9.20,7.86" target="#b4">[4]</ref>. They view the blog search problem as the task of ranking collec-. tions of blog posts rather than single documents. A similar approach has been used in <ref type="bibr" coords="1,429.03,238.90,13.49,7.86" target="#b12">[12]</ref>, where they again consider a blog as a collection of postings and use resource selection approaches. Their intuition is that finding relevant blogs is similar to finding relevant collections in a distributed search environment. In <ref type="bibr" coords="1,386.81,280.75,9.20,7.86" target="#b8">[8]</ref>, the authors modelled blog distillation as an expert search problem and use a voting model for tackling it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ordered Weighted Averaging Operators in Combining Scores</head><p>The ordered weighted averaging operator, commonly called OWA operator, was introduced by Yager <ref type="bibr" coords="1,512.06,354.68,13.49,7.86" target="#b13">[13]</ref>. OWA provides a parametrized class of mean type aggregation operators, that can generate OR operator(M ax), AN D operator(M in) and any other aggregation operator between them.</p><p>An OWA operator of dimension n is a mapping F : R n → R that has an associated weighting vector W ,</p><formula xml:id="formula_0" coords="1,316.81,431.12,168.48,56.14">W = [w1, w2, ..., wn] T such that n X i=1 wi = 1, 0 ≤ wi ≤ 1,</formula><p>and where</p><formula xml:id="formula_1" coords="1,389.75,503.79,162.25,26.84">F (a1, ..., an) = n X i=1 wibi (<label>1</label></formula><formula xml:id="formula_2" coords="1,552.00,512.95,3.92,7.86">)</formula><p>where bi is the ith largest element in the collection a1, ..., an.</p><p>There are different methods for indicating weighting vector W . We use a quantifier based method introduced by Yager <ref type="bibr" coords="1,316.81,567.43,13.49,7.86" target="#b13">[13]</ref>.</p><p>OWA operator has different behaviours based on the weighting vector associated with it. Yager introduced two measure for characterizing OWA operator <ref type="bibr" coords="1,481.26,598.82,15.67,7.86" target="#b13">[13]</ref>. The first one is called orness and is defined as:</p><formula xml:id="formula_3" coords="1,369.44,621.37,186.48,46.11">orness(W ) = 1 n -1 n X i=1 (n -i)wi (2) orness(W ) ∈ [0, 1]</formula><p>which characterizes the degree to which the operator behaves like an or operator. The second measure is dispersion and is defined as</p><formula xml:id="formula_4" coords="1,370.41,707.57,185.52,26.84">dispersion(W ) = - n X i=1 wi ln(wi)<label>(3)</label></formula><p>and measures the degree to which OWA operator takes into account all information in the aggregation.</p><p>For applying OWA operator to the problem, one important issue is determining weighting vector. Yager introduced a method based on linguistic quantifiers for obtaining this weights:</p><formula xml:id="formula_5" coords="2,95.83,140.70,197.07,19.74">wi = Q( i n ) -Q( i -1 n ), i = 1, 2, ..., n<label>(4)</label></formula><p>where n is the number of operands to be combined, and Q is the fuzzy linguistic quantifier. We use the following definition for the Q function as suggested by Zadeh <ref type="bibr" coords="2,257.21,189.87,16.57,7.86" target="#b16">[15]</ref>:</p><formula xml:id="formula_6" coords="2,103.63,208.66,189.27,37.28">Q(r) = 8 &gt; &lt; &gt; : 0, if r &lt; a r -a b -a , if a ≤ r ≤ b 1, if r &gt; b<label>(5)</label></formula><p>with a, b, r ∈ [0, 1]. We used parameter (a, b) with three different values, (0, 0.5), (0.3, 0.8) and (0.5, 1), as three quantifiers with different levels of orness. Table <ref type="table" coords="2,218.19,279.01,4.61,7.86" target="#tab_1">1</ref> shows orness and dispersion for each quantifier with values of 5, 10, 20 for n.</p><p>In this model, n is the number of top relevant posts in each blog that we want to aggregate their relevance score. These relevance scores are calculated by BM25 model in terrier for posts in each blog. Figure <ref type="figure" coords="2,164.91,331.31,4.61,7.86">1</ref> and Figure <ref type="figure" coords="2,220.89,331.31,4.61,7.86" target="#fig_0">2</ref> show Mean Average Precision(MAP) and Precision at 10 for experiments over TREC07 datasets. These results reveal that a fixed number of highly relevant posts in each blog is a reliable evidence, using which can result in an effective blog retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regularizing Relevance Scores</head><p>Score regularization is a way of re-calibrating relevance scores for documents based on the relationship between them. The idea behind score regularization is that in accordance with the Clustering Hypothesis, related documents should have similar scores for the same query. The authors of <ref type="bibr" coords="2,65.01,470.48,9.72,7.86" target="#b2">[2,</ref><ref type="bibr" coords="2,78.70,470.48,11.76,7.86" target="#b10">10]</ref> propose general models for smoothing document scores based on this hypothesis. In <ref type="bibr" coords="2,208.06,480.94,9.20,7.86" target="#b2">[2]</ref>, Diaz models the problem in terms of optimization. The goal is to calculate for each document a new (smoothed) score with two contending objectives: score consistency with related documents and score consistency with the initial retrieval score. Diaz defines a cost function ζ(f ) as follows:</p><formula xml:id="formula_7" coords="2,84.65,552.96,208.26,34.40">ζ(f ) = σ(f ) + µε(f ) = X i =j (wijfi -wjifj) 2 + µ X i (fi -yi) 2<label>(6)</label></formula><p>Here f is a vector of regularized scores over n documents, σ(f ) is a cost function associated with the inter-document consistency of the scores; if related documents have inconsistent scores, the value of this function will be high. A second cost function ε(f ) measures the consistency with the original scores; if document scores are inconsistent with the original scores, the value of this function will be high. A regularization parameter µ controls the trade off between inter-document smoothing and consistency with the original score vector y. The coefficient wij in the expansion of σ(f ) weights the score of the ith document by its similarity to the jth document and is calculated by normalizing (and taking the square root of) values from a symmetric affinity </p><formula xml:id="formula_8" coords="2,401.33,165.60,150.67,25.26">wij = s Wij P j Wij . (<label>7</label></formula><formula xml:id="formula_9" coords="2,552.00,174.75,3.92,7.86">)</formula><p>Here Wij denotes the similarity between documents i and j. In order to keep the affinity matrix sparse, only the k most similar documents j for each document i have nonzero Wij values <ref type="foot" coords="2,381.13,229.87,3.65,5.24" target="#foot_0">1</ref> . The diagonal values in the matrix Wii are defined to be zero. An iterative solution for the above defined optimization problem is the following:</p><formula xml:id="formula_10" coords="2,386.32,267.42,169.60,10.13">f t+1 = (1 -α)y + α W f t<label>(8)</label></formula><p>Where α = 1/(1 + µ) is a parameter, y = f 0 is the initial score vector, f t is the score vector after t iterations and W is a normalized affinity matrix such that Wij = wijwji. The closed form solution of this problem is given by:</p><formula xml:id="formula_11" coords="2,398.43,333.05,157.49,10.13">f * = (I -α W ) -1 y<label>(9)</label></formula><p>We used this equation in our experiments. We note that we did not introduce a new model here, but simply investigated the application of graph-based regularization frameworks <ref type="bibr" coords="2,343.35,383.83,9.72,7.86" target="#b2">[2,</ref><ref type="bibr" coords="2,356.60,383.83,11.77,7.86" target="#b10">10]</ref> to the problem of blog distillation, where the aim is not just to rank documents, but to rank blogs which are themselves composed of many documents (posts).</p><p>Based on this method we regularize relevance score, which could be the score of the posts or the score of the blog as a whole. In case of posts relevance score we have to aggregate regularized scores again, where we use simple averaging as the aggregation. And in case of regularizing blog score as a whole, we generate one document per blog which is concatenation of its most relevant posts. We use the similarity score of this large document as the blog relevance score and use it on regularization. Table <ref type="table" coords="2,425.62,498.89,4.61,7.86" target="#tab_0">2</ref> shows the results of posts relevance score regularization over Blog06 dataset with TREC07 and TREC08 query sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Faceted Search</head><p>For the faceted rankings, we first generated positive and negative facet scores for each retrieved document, denoted pos(d) and neg(d) respectively. These facet scores induce a ranking, denoted rpos(d, q) and rneg(d, q), which we combined with the original relevance ranking rrel(d, q) using the Borda Fuse aggregation method as follows:</p><formula xml:id="formula_12" coords="2,333.69,602.42,222.23,37.22">2 scoreBF(d, q) = α rrel(d, q) + (1 -α) rfacet(d, q)<label>(10)</label></formula><p>Without any training data (i.e. relevance judgments) we were unable to choose an appropriate value for the weighting coefficient α and thus set its value to 0.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">In-depth versus Shallow</head><p>For the in-depth versus shallow facet, we calculated the Cross Entropy (CE) between each retrieved document and the collection as a whole. We used CE as the positive score for the positive (in-depth) facet value since high CE indicates that the document contains many rare and informative words:</p><formula xml:id="formula_13" coords="3,67.87,472.32,225.03,23.66">pos(d) = CE(p(.|d), p(.|c)) = X t∈d p(t|d) log 1 p(t|c)<label>(11)</label></formula><p>Here p(t|d) is the probability of a term t appearing within the document d, which we calculate using the relative term frequency as follows: p(t|d) = tf(t, d)/ P t tf(t , d), where tf(t, d) is the absolute term frequency. Meanwhile p(t|c) denotes is the probability of a term across the whole collection c, for which we use a document frequency based estimate p(t|c) = df(t)/|c| where |c| is the number of documents in the collection. Our rational for using a df rather than tf based estimate is that the former appears less susceptible to noise from spam documents, which oftentimes include terms with very high frequency (high tf values).</p><p>For the negative (shallow ) facet score we simply use the negation of the CE, i.e. neg(d) = -pos(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Opinion versus Factual</head><p>For the opinion versus factual facet, we built lexicons of opinionated and objective words using the TREC Blog06 collection and corresponding relevance/opionion judgments. In the lexicon, terms were weighted according to a document-frequency based version of the Mutual Information (MI) metric <ref type="bibr" coords="3,138.08,713.68,9.20,7.86" target="#b9">[9]</ref>. We then calculated (positive and negative) facet scores for each retrieved document by aver-aging over the lexicon weights for each word in the document (see equation 14 below.)</p><p>In order to calculate both positive (opinionated ) and negative (factual ) facet weights for terms we split the Mutual Information metric into two values as follows. Let T denote the event that a document contains the particular term t, and T the event that the document doesn't contain the term. Then let O denote the event that a document is classed as being (relevant and) opinionated about the query and Ō that it is (relevant but) not opinionated about the query. We calculate the positive facet score for a term by calculating the MI summation only over the two positively correlated quadrants (i.e. T ∩ O and T ∩ Ō) as follows:</p><formula xml:id="formula_14" coords="3,321.86,531.76,234.05,32.28">pos(t) = p(T , O) log p(T , O) p(T ), p(O) + p( T , Ō) log p( T , Ō) p( T ), p( Ō)<label>(12)</label></formula><p>The negative facet score is calculated analogously as follows:</p><formula xml:id="formula_15" coords="3,321.27,580.57,234.64,32.28">neg(t) = p(T , Ō) log p(T , Ō) p(T ), p( Ō) + p( T , O) log p( T , O) p( T ), p(O)<label>(13)</label></formula><p>We calculate the required joint and marginal probabilities using document frequency estimates using the sets of opinionated O and relevant R documents in the TREC Blog06 collection as:</p><formula xml:id="formula_16" coords="3,390.80,664.56,91.14,28.78">p(T , O) = df(t, O)/|R| p(T ) = df(t, R)/|R| p(O) = |O|/|R|</formula><p>Where df(t, O) is the number of opinionated documents containing the term t. The other joint and marginal probabilities required for equations 12 and 13 are estimated analo-gously.</p><p>Having calculated positive and negative weights for each term, we then averaged these lexicon weights over each document to calculate positive and negative facet scores for the document as follows:</p><formula xml:id="formula_17" coords="4,97.76,136.56,195.15,20.63">pos(d) = E d [pos(t)] = X t∈d p(t|d)pos(t)<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Personal versus Official</head><p>Finally for the personal versus official facet, the same scores were used as in the opinion case, since we believe that more "personal content" is on the whole more likely to contain opinions than more "official content".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TOP STORIES IDENTIFICATION</head><p>Our method for the top stories task proceeded as follows. We first extracted time-stamped news stories for each query date while filtering out non-news related items. For each query date we also extracted the set of blog posts that were posted on the same or following days and where the post had some vocabulary overlap with corresponding set of news stories. Each set of blog posts was then clustered using an incremental clustering algorithm. Next we ranked clusters with respect to size and time-span in order to identify the most important clusters pertaining to the corresponding news stories. Finally we identified the most authoritative document for the 10 most important clusters on each query date.</p><p>In the following sections we outline our approach in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The algorithm</head><p>In this section we present details of our algorithm. Our method for top stories task proceeds as follows.</p><p>1. for every query date we extract a set of time-stamped news stories by using the date part of the permanent links of the urls and we filter out non-news related documents 2. for every query date we extract a set of time-stamped blog posts that satisfy the following condtions: (I) they were posted on the same day and a following three days and (II) they have a vocabulary overlap with the corresponding time-stamped sets of news stories.</p><p>3. we cluster every time-stamped blog-post set using an incremental clustering algorithm whose details are presented in Section 3.2.</p><p>4. we identify the most important clusters pertaining to the corresponding time-stamped news stories by ranking clusters with respect to size and time-span.</p><p>5. we filter out clusters that correspond to the newsstories by using the centroid score.</p><p>6. we identify the most authoritative document for the top-10 most important clusters for every query date using the ranking algorithm presented in Section 3.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incremental document clustering in sliding time-window</head><p>In this section we present our implementation of an incremental variant of a non-hierarchical document clustering algorithm using a similarity measure based on nearest neighbors (NN-based) <ref type="bibr" coords="4,386.72,236.15,9.71,7.86">[5,</ref><ref type="bibr" coords="4,399.50,236.15,7.16,7.86" target="#b6">6,</ref><ref type="bibr" coords="4,409.72,236.15,6.48,7.86" target="#b1">1]</ref>.</p><p>We preprocess every document using the following steps: (I) HTML parsing; (II) tokenization; (III) stemming; and (IV) stopwords removal. We represent a document d using the term vector model where d = [w1, w2, . . . , w d ] and wi is the weight of the the i-th term (word) that was extracted from the document after the preprocessing of the original document. The reason we use a NN-based similarity measure between news stories is because direct similarity measures between two vectors like Euclidean distance and the dot product have the following problems in a high dimensional space: there is an experimental evidence that they are not reliable <ref type="bibr" coords="4,379.31,361.68,9.71,7.86">[5]</ref> and the triangle inequality does not hold. For an example where the triangle inequality does not hold consider the following three vectors representing hypothetical documents: d1, d2 and d3 over six terms in Table <ref type="table" coords="4,548.76,393.06,3.58,7.86" target="#tab_2">3</ref>. Thus, although d1 is close to d2 by sharing one term and d2 is close to d3 by also sharing one term d1 and d3 do not share any terms. There are the following reasons why the triangle inequality does not hold for documents: (I) diversity of term usage to express the same meaning with respect to the same event, which is aggravated by the fact that we consider similarity between documents across different news sources; and (II) content of stories reporting the same event may change throughout time and may use a different vocabulary. Therefore the clusters containing the documents are inherently non globular justifying the use of the NN-based versus a centroid-based similarity measure.</p><p>We perform the standard TF-IDF weighting of the document term vector d: wi = tfi • idfi, where: tfi is within document term frequency of term ti and idfi = log(N/dfi) is the inverse document frequency, where N is the total number of documents in the collection and dfi is the document frequency of term ti defined as the number of documents containing the term in the collection.</p><p>In order to present the clustering algorithm we introduce the following notation. Let sim(di, dj) =  i) be the document frequency vector for stream i. Let currentT ime be the timestamp of the most recent document in the window, i.e, the current timestamp of the window. The clustering algorithm proceeds as follows:</p><formula xml:id="formula_18" coords="4,494.64,609.55,45.48,18.87">P n k=1 d i k •d j k d i • d j</formula><p>1. Neighborhood search: given a new document di identify its neighborhood Nτ d (di)</p><p>2. Identification of a cluster that can accept a new document: For every cluster</p><formula xml:id="formula_19" coords="5,76.21,169.96,216.70,42.04">C ∈ C(Nτ d (di)) com- pute ∆(di, Nτ d (C, di)). Select a cluster Cmax = max C∈C(N T (C,d i )) ∆(di, NT (C, di)).</formula><p>If Nτ d (di) is empty then create a new cluster Cnew for di.</p><p>3. Merging: merge every set</p><formula xml:id="formula_20" coords="5,76.21,248.90,216.19,18.32">C ∈ C(Nτ d (di)) \ Cmax with Cmax.</formula><p>For achieving an efficient neighborhood search in the window we dynamically maintain an inverted index data structure in the time-window. Also we maintain an independent document frequency vector df (i) for each stream i in order to suppress terms whose popularity is specific to a particular news source.</p><p>The sliding window process proceeds as follows. When a new document d (i) t arrives the following actions are executed: (I) the document is added to the window, which involves adding the corresponding terms to: the inverted index and df (i) vector; (II) d (i) t is clustered using the presented algorithm and if the result is a singleton cluster then it is added to the set of active clusters C; (III) currentT ime is set to d (i) t .timestamp; (IV) documents which are older than currentT ime -w are removed from the window, which involves removing corresponding entries in: the set of active clusters C and the inverted index Thus the presented clustering algorithm has the following parameters: (I) the time-window size w = 24 hours; (II) the document similarity threshold τ d = 0.5. Our evaluation of the clustering results suggest that P recision = 95%. We selected τ d = 0.5 based on an experimental evaluation that showed τ d = 0.5 to be a good compromise with respect to precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Content-aware ranking function</head><p>In this section we present a content-aware ranking function that ranks with respect to the following factors:</p><p>1. the importance of a cluster increases with its size and decreases with its time-span (the time distance between the first and the last document)</p><p>2. the importance of a document in a given position (its authority) in a time-ordered cluster is proportional to the difference between the average combined similarity (content similarity and temporal distance) for the following documents and the previous documents in the cluster.</p><p>The first factor is an extension of the first factor for the probabilistic ranking function by prioritizing clusters that are proximate in time. This corresponds to the fact that a large cluster on a given event that is proximate in time means that the event is very important since every source reports it in a very short time window.</p><p>The second factor has the following motivation. It is known that news stories discussing the same event tend to be temporally proximate across the news streams <ref type="bibr" coords="5,507.68,112.43,13.49,7.86" target="#b15">[14]</ref>. Therefore we use a combine similarity measure that increases with the content similarity and decreases with the temporal distance. Let ∆t(i, j) be the temporal distance between documents di and dj, where ∆t(i, j) = e -α(d i .time-d j .time) and α = -ln(dF actor) w , where dF actor is the decaying factor that denotes the factor by which the value of the function decays within the time interval w being the time window size. Then the combined similarity w(di, dj), can be expressed as follows</p><formula xml:id="formula_21" coords="5,376.26,225.65,179.66,7.86">w(di, dj) = sim(i, j) • ∆t(i, j),<label>(15)</label></formula><p>where sim(i, j) is the content similarity. Figure <ref type="figure" coords="5,508.81,242.09,4.61,7.86">3</ref> presents a graphical representation of the dependencies between documents in a cluster with respect to the combined similarity, where a directed edge from an earlier to a more recent document has a weight equal to the combine similarity. Out(s, d1.steam, 1) = w(1,2)+w(1,3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>In(s, d1.steam, 1) = w(0, 1) P rev(s, d1.steam, 1) = {d0} w(di, dj) = sim(i, j)∆t(i, j) w(d2, d3)</p><p>Figure <ref type="figure" coords="5,355.01,445.40,4.12,7.89">3</ref>: Combined similarity between documents in itemset-sequence (cluster) s = d2, d3, d4]. P rev(s, d1.stream, 1) and F ollow(s, d1.stream, 1) are sets of documents preceding and following document d1 in position 1.</p><p>In(s, d1.stream, 1) and Out(s, d2.stream, 1) are the average combined similarity for P rev(s, d1.stream, 1) and F ollow(s, d1.stream, 1) respectively. authority(s, d1.stream, 1) is the authority of source d1.stream in position 1. Now we define the average combined similarity with respect to previous and following documents. Given an itemset-sequence s ∈ S we define the following two sets. Let P rev(s, i, j) = S l=1,l≤j d l be the set of documents that precede document d (i) .stream = i in position j in s. Let F ollow(s, i, j) = S l=j+1,l≤|s| d l be the set of documents that follow document d (i) .stream = i in position j in s. Then the average combined similarity with respect to previous documents (in positions j &lt; j), denoted In(s, i, j), can be expressed as follows</p><formula xml:id="formula_22" coords="5,334.41,678.91,221.51,23.99">In(s, i, j) = 1 |P rev(s, i, j)| X d∈P rev(s,i,j) w(d, dj).<label>(16)</label></formula><p>Also the average combined similarity with respect to the following documents (in positions j &lt; j ), denoted Out(s, i, j), can be expressed as follows</p><formula xml:id="formula_23" coords="6,61.33,86.57,231.59,23.99">Out(s, i, j) = 1 |F ollow(s, i, j)| X d∈F ollow(s,i,j) w(d, dj). (17)</formula><p>Given the value of In(s, i, j) and Out(s, i, j) we define "authority" of source i corresponding to a document in position j as follows authority(s, i, j) = Out(s, i, j) -In(s, i, j).</p><p>(</p><formula xml:id="formula_24" coords="6,280.64,159.80,12.27,7.86">)<label>18</label></formula><p>This measure of authoritativeness prioritizes sources that: (I) "borrow" little content form previous documents (In(s, i, j)) and whose content is widely "borrowed" by following documents in the cluster (Out(s, i, j)) and (II) produce a timely content (|F ollow(s, i, j)| is the biggest equal to |s| -1 and |P rev(s, i, j)| is the smallest equal to 0 for the first story in the cluster (j = 0)). This measure of authoritativeness has many desired properties. For example consider a case where there is source i2, which always follows an authoritative source i1 with very similar content. Then authority(s, i2, j) will be very small (even negative) for i2 since it only "repeats" the content of i1. Thus, this case may correspond to a reuse of content by i2 from i1, where i2 repeats content from i1 within a short time window. In other words (18) discriminates between "producers" of the content (positive value of (18)) and "repeaters" (negative value of (18)). However, note that because of limitations of the cosine similarity measure we are unable to decide with hundred percent confidence that one story is reusing content from another one. We now define the rank of a cluster s as follows</p><formula xml:id="formula_25" coords="6,75.76,404.86,217.15,8.35">rankCluster(s) = w cluster (k) • ∆t(0, |s| -1)<label>(19)</label></formula><p>where w cluster (k) is the weight of the cluster of size k (size of the cluster) and ∆t(0, |s| -1) is the time-span of the cluster. Despite the sophisticated clustering machinery used in the top stories identification, our results were poor due to the fact that we were only able to run the clustering over a small subset (around 10%) of the data. It was mainly because of the time restriction and the computational load required by the algorithm on the very large dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>We have described our participation in TREC 2009 Blog track for faceted blog distillation and top stories. We implemented two types of algorithms for blog distillation. In one of our experiments, we used fuzzy aggregation methods for combining post relevance scores in each blog to calculate blog scores as a whole. In another part of the experiments, we used regularization methods for smoothing relevance scores based on the similarity between the retrieved blogs. We carried out regularization on two types of scores: posts relevance scores and large document relevance scores (where each blog is represented by the concatenation of its most relevant posts). Finally we combined the two methods (regularization and OWA) to take into account the similarity between retrieved posts while performing good aggregation over them, to generate new scores for each blog.</p><p>For the faceted rankings, we first generated positive and negative facet scores for each retrieved document and then combined the facet rankings with the relevance ranking using Borda Fuse.</p><p>For top stories task we first extracted time-stamped news stories for each query date while filtering out non-news related items. For each query date we also extracted the set of blog posts that were posted on the same or following days and where the post had some vocabulary overlap with corresponding set of news stories. Each set of blog posts was then clustered using an incremental clustering algorithm. Next we ranked clusters with respect to size and time-span in order to identify the most important clusters pertaining to the corresponding news stories. Finally we identified the most authoritative document for the 10 most important clusters on each query date.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,363.01,353.92,115.89,7.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision at 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,546.46,615.48,9.46,7.86;4,316.81,629.99,239.11,7.86;4,316.81,640.45,219.32,7.86;4,536.13,645.24,3.68,4.98;4,540.81,640.45,15.10,7.86;4,316.81,650.91,239.10,7.86;4,316.81,661.37,173.87,8.35;4,490.68,666.16,3.68,4.98;4,495.36,661.37,60.56,7.86;4,316.81,671.83,239.11,7.86;4,316.81,682.29,36.90,7.86;4,353.72,687.08,3.68,4.98;4,358.40,682.29,197.52,7.86;4,316.81,692.75,70.28,7.86;4,387.09,697.54,3.68,4.98;4,391.77,692.75,54.17,7.86;4,445.94,697.54,3.68,4.98;4,450.62,692.75,105.30,7.86;4,316.81,703.22,67.33,7.86;4,384.14,708.00,3.68,4.98;4,388.82,703.22,167.10,7.86;4,316.81,713.68,11.03,7.86;4,327.84,718.46,3.68,4.98;4,332.52,713.68,201.05,7.86;4,533.58,718.46,3.68,4.98;4,538.26,713.68,17.66,7.86;4,316.81,723.16,75.46,8.84;4,392.27,728.92,14.10,5.24;4,408.82,724.14,147.10,7.86;5,53.80,70.59,178.65,7.86;5,233.43,68.82,2.81,5.24"><head>be the cosine similarity</head><label></label><figDesc>or content similarity between documents di and dj, where sim(di, dj) ∈ [0, 1]. Let Nτ d (di) be the neighborhood of di defined as a set of documents for which sim(di, d) ≥ τ d , where d ∈ Nτ d (di). Let C = {C1, C2, . . . , Cn} be the set of active clusters in the window. Let C(Nτ d (di)) ⊆ C be the set of clusters that contain any documents in Nτ d (di). Let Nτ d (Cj, di) be the subsets of documents in Nτ d (di) belonging to cluster Cj ∈ C such that Nτ d (Cj, di) = ∅ if cluster Cj has no members in Nτ d (di). Let ∆(di, C) = P d∈C sim(di, d) be the similarity between di and the set of documents d ∈ C. Let df (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,325.12,348.07,7.59,6.97;5,461.20,348.07,7.59,6.97;5,526.36,348.07,7.59,6.97;5,393.16,348.07,7.59,6.97;5,405.64,307.27,31.91,6.97;5,368.20,360.07,31.91,6.97;5,439.60,325.99,32.03,6.97;5,347.80,341.35,31.91,6.97;5,414.04,341.35,32.03,6.97;5,429.40,390.67,87.24,6.97;5,518.93,390.45,14.36,12.09;5,534.76,390.67,12.12,6.97;5,323.93,422.95,100.80,6.97;5,428.20,420.85,44.10,4.98;5,448.60,425.78,3.39,6.05;5,475.48,422.73,6.23,12.09;5,483.52,422.95,23.87,6.97"><head>F 2 -</head><label>2</label><figDesc>ollow(s, d1.steam, 1) = {d2, d3} authority(s, d1.stream, 1) = w(1,2)+w(1,3) w(0, 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,316.81,76.75,239.10,82.54"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="2,316.81,76.75,239.10,82.54"><row><cell cols="3">: Regularization Results for TREC07 and</cell></row><row><cell>TREC08 query sets.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>MAP</cell><cell>P@10 nDCG Bpref</cell></row><row><cell cols="3">TREC07 query sets 0.3126 0.4956 0.5483 0.3118</cell></row><row><cell cols="3">TREC08 query sets 0.2375 0.3480 0.6990 0.2196</cell></row><row><cell>matrix W as follows:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,54.32,76.75,473.28,285.07"><head>Table 1 :</head><label>1</label><figDesc>Orness and dispersion for experimented quantifiers in OWA operator</figDesc><table coords="3,185.85,86.83,234.95,49.71"><row><cell></cell><cell cols="3">orness dispersion linguistic quantifier n=10 n=10</cell></row><row><cell>a=0.0 , b=0.5</cell><cell>0.77</cell><cell>1.609</cell><cell>At least half</cell></row><row><cell>a=0.3 , b=0.8</cell><cell>0.44</cell><cell>1.609</cell><cell>Most</cell></row><row><cell>a=0.5 , b=1.0</cell><cell>0.22</cell><cell>1.609</cell><cell>As many as possible</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,316.81,76.75,239.12,60.74"><head>Table 3 :</head><label>3</label><figDesc>Example documents for which the triangle inequality does not hold</figDesc><table coords="4,367.28,106.11,137.66,31.38"><row><cell>doc</cell><cell>w1</cell><cell>w2</cell><cell>w3</cell><cell>w4</cell><cell>w5</cell><cell>w6</cell></row><row><cell>d1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>d2</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>d3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,321.42,686.91,234.50,7.86;2,316.81,695.87,215.76,7.86"><p>Some documents may need to have more than k non-zero affinity values in order to keep the matrix symmetric.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,321.42,706.20,234.50,7.86;2,316.81,715.17,239.11,7.86;2,316.81,724.14,239.11,7.86"><p>Note that whenever there are ties in the ranking, (i.e. documents d1 and d2 have the same score), then the rank for those documents is the average of the (total order) ranking.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,321.30,206.82,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,220.53,210.55,7.86;6,335.61,230.99,213.83,7.86;6,335.61,241.45,213.89,7.86;6,335.61,251.91,61.33,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,444.25,220.53,101.91,7.86;6,335.61,230.99,162.71,7.86">Dynamic pattern mining: An incremental data clustering approach</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,505.72,230.99,43.72,7.86;6,335.61,241.45,61.83,7.86">Journal on Data Semantics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="85" to="112" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,263.37,192.57,7.86;6,335.61,273.83,216.99,7.86;6,335.61,284.29,203.84,7.86;6,335.61,294.75,58.77,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,371.14,263.37,141.79,7.86">Regularizing ad hoc retrieval scores</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.61,273.83,216.99,7.86;6,335.61,284.29,173.91,7.86">Proceedings of the 14th ACM international conference on Information and knowledge management</title>
		<meeting>the 14th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="672" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,306.20,208.00,7.86;6,335.61,316.67,213.50,7.86;6,335.61,327.13,153.74,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,491.87,306.20,51.73,7.86;6,335.61,316.67,173.82,7.86">University of Texas School of Information at TREC 2007</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Turnbull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ovalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,528.17,316.67,20.94,7.86;6,335.61,327.13,125.35,7.86">Proc. of the 2007 Text Retrieval Conf</title>
		<meeting>of the 2007 Text Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,338.58,178.13,7.86;6,335.61,349.04,220.32,7.86;6,335.61,359.50,157.59,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,380.32,349.04,175.60,7.86;6,335.61,359.50,23.52,7.86">Retrieval and feedback models for blog feed search</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,377.60,359.50,23.87,7.86">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,370.96,192.92,7.86;6,335.61,381.42,198.86,7.86;6,335.61,391.88,199.68,7.86;6,335.61,402.34,193.66,7.86;6,335.61,412.80,55.04,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,497.47,370.96,31.06,7.86;6,335.61,381.42,198.86,7.86;6,335.61,391.88,113.09,7.86">Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ertöz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,478.62,391.88,56.67,7.86;6,335.61,402.34,193.66,7.86;6,335.61,412.80,26.59,7.86">Proceedings of Second SIAM International Conference on Data Mining</title>
		<meeting>Second SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,424.26,202.20,7.86;6,335.61,434.72,205.93,7.86;6,335.61,445.18,214.40,7.86;6,335.61,455.64,20.96,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,464.56,424.26,73.25,7.86;6,335.61,434.72,201.84,7.86">Clustering using a similarity measure based on shared near neighbors</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Patrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,335.61,445.18,134.68,7.86">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1025" to="1034" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,467.10,214.06,7.86;6,335.61,477.56,185.21,7.86;6,335.61,488.02,207.57,7.86;6,335.61,498.48,20.96,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,501.54,467.10,48.13,7.86;6,335.61,477.56,94.88,7.86">Overview of the trec-2007 blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.88,477.56,71.93,7.86;6,335.61,488.02,203.39,7.86">Proceedings of the Sixteenth Text REtrieval Conference (TREC 2007)</title>
		<meeting>the Sixteenth Text REtrieval Conference (TREC 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,509.94,214.06,7.86;6,335.61,520.40,185.21,7.86;6,335.61,530.86,207.57,7.86;6,335.61,541.32,20.96,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,501.54,509.94,48.13,7.86;6,335.61,520.40,94.88,7.86">Overview of the trec-2007 blog track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,448.88,520.40,71.93,7.86;6,335.61,530.86,203.39,7.86">Proceedings of the Sixteenth Text REtrieval Conference (TREC 2007)</title>
		<meeting>the Sixteenth Text REtrieval Conference (TREC 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,552.78,184.95,7.86;6,335.61,563.24,201.16,7.86;6,335.61,573.70,69.93,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schtze</surname></persName>
		</author>
		<title level="m" coord="6,460.74,552.78,59.82,7.86;6,335.61,563.24,156.74,7.86">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,585.16,169.15,7.86;6,335.61,595.62,193.14,7.86;6,335.61,606.08,171.41,7.86;6,335.61,616.54,201.58,7.86;6,335.61,627.00,203.92,7.86;6,335.61,637.46,200.40,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,466.12,585.16,38.64,7.86;6,335.61,595.62,193.14,7.86;6,335.61,606.08,108.58,7.86">A general optimization framework for smoothing language models on graph structures</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,462.61,606.08,44.41,7.86;6,335.61,616.54,201.58,7.86;6,335.61,627.00,203.92,7.86;6,335.61,637.46,82.54,7.86">SIGIR &apos;08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,648.92,217.23,7.86;6,335.61,659.38,220.20,7.86;6,335.61,669.84,166.36,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,383.42,659.38,157.05,7.86">Overview of the TREC-2006 blog track</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,335.61,669.84,82.88,7.86">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="15" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,681.30,197.76,7.86;6,335.61,691.76,212.90,7.86;6,335.61,702.22,45.85,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,422.68,681.30,110.68,7.86;6,335.61,691.76,65.21,7.86">UMass at TREC 2007 Blog Distillation Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,419.68,691.76,128.82,7.86;6,335.61,702.22,17.46,7.86">Proc. of the 2007 Text Retrieval Conf</title>
		<meeting>of the 2007 Text Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,335.60,713.68,178.60,7.86;6,335.61,724.14,220.31,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,389.07,713.68,125.13,7.86;6,335.61,724.14,215.75,7.86">On ordered weighted averaging aggregation operators in multicriteria decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Yager</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,70.59,217.36,7.86" xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j" coord="7,72.59,70.59,127.83,7.86">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,82.05,203.03,7.86;7,72.59,92.51,215.77,7.86;7,72.59,102.97,212.87,7.86;7,72.59,113.43,110.67,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,191.47,92.51,96.89,7.86;7,72.59,102.97,139.65,7.86">Learning approaches for detecting and tracking news events</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,219.22,102.97,66.23,7.86;7,72.59,113.43,30.21,7.86">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.59,124.88,184.19,7.86;7,72.59,135.35,211.06,7.86;7,72.59,145.81,217.60,7.86;7,72.59,156.27,65.93,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="7,114.14,124.88,142.64,7.86;7,72.59,135.35,211.06,7.86;7,72.59,145.81,213.75,7.86">A computational approach to fuzzy quantifiers in natural languages. International series in modern applied mathematics and computer science</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="149" to="184" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
