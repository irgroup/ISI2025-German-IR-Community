<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.94,76.00,446.15,14.36;1,115.22,94.36,381.25,14.36">Entity Retrieval by Hierarchical Relevance Model, Exploiting the Structure of Tables and Learning Homepage Classifiers</title>
				<funder ref="#_AJQmxFt">
					<orgName type="full">National Nature Science Foundation</orgName>
				</funder>
				<funder ref="#_jCmz58A">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_xHdQWek">
					<orgName type="full">China and The Key Project of Yunnan Nature Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Indiana Economic Development Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">Purdue University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,147.74,133.86,30.39,8.96"><forename type="first">Yi</forename><surname>Fang</surname></persName>
							<email>fangy@cs.purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.14,133.86,26.87,8.96"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.27,133.86,49.92,8.96"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Engineering and Automation Kunming</orgName>
								<orgName type="institution">University of Science and Technology Kunming</orgName>
								<address>
									<postCode>650051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.06,133.86,55.95,8.96"><forename type="first">Yantuan</forename><surname>Xian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Engineering and Automation Kunming</orgName>
								<orgName type="institution">University of Science and Technology Kunming</orgName>
								<address>
									<postCode>650051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,467.87,133.86,43.93,8.96"><forename type="first">Yangbo</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Engineering and Automation Kunming</orgName>
								<orgName type="institution">University of Science and Technology Kunming</orgName>
								<address>
									<postCode>650051</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.94,76.00,446.15,14.36;1,115.22,94.36,381.25,14.36">Entity Retrieval by Hierarchical Relevance Model, Exploiting the Structure of Tables and Learning Homepage Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D7C234A220D3C1C6B7FD25A74A1475F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper gives an overview of our work done for the TREC 2009 Entity track. We propose a hierarchical relevance retrieval model for entity ranking. In this model, three levels of relevance are examined which are document, passage and entity, respectively. The final ranking score is a linear combination of the relevance scores from the three levels. Furthermore, we exploit the structure of tables and lists to identify the target entities from them by making a joint decision on all the entities with the same attribute. To find entity homepages, we train logistic regression models for each type of entities. A set of templates and filtering rules are also used to identify target entities. The key lessons that we learned by participating this year's Entity track include: 1) our special treatment of table and list data is well rewarding; 2) The high accuracy of homepage finding is crucial in this track; 3) Wikipedia can serve as a valuable knowledge resource for different aspects of the related entity finding task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>As the Web has evolved into a data-rich repository, both commercial systems and the information retrieval community have shown increasing interest in not just returning documents, but specific entities in response to a user's query. The task of the Entity track is to investigate the problem of related entity finding. Specifically, given the name and homepage of an entity, as well as a context described in natural language text, the retrieval system needs to find target entities with homepages that are of the specified type. In this year's task, entity types are limited to people, organizations, and products.</p><p>In fact, related entity finding incorporates both expert finding and homepage finding tasks from previous TREC tracks. In this paper, we propose a hierarchical relevance retrieval model for entity ranking. In particular, relevance is examined at three levels such as document, passage and entity. The final ranking score is a linear combination of the relevance scores from the three levels. Moreover, as many entities are stored in a structural or semi-structure form such as tables and lists, we exploit the structure of these data to reduce the uncertainty in entity recognition and finding. For entity homepage finding, in addition to utilize Wikipedia as a direct reference, we treat it as a classification problem by training logistic regression models for the three target entity types respectively. Because it is very time consuming to manually identify the positive training instances from a huge pool of candidate web pages, we use an incremental learning strategy to train the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">APPROACHES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SYSTEM ARCHITECTURE</head><p>The architecture of our participant system can be seen in Figure <ref type="figure" coords="1,350.38,634.82,3.76,8.96" target="#fig_0">1</ref>. Similar to many question answering (QA) systems, our system also includes three major components: input (question) analysis, document/passage retrieval, and entity (answer) extraction. These three components have been widely adopted to build QA systems. However, there are several key operations in our system that were rarely performed by QA. The first one is the table/list processing component which exploits the structure of tables and lists to make joint decisions on all the elements of the tables and lists. The second key component is to extract the constraints on target entities from the query topics and to utilize the constraints to better locate the related entities. The entity ranking component is the hierarchical relevance model that we proposed to consider the relevance at different levels. Another component unique for this entity track is the homepage detection algorithm that identifies the homepages for the extracted entities. The following sections explain the details of the individual components of our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">HIERACHICAL RELEVANCE RETRIEVAL MODEL</head><p>We state the problem of identifying candidates who are related entities as follows: what is the probability of a candidate ğ‘’ being the target entity given a query ğ‘ and target type ğ‘¡? That is, we determine ğ‘(ğ‘’|ğ‘, ğ‘¡), and rank candidate ğ‘’ according to this probability. The top k candidates are deemed the most probable entities. By applying Bayes' Theorem and the chain rule, we can decompose ğ‘(ğ‘’|ğ‘, ğ‘¡) into the following form ğ‘(ğ‘’|ğ‘, ğ‘¡) âˆ ğ‘ ğ‘ ğ‘‘ ğ‘  ğ‘‘ ğ‘(ğ‘|ğ‘ , ğ‘‘)ğ‘(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘)</p><p>where ğ‘  denotes a supporting passage in a supporting document ğ‘‘. The first quantity ğ‘ ğ‘ ğ‘‘ on the right hand side is the probability that the query is generated by the supporting document, which reflects the association between the query and the document. Similarly, the first quantity ğ‘(ğ‘|ğ‘ , ğ‘‘) reflects the association between the query and the supporting passage. The last quantity ğ‘(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘) is the probability that a candidate entity ğ‘’ is the related entity given passage ğ‘ , type t and query ğ‘. In summary, this probabilistic retrieval model considers the relevance at three different levels: document, passage and entity. However, accurately estimating these probabilities is difficult for generative probabilistic language modeling techniques. Instead, motivated by the idea of the hierarchical relevance model, we use a linear combination of relevance scores from these three levels to yield the final ranking score ğ‘“ ğ‘’ ğ‘, ğ‘¡ as follows:</p><p>ğ‘“ ğ‘’ ğ‘, ğ‘¡ = ğ›¼ğ‘“ ğ‘ ğ‘‘ ğ‘  ğ‘‘ + ğ›½ğ‘“ ğ‘ ğ‘ , ğ‘‘ + ğ›¾ğ‘“(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘)</p><p>where ğ›¼, ğ›½ and ğ›¾ are the combination coefficients for the retrieval scores ğ‘“ ğ‘ ğ‘‘ , ğ‘“ ğ‘ ğ‘ , ğ‘‘ and ğ‘“(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘). In the subsequent sections, we show how to calculate these individual retrieval scores. All the retrieval scores are finally normalized by the maximum score in their level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">DOCUMENT RETRIEVAL</head><p>The main role of the document retrieval stage is to retrieve a hopefully very small subset of the entire collection which will be processed in detail by the entity extraction and other components. The formulation of query from a natural language narrative should maximize the performance of document retrieval. Many entities exist in the documents or queries in the form of acronym such as IU for Indiana University. We expand the original narrative query to include the acronym or full name of the source entity which is likely to cause more retrieved documents containing related entities. Without external resources, to find the acronym of an entity can sometimes be difficult such as LVMH for MoÃ«t Hennessy-Louis Vuitton. In our runs, we resort to Farlex Free Dictionary 1 to find acronyms. To retrieve documents from the data collection, we experimented to use the INDRI toolbox<ref type="foot" coords="3,487.66,72.01,3.24,5.83" target="#foot_1">2</ref> and Google respectively. In one run which is not submitted, we use the following INDRI query to retrieve the top 100 documents for each topic: #weight(3.0 @odN(source entity) 3.0 @odN(keyword) 2.0 @odN(phrase) 1.0 (each term) 1.5 (acronym or full name of source entity) 1.0 (synonym and antonym of keyword) )</p><p>where N is the number of words in the phrase. The keyword is the term in the narrative that reflects the main property of the target entity.</p><p>We found in our experiments the INDRI document retrieval was generally not effective in locating supporting documents. In our submitted runs, we obtain the top 1000 results from Google by the query similar to the above INDRI query except no weights associated with terms. Then we remove those pages that are not in the Clueweb09 test collection and use the top 5 pages in the remaining pages as the candidate documents to extract related entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">COMPUTING ğ‘“ ğ‘ ğ‘‘</head><p>ğ‘“ ğ‘ ğ‘‘ reflects the similarity between the query topic/narrative and the supporting document. Given ğ‘ and ğ‘‘, it can be computed by retrieval models such as BM25 and language modeling. Because the title of a web page is usually a good and concise summary of the whole document, ğ‘‘ in our runs is represented by the TITLE element of the web page instead of using the full text. This strategy seems especially effective for this year's topics, as the titles of many supporting documents exhibit strong correlations with the query narratives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">COMPUTING ğ‘“ ğ‘ ğ‘ , ğ‘‘</head><p>ğ‘“ ğ‘ ğ‘ , ğ‘‘ reflects the similarity between the query and the supporting passage. Passages are sentences segmented by a set of predefined rules. For tables, the elements of the same attributes belong to a passage along with the attribute name, the header and preceding sentence of the table. In our runs, ğ‘“ ğ‘ ğ‘ , ğ‘‘ is computed by summing over all the similarity scores ğ‘… between the terms ğ‘¤ ğ‘ in the query and the terms ğ‘¤ ğ‘  in the passage as follows:</p><p>ğ‘“ ğ‘ ğ‘ , ğ‘‘ = ğ‘…(ğ‘¤ ğ‘ , ğ‘¤ ğ‘  )</p><formula xml:id="formula_0" coords="3,289.49,408.75,41.39,9.25">ğ‘¤ ğ‘  âˆˆğ‘  ğ‘¤ ğ‘ âˆˆğ‘</formula><p>where the similarity score ğ‘… is computed based on the distance defined by WordNet<ref type="foot" coords="3,408.43,418.14,3.24,5.83" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">COMPUTING ğ‘“(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘)</head><p>ğ‘“(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘) reflects the confidence that the entity ğ‘’ is the target entity given the corresponding evidence, which is the relevance score at the entity level. First of all, we use Stanford Named Entity Recognizer<ref type="foot" coords="3,467.98,476.12,3.24,5.83" target="#foot_3">4</ref> (NER) and LBJ Named Entity Tagger<ref type="foot" coords="3,161.18,487.64,3.24,5.83" target="#foot_4">5</ref> to extract entities of the target type from the passage. These NERs can directly recognize persons and organizations, but not products. We train a CRF model for named product recognition by labeling 4000 documents about product entities.</p><p>After named entity recognition, we then compute the relevance score ğ‘“(ğ‘’|ğ‘, ğ‘¡, ğ‘ , ğ‘‘). Specifically, we examine how consistent that the extracted entity satisfies the property specified in the query by looking at the homepage of the entity. In our runs, we calculate the frequency ğ‘“ 1 of the keyword appearing in the entity's Wiki page if the page exists. The keyword is simply the first term or phrase of each query narrative since we found the first terms characterize the target entities (e.g., carriers in Topic 1 "Carriers that Blackberry makes phones for"). We further compute the frequency ğ‘“ 2 of the keyword appearing in the "Categories" section in the bottom of the Wiki page as it can reflect a higher degree of confidence. For example, in Topic 7, the narrative is "Airlines that currently use Boeing 747 planes" and thus the keyword is "airlines". The Wiki page of the candidate entity "Air China" gives a high score ğ‘“ 2 , because "airlines" appears in the "Categories" section which clearly shows the entity is an airline. If the Wiki page does not exist, we instead use its homepage found by our homepage detection algorithm described in the next section.</p><p>We also consider the proximity of the entity and the query terms in a passage as follows:</p><formula xml:id="formula_1" coords="4,198.62,114.79,214.99,45.07">ğ‘“ 3 = ğ‘€ - 1 ğ‘  ğ·(ğ‘¤ ğ‘’ , ğ‘¤ ğ‘ )ğ‘…(ğ‘¤ ğ‘’ , ğ‘¤ ğ‘ ) ğ‘¤ ğ‘ âˆˆğ‘ âˆ©ğ‘ ğ‘€ ğ‘¤ ğ‘’ âˆˆğ‘’</formula><p>where ğ·(ğ‘¤ ğ‘’ , ğ‘¤ ğ‘ ) counts the number of words between the entity name ğ‘¤ ğ‘’ and the query term ğ‘¤ ğ‘ in the passage. ğ‘…(ğ‘¤ ğ‘’ , ğ‘¤ ğ‘ ) measures the similarity between ğ‘¤ ğ‘’ and ğ‘¤ ğ‘ as described in Section 2.2.3. ğ‘  is the total number of words in the passage and ğ‘€ is a constant to scale the quantity to be positive.</p><p>Consider the above three factors, the calculation of ğ‘“ ğ‘’ ğ‘, ğ‘¡, ğ‘ , ğ‘‘ is as follows ğ‘“ ğ‘’ ğ‘, ğ‘¡, ğ‘ , ğ‘‘ = ğ›¿ 1 ğ‘“ 1 + ğ›¿ 2 ğ‘“ 2 + ğ›¿ 3 ğ‘“ 3</p><p>where ğ›¿ 1 , ğ›¿ 2 and ğ›¿ 3 are the combination weights (with ğ›¿ 2 â‰« ğ›¿ 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ENTITY EXTRACTION FROM TABLES AND LISTS</head><p>The Web contains a large amount of structured data embedded in natural language text, tables, lists, and other forms.</p><p>A lot of entities exist in these structural forms and this is also manifested in this year's related entity finding task as many targeted entities are stored in the tables and lists. On the one hand, this poses challenges to entity extraction because most NERs utilize the context information to recognize the named entities (e.g., Conditional Random Fields or Hidden Markov Fields) while there is few context for the elements in tables. Without the context, to accurately recognize the types of the entities in the tables is difficult. To acquire the context of the entities, we use the element names as queries in Google to retrieve relevant documents, and then use NERs to recognize the types of the elements in these documents by majority voting.</p><p>On the other hand, the structure of tables and lists can facilitate the entity extraction. For example, in a table, all the elements with the same attribute have similar properties and are likely to share the same entity types. Moreover, they are likely altogether to be the target entities. Therefore, we can utilize this structural information to reduce the uncertainty on a single entity and to make a joint decision on all the entities with the same attribute. In our submitted runs, we utilize this fact by assuming that if the majority of the elements with the same attribute are of the same type or identified as target entities, all these elements have the same type or are the target entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ENTITY EXTRACTION BY SURFACE TEXT PATTERNS</head><p>The power of surface text patterns has been demonstrated in previous TREC QA tracks <ref type="bibr" coords="4,429.66,487.91,10.68,8.96" target="#b0">[1]</ref>. In this Entity track, we can define a set of simple templates which describe the relation between target and source entities. For example, we define a template as follows: &lt;TARGET&gt; is &lt;narrative&gt;. If an entity can be extracted from templates, it will be directly placed at the top of the ranked list while other candidate entities are ranked by the three-level relevance retrieval model. After completing the ranked list, the homepage finding procedure is invoked to find the homepages for the entities and the filtering rules are applied to refine the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">ENTITY HOMEPAGE FINDING</head><p>In this track, an entity is uniquely identifiable by one of its primary homepages. After extracting the names of related entities, we need to find their homepages. Wikipedia can be directly used to find Wiki homepages as well as primary homepages through the external links on the Wiki pages. To identify other homepages that go beyond Wikipedia, we treat homepage finding as a classification problem by training logistic regression (LR) models for the three target entity types respectively. We use the 11 training topics provided by the track and also select 421 persons, 568 organizations and 216 products from Google Directory<ref type="foot" coords="4,307.49,651.83,3.24,5.83" target="#foot_5">6</ref> for training. The Google Directory lists each entity's homepages. To obtain a set of negative training instances, we input each selected entity in the general Google search and manually label the top 5 returned results. Table <ref type="table" coords="4,280.35,677.18,4.98,8.96" target="#tab_0">1</ref> contains the features used in logistic regression.</p><p>It is relatively easy to find the primary official homepage and its Wiki homepage of an entity. For example, if we type the entity name in Google, the official and Wiki homepages (if they exist) usually show at the top of the returned results. However, for the secondary non-wiki homepages, it may rank very low (probably because of their low PageRanks). Therefore, it is time consuming to find and label these secondary descriptive homepages. Motivated by active learning, we first label a small number of secondary homepages (labeled on the top 5 Google returned results) and use the trained homepage classifiers to classify the web pages from a large pool of the retrieved documents. Then we only look at the positively labeled pages, manually classify them and incorporate these pages as the training data to retrain the model. In this way, the true positive instances are more likely to be labeled. As more positive instances are included, the classification performance is expected to improve.</p><p>Another issue is the choice of threshold in LR models where 0.5 is usually as a cut-off probability to determine whether a page is homepage or not. In one run (KMR3PU), we use 0.45 as the threshold in an attempt to include possibly more relevant entities. To adopt a conservative strategy, we forcefully put the newly found entities at the end of the ranked lists. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">ENTITY FILTERING</head><p>After obtaining a list of ranked entities, we use a set of heuristic rules below to further refine the results.</p><p>ï‚· We remove the entities that do not have non-wiki homepages ï‚· We experimented to use nearest neighbor clustering to merge the same entities that have different names such as Deborah Estrin and as D. Estrin. ï‚· Limit the length of entity names for different target types (i.e., 3 words for person, 5 words for organization and 8 words for product) ï‚· Remove the target entity names that heavily overlap with the source entity (e.g., remove "BlackBerry Bold" for the test Topic 1)</p><p>If there is not enough entities remaining after filtering, the system goes back to retrieve more documents (i.e., taking the top 10 pages as the candidate documents) or refine the query, and repeats the process as in <ref type="bibr" coords="5,450.43,591.83,10.66,8.96" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head><p>We submitted the following three runs, based on different homepage classification strategies:</p><p>ï‚· KMR1PU: Run produced using the complete approaches described in the previous sections; ï‚· KMR2PU: Run without iteratively and incrementally learning homepage classifiers (i.e., only training the LR models one time using the collected data from Google); ï‚· KMR3PU: Run with the threshold of 0.45 set in LR models as described in Section 2.5.</p><p>Table <ref type="table" coords="6,97.82,74.20,4.98,8.96" target="#tab_1">2</ref> lists the results for these six runs. From the table, in KMR1PU vs KMR2PU, we can see that by iteratively learning the homepage classifiers, the system could identify more relevant entities and even more primary homepages and thus the nDCG_R value increased. On the other hand, in KMR1PU vs KMR3PU, adjusting the threshold of the LR models did not find more relevant entities and homepages although more entities were retrieved, and as a result, nDCG_R deteriorated a bit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>Our submitted runs are ranked at the top in P@10 and NDCG@R among all the submissions, which shows the effectiveness of our approaches. There are several key lessons that we have learned by participating this year's entity track. First of all, we found in our experiments that with a small number of documents retrieved, the documents retrieved by Google seem much more likely to contain the related target entities than those retrieved by INDRI do.</p><p>In consequence, only a small number of candidate documents are considered in our runs, which results in relatively fewer false negative entities. Secondly, our special treatment of table or list data is well rewarding. In the test topics, over half of them have target entities existing in tables or lists. This may reflect the trend in the evolving Web data: more and more entities are stored in a structural form and there is also a lot of effort to convert unstructured data to the structured such as Freebase 7 , Factual 8 and Semantic Web. Thirdly, the high accuracy of homepage finding is crucial in this track, because by definition an entity is uniquely defined by its primary homepage, not by its name. Both precision and recall of the homepage classifiers are important in yielding good final results. Fourthly, in our experiments we found Wikipedia can serve as a valuable knowledge resource to verify whether the candidate entities satisfy the property specified in the topic. It is also very useful in locating certain homepages with high accuracy. Last but not the least, engineering effort and heuristic rules can further improve the overall retrieval performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,198.14,254.73,215.74,8.96;2,73.50,118.00,475.30,130.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of Our Entity Retrieval System</figDesc><graphic coords="2,73.50,118.00,475.30,130.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.02,231.66,387.88,197.15"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="5,72.02,231.66,387.88,197.15"><row><cell></cell><cell>: Features used in logistic regression for homepage classification</cell></row><row><cell>URL features</cell><cell>Whether includes the full entity name</cell></row><row><cell></cell><cell>Whether includes partial of the entity name</cell></row><row><cell></cell><cell>Whether includes the entity name behind the last slash</cell></row><row><cell></cell><cell>Whether contains keywords such as "wiki", "index" and "default", etc.</cell></row><row><cell></cell><cell>Whether contains acronyms of the entity name</cell></row><row><cell></cell><cell>Length in characters</cell></row><row><cell></cell><cell>Numbers of slashes, question marks, underscores and digits</cell></row><row><cell>Document features</cell><cell>Frequency of the entity name in the document</cell></row><row><cell></cell><cell>Whether contains keywords such as "official", "main", "home", etc</cell></row><row><cell></cell><cell>Whether the TITLE element contains the entity name</cell></row><row><cell></cell><cell>Length in words</cell></row><row><cell></cell><cell>PageRank</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.02,147.18,468.04,95.48"><head>Table 2 :</head><label>2</label><figDesc>Performance of the three submitted runs. The columns of the table (from left to right) are: whether</figDesc><table coords="6,72.02,158.70,468.04,83.96"><row><cell cols="8">Wikipedia received a special treatment, whether any external resources were used, the total number of entities</cell></row><row><cell cols="8">retrieved (num_ret), the number of relevant and primary entities retrieved (rel_ret and pri_ret), P@10 scores (P10),</cell></row><row><cell>and NDCG@R.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>WP</cell><cell>Ext.</cell><cell>num_ret</cell><cell>rel_ret</cell><cell>pri_ret</cell><cell>P10</cell><cell>nDCG_R</cell></row><row><cell>KMR1PU</cell><cell>Y</cell><cell>Y</cell><cell>214</cell><cell>126</cell><cell>61</cell><cell>0.2350</cell><cell>0.3061</cell></row><row><cell>KMR2PU</cell><cell>Y</cell><cell>Y</cell><cell>192</cell><cell>115</cell><cell>56</cell><cell>0.2350</cell><cell>0.2916</cell></row><row><cell>KMR3PU</cell><cell>Y</cell><cell>Y</cell><cell>260</cell><cell>126</cell><cell>61</cell><cell>0.2350</cell><cell>0.3060</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,77.78,706.93,155.86,8.96"><p>http://acronyms.thefreedictionary.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,77.78,663.86,139.31,8.96"><p>http://www.lemurproject.org/indri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,77.30,679.03,105.01,8.10"><p>http://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,77.30,693.43,176.14,8.10"><p>http://nlp.stanford.edu/software/CRF-NER.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,77.30,707.71,225.18,8.10"><p>http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=FLBJNE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,77.30,707.71,100.71,8.10"><p>http://directory.google.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p><rs type="person">Yi Fang</rs> and <rs type="person">Luo Si</rs> have been supported by a research grant from <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">IIS-0746830</rs>), a research grant from <rs type="funder">Indiana Economic Development Corporation</rs>, and a research grant from <rs type="funder">Purdue University</rs>. <rs type="person">Zhengtao Yu</rs>, <rs type="person">Yantuan Xian</rs> and <rs type="person">Yangbo Xu</rs> are supported by <rs type="funder">National Nature Science Foundation</rs> (No. <rs type="grantNumber">60863011</rs>) of <rs type="funder">China and The Key Project of Yunnan Nature Science Foundation</rs> (No. <rs type="grantNumber">2008CC023</rs>) of China. We would like to thank <rs type="person">Lina Li</rs>, <rs type="person">Lei Su</rs>, <rs type="person">Junjie Zou</rs>, <rs type="person">Yihao Zhang</rs>, <rs type="person">Xianming Yao</rs>, <rs type="person">Chaosheng Zhang</rs>, <rs type="person">Shaoming Zhang</rs>, <rs type="person">Jianyi Guo</rs>, and <rs type="person">Cunli Mao</rs>, all of <rs type="affiliation">Kunming University of Science and Technology</rs>, for their help and assistance for the work. Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jCmz58A">
					<idno type="grant-number">IIS-0746830</idno>
				</org>
				<org type="funding" xml:id="_AJQmxFt">
					<idno type="grant-number">60863011</idno>
				</org>
				<org type="funding" xml:id="_xHdQWek">
					<idno type="grant-number">2008CC023</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,90.02,651.14,444.65,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,217.90,651.14,248.62,8.96">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,487.18,651.14,16.72,8.96">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,90.02,666.62,341.54,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,205.73,666.62,151.65,8.96">High performance question/answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,377.71,666.62,23.88,8.96">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
