<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.58,72.36,394.56,16.84;1,190.26,92.29,229.20,16.84">Delft University at the TREC 2009 Entity Track: Ranking Wikipedia Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,220.25,138.01,84.79,11.06"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
							<email>p.serdyukov@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,314.21,138.01,75.26,11.06"><forename type="first">Arjen</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.58,72.36,394.56,16.84;1,190.26,92.29,229.20,16.84">Delft University at the TREC 2009 Entity Track: Ranking Wikipedia Entities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F74955D5D21EF0CFB6D27047EFC58D45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the details of our participation in Entity track of the TREC 2009.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Entity ranking is a novel TREC task introduced this year and posing challenges similar to those already well-known in web retrieval research community. We present a system which leverages a number of popular web retrieval techniques, utilizes existing knowledge bases and relies on various task-specific heuristics to produce high quality runs. Since we had no training queries with relevant web-pages contained in Category B part of ClueWeb09 collection, we focused on using various strategies rather than on using one kind of approach with different parameter settings. However, in three of four submitted runs we treated Wikipedia part of the collection as the main source of evidence about relevance of entities that can be found on the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COLLECTION PROCESSING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing and querying ClueWeb09</head><p>Relying on the most convenient strategy, we indexed WARC bundles containing ClueWeb09 collection using Lemur Toolkit 4.10. We used no stemming and stop-word removal on the collection level. However, we applied a very simple stemming strategy for queries, by adding singulars to the query if there were any plurals. In addition, we used a stopword list with 320 words to exclude stopwords from queries. We used both the title and the narrative to build a query. Finally, we ranked the pages using Language model based approach implemented in Lemur with Dirichlet smoothing (µ = 1500).</p><p>In order to have some flexibility in development and also for the sake of better performance, we built several indexes of different purpose:</p><p>• Index containing all pages from ClueWeb09 (Category B) collection, except those contained in the bundles named as "enwpXX",</p><p>• Index containing all pages from the bundles named as "enwpXX" (Wikipedia pages),</p><p>• Index containing anchor text of all pages from the non-Wikipedia part of ClueWeb09 (Category B),</p><p>• Index containing anchor text of all pages from the Wikipedia part.</p><p>All pages and extracted anchor text were stored in the indexes as well, in order to fetch them dynamically at query time for the further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Treating Wikipedia as an entity repository</head><p>Wikipedia is an online encyclopedia that recently became one of the largest repositories of encyclopedic knowledge, with millions of articles available for a number of languages. English Wikipedia is also a part of the ClueWeb09 (Category B) and represents a spam-free collection of web-pages with dedicated descriptions of around 2,700,000 entities and concepts. While it is not possible to find any entity a user may ask for among those described in Wikipedia, there is still a very high chance that most potentially popular queries can be answered by ranking entities described in this repository.</p><p>Wikipedia part of the ClueWeb09 is a collection of raw HTML pages, so, it needs some basic cleaning in order to serve as an entity repository. While we indexed the entire Wikipedia, we always ignored non-article pages (lists, disambiguation, category pages, etc.) at the query postprocessing stage. However, we still had a problem of duplicates, since typically a number of URLs, all existing name variants of the same entity(e.g. "/wiki/Crackberry" and "/wiki/Blackberry_Storm") and all different ClueWeb09 documents, are redirected to one page in Wikipedia (e.g. "/wiki/Blackberry"). All these URLs have the same title still. So, since we had to submit only one Wikipedia document per entity, we always selected the URL with the highest number of inlinks within Wikipedia collection among documents with the same title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BASELINE: ANCHOR-BASED ENTITY RANKING</head><p>In order to first approach the problem of ranking with the simplest strategy possible, we decided to find entity mentions at the given primary page not using any given dictionary of entities (e.g. the titles of Wikipedia articles). While, the traditional approach would be to run a named entity tagger against the primary web-page of the query entity given, we did not follow this strategy by several reasons. First, such tagging would give us potentially a very long list of entities with still the need to disambiguate many of them. And, second, we would still need to search for primary pages for each of discovered entities and rank them by their relevance. While we still consider worthwhile to follow that path, we decided to rely on the clues left for us by the author of the primary web-page. We considered anchor text of its outlinks as entity names, and the pages linked as primary for these entities. As a result, it left us with no need to disambiguate names and search for primary pages, as well as we had the reason to hope that only important entities with strong relation to the main subject of the page were linked by its author. In order to extract actual entity names from anchors we also removed the following words: "homepage of", "about", "information about". We then found the pages in the collection, ranked them and also relied on the following ranking rules, mainly following the instructions specified in the guidelines for the task:</p><p>• For the entities of the type "product" we always ranked pages hosted at the domain of the given primary page higher than external pages,</p><p>• For the entities of the type "person" or "organization" we always ranked pages from outside of the domain of the given primary page higher than pages from the same domain.</p><p>Since, the primary page itself not always contains actual content, we ranked also the pages at the second level on the link path starting from the primary page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">WIKIPEDIA-BASED ENTITY RANKING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Building a candidate list</head><p>Since we considered Wikipedia as the primary repository of entity descriptions, we always started from ranking Wikipedia articles and regarding first 3,000 entities as our candidates. After the basic filtering steps (see Section 2.1), we faced two challenges: finding additional evidence of entity relevance using the entire ClueWeb09 (Category B) and filtering out entities of undesired types.</p><p>Thanks to Wikipedia category structure and enthusiasm of Wikipedia contributors, it was pretty straightforward to find articles describing persons. We believe that most biographical articles belong either to the category "Living people" (around 337,000), or to the categories "XXXX births" and "XXXX deaths" . So, we just searched for these strings in the text of retrieved Wikipedia pages to filter out persons or to consider them as the only entities allowed (actually, using #scoreif directive of the Lemur query language). Since we still could not be sure that all relevant entities are properly categorized, we returned them in the ranking as well, but after the matching ones. All approaches described further benefited from this strategy, including the approach using external ontologies to guess about the other types of entities (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Using Wikipedia to find named entities</head><p>There are several ways to deal with Wikipedia corpus in order to find relevant named entities: simply rank Wikipedia pages, rank Wikipedia pages whose names appear on the given primary Web page of the query entity or rank Wikipedia pages whose names can be found at the top ranked Web pages. While there are pros and cons for each approach, we built our runs using only the latter two strategies (see Section 5). Despite that we regarded all Web pages from the top ranked set as equally important to promote entities, we tried to bring in some variety in their nature and also prevent spam pages from taking over the entire top. So, in the case when we relied on top ranked pages, we always considered 5 top ranked pages from the domain of the given primary page (including the primary page itself) and also top 20 pages from other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Finding primary Wikipedia pages for the given entity</head><p>While our task was to search for the entities related to the one specified in the query, there was a very high chance that we find also Wikipedia article(s) describing the query entity itself. Considering that such an article would highly likely lead also to the important entities related to the query entity, we tried to find it and rank only its outlinks. However, since the risk of selecting a wrong "primary" wikipedia page for the query entity was quite high with potentially dramatic loss in performance, we decided to use two sources of evidence: title/narrative and the primary homepage of the query entity. We proceeded as follows:</p><p>1. Retrieved top 5 Wikipedia articles as a candidate set, 2. Extracted all URLs from the "External links" (or "References", if necessary) section of each article, preserving the ranking order specified by its author, 3. Selected the Wikipedia article from the candidate set, whose editor assigned the highest rank to the primary homepage of the query entity.</p><p>In other words, we either selected the top ranked article, or one of the most highly ranked articles which was the most related to the primary homepage of the query entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Using existing ontologies to find entities of the desired type</head><p>Another possible source of additional performance is the correct classification of entities into four classes (person, organization, product, other) and selecting only the Wikipedia entities matching the given target class. Since, the sufficient amount of training data is not available, we utilize external resources for the filtering purposes, namely DBPedia<ref type="foot" coords="2,533.24,476.07,3.65,5.24" target="#foot_0">1</ref> and Yago<ref type="foot" coords="2,336.78,486.53,3.65,5.24" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Using DBPedia</head><p>DBPedia is a highly structured representation of Wikipedia, describing not only typed relations between Wikipedia entities, but also containing contextual links to other repositories. It also provides a basic ontology<ref type="foot" coords="2,489.39,552.17,3.65,5.24" target="#foot_2">3</ref> , ideally suited for the entity ranking task, since it has almost exactly the same classes as allowed in Entity track, at the top of its hierarchy. Particularly, we considered 4 of them: "Organization", "Person", "Work" (actually, artifacts made by humans) and "Drug" (conceptually, could be a sub-class of "Work"). The union of the latter two we consider to be equivalent to the "product" class specified in our queries. However, only around 1 million entities are currently classified into this ontology. So, we only filtered out those entities for which the class is known and it does not match the one specified in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Using infoboxes, Yago and Wordnet</head><p>Since only a limited number of Wikipedia articles are categorized into DBpedia ontology, we tackle the filtering problem by building additional "feature space" for its classes using two sources: properties specified in the infobox of almost every article (around 1,750,000), and Wordnet classes assigned by Yago to almost every second article (around 1,800,000). While learning an appropriate classifier is a promising direction of the future work, we applied a simple rule-based filtering approach based on discovered feature sets. We filter out entities with the features that do not belong to the class specified in the query, so there is no any other entity that belongs to the query class and has the same feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FINDING PRIMARY HOMEPAGES FOR WIKIPEDIA ENTITIES</head><p>After the candidate Wikipedia pages are ranked, we still need to look for the canonical names of the entities they describe and also for up to 3 candidate homepages to return. Thanks to authors of Wikipedia pages, we are almost always able to find primary web-pages among the "External links" section, as we did to discover "primary" Wikipedia pages. However, since URLs under "External links" are not only homepages, but also just relevant pages, we consider only the top 3 of them. However, in cases when some of these URLs belong to the same domain, we consider only the highest ranked of them and hence end up with less than 3 pages. Another problem is that Category B part of ClueWeb09 contains far not all pages linked from Wikipedia and hence we need some backup strategy to find the pages we still miss. So, we used the anchor text index for that purpose and searched it using either names of the candidate Wikipedia entities or the most popular anchor text strings associated with their URLs. We also did not allow more than one page from the same domain, but since this time they were ranked not by a human editor, we considered that not the highest ranked, but the shortest URL has more chances to be an entry to the entity homepage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS</head><p>We submitted the following runs:</p><p>• tudwebtop: Run produced using the approach described in Section 2.2, where every outlink from the given primary web-page is ranked,</p><p>• tudwtop: Run produced using the approach described in Section 4.1, where those Wikipedia entities that appear at the top web-pages are ranked first,</p><p>• tudpw: Run produced using the approach described in Section 4.2, where those Wikipedia entities that appear at the discovered primary Wikipedia page are ranked first,</p><p>• tudpwkntop: Run produced using the approach described in Section 4.3, where Wikipedia entities found using the tudpw run are post-filtered using ontologies and top web pages.</p><p>We decided to compare runs using the variant of evaluation when wikipedia pages are not left out, since our approaches are mainly built on Wikipedia. We do not report values for all measures for every topic here, but rather than analyzing averaged values, we are more interested in the number of topics for which the approach was one of the best performing. For example, for the representative P@10 measure, we see that tudpwkntop was the best for 10 topics, tudpw for 6 topics, tudwtop for 6 topics, and tudwebtop for 2 topics. Note that while tudwebtop has shown zero performance for 15 topics out of 20, it was the only approach that worked for 2 topics (8th and 16th topics). Also, all approaches failed to produce non-zero performance for the 3rd topic. In two cases, one of the approaches has shown the maximum possible performance (1,0 for 17th and 18th topics). Averaged measures produced for three different sets of qrels are demonstrated in the following tables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Entity ranking with the focus on Wikipedia is a wellknown task being run at INEX conference 4 since 2007 <ref type="bibr" coords="3,543.63,592.38,9.22,7.86" target="#b0">[1]</ref>. The main difference, besides the smaller corpus, is in the larger number of entity types allowed. Besides, they are usually specified on much less abstract level, since each query is supplemented with the actual Wikipedia category related to relevant entities (but not always directly). The usefulness of Wikipedia for finding representative keywords and named entities on web pages is demonstrated in several publications. First, this idea was introduced in the Wikify system, which not only matched keyphrases to Wikipedia entities, but also selected the most important of them using tudwebtop P@10 tudpwkntop P@10 tudwtop P@10 tudpw P@10 Figure <ref type="figure" coords="4,132.73,291.29,4.13,7.89">1</ref>: P@10 of all runs per topic for the evaluation considering also Wikipedia pages the number of article's inlinks <ref type="bibr" coords="4,176.08,320.96,9.22,7.86" target="#b2">[3]</ref>. Later, this technique was extended by learning the function of link appropriateness using the number of features: the relatedness of linked entities to the content of the page under study, the minimum depth at which the entity is located in Wikipedias category tree, number of mentions at the page, their locations and proximity to each other <ref type="bibr" coords="4,126.10,383.72,9.22,7.86" target="#b3">[4]</ref>. Another recently proposed approach uses the link structure of candidate Wikipedia entities to first group them into clusters and then rank these clusters by the overall importance of their entities, measured using their inlinks, in the way also used by the Wikify system <ref type="bibr" coords="4,280.62,425.57,9.22,7.86" target="#b1">[2]</ref>. Despite that some of these methods might be useful also in the present setup, we believe that the task that we deal with suffers much less from the word ambiguity problem, which is the primary issue for the keyword extraction methods. In the case of entity ranking, we have a "query layer" which connects relevant Wikipedia pages with relevant Web pages and hence should implicitly disambiguate entities mentioned at the web-pages. However, it would still be interesting to test the value of link-based entity authority and number of mentions for detection of relevant entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>We described our approaches to entity ranking used to produce the submitted runs. We relied on two strategies, one considering outlinks and their anchor text that can be found at the primary web-pages as entity mentions, and on another one, used in three of four runs, fully relying upon Wikipedia as on the repository of entities. Wikipedia-based approaches outperformed the baseline outlink-base approach. In a few cases, Wikipedia-based techniques failed to show non-zero performance what confirms that the coverage of Wikipedia is limited and can not be used to answer all queries. However, it was possible to retrieve at least one relevant Wikipedia entity among the first 10 in 80% cases (for all topics except the 3rd, 8th, 13th and 16th).</p><p>We clearly see two ways to improve the Wikipedia-based entity ranking. First, we need to improve the classification part, since current ontologies do not cover the entire Wikipedia and hence can not serve as a sufficient solution.</p><p>Second, we need to evaluate the relevance of each entity by analyzing the entity co-occurrence within the smaller context, e.g. at the paragraph or sentence level. Of particular interest is also the task of finding primary/relevant homepages for the given Wikipedia entity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,316.81,212.12,239.11,192.50"><head>Table 1 :</head><label>1</label><figDesc>Performance of all submitted runs for NGCG R, P@10 scores, and the number of relevant and primary entities (rel ret and pri ret, respectively) retrieved. Official qrels (Wikipedia pages are not considered) are used.</figDesc><table coords="3,334.20,212.12,206.63,192.50"><row><cell></cell><cell cols="2">nDCG R P10</cell><cell cols="2">rel ret pri ret</cell></row><row><cell cols="2">tudwebtop 0.1218</cell><cell cols="2">0.0600 103</cell><cell>28</cell></row><row><cell>tudwtop</cell><cell>0.1244</cell><cell cols="2">0.0650 125</cell><cell>50</cell></row><row><cell>tudpw</cell><cell>0.1351</cell><cell cols="2">0.0950 108</cell><cell>42</cell></row><row><cell cols="2">tudpwkntop 0.1334</cell><cell cols="2">0.1150 108</cell><cell>41</cell></row><row><cell>Run</cell><cell cols="2">nDCG R P10</cell><cell cols="2">rel ret pri ret</cell></row><row><cell cols="2">tudwebtop 0.1009</cell><cell cols="2">0.0600 103</cell><cell>28</cell></row><row><cell>tudwtop</cell><cell>0.1672</cell><cell cols="2">0.2250 168</cell><cell>144</cell></row><row><cell>tudpw</cell><cell>0.1767</cell><cell cols="2">0.2400 140</cell><cell>122</cell></row><row><cell cols="2">tudpwkntop 0.1778</cell><cell cols="2">0.2700 140</cell><cell>120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,316.81,426.62,239.10,78.05"><head>Table 2 :</head><label>2</label><figDesc>Performance of all submitted runs for all measures. Wikipedia pages are considered.</figDesc><table coords="3,334.20,465.03,206.63,39.65"><row><cell>Run</cell><cell cols="2">nDCG R P10</cell><cell cols="2">rel ret pri ret</cell></row><row><cell>tudwtop</cell><cell>0.2551</cell><cell cols="2">0.2150 43</cell><cell>94</cell></row><row><cell>tudpw</cell><cell>0.2836</cell><cell cols="2">0.2300 32</cell><cell>80</cell></row><row><cell cols="2">tudpwkntop 0.2826</cell><cell cols="2">0.2600 32</cell><cell>79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,316.81,526.67,239.10,18.35"><head>Table 3 :</head><label>3</label><figDesc>Performance of all submitted runs for all measures. Only Wikipedia pages are considered.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,317.27,709.42,135.96,9.88"><head></head><label></label><figDesc>4 http://www.inex.otago.ac.nz/</figDesc><table coords="4,99.63,39.61,404.69,242.80"><row><cell></cell><cell></cell><cell cols="3">tudwebtop</cell><cell></cell><cell></cell><cell></cell><cell cols="4">tudpwkntop</cell><cell>tudwtop</cell><cell></cell><cell>tudpw</cell></row><row><cell>1</cell><cell>1</cell><cell>P@10</cell><cell></cell><cell>0</cell><cell cols="2">NDCG</cell><cell>0</cell><cell cols="2">P@10</cell><cell>0,1</cell><cell>NDCG 0,1541</cell><cell>P@10</cell><cell>0</cell><cell>NDCG 0,1316</cell><cell>P@10</cell><cell>NDCG 0 0.1343</cell></row><row><cell>0,9</cell><cell>2</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0,2</cell><cell>0,3126</cell><cell cols="2">0,3</cell><cell>0,4</cell><cell>0,2</cell><cell>0,2168</cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>0,8</cell><cell>4</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>0,1481</cell><cell cols="2">0,1</cell><cell>0,11</cell><cell>0,1</cell><cell>0,2288</cell></row><row><cell>0,7</cell><cell>5 6</cell><cell></cell><cell cols="2">0,2 0</cell><cell></cell><cell cols="2">0,2589 0,0793</cell><cell></cell><cell></cell><cell>0,1 0,1</cell><cell>0,1475 0,1339</cell><cell cols="2">0,4 0</cell><cell>0,2384 0,1556</cell><cell>0,1 0,1</cell><cell>0,1583 0,1647</cell></row><row><cell>0,6</cell><cell>7 8</cell><cell></cell><cell cols="2">0 0,5</cell><cell></cell><cell cols="2">0 0,3507</cell><cell></cell><cell></cell><cell>0,8 0</cell><cell>0,2182 0,1453</cell><cell cols="2">0,3 0</cell><cell>0,2355 0,1612</cell><cell>0,6 0</cell><cell>0,2 0,1454</cell></row><row><cell>0,5</cell><cell>9 10</cell><cell></cell><cell></cell><cell>0 0</cell><cell></cell><cell cols="2">0,2327 0,0598</cell><cell></cell><cell></cell><cell>0,6 0,4</cell><cell>0,4653 0,2534</cell><cell cols="2">0,5 0</cell><cell>0,3187 0,14</cell><cell>0,6 0,3</cell><cell>0,4653 0,2355</cell></row><row><cell>0,4</cell><cell>11</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell cols="2">0,0692</cell><cell></cell><cell></cell><cell>0,2</cell><cell>0,0817</cell><cell></cell><cell>0</cell><cell>0,0177</cell><cell>0</cell><cell>0,0346</cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0,2</cell><cell>0,2122</cell><cell></cell><cell>0</cell><cell>0,1368</cell><cell>0,1</cell><cell>0,2497</cell></row><row><cell>0,3</cell><cell>13</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell cols="2">0,1202</cell><cell></cell><cell></cell><cell>0</cell><cell>0,0862</cell><cell></cell><cell>0</cell><cell>0,1038</cell><cell>0</cell><cell>0,1218</cell></row><row><cell>0,2</cell><cell>14 15</cell><cell></cell><cell></cell><cell>0 0</cell><cell></cell><cell cols="2">0 0,055</cell><cell></cell><cell></cell><cell>0,1 0,1</cell><cell>0,1226 0,224</cell><cell cols="2">0 0,1</cell><cell>0 0,1405</cell><cell>0,1 0,1</cell><cell>0,1218 0,2086</cell></row><row><cell>0,1</cell><cell>16 17</cell><cell></cell><cell cols="2">0,3 0,1</cell><cell></cell><cell cols="2">0,3598 0,1831</cell><cell></cell><cell></cell><cell>0 0,7</cell><cell>0 0,155</cell><cell></cell><cell>0 1</cell><cell>0 0,3421</cell><cell>0 0,7</cell><cell>0 0,155</cell></row><row><cell>0</cell><cell>18</cell><cell></cell><cell cols="2">0,1</cell><cell></cell><cell cols="2">0,096</cell><cell></cell><cell></cell><cell>1</cell><cell>0,2915</cell><cell cols="2">0,9</cell><cell>0,3105</cell><cell>1</cell><cell>0,29</cell></row><row><cell></cell><cell>19 20 1</cell><cell>2</cell><cell>3</cell><cell>0 0</cell><cell>4</cell><cell cols="3">0 0,1528 5 6</cell><cell>7</cell><cell>0,1 0,7 8</cell><cell cols="4">0,1425 0,2615 9 10 11 12 13 14 15 16 17 18 19 20 0 0,0694 0,1 0,1353 0,9 0,3188 0,7 0,2664</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,321.42,691.13,84.73,7.47"><p>http://dbpedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,321.42,701.48,170.46,7.47"><p>http://www.mpi-inf.mpg.de/yago-naga/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,321.42,711.83,131.81,7.47"><p>http://dbpedia.org/ontology/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">ACKNOWLEDGMENTS</head><p>We would like to sincerely thank <rs type="person">Claudia Hauff</rs> and <rs type="person">Djoerd Hiemstra</rs> for the help with collection preprocessing.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,331.02,478.57,207.90,7.86;4,331.02,489.03,202.81,7.86;4,331.02,499.49,210.93,7.86;4,331.02,509.95,208.29,7.86;4,331.02,520.41,123.48,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,331.02,489.03,187.46,7.86">Overview of the inex 2008 entity ranking track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Demartini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iofciu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="4,394.77,509.95,140.53,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5631</biblScope>
			<biblScope unit="page" from="243" to="252" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.02,531.87,210.76,7.86;4,331.02,542.33,211.91,7.86;4,331.02,552.79,197.78,7.86;4,331.02,563.25,208.20,7.86;4,331.02,573.71,117.25,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,498.89,531.87,42.90,7.86;4,331.02,542.33,195.83,7.86">Extracting key terms from noisy and multitheme documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grineva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grinev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lizorkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,331.02,552.79,197.78,7.86;4,331.02,563.25,120.14,7.86">WWW &apos;09: Proceedings of the 18th international conference on World wide web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.02,585.17,181.15,7.86;4,331.02,595.63,213.59,7.86;4,331.02,606.09,192.83,7.86;4,331.02,616.55,224.89,7.86;4,331.02,627.01,201.01,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,448.45,585.17,63.73,7.86;4,331.02,595.63,150.79,7.86">Wikify!: linking documents to encyclopedic knowledge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,500.81,595.63,43.81,7.86;4,331.02,606.09,192.83,7.86;4,331.02,616.55,220.06,7.86">CIKM &apos;07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.02,638.47,197.90,7.86;4,331.02,648.93,217.03,7.86;4,331.02,659.39,223.70,7.86;4,331.02,669.85,201.01,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,443.01,638.47,85.91,7.86;4,331.02,648.93,36.86,7.86">Learning to link with wikipedia</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,386.57,648.93,161.48,7.86;4,331.02,659.39,218.87,7.86">CIKM &apos;08: Proceeding of the 17th ACM conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
