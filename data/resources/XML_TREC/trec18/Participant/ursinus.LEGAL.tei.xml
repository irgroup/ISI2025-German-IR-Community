<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.72,80.11,214.68,15.24;1,151.44,100.03,309.07,15.24">Sparse Matrix Factorization: Applications to Latent Semantic Indexing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,185.28,127.40,68.40,9.86"><forename type="first">Erin</forename><surname>Moulding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Saskatchewan</orgName>
								<address>
									<postCode>S7N 5C9</postCode>
									<settlement>Saskatoon</settlement>
									<region>SK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,133.32,168.32,86.27,9.86"><forename type="first">April</forename><surname>Kontostathis</surname></persName>
							<email>akontostathis@ursinus.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Computer Science</orgName>
								<orgName type="institution">Ursinus College</orgName>
								<address>
									<postCode>19426</postCode>
									<settlement>Collegeville</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,174.00,209.12,92.04,9.86"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Spiteri</surname></persName>
							<email>spiteri@cs.usask.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Saskatchewan</orgName>
								<address>
									<postCode>S7N 5C9</postCode>
									<settlement>Saskatoon</settlement>
									<region>SK</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.72,80.11,214.68,15.24;1,151.44,100.03,309.07,15.24">Sparse Matrix Factorization: Applications to Latent Semantic Indexing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CAA7D78A8C5D0A4EEA09DA0351B02CDC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the use of Latent Semantic Indexing (LSI) and some of its variants for the TREC Legal batch task. Both folding-in and Essential Dimensions of LSI (EDLSI) appeared as if they might be successful for recall-focused retrieval on a collection of this size. Furthermore, we developed a new LSI technique, one which replaces the Singular Value Decomposition (SVD) with another technique for matrix factorization, the sparse column-row approximation (SCRA). We were able to conclude that all three LSI techniques have similar performance. Although our 2009 results showed significant improvement when compared to our 2008 results, the use of a better method for selection of the parameter K, which is the ranking that results in the best balance between precision and recall, appears to have provided the most benefit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Our submissions for the 2007 and 2008 TREC Legal competitions tested a variety of both simple and stateof-the-art normalization systems. For the 2009 TREC Legal Competition, we tested a new matrix factorization for Latent Semantic Indexing (LSI).</p><p>We have extensive experience with LSI, including previous years in TREC legal competitions. We also have used the new Essential Dimensions of LSI (EDLSI) approach, and tested it on small and large datasets. EDLSI allows a much smaller LSI dimensionality reduction parameter (k) to be used, and weights the LSI results with the results from traditional vector-space retrieval. EDLSI has been shown to consistently match or improve on LSI <ref type="bibr" coords="1,138.84,665.32,10.60,8.97" target="#b6">[8]</ref>. This suggested to us that the PSVD approximation used in LSI may not be ideal, and instead, an approximation should be used that somehow captures more of the term-document matrix. Investigating this led to the sparse column-row approximation (SCRA), which we test here in its first application to information retrieval with datasets of this order of magnitude.</p><p>Teams this year were permitted only three runs. Our first run was a LSI run using folding-in, the second was a fully distributed EDLSI run, and the third was a fully distributed run using the SCRA. As in last year's competition, teams were required to set K and K h for each query. These parameters indicate where the system believes that precision and recall are best balanced for relevant and highly relevant documents respectively. The balance is indicated by the scoring measure F 1@K: F 1@K = 2 * P @K * R@K P @K + R@K The rest of this paper is organized as follows. Section 2 presents necessary background information on the methods used. Section 3 describes our approach and the different runs submitted. Section 4 presents the results, and Section 5 gives our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section we begin with a description of vectorspace retrieval, which forms the foundation for Latent Semantic Indexing (LSI). We also present a brief overview of LSI <ref type="bibr" coords="1,381.24,610.00,10.60,8.97" target="#b2">[3]</ref>, including the technique of foldingin. We discuss Essential Dimensions of LSI (EDLSI) and its improvements over LSI. We also describe the sparse column-row approximation (SCRA) used for our new work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vector-Space Retrieval</head><p>In vector-space retrieval, a document is represented as a vector in t-dimensional space, where t is the number of terms in the lexicon being used. If there are d documents in the collection, then the vectors representing the documents can be represented by a matrix A ∈ ℜ t×d , called the term-document matrix. Entry a i,j of matrix A indicates how important term i is to document j, where 1 ≤ i ≤ t and 1 ≤ j ≤ d.</p><p>The entries in A can be binary numbers (1 if the term appears in the document and 0 otherwise), raw term frequencies (the number of times the term appears in the document), or weighted term frequencies. Weighting can be done using either local weighting, global weighting, or a combination of both. The purpose of local weighting is to capture the relative importance of a term within a specific document; therefore, local weighting uses the frequency of the term within the document to calculate the weight and assigns a higher weight if the frequency is higher. The purpose of global weighting is to identify terms that discriminate effectively between documents; thus, global weighting uses the frequency of the term within the entire document collection to calculate the weight and assigns a higher weight if the frequency is lower. Because document size often varies widely, the weights are also usually normalized; otherwise, long documents are more likely to be retrieved. See, e.g., <ref type="bibr" coords="2,285.96,351.88,10.60,8.97" target="#b0">[1]</ref>, <ref type="bibr" coords="2,72.00,363.88,16.64,8.97" target="#b10">[12]</ref> for a comprehensive discussion of local and global weighting techniques.</p><p>Common words, such as and, the, if, etc., are considered to be stop-words <ref type="bibr" coords="2,168.12,399.52,11.60,8.97" target="#b0">[1]</ref> and are not included in the term-document matrix. Words that appear infrequently are often excluded to reduce the size of the lexicon.</p><p>Like the documents, queries are represented as tdimensional vectors, and the same weighting is applied to them. Documents are retrieved by mapping q into the row (document) space of the term-document matrix, A:</p><formula xml:id="formula_0" coords="2,163.92,489.80,44.17,11.53">w = q T A.</formula><p>After this calculation, w is a d-dimensional row vector, entry j of which is a measure of how relevant document j is to query q. In a traditional search-andretrieval application, documents are sorted based on their relevance score (i.e., vector w) and returned to the user with the highest-scoring document appearing first. The order in which a document is retrieved is referred to as the rank<ref type="foot" coords="2,134.04,593.08,3.49,6.28" target="#foot_0">1</ref> of the document with respect to the query. The experiments in this paper run multiple queries against a given dataset, so in general the query vectors q 1 , q 2 , . . . , q n are collected into a matrix Q ∈ ℜ t×n and their relevance scores are computed as</p><formula xml:id="formula_1" coords="2,161.76,661.28,48.49,11.41">W = Q T A,</formula><p>where entry w j,k in W ∈ ℜ n×d is a measure of how relevant document j is to query k.</p><p>There are two immediate deficiencies of vector-space retrieval. First, W might pick up documents that are not relevant to the queries in Q but contain some of the same words. Second, Q may overlook documents that are relevant but that do not use the exact words being queried. The partial singular value decomposition (PSVD) that forms the heart of LSI is used to capture term relationship information in the term-document space. Documents that contain relevant terms but perhaps not exact matches will ideally still end up 'close' to the query in the LSI space <ref type="bibr" coords="2,409.08,196.72,10.60,8.97" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Semantic Indexing</head><p>LSI uses the PSVD to approximate A, alleviating the deficiencies of vector-space retrieval described above.</p><p>The PSVD, also known as the truncated SVD, is derived from the SVD. The (reduced) SVD decomposes the term-document matrix into the product of three matrices: U ∈ ℜ t×r , Σ ∈ ℜ r×r , and V ∈ ℜ d×r , where r is the rank of the matrix A. The columns of U and V are orthonormal, and Σ is a diagonal matrix, the diagonal entries of which are the r non-zero singular values of A, customarily arranged in non-increasing order. Thus A is factored as</p><formula xml:id="formula_2" coords="2,399.48,386.00,52.93,11.41">A = U ΣV T .</formula><p>The PSVD produces an optimal rank-k (k &lt; r) approximation to A by truncating Σ after the first (and largest) k singular values. The corresponding columns from k + 1 to r of U and V are also truncated, leading to matrices U k ∈ ℜ t×k , Σ k ∈ ℜ k×k , and V k ∈ ℜ d×k . A is then approximated by</p><formula xml:id="formula_3" coords="2,382.32,487.40,87.37,12.74">A ≈ A k = U k Σ k V T k .</formula><p>In the context of LSI, there is evidence to show that A k provides a better model of the semantic structure of the corpus than the original term-document matrix was able to provide for some collections <ref type="bibr" coords="2,488.88,545.08,10.60,8.97" target="#b2">[3]</ref>, <ref type="bibr" coords="2,507.36,545.08,10.60,8.97" target="#b3">[4]</ref>, <ref type="bibr" coords="2,525.84,545.08,10.69,8.97" target="#b4">[5]</ref>, <ref type="bibr" coords="2,312.00,557.08,10.60,8.97" target="#b1">[2]</ref>. For example, searchers may choose a term, t 1 , that is synonymous with a term, t 2 , that appears in a given document, d 1 . If k is chosen appropriately and there is ample use of the terms t 1 and t 2 in other documents (in similar contexts), the PSVD will give t 1 a large weight in the d 1 dimension of A k even though t 1 does not appear in d 1 . Similarly, an ancillary term t 3 that appears in d 1 , even though d 1 is not 'about' t 3 , may well receive a lower or negative weight in A k matrix entry (t 3 , d 1 ).</p><p>Choosing an optimal LSI dimension k for each collection remains elusive. Traditionally, an acceptable k has been chosen by running a set of queries with known relevant document sets for multiple values of k. The k that results in the best retrieval performance is chosen as the optimal k for each collection. Optimal k values are typically in the range of 100-300 dimensions <ref type="bibr" coords="3,261.84,89.08,10.69,8.97" target="#b3">[4]</ref>, <ref type="bibr" coords="3,279.48,89.08,15.34,8.97" target="#b8">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Folding-In</head><p>The PSVD is useful in information retrieval, but calculating it is computationally expensive <ref type="bibr" coords="3,225.48,146.20,15.34,8.97" target="#b14">[16]</ref>. This expense is offset by the fact that the LSI space for a given set of documents can be used for many queries. However, if terms or documents are added to the initial dataset, then either the PSVD must be recomputed for the new A or the new information must be added to the current PSVD. In LSI, the two traditional ways of adding information to an existing PSVD are folding-in <ref type="bibr" coords="3,208.92,229.96,15.24,8.97" target="#b9">[11]</ref>, <ref type="bibr" coords="3,231.72,229.96,11.60,8.97" target="#b1">[2]</ref> and updating <ref type="bibr" coords="3,72.00,241.84,15.34,8.97" target="#b14">[16]</ref>. We describe folding-in here.</p><p>Folding-in is computationally inexpensive, but the performance of LSI generally deteriorates as documents are folded-in <ref type="bibr" coords="3,130.20,277.96,10.60,8.97" target="#b1">[2]</ref>, <ref type="bibr" coords="3,149.04,277.96,15.34,8.97" target="#b9">[11]</ref>. In order to fold-in new document vectors, the documents are first projected into the reduced k-dimensional LSI space and then appended to the bottom of V k . Suppose D ∈ ℜ t×p is a matrix whose columns are the p new documents to be added. The projection is</p><formula xml:id="formula_4" coords="3,149.16,356.96,70.45,11.78">D k = D T U k Σ -1</formula><p>k , and D k is then appended to the bottom of V k , so the PSVD is now</p><formula xml:id="formula_5" coords="3,113.64,407.72,144.73,12.74">[ A, D ] ≈ U k Σ k [ V T k , D T k ].</formula><p>Note that U k and Σ k are not modified when using folding-in. A similar procedure can be applied to U k to fold-in terms <ref type="bibr" coords="3,127.08,452.56,10.60,8.97" target="#b1">[2]</ref>. Folding-in may be useful if only a few documents are to be added to the dataset or if word usage patterns do not vary significantly, as may be the case in a restricted lexicon. However, in general, each application of folding-in corrupts the orthogonality of the columns of U k or V k , eventually reducing the effectiveness of LSI. It is not generally advisable to rely exclusively on folding-in when the dataset changes frequently <ref type="bibr" coords="3,267.24,536.32,15.34,8.97" target="#b13">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Essential Dimensions of LSI</head><p>As the dimension of the LSI space k approaches the rank r of the term-document matrix A, LSI approaches vector-space retrieval. In particular, vector-space retrieval is equivalent to LSI when k = r.</p><p>Figures <ref type="figure" coords="3,116.88,629.56,4.98,8.97">1</ref><ref type="figure" coords="3,121.86,629.56,4.98,8.97">2</ref><ref type="figure" coords="3,126.84,629.56,4.98,8.97">3</ref>show graphically that the performance of LSI may essentially match (or even exceed) that of vector-space retrieval even when k ≪ r. For the CACM <ref type="bibr" coords="3,105.12,665.32,16.64,8.97" target="#b11">[13]</ref> and NPL [6] collections, we see that LSI retrieval performance continues to increase as additional dimensions are added, whereas retrieval performance of LSI for the MED collection peaks when k = 75 and then decays to the level of vector-space retrieval. Thus we see that vector-space retrieval outperforms LSI on some collections, even for relatively large values of k. Other examples of collections that do not benefit from LSI can be found in <ref type="bibr" coords="3,398.64,113.08,11.60,8.97" target="#b7">[9]</ref> and <ref type="bibr" coords="3,431.52,113.08,10.60,8.97" target="#b5">[7]</ref>. These data suggest that we can use the term relationship information captured in the first few SVD vectors, in combination with vector-space retrieval, a technique referred to as Essential Dimensions of Latent Semantic Indexing (EDLSI). Kontostathis demonstrated good performance on a variety of collections by using only the first 10 dimensions of the SVD <ref type="bibr" coords="4,239.04,101.08,10.60,8.97" target="#b6">[8]</ref>. The model obtains final document scores by computing a weighted average of the traditional LSI score using a small value for k and the vector-space retrieval score. The result vector computation is</p><formula xml:id="formula_6" coords="4,116.52,167.96,138.85,11.78">W = x(Q T A k ) + (1 -x)(Q T A),</formula><p>where x is a weighting factor (0 ≤ x ≤ 1) and k is small.</p><p>In <ref type="bibr" coords="4,95.04,213.04,10.60,8.97" target="#b6">[8]</ref>, parameter settings of k = 10 and x = 0.2 were shown to provide consistently good results across a variety of large and small collections studied. On average, EDLSI improved retrieval performance an average of 12% over vector-space retrieval. All collections showed significant improvements, ranging from 8% to 19%. Significant improvements over LSI were also noted in most cases. LSI outperformed EDLSI for k = 10 and x = 0.2 on only two small datasets, MED and CRAN. It is well known that LSI happens to perform particularly well on these datasets. Optimizing k and x for these specific datasets restored the outperformance of EDLSI.</p><p>Furthermore, computation of only a few singular values and their associated singular vectors has a significantly reduced cost when compared to the usual 100-300 dimensions required for traditional LSI. EDLSI also requires minimal extra memory during query run time when compared to vector-space retrieval and much less memory than LSI <ref type="bibr" coords="4,147.72,428.32,10.69,8.97" target="#b6">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Sparse Column-Row Approximation</head><p>The performance of EDLSI suggested to us that the PSVD approximation that forms the basis for LSI may not be the best approximation for use in the context of information retrieval. EDLSI provides improvement to LSI results using a weighted combination of LSI and vector-space, and the best results are seen when the weighting of LSI is small, e.g., x = 0.1 or 0.2. However, the vast proportion of the work done in EDLSI in the computation of the LSI component.</p><p>The PSVD approximation used for LSI is well known to be the optimal rank-k approximation to A, such that the difference</p><formula xml:id="formula_7" coords="4,117.24,620.24,133.23,12.86">A k -A = U k Σ k V T k -U ΣV T</formula><p>, where . can be the Frobenius norm or the 2-norm, is minimized. This does not necessarily correspond to optimality for information retrieval. A k is also, in general, not sparse, even if A is. This leads to increased storage needs.</p><p>Stewart proposed in <ref type="bibr" coords="4,175.56,701.20,16.64,8.97" target="#b12">[14]</ref> a matrix approximation called the Sparse Column-Row Approximation (SCRA) based on a quasi-Gram Schmidt method for computing truncated, pivoted QR approximations for sparse matrices. The SCRA factors A into the product of three matrices: Y ∈ ℜ t×s , T ∈ ℜ s×s , and Z ∈ ℜ d×s , where s is the reduced dimension. Then A is approximated as:</p><formula xml:id="formula_8" coords="4,399.72,144.08,52.57,11.41">A ≈ Y T Z T .</formula><p>The matrices Y and Z are formed by taking optimal QR approximations of A and A T respectively, such that Y is composed of columns of A and Z T is composed of rows of A. The matrix T is a dense matrix chosen such that, given Y and Z, the difference</p><formula xml:id="formula_9" coords="4,312.00,232.28,145.10,30.16">Y T Z T -A F is minimized.</formula><p>The code accepts either a dimension s, or a tolerance τ that determines s. The matrices Y and Z are built up column by column, incorporating at each step the most important column of A or A T , respectively. When the norm of the discarded portion of the matrix is smaller than the tolerance, or the reduced dimension s is reached, the factorization is finished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our indexing of the dataset resulted in a term-document matrix of 486 654 terms and 6 827 940 documents. We divided this into 81 pieces of approximately 84 000 documents each for ease in loading and working with the matrix.</p><p>Optimization runs were done for each of the three runs, using the set of document judgements given. These judgements resulted in a term-document matrix of 486 654 terms and 20 090 documents. When forming the matrix of relevance judgements, documents assigned relevance ratings of 0, -1, and -2, as well as documents that were not judged for the query in question, were all assigned as non-relevant. Ratings of highly relevant and relevant were assigned as given. The optimization runs were performed for a range of parameters, and the parameters that gave the highest 11-point average precision, rounded to 2 decimal places, for all relevant documents were chosen as optimal, and were used for the full runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSI with Folding-In</head><p>The first run we submitted used LSI with the folding-in algorithm for new documents. For the optimization run, the judged dataset was divided into an intial dataset of 10 000 documents, with increments of 500 documents each. The PSVD decomposition was performed on the initial set, and each increment was then folded-in. The run tested k = 100 to 1000 in steps of 100 at first, then from 200 to 400 in steps of 10. The optimal value of k was 220, with an 11 point average precision of 0.19 for all relevant documents and 0.09 for highly relevant documents.</p><p>For the full run, the same approach was taken. A PSVD was performed on the first of the 81 pieces of the full set, then the remaining 80 were folded-in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distributed EDLSI</head><p>This run treated each division of the matrix as a separate collection, and performed EDLSI on each. The optimization run divided the judged dataset into 80 pieces, and after EDLSI was perfomed on each, the scores were collected and ranked overall. We intended to test values of k from 5 to 100 in increments of 5, and values of x from 0.1 to 0.5 in increments of 0.1. However, after seeing results up to k = 45, with the results steadily worsening, and being short on time, we stopped there. The optimal combination was k = 5 and x = 0.1, with an 11 point average precision of 0.20 for all relevant documents and 0.10 for highly relevant documents.</p><p>The full run proceeded similarly, with the 81 pieces of the matrix treated individally, then the results combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distributed SCRA-Based LSI</head><p>Our final run made use of the SCRA to approximate the term-document matrix instead of the SVD. The run was fully distributed, with LSI performed on each piece, but using the SCRA in place of the PSVD. To optimize, the judged matrix was divided into 80 pieces. Instead of inputing a dimension s, we used a tolerance, which we calculated relative to the norm of the full matrix A. We tested tolerance parameters of 0.05 to 0.3 in steps of 0.05, and the absolute tolerance was computed by multiplying the relative tolerance with the norm of A. The optimal value for the tolerance was 0.10, with an 11 point average precision of 0.22 for all relevant documents and 0.12 for highly relevant documents.</p><p>The full dataset was treated similarly. Unfortunately, with 81 partitions and a tolerance of 0.10, the run would not have completed in time for submission. For this reason, the 81 pieces were each further subdivided into 40 partitions, and the tolerance was increased to 0.50. Though we did not test this tolerance, we note that the 11 point average precision for all relevant documents for the highest tested tolerance of 0.30 was only reduced to 0.17. This run was able to finish in time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Setting K and K h</head><p>Teams were required to set values of K and K h for each query, indicating where they believe a balance between precision and recall is achieved. Table <ref type="table" coords="6,225.72,474.52,4.98,8.97" target="#tab_0">1</ref> shows the values we chose for each query. These values were the same for the three runs.</p><p>To set the value of K for each query, we decided to make use of the three runs. The runs used three different algorithms, and any document that was highly ranked by all three should be relevant. With this idea in mind, we calculated for each query the proportion of documents that all three methods had in common when looking only at the first n documents, where n is less than the maximum of 1 500 000 documents that may be submitted. We then chose K to maximize percent documents in common. Figures <ref type="figure" coords="6,241.56,617.80,4.98,8.97" target="#fig_1">4</ref> and<ref type="figure" coords="6,269.88,617.80,4.98,8.97" target="#fig_2">5</ref> show the percent documents in common for queries 1-5 and 6-10 respectively.</p><p>To set K h , we looked at the scores for each query and run. We noticed that for a small number of documents, scores were quite high, but the scores dropped off sharply within the first 1 000 documents. These high scores indicate that the documents are very 'close' in space to the query, and may indicate a higher relevance. Accordingly, we chose K h representing approximately the point where the scores had dropped off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Figure <ref type="figure" coords="6,343.92,497.20,4.98,8.97">6</ref> presents the results for our standard runs. At first glance the chart implies that LSI outperforms EDLSI and SCRA on the competition metric of Estimated F 1 at K. However, when the results are broken down by query, we see that all the performance on individual queries is split almost evenly between the three methods (see Table <ref type="table" coords="6,398.04,568.84,3.71,8.97">7</ref>, which has the best performing method highlighted in each row). These results imply, to our surprise, that there is no clear advantage to any particular method.</p><p>In general the highly relevant results were not very strong, most likely due to a poor choice for the K h value, which we continue to find very difficult to set (see Figure <ref type="figure" coords="6,312.00,652.96,3.59,8.97" target="#fig_5">8</ref>).</p><p>Interestingly, some queries continue to be much more difficult than others. Our results range from a high of 43.21% to a low of .41% (estimated F 1 at K), and we see many instances where estimated recall at K is reasonable (greater than 50%) but precision is so low as to make the  task unreasonable (finding 3% of 1,500,000 documents). We believe more research on determining appropriate K values is necessary. It appears as if retrieving large numbers of documents continues to the best approach for maximizing the competition metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We tested three LSI variants on a series of 10 queries on the TREC Legal batch corpus, the IIT Complex Document Information Processing (IIT CDIP) test collection, which contains approximately 7 million documents (57 GB of uncompressed text). Our early results show that all three methods have similar performance. However, a suboptimal parameter size was used for the SCRA method, due to time constraints. Additional experimentation with this method is warranted to determine if it outperforms EDLSI and LSI with folding-in.</p><p>The current three methods outperformed our LSI methods from 2008. Most of the gains appear to be obtained by selecting much larger K values; however, precision at B (the number of documents retrieved by a boolean query run), which does not depend on K, is much higher this year. !" !# %&amp;%%11 %&amp;%%2 %&amp;%%) 1% ""# ! ## %&amp;22 %&amp;%0(2 %&amp;%(22 3" " # 14 %&amp;('4' %&amp;%%40 %&amp;% 1) "$ # "33$ 3 %2 # %&amp;) 2% %&amp;0'') %&amp;01 ( !! !$"# !$ "333 !$ %0 !"!3 " "$3 !! " "3" %&amp;01') %&amp;2'1) %&amp;0%4) %) ! %&amp;%12) %&amp; 2(( %&amp;%441 # " # !3 33 #3 %' $" " " %&amp;' 11 %&amp; 0(1 %&amp;2 (( ! #" "! #"# 01 %&amp;(2)4 %&amp;%) 2 %&amp;%2 #$ !# #$$ # !" #$ )' $ !! ! " %&amp;'020 %&amp;0(0 %&amp;)02 3 " ! ! 566 # !$! !"3 ## ! ! #"" ! $ ¦ 789@ 89@ 9A © 5 B CDEF G H EIIPQR SR TECQR </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,312.00,139.49,228.02,9.22;3,312.00,151.05,84.49,9.96"><head>Fig. 1 .Fig. 2 .Fig. 3 .</head><label>123</label><figDesc>Fig. 1. LSI vs. vector-space retrieval for the CACM Corpus (r = 3 204).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,77.73,238.02,9.96"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Percent documents in common for queries 1-5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,72.00,77.73,243.06,9.96"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Percent documents in common for queries 6-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,72.00,258.05,106.98,9.22"><head>Fig</head><label></label><figDesc>Fig. 6. Result Summary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,72.00,78.17,150.42,9.22"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Result Summary by Query</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,72.00,286.61,193.21,9.22"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Result Summary for Highly Relevant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,222.72,72.05,166.86,160.36"><head>TABLE 1 K</head><label>1</label><figDesc>and K h values for each query</figDesc><table coords="7,222.72,100.87,166.86,131.54"><row><cell cols="3">Query number K value K h value</cell></row><row><cell>7</cell><cell>1 500 000</cell><cell>500</cell></row><row><cell>51</cell><cell>100 000</cell><cell>300</cell></row><row><cell>80</cell><cell>1 500 000</cell><cell>600</cell></row><row><cell>89</cell><cell>1 500 000</cell><cell>100</cell></row><row><cell>102</cell><cell>600 000</cell><cell>1 000</cell></row><row><cell>103</cell><cell>1 500 000</cell><cell>500</cell></row><row><cell>104</cell><cell>300 000</cell><cell>300</cell></row><row><cell>105</cell><cell>400 000</cell><cell>500</cell></row><row><cell>138</cell><cell>1 500 000</cell><cell>1 000</cell></row><row><cell>145</cell><cell>700 000</cell><cell>1 000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,88.68,714.56,209.71,7.17"><p>This rank is unrelated to the rank of a matrix mentioned below.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,330.24,544.16,209.67,7.17;7,330.24,553.16,209.71,7.17;7,330.24,562.16,34.75,7.17" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m" coord="7,514.80,544.16,25.11,7.17;7,330.24,553.16,67.67,7.17">Modern Information Retrieval</title>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.24,571.04,209.53,7.17;7,330.24,580.04,209.59,7.17;7,330.24,589.04,70.03,7.17" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,499.44,571.04,40.33,7.17;7,330.24,580.04,147.79,7.17">Using linear algebra for intelligent information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>O'brien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,492.00,580.04,44.32,7.17">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="575" to="595" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.24,598.04,209.71,7.17;7,330.24,606.92,209.65,7.17;7,330.24,615.92,209.47,7.17;7,330.24,624.92,70.03,7.17" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,413.88,606.92,126.01,7.17;7,330.24,615.92,7.79,7.17">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,347.76,615.92,188.70,7.17">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,330.24,633.92,209.71,7.17;7,330.24,642.80,209.46,7.17;7,330.24,651.80,209.65,7.17;7,330.24,660.80,88.15,7.17" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<title level="m" coord="7,378.12,633.92,107.74,7.17;7,354.00,642.80,185.70,7.17;7,330.24,651.80,209.65,7.17;7,330.24,660.80,10.40,7.17">The First Text REtrieval Conference (TREC1), National Institute of Standards and Technology Special Publication 500-207</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harmon</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="137" to="152" />
		</imprint>
	</monogr>
	<note>LSI meets TREC: A status report</note>
</biblStruct>

<biblStruct coords="7,330.24,669.68,209.71,7.17;7,330.24,678.68,209.46,7.17;7,330.24,687.68,209.46,7.17;7,330.24,696.68,142.75,7.17" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,385.56,669.68,150.25,7.17">Latent semantic indexing (LSI) and TREC-2</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,409.20,678.68,130.50,7.17;7,330.24,687.68,209.46,7.17;7,330.24,696.68,64.80,7.17">The Second Text REtrieval Conference (TREC2), National Institute of Standards and Technology Special Publication 500-215</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Harmon</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,508.76,209.57,7.17;8,90.24,517.76,209.65,7.17;8,90.24,526.76,209.83,7.17;8,90.24,535.76,199.15,7.17" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,195.96,508.76,103.86,7.17;8,90.24,517.76,166.98,7.17">Taking a new look at the latent semantic analysis approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">R</forename><surname>Jessup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,274.32,517.76,25.57,7.17;8,90.24,526.76,95.39,7.17">Computational Information Retrieval</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="121" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,545.84,209.54,7.17;8,90.24,554.84,209.46,7.17;8,90.24,563.84,109.87,7.17" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,144.48,545.84,155.31,7.17;8,90.24,554.84,12.26,7.17">Essential dimensions of latent semantic indexing (lsi)</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kontostathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,109.80,554.84,189.90,7.17;8,90.24,563.84,85.67,7.17">Proceedings of the 40th Hawaii International Conference on System Sciences -2007</title>
		<meeting>the 40th Hawaii International Conference on System Sciences -2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,574.04,209.65,7.17;8,90.24,582.92,209.78,7.17;8,90.24,591.92,209.59,7.17;8,90.24,600.92,209.62,7.17;8,90.24,609.92,100.03,7.17" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,283.20,574.04,16.69,7.17;8,90.24,582.92,189.40,7.17">Identification of critical values in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kontostathis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">M</forename><surname>Pottenger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,90.24,600.92,184.36,7.17">Foundations of Data Mining and Knowledge Discovery</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ohsuga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Liau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Tsumoto</surname></persName>
		</editor>
		<imprint>
			<publisher>Spring-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="333" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,620.00,209.33,7.17;8,90.24,629.00,209.65,7.17;8,90.24,638.00,59.35,7.17" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,194.64,620.00,104.94,7.17;8,90.24,629.00,102.46,7.17">Large-scale information retrieval with Latent Semantic Indexing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Letsche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,203.52,629.00,67.76,7.17">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="105" to="137" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,648.20,209.54,7.17;8,90.24,657.08,209.71,7.17" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,145.80,648.20,153.99,7.17;8,90.24,657.08,52.07,7.17">Information tools for updating an svd-encoded indexing scheme</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>O'brien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of Tennessee</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="8,90.24,667.28,209.78,7.17;8,90.24,676.28,209.47,7.17;8,90.24,685.28,17.83,7.17" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,196.56,667.28,103.47,7.17;8,90.24,676.28,75.18,7.17">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,174.72,676.28,68.49,7.17">Inf. Process Manage</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,695.36,162.19,7.17" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Smart</surname></persName>
		</author>
		<ptr target="ftp://ftp.cs.cornell.edu/pub/smart/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.24,705.56,209.42,7.17;8,90.24,714.56,209.77,7.17;8,330.24,508.76,144.31,7.17" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,147.24,705.56,152.43,7.17;8,90.24,714.56,189.80,7.17">Four algorithms for the efficient computation of truncated pivoted qr approximations to a sparse matrix</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,288.12,714.56,11.89,7.17;8,330.24,508.76,67.51,7.17">Nu-merische Mathematik</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,330.24,517.76,209.53,7.17;8,330.24,526.76,209.23,7.17;8,330.24,535.76,202.39,7.17" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,442.80,517.76,96.97,7.17;8,330.24,526.76,156.16,7.17">Updating the partial singular value decomposition in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Tougas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Spiteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,493.56,526.76,42.63,7.17;8,330.24,535.76,135.58,7.17">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>ScienceDirect</note>
</biblStruct>

<biblStruct coords="8,330.24,544.64,209.46,7.17;8,330.24,553.64,209.59,7.17;8,330.24,562.64,17.83,7.17" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,431.16,544.64,108.54,7.17;8,330.24,553.64,56.92,7.17">On updating problems in latent semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,393.84,553.64,67.83,7.17">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
