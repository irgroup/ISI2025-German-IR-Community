<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,186.47,97.49,239.07,14.93;1,231.85,114.16,148.29,8.64">Microsoft Research at TREC 2009 Web and Relevance Feedback Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.64,143.18,66.66,10.37"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.35,143.18,71.88,10.37"><forename type="first">Dennis</forename><surname>Fetterly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.60,143.18,59.23,10.37"><forename type="first">Marc</forename><surname>Najork</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.99,143.18,88.43,10.37"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,430.62,143.18,68.74,10.37"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,186.47,97.49,239.07,14.93;1,231.85,114.16,148.29,8.64">Microsoft Research at TREC 2009 Web and Relevance Feedback Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C7AE6C2F0E1FE5F1ABD71185027BFB5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We took part in the Web and Relevance Feedback tracks, using the ClueWeb09 corpus. To process the corpus, we developed a parallel processing pipeline which avoids the generation of an inverted file. We describe the components of the parallel architecture and the pipeline and how we ran the TREC experiments, and we present effectiveness results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report is from a transatlantic team from Microsoft Research, comprising members of the Silicon Valley and Cambridge UK labs. We processed the billion-page ClueWeb09 corpus using a framework for data-parallel computation running on a cluster of 240 machines, as described in section. 2. Features used in our runs included BM25 <ref type="bibr" coords="1,542.55,362.77,11.59,9.46" target="#b6">[7]</ref>, BM25F <ref type="bibr" coords="1,90.70,376.32,16.72,9.46" target="#b9">[10]</ref>, SALSA-SETR <ref type="bibr" coords="1,181.94,376.32,11.59,9.46" target="#b4">[5]</ref>, matching anchor counts, tf-idf, various language model scores, PageRank <ref type="bibr" coords="1,524.05,376.32,11.59,9.46" target="#b5">[6]</ref>, and inter-domain in-degree; further details are provided in section 3. Some tuning was done using home-grown evaluation data, described in sections 4. We used the same data-processing setup to take part in the Web track (section 5) and the Relevance Feedback track (section 6), although in the latter case we were unable to complete any significant work in time for the deadlines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Computational Infrastructure</head><p>We processed the corpus using DryadLINQ <ref type="bibr" coords="1,250.39,490.47,11.59,9.46" target="#b8">[9]</ref>, a platform for data-parallel computation that empowers programmers to utilize large clusters of computers without having to worry about the intricacies of distributed systems. DryadLINQ programs are written in C# utilizing LINQ (Language INtegrated Queries), a declarative query language in the SQL tradition. The bulk of a typical program consists of straightforward, sequential C# statements, with LINQ expressions embedded to process relational data. As the program is executed, the DryadLINQ system ships each LINQ expression to all machines in the cluster, where the query is processed in parallel, and the results are being sent back to the machine running the sequential C# program. The following C# method for computing document frequencies for all query terms provides a flavor of how we used DryadLINQ: To compute certain link-based features (most notably SALSA), we used the Scalable Hyperlink Store <ref type="bibr" coords="2,533.46,57.86,11.59,9.46" target="#b3">[4]</ref>, a special-purpose storage system for web graphs. SHS keeps a compressed version of the web graph in main memory, distributed over a cluster of machines. It provides a simple API to its clients that hides the details of data distribution, fault tolerance, etc. We created an SHS database over 12 servers containing the inter-domain web graph induced by the ClueWeb09 corpus, totaling 4.8 billion nodes and 5.2 billion edges.</p><p>Finally, we computed PageRank scores using a custom distributed program. We could have used DryadLINQ as well, but we happened to have a pre-existing implementation that could handle the ClueWeb09 corpus quite easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Processing pipeline</head><p>Our processing pipeline consisted of the following stages:</p><p>1. Parsing: We used DryadLINQ to parse the HTML web pages, tokenizing them into individual words and hyperlinks. We associated each title and content word occurrence with the document containing it; in addition, we associated each anchor word occurrence with the document referenced by the anchor's hyperlink.</p><p>2. Document Frequencies: We computed DFs for the union of all terms of the 172 (training and test) queries we were considering.</p><p>3. Initial scoring: Still in DryadLINQ, we computed a single score for each query and each document in the collection, retaining the top 5000 results per query. For the Web track, the single score was BM25F and the process was run on the entire category A collection. For the Relevance Feedback track, the single score was plain BM25 and the process was run only on the restricted category B set; on the same pass, however, tf-idf and language model scores were also calculated.</p><p>4. Inter-domain in-degree (IDID): We used SHS to compute the inter-domain in-degree (IDID) for each of the top-5000 (according to BM25F) results of each query, that is, the number of pages in different domains linking to the result.</p><p>5. SALSA: Furthermore, we used SHS to run SALSA-SETR <ref type="bibr" coords="2,337.53,433.63,12.72,9.46" target="#b4">[5]</ref> on the top-5000 results of each query. SALSA-SETR is a variant of Lempel &amp; Moran's SALSA algorithm <ref type="bibr" coords="2,340.28,447.18,11.59,9.46" target="#b2">[3]</ref>, which in turn is a variant of Kleinberg's HITS algorithm <ref type="bibr" coords="2,126.43,460.73,11.59,9.46" target="#b1">[2]</ref>, one of the best-known link-based ranking algorithms.</p><p>6. PageRank: We computed PageRank scores for each page in the ClueWeb09 corpus, using a special-purpose distributed program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Matching anchor count (MAC):</head><p>We resolved all the symbolic hostnames in ClueWeb09 URLs/links into IP addresses. Then using DryadLINQ we identified unique s, t, a triples, where s is the IP address of a document, t is the target URL of a link within that document and a is the anchor text of that link. Then we built the MAC ranking feature for each query-target pair q, t , counting the number of source IPs s that link to target t with anchor a = q.</p><p>8. Extraction: For each query we identified a pool of documents and collated our ranking features. The resulting "extraction" file could be used for training or for generating submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.</head><p>Training: Using an extraction of training queries and some surrogate qrels (see Section 4), we trained a singlelayer LambdaRank model <ref type="bibr" coords="2,197.31,645.63,11.59,9.46" target="#b0">[1]</ref>.</p><p>10. Runs: Using an extraction of the 50 test queries, we ran our trained model to produce our submitted runs.</p><p>Various free parameters of our methods, including the BM25F parameters, those used in SALSA-SETR, and the various weighted linear combinations) needed to be tuned in some way. The basic approach to this was two-fold. We selected 118 queries from a search engine log as a training set. We ran these queries against the Bing search engine, and used Bing ranks to define a set of relevance labels, on the assumption that the Bing results were good. Rather than using the entire top-5000 BM25F as our training documents, we first take the intersection between top-5000 and the Bing top-10 ('Good') documents. We then add random ('Bad') documents to get a pool of 10 documents. This is further augmented by adding top-5000 documents that are in the top-4 on one of several features: BM25F, SALSA-SETR, PageRank and Anchor We then manually assessed all the pooled query-result pairs, using a 3-point scale. When Bing ranks are used to assign relevance labels to documents, documents retrieved at rank 1 were assumed to be perfect, ranks 2-3 excellent, 4-5 good, 6-10 fair, and documents retrieved lower down are assumed to be nonrelevant. We used both the Bing-induced labels and the manual assessments to guide the tuning of the free parameters. We trained a single layer LambdaRank model for 300 epochs and we used a separate validation set to pick the best performing epoch. When the manual labels are used for training, the Bing-induced labels are used as the validation set and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Web track submissions</head><p>We submitted three runs to the diversity task:</p><p>MSDiv1 A linear combination of BM25F, IDID, SALSA and PageRank. Manual labels are used as the training set and the Bing-induced labels are used as the validation set.</p><p>MSDiv2 A linear combination of BM25F, IDID, SALSA, PageRank and MAC. Manual labels are used as the training set and the Bing-induced labels are used as the validation set.</p><p>MSDiv3 A linear combination of BM25F, PageRank and MAC. We attempted to perform "host collapsing" -including only the two highest-scoring results from each host -so as to increase result diversity.</p><p>We also submitted MSDiv1 as MS1 and MSDiv2 as MS2 to the ad-hoc task. Table <ref type="table" coords="3,419.23,463.05,5.45,9.46" target="#tab_0">1</ref> shows the performance of our submitted runs. MSDiv2 and MSDiv3 (which both include the MAC feature) clearly outperform MSDiv1 (which does not include MAC), and their relative ordering varies depending on which performance measure is considered.</p><p>Run-ID αnDCG@5 αnDCG@10 αnDCG@20 IA-P@5 IA-P@10 IA-P@20 MSDiv1/MS1 After the submission and while performing additional analysis for the final version of this paper, we discovered that our "host collapse" implementation used in preparing the MSDiv3 submission contained a bug -it excluded many more results than it should have. Table <ref type="table" coords="3,267.48,640.61,5.45,9.46" target="#tab_1">2</ref> shows how a bug-free implementation would have performed. The four rows in the table illustrate the effect of including the top one, two, three, or all results for a given host. We can draw three conclusions from this data: first, fixing the bug in the host collapse implementation improved performance; second, showing only two results per host produces only a mild improvement under some measures (e.g. αnDCG@5) and is detrimental under others; and third, the ranking function that produced MSDiv3 without host collapse (which used only three features) did better than the five-feature function that produced MSDiv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-ID</head><p>αnDCG@5 αnDCG@10 αnDCG@20 IA-P@5 IA-P@10 IA-P@20 MSDiv3( <ref type="formula" coords="4,137.83,70.73,5.56,9.46">1</ref> In order to understand why a ranking function incorporating fewer features did better, we measured the performance of each of the five features in isolation. Table <ref type="table" coords="4,281.35,192.99,5.45,9.46" target="#tab_2">3</ref> shows the results. According to the table, the matching anchor count feature in isolation performs about as well as any of the ranking functions in Table <ref type="table" coords="4,440.96,206.54,5.45,9.46" target="#tab_1">2</ref> for low document cut-off values, and not much worse for higher cut-offs. BM25F also performed quite well, but worse than MAC. Among the link-based features, inter-domain in-indegree outperformed SALSA and PageRank -which greatly surprised us, since earlier work suggested that SALSA should significantly outperform in-degree and PageRank <ref type="bibr" coords="4,486.23,247.19,11.59,9.46" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-ID</head><p>αnDCG@5 αnDCG@10 αnDCG@20 IA-P@5 IA-P@ Inspection of scores and judgments of a few queries revealed why the link-based features performed counter to previous observations: Many of the results with high SALSA scores were not judged (the same is true for PageRank, albeit to a lesser degree). In order to account for this fact, we devised a simple measure that is inspired by the standard precision measure, but ignores unjudged results: we consider the twenty highest-scoring results and simply divide the number of relevant results by the number of judged results. Table <ref type="table" coords="4,366.36,451.84,5.45,9.46" target="#tab_3">4</ref> shows the number of judged and relevant results together with their ratio, for each individual feature. Under this measure, SALSA performs best, closely followed by matching anchor count. We hypothesize that SALSA performs differently under the relevant/judged measure versus standard measures because it is a fairly orthogonal feature: It surfaces many results that are not surfaced by other features. The judgment process was driven by the pooling of submitted runs, and we did not submit any runs using SALSA as the sole feature. Finally, Table <ref type="table" coords="4,132.85,672.25,5.45,9.46" target="#tab_4">5</ref> shows the number of judged and relevant results together with their ratio, for the three submitted runs (bottom rows) as well as MSDiv3 with the host-collapse bug fixed (top rows). Under this measure, MSDiv3 slightly outperforms MSDiv2 and greatly outperforms MSDiv1 (consistent with Table <ref type="table" coords="4,439.27,699.35,3.94,9.46" target="#tab_0">1</ref>), fixing the host-collapse bug improves performance (consistent with Table <ref type="table" coords="4,271.63,712.90,3.94,9.46" target="#tab_1">2</ref>), and SALSA and MAC in isolation perform better than the best parametrization of MSDiv3. There are two plausible explanations for this: one, the relevant/judged measure may become more fragile as the fraction of unjudged results goes up; and two, we may have combined features in a less than optimal way in our submissions. In order to check whether we combined features in a suboptimal way, we performed two sets of experiments pertaining to combination of evidence. First, we experimented with weighted linear combinations of BM25F and MAC, i.e. score = BM25F + w • MAC. We found that performance plateaued for w ≥ 2, as shown in the top half of Table <ref type="table" coords="5,92.68,304.08,4.09,9.46" target="#tab_5">6</ref>. We also tried applying a log-based transform to MAC, i.e. score = BM25F + w • log MAC, and found that performance plateaued for w ≥ 1, as shown in the bottom half of Table <ref type="table" coords="5,384.12,317.63,4.09,9.46" target="#tab_5">6</ref>. In other words, MAC is the dominant ranking signal, and BM25F serves as a "tie-breaker". w αnDCG@5 αnDCG@10 αnDCG@20 IA-P@5 IA-P@10 IA-P@20 0. Next, we tried three-way combinations using the scoring function score = BM25F + MAC + w • log L, with L being one of the link-based features PageRank, IDID and SALSA. We found that the performance of each three-way combination showed a sharp peak for a particular value of w, unlike what we observed for the two-way combination of BM25F and MAC. Table <ref type="table" coords="5,180.68,618.05,5.45,9.46" target="#tab_6">7</ref> shows the results for the best-performing values of w. Adding SALSA as the third feature increased performance more than either PageRank or IDID did. Comparing PageRank and IDID, PageRank contributed more under αnDCG@5 and IA-P@5, while IDID was preferable at higher document cut-off values. We also note that the fairly simple combination of MAC, BM25F and SALSA performs substantially better than any of our submitted runs (cf. Table <ref type="table" coords="5,184.02,672.25,3.94,9.46" target="#tab_0">1</ref>), even after discounting the "host-collapse bug" (cf. Table <ref type="table" coords="5,449.01,672.25,3.94,9.46" target="#tab_1">2</ref>). In other words, when preparing our submitted runs, we did indeed combine features in a suboptimal fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Finally, Table <ref type="table" coords="5,133.25,699.35,5.45,9.46" target="#tab_7">8</ref> shows the performance of three-way combinations of evidence when ignoring unjudged results. The combination of SALSA, BM25F and MAC performs best as expected. Two things to note are that the best score = BM25F + MAC + . . . αnDCG@5 αnDCG@10 αnDCG@20 IA-P@5 IA-P@10 IA-P@20 . . . performance is higher than that of MSDiv3(3) (cf. Table <ref type="table" coords="6,312.59,153.94,4.54,9.46" target="#tab_4">5</ref>) as well as that of SALSA in isolation (cf. Table <ref type="table" coords="6,546.18,153.94,3.94,9.46" target="#tab_3">4</ref>), further supporting the notion that MSDiv3 combines features in a suboptimal way. Second, performance is maximal for w ≈ 5000 (as opposed to w ≈ 500 in Table <ref type="table" coords="6,267.96,181.03,4.09,9.46" target="#tab_6">7</ref>. This is because giving higher weight to SALSA draws in more unjudged results, which is being penalized by the metrics used in Table <ref type="table" coords="6,367.33,194.58,5.45,9.46" target="#tab_6">7</ref> but not those used in Table <ref type="table" coords="6,495.02,194.58,4.09,9.46" target="#tab_7">8</ref>.</p><p>Weight  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Relevance feedback track submissions</head><p>In the first phase of the Relevance Feedback Track, we used BM25, tf-idf and language modeling parameters computed on the Category B collection as features. To pick the feedback documents, a baseline ranker was obtained by forming a weighted linear combination of these parameters. The feedback documents were picked using two different methodologies:</p><p>MSRC1 Top 5 documents retrieved by our baseline ranker are submitted for feedback.</p><p>MSRC2 Top 20 documents were extracted using our baseline ranker. The top document is always picked for feedback. The second document picked for feedback is the one that is the least similar of the remaining documents to the first document, and so on. The similarities between different documents are computed using cosine similarity as the metric.</p><p>For the second phase of the track, our intention was to use a relevance feedback model based on manifold regularization and document contents. Using some previous training set (formed from the Bing-induced labels or manual assessments), we use a regression based model to assign some initial weight to each of our features. Given the feedback documents for each query, we would then like to update these weights separately for each query as different features may be important for different queries. Hence, we learn a different ranker separately for each query. This could be done by updating the initial weights using regression on the feedback documents. However, since the number of judged documents is very limited, using simple regression on the feedback documents will easily run into the problem of overfitting. To overcome this problem, we add a regularizer to the regression model to include the information provided by thousands of unjudged documents. The regression model guarantees that the feature weights are updated so that the scores of the feedback documents match their labels, and the regularizer guarantees that the documents that are similar to each other in terms of contents are ranked similarly.</p><p>Figure <ref type="figure" coords="7,102.97,590.96,5.45,9.46" target="#fig_1">1</ref> shows the result of our initial experiments using data from TREC 6, 7 and 8 Ad-hoc retrieval track. Collectively, this dataset contains 150 queries and is split into five parts in order to conduct five-fold cross validation. For each fold, we use one part for training the baseline ranker, one fold for validation and three folds for testing our relevance feedback algorithm. The left and right plots in the figure shows how relevance feedback using manifold regularization compares with the quality of the baseline ranker using top 5 and 10 documents from the baseline ranker as the feedback documents, respectively. The x axis in the figure show the fold number and the y axis shows the average precision value. It can be seen that using the manifold regularization based relevance feedback algorithm, one can obtain significant improvements over the baseline ranker. The differences are statistically significant according to a Wilcoxon sign-rank test at p = 0.5 significance level. The details of both the experiments and the manifold regularization based relevance feedback algorithm will be published elsewhere.</p><p>Unfortunately, we were unable to finish the formulation of this algorithm in time for the Relevance Feedback Track. Instead we submitted one run using a single feedback set (from one of the other participants) and a method inspired by the usual Rocchio approach <ref type="bibr" coords="8,235.06,84.96,11.59,9.46" target="#b7">[8]</ref>. We extract the top 50 terms from the positive (relevant) documents according to their term frequencies (largest sum of term frequencies over the relevant set). We then alter the query using these terms, retrieve a new list of documents according to BM25 and language modeling features for the altered query. If s 1 is the score of a document in the original ranked list and s 2 is the score of a document in the ranked list for the altered query, we now merge the two results lists using a score of s 1 + 0.02 * s 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Although most TREC participants use systems based on the usual inverted-file structure, there have been occasional forays into different experimental infrastructures (at the first TREC, for example, there was one system based on a hardware text scanner, another on passing documents sequentially for matching against an inverted file of queries). For this submission we have begun experimenting with a new setup which avoids inverted-file building. As a preliminary impression, it appears to provide a flexible environment in which to do a range of experiments. But we need to do many more to discover its benefits and limitations. As regards the value of the specific approaches to the Web and Relevance Feedback tasks, we have yet to perform the necessary analyses and further experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,54.00,609.34,419.63,6.31;1,64.76,619.31,441.15,6.31;1,64.76,629.27,150.64,6.31;1,129.32,639.23,177.53,6.31;1,129.32,649.19,166.78,6.31;1,129.32,659.16,64.56,6.31;1,64.76,669.12,220.57,6.31;1,145.46,679.08,134.50,6.31;1,145.46,689.04,252.85,6.31;1,64.76,699.01,204.43,6.31;1,54.00,708.97,5.38,6.31"><head></head><label></label><figDesc>static void DFs(HashSet&lt;string&gt; queryTerms, string docsURL, string freqsURL) { PartitionedTable&lt;TextDocument&gt; docs = PartitionedTable.Get&lt;TextDocument&gt;(docsURL); var words = from doc in docs from word in doc.words.Distinct() where queryTerms.Contains(word) select word; var docFreqs = from term in words.Merge() group term by term into g select new Pair&lt;string, int&gt;(g.Key, g.Count()); docFreqs.ToPartitionedTable(freqsURL); }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,54.00,227.85,504.00,9.46;7,54.00,241.40,187.88,9.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Improvements over the 5 folds when the feedback documents are (left) the top 5 documents, and (right) the top 10 documents in the initial ranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,95.88,528.30,422.23,59.42"><head>Table 1 :</head><label>1</label><figDesc>Performance of of web track submissions</figDesc><table coords="3,95.88,528.30,422.23,36.56"><row><cell></cell><cell>0.178</cell><cell>0.234</cell><cell>0.275</cell><cell>0.083</cell><cell>0.098</cell><cell>0.091</cell></row><row><cell>MSDiv2/MS2</cell><cell>0.267</cell><cell>0.306</cell><cell>0.352</cell><cell>0.130</cell><cell>0.109</cell><cell>0.102</cell></row><row><cell>MSDiv3</cell><cell>0.268</cell><cell>0.309</cell><cell>0.346</cell><cell>0.127</cell><cell>0.117</cell><cell>0.105</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,98.92,70.73,416.16,72.97"><head>Table 2 :</head><label>2</label><figDesc>Performance of MSDiv3, after fixing bug in "host-collapse" code</figDesc><table coords="4,98.92,70.73,416.16,50.11"><row><cell>)</cell><cell>0.279</cell><cell>0.317</cell><cell>0.352</cell><cell>0.129</cell><cell>0.117</cell><cell>0.104</cell></row><row><cell>MSDiv3(2)</cell><cell>0.285</cell><cell>0.321</cell><cell>0.359</cell><cell>0.136</cell><cell>0.121</cell><cell>0.109</cell></row><row><cell>MSDiv3(3)</cell><cell>0.283</cell><cell>0.321</cell><cell>0.359</cell><cell>0.137</cell><cell>0.120</cell><cell>0.110</cell></row><row><cell>MSDiv3(∞)</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,104.62,271.39,404.75,100.46"><head>Table 3 :</head><label>3</label><figDesc>Performance of individual features</figDesc><table coords="4,443.20,271.39,66.17,9.46"><row><cell>10 IA-P@20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,132.02,543.78,347.96,100.47"><head>Table 4 :</head><label>4</label><figDesc>Performance of individual features, ignoring results without judgments</figDesc><table coords="4,200.23,543.78,211.55,77.60"><row><cell>-ID</cell><cell cols="3">judged relevant relevant/judged</cell></row><row><cell>SALSA</cell><cell>6.96</cell><cell>2.50</cell><cell>0.359</cell></row><row><cell>MAC</cell><cell>15.78</cell><cell>5.58</cell><cell>0.354</cell></row><row><cell>BM25F</cell><cell>15.30</cell><cell>4.20</cell><cell>0.275</cell></row><row><cell>IDID</cell><cell>12.36</cell><cell>3.04</cell><cell>0.246</cell></row><row><cell>PageRank</cell><cell>9.72</cell><cell>2.32</cell><cell>0.239</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,154.61,108.31,302.78,127.96"><head>Table 5 :</head><label>5</label><figDesc>Performance of our runs, ignoring results without judgments</figDesc><table coords="5,191.49,108.31,229.03,105.10"><row><cell>-ID</cell><cell cols="3">judged relevant relevant/judged</cell></row><row><cell>MSDiv3(3)</cell><cell>19.80</cell><cell>6.62</cell><cell>0.334</cell></row><row><cell>MSDiv3(2)</cell><cell>19.64</cell><cell>6.56</cell><cell>0.334</cell></row><row><cell>MSDiv3(∞)</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row><row><cell>MSDiv3(1)</cell><cell>19.34</cell><cell>6.28</cell><cell>0.325</cell></row><row><cell>MSDiv3</cell><cell>19.96</cell><cell>6.32</cell><cell>0.317</cell></row><row><cell>MSDiv2/MS2</cell><cell>19.98</cell><cell>6.24</cell><cell>0.312</cell></row><row><cell>MSDiv1/MS1</cell><cell>19.94</cell><cell>5.46</cell><cell>0.274</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,112.03,368.49,387.95,181.76"><head>Table 6 :</head><label>6</label><figDesc>Combining BM25F and MAC</figDesc><table coords="5,112.03,368.49,387.95,158.90"><row><cell>5</cell><cell>0.273</cell><cell>0.307</cell><cell>0.356</cell><cell>0.133</cell><cell>0.114</cell><cell>0.108</cell></row><row><cell>1</cell><cell>0.274</cell><cell>0.319</cell><cell>0.361</cell><cell>0.133</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>2</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>10</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>100</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>1000</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>10000</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>0.1</cell><cell>0.265</cell><cell>0.309</cell><cell>0.355</cell><cell>0.126</cell><cell>0.115</cell><cell>0.105</cell></row><row><cell>1</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>10</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>100</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row><row><cell>1000</cell><cell>0.283</cell><cell>0.320</cell><cell>0.363</cell><cell>0.139</cell><cell>0.118</cell><cell>0.109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,55.54,70.38,500.50,59.77"><head>Table 7 :</head><label>7</label><figDesc>Combining BM25F and MAC with one of SALSA, IDID or PageRank</figDesc><table coords="6,55.54,70.38,500.50,36.91"><row><cell>500 • log SALSA</cell><cell>0.291</cell><cell>0.331</cell><cell>0.375</cell><cell>0.138</cell><cell>0.122</cell><cell>0.112</cell></row><row><cell>. . . 0.1 • log IDID</cell><cell>0.279</cell><cell>0.328</cell><cell>0.372</cell><cell>0.134</cell><cell>0.120</cell><cell>0.109</cell></row><row><cell>. . . 10 6 • log PageRank</cell><cell>0.285</cell><cell>0.322</cell><cell>0.364</cell><cell>0.139</cell><cell>0.117</cell><cell>0.109</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,59.59,218.78,492.82,412.49"><head>Table 8 :</head><label>8</label><figDesc>Combining BM25F and MAC with SALSA, IDID or PageRank, and ignoring results without judgments</figDesc><table coords="6,206.64,218.78,198.72,389.63"><row><cell></cell><cell cols="3">judged relevant relevant/judged</cell></row><row><cell cols="3">BM25F + MAC + w log SALSA</cell><cell></cell></row><row><cell>100</cell><cell>19.76</cell><cell>6.64</cell><cell>0.336</cell></row><row><cell>500</cell><cell>19.04</cell><cell>6.72</cell><cell>0.353</cell></row><row><cell>1000</cell><cell>18.44</cell><cell>6.56</cell><cell>0.356</cell></row><row><cell>5000</cell><cell>13.86</cell><cell>5.16</cell><cell>0.372</cell></row><row><cell>10000</cell><cell>11.16</cell><cell>3.98</cell><cell>0.357</cell></row><row><cell></cell><cell cols="2">BM25F + MAC + w log IDID</cell><cell></cell></row><row><cell>0</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row><row><cell>0.001</cell><cell>19.88</cell><cell>6.64</cell><cell>0.334</cell></row><row><cell>0.01</cell><cell>19.88</cell><cell>6.66</cell><cell>0.335</cell></row><row><cell>0.05</cell><cell>19.54</cell><cell>6.66</cell><cell>0.341</cell></row><row><cell>0.1</cell><cell>19.04</cell><cell>6.56</cell><cell>0.345</cell></row><row><cell>0.2</cell><cell>18.82</cell><cell>6.38</cell><cell>0.339</cell></row><row><cell>0.5</cell><cell>17.22</cell><cell>5.54</cell><cell>0.322</cell></row><row><cell>1</cell><cell>15.50</cell><cell>4.54</cell><cell>0.293</cell></row><row><cell cols="4">BM25F + MAC + w log PageRank</cell></row><row><cell>10 9</cell><cell>18.46</cell><cell>5.90</cell><cell>0.320</cell></row><row><cell>10 8</cell><cell>19.82</cell><cell>6.66</cell><cell>0.336</cell></row><row><cell>10 7</cell><cell>19.86</cell><cell>6.64</cell><cell>0.334</cell></row><row><cell>10 6</cell><cell>19.86</cell><cell>6.64</cell><cell>0.334</cell></row><row><cell>10 5</cell><cell>19.84</cell><cell>6.62</cell><cell>0.334</cell></row><row><cell>10 4</cell><cell>19.84</cell><cell>6.62</cell><cell>0.334</cell></row><row><cell>10 3</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row><row><cell>10 2</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row><row><cell>10 1</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row><row><cell>10 0</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row><row><cell>0</cell><cell>19.86</cell><cell>6.62</cell><cell>0.333</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,77.63,339.65,480.37,9.64;8,77.63,353.20,374.67,9.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,249.44,339.83,213.50,9.46">Learning to rank with non-smooth cost functions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,484.26,339.65,73.73,9.39;8,77.63,353.20,271.54,9.39">Proc. of the 20th Annual Conference on Neural Information Processing Systems</title>
		<meeting>of the 20th Annual Conference on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,375.18,480.37,9.64;8,77.63,388.73,255.03,9.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,154.54,375.37,223.91,9.46">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,400.57,375.18,157.43,9.39;8,77.63,388.73,151.90,9.39">Proc. of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,410.90,480.37,9.46;8,77.63,424.26,282.11,9.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,197.16,410.90,356.94,9.46">The stochastic approach for link-structure analysis (SALSA) and the TKC effect</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,77.63,424.26,168.97,9.39">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1-6</biblScope>
			<biblScope unit="page" from="387" to="401" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,446.25,480.37,9.64;8,77.63,459.98,84.84,9.46" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,129.88,446.43,122.11,9.46">The scalable hyperlink store</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,271.88,446.25,280.97,9.39">Proc of the 20th ACM Conference on Hypertext and Hypermedia</title>
		<meeting>of the 20th ACM Conference on Hypertext and Hypermedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,481.97,480.37,9.46;8,77.63,495.33,480.37,9.64;8,77.63,509.07,68.18,9.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,274.66,481.97,283.34,9.46;8,77.63,495.52,69.73,9.46">Less is More: Sampling the neighborhood graph makes SALSA better and faster</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,169.34,495.33,355.46,9.39">Proc of the 2nd ACM International Conference on Web Search and Data Mining</title>
		<meeting>of the 2nd ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,531.05,480.37,9.46;8,77.63,544.60,140.59,9.46" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,243.20,531.05,259.90,9.46">The PageRank citation ranking: bringing order to the web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,77.63,566.40,480.36,9.64;8,77.63,579.95,162.03,9.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,409.10,566.58,74.69,9.46">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,506.61,566.40,51.38,9.39;8,77.63,579.95,129.97,9.39">Proc. of the 3rd Text REtrieval Conference</title>
		<meeting>of the 3rd Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,601.93,480.37,9.64;8,77.63,615.48,480.37,9.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,144.47,602.12,193.26,9.46">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,460.71,601.93,97.29,9.39;8,77.63,615.48,247.50,9.39">The SMART Retrieval System -Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood, Cliffs, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,635.27,480.37,11.84;8,77.63,651.02,480.37,9.64;8,77.63,664.56,356.77,9.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,441.82,637.65,116.18,9.46;8,77.63,651.20,349.28,9.46">DryadLINQ: a system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ú</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,448.86,651.02,109.14,9.39;8,77.63,664.56,269.91,9.39">Proc. of the 8th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 8th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,77.63,686.73,480.37,9.46;8,77.63,700.10,296.45,9.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,362.21,686.73,195.79,9.46;8,77.63,700.28,58.30,9.46">Microsoft Cambridge at TREC-13: Web and HARD tracks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,155.78,700.10,186.24,9.39">Proc. of the 13th Text Retrieval Conference</title>
		<meeting>of the 13th Text Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
