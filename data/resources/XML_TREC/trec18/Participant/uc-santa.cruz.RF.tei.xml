<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.26,72.35,303.19,16.84">UCSC at Relevance Feedback Track</title>
				<funder ref="#_gwc7Yhr">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,96.25,118.05,67.57,11.06"><forename type="first">Lanbo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName coords="1,173.35,118.05,47.60,11.06"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,264.96,118.05,79.79,11.06;1,344.75,118.67,1.27,5.24"><forename type="first">Jadiel</forename><surname>De Arma</surname></persName>
							<email>jadielam@gmail.com</email>
						</author>
						<author>
							<persName coords="1,434.16,118.05,33.90,11.06"><forename type="first">Kai</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering UC Santa Cruz</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Arizona State University Phoenix</orgName>
								<address>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">NEC Laboratories America Cupertino</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.26,72.35,303.19,16.84">UCSC at Relevance Feedback Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DFA207B01BD9A05C357FF6163076402</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The relevance feedback track in TREC 2009 focuses on two sub tasks: actively selecting good documents for users to provide relevance feedback and retrieving documents based on user relevance feedback. For the first task, we tried a clustering based method and the Transductive Experimental Design (TED) method proposed by Yu et al. <ref type="bibr" coords="1,260.11,276.81,9.20,7.86" target="#b5">[5]</ref>. For clustering based method, we use the K-means algorithm to cluster the top retrieved documents and choose the most representative document of each cluster. The TED method aims to find documents that are hard-to-predict and representative of the unlabeled documents. For the second task, we did query expansion based on a relevance model learned on the relevant documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We assume good documents for relevance feedback should firstly be relevant, or near relevant, since prior research has shown that relevant documents are more informative than non-relevant ones for commonly used relevance feedback algorithms. Thus a simple idea of selecting documents for relevance feedback is just to choose the top retrieved documents. However, this approach may return similar documents and thus suffers from two problems: diminishing of return and all eggs in one basket. Let's consider one case where the top 5 documents are very similar to each other (or near duplicates). Knowing all of them are relevant won't be more valuable than knowing just one of them is relevant, which is the so-called "diminishing of return" problem. Meanwhile, if one of them is non-relevant, usually all of them are non-relevant, which is the so-called "all eggs in one basket" problem. To avoid these problems, we hope to diversify the selected document set so that we have more chances of finding relevant documents and learning more relevant factors or terms for query expansion. Motivated by the above intuitions, we tried two approaches to find good documents for relevance feedback in our experiments. One approach is based on document clustering, and the other approach is based on Transductive Experimental Design <ref type="bibr" coords="1,234.52,620.92,9.20,7.86" target="#b5">[5]</ref>.</p><p>For the retrieval task, we adapt a language modeling approach based on the Indri (part of lemur) information retrieval toolkit. The language modeling approaches are widely used on several standard information retrieval benchmark data sets, and we want to see how they perform on the new web data ClueWeb09. In our experiments, we go beyond</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CHOOSING DOCUMENTS FOR RELE-VANCE FEEDBACK BASED ON DOCU-MENT CLUSTERING</head><p>There are two motivations for us to choose the clustering approach. First, two similar documents sharing most of the common words won't be more informative than only one of them, and a diverse set of relevant documents can help us to find a more diverse set of relevant terms/factors for query expansion. Second, for an ambiguous query, a group of documents representing different possible user intentions will increase the probability that at least one of them is relevant.</p><p>We use the K-means algorithm to cluster the top 50 retrieved documents into 5 clusters. A document from each cluster C is chosen as follows:</p><formula xml:id="formula_0" coords="1,363.68,416.08,192.25,18.93">dC = arg max d i ∈C d j ∈C similarity(di, dj)<label>(1)</label></formula><p>Each document is represented as a vector, where each dimension is the TFIDF score of the corresponding term m:</p><formula xml:id="formula_1" coords="1,324.44,468.50,231.48,23.51">d(m) = (k1 + 1)tf m,d k1(1 -b + b L d Lavg ) + tf m,d log N -dfm + 0.5 dfm + 0.5<label>(2)</label></formula><p>where tf m,d is the term frequency of term m in document d, L d is the length of d, Lavg is the average document length, N is the total number of documents in the corpus, dfm is the document frequency of term m, and k1, b are two parameters that need to be set manually. Cosine similarity is used to measure the distance between two documents:</p><formula xml:id="formula_2" coords="1,374.64,564.93,181.27,19.74">similarity(di, dj) = di • dj ||di|| ||dj||<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CHOOSING DOCUMENTS FOR RELE-VANCE FEEDBACK BASED ON TRANS-DUCTIVE EXPERIMENTAL DESIGN (TED)</head><p>Transductive Experimental Design <ref type="bibr" coords="1,470.86,638.62,9.71,7.86" target="#b5">[5]</ref> aims to select instances that are hard-to-predict and representative of unlabeled instances. More specifically, TED tries to solve the following optimization problem: min</p><formula xml:id="formula_3" coords="1,351.57,685.63,204.35,34.27">A,α i ∈R K x i ∈X P {|| xi -X T A αi || 2 + µ || αi || 2 } subject to |A| = K, A ∈ C (4)</formula><p>Algorithm 1 Sequential Algorithm Input: XC, XP, µ &gt; 0, K 1: Initialization: αi,j ← xi • xj/ ||xi|| ||xj|| , i ∈ P, j ∈ C 2: repeat: 3: j ← arg maxj∈C i∈P α 2 i,j /(αj,j + µ) 4: A ← A ∪ {j} 5: for i ∈ P, i ∈ C α i,i ← α i,i -αi,jα i ,j /(αj,j + µ) 6: until |A| = K 7: return A where P is the set of all instances, C is the set of candidate instances (i.e. documents), A is the set of selected instances. XP, XC and XA are the the matrix representations of all instances in P, C, A, and K is the number of instances we want to select.</p><p>TED aims to find the optimal set of examples A to approximate each instance xi of XP. The approximation can be seen as the regularized projection of xi onto the linear subspace spanned by XA. Therefore, TED has a geometric interpretation that it tends to find the representative data set XA that span a linear subspace that retains most of the information of the whole set of instances XP. Therefore, given a sufficiently large set XP, TED actually explores the information about the distribution of unlabeled data.</p><p>The optimization problem is NP-hard, and we used the sequential algorithm introduced in <ref type="bibr" coords="2,196.78,330.00,9.72,7.86" target="#b6">[6]</ref> to find a suboptimal solution (Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RETRIEVAL MODELS</head><p>In the second phase, we tried a learning to rank approach (Multiple Addictive Regression Tree) and language models. We use .GOV data set to tune the parameters and evaluate different approaches. Because the settings of the TREC experiments are very different from the standard learning to rank scenario, MART does not perform well on the .GOV data set, thus we didn't submit the corresponding results. Now we briefly discuss the language modeling approach used for generating the submitted run.</p><p>The language modeling approach has been successfully used in standard non web retrieval tasks. We are interested to see whether it can do well on the relevance feedback task with web data. Indri <ref type="bibr" coords="2,146.00,502.26,9.71,7.86" target="#b1">[1]</ref> (in Lemur toolkit) is a standard open source search engine based on language models. It is designed to handle large datasets, and supports a very flexible query language. Given the limited time, we used Indri as our primary search tool throughout our experiments and implemented our retrieval models based on the Indri query language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Model</head><p>In the traditional "bag-of-words" models, the word position information is ignored. However, word position information might be of great value in some cases. For example, for query "the music man", which is a musical name, what the user really wants is information about the musical. Thus a relevant document must contain exactly the phrase "the music man". Besides, intuitively all query words are expected to occur closely in relevant documents, so that their combination makes the document relevant. These motivate us to use both phrase match and text window match in our retrieval models.</p><p>Our retrieval models are based on the multiple Bernoulli In this model, a binary random variable is defined for each feature (which could be a word, a phrase, or a text window) to indicate whether this feature fi is present(fi = 1) or absent(fi = 0) in a document d. We treat each position of this document as a sample from the model where some features have shown up and other features are absent. For example, in a document with content "A B C", the features present in the first position include the word "A", the phrase "AB" (phr(AB)), the phrase "ABA" (phr(ABA)), the text window of length 2 and containing both "A" and "B" (wind2(AB)), the text window of length 3 and containing both "A" and "C" (wind3(AC)), and the text window of length 3 and containing "A", "B", and "C" (wind3(ABC)). The MLE of p(fi = 1|d) is calculated based on the frequency of feature fi occurring in d.</p><p>In many cases, the title field of a web page includes very important information about this page. Prior research also found that the anchor texts of web pages are useful for retrieval, possibly because they are usually not written by the page author and thus unlikely to be biased. To take advantages of the prior findings, we tried a mixture model with different document representations in our baseline retrieval.</p><p>The fields we used are: (t)itle, (a)nchor, (h)eading (text with h1 and h2 tags), and the whole (d)ocument (which includes all the other fields). The mixture model is:</p><formula xml:id="formula_4" coords="2,316.81,524.92,264.51,43.57">P (fi = 1|d) = 1 λt + λa + λ h + λ d {λtP (fi = 1|θt) + λaP (fi = 1|θa) + λ h P (fi = 1|θ h ) + λ d P (fi = 1|θ d )}<label>(5)</label></formula><p>where fi could be a word, a phrase, or a text window.</p><p>Figure <ref type="figure" coords="2,353.92,588.80,4.61,7.86" target="#fig_0">1</ref> shows the baseline retrieval methods with a query example "w1 w2". We assign particular weights to the word match (λ1 for w1 and λ2 for w2, which are chosen to be equal in our experiment), phrase match (λ3), and text window match (λ4) respectively, then documents could be ranked according to:</p><formula xml:id="formula_5" coords="2,331.75,656.96,224.17,58.95">S0(q, d) = 1 λ1 + λ2 + λ3 + λ4 (λ1 * logP (w1 = 1|d) + λ2 * logP (w2 = 1|d) + λ3 * logP (phr(w1w2) = 1|d) + λ4 * logP (wind8(w1w2) = 1|d))<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relevance Model</head><p>A relevance model is estimated based on the relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P (w = 1|R) =</head><p>P (w = 1, R) P (R) = d∈D + P (w = 1|d)P (R|d)P (d) P (R) (7) D + is the set of relevant documents of the query, P (d) is assumed to be uniform over all documents, P (R|d) is the probability of relevance of document d, which is given. According to this model, we select the top N words with largest probabilities P (w = 1|R) for query expansion.</p><p>The relevance model is shown in figure <ref type="figure" coords="3,224.51,186.55,3.58,7.86" target="#fig_2">2</ref>. The weight of each new word is set according to P (w = 1|R). Based on the relevance model, we calculate: S1(q, d) = w i ∈Wr P (wi = 1|R)logP (wi = 1|d)</p><formula xml:id="formula_6" coords="3,160.78,242.80,77.41,11.09">w i ∈Wr P (wi = 1|R)</formula><p>where Wr is the set of words that are selected for query expansion.</p><p>Combining the original query model with the relevance model, we rank the documents based on:</p><formula xml:id="formula_7" coords="3,87.21,320.31,205.70,8.35">S(qexp, d) = (1 -λ f b )S0(q, d) + λ f b S1(q, d)<label>(8)</label></formula><p>where λ f b is the combination weight that reflects how much we believe the relevance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL SETUP</head><p>The large web collection Clueweb09 is used this year. It contains around 1 billion web pages. We chose to work on its subset B, which contains about fifty million documents. We index the documents using the Indri toolkit <ref type="bibr" coords="3,249.34,434.19,9.20,7.86" target="#b2">[2]</ref>. A stop word list is used when creating the index. We also chose to create an index on different document fields, including title, heading, anchor, and the full document (all the other fields).</p><p>Indri Query Language that considers phrase match and text window match is used in our experiment. <ref type="bibr" coords="3,241.59,486.49,9.20,7.86" target="#b4">[4]</ref>. Take the query "obama family tree" as an example, the indri query string would be: In the clustering experiment, we used K-means to cluster the top 50 retrieved documents into 5 clusters. The reason why we chose a small number (50 in our case) as the cut line is that we hope the chosen documents have high probabilities of being relevant. Equation ( <ref type="formula" coords="3,468.33,174.14,3.92,7.86" target="#formula_1">2</ref>) is used to calculate the document vectors. Cosine similarity measure is used to calculate the distance. When calculating the document vectors, k1 and b are set as 1.2 and 0.75 respectively.</p><p>In the TED experiments, we kept the top 1000 retrieved documents, which consist of the set P (see section 3). The candidate set C is the top 50 documents. The sequential algorithm introduced in section 3 is used to select 5 best documents from the top 50 based on the top 1000 documents. Equation ( <ref type="formula" coords="3,361.24,268.28,3.92,7.86" target="#formula_1">2</ref>) is used to calculate the document vectors as well.</p><p>In phase 2, all parameters introduced in Section 4 are trained on last year's relevance feedback data set. The Indri query string of our relevance feedback model is created as follows:</p><formula xml:id="formula_8" coords="3,354.38,337.74,146.04,8.35">#weight((1 -λ f b )QSorg λ f b QS rel )</formula><p>where QSorg is the original query. QS rel is the following query string created based on the relevance model: #weight(P (w1|R) w1 P (w2|R) w2 • • • P (wN |R) wN )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Which documents are good for relevance feedback</head><p>We submitted two runs in phase 1: UCSC.1 (generated by TED) and UCSC.2 (generated by clustering). Based on the evaluation metric 1 used for phase 1, UCSC.1 is ranked higher than UCSC.2, which means that the TED method performs better than the clustering method in general. Table 1 compares the document sets for UCSC.1 and UCSC.2. We notice that the TED method returns more relevant documents (126) than the clustering method (116). This is probably why UCSC.1 performs better than UCSC.2. However, UCSC.2 has fewer topics with zero relevant documents (9) than UCSC. <ref type="bibr" coords="3,366.85,558.71,25.10,7.86">1 (12)</ref>. This is consistent with what we expected: for ambiguous queries, clustering method tends to choose a diversified set of documents, so it is more likely that some relevant documents are covered.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance of relevance feedback algorithm</head><p>Our submissions in phase 2 are ranked the 2nd among all participants. This indicates the effectiveness of our relevance feedback algorithm. Also, our RF algorithm performs similarly well on each of the eight document sets, which shows the robustness of our algorithm.</p><p>The topic-level performances of our RF algorithm are shown in table <ref type="table" coords="4,86.20,401.53,3.58,7.86" target="#tab_1">2</ref>. All the results are averaged over 8 document sets. The average retrieval performance (stAP) over all topics is improved by 34.9% over the baseline case where no RF is used.</p><p>We also notice that relevance feedback may hurt some topics. Among the 50 topics, the retrieval performances get worse on 13 ones (which is more than 1/4 of all topics) when using relevance feedback. Table <ref type="table" coords="4,186.01,474.76,4.61,7.86">3</ref> shows these topics. Further failure analysis shows that relevance feedback actually exacerbates the focus on some aspects of these topics and neglects some other key aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We tried two methods in finding good documents for relevance feedback. We found that the TED method performs better than the clustering method mainly because it returns more relevant documents. Our baseline retrieval model goes beyond the commonly used "bag-of-words" approach, and our relevance feedback model improves retrieval performance by 35% over a baseline without RF. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,353.48,226.47,165.77,7.89;2,317.57,53.80,237.59,158.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Baseline retrieval method</figDesc><graphic coords="2,317.57,53.80,237.59,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,93.17,530.10,38.90,7.86;3,141.04,543.55,38.68,7.86;3,141.04,557.00,37.16,7.86;3,141.04,570.45,27.47,7.86;3,141.04,583.89,86.26,7.86;3,141.04,597.34,75.05,7.86;3,141.04,610.79,104.70,7.86;3,141.04,624.24,98.40,7.86;3,141.04,637.69,88.72,7.86;3,141.04,651.14,87.19,7.86;3,141.04,664.59,121.46,7.86;3,110.83,678.04,3.58,7.86;3,62.77,700.73,230.14,7.86;3,53.80,711.19,239.10,7.86;3,316.81,57.64,120.39,7.86;3,357.69,74.79,35.82,7.86;3,357.69,88.24,147.10,7.86;3,357.69,101.69,175.29,8.35;3,357.69,115.14,3.58,7.86"><head>λ2 # 1 (</head><label>1</label><figDesc>family tree) λ2 #1(obama family tree) λ3 #uw8(obama family) λ3 #uw8(obama tree) λ3 #uw8(family tree) λ3 #uw12(obama family tree) )For the mixture model of different document representations, let's look at the word "obama" as an example. In this case, the indri query string is: #wsum( λt obama.(title) λa obama.(anchor) λ h obama.(heading) λ d obama.(document) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,105.53,284.08,398.65,7.89;4,142.86,53.80,324.01,216.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relevance feedback method. wi, wj... are words selected for query expansion</figDesc><graphic coords="4,142.86,53.80,324.01,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,317.27,617.89,244.14,101.16"><head>Table 1 :</head><label>1</label><figDesc>Clustering v.s. TED</figDesc><table coords="3,322.19,637.16,239.21,40.27"><row><cell>doc set</cell><cell>method</cell><cell># of rel docs</cell><cell># of topics</cell></row><row><cell></cell><cell></cell><cell>returned</cell><cell>with no rel doc</cell></row><row><cell>UCSC.2</cell><cell cols="2">Clustering 116</cell><cell>9</cell></row><row><cell>UCSC.1</cell><cell>TED</cell><cell>126</cell><cell>12</cell></row></table><note coords="3,317.27,709.42,3.65,5.24;3,321.42,711.19,143.87,7.86"><p><p>1 </p>See track overview paper for details</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,53.80,82.42,511.51,605.20"><head>Table 2 :</head><label>2</label><figDesc>Performances of relevance feedback on each topic</figDesc><table coords="5,59.18,121.33,506.13,566.29"><row><cell cols="2">topic id stAP of</cell><cell>stAP</cell><cell cols="2">improve average #</cell><cell></cell><cell></cell></row><row><cell></cell><cell>baseline</cell><cell>with RF</cell><cell></cell><cell>of rel docs</cell><cell></cell><cell></cell></row><row><cell>overall</cell><cell>0.1718</cell><cell>0.2318</cell><cell>34.9%</cell><cell>1.97</cell><cell></cell><cell></cell></row><row><cell>rf09-1</cell><cell>0.4021</cell><cell>0.6860</cell><cell>70.6%</cell><cell>1.9</cell><cell></cell><cell></cell></row><row><cell>rf09-2</cell><cell>0.3383</cell><cell>0.2670</cell><cell>-21.1%</cell><cell>3.4</cell><cell></cell><cell></cell></row><row><cell>rf09-3</cell><cell>0.0042</cell><cell>0.0124</cell><cell>194.6%</cell><cell>2.0</cell><cell></cell><cell></cell></row><row><cell>rf09-4</cell><cell>0.0465</cell><cell>0.1344</cell><cell>189.1%</cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>rf09-5</cell><cell>0.0878</cell><cell>0.1049</cell><cell>19.4%</cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell>rf09-6</cell><cell>0.0872</cell><cell>0.3701</cell><cell>324.3%</cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>rf09-7</cell><cell>0.0369</cell><cell>0.0262</cell><cell>-29.0%</cell><cell>3.5</cell><cell></cell><cell></cell></row><row><cell>rf09-8</cell><cell>0.0024</cell><cell>0.0319</cell><cell>1226.5%</cell><cell>0.6</cell><cell></cell><cell></cell></row><row><cell>rf09-9</cell><cell>0.0365</cell><cell>0.0292</cell><cell>-20.1%</cell><cell>3.4</cell><cell></cell><cell></cell></row><row><cell>rf09-10</cell><cell>0.0768</cell><cell>0.3783</cell><cell>392.9%</cell><cell>1.6</cell><cell></cell><cell></cell></row><row><cell>rf09-11</cell><cell>0.1303</cell><cell>0.1752</cell><cell>34.4%</cell><cell>3.3</cell><cell></cell><cell></cell></row><row><cell>rf09-12</cell><cell>0.2155</cell><cell>0.1961</cell><cell>-9.0%</cell><cell>2.9</cell><cell cols="2">Table 3: Topic examples where relevance feedback</cell></row><row><cell>rf09-13</cell><cell>0.0008</cell><cell>0.0006</cell><cell>-21.0%</cell><cell>0.3</cell><cell>(RF) hurts</cell><cell></cell></row><row><cell>rf09-14</cell><cell>0.0101</cell><cell>0.0265</cell><cell>162.1%</cell><cell>1.3</cell><cell></cell><cell></cell></row><row><cell>rf09-15 rf09-16 rf09-17 rf09-18 rf09-19 rf09-21 rf09-22 rf09-23 rf09-24 rf09-25 rf09-26 rf09-27 rf09-28 rf09-29 rf09-30 rf09-31 rf09-32 rf09-33</cell><cell>0.2886 0.3092 0.0972 0.1449 0.0016 0.2960 0.4201 0.0280 0.0213 0.2458 0.1858 0.2033 0.5131 0.0483 0.2775 0.1331 0.0125 0.4390</cell><cell>0.3559 0.4999 0.0952 0.2234 0.0019 0.3878 0.4864 0.0306 0.0520 0.4280 0.2061 0.2166 0.4535 0.0532 0.2478 0.4079 0.0078 0.3959</cell><cell>23.3% 61.7% -2.0% 54.2% 17.7% 31.0% 15.8% 9.4% 144.6% 74.1% 10.9% 6.5% -11.6% 10.1% -10.7% 206.6% -37.9% -9.8%</cell><cell>4.0 2.8 1.1 2.0 0.0 3.6 4.4 0.9 2.1 1.4 2.5 1.8 3.0 0.1 3.4 4.3 2.9 3.4</cell><cell>id rf09-38 dogs for adoption topic rf09-32 website design hosting rf09-7 air travel information rf09-2 french lick resort and casino 0.3383 stAP with-out RF 0.1793 0.0125 0.0369 rf09-13 map 0.0008 rf09-9 used car parts 0.0365 rf09-50 dog heat 0.1258 rf09-28 inuyasha 0.5131 rf09-30 diabetes education 0.2775 rf09-33 elliptical trainer 0.4390 rf09-12 djs 0.2155 rf09-46 alexian brothers hospital 0.6565 rf09-17 poker tournaments 0.0972</cell><cell>stAP improve-ment with RF -42.6% -37.9% -29.0% -21.1% -21.0% -20.1% -13.2% -11.6% -10.7% -9.8% -9.0% -3.0% -2.0%</cell></row><row><cell>rf09-34</cell><cell>0.0274</cell><cell>0.0736</cell><cell>168.5%</cell><cell>0.6</cell><cell></cell><cell></cell></row><row><cell>rf09-35</cell><cell>0.3079</cell><cell>0.3423</cell><cell>11.2%</cell><cell>4.0</cell><cell></cell><cell></cell></row><row><cell>rf09-36</cell><cell>0.0789</cell><cell>0.1482</cell><cell>88.0%</cell><cell>1.3</cell><cell></cell><cell></cell></row><row><cell>rf09-37</cell><cell>0.0516</cell><cell>0.0566</cell><cell>9.6%</cell><cell>0.3</cell><cell></cell><cell></cell></row><row><cell>rf09-38</cell><cell>0.1793</cell><cell>0.1029</cell><cell>-42.6%</cell><cell>2.3</cell><cell></cell><cell></cell></row><row><cell>rf09-39</cell><cell>0.1436</cell><cell>0.2709</cell><cell>88.6%</cell><cell>2.3</cell><cell></cell><cell></cell></row><row><cell>rf09-40</cell><cell>0.1519</cell><cell>0.2286</cell><cell>50.5%</cell><cell>1.5</cell><cell></cell><cell></cell></row><row><cell>rf09-41</cell><cell>0.2137</cell><cell>0.2251</cell><cell>5.3%</cell><cell>2.8</cell><cell></cell><cell></cell></row><row><cell>rf09-42</cell><cell>0.0504</cell><cell>0.1396</cell><cell>177.0%</cell><cell>0.3</cell><cell></cell><cell></cell></row><row><cell>rf09-43</cell><cell>0.2324</cell><cell>0.4128</cell><cell>77.6%</cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>rf09-44</cell><cell>0.0123</cell><cell>0.0592</cell><cell>381.4%</cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>rf09-45</cell><cell>0.1854</cell><cell>0.1880</cell><cell>1.4%</cell><cell>2.8</cell><cell></cell><cell></cell></row><row><cell>rf09-46</cell><cell>0.6565</cell><cell>0.6366</cell><cell>-3.0%</cell><cell>3.4</cell><cell></cell><cell></cell></row><row><cell>rf09-47</cell><cell>0.4392</cell><cell>0.6092</cell><cell>38.7%</cell><cell>2.6</cell><cell></cell><cell></cell></row><row><cell>rf09-48</cell><cell>0.1618</cell><cell>0.2698</cell><cell>66.7%</cell><cell>1.3</cell><cell></cell><cell></cell></row><row><cell>rf09-49</cell><cell>0.2591</cell><cell>0.4948</cell><cell>90.9%</cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>rf09-50</cell><cell>0.1258</cell><cell>0.1092</cell><cell>-13.2%</cell><cell>1.0</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGEMENT</head><p>This research was funded by <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">IIS-0713111</rs>, <rs type="institution">AFOSR/AFRL</rs> and <rs type="institution">ISSDM at University of California Santa Cruz</rs>. Any opinions, findings, conclusions or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gwc7Yhr">
					<idno type="grant-number">IIS-0713111</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,58.28,714.82,96.81,10.75" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,314.16,210.49,7.86;4,331.01,324.62,193.93,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,357.35,314.16,179.90,7.86">Language modeling meets inference networks</title>
		<author>
			<persName coords=""><surname>Indri</surname></persName>
		</author>
		<ptr target="http://sifter.org/simon/journal/20061211.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,336.08,203.95,7.86;4,331.01,346.54,224.66,7.86;4,331.01,357.00,219.46,7.86;4,331.01,367.46,224.39,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,437.04,336.08,97.91,7.86;4,331.01,346.54,209.61,7.86">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,331.01,357.00,219.46,7.86;4,331.01,367.46,196.53,7.86">Information Processing and Management Special Issue on Bayesian Networks and Information Retrieval</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,378.91,204.15,7.86;4,331.01,389.38,208.70,7.86;4,331.01,399.84,159.42,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,506.87,378.91,28.29,7.86;4,331.01,389.38,192.85,7.86">Formal multiple-bernoulli models for language modeling</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,331.01,399.84,110.75,7.86">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,411.29,205.78,7.86;4,331.01,421.75,210.13,7.86;4,331.01,432.21,192.09,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,487.40,411.29,49.38,7.86;4,331.01,421.75,82.89,7.86">Indri at trec 2004: Terabyte track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T T H</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,432.30,421.75,108.84,7.86;4,331.01,432.21,106.51,7.86">Online Proceedings of 2004 Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct coords="4,331.00,443.67,189.30,7.86;4,331.01,454.13,221.90,7.86;4,331.01,464.59,209.81,7.86;4,331.01,475.05,80.32,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,444.09,443.67,76.22,7.86;4,331.01,454.13,131.46,7.86">Active learning via transductive experimental design</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,480.97,454.13,71.93,7.86;4,331.01,464.59,209.81,7.86;4,331.01,475.05,52.11,7.86">Proceedings of the 23rd International Conference on Machine Learning (ICML 2006)</title>
		<meeting>the 23rd International Conference on Machine Learning (ICML 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,331.00,486.51,224.56,7.86;4,331.01,496.97,179.08,7.86;4,331.01,507.43,221.90,7.86;4,331.01,517.89,205.96,7.86;4,331.01,528.35,72.87,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,482.92,486.51,72.64,7.86;4,331.01,496.97,179.08,7.86;4,331.01,507.43,131.46,7.86">Non-greedy active learning for text categorization using convex transductive experimental design</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,480.97,507.43,71.93,7.86;4,331.01,517.89,205.96,7.86;4,331.01,528.35,44.92,7.86">Proceedings of the 31st Annual International ACM SIGIR Conference (SIGIR 08)</title>
		<meeting>the 31st Annual International ACM SIGIR Conference (SIGIR 08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
