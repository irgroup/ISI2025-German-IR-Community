<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.98,151.79,296.77,12.58">BIT at TREC 2009 Faceted Blog Distillation Task</title>
				<funder ref="#_UMnGNSB">
					<orgName type="full">Chinese National Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.88,190.14,42.19,9.02"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.44,190.14,41.90,9.02"><forename type="first">Qing</forename><surname>Yang</surname></persName>
							<email>yangqing2005@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.67,190.14,59.76,9.02"><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
							<email>cxzhang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.60,190.14,58.10,9.02"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
							<email>zniu@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.98,151.79,296.77,12.58">BIT at TREC 2009 Faceted Blog Distillation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B0D21B6613A021E90CFF18EF3939DF84</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This Paper presents the work done for the TREC 2009 faceted blog distillation task of blog track. In our approach, we use a mixture of language models based on global representation. Our model can be regarded as a combination of topic relevance model and faceted relevance model. By pseudorelevance feedback method, we can estimate the above two models from topic relevance feedback documents and facet relevance feedback documents respectively. Experimental results on TREC blogs08 collection show the effectiveness of our proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first time that Beijing Institute of Technology participates in TREC and our focus is on faceted blog distillation task of blog track. For the faceted blog distillation task, we aim to evaluate the effectiveness of combining different language models for faceted blog ranking and using global representation for feed retrieval. Experiments results on TREC blogs08 datasets show improvement by using facetspecific language models over the baseline. Moreover, the lexicons (subjective or objective lexicon) used are domain-independent. Hence our proposed approach is applicable to all retrieval tasks on any text resource containing information about topic and multiple facets such as opinionated nature, authors' trustworthiness and writing style.</p><p>The goal of the blog distillation task is to'Find feeds that are principally devoted to topic X'. The blog distillation task is different from the other tasks in that its retrieval unit is feed which contains a number of web documents. This year, blog distillation task involves a number of facets, such as opinionated, personal and indepth. It goes beyond the topic relevance and thus the facet nature must be considered.</p><p>The rest of the paper is organized as follows. In Section 2, a system overview is presented briefly. The query generation is described in Section 3. Section 4 introduces the dataset and index information. The whole approach is described in Section 5. The experimental results are presented in Section 6. Finally we conclude the paper and discuss the future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>Our system consists of four main parts: the query generator, the blog retrieval component, the language model building component and the rank component. The architecture is shown in Fig. <ref type="figure" coords="2,240.86,149.76,3.76,9.02" target="#fig_0">1</ref>. The query generator is responsible for parsing topic and generates query strings. The Blog retrieval component indexes blog corpus, and finds all blog posts corresponding to each query string. All posts are then combined to blogs by their feedno. After this, the component finds all posts which belong to a given blog by the feedno. The Language model building component is responsible for generating topicfaceted language model, which is a combination of query language model, topic relevant language model and facet language model. This component generates the above language models respectively and then combines them. The Rank component is in charge of ranking all blogs based on Kullback-Leibler divergence language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Query Generation</head><p>All of the three fields (title, description, narrative) are used for generating query. First, we filter out unnecessary punctuation marks in the text of the above three fields from each topic. All verbs are replaced by their infinitives and all nouns by their singular forms. Then, we extract the keywords to build three bags of words to three fields of topic, after which WordNet is used to expand these three bags of words. The word and its synonyms in WordNet are built into an indri query unit, e.g. the indri query unit of "subsidy": #or(subsidy grant subvention)</p><p>The query units of all words in the bag form an indri query. For example, a bag of words (w 1 , w 2 , w 3 , …, w n ) can form into an indri query:</p><formula xml:id="formula_0" coords="3,124.74,202.20,221.92,27.26">#combine(u 1 , u 1 , …, u i , …, u n ) u i is the indri query unit such as #or(w i , s i1 , s i2 , …, s im )</formula><p>, where s im is m-th synonym of w 1 . The three fields of topic are assigned different weights, so the final indri query for a given topic is: #weight(0.6 field title 0.2 field desc 0.2 field narr ) field title , field desc and field narr are the indri query strings generated by title, description and narrative field of topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset and Index</head><p>TREC Blog09 collection contains permalinks, feed pages and homepages. We use the permalinks and homepages for the faceted blog distillation task. Feed pages collection is not used because feed pages usually contain few sentences of each post and therefore cannot reflect the topic or opinion well.</p><p>The permalinks and homepages are encoded by HTML. We use Indri to index them respectively. We specify some fields and metadata for the index so that we can search and combine posts to blogs flexibly. These fields and metadata are shown in Table <ref type="table" coords="3,150.01,424.86,3.75,9.02" target="#tab_0">1</ref>. Krovetz stemmer and a list with 450 stop words (e.g. a, about, above or many other common but useless words) are used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Our Approach</head><p>We choose Global Representation model[1] to represent feed. This model treats a blog as a virtual document which is comprised of all posts of the blog. Thus, this model can factually reflect the recurring interest in a given topic over the time span of the feed. In addition, since we use language model based approach to rank feed, Global Representation model, which combines many posts into a large document, can avoid the problem of sparsity of words as far as possible.</p><p>In order to combine all posts into one large virtual document, the first step is to find all relevant posts for a given topic, which are then combined into some feeds by their feedno fields. Then, metadata search is used to get all posts of a given feed by the feed's feedno. Finally, we obtain all relevant feeds which contain not only relevant posts but also irrelevant posts.</p><p>We rank feeds by the Kullback-Leibler Divergence of a feed language model and a topic-facet language model. In this solution, two different language models are defined: one for a topic with facet value (θ TF ); and another for the virtual document of a feed (θ D ). That is, we assume that θ TF represents the topic and facet information need, while θ D represents a feed. The KL-divergence of these two models is able to measure how close they are to each other. Thus, we can rank feeds by following formula:</p><formula xml:id="formula_1" coords="4,176.94,260.74,287.74,72.25">( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) || | | log | | log | TF TF D TF TF w D TF D TF w score D D p w p w p w θ θ θ = - = - = + ∑ ∑ (1)</formula><p>Because the constant cons(θ TF ) (the entropy of θ TF ) does not affect the results of ranking feeds, we do not compute it in our system. θ D can be estimated by query likelihood retrieval model <ref type="bibr" coords="4,225.44,359.64,13.05,9.02">[2]</ref>. Thus, the main task is to estimate θ TF . θ TF is the language model which reflects not only the topic information need but also the facet information need. Hence a mixture of language models is used to estimate it. In our solution, we define three language models, namely, the query language model (θ Q ), the topic relevant language model (θ T ) and the facet relevant language model (θ F ). The topic and facet language model θ TF is a linear combination of the three language models θ Q , θ T , and θ F :</p><formula xml:id="formula_2" coords="4,237.36,440.77,227.32,14.51">TF Q T F θ αθ βθ γθ = + +<label>(2)</label></formula><p>Where α, β and γ are given, and sum up to 1.</p><p>In equation ( <ref type="formula" coords="4,188.53,475.80,3.54,9.02" target="#formula_2">2</ref>), θ Q can be computed by query likelihood retrieval model. θ T can be regarded as a feedback topic model estimated by pseudo-relevance feedback method. In order to increase the quality of feedback document, we index the Wikipedia corpus to obtain feedback documents.</p><p>The final step is to estimate θ F in equation ( <ref type="formula" coords="4,341.05,521.76,3.55,9.02" target="#formula_2">2</ref>). It reflects the users' facet information need. In this year's task, there are three facets to be considered: opinionated, personal and in-depth. Each facet has two faceted values. For opinionated facet, the values of interest are "opinionated" and "factual". A subjective lexicon and an objective lexicon are chosen. We compute the mutual information between words w i from the above lexicons and the query Q of a given topic:</p><formula xml:id="formula_3" coords="4,160.92,586.37,303.76,33.66">( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) # 15 , , log log i i i i i Hits uw w Q C p w Q MI w Q p w p Q Hits w Hits Q × = = × (3)</formula><p>Here |C| is the total number of documents in corpus. Hits(w i ) and Hits(Q) are the counts of documents which contain w i and Q respectively. Hits(#uw15(w i Q)) is the count of documents which contain w i and Q in a window of 15 terms. The reason why we use a fixed size window instead of a sentence is that: it is time-consuming and unpractical to split all text into sentences, and the problems related to inaccuracy can be ignored when large corpus is used. Finally we choose the top 100 subjective/objective words according to the mutual information value to expand query and re-retrieve from the posts which are retrieved for the first time. The subjective/objective feedback documents can be used to build opinionated/factual faceted language model by pseudo-relevance feedback method.</p><p>For personal facet, the values of interest are "personal" and "official". In order to build θ F for these two faceted values, we seek help from icerocket blog meta-search engine and OpenSearch. Because all blogs in blogs.myspace are personal, we use icerocket to get the top 30 documents from blogs.myspace for a given topic, and all of them can be used as relevant feedback documents for personal facet value. We select some tags such as "company" and "commercial", which can distinguish official blogs from personal blogs, and add them to icerocket to search for official blog posts. The search results for official posts are then used as relevant feedback documents for official facet value. Pseudo-relevance feedback method is used again for the estimate of θ F . For in-depth facet, we assume its language model is similar to that of opinionated facet, but with different values of parameters. For example, a factual blog with high topic relevance is always an in-depth blog, but an opinionated blog with low topic relevance is always a shallow blog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We submit 2 runs: BIT09PH used permalinks indexed corpus, while BIT09P used homepages indexed corpus. Results are provided in Table <ref type="table" coords="5,363.09,400.32,3.76,9.02" target="#tab_2">3</ref>. The relevance and facet judgments of the TREC 2009 faceted blog distillation task are categorized into five grades, i.e. not judged (-1), not relevant (0), relevant (1), relevant and inclined towards first facet value (2) and relevant and inclined towards second facet value (3).   The index for our submitted results does not contain the data from January 2008 (14/01/2008 -31/01/2008) due the time constraints. Subsequently, we index this omitted data and add it to the whole index. The final results are as shown in Table <ref type="table" coords="6,463.20,520.32,3.76,9.02" target="#tab_2">3</ref>. It can be seen that the performances has improved significantly. This is due to the large data size of January 2008: it is almost comparable to half the size of the whole dataset. Relevant blogs which are retrieved by our system from the dataset excluding January 2008 only cover 44.47% of the relevant blogs. However the coverage increases to 89.81% after the data of January 2008 is included. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,235.50,430.89,124.32,8.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The System Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,136.08,593.40,334.57,9.02;5,124.74,604.86,345.92,9.02;5,124.74,616.38,345.92,9.02;5,124.74,627.84,345.93,9.02;5,124.74,639.36,53.04,9.02"><head></head><label></label><figDesc>Figures 2 (a) (b) and (c) show our system's per-topic performance in terms of average precision (AP), alongside with the per-topic median and best performance, respectively. All 39 topics are sorted along the x axis in descending order of BIT09PH performance. The dots indicate the topics for which BIT09PH obtains the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,174.90,474.87,245.55,8.10"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average precision of the three facet values for each topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,163.02,459.69,269.24,95.98"><head>Table 1 . Fields and Metadata of Permalinks and Homepages Index</head><label>1</label><figDesc></figDesc><table coords="3,163.02,476.14,269.24,79.53"><row><cell>Name</cell><cell>Type</cell><cell>Description</cell></row><row><cell>TITLE</cell><cell>Field</cell><cell>The content in title tag</cell></row><row><cell>DATE_XML</cell><cell>Field</cell><cell>Data information of a permalink document</cell></row><row><cell>FEEDNO</cell><cell cols="2">Metadata for permalinks The ID of a feed document</cell></row><row><cell>FEEDURL</cell><cell cols="2">Metadata for permalinks The URL of a feed document</cell></row><row><cell cols="3">BLOGHPURL Metadata for permalinks The URL of a blog</cell></row><row><cell>PERMALINK</cell><cell cols="2">Metadata for permalinks The URL of a permalink document</cell></row><row><cell>HPNO</cell><cell cols="2">Metadata for homepages The ID of a homepage document</cell></row><row><cell>URL</cell><cell cols="2">Metadata for homepages The URL of a homepage document</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,136.08,458.13,324.72,144.28"><head>Table 2 . Results on dataset excluding data of January 2008</head><label>2</label><figDesc></figDesc><table coords="5,136.08,477.27,324.72,125.14"><row><cell></cell><cell>Run</cell><cell>Map</cell><cell>P10</cell><cell>R-prec</cell><cell>bpref</cell></row><row><cell>first</cell><cell>BIT09PH BIT09P</cell><cell>0.1168 0.0895</cell><cell>0.1256 0.1154</cell><cell>0.1276 0.1149</cell><cell>0.1028 0.0881</cell></row><row><cell>second</cell><cell>BIT09PH BIT09P</cell><cell>0.0884 0.0673</cell><cell>0.0897 0.0538</cell><cell>0.1011 0.0617</cell><cell>0.0758 0.0465</cell></row><row><cell>none</cell><cell>BIT09PH BIT09P</cell><cell>0.1165 0.0708</cell><cell>0.2513 0.2000</cell><cell>0.1714 0.1852</cell><cell>0.1347 0.1207</cell></row><row><cell>Figures 2 (a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,181.50,601.11,243.64,72.96"><head>Table 3 . Results on dataset including data of January 2008</head><label>3</label><figDesc></figDesc><table coords="6,181.50,620.25,243.64,53.82"><row><cell></cell><cell>Map</cell><cell>P10</cell><cell>R-prec</cell><cell>bpref</cell></row><row><cell>first</cell><cell>0.2228</cell><cell>0.2385</cell><cell>0.2437</cell><cell>0.2000</cell></row><row><cell>second</cell><cell>0.1796</cell><cell>0.1436</cell><cell>0.1565</cell><cell>0.1341</cell></row><row><cell>none</cell><cell>0.3009</cell><cell>0.4462</cell><cell>0.3502</cell><cell>0.3210</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Conclusions</head><p>We apply a mixture of language models based on a global representation of the blogs to the faceted blog distillation task. The results have proved the effectiveness of our approach, especially after including the omitted data of January 2008. There is still a huge potential space for further research to improve the performance of blog retrieval. We will explore new models or approaches to rank blogs, and ways to fuse different models to a better model.</p><p>Acknowledgments. This work is supported by the grant from <rs type="funder">Chinese National Natural Science Foundation</rs> (No: <rs type="grantNumber">60705022</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UMnGNSB">
					<idno type="grant-number">60705022</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,140.22,320.37,330.43,8.10;7,124.74,330.75,35.28,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,217.72,320.37,163.25,8.10">UMass at TREC 2008 Blog Distillation Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,387.68,320.37,82.97,8.10;7,124.74,330.75,9.00,8.10">Proceedings of TREC-08</title>
		<meeting>TREC-08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.58,341.07,328.07,8.10;7,124.74,351.39,209.78,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,182.21,341.07,284.18,8.10">Statistical Language Models for Information Retrieval A Critical Review</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,124.74,351.39,183.48,8.10">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
