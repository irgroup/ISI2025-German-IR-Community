<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,186.84,120.87,221.50,14.41">FDUQA on TREC2003 QA task</title>
				<funder ref="#_VeVh934 #_S4Rx25k">
					<orgName type="full">NSF of China</orgName>
				</funder>
				<funder ref="#_Ez5EKug #_Tw2ZaBq">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.46,140.84,39.09,10.79"><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.76,140.84,79.89,10.79"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.22,140.84,63.42,10.79"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.90,140.84,64.63,10.79"><forename type="first">Yongping</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.87,140.84,43.37,10.79"><forename type="first">Lan</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,186.84,120.87,221.50,14.41">FDUQA on TREC2003 QA task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BBD9AABABB3EA8E0917AA8EBD47F8CC4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is the fourth time that we take part in the QA track. Our system, FDUQA, is based on our previous system <ref type="bibr" coords="1,181.83,209.27,71.00,9.49" target="#b6">(Wu et al, 2002)</ref>. FDUQA includes an offline part and an online part. We make great efforts on the online part while leaving the offline part unchanged. We have tried many natural language processing techniques, and incorporated many sources of world knowledge, including Web. A novel Query formulation technique has also been put forward.</p><p>In addition, we've tried another attempt on answer extraction in this year's task. In the second section, we will describe the architecture of our QA system; and give a detailed description on the Query formulation for Web search in the third section; while in the fourth section, we will introduce our new attempt on answer extraction; and we will present our performance in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>FDUQA's architecture is shown in figure1. Our system can be divided in two ways. One is traditional: question analysis, retrieval, and answer extraction, as shown in figure <ref type="figure" coords="1,470.28,366.29,5.68,9.49">1</ref> by the two horizontal lines. The other is more natural: answer type decision-making, candidate answer decision-making, and final answer decision-making, as shown in figure <ref type="figure" coords="1,450.69,389.81,5.68,9.49">1</ref> by the two vertical lines. We'll describe the FDUQA system in the latter.</p><p>In the answer type decision-making step, FDUQA system determines the answer type of the input question based on the question's interrogative and focus words. The classifier and focus words decision algorithm are both based on heuristic rules. We adopt an eighteenclass answer type classify system, illustrated in table1. At this step our system can achieve the precision of 80%  At the second step, candidate answer decision-making, our system searches the Web by Google and then tries to find the answer in the returned snippets. Finding an answer in the huge corpus is easier than in a smaller one in some sense, because system can search the corpus more easily and get more confident answer by stricter query (or query set). For example, questions such as "Where was Hans Christian Anderson born?" are very easy for Web search engine to find the answer by inputting query as "Hans Christian Anderson was born in". We'll describe the Query formulation for Web Search module in great details in the next section. The upper two modules can be taken for question analysis and retrieval steps for the traditional QA system, while the following modules make up of the answer extraction step. Candidate answer tagging module tags candidate answers in the returned snippets based on their NE tagging and Base NP tagging results. Consider the distances of the key concepts and the distances between the candidate answers and key concepts, the multi-policy boosting module gives score to every candidate answer and snippet pair. The pairs are clustered by candidate answers, and each candidate answer set can get its score by adding up all of its elements' scores. Thus, the candidate answers can be sorted with their scores.</p><p>The third step is final answer decision-making section. At this step, the first two modules, Query generation for TREC search and search engine, are the same as last year. The following modules are almost the same as the corresponding modules in last step. The only difference between two "Candidate Answer tagging" modules is that system tagging the candidate answer in this step based on the candidate answer generated in last step, the candidate answer decision-making step. Support selection module sets the support score by adopting the same technique that used in multi-policy boosting module that give score to every candidate answer and snippet pair. The system integrates every candidate answer's support scores and their ranks in last step to sort them. FDUQA system considers the top one candidate answer as the final answer.</p><p>3 Query formulation for Web search</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Formulation</head><p>Answer of a question may appear in a context, which is just the statement form of the question. For example, the answer of question "What book did Rachel Carson write in 1962?" appears in a context like "Rachel Carson wrote &lt;Answer&gt; in 1962". But mostly such a context doesn't exist in a limited corpus like AQUAINT. However, we do retrieval not only on the AQUAINT corpus, but also on Internet like some other systems <ref type="bibr" coords="3,419.45,219.11,88.19,9.49" target="#b1">(Kwok et al., 2001;</ref><ref type="bibr" coords="3,87.54,230.87,93.06,9.49" target="#b0">Dumais et al., 2002)</ref>. Because of the largeness and variety of information on Internet, this context can be retrieved now. Based on this idea, we formulate queries for Web retrieval. Figure <ref type="figure" coords="3,119.32,254.39,5.68,9.49">2</ref> describes the process of query formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 process of query formulation</head><p>Our system first parses questions using LinkParser <ref type="bibr" coords="3,349.20,486.95,138.77,9.49" target="#b3">(Sleator and Temperley, 1993)</ref>, an English parser based on link grammar. Its precision is up to 0.9. Next we extract four constituents from the parsed question: subject, predicate, object and adverbial modifier. These constituents are then used to formulate queries for Web retrieval.</p><p>For example, we parsed the question "What book did Rachel Carson write in 1962?" Its constituents are: "Rachel Carson" -subject; "wrote" -predicate; "in 1962" --adverbial modifier. In this question, object of "wrote" is the question focus. The queries formulated from the above constituents are: "Rachel Carson wrote" "in 1962" "Rachel Carson wrote" in 1962 "Rachel Carson" wrote in 1962 Rachel Carson wrote in 1962 Words in quote marks must appear continuously in retrieved snippets, while others may appear dispersedly or even not appear. Obviously, the first query is a tight one. And the followings are relatively looser. We generate loose queries allowing for other forms of context on Web. For example, "1962 Rachel Carson wrote Silent Spring that was aimed at the general public and became the Uncle Tom's Cabin of the new environmentalism". This snippet can't be retrieved with the first query, but can be retrieved by the later three. And "... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval on Web</head><p>Among various Web search engines, we select Google because of its high performance. And the formulated queries are specified according to the requirement of Google. We submit queries to Google from tight ones to loose. Thus we can find snippets with answers for most of the questions.</p><p>We have done an experiment on questions of TREC2002 QA Track. In these 500 questions, TREC provided answers for 444 of them. So we only considered these 444 questions. And only the first 20 received results for each query are used. The result of Web retrieval is listed in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#question</head><p>#question (has answer in snippets)</p><p>#question (has answer in snippets and some snippet supports the answer) 444 367 (82.7%) 341 (76.8%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 Web retrieval Result</head><p>We can find answers in the retrieved snippets for 82.7% of the 444 questions. And in a closer observation, the retrieved snippets support 341 answers, that is 76.8% of all these questions. Thus, most of the search results contain answers. It's important for the later processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">New attempt for answer extraction</head><p>Pattern based method has been used by many other question answering systems, InsightSoft <ref type="bibr" coords="4,139.50,458.15,152.08,9.49" target="#b4">(Soubbotin and Soubbotin, 2001;</ref><ref type="bibr" coords="4,295.31,458.15,149.08,9.49" target="#b4">Soubbotin and Soubbotin, 2001)</ref> has acquired good performance, ISI developed a method for learning patterns automatically <ref type="bibr" coords="4,87.54,481.67,142.23,9.49" target="#b2">(Ravichandran and Hovy, 2002)</ref>.</p><p>We try a new pattern based method for implementing the answer extraction and give a solution to the problems that other system failed, such as only one key phrase of the question can be included within pattern. We will introduce the process of pattern learning and answer extraction with them.</p><p>The pattern for answer extraction is called context pattern, it is consisted of the following three parts: &lt;Q_Tag&gt;+[ConstString]+&lt;A&gt;. Here, &lt;Q_Tag&gt; stands for the key phrase in question, it includes different elements of the question, and we will introduce them later. &lt;A&gt; stands for the answer, any string holding the position will be extracted as the answer. "[ConstString]" is a sequence of words.</p><p>Context patterns can be learned automatically using the &lt;Q_Tag , A&gt; pairs as training examples. For instance, context pattern "&lt;A&gt;, Q_Focus of Q_NameEntity" can be used to answer the question "What is the capital of Syria?" "Q_Focus" represents the question term "the capital" and "Q_NameEntity" represents the question term "Syria".</p><p>We take the 500 questions of TREC 2002 as our training data for learning these context patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Analysis</head><p>We define a set of notations to represent questions in advance as illustrated in table <ref type="table" coords="5,499.15,143.87,4.26,9.49">3</ref>. They are the object or event the question asks about.</p><p>All these Q_Tag have different importance scores taking into account the possibility they appear around the answer.</p><p>The question pattern (Q_Pattern) is generated from its Q_Tag symbol set, and then the classification of questions will be built based on the Q_Pattern and the answer type. A case in point is that the question class " [DAT] When was Q_BNP_1 Q_Verb? " covered the question " When was Apollo 11 launched? ", " When was the first atomic bomb dropped? " and so on. Table <ref type="table" coords="5,250.48,445.43,5.68,9.45">3</ref> Symbol Set of Question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q_Tag</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pattern Learning and Evaluation</head><p>We will explain our approach with the sample example below.</p><p>Sample question class: [LCN] What Q_Verb Q_Focus of Q_NameEntity? Sample question: What is the capital city of New Zealand? Where Q_Verb = "is", Q_Focus = "the capital city", Q_NameEntity = "New Zealand", and Answer = "Wellington".</p><p>The context patterns of each question class are learned by the following algorithm: 1. Constructing Query: "Q_Focus + Q_NameEntity +Answer" is constructed as the query. For example, the query of above sample question is: "the capital city"+"New Zealand"+ "Wellington".</p><p>2. Searching: the query is submitted to the search engine Google and the top 100 Web documents are downloaded.</p><p>3. Snippet Selection and Filtering: the snippets for pattern learning are extracted from the Web documents. The answer, the nearest ten words left to it, and the nearest ten words right to it are retained.</p><p>4. Context Pattern Extraction: replace the question term in each snippet by the corresponding Q_Tag, and the answer term by the tag &lt;A&gt;. The minimum length string containing the Q_Tag and the tag &lt;A&gt; is extracted as the context pattern. For example, consider the string "…the number of languages that are being spoken. Wellington the capital city of New Zealand and …", context pattern "&lt;A&gt; Q_Focus of Q_NameEntity" is extracted. Num Correct_Match denotes the number of snippets that tag &lt;A&gt; is matched by the correct answer; Num Match denotes the number of the snippets that tag &lt;A&gt; is matched by any word.</p><p>At last the score of the context pattern is computed with the formula : (α=0.3,β=0.7)</p><formula xml:id="formula_0" coords="6,124.02,421.59,193.22,11.29">Score Match Scor Initial Score Pattern _ _ _ • + • = β α</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Answer Extraction</head><p>The context patterns can be used to extract answer to a new unseen question as follows:</p><p>1. Determine the question class of the unseen question based on its Q_Pattern and answer type. The corresponding context patterns are also selected.</p><p>2. Replace the Q_Tag symbols in the context pattern with the corresponding word string of the question.</p><p>3. For each context pattern and each snippet search engine returned, select the words matching tag &lt;A&gt; as the answer.</p><p>4. Sort the answers by their context pattern's score and their frequency. The first answer is returned to the factoid question and the top five answers are returned to the list question and definition question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This year we only take part in the main task of QA, and submit three runs. Our results are not very satisfactory. Our first run, FDUT12QA1, is based on our main architecture; FDUT12QA2 is our new attempt; and FDUT12QA3 is the simple combination of FDUT12QA1 and FDUT12QA2. Their detail evaluation report is illustrated in table 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FDUT12QA1</head><p>FDUT12QA2 FDUT12QA3  We find in table 4, that the numbers of unsupported questions of factoid questions are very big compared with their corresponding right answered questions. That's because we can't well integrate the Web into our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,212.76,441.80,169.70,9.01;2,99.18,105.78,420.92,314.70"><head>Figure</head><label></label><figDesc>Figure 1 FDUQA system architecture</figDesc><graphic coords="2,99.18,105.78,420.92,314.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,118.62,107.99,389.04,9.49;6,87.54,119.69,420.13,9.49;6,87.54,131.39,202.47,9.49;6,263.34,145.31,9.78,16.82;6,267.54,160.35,2.93,6.54;6,257.40,147.75,4.20,11.21;6,242.16,147.75,5.02,11.21;6,198.36,147.75,4.20,11.21;6,182.82,147.75,5.02,11.21;6,267.06,144.01,2.67,4.81;6,265.26,162.07,1.49,4.81;6,378.12,148.15,1.49,4.81;6,337.98,157.96,18.82,8.24;6,312.84,157.96,12.72,8.24;6,302.34,157.96,9.16,8.24;6,353.82,143.26,20.87,8.24;6,328.50,143.26,20.86,8.24;6,310.74,143.26,8.13,8.24;6,275.88,143.26,21.88,8.24;6,226.20,157.96,12.71,8.24;6,205.50,157.96,19.85,8.24;6,158.40,150.70,20.86,8.24;6,124.92,150.70,22.40,8.24;6,270.48,162.07,2.67,4.81;6,326.46,157.96,11.19,8.24;6,381.60,143.26,3.05,8.24;6,350.64,143.26,3.05,8.24;6,321.30,143.26,4.58,8.24;6,299.22,143.26,11.19,8.24;6,219.90,144.88,4.58,8.24;6,151.20,150.70,4.58,8.24;6,249.48,147.23,5.02,11.85;6,189.84,147.23,5.77,11.85;6,199.20,188.17,4.51,8.13;6,226.02,174.31,4.51,8.13;6,191.58,174.31,4.51,8.13;6,172.32,174.31,4.51,8.13;6,147.12,181.09,8.02,8.13;6,120.24,181.09,12.54,8.13;6,231.54,179.10,2.63,4.74;6,232.32,172.80,2.63,4.74;6,197.88,172.80,2.63,4.74;6,197.16,179.10,2.63,4.74;6,178.62,172.80,2.63,4.74;6,177.36,179.10,2.63,4.74;6,210.42,174.31,6.77,8.13;6,134.52,181.09,11.03,8.13;6,218.94,171.41,4.95,11.05;6,203.82,171.41,4.95,11.05;6,184.56,171.41,4.95,11.05;6,157.86,178.19,4.95,11.05;6,181.20,199.88,9.68,16.64;6,185.16,214.77,2.90,6.47;6,173.64,202.33,4.97,11.09;6,184.14,198.59,3.82,4.76;6,181.92,216.47,2.35,4.76;6,263.76,210.11,2.35,4.76;6,249.06,205.25,14.10,8.16;6,235.50,205.25,6.54,8.16;6,224.40,205.25,8.05,8.16;6,191.64,205.25,21.65,8.16;6,151.98,205.25,18.63,8.16;6,119.22,205.25,21.65,8.16;6,187.98,216.47,2.64,4.76;6,267.54,205.25,3.02,8.16;6,243.54,205.25,4.53,8.16;6,232.50,205.25,3.02,8.16;6,213.60,205.25,11.08,8.16;6,141.18,205.25,11.08,8.16;6,110.88,235.91,396.72,9.49;6,87.54,247.67,414.97,11.41;6,110.88,261.35,396.72,9.49;6,87.54,273.11,420.19,9.49;6,87.54,284.81,31.81,9.49;6,139.23,284.81,40.90,9.49;6,200.00,284.81,51.11,9.49;6,270.83,284.81,17.69,9.49;6,308.39,284.81,25.53,9.49;6,353.72,284.81,36.87,9.49;6,410.50,284.81,8.53,9.49;6,438.84,284.81,23.39,9.49;6,482.06,284.81,25.56,9.49;6,87.54,296.57,244.82,9.49;6,110.88,308.33,396.86,9.49;6,87.54,320.09,420.22,9.49;6,87.54,331.79,420.13,9.49;6,87.54,343.55,141.29,9.49;6,203.58,374.10,13.02,4.59;6,212.76,360.54,13.02,4.59;6,194.10,360.54,15.85,4.59;6,188.22,369.45,16.50,7.86;6,178.98,355.83,16.50,7.86;6,151.50,362.49,19.89,7.86;6,123.90,362.49,22.32,7.86;6,209.64,360.54,2.55,4.59;6,145.86,362.49,4.37,7.86;6,171.72,359.68,4.79,10.69"><head>5.</head><label></label><figDesc>Computing the Initial Score of Context Pattern: the score is computed as the following formula considering the importance of the Q_Tag and the distance between the different Q_Tag and the answer. (α=1,β=0.6) number of Q_Tag the question class contains, n is the number of the Q_Tag the context pattern contains, d i is the distance between the different Q_Tag and the answer.The approach to context pattern evaluation is as follows. Query for each context pattern is formed and submitted to the Google, and the top 100 snippets are downloaded for context pattern +[Post_Part ]+[Q_Focus + Q_NameEntity ].[ Pre_Part] stands for the word string left to tag &lt;A&gt; of the context pattern, and that [Post_Part ] stands for the word string right to tag &lt;A&gt; of the context pattern. [Q_Tag] is composed of the Q_Focus and Q_NameEntity of the question. The matching score of each pattern is calculated as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="1,226.50,568.70,142.10,9.01"><head>Table 1 Answer Type concepts</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,86.40,305.45,416.86,103.77"><head></head><label></label><figDesc>Rachel Carson grew up on a small Pennsylvania farm, where she ... her degrees in 1932, she wrote science articles ... of the Sea, and finally Silent Spring in 1962" can only be retrieved by the last two queries.</figDesc><table coords="3,86.40,305.45,416.86,103.77"><row><cell>Question</cell><cell>(using Link Parser)</cell><cell>Parsed Question Sentences</cell><cell>Extracting Constituents</cell></row><row><cell>Queries</cell><cell></cell><cell>Formulating Queries</cell><cell>Constituents of Question Sentences</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,239.46,244.58,116.17,9.01"><head>Table 4 Evaluation report</head><label>4</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was partly supported by <rs type="funder">NSF of China</rs> under contracts of <rs type="grantNumber">69935010</rs> and <rs type="grantNumber">60103014</rs>, as well as the 863 National High-tech Promotion Project of China under contracts of <rs type="grantNumber">2001AA114120</rs> and <rs type="grantNumber">2002AA142090</rs>. We are thankful to <rs type="person">Xin Li</rs>, <rs type="person">Ningyu Chen</rs>, <rs type="person">Liyuan Han</rs> for their help in the implementation.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VeVh934">
					<idno type="grant-number">69935010</idno>
				</org>
				<org type="funding" xml:id="_S4Rx25k">
					<idno type="grant-number">60103014</idno>
				</org>
				<org type="funding" xml:id="_Ez5EKug">
					<idno type="grant-number">2001AA114120</idno>
				</org>
				<org type="funding" xml:id="_Tw2ZaBq">
					<idno type="grant-number">2002AA142090</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,87.54,451.61,419.99,9.49;7,102.90,463.37,404.78,9.49;7,102.90,475.13,404.77,9.49;7,102.90,486.83,145.84,9.49" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<title level="m" coord="7,441.90,451.61,65.63,9.49;7,102.90,463.37,404.78,9.49;7,102.90,475.13,399.92,9.49">Web Question Answering: Is More Always Better? Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2002)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">2002. August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.54,510.23,420.03,9.49;7,102.90,521.99,358.62,9.49" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,427.67,510.23,79.90,9.49;7,102.90,521.99,96.72,9.49">Scaling Question Answering to the Web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Cody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,208.42,521.99,158.95,9.49">Tenth World Wide Web Conference</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">May 1-5, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.54,545.39,420.08,9.49;7,102.90,557.15,226.42,9.49" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,292.77,545.39,214.85,9.49;7,102.90,557.15,82.07,9.49">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Deepak Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,193.04,557.15,107.82,9.49">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.54,580.55,420.07,9.49;7,102.90,592.25,223.00,9.49" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,296.66,580.55,176.78,9.49">Parsing English with a Link Grammar</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sleator</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Davy</forename><surname>Temperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,484.36,580.55,23.26,9.49;7,102.90,592.25,218.11,9.49">Third International Workshop on Parsing Technologies</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.54,615.71,420.09,9.49;7,102.90,627.47,383.69,9.49" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,273.08,615.71,234.56,9.49;7,102.90,627.47,86.74,9.49">Patterns of Potential Answer Expressions as Clues to the Right Answer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,197.65,627.47,127.74,9.49">Proceedings of the TREC-10</title>
		<meeting>the TREC-10<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.54,650.87,420.11,9.49;7,102.90,662.57,404.78,9.49;7,102.90,674.39,87.98,9.49" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,319.42,650.87,188.24,9.49;7,102.90,662.57,182.23,9.49">Use of Patterns for Detection of Likely Answer String: A Systematic Approach</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergei</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,295.33,662.57,134.58,9.49">Proceedings of the TREC-11</title>
		<meeting>the TREC-11<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.54,697.79,419.97,9.49;7,102.90,709.49,404.74,9.49;7,102.90,721.25,109.56,9.49" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,473.29,697.79,34.22,9.49;7,102.90,709.49,244.61,9.49">FDU at TREC2002: Filtering, QA, Web and Video Tasks</title>
		<author>
			<persName coords=""><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,360.11,709.49,141.86,9.49">Proceedings of the TREC-11</title>
		<meeting>the TREC-11<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
