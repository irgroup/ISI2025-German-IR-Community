<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,242.88,87.44,126.24,14.88;1,134.28,120.86,343.43,14.88;1,231.30,139.22,149.42,14.88">IIT at TREC-2003 Task Classification &amp; Document Structure for Known-Item Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,116.64,186.72,56.81,9.57"><forename type="first">Steve</forename><surname>Beitzel</surname></persName>
						</author>
						<author>
							<persName coords="1,180.45,186.72,48.53,9.57"><forename type="first">Eric</forename><surname>Jensen</surname></persName>
						</author>
						<author>
							<persName coords="1,117.66,199.38,69.08,9.57"><forename type="first">Rebecca</forename><surname>Cathey</surname></persName>
							<email>cathey@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,194.35,199.38,38.15,9.57"><forename type="first">Ling</forename><surname>Ma</surname></persName>
							<email>maling@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,105.18,212.04,71.61,9.57"><forename type="first">David</forename><surname>Grossman</surname></persName>
							<email>grossman@ir.iit.edu</email>
						</author>
						<author>
							<persName coords="1,184.81,212.04,60.16,9.57"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Information Retrieval Lab Department of Computer Science Illinois Institute of Technology Chicago</orgName>
								<orgName type="department" key="dep2">Illinois Abdur Chowdhury</orgName>
								<address>
									<settlement>Greg Pass</settlement>
									<country>Herman Vandermolen</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Inc. Dulles</orgName>
								<orgName type="laboratory">Search Technologies Group America Online</orgName>
								<address>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,242.88,87.44,126.24,14.88;1,134.28,120.86,343.43,14.88;1,231.30,139.22,149.42,14.88">IIT at TREC-2003 Task Classification &amp; Document Structure for Known-Item Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B70B5C918E8C75A9D1B7A692A865D46</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Known-item search</term>
					<term>document structure retrieval</term>
					<term>query task classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This year's TREC 2003 web task incorporated two retrieval tasks into a single set of experiments for</head><p>Known-Item retrieval. We hypothesized that not all retrieval tasks should use the same retrieval approach when a single search entry point is used. We applied task classifiers on top of traditional web retrieval approaches. Our traditional retrieval is based on fusion of result sets generated by query runs over independent parts of the document structure. Our task classifiers combine query term analysis with known information resources and URL depth. This approach to task classification shows promise: our classified runs improved overall MRR effectiveness over our traditional retrieval results by ~10%; provided an MRR of .665; ranked 87% of relevant results in the top 10; correctly ranked the #1result 56% of the time. 67% of the queries performed above the average, and 49% above the median.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Many years of research have been devoted to examining the question of what is the best retrieval strategy for retrieving information. This year we explore a variation on the task in which a specific home/named page or known-item is sought after given a query or topic. Our research this year builds on prior knownitem and homepage retrieval techniques by examining the question of whether these two tasks should be treated differently.</p><p>Basic retrieval work has focused on ranking strategies: for example, some of the most studied algorithms include PDLN (Pivoted Document Length Normalization) <ref type="bibr" coords="1,331.60,585.15,10.98,8.74" target="#b0">[1]</ref>, Okapi BM25 <ref type="bibr" coords="1,406.69,585.15,11.03,8.74" target="#b1">[2]</ref>, Self-Relevance <ref type="bibr" coords="1,489.42,585.15,11.01,8.74" target="#b2">[3]</ref>, and Language Models <ref type="bibr" coords="1,166.28,596.79,15.36,8.74" target="#b17">[18]</ref>. All these ranking strategies try and find better ways to estimate relevance, as do many of the newer language models. In our tests, BM25 has consistently outperformed the other strategies, so we use it in our experiments.</p><p>Web retrieval extends basic full-text retrieval by using link and document structures to provide various document representations <ref type="bibr" coords="1,196.36,655.29,11.01,8.74" target="#b3">[4]</ref>. This multi-document representation approach was shown to be effective in the top web track systems at the 2002 TREC conference. The basic hypothesis is that content developers use HTML elements/tags to improve the readability of their documents, thus using that information during the ranking process via multiple document representations will improve effectiveness. Examples of these representations could be title, section headers, anchor text, bold, underlines, comments, referring page anchor text, etc. We initially focus on title, anchor text, and referring anchor text.</p><p>Given multiple document representations, the most fitting method of using and combining those representations for a given query becomes a research question. In recent years, the category of work known as data fusion, or multiple-evidence, describes a range of techniques in information retrieval whereby multiple pieces of information are combined to achieve improvements in retrieval effectiveness. These pieces of information can take many forms including different query representations, different document representations, and different retrieval strategies used to obtain a measure of relationship between a query and a document.</p><p>Several researchers have used combinations of different retrieval strategies to varying degrees of success in their systems <ref type="bibr" coords="2,144.99,213.57,11.35,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,158.88,212.94,7.57,9.57" target="#b5">6]</ref>. Belkin et al. examined the effects of combining several different query representations to achieve improvements in effectiveness <ref type="bibr" coords="2,265.79,226.23,11.40,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,281.22,225.60,7.55,9.57" target="#b7">8]</ref>. Lee examined the effect of using different weighting schemes to retrieve different sets of documents using a single query and document representation, and a single retrieval strategy <ref type="bibr" coords="2,188.46,250.35,11.05,8.74" target="#b8">[9]</ref>. Fox and Shaw examined combination algorithms that increase the score of a document based on repeated evidence of its relevance in <ref type="bibr" coords="2,318.81,262.05,10.63,8.74" target="#b4">[5]</ref>.</p><p>One of the algorithms designed by Fox and Shaw, CombMNZ, has proven to be a simple, effective method for combining result sets. It was used by Lee in his fusion experiments, and has become the standard by which newly developed result combination algorithms are judged. More recent research in the area of meta-search engines has led to the proposal of several new result combination algorithms <ref type="bibr" coords="2,467.13,320.49,16.88,8.74" target="#b9">[10,</ref><ref type="bibr" coords="2,487.80,319.86,13.54,9.57" target="#b10">11,</ref><ref type="bibr" coords="2,505.14,319.86,12.63,9.57" target="#b11">12]</ref>.</p><p>Although these algorithms were shown to be comparable, and on occasion superior, to CombMNZ, we use the widely-used CombMNZ for this work, leaving other approaches as a topic of further research.</p><p>Our traditional web search approach fuses the results from different document structure indices to produce a single ranked list for the known-item task. The results were fused using linear combinations based on estimated MRR values in order to maximize mutual evidence <ref type="bibr" coords="2,338.23,389.67,15.37,8.74" target="#b12">[13]</ref>.</p><p>In the next section we describe our basic search approach in more detail. In the task classification section we present our approach to using task information to improving task and overall system effectiveness. Lastly, we present our experimental results and conclude with future possible research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Traditional Search Approach</head><p>To conduct our research we use the IIT retrieval system AIRE (http://ir.iit.edu/projects/AIRE.html) <ref type="bibr" coords="2,501.76,476.91,16.18,8.74" target="#b13">[14]</ref>. This system builds a traditional inverted index based on a given document structure(s). For stemming, our system uses conflation classes <ref type="bibr" coords="2,214.71,501.09,17.71,8.74" target="#b14">[15]</ref> instead of a more commonly used stemmer such as Porter <ref type="bibr" coords="2,471.74,501.09,16.15,8.74" target="#b15">[16]</ref>. Those classes have been modified over the years as problem term variants have been encountered. Additionally, AIRE uses a generated statistical phrase list, where the statistical phrases were generated with a news collection and IDF filtering to reduce the final phrase list size. Phrases are generated via a bi-gram sliding window algorithm and weighted with 25% importance in relation to keyword weighting for retrieval. Basic term weighting uses the Okapi BM25, Equation <ref type="formula" coords="2,284.49,558.75,3.77,8.74">1</ref>.</p><formula xml:id="formula_0" coords="2,204.96,589.05,200.35,66.57">( ) ( )         + + + + *         + + - ∑ ) 3 ( * ) 1 3 ( * ) ( * ) 1 1 ( 5 . 5 . log qtf k qtf k tf K tf k n n N ) / * ) 1 (( * 1 avdl dl b b k K + - = Equation 1: Okapi BM25</formula><p>Where:</p><p>• tf = frequency of occurrences of the term in the document • qtf = frequency of occurrences of the term in the query • k3 = 7, set to 7 or 1000, controls the effect of the query term frequency on the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Parsing</head><p>We indexed the 18GB .GOV collection producing a full-text index, an HTML title term index, and an anchor text index. The anchor text index differed from the other indexes, in that an additional mapping stage was required so referencing anchor text data can be linked to the referenced TREC document name.</p><p>For our experimental layout we first produced a baseline run using BM25, conflation classes, phrases, and full-text indexing (referred to as the "Full text" run in the results summarized in Table <ref type="table" coords="3,437.21,233.70,3.63,8.77" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Fusion</head><p>Our linear combination consists of the following steps. First, for each document representation retrieved, the scores are normalized using exponential z-score normalization, as in Equation <ref type="formula" coords="3,422.16,282.00,3.77,8.77">3</ref>. The advantage of this method is that it preserves all relationships of the values exactly; it does not introduce any potential bias into the data. The final scores are calculated using CombMNZ, as in Equation <ref type="formula" coords="3,420.64,304.98,3.77,8.77">2</ref>, where each individual score is biased via weights assigned to the document structure by prior MRR estimates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Classification</head><p>To explore our hypothesis, we identify home pages via two techniques. The first technique uses known information resources and seeks to match those resources to queries. The second approach classifies queries based on keywords like "homepage", and then uses probability distributions of URL length to improve the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Known-Resource Matching</head><p>As many of the homepages in the .GOV domain are government agencies, we hypothesized that simply pairing queries with homepages by matching names and acronyms of agencies would be effective. We searched the web for lists of government agencies and their associated acronyms and homepages, choosing http://www.ulib.iupui.edu/subjectareas/gov/docs_abbrev.html because it provided all three pieces of information, was reasonably large, and was easy to parse with a simple regular expression.</p><p>We matched queries to this parsed list of agency name, acronym, and URL tuples using the matching algorithm below. Our system matched 26 of the 300 queries, found the correct homepage for 24 of them, improving our results for 11 queries over our traditional web approach combined with URL normalization. We combined these matching homepages with the final result sets by simply inserting them at rank one. Of the matching queries, 13 already had the matched result at rank one in our final fused, URL lengthweighted result set and 2 had not previously been found in the top 1000 results. The other 11 queries matched the relevant homepage, so inserting that homepage at the first result instead of its previous lower position in the result set offered an improvement. In total, MRR was improved by 0.05. Our complete known-resource matching algorithm is shown in Figure <ref type="figure" coords="4,315.56,86.10,3.77,8.77" target="#fig_2">1</ref>.</p><p>Known-Resource Matching Algorithm:</p><p>Step 1. Strip "home", "homepage", and "page" from the query. Strip "the" if it appears as the first word.</p><p>Step 2. If the remaining query is an acronym (any sequence of capital letters and spaces), look it up in the list of acronyms by case-insensitive exact string matching. Else, remove any acronyms that might be present alongside other terms from the query, normalize the spacing in the query, and look it up in the list of agency names by case-insensitive exact string matching.</p><p>Step 3. If we found a matching acronym or agency name, convert its URL to a canonical form by stripping "http://", "www", trailing slashes, etc. and look it up in a list of all the URLs in the .GOV by case-insensitive exact string matching.</p><p>Step 4. If we found a matching acronym, but could not find its corresponding URL in the .GOV, look for its corresponding URL with the last path element stripped off and just the matching acronym as "http://www.ACRONYM.gov" in the .GOV </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Classification</head><p>Kraaij, Westerveld, and Hiemstra <ref type="bibr" coords="4,229.91,318.84,16.73,8.77" target="#b16">[17]</ref> previously examined differences in the distributions of URL depth (the length of the path in the URL) between known home pages (from TREC-2001 answers) and the WT10g test collection. They showed that these distributions were very different, and that this could be used to improve the ranking of the results for home page queries. Thus it appeared that if we would be able to successfully classify queries as either home page queries or something else (named page queries in this case), we should be able to improve the results for the homepage queries.</p><p>We used a definition of URL depth that was slightly different from the one used by Kraaij et al. but confirmed the differences in distributions. We removed from the URL the leading parts, including host, domain, port, etc., up to the path. We then removed trailing occurrences of "index.htm" and "index.html", and counted the number of path elements remaining to determine the URL depth. The graph below shows the URL depth distribution for the WT10g collection and the correct answers for the TREC-2001 homepage task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U R L d e p t h % URLs</head><p>w t1 0 g</p><p>For TREC-2003 we ran the same analysis against the .GOV collection and the now known correct answers (qrels) for both the homepage queries and the named-page queries. The analysis shows that for homepage queries, the same distribution differences can be seen between the correct answers and the collection as a whole that were observed in TREC-2001. In addition, it shows that the URL depth distributions for the named page query results are virtually identical to that of the collection as a whole, and thus no advantage can be gained for named page queries. To determine if there were other variables we could utilize, we examined last-modified-date, and in-domain and out-domain link information, and found no significant difference in distributions for the correct answers versus the .GOV collection as a whole.</p><p>To be able to take advantage of the URL depth information for home page queries without disturbing the rankings for the named page queries, we attempted to classify the queries into one of these two groups. We created a list of 32 keywords that we believed were good indicators of a home page query. This list includes words like "home", "homepage", "administration", "agency", etc. Some of these terms were generic, but many would likely be specific to the GOV collection. We parsed the queries looking for these words. Our algorithm categorized 108 (36%) queries (out of 300 combined) as home page queries. Of those 108, 15 were false positives, and 93 were correctly classified. Of the 150 home page queries in the query set, 57 did not match any of the criteria in the classifier and were not marked as home page queries (false negatives).</p><p>We took the results from the fusion run and modified the scores of the documents for those queries that our classifier marked as home page queries. The algorithm for the score boosting is shown in Equation <ref type="formula" coords="5,488.65,601.28,3.91,8.77">4</ref>:</p><formula xml:id="formula_1" coords="5,272.82,633.64,66.21,11.65">S i * = S i + α P(d i )</formula><p>Equation 4: Score Boosting Formula where S i * is the newly assigned score of document i, S i is the original score of document i (after fusion), α is a constant, d i is the URL depth of document i, and P(d i ) is the probability that a document would have URL depth d i if it was given that it was a home page. After some experimentation we set the value of α to 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our approach is to build on prior approaches to known-item retrieval. To that end, we first examined the effectiveness of a full text approach based solely on BM25 ranking. In the following table we see that our estimated (.52) and actual (.42) effectiveness of this approach can be improved.</p><p>We next followed the structured approach that others have shown to be effective by exploiting HTML structure. In the second set of experiments we fused the title, anchor, and full text indices with the CombMNZ algorithm with linear weighting based on estimated MRR values. The Appendix displays the results of those experiments; while we did not have the final qrels, our estimated qrels provided equivalent results to real probabilities. The overall improvement of using document structure over full text retrieval by using CombMNZ with MRR linear combinations improved our effectiveness by 42%.</p><p>We next examined our use of known resource information to our traditional web based search approach. By using our known resource information that is based on that task, our MRR is .65.</p><p>Next, we examined our classification approach over prior web techniques. With the usage of prior probability factoring with our task classifier, we improved the effectiveness of the system by 3% estimated and 4% actually. We then examined this effectiveness assuming a perfect classifier, and found that our MRR increased to .663, or an additional 4% improvement.</p><p>Finally, we examined the improvements of combining both our known resource and URL factoring on the overall effectiveness: we found that by combining those approaches our MMR was raised to .665 and with a perfect classifier .685, an improvement of 9% over our fused results and 55% over our full text results. Our final iit03sau approach for the known-item task was 49% of the time equal or above the median and 67% above the mean score of submitted runs. Additionally, our approach produced the item in the top 10 results 87% of the time and only missed 7% of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>These results provide validation of the robustness of our task algorithm; more research needs to be conducted to find other task specific information to determine how that information should be incorporated into the ranking strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminary Failure Analysis</head><p>Examining Table <ref type="table" coords="7,165.21,315.12,5.03,8.77" target="#tab_2">2</ref> gives some indication of how each approach performs. Notice for the iit03sa and iit03sau runs that relevant documents are not found for approximately 7% of the queries, and are found in the top ten for 87% of the queries, leaving 6% for which the relevant document is found, but is poorly ranked. For failure analysis we focus on these runs, as they achieve the highest MRR.</p><p>We examined the queries where the relevant document was not found or poorly ranked (found, but not in the top ten). For each of these queries from the iit03sa and iit03sau runs, we examined the relevant documents and noted where in the each document the query text was present. Our hope was that this analysis would give us an idea of which of our document representations was most likely to contain query text for queries that performed poorly, and we could use that knowledge to improve our parsers. This analysis is shown in Table <ref type="table" coords="7,198.44,424.62,3.77,8.77" target="#tab_3">3</ref>, where the left side represents queries where the relevant document was poorly ranked, and the right side represents queries for which the relevant document was missing.</p><p>From Table <ref type="table" coords="7,140.85,453.60,5.03,8.77" target="#tab_3">3</ref> we see that for queries where the relevant document is missing or poorly ranked, the query text appears within the relevant document approximately 95% of the time. Since the title is most likely to contain the query text (72-82%), it is reasonable to conclude that our title parser might be failing often on these documents. Query text is also likely to appear in the body of the document (56-62%), and particularly in the anchor text for queries where the relevant document is missing (50-52%). From these results we conclude that our title and body parsers may be at fault and worthy of further examination.</p><p>To examine this further, we attempted to determine where our parsers were failing, paying particular attention to the anchor text and title parsers. We noticed that although the title parser extracts most of the titles, there are several instances where it fails for various idiosyncratic reasons. Likewise, the anchor text parser also worked fairly well, extracting all the data from within the &lt;a&gt; tags. However, it did not extract any data where the "href" parameter of the &lt;a&gt; tag was pointing to itself. For example, when the "#" sign is used with hyperlinks it is usually followed by redirection within the page such as "top" or "back". However, in some cases, there are hyperlinks with "#" sign followed by meaningful text. In the case of "&lt;a href = "#3214"&gt;u.s.s. monitor &lt;/a&gt;", our anchor text parser would ignore "u.s.s. monitor". By improving the efficiency of both the title and anchor text parsers, we believe the accuracy of each individual result file can be improved, thus improving the accuracy of the final results file.</p><p>We also did some analysis to try and determine whether our fusion process was causing relevant documents to be pushed down in the result set to poor ranks. To test this, we examined the three result files used in the fusion process: iit03wp75, iit03t_np, and iit03a_np. Once again we examined two cases: poorly ranked documents and missing documents. In the final result file there are 18 queries that perform poorly and 21 queries that don't return relevant documents. For each of these queries, we calculated the distribution of relevant documents over each individual result file. In addition, we computed the percentage of relevant documents whose ranks occurred in the top ten, in between ten and fifty, and over fifty. The results of this analysis are given in Table <ref type="table" coords="8,200.13,86.10,3.77,8.77" target="#tab_4">4</ref>.</p><p>For queries with poor performance, relevant documents are most likely to occur in the title (45%) and word (66%) result files. Overall, 50% of the relevant documents occurred without overlap in the top ten of the individual result files, and the fusion process increases these ranks, thereby damaging overall MRR.</p><p>Similarly, for queries whose relevant documents are missing, the word and title result files contain a high percentage of relevant documents. Unfortunately in this case the majority of the relevant documents occurring in the word and title result files have ranks over fifty prior to fusion, therefore, performance on these queries was initially bad and the fusion process is not likely to have had a significantly negative impact in this case. From the analysis we can conclude that a more finely tuned fusion method may have the potential to help cases where the relevant documents end up poorly ranked in the final result set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poorly</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This year we participated in the homepage and known-item web retrieval task. We explored the concept of multiple tasks being issued via the same interface. To that end we explored using a task classification approach where we could use task specific information to improve those queries. This approached showed promise in that by using task specific information our results improved ~10% over our baseline traditional web retrieval approach and would have improved by 12% given an optimal classifier. Given the simplicity of our classifier this approach seems to help the overall system effectiveness. Our future work will continue examining other features that can help in the other tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,108.00,71.83,103.60,12.26;3,108.00,84.07,146.80,12.26;3,108.00,96.25,214.50,12.26;3,108.00,108.49,228.01,12.26;3,108.00,120.67,51.35,12.26;3,108.00,132.91,379.96,12.26"><head>•</head><label></label><figDesc>dl = document length • avdl = average document length • N = is the number of documents in the collection • n = is the number of documents containing the word • k1 = 1.2 • b = 0.75 or 0.25 (we use .75 for full text and .25 for shorter representations, see appendix)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,162.72,350.02,283.55,7.85;3,254.70,378.81,102.67,8.74;3,168.24,408.18,249.01,8.77;3,100.98,437.85,410.07,8.74"><head>Equation 3 :</head><label>3</label><figDesc>CombMNZ = SUM(Individual Similarities) * Number of Nonzero Similarities Equation 2: CombMNZ norm_score(d,x) = ((e^(orig_score(d)) -mean(x)) / stddev(x)) Exponential Z-Score Normalization for document d and document representation x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,202.56,287.73,206.79,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Known-Resource Matching Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,214.74,172.71,182.56,8.74;5,265.19,195.59,81.49,3.94;5,117.78,379.01,11.89,4.36;5,113.94,350.87,15.73,4.36;5,113.94,322.66,15.73,4.36;5,113.94,294.52,15.73,4.36;5,113.94,266.26,15.73,4.36;5,113.94,238.12,15.73,4.36;5,113.94,209.92,15.73,4.36;5,146.15,384.83,2.17,4.36;5,168.29,384.83,2.17,4.36;5,190.37,384.83,2.17,4.36;5,212.45,384.83,2.17,4.36;5,234.59,384.83,2.17,4.36;5,256.67,384.83,2.17,4.36;5,278.99,384.83,2.17,4.36;5,301.07,384.83,2.17,4.36;5,323.20,384.83,2.17,4.36;5,345.28,384.83,2.17,4.36;5,365.38,384.83,6.01,4.36;5,387.64,384.83,6.07,4.36;5,409.78,384.83,6.01,4.36;5,431.86,384.83,6.01,4.36;5,453.99,384.83,6.01,4.36;5,476.07,384.83,6.01,4.36;5,498.15,384.83,6.07,4.36;5,315.71,392.58,17.15,3.63;5,102.55,293.76,3.63,6.10;5,373.47,258.47,8.92,4.12;5,373.46,264.27,26.55,4.12;5,373.44,270.07,26.55,4.12;5,373.43,275.88,22.08,4.12;5,373.41,281.68,22.08,4.12"><head>Table 2 :U R L d e p t h d i s t r i b u t i o n 0</head><label>20</label><figDesc>GOV collection URL Distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,128.76,474.51,357.07,216.98"><head>Table 1 : WT10g collection URL distributions U R L d e p t h d i s t r i b u t i o n 2 0 0 1</head><label>1</label><figDesc></figDesc><table coords="4,128.76,511.79,357.07,179.70"><row><cell>6 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>1 0</cell><cell>1 1</cell><cell>1 2</cell><cell>1 3</cell><cell>1 4</cell><cell>1 5</cell><cell>1 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,113.22,392.49,385.68,323.82"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="6,113.22,392.49,385.68,323.82"><row><cell></cell><cell>Run Tag</cell><cell>Description</cell><cell>Training</cell><cell>Actual</cell><cell>W/</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MRR</cell><cell>MRR</cell><cell>Perfect</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&amp; P Dist</cell></row><row><cell>Full text</cell><cell cols="2">iit03wp75 Full text using</cell><cell>.52</cell><cell>.43</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell>statistical phrases</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>weighted at 0.25, BM25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>with b=0.75</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fusion</cell><cell cols="2">Iit03wtaez CombMNZ(fulltext</cell><cell>.62</cell><cell>.61</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell>b=0.75, title b=0.25,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>anchor b=0.25) Using</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Z-Score and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Exponential</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Normalization</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fusion,</cell><cell>iit03sa</cell><cell>Same fusion, insert</cell><cell>.6889</cell><cell>.65</cell><cell>.65</cell></row><row><cell>Known-Resources</cell><cell></cell><cell>known resources with</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>matching names or</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>acronyms at first</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>position</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fusion,</cell><cell>iit03su</cell><cell>Same fusion, re-weight</cell><cell>.6430</cell><cell>.636</cell><cell>.663</cell></row><row><cell>URL Length Weighting</cell><cell></cell><cell>results using prior</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>probabilities of</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>relevance given URL</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>lengths calculated by</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>maximum likelihood of</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>training qrels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fusion,</cell><cell>iit03sau</cell><cell>Same fusion, same re-</cell><cell>.6945</cell><cell>.665</cell><cell>.685</cell></row><row><cell>URL Length Weighting,</cell><cell></cell><cell>weighting based on</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Known-Resources</cell><cell></cell><cell>URL length priors, and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>same known-resource</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>insertion</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,210.24,182.49,191.52,8.74"><head>Table 2 : Submitted Runs Official Evaluation</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,136.98,232.23,338.13,186.11"><head>Table 3 : Presence of Query Terms in Documents Poorly Ranked Missing</head><label>3</label><figDesc></figDesc><table coords="8,136.98,232.23,338.13,186.11"><row><cell></cell><cell></cell><cell></cell><cell>Ranked</cell><cell cols="2">Missing</cell><cell></cell></row><row><cell></cell><cell></cell><cell>iit03sa</cell><cell cols="3">Iit03sau Iit03sa iit03sau</cell><cell></cell></row><row><cell></cell><cell>Title</cell><cell>.72</cell><cell>.72</cell><cell>.82</cell><cell>.81</cell><cell></cell></row><row><cell></cell><cell>Word</cell><cell>.56</cell><cell>.56</cell><cell>.59</cell><cell>.62</cell><cell></cell></row><row><cell></cell><cell>ImgAlt</cell><cell>.44</cell><cell>.39</cell><cell>.45</cell><cell>.48</cell><cell></cell></row><row><cell></cell><cell>Anchortext</cell><cell>.39</cell><cell>.39</cell><cell>.50</cell><cell>.52</cell><cell></cell></row><row><cell></cell><cell>Meta</cell><cell>.28</cell><cell>.28</cell><cell>.14</cell><cell>.10</cell><cell></cell></row><row><cell></cell><cell>Not Found</cell><cell>.06</cell><cell>.06</cell><cell>.05</cell><cell>.05</cell><cell></cell></row><row><cell>Rank in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">resultfile iit03wp75 iit03t_np Iit03a_np iit03wp75 iit03t_np iit03a_np</cell></row><row><cell>top 10</cell><cell>.22</cell><cell>.17</cell><cell>.11</cell><cell>.05</cell><cell>.0</cell><cell>.0</cell></row><row><cell>10-50</cell><cell>.44</cell><cell>.28</cell><cell>.22</cell><cell>.10</cell><cell>.10</cell><cell>.0</cell></row><row><cell>over 50</cell><cell>.28</cell><cell>.28</cell><cell>.0</cell><cell>.57</cell><cell>.48</cell><cell>.29</cell></row><row><cell>total</cell><cell>.94</cell><cell>.73</cell><cell>.33</cell><cell>.72</cell><cell>.58</cell><cell>.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,163.44,428.73,285.09,8.74"><head>Table 4 : Presence of Relevant Documents in Individual Result files</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,90.00,590.31,385.94,124.82"><head>Table 5 : B-value = .25</head><label>5</label><figDesc></figDesc><table coords="8,90.00,612.89,385.94,102.24"><row><cell>Index</cell><cell>Run Description</cell><cell cols="3">Run Name Est. MRR Actual MRR</cell></row><row><cell cols="2">gov.anchor .GOV anchor terms only, good HTML parser, no phrases</cell><cell>iit03a_np.dat</cell><cell>0.24</cell><cell>0.29</cell></row><row><cell cols="2">gov.anchor .GOV anchor terms only, good HTML parser, with phrases</cell><cell>iit03a_p.dat</cell><cell>0.24</cell><cell>0.30</cell></row><row><cell>gov.title</cell><cell>.GOV title terms only, good HTML parser, no phrases</cell><cell>iit03t_np.dat</cell><cell>0.34</cell><cell>0.33</cell></row><row><cell>gov.title</cell><cell>.GOV title terms only, good HTML parser, with phrases</cell><cell>iit03t_p.dat</cell><cell>0.35</cell><cell>0.34</cell></row><row><cell cols="2">gov.bigtext .GOV bigtext terms only, good HTML parser, no phrases</cell><cell>iit03b_np.dat</cell><cell>0.18</cell><cell>0.15</cell></row><row><cell cols="2">gov.bigtext .GOV bigtext terms only, good HTML parser, with phrases</cell><cell>iit03b_p.dat</cell><cell>0.18</cell><cell>0.15</cell></row><row><cell cols="3">gov.word .GOV conglomerate, Words only, good HTML parser, no phrases iit03w_np.dat</cell><cell>0.34</cell><cell>0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,90.00,151.23,393.93,192.31"><head>Table 6 : B-value = .75</head><label>6</label><figDesc></figDesc><table coords="9,90.00,173.87,393.93,169.68"><row><cell>Index</cell><cell>Run Description</cell><cell>Run Name</cell><cell cols="2">est. MRR actual MRR</cell></row><row><cell cols="2">gov.anchor .GOV anchor terms only, good HTML parser, no phrases</cell><cell>iit03a_np75.dat</cell><cell>0.29</cell><cell>0.33</cell></row><row><cell cols="2">gov.anchor .GOV anchor terms only, good HTML parser, with phrases</cell><cell>iit03a_p75.dat</cell><cell>0.29</cell><cell>0.33</cell></row><row><cell>gov.title</cell><cell>.GOV title terms only, good HTML parser, no phrases</cell><cell>iit03t_np75.dat</cell><cell>0.30</cell><cell>0.26</cell></row><row><cell>gov.title</cell><cell>.GOV title terms only, good HTML parser, with phrases</cell><cell>iit03t_p75.dat</cell><cell>0.30</cell><cell>0.26</cell></row><row><cell cols="2">gov.bigtext .GOV bigtext terms only, good HTML parser, no phrases</cell><cell>iit03b_np75.dat</cell><cell>0.18</cell><cell>0.16</cell></row><row><cell cols="2">gov.bigtext .GOV bigtext terms only, good HTML parser, with phrases</cell><cell>iit03b_p75.dat</cell><cell>0.18</cell><cell>0.16</cell></row><row><cell cols="3">gov.word .GOV conglomerate, Words only, good HTML parser, no phrases iit03w_np75.dat</cell><cell>0.49</cell><cell>0.42</cell></row><row><cell cols="3">gov.word .GOV conglomerate, Words only, good HTML parser, with phrases iit03w_p75.dat</cell><cell>0.52</cell><cell>0.43</cell></row><row><cell cols="2">gov.meta .GOV meta, Words only, good HTML parser, no phrases</cell><cell>iit03m_np75.dat</cell><cell>0.12</cell><cell>0.09</cell></row><row><cell cols="2">gov.meta .GOV meta, Words only, good HTML parser, with phrases</cell><cell>iit03m_p75.dat</cell><cell>0.11</cell><cell>0.09</cell></row><row><cell cols="2">gov.imgalt .GOV img/alt, Words only, good HTML parser, no phrases</cell><cell>iit03i_np75.dat</cell><cell>0.12</cell><cell>0.11</cell></row><row><cell cols="2">gov.imgalt .GOV img/alt, Words only, good HTML parser, with phrases</cell><cell>iit03i_p75.dat</cell><cell>0.12</cell><cell>0.11</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,103.26,417.64,291.59,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,172.48,417.64,140.74,7.85">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,322.98,417.64,44.37,7.85">ACM-SIGIR</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,103.38,435.10,223.55,7.85;9,326.88,433.04,4.68,5.23;9,333.90,435.10,188.19,7.85;9,90.00,445.60,60.56,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,181.31,435.10,60.03,7.85">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.82,435.10,74.11,7.85;9,326.88,433.04,4.68,5.23;9,333.90,435.10,159.22,7.85">Proceedings of the 4 th annual Text Retrieval Conference (TREC-4)</title>
		<meeting>the 4 th annual Text Retrieval Conference (TREC-4)</meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,103.68,463.00,413.62,7.85;9,517.32,460.94,4.68,5.23;9,90.00,473.44,250.75,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,169.95,463.00,260.90,7.85">TREC-7 Ad-Hoc, High precision and filtering experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,442.33,463.00,74.98,7.85;9,517.32,460.94,4.68,5.23;9,90.00,473.44,158.91,7.85">Proceedings of the 7 th annual Text Retrieval Conference (TREC-7)</title>
		<meeting>the 7 th annual Text Retrieval Conference (TREC-7)</meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,103.31,490.84,418.60,7.85;9,90.00,501.28,90.39,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,169.74,490.84,232.01,7.85">The Anatomy of a Large-Scale Hypertextual Web Search Engine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,411.86,490.84,110.04,7.85">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct coords="9,104.04,518.68,311.70,7.85;9,415.74,516.62,6.00,5.23;9,424.74,518.68,97.29,7.85;9,90.00,529.12,238.62,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,203.95,518.68,125.05,7.85">Combination of Multiple Searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,339.66,518.68,76.09,7.85;9,415.74,516.62,6.00,5.23;9,424.74,518.68,97.29,7.85;9,90.00,529.12,34.50,7.85">Proceedings of the 2 nd Text Retrieval Conference (TREC-2)</title>
		<meeting>the 2 nd Text Retrieval Conference (TREC-2)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.12,546.52,416.88,7.85;9,90.00,557.02,78.43,7.85;9,168.42,554.96,4.68,5.23;9,175.38,557.02,148.78,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,285.60,546.52,228.65,7.85">Automatic Combination of multiple ranked retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename><surname>Bartell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,90.00,557.02,78.43,7.85;9,168.42,554.96,4.68,5.23;9,175.38,557.02,73.17,7.85">Proceedings of the 17 th Annual ACM-SIGIR</title>
		<meeting>the 17 th Annual ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,104.52,574.36,417.47,7.85;9,90.00,584.86,164.99,7.85;9,254.94,582.80,4.68,5.23;9,261.90,584.86,148.81,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,299.25,574.36,222.75,7.85;9,90.00,584.86,76.25,7.85">The effect of multiple query representations on information retrieval performance</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,176.51,584.86,78.47,7.85;9,254.94,582.80,4.68,5.23;9,261.90,584.86,73.19,7.85">Proceedings of the 16 th Annual ACM-SIGIR</title>
		<meeting>the 16 th Annual ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.18,602.20,416.83,7.85;9,90.00,612.70,355.74,7.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,305.28,602.20,216.74,7.85;9,90.00,612.70,74.54,7.85">Combining evidence of multiple query representation for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,173.43,612.70,140.82,7.85">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,103.32,630.04,414.08,7.85;9,517.32,627.98,4.68,5.23;9,90.00,640.54,148.75,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,142.78,630.04,285.37,7.85">Combining Multiple Evidence from Different Properties of Weighting Schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,438.94,630.04,78.45,7.85;9,517.32,627.98,4.68,5.23;9,90.00,640.54,73.16,7.85">Proceedings of the 18 th Annual ACM-SIGIR</title>
		<meeting>the 18 th Annual ACM-SIGIR</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,108.48,657.88,301.64,7.85;9,410.16,655.82,4.68,5.23;9,417.24,657.88,104.77,7.85;9,90.00,668.38,286.59,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,239.04,657.88,81.83,7.85">Models for Metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,331.30,657.88,78.81,7.85;9,410.16,655.82,4.68,5.23;9,417.24,657.88,104.77,7.85;9,90.00,668.38,219.21,7.85">Proceedings of the 24 th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 24 th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,109.62,685.78,353.72,7.85;9,463.26,683.72,4.68,5.23;9,471.48,685.78,50.52,7.85;9,90.00,696.22,264.69,7.85" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,194.87,685.78,174.68,7.85">Relevance Score Normalization for Metasearch</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,381.08,685.78,82.27,7.85;9,463.26,683.72,4.68,5.23;9,471.48,685.78,50.52,7.85;9,90.00,696.22,237.59,7.85">Proceedings of the 10 th Annual ACM Conference for Information and Knowledge Management (CIKM)</title>
		<meeting>the 10 th Annual ACM Conference for Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.30,86.80,406.37,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,188.22,86.80,148.71,7.85">Condorcet Fusion for Improved Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.35,86.80,99.99,7.85">Proceedings of ACM-CIKM</title>
		<meeting>ACM-CIKM</meeting>
		<imprint>
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.33,103.24,408.98,7.85;10,517.32,101.18,4.68,5.23;10,90.00,113.62,352.50,7.85" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,194.22,103.24,230.71,7.85">Combining Document Representations for Known-Item Search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.85,103.24,81.46,7.85;10,517.32,101.18,4.68,5.23;10,90.00,113.62,325.95,7.85">Proceedings of the 26 th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 26 th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,109.80,130.96,387.29,7.85;10,497.04,128.90,4.68,5.23;10,505.44,130.96,16.55,7.85;10,90.00,141.46,180.97,7.85" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,200.46,130.96,206.45,7.85">Improved query precision using a unified fusion model</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,418.83,130.96,78.27,7.85;10,497.04,128.90,4.68,5.23;10,505.44,130.96,16.55,7.85;10,90.00,141.46,113.73,7.85">Proceedings of the 9 th Text Retrieval Conference (TREC-9)</title>
		<meeting>the 9 th Text Retrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.18,158.80,410.82,7.85;10,90.00,169.30,132.92,7.85" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,184.23,158.80,239.95,7.85">Corpus-based stemming using co-occurrence of word variants</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,436.26,158.80,85.74,7.85;10,90.00,169.30,73.66,7.85">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="1998-01">January, 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,108.30,186.70,276.25,7.85" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,139.86,186.70,117.03,7.85">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,266.13,186.70,29.12,7.85">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.26,203.14,414.80,7.85;10,90.00,213.52,431.97,7.85;10,90.00,223.84,191.33,7.85" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,267.04,203.14,210.80,7.85">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,493.54,203.14,28.52,7.85;10,90.00,213.52,404.60,7.85">Proc. of the 25th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>of the 25th annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.77,240.22,414.22,7.85;10,90.00,250.54,288.78,7.85" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,207.09,240.22,201.48,7.85">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,427.60,240.22,94.38,7.85;10,90.00,250.54,231.63,7.85">21st ACM Conference on Research and Development in Information Retrieval (SIGIR&apos;98)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
