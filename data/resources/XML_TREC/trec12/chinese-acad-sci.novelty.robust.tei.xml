<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.76,81.73,325.70,16.20">NLPR at TREC 2003: Novelty and Robust</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,225.48,107.24,49.27,10.80"><forename type="first">Qianli</forename><surname>Jin</surname></persName>
							<email>qljin@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.44,107.24,46.02,10.80"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.40,107.24,32.36,10.80"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.76,81.73,325.70,16.20">NLPR at TREC 2003: Novelty and Robust</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5747E525E8437EE23846110EA7AF6AC5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is the first time that the Chinese Information Processing group of NLPR participates in TREC. Our goal in this year is to test our IR system and get some experience about the TREC evaluation. So, we select two retrieval tasks: Novelty Track and Robust Track. We build a new IR system based on two key technologies: Window-based weighting method and Semantic Tree Model for query expansion. In this paper, the IR system and some new technologies are described first, and then some detail work and results in Novelty and Robust Track are listed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Window-based Weighting</head><p>The key algorithm of an IR system is similarity computing between queries and uery: "Can radio waves from radio towers or car phones affect brain cancer Document A: "John claimed his brain documents. Till now, the most popular algorithm is the inner product of vectors, and the vectors can be built by using weighting technologies, such as binary weight, tf-idf, query expansion, relevant feedback and etc. In other words, most of the existing algorithms are based on vector computing. However, this method usually gets limited precision, because sometimes, a vector can not represent a query properly. For instance, if we have the following query and two documents: Q occurrence?" cancer was caused by the wave from his cellular phone. That claim, put forth in a lawsuit, has no basis in accepted scientific fact."</p><p>"I was listening to the r Document B: adio, when the tower collapsed. I ran several blocks before my brain kicked in, and saw that another wave of people started running towards a police car." e definitely know that Document A is relevant to the Query, while Document B is e develop two key notions as follows: contributions , ased on the above two key notions, we developed three window-based models for the .2.1 Model One: Simple Window-based Model s in a document, the larger the W not. But if binary word vector model is used, the similarity value between Document B and the query is larger than the one between Document A and the query. We also try some other weighting technologies such as tf-idf, query expansion and etc., but find that in this case, based on vector computing, Document B seems more "relevant" to the query than Document A.</p><p>In order to solve this problem, w 1. Query words appearing closely in the document provide more to the similarity value than the ones appearing separately. The closer the query words in a document, the larger the similarity value between the query and the document.</p><p>2. Some query words, like named entities and baseNP are called "Core Words" while the other words are called "Surrounding Words". "Core Words" are much more important than "Surrounding Words", and should have special status in the retrieval processing (i.e. having larger weights). B application of information retrieval. They are called "Simple Window-based Model", "Dynamic Window-based Model" and "Core-window-based Model", from the simplest model to the most complex one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>As our first key notion, the closer the query word similarity value between the query and the document. So, we introduce a window in the retrieval processing. When the query words co-occur in the window, a larger similarity weight is provided. First we put all the words of the document into the word sequence orderly, like the Figure <ref type="figure" coords="3,261.35,75.86,4.50,10.80">1</ref>. Each sub-sequence with d continual words is included in a d-width window.</p><p>Let N denote the number of the words in the whole sequence, and denote the width can be represented as follows: ) between query and document  But what is the distribution of these query words in the window? They can be separate or conjoint. If these query words in the window are conjoint, they maybe form a phrase. As we all know, phrases usually are less ambiguous than words. So, we should give the conjoint query words larger weight than the separate query words in the window. Another problem in Model One is that it is difficult to decide the width of window in real applications. A fixed window width cannot be suitable for all queries. In order to solve the above two problems, Model Two is proposed, which is called Dynamic Window-based Model. In the new Model, a dynamic window width called "TightWin" is developed to modify the original fixed window. Define: T words in the original window.</p><formula xml:id="formula_0" coords="3,92.16,377.85,202.10,29.82">d N ∑ = + = i d i i SWin d q Sim 1 )</formula><p>In Figure <ref type="figure" coords="4,137.45,497.05,4.50,10.81" target="#fig_1">2</ref> ) can be represented as follows: is the binary signal of the jth word in the whole sequence. Here, is equal to ONE if the jth word is a query word, otherwise, it is equal to ZERO. And is the inverse document frequency of the jth word in the sequence. TightWin is defined above, and p is a parameter, which is larger than zero.</p><formula xml:id="formula_1" coords="4,92.22,625.20,209.31,32.13">∑ - = + = d N i d i i DWin d q Sim 1 ) 3 ......( ) , ( ) ,<label>( 2</label></formula><p>Compared with Model One, Model Two has an additional item  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.3">Model Three: Core Window-based Model</head><p>In the above two models, when query words appear closely in the document, they will be given larger weight. In some cases, it may bring some problems. Take a look at the above example query again.</p><p>"Can radio waves from radio towers or car phones affect brain cancer occurrence?" When the query words "radio waves" and "brain cancer" appear closely in a document, we can say that this document is most likely relevant to the query. But, when the query words "car phone" and "affect" appear closely in a document, we are not sure whether it is relevant. So, based our second key notion, we parse the query sentence and classify the query words into two groups. They are "Core Words" and "Surrounding Words" defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define:</head><p>(1) The query words, which represent the main meaning of the query, such as baseNP and Named Entities, are called "Core Words". (2) The query words, which are not core words, are called "Surrounding Word".</p><p>(3) A window is called "Active Window", if and only if it includes Core Words.</p><p>Obviously, Core Words are much more important than Surrounding Word. So, Active Window should have larger weight than the common window. Let N denote the number of the words in the whole sequence, and d denote the width of the window. Then the similarity value between query and document in Model Three ( Sim ) can be represented as follows: ) , ( d q </p><formula xml:id="formula_2" coords="5,92.16,687.57,207.21,32.10">∑ - = + = N i d CWin d q Sim 1 ) 5 ......( ) ) ,<label>( 3 ) 6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Semantic Tree Model for Query Expansion</head><p>The key problem of query expansion is to compute the similarities between terms and the original query. In other words, the original query can be regard as a point in the semantic space, and the goal of query expansion is to select some additional terms, which have the closest meaning to the point. So, as the first step, like most of the former methods, we need to compute the prior similarities between the terms. And we use Term Similarity Trees to represent and estimate the similarities between terms, which can cluster the terms according to their meaning. Then, we use the TSTM to expand queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1">Grow Term Similarity Trees based on prior similarities between terms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1.1">Build elements of Term Similarity Tree</head><p>Let denote the prior similarity between the term q and p. For a given term q, use a tree to represent the sorted similarities in descending order between q ) , ( p q PSim and all the other words, like the left part of the following Figure . 

Note that the weight of the branch between and is , and:</p><formula xml:id="formula_3" coords="7,98.70,223.86,346.26,42.64">q m p ) , ( m p q PSim . ... ) , ( ... ) , ( 1 ≥ ≥ ≥ m p q PSim p q PSim</formula><p>Then, we keep the first m leaves and discard the rest to build m-best tree of prior similarities, like the right part of the Figure . There are several methods for computing the prior similarities between terms. Here, we use normalized local co-occurrence algorithm to estimate them. ) ,..., ,..., , (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1.2">Grow Term Similarity Tree</head><formula xml:id="formula_4" coords="7,113.58,376.39,389.05,60.17">2 1 K i q q q q Q = ) , , ( m v Q i q</formula><p>Each part framed by a quadrangle in the Figure denotes an element of TSTM, which is an m-best tree of prior similarities. Using the multi-level term similarity tree, we can easily compute the semantic similarity between two terms (one query term and one other term), no matter whether they co-occur in the training corpora. Note that each term in the query, like , has its own sub-tree, whose root is the query-term itself. We define: i q (a) A path between the query-term and its leave-term p is the route from the root node to the leave node p. i q i q (b) The weight of a path between the query-term and its leave-term p is the product of all the branch weights (prior similarity) on the path from to p. i q i q (c) The shortest path between the query-term and its leave-term p is the path between and p, which has the largest weight.</p><p>i q i q (d) The similarity between the query-term and its leave-term p is defined as the weight of their shortest path. For instance, we can compute similarity between two terms and as:</p><p>1</p><formula xml:id="formula_5" coords="8,155.58,246.86,305.75,58.87">q j i p , , 1 ) 7 ( ...... ) ,<label>( ) , ( ) , ( ) , ( , , 1 , 1 , 1 1 ,</label></formula><formula xml:id="formula_6" coords="8,168.06,281.58,277.09,5.85">, 1 1 , , 1<label>1</label></formula><formula xml:id="formula_7" coords="8,132.18,272.00,322.50,33.73">j i i i j i j i p p PSim p q PSim p q path shortest of weight p q Sim × = - - - =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">Query Expansion based on TSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>denote the original query including K terms, where is the ith term in query. The is the term similarity tree of query Q.</p><p>) ,..., ,..., , (</p><formula xml:id="formula_8" coords="8,113.58,352.81,389.05,44.76">2 1 K i q q q q Q = i q ) , , ( m v Q TSTM</formula><p>The term w is expanded to query Q, when w satisfies two conditions illustrated as the following two formulae:</p><formula xml:id="formula_9" coords="8,91.98,456.54,167.88,50.88">) 8 ( ...... ) ), , ,<label>( ( ) , ( ) , ( 1 </label></formula><formula xml:id="formula_10" coords="8,91.92,446.35,220.95,124.39">    × ≥ ≥ = ∑ = K percent w m v Q TSTM Overlay cv w q Sim w Q Sim K i i (q Sim i i q</formula><p>where is the similarity between the query Q and the term w. is the similarity between the term and w, which can be estimated as formula (7). And cv is the threshold value of similarity.</p><p>) , ( w Q Sim ) , w</p><p>) denotes the occurrence times of term w in the sub-trees of . It means that in the total K sub-trees of Q's term similarity tree, how many sub-trees include (overlay) the term w. percent is the threshold value of overlay degree.</p><p>), , , ( (</p><formula xml:id="formula_11" coords="8,97.56,604.93,127.24,42.02">w m v Q TSTM Overlay ) , , ( m v Q TSTM</formula><p>The first discrimination function in formula (8) is used to estimate the similarities between the term w and the query terms q respectively. And the second discrimination function in formula ( <ref type="formula" coords="8,317.18,738.67,4.66,10.82" target="#formula_1">2</ref>) is used to estimate the similarity between the term w and the whole query Q. The query Q can be regard as a point in K i q q q ,..., ,..., , 2 1 the semantic space, so we need to know whether the term w is close in meaning to this point, not just close to the independent meaning of each . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Novelty Track</head><p>The Novelty Track is designed to investigate systems' abilities to locate relevant and new information within a set of documents relevant to a TREC topic. The goal of the track is to find out relevant/new sentences, instead of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevant</head><p>Considering sentences have few words than documents, query expansion is much more important. So, we use the following process to deal with relevant retrieval: A) Two Stages Query Expansion. In the first stage, we use Term Similarity Model to expand queries. In the second stage, we use Relevant Feedback to modify and expand queries again to improve the retrieval result. Usually, top 20% sentences are used for relevant feedback, after the first retrieval. B) We use two different methods to compute similarities between queries and sentences. The first is the traditional tf-idf method. And the second is window-based method to ensure that the closer the query words in sentences, the higher the similarity value. Actually, this method is the expansion of N-gram model (because window-based method does not require the query words appearing directly continual). C) We use an existing method called 'pivoted document length normalization' to normalize sentence length. (see the Reference [3]) D) The similarity values of different topics are usually different. The main reason is that queries have different length and query words have different characteristics (i.e. idf). So, it's unreasonable to use a simple and fixed threshold for every topic. Here, we developed one dynamic threshold for one topic, based on the probabilistic characteristics of similarity values between this topic and sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Novelty (new)</head><p>Having relevant sentences, we have another task to filter out repeated information. We define a value called 'New Information Degree'(NID) to present whether a sentence includes new information related to the former sentences. If the value of NID is big, this sentence is reserved, or it will be discarded. There are two different ways to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,414.90,309.86,6.00,10.80;3,90.00,332.12,206.67,10.80;3,343.08,332.11,4.00,10.81;3,331.26,332.11,3.00,10.81;3,320.76,332.11,4.00,10.81;3,315.72,332.11,26.05,10.81;3,325.08,332.11,6.01,10.81;3,298.62,332.11,18.02,10.81"><head>d</head><label></label><figDesc>of the window. Then the similarity value (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,407.82,642.57,3.49,6.29;3,400.92,642.57,1.94,6.29;3,380.22,698.14,3.99,10.78;3,353.28,698.14,3.99,10.78;3,357.48,691.03,12.79,21.98;3,360.72,687.12,3.83,8.55;3,362.76,710.70,3.83,8.55;3,365.10,689.37,3.49,6.29;3,358.20,689.37,1.94,6.29;3,366.78,712.95,1.94,6.29;3,360.12,712.95,1.94,6.29;3,376.56,704.49,1.94,6.29;3,371.52,698.14,3.33,10.78;4,90.00,76.04,252.14,10.80;4,350.95,91.45,154.40,10.81;4,139.72,263.17,365.54,10.81;4,90.00,91.45,254.11,10.81;4,90.00,107.05,174.67,10.81"><head>2</head><label>2</label><figDesc>Model Two: Dynamic Window-based Model weight to the window, which ightWin is the smallest window width, which can overlay all the query In Simple Window-based Model, we give larger includes more than one query word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,141.96,497.05,363.44,10.81;4,196.53,559.43,308.83,10.85;4,90.00,512.65,415.34,10.81;4,90.00,528.25,415.30,10.81;4,90.00,543.85,88.72,10.81;4,90.00,559.45,106.63,10.81;4,90.00,575.05,415.32,10.81;4,90.00,597.31,4.00,10.81;4,142.56,597.31,4.00,10.81;4,130.86,597.31,3.00,10.81;4,120.36,597.31,4.00,10.81;4,114.36,597.29,26.97,10.85;4,124.68,597.29,6.03,10.85;4,95.94,597.29,18.08,10.85"><head></head><label></label><figDesc>, we give several examples about the value of TightWin. If the query words ber of the words in the whole sequence, and d denote the width distribute separately in the original window, the value of TightWin is large. And if they are conjoint, the TightWin is small. So, we should give a large weight when TightWin is small. Let N denote the num of the window. Then the similarity values between query and document in Model Two (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,497.64,699.34,4.02,10.86;4,491.88,699.34,6.03,10.86;4,470.28,699.34,22.11,10.86;4,460.08,699.34,4.02,10.86;4,448.86,681.46,4.02,10.86;4,422.22,681.46,4.02,10.86;4,411.24,699.34,4.02,10.86;4,404.10,699.34,6.03,10.86;4,399.36,699.34,4.02,10.86;4,372.78,699.34,4.02,10.86;4,366.36,699.34,6.03,10.86;4,361.62,699.34,4.02,10.86;4,336.30,699.34,6.03,10.86;4,308.34,699.34,4.02,10.86;4,286.50,699.34,4.02,10.86;4,275.22,681.46,4.02,10.86;4,248.64,681.46,4.02,10.86;4,237.66,699.34,4.02,10.86;4,230.52,699.34,6.03,10.86;4,225.24,699.34,4.02,10.86;4,199.86,699.34,3.02,10.86;4,192.66,699.34,4.02,10.86;4,151.80,699.34,4.02,10.86;4,126.48,699.34,3.02,10.86;4,119.22,699.34,4.02,10.86;4,465.66,697.37,3.52,6.33;4,433.68,672.65,3.52,6.33;4,427.08,672.65,1.96,6.33;4,435.30,696.41,1.96,6.33;4,428.94,696.41,1.96,6.33;4,445.02,687.95,1.96,6.33;4,384.18,690.53,3.52,6.33;4,377.64,690.53,1.96,6.33;4,385.80,714.29,1.96,6.33;4,379.44,714.29,1.96,6.33;4,395.52,705.77,1.96,6.33;4,319.80,690.53,3.52,6.33;4,313.20,690.53,1.96,6.33;4,321.42,714.29,1.96,6.33;4,315.06,714.29,1.96,6.33;4,357.78,705.77,1.96,6.33;4,331.14,705.77,1.96,6.33;4,292.02,697.37,3.52,6.33;4,260.10,672.65,3.52,6.33;4,253.50,672.65,1.96,6.33;4,261.72,696.41,1.96,6.33;4,255.36,696.41,1.96,6.33;4,271.44,687.95,1.96,6.33;4,415.62,708.76,44.91,10.86;4,439.80,681.46,3.35,10.86;4,390.36,699.34,3.35,10.86;4,343.20,699.34,12.74,10.86;4,325.92,699.34,3.35,10.86;4,242.04,708.76,44.91,10.86;4,266.22,681.46,3.35,10.86;4,218.16,699.34,6.03,10.86;4,203.76,699.34,3.35,10.86;4,196.44,699.34,3.35,10.86;4,168.00,699.34,25.47,10.86;4,144.78,699.34,6.03,10.86;4,130.38,699.34,3.35,10.86;4,123.06,699.34,3.35,10.86;4,92.10,699.34,28.14,10.86;4,426.30,674.34,12.88,22.15;4,376.80,692.15,12.88,22.15;4,312.42,692.15,12.88,22.15;4,252.72,674.34,12.88,22.15;4,429.48,670.39,3.86,8.61;4,431.40,694.15,3.86,8.61;4,379.98,688.27,3.86,8.61;4,381.96,712.03,3.86,8.61;4,315.60,688.27,3.86,8.61;4,317.52,712.03,3.86,8.61;4,255.90,670.39,3.86,8.61;4,257.82,694.15,3.86,8.61;4,300.00,695.46,6.61,14.77;4,209.40,695.46,6.61,14.77;4,158.58,695.46,6.61,14.77;4,136.02,695.46,6.61,14.77;4,90.00,737.71,29.35,10.81;4,189.31,737.71,4.00,10.81;4,163.08,737.71,3.00,10.81;4,155.58,737.71,4.00,10.81;4,182.04,737.69,6.03,10.85;4,167.16,737.69,3.35,10.85;4,159.54,737.69,3.35,10.85;4,127.56,737.69,28.11,10.85;4,173.04,733.85,6.58,14.71;4,201.36,737.69,304.05,10.85;5,90.00,75.83,415.33,10.85;5,90.00,97.45,113.70,10.81"><head></head><label></label><figDesc>denotes the similarity value between the query and the d-width window from the ith word to (i+d)th word in the whole sequence in Dynamic Window-based Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,217.56,103.91,1.96,6.33;5,212.28,97.49,3.35,10.85;5,214.86,166.31,1.96,6.33;5,180.00,135.11,1.96,6.33;5,174.66,128.69,3.35,10.85;5,200.10,159.89,12.73,10.85;5,463.02,229.87,3.51,6.31;5,430.62,205.51,3.51,6.31;5,423.72,205.51,1.95,6.31;5,432.30,229.21,1.95,6.31;5,425.64,229.21,1.95,6.31;5,442.08,220.69,1.95,6.31;5,411.60,241.21,44.74,10.82;5,437.04,214.27,3.34,10.82;5,457.62,231.79,4.00,10.82;5,445.74,214.27,4.00,10.82;5,418.74,214.27,4.00,10.82;5,407.04,231.79,4.00,10.82;5,423.00,207.18,12.84,22.07;5,426.24,203.25,3.84,8.58;5,428.28,226.95,3.84,8.58;5,468.42,231.79,36.92,10.82;5,90.00,263.05,415.31,10.82;5,90.00,278.65,227.01,10.82"><head></head><label></label><figDesc>adjustment to the original fixed window. The conjoint query words provide more contributions to the final similarity value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,90.12,378.43,16.03,10.82;7,235.38,378.43,251.36,10.82;7,90.12,403.45,415.24,10.82;7,90.12,425.71,39.55,10.82;7,179.34,425.71,326.08,10.82;7,90.12,450.25,415.27,10.82;7,90.12,465.85,284.34,10.82"><head></head><label></label><figDesc>Let denote the original query including K terms, where is the ith term in query. Then, we grow the Term Similarity Tree of query Q asTSTM , where v denotes the expanded level and m indicates that each element in term similarity tree is an m-best tree of prior similarities. The term similarity tree of query Q is shown in the following Figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="9,371.76,104.42,1.95,6.30;9,365.82,97.99,6.01,10.82;9,90.00,138.25,415.32,10.82;9,90.00,153.98,71.28,10.80"><head>q</head><label></label><figDesc>Detail evaluations of Semantic Tree Model for query expansion are included in the Reference<ref type="bibr" coords="9,144.27,153.98,12.76,10.80" target="#b1">[2]</ref>.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>define NID of the latter sentence related to the former sentence.</p><p>Sum the 'idf' value of words appeared in both sentences NID_1 = <ref type="figure" coords="10,136.12,122.65,284.80,10.82">1---------------------------------------------------------------------</ref>Sum the 'idf' value of words appeared in latter sentences</p><p>The number of matched bi-gram word sequences NID_2 = <ref type="figure" coords="10,136.13,185.05,319.69,10.82">1 -----------------------------------------------------------------------------</ref>Total number of bi-gram word sequences in latter sentences Usually, if the value of NID is bigger than 10%-20%, the latter sentence will be considered useful (including new information). 0.773 Form the above results, we find that the technologies of window-based weighting method and relevant feedback are useful. In task 2,3,4, we all get very big values of average F measure, but in task 1, it is small. The reason is that in task 1, we use TREC Novelty 2002 data as our training corpora, which have quite few relevant sentences. However, the data for 2003 have many relevant sentences (even more than 50% for some topics), so we should use small thresholds. The big thresholds from the training corpora (2002 data) make good precision and poor recall (also poor F measure). In task 2-4, we use a part of the 2003 data as the training corpora and get better F values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Official Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dyn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Robust Track</head><p>As a mature IR system, the robustness is quite important. The goal of the Robust Track is to improve the consistency of retrieval technology by focusing on poorly performing topics. So, in this track, we improve our system based on the following two points: 1) Considering that even the worst topic should have an acceptable result, a robust algorithm of similarity should be produced to improve precision for each topic. 2) Though at most 1000 documents will be accepted by NIST per topic, we suppose that most users only look through the former part of retrieval result (Maybe 2 or 3 screens). So, for each run, we submit several dozens of retrieved documents. We should make sure that the true relevant documents have the top similarity values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Processing</head><p>In order to improve the robustness described above, we use a combined algorithm, including four technologies. The processing of our system is as follows: A) Query expansion based on Term Similarity Trees. It is described in the first part of this paper and also in the Reference <ref type="bibr" coords="11,301.17,700.93,12.74,10.82" target="#b0">[1]</ref>. B) Window-based weighting methods for computing similarities. It is described in the first part of this paper and also in the Reference <ref type="bibr" coords="11,362.88,732.13,12.69,10.82" target="#b1">[2]</ref>. C) Length normalization (see the Reference The main reason is that queries have different length and query words have different characteristics (i.e. df). So, it's unreasonable to use a simple and fixed threshold for every topic. Here, we developed one dynamic threshold for one topic, based on the probabilistic characteristics of similarity values between this topic and documents. From the above table, we can see that we get a low value of average precision. The reason is that for each topic, only several dozens of retrieved documents are returned, not 1000. We suppose that most users only look through the former part of retrieval result and a robust IR system should ensure that users can find relevant documents at the top 10 or 20. Based on the experiments, the window-based methods, especially core window based method, outperform most traditional tf-idf weighting method. Detail evaluation and discussion are given in the paper of reference <ref type="bibr" coords="12,416.17,578.53,12.75,10.82" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Official Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,108.78,640.93,396.54,10.82;12,108.00,656.53,381.83,10.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,260.52,640.93,240.11,10.82">Window-based Method for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Qianli</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,137.79,656.53,352.04,10.82">The First International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,108.78,672.13,396.49,10.82;12,108.00,687.73,397.32,10.82;12,108.00,703.33,93.63,10.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,260.46,672.13,240.01,10.82">Query Expansion based on Term Similarity Tree</title>
		<author>
			<persName coords=""><forename type="first">Qianli</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,136.63,687.73,368.69,10.82;12,108.00,703.33,56.51,10.82">International Conference on Natural Language Processing and Knowledge Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,113.18,718.93,392.21,10.82;12,108.00,734.53,136.67,10.82" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,367.74,718.93,137.65,10.82;12,108.00,734.53,67.20,10.82">Pivoted Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
