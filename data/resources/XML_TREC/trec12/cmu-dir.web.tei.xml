<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.04,75.87,391.89,13.93;1,161.34,93.69,289.26,13.93">Combining Structural Information and the Use of Priors in Mixed Named-Page and Homepage Finding</title>
				<funder ref="#_K94dhgT">
					<orgName type="full">Advanced Research and Development Activity in Information Technology</orgName>
					<orgName type="abbreviated">ARDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,245.16,123.76,50.35,9.02"><forename type="first">Paul</forename><surname>Ogilvie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,314.88,123.76,51.87,9.02"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.04,75.87,391.89,13.93;1,161.34,93.69,289.26,13.93">Combining Structural Information and the Use of Priors in Mixed Named-Page and Homepage Finding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">524ADF0F4A85FDDDE6A080838AA78161</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Carnegie Mellon University's experiments on the mixed named-page and homepage finding task of the TREC 12 Web Track. Our results were strong; we achieved the success using language models estimated from combining information from document text, in-link text, and information present in the structure of the documents. We also present experiments using expectations about posterior distributions to create class-based prior probabilities. We find that priors do provide a large gain for our official runs, but we do further experiments that show the priors do not always help. Some preliminary analysis shows that the prior probabilities are not providing the desired posterior distributions. In cases where applying the priors harm performance, the observed posterior distributions in the rankings are far off of the desired posterior distributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Documents found on the Internet are rich in structure, and this provides many information sources useful for retrieval. In particular, structural information has been found useful for known-item searches such as homepage finding and named-page finding <ref type="bibr" coords="1,204.06,349.66,58.79,9.02" target="#b1">[Craswell 2001</ref>][ <ref type="bibr" coords="1,271.45,349.66,46.00,9.02" target="#b2">Kraaij 2002</ref>] <ref type="bibr" coords="1,321.81,349.66,56.62,9.02" target="#b3">[Ogilvie 2003</ref>][ <ref type="bibr" coords="1,387.91,349.66,43.67,9.02" target="#b5">Zhang 2002</ref>]. A known-item retrieval system should attempt to leverage the structural information in a manner that is consistent with its model and provides an improvement in retrieval performance.</p><p>In this paper, we present experiments where we combine structural information using language modeling. We create several document representations for each document using the structural information present in HTML documents. From these document representations, we form language models. We combine the language models using a linear interpolation to form a new language model. This new language model is then used to estimate the probability that the document has generated the query.</p><p>It is also possible to leverage query independent information through the use of document priors. Document priors are probabilities or beliefs that the document is relevant to the query independent of any knowledge about the query. In previous homepage finding experiments, <ref type="bibr" coords="1,251.82,476.62,50.20,9.02" target="#b2">[Kraaij 2002</ref>] found a prior based on the type of the URL to be a very effective source of information. The URL types ROOT, SUBROOT, FILE, and PATH form four distinct document classes. We hypothesize that these classes will also be useful for a mixed homepage/named-page finding task, and present experiments using these priors. The prior probabilities are estimated from training data which gives us a desired posterior distribution. This desired posterior distribution defines ratios of the document classes what we would like to observe in the rankings. Using Bayes' rule, we can estimate the prior probabilities from training data and corpus statistics. A detailed derivation is provided Section 5.</p><p>In the next section, we describe the basic generative language model where we are using information from only one document representation. Section 3 describes combining information from several language models. Section 4 briefly discusses system specifics for the experiments. Section 5 provides a detailed derivation of the priors and posterior distribution we used for the mixed homepage/named-page finding track of the TREC 12 Web Track, and Section 6 describes our official runs and other experiments. Section 7 provides a discussion of the URL priors and their effectiveness in producing the desired posterior distributions over the rankings. We state conclusions in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Generative Language Models</head><p>A unigram language model defines a multinomial probability distribution over all words in the vocabulary of the corpus. These probabilities are interpreted as word generation probabilities, and documents are ranked by their probability of generating the query. This generation probability is computed by taking the product over all query terms of the probability of the query term given the language model <ref type="bibr" coords="1,345.84,697.90,41.54,9.02" target="#b4">[Zhai 2001</ref>]:</p><formula xml:id="formula_0" coords="2,79.26,74.93,86.02,27.25">( ) ( ) ∏ = = Q 1 D D θ P θ Q P i i q</formula><p>(1) where q i is the i th query term of query Q, |Q| is the length of Q, and θ D is a language model estimated from document D. In the case where we desire a ranking using only one of the document representations, we directly take</p><formula xml:id="formula_1" coords="2,79.26,125.59,77.80,23.64">( ) ( ) ( ) i w w D D θ P θ P =</formula><p>(2) where i indicates a specific document representation.</p><p>The language models for an individual document representation can be estimated by smoothing a maximum likelihood estimator (MLE) with a collection-wide document representation model:</p><formula xml:id="formula_2" coords="2,79.26,191.05,178.06,23.64">( ) ( ) ( ) ( ) ( ) i i i i i w w w C P 1 D P θ P MLE MLE D λ λ - + =</formula><p>(3) where C(i) is the aggregate over all document representations D(i). The MLE for a document representation is:</p><formula xml:id="formula_3" coords="2,79.26,230.83,455.38,34.00">( ) ( ) i i i w count w D D ; D P MLE = (4)</formula><p>The MLE distribution for the C(i) is estimated similarly. It is common to set the linear interpolation parameters λ 1 and λ 2 in Equation 3 using guidance from Dirichlet prior smoothing:</p><formula xml:id="formula_4" coords="2,79.08,298.67,455.56,28.14">i i µ λ + = i i D D (5)</formula><p>where ¤ i is a parameter often set emprically or by using cross-validation <ref type="bibr" coords="2,362.64,329.32,41.35,9.02" target="#b6">[Zhai 2002</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Combining Information Using Language Models</head><p>When we wish to combine information formed from a variety of document representations, we take a linear interpolation of the unigram language models estimated from the individual representations <ref type="bibr" coords="2,452.28,372.10,54.13,9.02" target="#b3">[Ogilvie 2003</ref>]. This new language model for a document should be designed so that it closely models what we would expect a user to write as a query when requesting the document. This is different from doing a linear combination of scores from different systems as we directly estimate the probability of a word given the differing language models. This by replacing Equation 2 with:</p><formula xml:id="formula_5" coords="2,79.26,431.90,455.38,30.21">( ) ( ) ( ) ∑ = = k i i i w w 1 D D θ P θ P ϕ (6)</formula><p>where k is the number of document representations for a document and φ i is the weight placed on the i th document representation. The φ values must be positive and sum to one.</p><p>When we wish to incorporate a prior probability in the ranking, we restate the problem as one of estimating the probability of the document given the query and applying Bayes' rule:</p><formula xml:id="formula_6" coords="2,79.26,515.60,455.38,35.50">( ) ( ) ( ) ( ) Q P D P D Q P Q D P =<label>(7)</label></formula><p>The P(Q) constant can be ignored in ranking, and the P(θ D ) component is the prior. We estimate P(Q|D) using the generative probability and estimate the priors using the URL class of the web page, giving:</p><formula xml:id="formula_7" coords="2,79.26,575.18,149.44,20.47">( ) ( ) ( ) ( ) D HP NP P θ Q P Q D P D type ∨ ∝</formula><p>(8) where P(NP or HP|type(D)) is the prior probability of the page being a named page or homepage given the URL type of the document and P(Q|θ D ) is the generative probability defined in Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System Specifics</head><p>We use the Lemur toolkit [Lemur] for document indexing and retrieval. For document tokenization we used Inquery's stopword list and the Porter stemmer. The URLs were tokenized on punctuation (., /) and were not stemmed. A shorter stopword list was used for URLs ("http", "www", "com", "gov", "html", etc.). Each document had as many as seven document representations, as shown in Table <ref type="table" coords="2,345.48,676.90,3.77,9.02" target="#tab_0">1</ref>. For every representation except the URL, we formed language models using Dirichlet prior smoothing. The Dirichlet prior parameter was chosen to be close to twice the average length of the representation. This is not an optimal parameter setting, but may not have a large effect on results. See <ref type="bibr" coords="3,173.64,74.32,44.85,9.02" target="#b4">[Zhai 2001</ref>] <ref type="bibr" coords="3,223.00,74.32,46.32,9.02" target="#b6">[Zhai 2002</ref>] for more information. The probability of a word given the document's URL was computed treating the URL and word as a character sequence, then computing a characterbased trigram generative probability. The numerator and denominator probabilities in the trigram expansion were estimated using a linear interpolation with the collection model (all URLs in the corpus). The final document scores were computed as the generative probability of the query given the document, taking the linear interpolation over the document representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Description Alt</head><p>Image </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Parameter Estimation</head><p>The weights for the document representations (the φ parameters used in Equation <ref type="formula" coords="3,399.78,279.10,4.19,9.02">2</ref>) were estimated by averaging the MRR (Mean Reciprocal Rank) scores of the individual document representations on the TREC 10 Homepage Finding task and the TREC 11 Named-Page Finding task. Table <ref type="table" coords="3,335.16,302.08,5.01,9.02">2</ref> shows the MRR of the individual representations on the previous known-item tasks and the resulting scaled weights. Table 2 also shows the φ we estimated from the TREC 10 and TREC 11 data. We estimated the φ values by scaling the TREC 10 and TREC 11 columns of Table <ref type="table" coords="3,534.96,325.12,5.01,9.02">2</ref> to sum to one then averaged the two scaled columns.  <ref type="table" coords="3,101.46,464.08,4.19,9.02">2</ref>: Estimating representation weight from TREC 10 and TREC 11 data Table <ref type="table" coords="3,97.86,481.60,5.01,9.02" target="#tab_2">3</ref> shows the actual performance of the document representations on the TREC 12 test data. We can see that the performance of the document representations for named page finding in TREC 11 is similar to their performance in TREC 12. This is not surprising; the TREC 11 and TREC 12 named-page topics were selected in a similar manner and both use the .GOV corpus. However, the performance of the individual document representations TREC 10 homepage finding task is not as predictive for the TREC 12 homepage topics. In particular, the full document text is much less useful for the TREC 12 topics than for the TREC 10 topics. This could be a result of variance or small sample sizes, but we believe it is more likely a result of different corpus characteristics. The priors were estimated from TREC 10 data and TREC 11 data. We made the assumption that the URLs of homepages in the .GOV corpus would have similar characteristics to those in the WT10G corpus. In addition to using the test topics for the TREC 10 Homepage Finding task, we also used the 80 training topics provided that year. We leveraged the knowledge that there would be an equal number of homepage topics and named-page topics in the test set by scaling our estimates on the posterior P(type|NP or HP) to the same number of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation</head><p>There are simpler methods to devise equivalent numbers for ranking purposes, but in the interests of describing what we did with the numbers we used, we present our actual numbers and computations. These numbers are presented in Table <ref type="table" coords="4,108.48,137.80,3.77,9.02" target="#tab_2">3</ref>, and we provide a justification for the method used in Table <ref type="table" coords="4,363.24,137.80,5.01,9.02" target="#tab_2">3</ref> below. In these derivations NP denotes a named page, HP denotes a homepage, and t is a page type (ROOT, SUBROOT, FILE, or PATH). The method of derivation for priors is similar to the approach used in <ref type="bibr" coords="4,289.74,160.84,48.07,9.02" target="#b2">[Kraaij 2002</ref> (10) where c(t,NP) denotes the count of named pages of type t in the training data and c(NP) denotes the number of named page queries in the training data. In this step we approximated the values with training data. Leveraging the fact that we know that homepages and named-pages are equally likely in the test data, we assumed P(NP) = P(HP) = 0.5. What we're substituting in for P(HP) and P(NP) is actually P(correct document is a HP) and P(correct document is a NP). This is fine here, but will have some implications to the correctness of their use for the priors. With a little rewriting, we get our formula for the estimation of the posterior distribution: (12) by Bayes rule. Note that now we are interpreting P(NP or HP|t) as P(this document is a NP or HP|t) and not P(correct document is a NP or HP|t) as we do not know that the document is correct. We believe the document may be correct, but we do not know. This means that substituting in the P(t|NP or HP) we estimated above is not the value we should be using here, but we will assume that it is close to the true value we desire. Doing so gives: (14) where c(t) is the number of documents of type t in the collection and |collection| is the number of documents in the collection. We can also ignore the constant size of the collection: We note that this method of parameter estimation for the posterior distribution expectation was very accurate given the training data. Figures <ref type="figure" coords="5,182.94,210.81,5.01,9.02">1</ref> and<ref type="figure" coords="5,209.70,210.81,5.01,9.02">2</ref> show pie charts corresponding to the distribution we estimated and the actual distribution observed in the relevance judgments. We believe it is reasonable to achieve such good estimation of this distribution in practice, as it is a relatively low cost activity to provide assessments for known-item tasks.</p><formula xml:id="formula_8" coords="4,72.00,327.32,462.64,93.94">( ) ( ) ( ) ( ) ( ) NP 2 HP NP HP , NP , c c c t c t c + =<label>(11) Prior: ( ) ( ) ( ) (</label></formula><formula xml:id="formula_9" coords="4,93.84,470.20,102.10,45.31">( ) ( ) ( ) ( ) ( )<label>(</label></formula><formula xml:id="formula_10" coords="4,94.20,627.86,90.82,45.33">( ) ( ) ( ) ( ) ( ) t c c c t c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated Distribution</head><p>Actual Distribution</p><formula xml:id="formula_11" coords="5,76.44,267.29,241.41,104.49">Root 31% Subroot 15% Path 7% File 47% Root 30% Subroot 17% Path 6% File 47%</formula><p>Figures 1 and 2: Estimated posterior distribution of page types for correct documents (left) and actual distribution of correct documents (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section we describe official and unofficial runs. We submitted four official runs LmrEq, LmrEst, LmrEqUrl, and LmrEstUrl. "Lmr" denotes the Lemur system, "Eq" indicates the φ values were set to be equal to each other, "Est" indicates that the φ were those presented in Table <ref type="table" coords="5,296.94,446.85,3.74,9.02">2</ref>, and "Url" signifies that the URL priors presented in Table <ref type="table" coords="5,72.00,458.37,5.01,9.02" target="#tab_5">4</ref> were applied to the scores. Table <ref type="table" coords="5,222.24,458.37,5.01,9.02">5</ref> summarizes these runs and their performance. LmrFlat and LmrFlatUrl are unofficial runs that do not use document structure or the text of the URL.  <ref type="table" coords="5,98.94,582.64,4.19,9.02">5</ref>: Summary of runs and their performance Our best performance was achieved when we used the estimated φ in combination with the URL priors. However, when we did not use the URL priors, the estimated φ parameters had worse performance than equally weighted φ values. This raises questions about both the use of URL priors and the method of training the φ values. We recognize that our approach to training the φ values is not optimal or always effective. What we found more interesting was that the URL priors did not help the different runs uniformly, despite the fact that they had similar initial performance. Applying the URL priors to the LmrFlat run severely degraded performance, so this leads us to have questions about the use of URL priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion of URL Priors</head><p>When we apply the priors to the scores returned in a ranking, we do so with the hopes that the reranked lists will match our expectations of the posterior distributions. The fact that the URL priors do help in some cases suggests that we may indeed be observing this behavior when the priors are helping.</p><p>To test this hypothesis, we plotted the cumulative distribution of results across all queries. We estimated the probability of a class given a run by counting all documents of a given class up to a rank threshold across all queries and dividing by the number of documents returned by that rank. Figures <ref type="figure" coords="6,378.24,151.59,4.45,9.02">4</ref><ref type="figure" coords="6,382.69,151.59,4.45,9.02">5</ref><ref type="figure" coords="6,387.14,151.59,4.45,9.02" target="#fig_2">6</ref>(last page of the paper) show these estimated distributions. The lines without the points are the desired posterior distribution we hoped to achieve by applying the prior probabilities. For LmrEqUrl and LmrEstUrl, we see that the FILE, PATH, and SUBROOT classes are favored more than desired, but the ROOT class is returned less often than we would like. However, we know that performance is increased by applying the priors to the LmrEq and LmrEst runs, and we can see in Figures <ref type="figure" coords="6,72.00,209.07,5.01,9.02">4</ref> and<ref type="figure" coords="6,96.48,209.07,5.01,9.02">5</ref> that the URL prior brought the results closer to the desired distribution of documents in the ranking.</p><p>On the other hand, LmrFlatUrl does not match our expected posterior distribution well (Figure <ref type="figure" coords="6,468.36,226.59,3.64,9.02" target="#fig_2">6</ref>). Applying the URL prior to LmrFlat produces very undesirable behavior in the rankings. It is not surprising that the mean reciprocal rank of LmrFlatUrl is much worse than that of LmrFlat. The cause of this behavior is not apparent from the current analysis, but the analysis does suggest a way to assess whether the priors producing a desirable effect on the posterior distributions. This can be done without relevance assessments, so it may be useful during the training and tuning phase of a retrieval experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>This paper described our TREC mixed homepage/named-page runs. We feel that our performance on this task was very strong. Our basic approach was to use language models estimated using structural information present in the documents to estimate the query generation probability. We found a URL-based prior that others found successful for the TREC 10 homepage finding task was also effective here. We described how we estimated the priors, and provided data analysis as to the effectiveness of the priors. However, we also demonstrate that the prior probabilities are not producing the desired expected posterior distributions. We provide some analysis suggesting that when the priors harm performance, they produce undesirable effects to the posterior distribution. This analysis is simple to do and may be useful for others when tuning their systems.</p><p>For future work, we would like to gain a better understanding of the reason why the priors are not producing desired posterior distributions. One hypothesis is that the distribution of documents in the ranking does not match the distribution of documents in the collection. This may cause any biases present in the original ranking to be present in the reranked results lists. Another cause may be that the scores behave differently for the different classes. In this case, a simple flat prior may not fix the problem. We feel that if we can come up with a solution that produces the desired posterior distribution while preserving as much information in the scores as possible, we may be able to improve on our already strong results.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,232.56,490.06,3.28,17.65;4,166.32,518.34,3.33,8.99;4,159.54,518.34,3.33,8.99;4,153.78,518.34,5.55,8.99;4,219.30,496.98,12.75,8.99;4,196.02,496.98,12.75,8.99;4,186.90,496.98,5.55,8.99;4,135.78,504.72,12.75,8.99;4,122.82,504.72,4.99,8.99;4,153.00,491.10,12.75,8.99;4,168.12,477.12,12.75,8.99;4,144.36,477.12,12.75,8.99;4,140.88,477.12,2.50,8.99;4,103.74,483.36,12.75,8.99;4,99.96,483.36,2.50,8.99;4,162.84,518.34,2.78,8.99;4,127.80,504.72,4.43,8.99;4,145.32,491.10,4.43,8.99;4,160.20,477.12,4.43,8.99;4,137.58,477.12,2.78,8.99;4,130.20,477.12,4.43,8.99;4,96.60,483.36,2.78,8.99;4,89.22,483.36,4.43,8.99;4,210.84,493.71,6.02,13.01;4,121.56,480.09,5.48,13.01;4,79.26,507.27,5.48,13.01;4,517.92,520.29,16.72,9.02;4,72.00,531.81,468.06,9.02;4,72.00,543.27,330.45,9.02;4,94.20,561.09,3.28,17.66;4,117.30,561.09,3.28,17.66;4,135.18,554.85,3.28,17.66;4,157.98,554.85,27.04,17.66;4,150.30,568.89,19.54,17.66;4,112.02,582.51,9.76,17.66;4,126.72,589.37,39.44,9.00;4,114.78,589.37,2.78,9.00;4,107.40,589.37,4.44,9.00;4,145.68,575.75,4.44,9.00;4,160.50,561.77,4.44,9.00;4,137.88,561.77,2.78,9.00;4,130.50,561.77,4.44,9.00;4,96.96,568.01,2.78,9.00;4,89.58,568.01,4.44,9.00;4,122.16,589.37,2.78,9.00;4,153.30,575.75,12.82,9.00;4,168.48,561.77,12.82,9.00;4,144.66,561.77,12.82,9.00;4,141.24,561.77,2.50,9.00;4,104.04,568.01,12.82,9.00;4,100.26,568.01,2.50,9.00;4,121.86,564.74,5.49,13.02;4,78.96,578.36,7.13,13.02"><head></head><label></label><figDesc>We estimate P(t) from the training data and discard the constants P(NP or HP) and 2c(NP) as our prior is multiplicative and discarding a multiplicative constant will not affect the rankings:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,72.00,248.01,439.33,9.02;8,72.00,259.53,460.95,9.02;8,72.00,270.99,31.43,9.02"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Posterior distribution for LmrEqUrl. Applying the URL prior to LmrEq gives result lists are more towards biased towards the FILE and SUBROOT classes than desired, and less biased toward the ROOT class then desired.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,72.00,653.43,438.84,9.02;8,72.00,664.89,453.79,9.02;8,72.00,676.41,436.15,9.02;8,72.00,687.93,435.95,9.02"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Posterior distribution for LmrFlatUrl.Applying the URL priors to LmrFlat results in a heavy bias towards ROOT pages at early ranks which decays rapidly. There is a trend towards an increasing bias toward the SUBROOT class. The FILE class has a much stronger bias against it then desired, and the PATH class has a stronger bias towards it then desired. The bias against the FILE class may account for the poor performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,162.82,219.34,93.98"><head>Table 1 :</head><label>1</label><figDesc>Document representations</figDesc><table coords="3,77.76,162.82,213.58,81.08"><row><cell></cell><cell>alternate text</cell></row><row><cell>Font</cell><cell>Changed font sizes and headings</cell></row><row><cell>Full</cell><cell>Full document text</cell></row><row><cell>Link</cell><cell>In-link text</cell></row><row><cell>Meta</cell><cell>Meta tags (keyword, description)</cell></row><row><cell>Title</cell><cell>Document title</cell></row><row><cell>URL</cell><cell>Character trigram on URL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,72.00,569.68,462.13,117.44"><head>Table 3 :</head><label>3</label><figDesc>Performance of individual representations on TREC 12 and hypothetical φ values estimated from test data</figDesc><table coords="3,77.76,569.68,366.71,104.54"><row><cell></cell><cell>TREC 12 Homepage</cell><cell>TREC 12 Named-Page</cell><cell>TREC 12 Mixed</cell></row><row><cell></cell><cell>(MRR)</cell><cell>(MRR)</cell><cell>(MRR)</cell></row><row><cell>Alt</cell><cell>0.167</cell><cell>0.171</cell><cell>0.169</cell></row><row><cell>Font</cell><cell>0.107</cell><cell>0.233</cell><cell>0.170</cell></row><row><cell>Full</cell><cell>0.125</cell><cell>0.394</cell><cell>0.260</cell></row><row><cell>Link</cell><cell>0.487</cell><cell>0.467</cell><cell>0.477</cell></row><row><cell>Meta</cell><cell>0.160</cell><cell>0.083</cell><cell>0.121</cell></row><row><cell>Title</cell><cell>0.284</cell><cell>0.416</cell><cell>0.350</cell></row><row><cell>URL</cell><cell>0.079</cell><cell>0.122</cell><cell>0.100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,72.00,160.84,462.64,94.88"><head></head><label></label><figDesc>].</figDesc><table coords="4,72.00,178.42,462.64,77.30"><row><cell cols="4">Posterior:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>P</cell><cell>( t</cell><cell>NP</cell><cell>∨</cell><cell cols="2">HP</cell><cell>) ( ) ( ) ( ) ( ) HP P HP P NP P NP P t t + =</cell><cell>(9)</cell></row><row><cell cols="7">since NP and HP are disjoint.</cell></row><row><cell>≈</cell><cell cols="4">( ) ( ) NP 2 NP , c t c</cell><cell>+</cell><cell>( ) ( ) HP 2 HP , c t c</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,79.26,391.91,161.15,29.35"><head></head><label></label><figDesc>)</figDesc><table coords="4,79.26,391.91,161.15,27.60"><row><cell>P</cell><cell>NP</cell><cell>∨</cell><cell>HP</cell><cell>t</cell><cell>=</cell><cell>P</cell><cell>t</cell><cell>NP</cell><cell>∨</cell><cell>P HP</cell><cell>t P</cell><cell>NP</cell><cell>∨</cell><cell>HP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,72.00,634.79,462.64,50.04"><head>Table 4 :</head><label>4</label><figDesc>This gives us the formula we used to estimate the priors. Estimation of prior probabilities and posterior distribution expectations</figDesc><table coords="4,78.96,634.79,102.34,29.62"><row><cell>∝</cell><cell>c</cell><cell>t</cell><cell>,</cell><cell>NP</cell><cell>+</cell><cell>,</cell><cell>HP HP</cell><cell>NP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,72.00,490.24,456.21,101.42"><head>Official Structure φ URL Prior Not found Found by 10 MRR LmrEq</head><label></label><figDesc></figDesc><table coords="5,72.00,503.37,456.21,88.28"><row><cell></cell><cell>YES</cell><cell>YES</cell><cell>equal</cell><cell>NO</cell><cell>8.3 %</cell><cell>83.3 %</cell><cell>0.652</cell></row><row><cell>LmrEst</cell><cell>YES</cell><cell>YES</cell><cell>estimated</cell><cell>NO</cell><cell>7.7 %</cell><cell>83.3 %</cell><cell>0.640</cell></row><row><cell>LmrFlat</cell><cell>NO</cell><cell>NO</cell><cell>-</cell><cell>NO</cell><cell>11.0 %</cell><cell>78.7 %</cell><cell>0.612</cell></row><row><cell>LmrEqUrl</cell><cell>YES</cell><cell>YES</cell><cell>equal</cell><cell>YES</cell><cell>5.3 %</cell><cell>88.0 %</cell><cell>0.713</cell></row><row><cell cols="2">LmrEstUrl YES</cell><cell>YES</cell><cell>estimated</cell><cell>YES</cell><cell>4.7 %</cell><cell>89.3 %</cell><cell>0.727</cell></row><row><cell cols="2">LmrFlatUrl NO</cell><cell>NO</cell><cell>-</cell><cell>YES</cell><cell>9.3 %</cell><cell>50.7 %</cell><cell>0.315</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">Acknowledgements</head><p>This work was sponsored in full by the <rs type="funder">Advanced Research and Development Activity in Information Technology (ARDA)</rs> under its <rs type="grantName">Statistical Language Modeling for Information Retrieval Research Program</rs>. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsor.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_K94dhgT">
					<orgName type="grant-name">Statistical Language Modeling for Information Retrieval Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,82.00,583.80,66.97,10.80;6,72.00,596.97,33.34,9.02;6,180.00,596.97,206.20,9.02" xml:id="b0">
	<monogr>
		<ptr target="http://www.cs.cmu.edu/~lemur" />
		<title level="m" coord="6,180.00,596.97,75.19,9.02">The Lemur Toolkit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,180.00,619.95,339.16,9.02;6,180.00,631.47,340.80,9.02;6,180.00,642.99,345.77,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,362.76,619.95,156.40,9.02;6,180.00,631.47,45.58,9.02">Effective site finding using link anchor information</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,245.58,631.47,275.22,9.02;6,180.00,642.99,316.37,9.02">Proceedings of the Twenty-Fourth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;01)</title>
		<meeting>the Twenty-Fourth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,180.00,665.97,341.86,9.02;6,180.00,677.49,355.32,9.02;6,180.00,688.95,345.77,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,359.40,665.97,162.46,9.02;6,180.00,677.49,68.52,9.02">The importance of prior probabilities for entry page search</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,268.38,677.49,266.94,9.02;6,180.00,688.95,316.37,9.02">Proceedings of the Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</title>
		<meeting>the Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,180.00,74.31,348.05,9.02;7,180.00,85.83,339.33,9.02;7,180.00,97.29,284.69,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,282.54,74.31,241.49,9.02">Combining document representations for known-item search</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,190.86,85.83,328.47,9.02;7,180.00,97.29,255.29,9.02">Proceedings of the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;03)</title>
		<meeting>the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,180.00,120.33,348.27,9.02;7,180.00,131.79,354.45,9.02;7,180.00,143.31,321.93,9.02;7,180.00,154.83,72.53,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,278.58,120.33,249.69,9.02;7,180.00,131.79,110.88,9.02">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,310.26,131.79,224.19,9.02;7,180.00,143.31,321.93,9.02;7,180.00,154.83,43.13,9.02">Proceedings of the Twenty-Fourth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;01)</title>
		<meeting>the Twenty-Fourth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,180.00,177.81,314.98,9.02;7,180.00,189.33,335.49,9.02;7,180.00,200.79,125.57,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,474.36,177.81,20.62,9.02;7,180.00,189.33,143.83,9.02">THU TREC2002 Web Track Experiments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,344.16,189.33,171.33,9.02;7,180.00,200.79,44.22,9.02">Proceedings of the Eleventh Text Retrieval Conference</title>
		<meeting>the Eleventh Text Retrieval Conference<address><addrLine>TREC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,180.00,223.77,326.19,9.02;7,180.00,235.29,327.99,9.02;7,180.00,246.81,283.49,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,278.64,223.77,210.67,9.02">Two-stage language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,180.00,235.29,327.99,9.02;7,180.00,246.81,254.20,9.02">Proceedings of the Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</title>
		<meeting>the Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
