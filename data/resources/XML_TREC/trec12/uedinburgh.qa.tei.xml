<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.92,78.14,386.12,15.83">QED: The Edinburgh TREC-2003 Question Answering System</title>
				<funder ref="#_tS4yR77">
					<orgName type="full">German Academic Exchange Service (DAAD)</orgName>
				</funder>
				<funder ref="#_XEkSSNx">
					<orgName type="full">Sydney University</orgName>
				</funder>
				<funder ref="#_7E8ufsy">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_vd9sVVJ">
					<orgName type="full">ESRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,132.19,126.92,93.12,13.19"><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
						</author>
						<author>
							<persName coords="1,234.29,126.92,52.62,13.19"><forename type="first">Johan</forename><surname>Bos</surname></persName>
						</author>
						<author>
							<persName coords="1,295.90,126.92,87.11,13.19"><forename type="first">Tiphaine</forename><surname>Dalmas</surname></persName>
						</author>
						<author>
							<persName coords="1,392.00,126.92,87.77,13.19"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
						</author>
						<author>
							<persName coords="1,130.72,140.86,74.05,13.19"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName coords="1,213.74,140.86,87.78,13.19"><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Bannard</surname></persName>
						</author>
						<author>
							<persName coords="1,310.50,140.86,79.12,13.19"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
						</author>
						<author>
							<persName coords="1,398.60,140.86,82.64,13.19"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Buccleuch Place</orgName>
								<address>
									<postCode>EH8 9LW</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.92,78.14,386.12,15.83">QED: The Edinburgh TREC-2003 Question Answering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6C86FF448CE601A2D2BAE535947FC9F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes a new open-domain answer retrieval system developed at the University of Edinburgh and gives results for the TREC-12 question answering track. Phrasal answers are identified by increasingly narrowing down the search space from a large text collection to a single phrase. The system uses document retrieval, query-based passage segmentation and ranking, semantic analysis from a wide-coverage parser, and a unification-like matching procedure to extract potential answers. A simple Web-based answer validation stage is also applied. The system is based on the Open Agent Architecture and has a parallel design so that multiple questions can be answered simultaneously on a Beowulf cluster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report describes QED, a new question answering (QA) system developed at the University of Edinburgh for TREC-12. A key feature of QED is the use of natural language processing (NLP) technology at all stages in the QA process; recent papers have shown the benefit of using NLP for QA <ref type="bibr" coords="1,134.78,567.69,91.76,10.99" target="#b6">(Moldovan et al., 2002)</ref>. In particular, we parse both the question and blocks of text potentially containing an answer, producing dependency graphs which are transformed into a fine grained semantic interpretation. A matching phase then determines if a potential answer is present, using the relations in WordNet to constrain the answer.</p><p>In order to process very large text collections, the system first uses shallow methods to identify text segments which may contain an answer, and these segments are passed to the parser. The segments are identified using a "tiler", which uses simple heuristics based on the words in the question and the text being processed.</p><p>We also use additional state-of-the-art text processing tools, including maximum entropy taggers for POS tagging and named entity (NE) recognition. POS tags and NE-tags are used during the construction of the semantic representation. Section 2 describes each component of the system in detail.</p><p>The main characteristics of the system architecture are the use of the Open Agent Architecture (OAA) and a parallel design which allows multiple questions to be answered simultaneously on a Beowulf cluster. The architecture is shown in Figure <ref type="figure" coords="1,418.94,377.08,3.72,10.99" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Component Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-processing and Indexing</head><p>The ACQUAINT document collection which forms the basis for TREC-2003 was pre-processed with a set of Perl scripts, one per newspaper collection, to identify and normalize meta-information. This meta-information included the document id and paragraph number, the title, publication date and story location. The markup for these last three fields was inconsistent, or even absent, in the various collections, and so collection-specific extraction scripts were required.</p><p>The collection was tokenized offline using a combination of the Penn Treebank sed script and Tom Morton's statistical tokenizer, available from the OpenNLP project. Ratnaparkhi's MXTERMINATOR program was used to perform sentence boundary detection <ref type="bibr" coords="1,491.68,591.04,48.29,10.99;1,313.20,603.00,77.50,10.99" target="#b7">(Reynar and Ratnaparkhi, 1997)</ref>. The result was indexed with the Managing Gigabytes (MG 1.3g) search engine <ref type="bibr" coords="1,500.07,614.95,39.90,10.99;1,313.20,626.91,35.69,10.99">(Witten et al., 1999)</ref>. For our TREC-2003 experiments, we used casesensitive indexing without stop-word removal and without stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Generation and Retrieval</head><p>Using ranked document retrieval, we obtained the best 100 documents from MG, using a query generated from the question. The question words were first augmented with their base forms obtained from <ref type="bibr" coords="2,72.00,385.00,80.84,10.99" target="#b5">Minnen et al. (2001)</ref>'s morphological analyser, which also performs case normalisation. Stopwords were then removed to form the query keywords. Any remaining uppercase keywords were required to be in the returned documents using MG's plus operator. All remaining lower case keywords were weighted by a factor of 12 (determined by experimentation). Without such a weighting, MG's ranking is too heavily influenced by the uppercase keywords (which are predominantly named entities).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Passage Segmentation and Ranking</head><p>Since our approach involves full parsing to obtain grammatical relations in later stages, we need to reduce the amount of text to be processed to a fraction of the amount returned by the search engine. To this end, we have implemented QTILE, a simple query-based text segmentation and passage ranking tool. This "tiler" extracts from the set of documents a set of segments ("tiles") based on the occurrence of relevant words in a query, which comprises the words of the question. A sliding window is shifted sentence by sentence over the text stream, retaining all window tiles that contain at least one of the words in the query and contain all upper-case query words.</p><p>Each tile gets assigned a score based on the following: the number of non-stopword query word tokens (as opposed to types) found in the tile; a comparison of the capitalization of query occurrence and tile occurrence of a term; and the occurrence of 2-grams and 3-grams in both question and tile. The score for every tile is multiplied with a window function (currently a simple triangle function) which weights sentences in the centre of a window higher than in the periphery.</p><p>Our tiler is implemented in C++, has linear asymptotic time complexity and requires constant space. For TREC- 2003 we use a window size of 3 sentences and pass forward the top-scoring 100 tiles (with duplicates eliminated using a hash signature test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Tagging And Syntactic Analysis</head><p>The C&amp;C maximum entropy POS tagger <ref type="bibr" coords="2,489.78,507.40,50.20,10.99;2,313.20,519.35,51.13,10.99">(Curran and Clark, 2003a</ref>) is used to tag the question words and the text segments returned by the tiler. The C&amp;C NE-tagger <ref type="bibr" coords="2,313.20,543.26,105.83,10.99">(Curran and Clark, 2003b)</ref> is also applied to the question and text segments, identifying named entities from the standard MUC-7 data set (locations, organisations, persons, dates, times and monetary amounts). The POS tags and NE-tags are used to construct a semantic representation from the output of the parser (see Section 2.5).</p><p>We used the RADISP system <ref type="bibr" coords="2,450.99,615.51,88.99,10.99;2,313.20,627.46,23.23,10.99" target="#b1">(Briscoe and Carroll, 2002)</ref> to parse the question and the text segments returned by the tiler. The RADISP parser returns syntactic dependencies represented by grammatical relations such as ncsubj (non-clausal subject), dobj (direct object), ncmod (non-clausal modifier), and so on. The set of dependencies for a sentence are annotated with POS and NE information and converted into a graph in Prolog format. The next section contains an example dependency graph.</p><p>To increase the quality of the parser output, we reformulated imperatives in "list questions" (e.g. Name countries in Europe) into proper question form (What are countries in Europe?). The RADISP parser was much better at returning the correct dependencies for such questions, largely because the RADISP POS tagger typically assigned the incorrect tag to Name in the imperative form. We applied a similar approach to other question types not handled well by the parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Semantic Analysis</head><p>The aim of this component is to build a semantic representation from the output of the parser. It is used for both the question under consideration and the text passages that might contain an answer to the question. The input to the semantic analysis is a set of dependency relations (describing a graph) between syntactic categories, as Figure <ref type="figure" coords="3,114.68,278.42,4.97,10.99" target="#fig_2">2</ref> illustrates. Categories contain the following information: the surface word-form, the lemmatized word-form, the word position in the sentence, the sentence position in the text, named-entity information, and a POS tag defining the category.</p><p>top(1, node('originate', 9) ).</p><p>cat(1, 'croquet', node('croquet', 8), 'NN1', 'O' ). cat(1, 'the', node('the', 5), 'AT', 'O' ). cat(1, 'did', node('do', 4), 'VDD', 'O' ). cat(1, 'originate', node('originate', 9), 'VV0', 'O' ). cat(1, 'game', node('game', 6), 'NN1', 'O' ). cat <ref type="bibr" coords="3,89.92,397.95,96.84,6.75">(1, 'what', node('what', 2)</ref>, 'DDQ', 'O' ). cat(1, 'country', node('country', 3), 'NN1', 'O' ). cat(1, 'of', node('of', 7), 'IO', 'O' ). cat <ref type="bibr" coords="3,89.92,418.87,10.75,6.75">(1,</ref><ref type="bibr" coords="3,104.28,418.87,17.92,6.75">'In',</ref><ref type="bibr" coords="3,125.79,418.87,35.85,6.75">node('In',</ref><ref type="bibr" coords="3,165.25,418.87,7.17,6.75">1)</ref>, 'II', 'O' ).</p><p>edge(1, node('originate', 9), ncsubj, node('game', 6) ). edge(1, node('what', 2), detmod, node('country', 3) ). edge(1, node('of', 7), ncmod1, node('game', 6) ). edge(1, node('of', 7), ncmod2, node('croquet', 8) ). edge(1, node('the', 5), detmod, node('game', 6) ). edge(1, node('originate', 9), aux, node('do', 4) ). edge(1, node('In', 1), ncmod1, node('originate', 9) ). edge(1, node('In', 1), ncmod2, node('country', 3) ).  Our semantic formalism is based on Discourse Representation Theory <ref type="bibr" coords="3,150.31,567.23,97.92,10.99" target="#b4">(Kamp and Reyle, 1993)</ref>, but we use an enriched form of Discourse Representation Structure (DRS), combining semantic information with syntactic and sortal information. DRSs are constructed from the dependency relations in a recursive way, starting with an empty DRS at the top node of the dependency graph, and adding semantic information to the DRS as we follow the dependency relations in the graph, using the POS information to decide on the nature of the semantic contribution of a category.</p><p>Following DRT, DRSs are defined as ordered pairs of a set of discourse referents and a set of DRS-conditions. The following types of basic DRS-conditions are con-sidered: pred(x,S), named(x,S), card(x,S), event(e,S), and argN(e,x), rel(x,y,S), mod(x,S), where e, x, y are discourse referents, S a constant, and N a number between 1 and 3. Questions introduce a special DRS-condition of the form answer(x,T) for a question type T. We call this the answer literal; answer literals play an important role in answer extraction (see Section 2.6).</p><p>Implemented in Prolog, we reached a recall of around 80%. (By recall we mean the percentage of categories that contributed to semantic information in the DRS). Note that each passage or question is translated into one single DRS; hence DRSs can span several sentences. Some basic techniques for pronoun resolution are implemented as well. However, to avoid complicating the answer extraction task too much, we only considered non-recursive DRSs in our TREC-2003 implementation, i.e. DRSs without complex conditions introducing nested DRSs for dealing with negation, disjunction, or universal quantification.</p><p>Finally, a set of DRS normalisation rules are applied in a post-processing step, thereby dealing with activepassive alternations, question typing, inferred semantic information, and the disambiguating of noun-noun compounds. The resulting DRS is enriched with information about the original surface word-forms and POS tags, by co-indexing the words, POS tags, the discourse referents, and DRS-conditions (see Figure <ref type="figure" coords="3,441.41,395.91,3.60,10.99" target="#fig_3">3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Answer Extraction</head><p>The answer extraction component takes as input a DRS for the question, and a set of DRSs for selected passages. The task of this component is to extract answer candidates from the passages. This is realised by performing a match between the question-DRS and a passage-DRS, by using a relaxed unification method and a scoring mechanism indicating how well the DRSs match each other.</p><p>Taking advantage of Prolog unification, we use Prolog variables for all discourse referents in the question-DRSs, and Prolog atoms in passage-DRSs. We then attempt to unify all terms of the question DRSs with terms in a passage-DRS, using an A * search algorithm. Each potential answer is associated with a score, which we call the DRS score. High scores are obtained for perfect matches (i.e., standard unification) between terms of the question and passage, low scores for less perfect matches (i.e., obtained by "relaxed" unification). Less perfect matches are granted for different semantic types, predicates with different argument order, or terms with symbols that are semantically familiar according to WordNet <ref type="bibr" coords="4,255.35,216.58,43.45,10.99;4,72.00,228.53,21.43,10.99">(Fellbaum, 1998)</ref>.</p><p>After a successful match the answer literal is identified with a particular discourse referent in the passage-DRS. Recall that the DRS-conditions and discourse referents are co-indexed with the surface word-forms of the source passage text. This information is used to generate an answer string, simply by collecting the words that belong to DRS-conditions with discourse referents denoting the answer. Finally, all answer candidates are output in an ordered list. Duplicate answers are eliminated, but answer frequency information is added to each answer in this final list. Figure <ref type="figure" coords="4,111.64,371.99,4.97,10.99">4</ref> gives an example output file. The columns designate the question-id, the source, the ranking score, the DRS score, the frequency of the answer, and a list of sequences of surface word-form, lemma, POS tag and word index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Heuristic Candidate Reranking</head><p>The system uses a final answer reranking and filtering component, defined slightly differently for each question type. For factoid questions, we rerank the top 5 answers using a function of the candidate answer frequency and the score assigned by the DRS matcher. For definition questions, the same process is used but with a filter which removes any candidate answers with a DRS score below a certain threshold. For list questions, the top 10 answers are considered and the same scoring function is used.</p><p>We also use two variations on this reranking algorithm. The first simply uses the DRS score directly, without the candidate answer frequency. The second uses frequency counts from Google to filter out improbable questionanswer combinations. A query is sent to Google based on a combination of keywords from the question and the candidate answer. If the document count returned by Google is below some threshold, the answer candidate is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Three runs were submitted: run A (EdinInf2003A) used Google as a filter; run B is the system using a function of  the candidate answer frequency and the DRS score; and run C is the system using the DRS score directly. On factoid questions, we obtained an accuracy of 0.073 (372 wrong, 5 correct but unsupported, 6 inexact, 30 correct) for runs A and B. For run C we obtained a score of 0.058. Figure <ref type="figure" coords="4,342.35,542.71,4.97,10.99" target="#fig_4">5</ref> gives some example extracted answers for factoid questions.</p><p>See Figure <ref type="figure" coords="4,369.77,567.73,4.97,10.99">6</ref> for a breakdown of factoid questions by wh-word for runs A and B. We obtained correct, but unsupported, answers for factoid <ref type="bibr" coords="4,450.34,591.64,89.64,10.99;4,313.20,603.59,71.26,10.99">questions 1971, 2023, 2048, 2115, 2245</ref> in runs A and B and a similar list excluding the latter two questions for run C. Our average F score over 37 list questions was 0.013; for the 50 definition questions we obtained an F score of 0.063. As a result, our final main task score is 0.056.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Future Work</head><p>In TREC 2003, the overall accuracy of the 54 runs submitted to the QA track ranged between 0.034 and 0.700 (median 0.177). For list questions, the best, median, and worst average F-scores were 0.396, 0.069, and 0.000, respectively. For definition questions, the F-scores ranged from 0 to 0.555 (with a median of 0.192).</p><p>In relation to this interval, our low score reflects the fact that our first year of track QA participation required a large resource commitment to develop a solid basic infrastructure. Such long-term investment will provide the basis for subsequent performance analysis, which in turn will lead to replaced components with superior performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,227.33,336.02,157.33,10.99"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The QED system architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,79.17,495.58,157.80,6.75"><head></head><label></label><figDesc>id(['Q_ID':'1394','Q_TYPE':'factoid'],[1]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,72.00,518.59,226.77,10.99;3,72.00,530.54,171.10,10.99"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dependency output for the question In what country did the game of croquet originate?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,313.20,580.23,226.76,10.99;3,313.20,592.19,138.15,10.99"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example DRS for the question In what country did the game of croquet originate?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,313.20,414.83,226.77,10.99;4,313.20,426.79,226.78,10.99;4,313.20,438.74,226.76,10.99;4,313.20,450.70,214.72,10.99"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Some correct (R) and wrong (W) answers from the EdinInf2003A run. The second column of the system response contains the question number; the third column contains the document the answer was retrieved from.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,203.95,145.37,204.07,10.99"><head>Figure 4 :Figure 6 :</head><label>46</label><figDesc>Figure 4: Example output file of answer extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,319.18,80.97,227.74,319.49"><head></head><label></label><figDesc>What country is Aswan High Dam located in? R 1900 XIE19960828.0011 Egypt What business was the source of John D. Rockefeller's fortune? R 1909 NYT19991109.0441 Standard Oil How many Earth days does it take for Mars to orbit the sun?</figDesc><table coords="4,319.18,131.31,224.06,269.15"><row><cell>R 2000 NYT19991220.0063 687 What river is under New York's George Washington bridge?</cell></row><row><cell>R 2330 NYT20000203.0416 the Hudson River What instrument did Louis Armstrong play?</cell></row><row><cell>R 2356 NYT19990830.0439 trumpet What is the name of the Chief Justice of the Supreme Court?</cell></row><row><cell>R 2198 XIE19971101.0185 Sajjad Ali Shah What membrane controls the amount of light entering the eye?</cell></row><row><cell>R 1941 APW19980609.1138 The iris What museum in Philadelphia was used in "Rocky"?</cell></row><row><cell>R 2044 NYT20000411.0123 the Museum of Art When was the first Star Wars movie made?</cell></row><row><cell>R 2069 NYT19990315.0214 1977 What composer wrote "Die Gotterdammerung"?</cell></row><row><cell>R 2301 NYT19980629.0183 Wagner</cell></row><row><cell>How late will airlines let you fly in pregnancy?</cell></row><row><cell>W 1952 NYT19990101.0001 NIL How fast can a nuclear submarine travel?</cell></row><row><cell>W 1937 APW20000814.0076 24 nuclear armed cruise missiles How many floors are in the Empire State Building?</cell></row><row><cell>W 1938 NYT19990121.0328 only the top 22 floors What did George Washington call his house?</cell></row><row><cell>W 1944 APW19990728.0148 the picture of George Washington</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We are grateful <rs type="person">Malvina Nissim</rs> for advice on Google and RASP and to the authors of all external progams we utilized for making them available. Leidner is supported by the <rs type="funder">German Academic Exchange Service (DAAD)</rs> under scholarship <rs type="grantNumber">D/02/01831</rs> and by <rs type="person">Linguit GmbH</rs> (research contract <rs type="grantNumber">UK-2002/2</rs>). Dalmas is supported by the <rs type="institution">School of Informatics, University of Edinburgh</rs>. Curran is supported by a Commonwealth scholarship and a <rs type="funder">Sydney University</rs> <rs type="grantName">Travelling scholarship</rs>. Clark is supported by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">GR/M96889</rs>. Bannard is supported by <rs type="funder">ESRC</rs> Grant <rs type="grantNumber">PTA-030-2002-01740</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tS4yR77">
					<idno type="grant-number">D/02/01831</idno>
				</org>
				<org type="funding" xml:id="_XEkSSNx">
					<idno type="grant-number">UK-2002/2</idno>
					<orgName type="grant-name">Travelling scholarship</orgName>
				</org>
				<org type="funding" xml:id="_7E8ufsy">
					<idno type="grant-number">GR/M96889</idno>
				</org>
				<org type="funding" xml:id="_vd9sVVJ">
					<idno type="grant-number">PTA-030-2002-01740</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,327.54,417.75,132.69,6.75;3,320.37,431.69,21.50,6.75;3,334.72,445.64,208.00,6.75;3,338.31,452.61,197.24,6.75;3,338.31,459.59,68.12,6.75;3,334.72,473.54,211.59,6.75;3,338.31,480.51,204.41,6.75;3,338.31,487.48,53.77,6.75;3,334.72,501.43,211.60,6.75;3,352.65,508.41,100.40,6.75;3,356.24,515.38,96.82,6.75;3,356.24,522.35,78.88,6.75;3,356.24,529.33,89.65,6.75;3,356.24,536.30,89.65,6.75;3,356.24,543.28,82.48,6.75;3,356.24,550.25,93.24,6.75" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,338.30,417.75,39.45,6.75;3,366.98,473.54,179.32,6.75;3,338.31,480.51,204.41,6.75;3,338.31,487.48,21.51,6.75">NN1&apos;), i(1004,&apos;VDD&apos;), i(1005</title>
		<ptr target="1).sem" />
	</analytic>
	<monogr>
		<title level="m" coord="3,356.24,515.38,93.24,6.75">1003:answer(x1003,country)</title>
		<imprint>
			<date type="published" when="1001">1001. 1001</date>
			<biblScope unit="volume">1006</biblScope>
			<biblScope unit="page">1009</biblScope>
		</imprint>
	</monogr>
	<note>NN1&apos;), i(1009. VV0&apos;)], [drs([0:x1008,1002:x1003,1004:e1004,1005:x1006,1009:e1009], [1001:rel(e1009,x1003. pred(x1006,game), 1007:rel(x1006,x1008,of), 1008:pred(x1008,croquet), 1009:arg1(e1009,x1006), 1009:event(e1009,originate</note>
</biblStruct>

<biblStruct coords="3,334.71,557.22,3.58,6.75;5,82.76,70.07,338.90,10.12;5,82.76,80.03,430.35,10.12;5,82.76,89.99,333.53,10.12;5,82.76,99.96,344.27,10.12;5,82.76,109.92,403.46,10.12;5,82.76,119.88,414.22,10.12;5,72.00,594.75,55.53,13.19;5,67.02,613.77,231.77,9.89;5,81.96,623.73,216.82,9.89;5,81.96,633.70,216.82,9.89;5,81.96,643.66,216.82,9.89;5,81.96,653.62,50.28,9.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,292.57,80.03,220.54,10.12;5,82.76,89.99,333.53,10.12;5,82.76,99.96,21.49,10.12;5,81.96,623.73,184.97,9.89">the the DT 158010 nation nation NN 158011 1394 APW19990616.0182 0.0923594 0.37 1 Tarzan Tarzan NNP 21011 1394</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<idno>APW20000827.0133 0.0651768 0.37 2 English English NN 219015 1394 APW20000827.0133 0.0651768 0.37 1 Additionally Additionally NNP 220001 1394 APW20000827.0133 0.0651768 0.37</idno>
	</analytic>
	<monogr>
		<title level="m" coord="5,281.81,119.88,215.16,10.12;5,72.00,594.75,55.53,13.19;5,282.77,623.73,16.01,9.89;5,81.96,633.70,216.82,9.89;5,81.96,643.66,94.68,9.89">Proceedings of the 3rd International Conference on Language Resources and Evaluation</title>
		<meeting>the 3rd International Conference on Language Resources and Evaluation<address><addrLine>Las Palmas, Gran Canaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1499" to="1504" />
		</imprint>
	</monogr>
	<note>4 the the DT 220010 U.S. U.S. NNP 220011 References</note>
</biblStruct>

<biblStruct coords="5,67.02,672.19,231.76,9.89;5,81.96,682.15,216.83,9.89;5,81.96,692.11,216.81,9.89;5,81.96,702.07,216.81,9.89;5,81.96,712.04,205.72,9.89;5,308.22,178.30,231.77,9.89;5,323.16,188.26,216.82,9.89;5,323.16,198.22,216.81,9.89;5,323.16,208.19,216.82,9.89;5,323.16,218.15,70.97,9.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,111.49,682.15,187.30,9.89;5,81.96,692.11,45.74,9.89;5,353.31,188.26,186.67,9.89;5,323.16,198.22,42.50,9.89">Investigating GIS and smoothing for maximum entropy taggers</title>
		<author>
			<persName coords=""><forename type="first">Clark</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,144.44,692.11,154.34,9.89;5,81.96,702.07,216.81,9.89;5,81.96,712.04,78.93,9.89;5,385.10,198.22,154.88,9.89;5,323.16,208.19,150.36,9.89">Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)</title>
		<meeting>the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)<address><addrLine>Budapest, Hungary; Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran and Clark</publisher>
			<date type="published" when="2003">2003. 2003. 2003. 2003</date>
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
	<note>Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03)</note>
</biblStruct>

<biblStruct coords="5,370.99,236.08,169.01,9.89;5,323.16,246.05,175.71,9.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,506.54,236.08,33.45,9.89;5,323.16,246.05,112.39,9.89">WordNet. An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,308.22,263.98,231.77,9.89;5,323.16,273.94,216.82,9.89;5,323.16,283.90,216.81,9.89;5,323.16,293.86,69.59,9.89" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,323.16,273.94,216.82,9.89;5,323.16,283.90,212.02,9.89">From Discourse to Logic. An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<author>
			<persName coords=""><forename type="first">Reyle</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,308.22,311.80,231.77,9.89;5,323.16,321.76,216.82,9.89;5,323.16,331.72,196.45,9.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,376.07,321.76,160.21,9.89">Applied morphological processing of English</title>
		<author>
			<persName coords=""><surname>Minnen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,323.16,331.72,149.74,9.89">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,308.22,349.66,231.76,9.89;5,323.16,359.62,216.83,9.89;5,323.16,369.58,216.83,9.89;5,323.16,379.54,216.81,9.89;5,323.16,389.51,216.83,9.89;5,323.16,399.47,57.67,9.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,497.00,359.62,42.99,9.89;5,323.16,369.58,180.11,9.89;5,323.16,379.54,127.55,9.89">Adrian Novischi, Adriana Badulescu, and Orest Bolohan</title>
		<author>
			<persName coords=""><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,471.44,379.54,68.53,9.89;5,323.16,389.51,176.53,9.89">Proceedings of the Eleventh Text Retrieval Conference (TREC-2002)</title>
		<meeting>the Eleventh Text Retrieval Conference (TREC-2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
	<note>LCC tools for Question Answering</note>
</biblStruct>

<biblStruct coords="5,308.22,417.40,231.77,9.89;5,323.16,427.37,216.81,9.89;5,323.16,437.33,216.82,9.89;5,323.16,447.29,216.82,9.89;5,323.16,457.25,32.86,9.89;5,308.22,475.19,231.76,9.89;5,323.16,485.15,216.82,9.89;5,323.16,495.11,216.82,9.89;5,323.16,505.07,83.68,9.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,395.89,427.37,144.08,9.89;5,323.16,437.33,92.41,9.89;5,505.86,447.29,34.12,9.89;5,323.16,457.25,32.86,9.89;5,308.22,475.19,231.76,9.89;5,323.16,485.15,216.82,9.89;5,323.16,495.11,118.40,9.89">Washington, D.C. [Witten et al.1999] Ian A. Witten, Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">Ratnaparkhi</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,433.03,437.33,106.96,9.89;5,323.16,447.29,176.41,9.89">Proceedings of the Fifth Conference on Applied Natural Language Processing</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing<address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
	<note>A maximum entropy approach to identifying sentence boundaries. 2nd edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
