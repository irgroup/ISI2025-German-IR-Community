<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.80,111.29,299.68,12.31;3,55.64,486.84,227.58,8.75;3,55.64,498.00,227.52,8.75;3,55.64,509.28,227.58,8.75;3,55.64,520.44,227.58,8.75;3,55.64,531.60,227.58,8.75;3,55.64,542.76,227.58,8.75;3,55.64,553.92,227.58,8.75;3,55.64,565.08,227.43,8.75;3,55.64,576.24,227.58,8.75;3,55.64,587.52,48.39,8.75;10,55.64,560.47,479.93,11.34">Bangor at TREC 2003: Q&amp;A and Genomics Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.00,125.95,67.38,9.61"><forename type="first">Terence</forename><surname>Clifton</surname></persName>
						</author>
						<author>
							<persName coords="1,266.00,125.95,70.86,9.61"><forename type="first">Alex</forename><surname>Colquhoun</surname></persName>
						</author>
						<author>
							<persName coords="1,370.88,125.95,69.18,9.61"><forename type="first">William</forename><surname>Teahan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Wales</orgName>
								<address>
									<settlement>Bangor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="laboratory">Towards a Framework for Knowledgeable Agents and Knowledge Grids&quot;. Artificial Intelligence and Intelligent Agents Tech Report AIIA03.2</orgName>
								<orgName type="institution">University of Wales</orgName>
								<address>
									<settlement>Bangor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Wales</orgName>
								<address>
									<settlement>Bangor</settlement>
									<region>in</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="laboratory">Towards a Framework for Knowledgeable Agents and Knowledge Grids&quot;. Artificial Intelligence and Intelligent Agents Tech Report AIIA03.2</orgName>
								<orgName type="institution">University of Wales Bangor</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.80,111.29,299.68,12.31;3,55.64,486.84,227.58,8.75;3,55.64,498.00,227.52,8.75;3,55.64,509.28,227.58,8.75;3,55.64,520.44,227.58,8.75;3,55.64,531.60,227.58,8.75;3,55.64,542.76,227.58,8.75;3,55.64,553.92,227.58,8.75;3,55.64,565.08,227.43,8.75;3,55.64,576.24,227.58,8.75;3,55.64,587.52,48.39,8.75;10,55.64,560.47,479.93,11.34">Bangor at TREC 2003: Q&amp;A and Genomics Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DF15B61F608FE583FF00F66A7A8C79B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Wooldridge</term>
					<term>M. 2002. An Introduction to MultiAgent systems. Wiley</term>
					<term>New York</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>at TREC'2003 in the Q&amp;A and Genomics Tracks. The paper is organized into three parts as follows. The first part provides a brief overview of the logicbased framework for Knowledgeable Agents that is currently being developed at Bangor. This was adopted as the basis for implementations used for both Tracks. The second part describes the Q&amp;A system that was developed based on the framework, and the final part describes some experiments that were conducted within the Genomics Track at specifying context using GeneRIFs (for a Q&amp;A system being developed for the BioMedical domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Knowing About" Knowledge: A Framework for Knowledgeable Agents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Introduction</head><p>We are in the process of designing and developing a novel logic-based framework for implementing knowledgeable agents that will become the core component for our multiagent information retrieval systems.</p><p>In Teahan ( <ref type="formula" coords="1,125.69,345.10,16.20,8.75">2003</ref>), we describe a framework for designing and implementing knowledgeable agents and Knowledge Grids. The framework is based on three types of knowledge relations: Knows, KnowsAbout, and KnowledgeableAbout. These are used to define what an agent knows, what it knows about, and whether an agent has been judged to be knowledgeable by other agents. In Teahan (2003)  and in the second part of this paper, we describe how a Knowledge Grid could be implemented (based on the framework) which has "knowledge" based on the three defined knowledge relations. Essentially, the architecture is based on using knowledgeable agents as a middle layer between the user and the information resources. A key aspect of the design is the use of information extraction coupled with compression-based language modelling technology (Teahan &amp; Harper, 2003)   and the use of a conversational agent that the user asks questions of and receives answers from the system.</p><p>In this architecture, there are three types of objects: users, knowledgeable agents and information resources. The users do not interface directly with the information resources. Instead, they must go through a knowledgeable agent who effectively acts as a knowledge broker in determining which of the information resources are likely to contain an answer to the user' s questions. Notice that knowledgeable agents may need to go though other knowledgeable agents in the hunt to find the most relevant answer to the user' s questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 A framework for knowledgeable agents</head><p>This section outlines the logic-based framework that we wish to use as the basis of knowledge within the Knowledge Grid architecture. We wish to stress that the framework as described below is still in its developmental stage, and its final form, we envisage, will be somewhat A.4 References W.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>different based on the experiences we garner from future research.</p><p>We feel that the traditional propositional truth-based approach that epistemic logic-based multi-agent systems take, which are usually formulated as normal modal logics using the semantics of Kripke <ref type="bibr" coords="1,465.32,345.10,76.00,8.75">(Wooldridge, 2002)</ref>, is not sufficiently expressive enough for our purposes. Instead, we would like to adopt some of the capabilities of Question/Answering systems within our inference capabilities. A problem with the propositional truth-based approach is that although we can state what an agent may know per se, it does not help us find out whether an agent knows an answer to a question, and just as importantly, what answers an agent knows to a question. Neither does it help us find out what an agent knows about (where knowing about a topic implies that you know something about the topic, but it does not imply that you know everything about the topic).</p><p>We feel that there are three necessary conditions for an agent to be "knowledgeable". The key condition, which we refer to as the Knowledge Test, is the following: "An agent is judged to be knowledgeable by other (external) knowledgeable agents". This states that judges are used to adjudicate on whether an agent is knowledgeable or not (analogously to the Turing Test in Artificial Intelligence). The judges are agents -either human or computer-based -that must also be "knowledgeable". Like the Turing Test, it is assumed that a question and answering testing process is used before making the judgment. The second condition is a logical consequence of the first condition: "Other agents must have the ability to learn about and/or be informed of what the agent knows about." Simply stated, if other agents don't know about what the agent knows about, then they can't make a judgment in the first place. The third condition states: "The agent must know: what it knows about, and what it doesn't know about." This again relies on the judging process used for the Knowledge Test: it would seem a natural response for a knowledgeable agent to answer "Sorry, I only know about X and not Y" to something it doesn't know about.</p><p>We have devised the following logic-based framework based on these conditions. We define three logical relationships -Knows, KnowsAbout and KnowledgeableAbout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 The Knows relation</head><p>We define Knows, a 5-tuple relation, as follows: Knows <ref type="bibr" coords="2,84.56,188.28,172.84,8.75">(agent, context, question, answer, relevance)</ref>. This is explained as follows: The specified agent believes that an answer to a question for a specified context has the specified relevance (this is a real number in the range 0 to 1.0 with 1.0 indicating absolute belief that the answer is relevant to the question). A representation of the context, question and answer is provided by the specified context, question and answer which can be arbitrary text passages or strings or some other representation (depending on the implementation). Note that it is possible to ask the same question but in different contexts. The following example is provided as further explanation.</p><p>Example 1: Knows (A, "Domain: Geography", "Where is Bangor?", "North Wales", 1.0).</p><p>In this example, the agent believes she knows that the answer to the question "Where is Bangor?" is " North Wales". She assigns a relevance ranking of 1.0 (in other words, she believes that the answer is certainly correct). The context in this case is the domain of geography.</p><p>An agent may believe many answers are relevant to a particular question and context. As a shorthand notation, we write this in the following manner: Knows (agent, context, question): answer 1 , r 1 ; answer 2 , r 2 ; …</p><p>We can also assign an agent's list of answers to a variable. For example, K A = Knows (A, "Domain: Geography", "Where is Bangor?"). Similarly, we can assign to a variable all that an agent knows on a particular context: K B = Knows (B, "Domain: Geography").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 The KnowsAbout relation</head><p>We define KnowsAbout, a 5-tuple relation as follows: KnowsAbout <ref type="bibr" coords="2,107.84,568.08,155.44,8.75">(agent, topic, context, knows, relevance)</ref>. This is explained as follows: The agent believes that the list of questions and answers denoted by knows are related to the topic given the context and have the specified relevance. Intuitively, the agent believes that she knows about the topic given a certain context because she knows the answers to the specified questions. Topics and contexts can be arbitrary text passages or strings as above for the Knows relation. Note that the agent may know about the same topic but in different contexts.</p><p>Example 2: KnowsAbout (B, "Bangor", " Domain: General Knowledge", K A , 0.9).</p><p>In this example, agent B has some general knowledge about the topic "Bangor". What he knows are the same answers that agent A knows to the question "Where is Bangor?" in a geographical context. He assigns a weighting of 0.9 to his belief in the relevance of agent A's answers.</p><p>It seems reasonable to assume that if an agent knows the answer to something, then it knows about that something. This is written as follows: ∀agent, context, question, relevance K i = Knows <ref type="bibr" coords="2,368.60,188.88,177.00,8.75">(agent, context, question, answer, relevance)</ref> ⇒ KnowsAbout (agent, question, context, K i , relevance).</p><p>In this case, the agent is inferred to know about each question because she knows the answers to them. So for example, from K A above, we can infer that agent A knows about the following: KnowsAbout (A, "Where is Bangor?", "Domain: Geography", K A , relevance).</p><p>By default, the same relevance from the Knows relation can be adopted for the KnowsAbout relation, although this can be overridden at a latter time.</p><p>It also seems reasonable to assume that if an agent knows the context of a given question and answer, then it knows about that context. This is written as follows: ∀agent, context, question</p><formula xml:id="formula_0" coords="2,317.96,346.80,227.67,31.91">K i = Knows (agent, context, question, answer, relevance) ⇒ KnowsAbout (agent, context, context, K i , relevance).</formula><p>In this case, the topic that the agent knows about is the context itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 The KnowledgeableAbout relation</head><p>We define KnowledgeableAbout, a 6-tuple relation as follows: <ref type="bibr" coords="2,317.96,448.08,223.98,8.75;2,317.96,459.36,149.68,8.75">KnowledeagbleAbout (knowledgeable-agent, testingagent, agent, topic, context, relevance)</ref>. This is explained as follows: The knowledgeableagent believes that the agent is knowledgeable about the topic given the context with the specified relevance because that agent knows about the same things as the testing-agent knows about. Effectively, an external agent, which is designated as being knowledgeable, uses test questions to determine if a person or agent knows about some topic. The knowledgeable agent delegates the testing agent to perform the test -this may be a "virtual" agent that is provided with sufficient knowledge necessary in relation to the test. The testing agent may in fact be provided with a subset of the knowledge known by the knowledgeable agent. Alternatively, other possibilities are having the knowledgeable agent spawn a testing agent to perform the test, or having the knowledgeable agent designate an independent testing agent to perform the test. Notice that the series of test questions have themselves now become a form of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Questions, contexts and topics</head><p>Note that in our definitions above of Knows, KnowsAbout and KnowledgeableAbout, we do not explicitly state how the questions are represented or how they are to be matched with each other, and similarly for the topics and contexts. If questions, contexts and topics consist of text strings, an inference system may simply impose the strictest requirement that the strings match exactly. Alternatively, a less strict matching system may be employed. For example, if contexts are specified as a set of labels that specify the context's relevant domains then the conditions for contexts to match may simply be that there exists at least one label common to both sets of domains. For example, the context for the question "How do you cook pumpkin pie?" may be the set of domain labels "Domain: Cooking, Recreation". Another agent may know about the answer to the same question, but in the slightly different context, "Domain: Cooking, Hobbies", although the inference system may infer the contexts match because of the common label "Cooking" present in both.</p><p>We have deliberately left open the specific representation of the questions, contexts and topics to the designer of the knowledge system. We feel that different representations are required in different applications depending on the nature of the knowledge that needs to be specified and/or manipulated. For example, in a knowledge-based information retrieval system, the context could be used to specify the information purpose of the agent that produced each document, and then this can be matched against the information need of the user based on the user's question and previous questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Introduction</head><p>In developing the QITEKAT system, we were aiming to take a first step on the TREC road, providing a foundation for future development within the School of Informatics, at the University of Wales Bangor, for knowledge representation, extraction and language processing techniques. Agent technologies and techniques are a popular tool in modern computer science, and have been applied to a number of problems, including previous TREC question and answering tracks <ref type="bibr" coords="3,209.12,710.40,74.07,8.75;3,55.64,721.56,20.92,8.75">(Chu-Carroll et al, 2002)</ref>. As a secondary goal, we were aiming to use the TREC Question Answering track as a benchmark to evaluate a developing framework for Knowledgeable Agents and Knowledge Grids <ref type="bibr" coords="3,452.60,419.76,93.03,8.75;3,317.96,431.04,21.02,8.75">(Cannataro and Talia, 2003)</ref>, based on the concepts of 'Knowing About Knowledge' <ref type="bibr" coords="3,368.48,442.20,57.64,8.75">(Teahan, 2003)</ref>.</p><p>The short development time period (7 weeks) meant that many of the core components of the QITEKAT system were based on standard information extraction and question/answering techniques, although we were able to incorporate a number of interesting features, particularly in Named Entity tagging and relevance ranking.</p><p>This report firstly describes the main components of the UWB QITEKAT Question Answering System (Section 2). Section 3 presents results obtained from various experiments on past and current TREC Q&amp;A data, and is followed by a brief analysis of the performance of the system (Section 4). The report concludes with a discussion of possible future enhancements (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. System Description</head><p>The QITEKAT system was designed not only to offer a practical implementation for the theoretical concepts of 'Knowing About Knowledge', which are explained in greater detail at the start of this paper, but to offer a foundation for the future development of information extraction and question answering techniques to enhance the system for future use, either through the TREC forum, or for practical applications. This need for extensibility, and to be able to swap out various sections of the system as new techniques were developed, leant itself to the use of an object-oriented development platform. We decided that the Java language would offer the greatest flexibility for future development.</p><p>The knowledge framework proposed by Teahan <ref type="bibr" coords="4,55.64,154.68,61.60,8.75">(Teahan, 2003)</ref>, which is used as the basis for the extraction of knowledge relations from suitable source documents essentially relies on a reverse approach to standard Q&amp;A techniques. Rather than using the question text to retrieve a subset of documents from the test collection, which are then analysed to find an answer, the QITEKAT system was designed to parse the entire collection, forming a number of question/answer relations before any actual questions are posed.</p><p>The TREC 2003 Q&amp;A Track uses the AQUAINT document collection as its source corpus, which consists of over 1 million documents, totalling 375 million words. Quite obviously, performing any kind of extensive parsing or analysis of this size of document collection would be computationally intensive, and not best suited to the Java language.</p><p>With these considerations in mind we adopted a 2level modular approach to the system development, using the Java language to facilitate extensibility, and C where speed was of the essence, integrated using Java native methods. The system was developed based around three main stages:</p><p>• document normalisation and storage;</p><p>• knowledgeable agents;</p><p>• question analysis and answer ranking. Figure <ref type="figure" coords="4,101.24,436.08,4.62,8.75">B</ref>.2 shows the component make up, and how each of the individual modules interacts with the rest of the system, and a more detailed explanation of each of the key components follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 XML Document System</head><p>Although the TREC Q&amp;A track was our main target during the development of the QITEKAT system, it was important that we consider its application to other areas and document sources. With this in mind we developed a rudimentary XML notation to normalise any source documents, and store them in a consistent fashion for analysis by the Knowledgeable Agents of the system. This means that the addition of a new corpora or alternative source of information could be handled using a simple Java based API, and plugged into the system as details of the data source become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Speech Tagger</head><p>The speech tagger forms a major portion of the QITEKAT development, as the part of speech and named entity tags are used as the basis for extracting knowledge relations from the AQUAINT documents. The system is loosely based upon the Fastus system <ref type="bibr" coords="5,55.64,121.20,83.10,8.75">(Hobbs et al, 1996)</ref>, employing the architecture of a cascading finite state automata in order to achieve usable levels of performance. Each stage of the system was developed as a switchable module, so it could be invoked as required, depending on the document structure it is being used to parse.</p><p>The system handles each document as a complete entity, separating it into sentences, and then words, before passing it on to first the POS tagger, and subsequently the NE tagger. Again, XML is used extensively to provide run-time modification of the rules and constructs used to tag the portions of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS Tagger and Phrase Chunker</head><p>The Part of Speech (POS) tagger is a 2-phase tagger, adopting ideas proposed by <ref type="bibr" coords="5,168.80,288.72,69.29,8.75">Brill (Brill, 1992)</ref>. It uses a frequency count, extrapolated from a pre-tagged version of the Brown corpus to assign preliminary part of speech tags to each word in a sentence, using the Penn Treebank tagset. These pre-tagged words are then re-examined by a transformation based tagger. The rules for this tagger were developed through automatic examination of the Brown corpus, with some minor manual modification.</p><p>Once tagging of individual parts of speech is complete, the sentences are passed on to the phrase chunking module, which adopts a three-stage approach. A POS is tagged with one of three standard types</p><p>• Inside a chunk;</p><p>• Outside a chunk;</p><p>• Boundary of a chunk.</p><p>These are based solely on the POS tag assigned to the particular word. This phase is succeeded by a transformation based chunk, again based on rules generated from the Brown Corpus.</p><p>Once the final chunking is complete, each phrase chunk is examined to determine its content, and is labelled accordingly (Verb, Proper Noun, Noun, Punctuation, Other).</p><p>XML is used to store the transformation and frequency rules for this portion of the speech tagging system, offering a look-ahead/behind matching system on three entity types:</p><p>• Words;</p><p>• POS Tags;</p><p>• Chunk Tags.</p><p>Figure B.3 shows an example of an XML rule description for transforming a noun tag (NN) to a verb tag (VB). We can see that the rule specifies that in order for the transformation to take place the POS tag TO must be found in the position before the tag being examined. Multiple conditions can be applied for each rule. The validity of the new tag is checked using the original tag frequency information from the Brown corpus, to ensure that the new tag is a suitable option for the current word.</p><p>Transformations are applied in frequency order and can be cascaded to apply multiple transformations to the same entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NE Tagger</head><p>Once phrase chunking and identification is complete the system is aware of the phrases in a document that correspond to Proper Noun phrases, and are therefore candidates for Named Entity Tagging. Each of the phrase chunks is passed to the NE tagger, which applies a cascading series of modules to determine the type of Named Entity that the chunk refers to: Currencies; Dates; Times; Locations; Professions; Relations; Measures; Organisations; Names (Pre/Post Honours).</p><p>Each of these types is defined by a series of rules, again stored as XML for easy modification, which rely on a combination of direct matching, designator matching and sure-fire context rules</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direct matching</head><p>Certain named entity types fall into this category, in particular dates and times, which follow a series of standard word patterns. Regular expression matching is used to identify matches, which are then tagged accordingly. For example the regular expression below can be used to match the initial portion of a date such as 23 rd October.</p><formula xml:id="formula_1" coords="5,318.92,534.41,225.58,6.89">((0?[1-9]|[1|2][\\d]|3[0|1])(st|nd|rd|th)?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Designator Matching</head><p>This method is adopted to determine such NEs as organisations and persons, and relies on common pre and post entity word matches. For example if we have the NE British Gas Plc, we can match the Plc designator, and tag the phrase as an organisation. Other such designators that the QITEKAT system relies upon are:</p><formula xml:id="formula_2" coords="5,376.04,645.60,100.74,20.39">Mr Sr Corp Dr</formula><p>Ltd Jr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sure-fire context rules</head><p>Certain sentence constructs are used to determine the type of Named Entity for a specific phrase, where the surrounding context unambiguously denotes a specific type. As an example, take the partial sentence: The context Shares in ??? implies that ??? is an organisation, and can be used as a suitable sure-fire rule to tag that particular unknown NE.</p><p>A small number of these sure-fire rules were manually created (again stored as XML constructs) in order to tag these particular sentence contexts.</p><p>PROFESSION of ??? PERSON RELATION of ??? PERSON ??? province LOCATION</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial Matching</head><p>Natural language, and particular the construction of news articles, which are the basis for the TREC Question &amp; Answering Track Corpora, often rely heavily human memory and implicit definition. For example, in an article about a particular person, they may be referred to by their full name only once, early in the article, yet will be referred to again on numerous occasions throughout the text. This may be by some abbreviation of their name, say their surname only, or some other means such as anaphora (He said…). It is important for a successful Named Entity tagging system to be able to handle this cross-reference in a particular document in order to correctly tag the unknown NEs present.</p><p>The QITEKAT system employs a simple partial matching algorithm to solve these problems, and cross-tag equivalent entities. This works by extracting all known NEs from a particular document (which have been identified previously, either by designator matching or some other means) and creating partial orders of each. These partial orders are then compared to the remaining unknown NEs in the document, and should a match occur, the new NE is tagged with the equivalent type.</p><p>As an example, take a document that discusses the work of Dr. Bill Teahan. This phrase would be correctly identified as a PERSON, by matching of the Dr. designator. Partial orderings of this phrase would then be constructed (retaining word order to ensure correct crossmatching):</p><p>Dr. Bill, Dr. Teahan, Bill Teahan Should these phrase constructs occur elsewhere in this same document, they would be tagged according to the original phrase (i.e. As a PERSON type).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPM-Based Language Modelling</head><p>The final stage of the QITEKAT speech tagging system focuses on labelling all remaining unknown NE phrases, and adopts a compression-based language modelling system to achieve this goal. Much research has been carried out into the use of PPM compression systems for the text classification <ref type="bibr" coords="6,141.20,682.20,103.86,8.75">(Teahan and Harper, 2003)</ref>, whether it be to identify languages, determine authorship or otherwise.</p><p>We have adopted a PPM based compression system to deal with unknown NE classification, by training PPM models on various known data sets corresponding to the available NE types in the QITEKAT system (PERSONS, ORGANISATIONS, etc). Given a suitably large data set of known phrases of each type, we have been able to train compression models for each. These models are then used in turn to compress unknown phrases from the document set. The model providing the best compression level (i.e. the shortest code length) is thus assumed to be the most appropriate type for the unidentified phrase.</p><p>In initial tests on 200 Reuters news articles, this compression system was able to produce very favourable results, when applied as the final stage in the QITEKAT tagging process.</p><p>Number </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Knowledgeable Agents</head><p>The theory of Knowledgeable Agents proposed in Teahan, 2003, and outlined at the start of this paper is used as the basis for the main document processing component of QITEKAT. Each agent is capable of running autonomously and analysing a given series of XML documents to generate Knows and KnowsAbout relations, which it then stores for the purpose of questionanswering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3.1 Regular Expressions</head><p>In order to extract Knows relations from the AQUAINT corpora, regular expressions were developed manually to pattern match sentence construction for common question types. These expressions were developed using the TREC 2001 question text, and focus on the Who and When question types only, due to time constraints.</p><p>It was important to make the best use of the tagged documents, and to ensure that regular expressions used by the system were not too specific as to require multiple expressions for a single question construct. This led us to develop a dynamic substitution system, whereby a generic RE was populated at run-time using the tagged contents of the sentence it was being applied to.</p><p>Again all rules are stored in an XML file, to enable rapid updating and maintenance of the rule base, and a typical entry looks as follows. The file denotes a basic regular expression format, suitable substitution types, an allowable answer type, and a question format for the particular relation</p><p>• When did OBJECT1 die?</p><p>• Who was OBJECT1? By using the NEs already tagged in this sentence, the system creates a number of regular expressions, substituting suitable NE types into the ANSWER and OBJECT locations. Given the sentence: John Lennon died on December 8 th , 1980 during a public dramatic interpretation of J.D. Salinger's "Catcher in the Rye", the QITEKAT system would tag 1 DATE entity (December 8 th , 1980) and 2 PERSON entities (John Lennon and J.D. Salinger) the QITEKAT system would dynamically produce 2 regular expressions: 1. (John Lennon)\sdied\s(( <ref type="formula" coords="7,207.17,245.57,69.94,7.64">on|in|around</ref>)\s(December 8 th , 1980) 2. (J.D.Salinger)\sdied\s(( <ref type="formula" coords="7,213.05,267.53,69.94,7.64">on|in|around</ref>)\s(December 8 th , 1980)</p><p>These would then be applied to the sentence to extract any matches which would be transformed into Knows relations. In this case, option 1 would match, resulting in the following relation (given that the "knowledgeable" agent who produced the document text referred to as A).</p><p>Knows(A, "Domain: PEOPLE", "When did John Lennon die?", "December 8 th , 1980", 1.0).</p><p>Further examples of extracted Knows relations: K 1 = Knows(A, "Domain: PEOPLE", "Who is George W. Bush?", "United States President", 1.0). K 2 = Knows(A, "Domain: PEOPLE", "When was George W. Bush born?", "July 6 th 1946", 1.0).</p><p>These Knows relations are then used to populate suitable KnowsAbout relations such as the following:</p><p>KnowsAbout(A, "Domain: PEOPLE", "George W. Bush", {K 1 , K 2 }, 1.0). KnowsAbout(A, "Domain: PEOPLE", "John Lennon", K a , 1.0).</p><p>A small number of broad domain types are used (PEOPLE, GEOGRAPHY, HISTORY, SPORT, MISC), and all relations are stored within the Knowledgeable Agents using serialized vectors, in order to achieve persistent data storage between executions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3.2 Distribution</head><p>In developing the QITEKAT system, consideration was given to its use as a prototype for a Knowledge Grid <ref type="bibr" coords="7,55.64,689.40,119.82,8.75">(Cannataro and Talia, 2003)</ref>, and for knowledgeable agents to communicate effectively with one another. This concept pointed toward the need for some kind of distributed system where agents could show mobility, and the ability to reside on a network, wherever there was data to process. In addition, the large amount of data that was being handled for the Q&amp;A task (1 million+ documents) lent itself to exploiting distributed paradigms to share the workload of examining this data and extracting suitable relations.</p><p>The QITEKAT system uses a simple UDP based system to handle communication between agents. This allows each agent to determine what other resources are available on the grid, and also inform others about the knowledge it possesses. As more agents are added to the grid, each becomes aware of what knowledge resources are available, and where a certain domain of questions may be best answered.</p><p>The This approach allows knowledge to propagate through the system, as each question is sent from agent to agent to discover answers. When an answer is found, the response is returned, and the agents in the chain are each able to 'learn' that fact. A user only needs to enquire of a single agent in the grid, and that agent will be able to find the other agents on the grid that may be capable of answering the users query, and forward the question as required. A typical interaction between Knowledgeable Agents on this grid system is outlined below: Receives responses from other agents and updates its KnowsAbout relations.</p><p>Receives question from user.</p><p>Checks its own Knows relations for a suitable answer -none found.</p><p>Checks its KnowsAbout relations for another agent that may have an answer -one found (Agent 2).</p><p>Tags the question and forwards it to Agent 2.</p><p>Agent 2 finds an answer to the question and sends it back to Agent 1 Agent 1 updates it's knows relations so it now knows the answer and won't need to ask Agent 2 next time.</p><p>Agent 1 forwards the answer to the user, and updates its local disk storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4 Confidence Ranking</head><p>In the specific area of question answering it is often the case that systems are able to generate a number of candidate answers for a particular query. In this year's TREC Q&amp;A track for example, an entire section of questions is devoted to returning multiple results for a single query (the so called List questions).</p><p>This poses the problem of determining the best result for a particular query, which is what is required by the standard questions in the Q&amp;A track, and is likely to be the requirements of any practical application of a Question-Answering system.</p><p>The way in which this is often achieved is through a confidence ranking for an answer, reflecting the degree of certainty the system places on the answer returned being correct. The confidence ranking is often returned as a decimal value in the range 0.0 (zero confidence that the answer is correct) to 1.0 (completely confident that the answer is correct).</p><p>Past Q&amp;A systems have used a number of means for determining a confidence measure from answers. Weighting based on matching NE types from the answer to that expected by a specific question type (i.e. A where type question expects a LOCATION type answer, and so a corresponding answer gets a higher weighting) is popular.</p><p>Other popular measures include keyword densities in the answer document, and vector matching of question and answer pairs. We adopted a new approach based on corroboration with external data sources (popular search engines)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4.1 Search Engine Corroboration</head><p>Search engines provide a large document base -Google for example currently claims to index over 3.3 billion Web pages, and as a result are likely to contain many examples of the correct answer to any query likely to be posed to a Q&amp;A system. Although this offers scope to use Web search results as a source corpora for practical Q&amp;A applications, the TREC Tracks require that all answers are found in the AQUAINT document collection. This does not preclude, however, the use of web search results to aid in the Q&amp;A process, and we have adopted a novel approach for confidence ranking of answers, based on the results of an appropriate Web search query.</p><p>The fact that a suitable query to a search engine, based on the original question, is likely to result in many examples of the correct answer means that we can use the proportion of each possible answer within these search results to determine a relevance rank for that answer.</p><p>The QITEKAT system achieves this through a simple search API, developed in Java, which queries a number of popular search engines. Noun and verb phrase chunks from the question text are used to form a suitable search query, and the abstracts of the first 1000 results are retrieved from the search engine. These results are then scanned to determine the frequency of each of the possible results as produced by the Q&amp;A system. The proportion of these frequencies are then used to calculate a relevance ranking. This is better explained using a simple example: So we have a corroborated relevance for each of the answers, and the Q&amp;A system is able to return the answer 8 th December as the most favourable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Results</head><p>Preliminary testing of the QITEKAT system showed positive results on previous TREC question sets, and these are confirmed by the TREC 2003 evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 Trained Question Types</head><p>In developing the regular expression rules to extract Knows relations from source corpora we used the question data supplied as part of the TREC 2001 Question-Answering track. We constructed 400 regular expression rules, although time constraints meant we were unable to construct rules for all question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 TREC 2002</head><p>Initial testing of the QITEKAT system was carried out on TREC 2002 Q&amp;A Track questions in order to provide an indication of how the system would perform under typical application. Manual examination shows that of the 500 questions provided, rules have been constructed that should be able to find answers to 122 of them, assuming those answers exist within the AQUAINT source documents.</p><p>The system registered 107 correct answers, of which 4 were NIL answer questions, as no answer existed in the AQUAINT corpus. 15 incorrect answers were registered, of which 2 should have been NIL answers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Analysis</head><p>The results produced by the QITEKAT system, both from in-house tests on previous Q&amp;A data, and on the current TREC Q&amp;A track questions are promising, particularly given the timescale of the development process. With levels of correct answers exceeding 80% in both tests, this implies a positive first step on the Q&amp;A ladder, and a solid foundation to build on the work in Knowledgeable Agents and the concepts of 'Knowing About Knowledge'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Question Types</head><p>The results gained by the QITEKAT system need to be considered in the context of the question types that were addressed in order to gain a more accurate indication of the performance of the system. It could be argued that the When and Who question types are the simpler of the main types used in the TREC evaluations, offering a definite answer type, and often more simple sentence constructs where an answer may be found. We felt this to be the case in this respect, and deliberately chose these types in order to aid the speed of system development in order to meet the deadline for run submission. We hope, however, that the underlying concepts of the system that we have adopted should be able to achieve similar results on all of the major question types, given suitable Regular Expressions on which to match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Speed</head><p>Analysis of the AQUAINT documents which formed the source corpora for the TREC 2003 Q&amp;A evaluation demonstrated the benefits of the distributed design adopted as the basis for the QITEKAT system, but also indicated a need for further speed improvements.</p><p>The final evaluation was carried out using a distributed network of 8 Pentium III computers, each using a 128Mb of local memory, and approximately 500Mb of local storage. The parsing and analysis of the 1 million documents took approximately 72 hours on this configuration. Although this level of performance is manageable, it would need to be improved if the system were to be applied to practical applications, or larger corpora, such as Web search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Future Directions</head><p>As a foundation for future Q&amp;A and language processing research, the QITEKAT system has performed well, although a number of areas have been targeted as areas for improvement. In particular it is important that we are able to handle a greater number of question types in order to perform a more accurate evaluation of the systems performance, and allow for a direct comparison to other research systems participating in the TREC tracks. Further additional features that we feel may improve system performance, both in terms of speed of execution, and the ability to determine answers are outlined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5.1 Improved NE Classification</head><p>Although the NE classifier developed as part of the QITEKAT system performs well, for the purposes of Q&amp;A it is important to broaden the scope of the system, and introduce further NE types in order to allow for more accurate answer matching. Sekine et al present a system offering a far greater number of NE classifications <ref type="bibr" coords="9,317.96,590.52,76.34,8.75">(Sekine et al, 2002)</ref>, which we feel would be a beneficial addition to the QITEKAT architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5.2 Synonym substitution</head><p>The present system architecture offers no methods for word substitution, which is a limiting factor, both in terms of matching questions with appropriate knowledge relations, and also extracting relations from document texts. The addition of a synonym system, such as WordNet <ref type="bibr" coords="9,356.60,690.96,55.20,8.75">(Miller, 1990)</ref> would enable a greater number of sentence constructs to be identified and extrapolation of questions to form multiple queries, offering a far greater chance of successful responses.</p><p>As an example, take the question text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Introduction</head><p>The Text Retrieval Conference (TREC) Genomics Track was started to provide a forum for information retrieval in the genomics area. The GeneRIF annotation is designed to allow the contents of a bioscience paper to be summarised in such a way as to show the function of the gene, which is the subject of the bioscience paper. An analysis by <ref type="bibr" coords="11,244.40,143.64,38.58,8.75;11,55.64,154.68,64.44,8.75">Mork and Aronson (2003)</ref> of NLM found that 95% of GeneRIF snippets contained some text from the title or abstract of the article. About 42% of the matches were taken directly from the title or abstract, 25% contained significant runs of words from pieces of the title or abstract.</p><p>The data provided for the Secondary Task consists of 139 GeneRIFs representing all of the articles appearing in five journals -Journal of Biological Chemistry, Journal of Cell Biology, Nucleic Acids Research, Proceedings of the National Academy of Sciences, and Science -during the latter half of 2002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Methodology</head><p>The main objective of our research is to design and implement a multi-agent based system for knowledgebased retrieval to biomedical literature. The purpose is to provide easy-to-use and seamless interfaces to biomedical literature both for the expert and for the layperson. The main components of the multi-agent systems we envisage will consist of peer-to-peer based communicating agents which are knowledgeable about biomedical resources. The biomedical information retrieval systems will provide scientists in the biomedical community with better support for searching the latest literature, therefore enabling them to be better informed about the latest developments in the biomedical field. The systems will also serve the wider community that will enable researchers, whether they are experts or non-specialists, a seamless and natural interface that will require minimal training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Knowledgeable Agent Framework</head><p>We are in the process of designing and developing a novel logic-based framework for implementing knowledgeable agents that will become the core components for our multi-agent biomedical information retrieval system. The logic is based on three relations that describe whether an agent is "knowledgeable" or not <ref type="bibr" coords="11,188.48,568.08,55.30,8.75">(Teahan, 2003</ref>; also see the first part of this paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 Context modelling</head><p>We feel that context has a very important role to play in specifying knowledge. For example, for an agent to answer the question "What is entropy?' " in a knowledgeable way, the agent must first appreciate the context in which the question is asked. A different answer is required to this question depending on whether the context concerns the domain of physics or the domain of information theory. If neither domain is apparent given the context, then it might be appropriate for an agent (if it wishes to be knowledgeable) to inform the person asking the question that more than one answer is possible to the question. We feel that different representations of context are required in different applications depending on the nature of the knowledge that needs to be specified and/or manipulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.2 GeneRIF as a context</head><p>For the application that we investigate in this paper, we explore the possibility of using GeneRIF-based annotation for specifying the context of biomedical documents in the Genomics domain. Importantly, in this "context" , we consider the MEDLINE GeneRIFs as provided in the training data for the TREC 2003 Genomics Secondary Task experiments as only a representation of the "true"' GeneRIF. By "true" , we mean what a correct annotation of the article might be if it was performed by a group of experts, rather than the GeneRIFs encountered in the MEDLINE database. Note that some of the MEDLINE GeneRIFs in the training data evidently fall short of what a group of experts might assign to the "correct" or "true" GeneRIF, if they had been allocated this task.</p><p>In light of this, we can also partially ignore the differences between the MEDLINE provided GeneRIF and the candidate GeneRIFs we are automatically generating, since we are only interested in finding a GeneRIF description that encompasses some notion of a true GeneRIF. Hence, providing GeneRIF strings that are potentially longer than the MEDLINE GeneRIF, but have a much greater chance of encompassing the "true" GeneRIF description makes sense, as our goal is to ensure as comprehensive description as possible is generated for information retrieval purposes. Our contention is this will make it more likely that a relevant document is retrieved when the GeneRIF is used in an IR system -in our case, within the distributed IR framework that we are developing based on Knowledgeable Agents.</p><p>The consequence of this is that we feel that a modified co-efficient is more suitable for our purposes to evaluate the "goodness" of the generated GeneRIF. This modified co-efficient is a measure of the percentage of words in X that occur in Y, so we are effectively ignoring the extraneous words in Y. The equation below is for the modified measure:</p><formula xml:id="formula_3" coords="11,336.56,566.84,202.68,16.22">( ) X Z D B A = ' , [1]</formula><p>as opposed to the original DICE co-efficient:</p><formula xml:id="formula_4" coords="11,319.88,592.66,219.36,19.80">( ) ( ) Y X Z D B A + × = 2 ' , [2]</formula><p>where A is the MEDLINE GeneRIF, B is the generated GeneRIF, Z is the number of words that occur in both A and B, X is the number of words in A, and Y is the number of words in B.</p><p>Changing the DICE co-efficient in this way shows that we are only interested in the following questions: "Does the generated GeneRIF contain the MEDLINE GeneRIF?" or " How much of it does it contain?" These questions are noticeably different to what the standard, unmodified DICE co-efficient measures which is: "How similar are the MEDLINE and generated GeneRIFs?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.3 Experimental Results</head><p>Table <ref type="table" coords="12,84.32,121.20,4.58,8.75" target="#tab_5">C</ref>.1 shows the results for the secondary task experiments, using both the standard DICE co-efficients (Classic DICE (see equation <ref type="bibr" coords="12,183.20,143.64,10.24,8.75">[2]</ref>), Modified Unigram DICE, Modified Bigram DICE, and Modified Bigram Phrases DICE), and the modified measure (see equation <ref type="bibr" coords="12,55.64,177.00,10.24,8.75">[1]</ref>). The table is divided into sections by experimental run, and these sections are divided further by the coefficient used to generate the average result for the particular run. The runs processed the following data: uwb2 used a concatenation of the document titles and the last line of the document abstracts as input data, uwb3 used the document titles as input data, uwb4 used the last line of the document abstracts as input data, and clairvoyant used pre-calculated DICE co-efficients to find the best possible generated GeneRIF from a choice of document title, first line of document abstract, penultimate line of the document abstract, and last line of document abstract. The clairvoyant run is therefore a measure of the best possible result that the chosen generated GeneRIF selection technique could produce, but used data that would not be available to the system when it operates autonomously. Due to this, only runs uwb2, uwb3, and uwb4 were submitted to TREC for evaluation. The use of the modified measure in the clairvoyant run is justified as the run produces the best standard DICE co-efficient results and the second best modified measure result.</p><p>The result the modified measure achieved (61.48%) indicates the better coverage of the input data of run uwb2. This is unsurprising, as the input data for this run was a concatenation of two strings; so the modified measure had more data to work with. However, incorrect data for run uwb2 was sent to TREC, but this did not have an effect on the results for the classic DICE, the modified unigram DICE, or the modified measure in this run. Our own analysis of the corrected data gives the results shown in table C.2.</p><p>The best run using the standard DICE co-efficients was the Modified Unigram DICE co-efficient in run uwb3, which produced 48.25%. This result is interesting as it shows that document titles do contain pertinent data for generating GeneRIFs. However, this advantage is lost when document titles are combined with other parts of the document, such as the last line of the document abstracts in run uwb2. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,55.64,716.32,116.78,7.02"><head>Figure</head><label></label><figDesc>Figure B.1 -Simple XML Notation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,322.28,246.16,170.14,7.02;5,328.28,151.97,38.45,8.39;5,328.28,164.09,173.09,8.39;5,328.28,176.21,121.85,8.39;5,328.28,188.21,166.73,8.39;5,328.28,200.33,160.25,8.39;5,328.28,212.45,160.25,8.39;5,328.28,224.57,44.93,8.39"><head>Figure</head><label></label><figDesc>Figure B.3 -XML Based Tag Transformation Rule &lt;rule&gt; &lt;initialtag&gt;NN&lt;/initialtag&gt; &lt;newtag&gt;VB&lt;/newtag&gt; &lt;condition&gt;POS&lt;/condition&gt; &lt;operator1&gt;TO&lt;/operator1&gt; &lt;operator2&gt;-1&lt;/operator2&gt; &lt;/rule&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,320.00,756.16,128.90,7.02"><head>Figure B. 5 -</head><label>5</label><figDesc>Figure B.5 -Typical Agent Interaction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,55.64,220.32,2.43,8.75"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,55.64,237.11,151.71,7.02"><head></head><label></label><figDesc>Figure B.6 -Results on TREC 2002 Questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,317.96,141.06,227.63,239.34"><head>•</head><label></label><figDesc>Given the question:</figDesc><table coords="8,317.96,155.65,227.63,224.76"><row><cell cols="3">When did John Lennon die? • We extract the noun and verb phrases</cell><cell></cell></row><row><cell>John Lennon</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Die • These are then passed as a search query to Google</cell></row><row><cell cols="4">"John Lennon" + "die" • The first 1000 abstracts are retrieved • The Knowledgeable Agents return three possible</cell></row><row><cell>answers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 th December</cell><cell></cell><cell></cell><cell></cell></row><row><cell>15 th August</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">19 th July • Thus we find frequency matches for each of these</cell></row><row><cell cols="4">answers in the Google abstracts, and calculate a</cell></row><row><cell>relevance rating:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ANSWER</cell><cell>FREQ</cell><cell>CALC</cell><cell>RELEVANCE</cell></row><row><cell cols="3">8 th December 462 462/533</cell><cell>0.87</cell></row><row><cell>15 th August</cell><cell>28</cell><cell>28/533</cell><cell>0.05</cell></row><row><cell>19 th July</cell><cell>43</cell><cell>43/533</cell><cell>0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,317.96,389.80,144.50,7.02"><head>Table B .1 -Relevance Ranking Calculation</head><label>B</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,337.04,110.29,185.52,311.15"><head>Table C .1: Experimental Results CO-EFFICIENT TYPE</head><label>C</label><figDesc></figDesc><table coords="12,337.04,133.09,185.52,288.35"><row><cell></cell><cell>Average %</cell></row><row><cell>uwb2</cell><cell></cell></row><row><cell>Classic DICE</cell><cell>44.41</cell></row><row><cell>Modified Unigram DICE</cell><cell>44.07</cell></row><row><cell>Modified Bigram DICE</cell><cell>2.33</cell></row><row><cell>Modified Bigram Phrases DICE</cell><cell>1.80</cell></row><row><cell>Modified Measure</cell><cell>61.48</cell></row><row><cell>uwb3</cell><cell></cell></row><row><cell>Classic DICE</cell><cell>46.48</cell></row><row><cell>Modified Unigram DICE</cell><cell>48.25</cell></row><row><cell>Modified Bigram DICE</cell><cell>29.53</cell></row><row><cell>Modified Bigram Phrases DICE</cell><cell>32.82</cell></row><row><cell>Modified Measure</cell><cell>42.74</cell></row><row><cell>uwb4</cell><cell></cell></row><row><cell>Classic DICE</cell><cell>36.28</cell></row><row><cell>Modified Unigram DICE</cell><cell>35.21</cell></row><row><cell>Modified Bigram DICE</cell><cell>22.73</cell></row><row><cell>Modified Bigram Phrases DICE</cell><cell>24.52</cell></row><row><cell>Modified Measure</cell><cell>38.80</cell></row><row><cell>clairvoyant</cell><cell></cell></row><row><cell>Classic DICE</cell><cell>58.16</cell></row><row><cell>Modified Unigram DICE</cell><cell>59.61</cell></row><row><cell>Modified Bigram DICE</cell><cell>45.04</cell></row><row><cell>Modified Bigram Phrases DICE</cell><cell>47.94</cell></row><row><cell>Modified Measure</cell><cell>54.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,317.96,435.84,227.64,202.19"><head>Table C .2: Corrected Results for uwb2</head><label>C</label><figDesc>Mork and L. Aronson. July 2003. medir.ohsu.edu/ ~genomics/protocol.html. W. Teahan, 2003. "Knowing about knowledge: Towards a framework for knowledgeable agents and Knowledge Grids", Tech Report AIIA 03.2, School of Informatics, University of Wales, Bangor.</figDesc><table coords="12,317.96,458.53,205.92,123.71"><row><cell>CO-EFFICIENT TYPE</cell><cell>Average %</cell></row><row><cell>uwb2</cell><cell></cell></row><row><cell>Classic DICE</cell><cell>44.53</cell></row><row><cell>Modified Unigram DICE</cell><cell>40.08</cell></row><row><cell>Modified Bigram DICE</cell><cell>29.20</cell></row><row><cell>Modified Bigram Phrases DICE</cell><cell>31.80</cell></row><row><cell>Modified Measure</cell><cell>-</cell></row><row><cell>C.4 References</cell><cell></cell></row><row><cell>J.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When did Charles Bronson die?</head><p>In the present QITEKAT system, this will match only those relations with an equivalent question construct, which may result in no answer being found. With synonym substitution, however, the query would be reformulated as:</p><p>When did Charles Bronson pass away? which may provide a positive match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5.3 Past Participle Determination</head><p>In a similar vein to synonym substitution, it would be useful to develop a feature within the system to automatically generate past participles of verbs, particularly for Search Engine Corroboration.</p><p>When querying a search engine, the system passes the main subjects of a question, so for example, given the question:</p><p>When did Charles Bronson die? The system forms a query using Charles Bronson and Die. It is likely however that in any documents retrieved by a search engine, the information that we are interested in would be described using the past participle (died), i.e.</p><p>Charles Bronson died on …..</p><p>Substituting the past participle may result in a more useful query string, and ultimately a greater number (or more accurate) results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5.4 Automate RE Production</head><p>Manual production of Regular Expressions to extract information from document texts was one of the more time consuming aspects of the initial QITEKAT development, and as a result meant we were only able to focus the tool at a limited number if question types in order to meet the TREC deadline. A key idea for future development of the system is to implement an automated system, capable of producing generic expressions which could then be used to extract further information. Initial thoughts are that this issue may lend itself to a transformation based system, similar to that found in Brill-type POS tagging systems <ref type="bibr" coords="10,453.08,132.36,49.36,8.75">(Brill, 1992)</ref>, where it would be possible for the system to learn a set of rules, based on existing, manually produced REs.</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
