<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.30,87.44,365.47,14.88">Multiple-Engine Question Answering in TextMap</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.34,108.59,104.88,10.46"><forename type="first">Abdessamad</forename><surname>Echihabi</surname></persName>
							<email>echihabi@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001 Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.30,108.59,70.24,10.46"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001 Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.25,108.59,61.42,10.46"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001 Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.84,122.39,63.63,10.46"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
							<email>marcu@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001 Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.20,122.39,45.07,10.46"><forename type="first">Eric</forename><surname>Melz</surname></persName>
							<email>melz@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001 Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.78,122.39,106.34,10.46"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute University of Southern California</orgName>
								<address>
									<addrLine>4676 Admiralty Way, Suite 1001 Marina del Rey</addrLine>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.30,87.44,365.47,14.88">Multiple-Engine Question Answering in TextMap</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">60FFD0532ED3389F636393B08AC28FD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>At TREC-2003, TextMap participated in the Main task, which encompassed answering the following types of questions:</p><p>• factoid questions;</p><p>• list questions;</p><p>• definition questions. In this paper, we overview the architecture of the TextMap system and report its performance, as evaluated by the NIST assessors, on each of these tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System architecture 2.1 Answering factoid and list questions</head><p>The TextMap system that answers factoid questions implements the following pipeline:</p><p>• A question analyzer identifies the expected answer type for the question given as input (see Section 3.1 for details). • A query generator produces Web-and TREC-specific queries. The query generator exploits a database of paraphrases (see Section 3.2). • Web queries are submitted to Google and TREC queries are submitted to the IR engine Inquery <ref type="bibr" coords="1,184.17,521.81,94.76,10.46" target="#b1">(Callan et al., 1995)</ref>, to retrieve respectively 100 Web and 100 TREC documents. • A sentence retrieval module selects 100 sentences each from the retrieved Web and TREC documents that are most likely to contain a correct answer.</p><p>• Three distinct answer selection modules <ref type="bibr" coords="1,308.20,585.83,213.80,10.46">(knowledge-, pattern-, and statistical-based)</ref> each pinpoint the correct answers in the resulting 200 sentences and assign them a score (see Section 3.3). • A maximum-entropy-based re-ranker is used to combine the outputs of the three answer selection modules into a single ranked list (Section 3.4). • If the top answer comes from a TREC sentence, the answer is presented to the user. If the top answer comes from a Web document, it is retrofitted to the TREC collection. When retrofitting fails, the system returns NIL.</p><p>List questions are answered by a system having the same architecture as the system used to answer factoid question. The only difference pertains to a simple backend algorithm that is used to output not one, but all answers whose score is higher than a given threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answering definition questions</head><p>Definition questions are answered by a system that uses many of the components described above and some additional ones. It scores answer candidates using the following resources:</p><p>• WordNet glosses</p><p>• Collection of 14,414 biographies from biography.com</p><p>• Mike Fleischman's corpus of 966,557 descriptors of proper people</p><p>• Set of subject-verb, object-verb, subject-copula-object relations More details of the QA system that answers definition questions are given in Section 3.5.</p><p>In what follows, we describe in more detail each of the individual components and report the performance of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question analysis</head><p>After parsing a question, TextMap determines its answer type, or "Qtarget", such as PROPER-PERSON, PHONE-NUMBER, or NP. We have built a typology of currently 185 different types, organized into several classes (Abstract, Semantic, Relational, Syntactic, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query generation via reformulation</head><p>To bridge the gap between question and answer sentence wordings, TextMap uses paraphrasing. For any given question, TextMap generates a set of high-precision meaning-preserving reformulations to increase the likelihood of finding correct answers in texts. These reformulations are used to generate more focused TREC and Web queries. For example, the question below produces the following reformulations:</p><p>Question: How did Mahatma Gandhi die?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reformulation patterns:</head><p>• Mahatma Gandhi died &lt;how&gt;?</p><p>• Mahatma Gandhi died of &lt;what&gt;?</p><p>• Mahatma Gandhi lost his life in &lt;what&gt;?</p><p>• Mahatma Gandhi was assassinated?</p><p>• Mahatma Gandhi committed suicide?</p><p>• … plus 40 other reformulations …</p><p>The fourth reformulation will easily match "Mahatma Gandhi was assassinated by a young Hindu extremist," preferring it over alternatives such as "Mahatma Gandhi died in 1948."</p><p>The reformulation collection in TextMap currently contains 550 assertions grouped into about 105 equivalence blocks. At run-time, the number of reformulations produced by our current system varies from one reformulation (which might just rephrase a question into a declarative form) to more than 40. On the TREC-2003 questions, we produced, on average, 5.03 reformulations per question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Knowledge-based answer selection</head><p>The knowledge-based answer selection module uses the CONTEX parser <ref type="bibr" coords="3,435.42,270.66,86.64,9.57" target="#b6">(Hermjakob, 1997;</ref><ref type="bibr" coords="3,90.00,283.68,23.67,9.57" target="#b9">2001)</ref>, a decision tree based deterministic parser, which has been enhanced for question answering by an additional treebank of 1,200 questions, named entity tagging that among other components uses BBN's IdentiFinder <ref type="bibr" coords="3,258.58,309.66,81.24,9.57" target="#b0">(Bikel et al., 1999)</ref>, and a semantically motivated parse tree structure that facilitates matching for paraphrasing and of question/answer pairs. Answer selection is guided by the degree of matching at the syntactic/semantic level between question and answer parse trees and several additional heuristics that penalize answers for a variety of reasons, including: The knowledge-based answer selection module uses a limited amount of external knowledge to enhance performance: the WordNet hierarchy; internal quantity and calendar conversion routines; and abbreviation routines. See <ref type="bibr" coords="3,382.54,472.25,75.97,10.46;3,90.00,486.05,63.27,10.46">(Echihabi et al., forthcoming)</ref> for a detailed description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Pattern-based answer selection</head><p>The pattern-based answer selection module uses a set of automatically learned patterns <ref type="bibr" coords="3,90.00,543.59,152.53,10.46" target="#b10">(Ravichandran and Hovy, 2002)</ref>. The learning proceeds in two steps:</p><p>1. Given a Qtarget from the TextMap ontology (a relation such as BIRTHYEAR), and a few instances of &lt;Question; Answer&gt; pairs such as (NAME_OF_PERSON, BIRTHYEAR), extract from the web all the different patterns (TEMPLATEs) that contain such a pair. 2. Calculate the precision of each pattern and keep the patterns of high precision. For example, for BIRTHYEAR, some of the patterns learned by the system and the precision of those patterns are: Once a set of patterns is learned, the pattern-based answer selection system uses them in order to find possible answer candidates. The set of potential answers is sorted into a ranked list using a maximum-entropy-based framework, in the style of <ref type="bibr" coords="4,431.57,157.97,85.00,10.46">Ittycheriah (2002)</ref>.</p><formula xml:id="formula_0" coords="3,90.00,667.91,90.88,10.46">Prec. #Correct #</formula><p>The features used by the system are the type of pattern; the frequency of an answer; the expected answer type; the word-overlap between the question and the answer sentence.</p><p>The pattern-based ranker was trained using the 1192 questions in the TREC 9 and TREC 10 data sets for training and the 500 questions in the TREC 11 data set for crossvalidation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Statistics-based answer selection</head><p>The statistics-based answer selection module implements a noisy-channel model for answer selection -see <ref type="bibr" coords="4,198.10,284.51,135.21,10.46">(Echihabi and Marcu, 2003)</ref> for details. This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of question-answer pairs (Q, S A ), one can train a probabilistic model for estimating the conditional probability P(Q | S A ). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by the IR engine, we find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ). The probability model is trained using off-the-shelf parameter estimation package that was developed for statistical machine translation.</p><formula xml:id="formula_1" coords="4,134.46,439.09,238.36,136.31">Test question Q S i,A i,j QA Model trained using GIZA S x,A x,y = argmax (P(Q | S i,A i,j</formula><p>))</p><formula xml:id="formula_2" coords="4,192.50,466.00,182.76,131.19">A = A x,y GIZA S 1 S m S 1,A 1,1 S 1,A 1,v S m,A m,1 S m,A m,w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IR</head><p>Figure <ref type="figure" coords="4,154.39,615.10,4.67,10.46">1</ref>: The noisy-channel approach to question answering.</p><p>For training, we use TREC 9 and TREC 10 questions (1091) with answer sentences (18618) automatically generated from the corresponding judgment sets. We also use questions ( <ref type="formula" coords="4,143.70,670.28,21.34,10.46">2000</ref>) from http://www.quiz-zone.co.uk with answer sentences (6516) semi-automatically collected from the web and annotated for correctness by a linguist<ref type="foot" coords="5,474.78,75.08,3.00,5.23" target="#foot_0">1</ref> . To estimate the parameters of our model, we use GIZA, a publicly available statistical machine translation package (http://www.clsp.jhu.edu/ws99/projects/mt/). Figure <ref type="figure" coords="5,124.35,116.57,6.00,10.46">1</ref> depicts graphically the noisy-channel based answer selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combining the output of multiple answer-selection modules</head><p>Simple inspection of the outputs produced by our individual components on a development corpus of questions from the TREC-2002 collection revealed several consistent patterns of error:</p><p>• The pattern-based answer selection module performs well on questions whose Qtargets are recognizable named entities (NAMES, ORGANIZATIONS, LOCATIONS). However, this module performs less well on questions with more general QTargets (NPs, for example). • The statistics-based answer selection module does not restrict the types of the answers it expects for a question Qtargets: it assumes that any semantic constituent can be an answer. As a consequence, many of the answers produced by this module may not be exact.</p><p>• Overall, all modules made some blatant mistakes. The pattern-based and statisticsbased modules in particular sometimes select as top answers strings like "he", "she", and "it", which are unlikely to be good answers for any factoid question one may imagine. To address these problems, we decided to use the maximum entropy framework to rerank the answers produced by the answer selection modules and root out the blatant errors. For this purpose, we have used a set of 48 feature functions, which can be classified into the following categories:</p><p>1. Component-specific: Scores from the individual answer selection module and associated features like the rank of the answer and the presence/absence of the answers produced by the individual answer selection module. We also add features based on the scores produced by the IR module and word overlap between question and answers.</p><p>2. Redundancy-specific: Count of candidate answers in the collection. We also add additional features for the logarithm and the square root of the counts.</p><p>3. Qtarget-specific: It was observed that some of the answer selection modules answer certain Qtargets better than others. We therefore model a set of features wherein we combine certain classes of Qtarget with the score of the individual answer selection module. This enables the maximum entropy model to assign different model parameters to different types of questions and answer selection module.</p><p>4. Blatant-error-specific: We model a set of features based on the development set in an iterative process. These include (negative) features such as "answers usually do not have personal pronouns" and "WHEN questions usually do not contain day of the week as answer", among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Special modules/resources for answering definition questions</head><p>The question-answering track for TREC-2003 included a new definition subtask, in which definition questions such as "Who is Aaron Copland?", "What is a golden parachute?", and "What is Bausch&amp;Lomb?" had to be answered by a set of relevant information nuggets.</p><p>Given a question like "Who is Aaron Copland?", the challenge here is to identify relevant sentences such as • Aaron Copland's death comes a definitive biography of America's most important composer • Copland was born in 1900, the son of Russian Jewish immigrants (original name: Kaplan) in a larger set of sentences that the IR module extracted as potential candidates:</p><p>• So she took me to meet Aaron Copland, who was then in his early thirties.</p><p>• Each recipient of the honor, known as the Aaron Copland Awards, will get the run of the place with a spouse or partner, but no children or pets are allowed.</p><p>We built and used several resources to identify relevant information</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Collection of Biographies</head><p>We were able to obtain 14,414 biographical entries from http://www.biography.com, which we used to extract core biographical information for specific people as well as identify words that are indicative of biographical information.</p><p>We built a list of 6,640 words that occur at least five times in biographies and occur much more frequently in biographical text (biography.com) than in standard text (Wall Street Journal). The top 20 terms (along with their "strength") are: The second use of the biography collection was to boost answer candidates with words from core biography terms for the specific person in question:</p><p>• Copland , Aaron 1900 --1990 is a Composer; born in New York City.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Collection of Descriptors for Proper People</head><p>TextMap uses a slightly cleaned-up version of Michael Fleischman's list of 966,557 descriptor entries for proper people <ref type="bibr" coords="7,262.20,146.51,119.67,10.46" target="#b5">(Fleischman et al., 2003)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">WordNet</head><p>If the anchor term of the definition question (e.g. "Aaron Copland") has a WordNet gloss <ref type="bibr" coords="7,90.00,342.05,134.42,10.46">(Miller and Fellbaum, 1998)</ref>, TextMap gives credit for sentences that contain words and expressions that overlap with that WordNet gloss (as underlined in the example below):</p><p>WordNet gloss: Copland, Aaron Copland --United States composer  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Semantic-Relationship Patterns</head><p>TextMap gives credit when the anchor of a question is found to match one of currently 110 semantic relations, for example when the question anchor is the logical subject of verbs like "to compose", "to write", "to teach"; when it is the logical object of verbs like "to bear" (to be born); when it is in a subject-copula-object relationship with a head noun like "composer", "advocate", "son" (or any other relative).</p><p>• Aaron Copland composed "Fanfare for the Common Man."</p><p>• Aaron Copland was born in 1900, the son of Russian Jewish immigrants</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.5">Duplicate Removal and Answer Cutoff</head><p>Duplicates are avoided by giving credit only for those words, expressions and relations that have not yet been observed in higher-scoring information nuggets.</p><p>To determine the number of answer nuggets for a given question, we start with a very low threshold score and then linearly increase the threshold for each answer, eliminating answers with a score lower than the rising threshold. This mechanism keeps the number of answers in a reasonable range while allowing more answers for questions with many</p><p>high-scoring answer candidates. This allowed our system to provide over 20 nuggets for questions such as "Who is Bill Bradley/Aaron Copland/Andrew Carnegie?" while limiting itself to as few as 3 answers questions such as "What is Iqra/El Shaddai/ Restorative Justice?" for which the system found fewer promising information nuggets.</p><p>For the definition questions in TREC-2003, our system returned an average of 12.2 answers and 1663.1 bytes per question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Results</head><p>In the official TREC evaluation, TextMap scored 33.7% for factoid questions, 11.8% for list questions, and 46.1% for definition questions, resulting in a composite score of 31.3%</p><p>We also performed the following three sets of experiments with the Web as our corpus.</p><p>1. Maximum Entropy re-ranking performed on individual answer selection modules: In this experiment, we turn off all the features in any answer selection module that depends on another answer selection module and we use for re-ranking only the top 50 answers supplied by one answer selection component. Thus this experiment enables us to assess the impact separately on each individual answer selection module of redundancy-, Qtarget-, and blatant-error-specific features, as well as improved weighting of the features specific to each module.</p><p>2. Maximum Entropy re-ranking on all answer selection modules: In this experiment we add all the features described in Section 5.3 and test the performance of the combined system after re-ranking. Re-ranking is performed on the answer set that results from combining the top 50 answers returned by each individual answer selection module. This experiment enables us to assess whether the ME framework enables us to better exploit strengths specific to each answer selection module.</p><p>3. Feature Selection: Since we have only 200 training examples and almost 50 features, the training is likely to be highly prone to over-fitting. To avoid this problem we apply feature selection as described in <ref type="bibr" coords="8,302.14,491.69,124.60,10.46">(Della Pietra et al., 1996)</ref>. This reduces the number of features from 48 to 31. The inspection of the weights computed by the ME-based feature selection algorithm shows that our log-linear model was able to lock and exploit strengths and weaknesses of the various modules. For example, the weight learned for a feature that assesses the likelihood of the pattern-based module to find correct answers for questions that have QUANTITY as a Qtarget is negative, which suggests that the pattern-based module is not good at answering QUANTITY-type questions. The weight learned for a feature that assesses the likelihood of the statistics-based module to find correct answers for questions that have an UNKNOWN or NP Qtarget is large, which suggests that the statistical module is better suited for answering these types of questions. The knowledge-based module is better than the others in answering QUANTITY and DEFINITION questions. These results are quite intuitive: The pattern-based module did not have enough QUANTITY-type questions in the training corpus to learn any useful patterns. The Statistics-based module implemented here does not explicitly use the Qtarget in answer selection and does not model the source probability of a given string being a correct answer. As a consequence, it explores a much larger space of choices when determining an answer compared to the other two modules. At the moment, the knowledge-based module yields the best individual results. This is not surprising given that it is a module that was engineered carefully, over a period of three years, to accommodate various types of Qtargets and knowledge resources. Many of the resources used by the knowledge-based module are not currently exploited by the other modules: the semantic relations identified by CONTEX; the ability to exploit the paraphrase patterns and advanced forms of reformulation for answer pinpointing; the external sources of knowledge (WordNet; abbreviation lists; etc.); and a significant set of heuristics (see Section 2.4).</p><p>Our results suggest that in order to build good answer selection modules, one needs to both exploit as many sources of knowledge as possible and have good methods for integrating them. The sources of knowledge used only by the knowledge-based answer selection module proved to have a stronger impact on the overall performance of our answer selection systems than the ability to automatically train parameters in the patternand statistics-based systems, which use poorer representations. Yet, the ability to properly weight the contribution of various knowledge resources was equally important. For example, Maximum Entropy naturally integrated additional features into the knowledgebased answer selection module; a significant part of the 9.2% increase in correct answers reported in Table <ref type="table" coords="10,175.62,171.77,6.00,10.46" target="#tab_3">1</ref> can be attributed to the addition of redundancy features, a source of knowledge that was unexploited by the base system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,376.81,427.76,13.44;3,90.00,395.83,330.74,13.44;3,90.00,414.84,432.01,13.44;3,108.00,431.64,63.53,9.57"><head>•</head><label></label><figDesc>Qtarget match factor: Q: How long did the Manson trial last? Semantic mismatch: 20 miles • Vagueness penalty: Q: Where is Luxor? Too vague: on the other side • Negation penalty: Q: Who invented the electric guitar? Negation: Fender did not invent the electric guitar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,129.00,538.67,345.60,163.05"><head>Table 1 .</head><label>1</label><figDesc>Performance of various answer selection modules in TextMap, an end-to-end QA system.Table1summarizes the results: it shows the percentage of correct, exact answers returned by each answer selection module with and without ME-based re-ranking, as well as the percentage of correct, exact answers returned by an end-to-end QA system that uses all three answer selection modules together. Table 1 also shows the performance of these systems in terms of percentage of correct answers ranked in the top 5 answers and the corresponding MRR scores.The results in Table1show that appropriate weighting of the features used by each answer selection module as well as the ability to capitalize on global features, such as the counts associated with each answer, are extremely important means for increasing the overall performance of a QA system. ME re-ranking led to significant increases in performance for each answer selection module individually. When measured with respect to the ability of our system to find correct, exact answers when returning only the top answer, it accounted for 14.33% reduction in the error rate of the knowledge-based answer selection module; 7.1% reduction in the error rate of the pattern-based answer selection module; and 13.85% reduction in the error rate of the statistical-based answer selection module. Combining the outputs of all systems yields an additional increase in performance; when over-fitting is avoided through feature selection, the overall reduction in error rate is 3.96%. This corresponds to an increase in performance from 45.03% to 47.21% on the top-answer accuracy metric.</figDesc><table coords="8,135.36,538.67,330.13,132.84"><row><cell>Metric</cell><cell cols="2">Knowledge-Based</cell><cell cols="2">Pattern-Based</cell><cell cols="2">Statistical-Based</cell><cell>Base from</cell><cell>Base from</cell></row><row><cell></cell><cell>Base</cell><cell>Base followe d by ME re-ranking</cell><cell>Base</cell><cell>Base followe d by ME re-ranking</cell><cell>Base</cell><cell>Base followe d by ME re-ranking</cell><cell>all systems followed by ME re-ranking (no</cell><cell>all systems followed by ME re-ranking (with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feature</cell><cell>feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>selection)</cell><cell>selection)</cell></row><row><cell>Top</cell><cell cols="7">35.83% 45.03% 25.18% 30.50% 21.30% 32.20% 46.37%</cell><cell>47.21%</cell></row><row><cell>answer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top 5</cell><cell cols="7">57.38% 56.41% 35.59% 43.09% 31.23% 40.92% 57.62%</cell><cell>57.62%</cell></row><row><cell>MRR</cell><cell>43.88</cell><cell>49.36</cell><cell>28.57</cell><cell>35.37</cell><cell>24.83</cell><cell>35.51</cell><cell>51.07</cell><cell>51.27</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,95.28,711.94,268.65,7.85"><p>We are grateful to Miruna Ticrea for annotating the question-answer pairs.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,90.00,248.97,431.99,8.74;10,104.40,260.49,201.97,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,298.38,248.97,177.74,8.74">An Algorithm that Learns What&apos;s in a Name</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,486.90,248.97,35.09,8.74;10,104.40,260.49,162.59,8.74">Machine Learning-Special Issue on NL Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,278.01,431.96,8.74;10,104.40,289.47,181.70,8.74" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,287.27,278.01,234.70,8.74;10,104.40,289.47,116.74,8.74">TREC and Tipster Experiments with Inquery. Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="343" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,306.99,432.09,8.74;10,104.40,318.51,117.83,8.74" xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Melz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,447.92,306.99,74.16,8.74;10,104.40,318.51,113.26,8.74">How to Select an Answer String. Forthcoming</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,335.97,431.96,8.74;10,104.40,347.49,10.05,8.74;10,114.48,345.31,4.32,5.65;10,121.26,347.49,368.62,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,232.36,335.97,206.71,8.74">A Noisy-Channel Approach to Question Answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,446.40,335.97,75.56,8.74;10,104.40,347.49,10.05,8.74;10,114.48,345.31,4.32,5.65;10,121.26,347.49,255.35,8.74">Proceedings of the 41 st Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 41 st Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-07">2003. July 7-12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,184.91,365.01,337.11,8.74;10,104.40,376.47,130.83,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,295.74,365.01,172.71,8.74">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="http://www.cogsci.princeton.edu" />
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,393.99,432.15,8.74;10,104.40,405.51,285.22,8.74;10,389.64,403.33,4.32,5.65;10,397.32,405.51,124.70,8.74;10,104.40,416.97,245.05,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,307.53,393.99,214.62,8.74;10,104.40,405.51,187.01,8.74">Offline Strategies for Online Question Answering: Answering Questions Before They Are Asked</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fleischman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,299.52,405.51,90.10,8.74;10,389.64,403.33,4.32,5.65;10,397.32,405.51,124.70,8.74;10,104.40,416.97,131.74,8.74">Proceedings of the 41 st Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 41 st Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-07">2003. July 7-12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,434.49,432.04,8.74;10,104.40,446.01,417.60,8.74;10,104.40,457.47,85.35,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,178.61,434.49,310.54,8.74">Learning Parse and Translation Decisions from Examples with Rich Context</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<ptr target="file://ftp.cs.utexas.edu/pub/mooney/papers/hermjakob-dissertation-97.ps.gz" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Texas Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct coords="10,90.00,474.99,432.17,8.74;10,104.40,486.51,398.33,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,300.78,474.99,221.39,8.74;10,104.40,486.51,167.94,8.74">Natural Language Based Reformulation Resource and Web Exploitation for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,279.42,486.51,112.75,8.74">Proceedings of the TREC-11</title>
		<meeting>the TREC-11<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,503.97,431.93,8.74;10,104.40,515.49,386.59,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,390.66,503.97,131.27,8.74;10,104.40,515.49,125.33,8.74">Using Knowledge to Facilitate Pinpointing of Factoid Answers</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.88,515.49,184.26,8.74">Proceedings of the COLING-2002 Conference</title>
		<meeting>the COLING-2002 Conference<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,532.95,432.05,8.74;10,104.40,544.47,190.35,8.74" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<title level="m" coord="10,185.10,532.95,285.84,8.74">Trainable Question Answering System. Ph.D. Dissertation, Rutgers</title>
		<meeting><address><addrLine>New Brunswick, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>The State University of New Jersey</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,90.00,561.99,432.07,8.74;10,104.40,573.45,95.34,8.74;10,199.74,571.27,5.04,5.65;10,209.88,573.45,312.02,8.74;10,104.40,584.97,97.85,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,254.52,561.99,262.98,8.74">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,104.40,573.45,95.34,8.74;10,199.74,571.27,5.04,5.65;10,209.88,573.45,307.44,8.74">Proceedings of the 40 th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40 th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
