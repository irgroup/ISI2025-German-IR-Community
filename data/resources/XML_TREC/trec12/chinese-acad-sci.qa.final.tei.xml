<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.44,82.87,334.53,13.50">TREC 2003 Question Answering Track at CAS-ICT</title>
				<funder>
					<orgName type="full">Institute Youth Fund</orgName>
				</funder>
				<funder ref="#_CpUqYxT">
					<orgName type="full">National High Technology Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,231.06,107.70,36.49,9.02"><forename type="first">Yi</forename><surname>Chang</surname></persName>
							<email>changyi@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.89,107.70,44.81,9.02"><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
							<email>hbxu@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.17,107.70,37.10,9.02"><forename type="first">Shuo</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.44,82.87,334.53,13.50">TREC 2003 Question Answering Track at CAS-ICT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F592F3BD7984AB7C5793FE0729FE16F1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Chunk</term>
					<term>Bi-sentence</term>
					<term>multilevel retrieval</term>
					<term>voting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our system, we make use of Chunk information to analyze the question. A multilevel method is fulfilled to retrieve candidate Bi-sentences. As to answer selecting, we proposed a voting method. We figure out the performance of each module of our system, and our study shows that 65.54% information has lost in document retrieval and Bi-sentence retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is the third time we take part in the TREC-QA track. We undertook the new main subtask and submitted three runs for evaluation.</p><p>Our QA system incorporates several useful tools. The first is LT_CHUNK, which is developed at University of Edinburgh. We use LT_CHUNK to get the chunks of each sentence and the POS tags of different words. The second is GATE, developed by University of Sheffield. We use GATE to identify some Named Entities.</p><p>In our previous QA system, we tried different kinds of elaborate algorithms, but the results were not satisfactory, and we didn't make it clear what is the performance of each step in our system. So we try to figure it out this year, and our study shows: 65.54% information has lost in document retrieval and Bi-sentence retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description</head><p>Our system contains four major modules, namely Question Analyzing Module, Multilevel Bi-sentence Retrieval Module, Entity Recognizing Module and Answer Selecting Module. However, for those definition questions, the Entity Recognizing Module is unnecessary. The system architecture is represented as below.</p><p>To answer each question, Question Analyzing Module makes use of NLP techniques to identify the right type of information that the question requires. We select top 50 out of the 1000 given relevant documents to find the candidate answer from them. Since there exists too much redundant information in these documents, Multilevel Bi-sentence Retrieval Module matches the question with the 50 relevant documents at different levels and retrieves some top ranked Bi-sentences to find the candidate answer from them. Entity Recognizing Module identities the candidate entities from the selected Bi-sentences, and Answering Selecting Module selects the answer in a voting method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Analyzing Module</head><p>Since most of the factoid and list questions ask for Named Entity (NE) as the response, Question Analyzing Module tries to identify the required NE type during parsing a question. Some question words can map to the required NE types directly, e.g. "who"(PERSON), "where"(LOCATION), "how many"(NUMBER).</p><p>However, for most of the questions whose question word is "which" or "what", we need to find the Core Noun to help us to identify the required NE type. Where Core Noun is a noun in each question that indicates the answer. For example: <ref type="bibr" coords="2,99.00,232.50,11.74,9.02" target="#b0">[1]</ref> Which city is home to Superman? <ref type="bibr" coords="2,99.00,248.10,11.74,9.02" target="#b1">[2]</ref> Which past and present NFL players have the last name of Johnson? <ref type="bibr" coords="2,99.00,263.70,11.74,9.02" target="#b2">[3]</ref> What type of bee drills holes in wood?</p><p>The Core Noun of question <ref type="bibr" coords="2,202.34,279.30,11.68,9.02" target="#b0">[1]</ref> is "city" since the answer to the question is a city, and the Core Noun of question <ref type="bibr" coords="2,126.63,294.90,11.68,9.02" target="#b1">[2]</ref> and <ref type="bibr" coords="2,158.21,294.90,11.68,9.02" target="#b2">[3]</ref> is respectively "players" and "bee". We identify the required NE type according to a predefined Map Lexicon that consists of hundreds of nouns mapping to the NE type that can be recognized later, e.g. "city"(CITY), "player"(PERSON). We build another Abstract Noun Lexicon that consists of some abstract nouns, e.g. "type", "breed".</p><p>The algorithm to find the Core Noun is as follows:</p><p>Step 1: Take the last noun in the first Noun Group as Core Noun;</p><p>Step 2: If the Core Noun is in Abstract Noun Lexicon, find the last noun in the next Noun Group as Core Noun;</p><p>Step 3: If there is no suitable noun that can be found, the Core Noun is empty.</p><p>Because there are some questions whose required NE type cannot be recognized later by Entity Identifying Module, e.g. "bee". We regard these questions as "other NE questions" and keep their Core Noun for further matching.</p><p>The question whose Core Noun is empty is called "miscellaneous question". For example: <ref type="bibr" coords="2,99.00,497.70,11.74,9.02" target="#b3">[4]</ref> How did Minnesota get its name? <ref type="bibr" coords="2,99.00,513.30,11.74,9.02" target="#b4">[5]</ref> What is tequila made from?</p><p>Applying some syntactic patterns to find its answer is a practical method, but in our current QA system we simply answer NIL.</p><p>Processing the definition question is rather simple, and we just extract the phrase to be explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multilevel Bi-sentence Retrieval Module</head><p>A Bi-sentence is a pair of consecutive sentences, and a Phrase is a sequence of keywords or one keyword in a question, where a Keyword is a word in the question but not in our Stop Word list.</p><p>The goal of Multilevel Bi-sentence Retrieval Module is to find some top ranked Bi-sentences most relevant to a question. According to our training results on TREC-11 QA corpus, we select top 20 Bi-sentences for the factoid and definition questions and top 50 Bi-sentences for the list question.</p><p>We assume: 1) Bi-sentences that can match a phrase of a question are more relevant than those only can match separate keywords. 2) Bi-sentences that can match a phase of a question in raw form are more relevant than those only can match in stemmed form.</p><p>Since the strictly matching will decrease the recall rate of the Bi-sentences retrieval, we parse a Bi-sentence for several times and match the question at different levels to improve the precision rate without decreasing the recall rate.</p><p>For factoid and list question, our system applies a four-level method to select candidate Bi-sentences. At each level, we define two kinds of substrings, Compulsory Phrase and Assistant Keyword. Compulsory Phrase is a phrase set in which each element is obligatory to match Bi-sentences.</p><p>Assistant Keyword is a keyword set in which each element is optional to match. Those words not belong to the Compulsory Phrase and Stop Word list are regarded as the elements of the Assistant Keyword. We compute the weight of a Bi-sentence as below:</p><formula xml:id="formula_0" coords="3,199.50,164.26,217.63,14.49">) _ _ /( _ _ p count q count a count p weight + × = β</formula><p>Where means the weight of the Bi-sentence, p weight _ a count _ means the number of matching Assistant Keyword between the question and the Bi-sentence, q count _ means the number of keywords in the question, p count _ means the number of keywords in the Bi-sentence, and β is an experiential parameter. All relevant Bi-sentences are ranked by: the Bi-sentence selected from the higher level has a higher priority, and in the same level, the Bi-sentence with a larger weight has a higher priority. Furthermore, the first level is based on raw matching, while the other three levels are based on stemmed matching.</p><p>At the first level, we take the last Noun Group and the last verb in the last Verb Group as the Compulsory Phrase. And those phrases with initial capital on each word are also regarded as the Compulsory Phrase. At the second level, we move the verb from the Compulsory Phrase to the Assistant Keyword because the verb is difficult to match and we don't fulfill any verb expansion. At the third level, we only leave those phrases composed of successive initial capital words as the Compulsory Phrase. At the last level, the Compulsory Phrase is empty, all words belong to the Assistant Keyword, and this is equal to the method in our previous QA system to compute relevance between the question and the candidate Bi-sentences.</p><p>For definition question, our system simply takes a two-level method to select a candidate Bi-sentence. At the first level, we take the phrase to be explained as the Compulsory Phrase. A Bi-sentence selected not only matches the Compulsory Phrase but also matches the definition pattern proposed by InsightSoft [1] . In the second level, we just regard each keyword in the question as Assistant Keyword, and then find the most relevant Bi-sentence according to the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity Recognizing Module</head><p>We use GATE to recognize some types of Named Entities, such as PERSON, LOCATION, COUNTRY, etc. And we take some new strategies to recognize other types, such as YEAR, COLOR, DISEASE, etc.</p><p>For those questions whose required NE type is identified, we recognize the required NE as our candidate answer. However for those questions whose required NE type cannot be identified, we make use of Core Noun to construct the possible phrase as the candidate entity by some syntactic rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Selecting Module</head><p>In the top 20 Bi-sentences for each factoid question, we may find more than one suitable Named Entity. How to choose the most suitable NE as the answer in our system is difficult, since the NE type is the only semantic information we used.</p><p>We assume that the answer in the top 20 Bi-sentences is most likely to appear several times, so we use a voting method to select the answer. In our experiments on TREC-11 QA corpus, the voting method gets an improvement of 15.58% comparing with the method that simply select the first one as the answer.</p><p>We also study the weighted voting method, where the weight of the candidate answer decreases with the rank of the Bi-sentences. According to our experiments on TREC-11 QA corpus, the results are similar to the simple voting method.</p><p>For list question, since the answer number of each question is unknown, we choose the entities whose frequencies in voting are beyond a threshold. According to training results on TREC-11 QA corpus, our threshold varies from the required NE types.</p><p>For definition question, we simply choose the first Bi-sentence as the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We don't try radically different methods in our three runs, but instead we simply make little change to get a steady output. Table <ref type="table" coords="4,239.47,185.70,4.17,9.02">3</ref>.1 shows our results. In run A, we answer NIL for all "other NE question" whose required NE type is highly depended on Core Noun, and we only take part of the definition patterns into effect. In run B, we use some looser rules to construct the possible NE for "other NE question", while in run C we use some stricter rules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Error Analysis</head><p>Here we just focus our analysis on factoid questions because it is easier to get a quantitative evaluation at each step than that of the other two types of questions. Since we simply answer NIL whenever we cannot find an answer, we don't have too many opinions on the NIL questions. Therefore we only focus our analysis on the question whose answer is not NIL. All of the error analysis is studied from the result of run C. The performance of our Question Analyzing Module is fairly satisfactory, and the overall accuracy of question analysis is 94%. There are two main reasons for the rest 30 questions incorrectly analyzed: 1) 16.7% (5/30) error is caused by chunk error of LT_CHUNK. 2) 83.3% (25/30) error is that our Question Analyzing algorithm cannot cover all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Analyzing Error</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieval Error</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Document Retrieval Error</head><p>According to the our statistical result, there are 275 out of 413 factoid questions whose answers can be got from the top 50 documents. Since there are total 383 questions whose answer is not NIL, the maximum accuracy of document retrieval with top 50 documents is 71.80%. That is, at least 28.20% of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Bi-sentence Retrieval Error</head><p>In our statistics, from the top 20 Bi-sentences, there are only 132 out of 275 factoid questions can be answered correctly based on the answer and the corresponding Document ID. So the maximum accuracy of Bi-sentence retrieval by our Multilevel Bi-sentence Retrieval Module is 48.0%. Also, at least 52.0% of 275 questions cannot be answered correctly in Bi-sentence retrieval process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Error</head><p>There are the two main reasons for the inexact answer error: 1) 50% (4/8) error is caused by the inexact identification of required NE type; 2) 50% (4/8) is caused by the inexact recognizing of NE.</p><p>Since we make use of no more semantic information than NE type, we cannot avoid the unsupported answer error.</p><p>According to our manual statistical result listed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Of 383 factoid questions whose answer is not NIL, only 46 are correctly answered in our system, so the precision is 12.01%. The accuracy of document retrieval is 71.80% and Bi-sentence retrieval 48.0%. The accuracy of question analyzing, NE recognizing and NE selecting adds up to 34.85%. That is, 65.54% error results from retrieval process including document retrieval and Bi-sentence retrieval, while only 22.45% error results from question analyzing, NE recognizing and NE selecting.</p><p>According to the study above, most information lost during retrieval process. So we hope to focus our further study on seeking better retrieval algorithms, especially Bi-sentence retrieval algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" coords="1,144.72,510.36,305.82,168.42"><head></head><label></label><figDesc></figDesc><graphic coords="1,144.72,510.36,305.82,168.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,100.38,248.58,392.35,96.68"><head>Table 3 . 1</head><label>31</label><figDesc></figDesc><table coords="4,100.38,248.58,392.35,72.92"><row><cell>RunID</cell><cell>Factoid</cell><cell>#Correct</cell><cell cols="3">#Inexact #Unsupported List</cell><cell>Definition</cell><cell>Final</cell></row><row><cell></cell><cell>Accuracy</cell><cell>(#NIL)</cell><cell></cell><cell></cell><cell>Ave_F</cell><cell>Ave_F</cell><cell>Score</cell></row><row><cell cols="2">ICTQA2003A 0.128</cell><cell>53(21)</cell><cell>6</cell><cell>6</cell><cell cols="2">0.091 0.142</cell><cell>0.122</cell></row><row><cell cols="2">ICTQA2003B 0.140</cell><cell>58(17)</cell><cell>5</cell><cell>8</cell><cell cols="2">0.089 0.149</cell><cell>0.130</cell></row><row><cell cols="2">ICTQA2003C 0.145</cell><cell>60(14)</cell><cell>8</cell><cell>10</cell><cell cols="2">0.091 0.149</cell><cell>0.133</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,469.02,415.18,108.02"><head>Table 4 .</head><label>4</label><figDesc><ref type="bibr" coords="4,145.99,469.02,4.17,9.02" target="#b0">1</ref> shows the distribution of the question analysis and the error rate in each question</figDesc><table coords="4,90.00,484.62,397.84,92.42"><row><cell>category.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Question</cell><cell>#Total</cell><cell>#NE</cell><cell>identified</cell><cell>#Other NE</cell><cell>#Miscellaneous</cell></row><row><cell>type</cell><cell>Questions</cell><cell cols="2">Questions/#Wrong</cell><cell>Questions/#Wrong</cell><cell>Questions/#Wrong</cell></row><row><cell>Factoid</cell><cell>413</cell><cell>275/5</cell><cell></cell><cell>123/24</cell><cell>15/1</cell></row><row><cell>List</cell><cell>37</cell><cell>24/0</cell><cell></cell><cell>13/0</cell><cell>0/0</cell></row><row><cell>Definition</cell><cell>50</cell><cell>/</cell><cell></cell><cell>/</cell><cell>50/0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,258.00,591.78,37.18,9.02"><head>Table 4 . 1</head><label>41</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,90.00,232.50,415.18,96.62"><head>Table 4 . 2</head><label>42</label><figDesc>Table 4.2, those questions whose required NE type can be identified are easier to answer.</figDesc><table coords="5,101.58,264.18,391.81,41.24"><row><cell>Question Type</cell><cell cols="5">#Not NIL Questions #Correct Precision #Inexact #Unsupported</cell></row><row><cell cols="2">NE identified Question 254</cell><cell>41</cell><cell>16.14%</cell><cell>5</cell><cell>8</cell></row><row><cell>Other NE Question</cell><cell>116</cell><cell>5</cell><cell>4.31%</cell><cell>3</cell><cell>2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is supported by the <rs type="funder">National High Technology Research and Development Program of China</rs>(<rs type="programName">863 program</rs>) under contact of <rs type="grantNumber">2002AA142110</rs>, the <rs type="funder">Institute Youth Fund</rs> under contact of 20026180-24. We give our thanks to all the people who have contributed to this research and development, in particular <rs type="person">Xueqi Cheng</rs>, <rs type="person">Bin Wang</rs>, <rs type="person">Dongbo Bu</rs>, <rs type="person">Li Guo</rs>, etc.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CpUqYxT">
					<idno type="grant-number">2002AA142110</idno>
					<orgName type="program" subtype="full">863 program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.57,593.28,399.69,9.02;5,90.00,608.88,203.67,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,175.86,593.28,296.37,9.02">Patterns of Potential Answer Expressions as Clues to the Right Answer</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,493.08,593.28,12.18,9.02;5,90.00,608.88,129.93,9.02">the Tenth Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,104.86,624.48,400.41,9.02;5,90.00,640.08,155.49,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,216.77,624.48,207.78,9.02">Information Extraction Supported Question Answer</title>
		<author>
			<persName coords=""><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,443.89,624.48,61.38,9.02;5,90.00,640.08,126.77,9.02">the Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,104.50,655.68,400.80,9.02;5,90.00,671.28,306.15,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,90.00,671.28,69.34,9.02">AT&amp;T at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,177.92,671.28,189.51,9.02">the Eighth Text REtrieval Conference (TREC 8)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,104.14,686.88,403.66,9.02;5,90.00,702.48,415.34,9.02;5,90.00,718.08,159.81,9.02" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finley</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adriana</forename><surname>Badulescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Orest</forename><surname>Bolohan</surname></persName>
		</author>
		<title level="m" coord="5,260.05,702.48,245.29,9.02;5,90.00,718.08,86.69,9.02">LCC Tools for Question Answering, In the Eleventh Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,104.83,733.68,400.38,9.02;5,90.00,749.28,178.65,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,252.37,733.68,183.65,9.02">ICT Experiments in TREC-11 QA Main Task</title>
		<author>
			<persName coords=""><forename type="first">Hongbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuo</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,454.92,733.68,50.28,9.02;5,90.00,749.28,105.54,9.02">the Eleventh Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
