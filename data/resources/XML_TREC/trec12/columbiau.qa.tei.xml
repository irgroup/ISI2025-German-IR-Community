<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.72,81.04,349.64,12.91">A Hybrid Approach for QA Track Definitional Questions</title>
				<funder ref="#_MkUBYAF">
					<orgName type="full">ARDA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,110.88,129.23,123.43,10.76"><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
							<email>sashabg@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.99,129.23,116.21,10.76"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,383.77,129.23,132.07,10.76"><forename type="first">Andrew</forename><surname>Hazen Schlaikjer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.72,81.04,349.64,12.91">A Hybrid Approach for QA Track Definitional Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C8C34E4165705E600BDA976B3BEA2863</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an overview of DefScriber, a system developed at Columbia University that combines knowledge-based and statistical methods to answer definitional questions of the form, "What is X?" We discuss how DefScriber was applied to the definition questions in the TREC 2003 QA track main task. We conclude with an analysis of our system's results on the definition questions. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the QA systems in TREC have reached a remarkably high level of performance <ref type="bibr" coords="1,72.00,499.55,74.73,9.82" target="#b16">(Voorhees, 2002)</ref>. Until this year, however, the task has focused on the short-answer, or factoid model, in which the goal is to answer questions for which the correct response is a number, short phrase, or sentence fragment. In this paper, we focus on the newly introduced definitional question type and present the results of a system which we have built to answer such questions.</p><p>Why build a system that is specific to definitional questions? Consider a student asked to prepare a report on the Hajj, an Islamic religious duty. In the context of short-answer QA, both patience and prescience will be required to elicit the core facts. First, a relatively long list of sub-questions would be required (e.g., "Where is the Hajj carried out?", "How long does it last?", "Who undertakes a Hajj?" etc.). Second, knowing which questions to ask requires knowledge that the questioner likely does not have. That is, the questions that best elicit a description of one thing (e.g., the Hajj) can be quite different from those best suited for finding out about something else (e.g., the Caspian Sea).</p><p>Instead, it is useful to have a sytem which can answer "What is X?" questions directly, presenting a comprehensive response which effectively combines the answers to the relevant sub-questions. This capability is a valuable complement to static knowledge sources like encyclopedias, especially in answering questions about an "X" whose meaning may be evolving, or in creating custom answers that focus on particular aspects of a definition.</p><p>The remainder of this paper presents DefScriber, a definitional QA system implemented at Columbia University. We first present a brief overview of the system's architecture and previous evaluations (more detail on these topics has been reported previously <ref type="bibr" coords="1,320.40,546.23,138.75,9.82" target="#b1">(Blair-Goldensohn et al., 2003)</ref>). We then focus on our performance on the 50 definitional questions included in this year's TREC QA main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture Overview</head><p>Figure <ref type="figure" coords="1,352.72,623.87,5.45,9.82" target="#fig_0">1</ref> gives a high-level view of DefScriber's operation, illustrating input and output of each stage. This example traces an actual answer generated for the question "What is the Hajj?"</p><p>The input is specified as a question, which feeds into the document retrieval phase. The user may specify which databases to search, a maximum number of documents to retrieve, and the desired answer length. 9 Genus-Species Sentences (from within the 383 NSD sents)</p><p>The Hajj, or pilgrimage to Makkah (Mecca), is the central duty of Islam. The Hajj is a milestone event in a Muslim 's life. The hajj is one of five pillars that make up the foundation of Islam. The Hajj is a week-long pilgrimage that begins in the 12 th month of the Islamic lunar calendar. Hajj is the highest of all Muslim practices, even if less than 10% of all Muslims ever manage to perform it . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="383">Nonspecific Definitional sentences</head><p>The Hajj, or pilgrimage to Makkah <ref type="bibr" coords="2,476.97,74.93,23.19,5.41">[Mecca]</ref>, is the central duty of Islam. More than two million Muslims are expected to take the Hajj this year. Muslims must perform the hajj at least once in their lifetime if physically and financially able. The Hajj is a milestone event in a Muslim's life. The annual hajj begins in the twelfth month of the Islamic year (which is lunar, not solar, so that hajj and Ramada-n fall sometimes in summer, sometimes in winter). The Hajj is a week-long pilgrimage that begins in the 12th month of the Islamic lunar calendar. Another ceremony, which was not connected with the rites of the Ka'ba before the rise of Islam, is the Hajj, the annual pilgrimage to 'Arafat, about two miles east of Mecca, toward Mina. The hajj is one of five pillars that make up the foundation of Islam. Not only was the kissing of this stone incorporated into Islam, but the whole form of the Hajj Pilgrimage today is fundamentally that of the Arabs before Islam. Rana Mikati of Rochester will make a pilgrimage, or Hajj, to the holy site of Mecca next week.  Currently, DefScriber is able to search the Internet via Google, as well as the TREC-11 and CNS<ref type="foot" coords="2,267.60,276.92,3.99,7.17" target="#foot_1">2</ref> collections, which are indexed locally.</p><p>The information retrieval (IR) module uses a fixed set of patterns to identify the term to be defined in the question, and then generates a set of search queries. These queries are sent to the selected search engine in order of decreasing expected precision until a threshold number of documents has been retrieved or the set of queries has been exhausted.</p><p>Once documents are retrieved, the primary goaldriven step is performed, with the system examining documents for instances of definitional predicates. Next, a data-driven analysis produces sentence clustering and ordering information. In the last step, a definitional answer is created via sentence extraction, guided by the results of the goal-and data-driven stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definitional Predicates: A Goal-Driven Approach</head><p>Answering a "What is X?" definitional question and creating a summary of query results for the search term "X" are strongly related problems. Yet, as readers, we have more specific expectations for a definition than for a general-use summary. The idea of definitional predicates is to model these special properties of a definition so the system can use them to create better answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Predicate Set</head><p>Our set of definitional predicates is shown in Table <ref type="table" coords="2,320.40,297.23,4.09,9.82">1</ref>. Currently, the system automatically identifies instances of three of these types in text: Genus, Species and Non-specific Definitional (NSD). Research on identifying Target Partition and History instances is ongoing.</p><p>An important distinction is that NSD subsumes all of the other more specific predicate types that appear underneath it in Table <ref type="table" coords="2,421.34,392.51,4.09,9.82">1</ref>. Thus, identifying NSD text is crucial because it is a cue to the presence of other predicates; it also removes noise and provides a set of useful definitional text which is given as input to datadriven methods even when the text cannot be further classified with a more specific predicate. We chose Genus and Species as the first specific predicates to implement because they are at the core of what definitions are; Related work <ref type="bibr" coords="2,428.36,500.87,127.21,9.82" target="#b12">(Sager and L'Homme, 1994;</ref><ref type="bibr" coords="2,320.40,514.43,60.15,9.82" target="#b15">Swartz, 1997;</ref><ref type="bibr" coords="2,383.19,514.43,117.89,9.82" target="#b13">Sarner and Carberry, 1988)</ref> consistently identifies these two concepts as key parts of defining a term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Predicate Identification</head><p>To use these predicates in our system, we must identify units of text which contain them. To do this, we first did a manual examination of documents to create sample data annotated with predicates. Using this data, we explored two approaches to identifying predicates. The first uses machine learning to learn a feature-based classifier that predicts when a predicate occurs. The second uses pattern-recognition over patterns extracted from the annotated data.</p><p>We used the machine learning approach to automatically identify NSD sentences. Using a set of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicate Description</head><p>Instance Example Non-specific Definitional Any type of information relevant in a detailed definition of the term. NSD are a superset of the below predicates.</p><p>Costs: Pilgrims pay substantial tariffs to the occupiers of Makkah and the rulers of...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Genus</head><p>Category to which term belongs.</p><p>The hajj is a type of ritual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Species</head><p>Describes properties other than or in addition to Genus. Species are a superset of the below predicates.</p><p>The annual hajj begins in the twelfth month of the Islamic year. Target Partition Divides the term into two or more conceptual or physical parts. Qiran, Tammatu' and Ifrad are three different types of Hajj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cause (effect)</head><p>States explicitly that the term is the cause (effect) of something.</p><p>The pilgrimage causes the past sins of a Muslim to be forgiven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>History</head><p>Gives historical information relating to the term. Mohammed, founder of Islam, started the tradition in 632 C.E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Etymology</head><p>Information on the term's genesis, e.g. adaptation from another language.</p><p>In Arabic, the word Hajj means a resolve of magnificent duty.</p><p>Table <ref type="table" coords="3,207.96,233.99,4.24,9.82">1</ref>: Definitional Predicates: Descriptions and Examples surface features such as sentence position (relative and absolute in a document) "term concentration" (i.e. the term's frequency within a sentence and/or nearby sentences), we applied two machine learning tools: the rule-learning tool Ripper <ref type="bibr" coords="3,226.52,321.47,63.05,9.82" target="#b2">(Cohen, 1995)</ref> and the boosting-based categorization system BoosTexter <ref type="bibr" coords="3,72.00,348.47,126.73,9.82" target="#b14">(Schapire and Singer, 2000)</ref>. Both algorithms performed similarly in terms of the accuracy of their predictions on test data; Ripper's rules are used in Def-Scriber since they were somewhat simpler to implement. Using cross-validation, accuracy of 81 percent was obtained with Ripper (76 percent using BoosTexter).</p><p>In order to identify Genus and Species predicates, we manually extracted a set of lexicosyntactic patterns to model sentences containing both Genus and Species (G-S) information, as these G-S sentences provide a strong grounding context for understanding the term. Rather than modeling the patterns at the word level, i.e. as flat templates with slots to fill, we model them as partially specified syntax trees (Figure <ref type="figure" coords="3,107.71,554.51,3.94,9.82" target="#fig_1">2</ref>). One such pattern can match a large class of syntactically similar sentences without having to model every type of possible lexical variation. This approach derives from techniques used in information extraction <ref type="bibr" coords="3,120.30,608.63,77.99,9.82" target="#b4">(Grishman, 1997)</ref>, where partial subtrees for matching domain-specific concepts and named entities are used because automatic derivation of full parse trees is not always reliable. However, datadriven techniques (Section 5) offer additional protection from false or extraneous matches by lowering the importance ranking of information not corroborated elsewhere in the data.</p><p>Figure <ref type="figure" coords="3,115.00,719.75,5.45,9.82" target="#fig_1">2</ref> illustrates the transformation from exam-ple sentence to pattern, and then shows a matching sentence. Our patterns are flexible -note that the example and matched sentences have somewhat different trees. Another point of flexibility is the verb itself; FormativeVb will match verbs in a set which our algorithm considers expressive of "belonging" to a category (e.g., "be," "represent," "exemplify").</p><p>Using our predicate-annotated data set, we have manually extracted 23 distinct patterns which match G-S sentences. Although it is difficult to reliably measure recall of the patterns without a larger set of annotated documents, precision in previous evaluations was approximately 96 perecent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data-Driven Techniques: Applying Summarization</head><p>While our set of predicates, including Genus and Species, are domain-neutral, they are not meant to model all possible important information for a given term definition. Some information types may be hard to define computationally a priori. Also, a given sentence may instantiate a definitional predicate but include only peripheral content. We address these issues in the data-driven stage of DefScriber's pipeline (Figure <ref type="figure" coords="3,338.89,597.71,3.94,9.82" target="#fig_0">1</ref>), applying statistical techniques adapted from multi-document summarization to the Non-specific Definitional sentences identified in the goal-driven stage.</p><p>First, a definition centroid is computed by creating a stemmed-word vector of all the NSD sentences. Then the individual sentences are sorted in order of decreasing "centrality," as approximated by IDF-weighted cosine distance from the definition centroid. This method creates a definition of length N by taking the first N unique sentences out of this sorted order, and serves as the TopN baseline method in our evaluation. Note that this method approximates centroidbased summarization, a competitive summarization technique <ref type="bibr" coords="4,117.53,303.23,84.55,9.82">(Radev et al., 2000)</ref>.</p><p>After ordering sentences with TopN, we perform a non-hierarchical clustering that we use to decrease redundancy by avoiding same-cluster sentences in the answer. Since our clustering similarity measure uses IDF computed over a large collection, it can suffer from overweighting of specialized terms; to account for this, we augment the cosine distance calculation, using local IDF values calculated dynamically from the pool of NSD sentences.</p><p>The final data-driven technique improves cohesion by considering the content of the previous answer sentence when choosing a sentence to add to the answer. After choosing the first sentence as in TopN, we choose each remaining sentence as follows: we define the goodness of a cluster as an equal weighted combination of (1) its cohesion to the previous sentence and (2) its overall importance, approximated by that cluster's centroid's distance from (1) the previously chosen sentence's cluster's centroid and (2) the centroid of all NSD sentences. We also add in a penalty for clusters from which sentences have already been chosen such that no cluster gets n sentences included in the answer before all clusters have n-1 included sentence. Once the "best" next cluster has been chosen in this manner, we add the next sentence to the definition as the top-ranked sentence in that cluster which has not yet been included in the definition. DefScriber's default configuration integrates all the above data-driven techniques (TopN, clustering, local IDF weighting, and cohesion ordering), combin-ing them with the goal-driven method of G-S predicate identification. We place the top-ranking (in terms of TopN) G-S sentence first in the definition, and use the cohesion-based ordering to add the remaining sentences. We call this integrated goal-and data-driven method DefScriber.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Goal-driven, or top-down, approaches are more often found in generation. Schemas <ref type="bibr" coords="4,472.39,380.15,78.61,9.82" target="#b8">(McKeown, 1985)</ref>, rhetorical structure theory <ref type="bibr" coords="4,448.41,393.71,107.55,9.82;4,320.40,407.27,25.09,9.82" target="#b6">(Mann and Thompson, 1988;</ref><ref type="bibr" coords="4,350.53,407.27,107.91,9.82" target="#b9">Moore and Paris, 1992;</ref><ref type="bibr" coords="4,463.48,407.27,55.90,9.82" target="#b5">Hovy, 1993;</ref><ref type="bibr" coords="4,524.42,407.27,31.20,9.82;4,320.40,420.83,25.69,9.82" target="#b7">Marcu, 1997)</ref> and plan-based approaches <ref type="bibr" coords="4,477.37,420.83,78.73,9.82;4,320.40,434.39,25.69,9.82" target="#b11">(Reiter and Dale, 2000)</ref> are examples of goal-driven approaches, where the schema or plan specifies the kind of information to include in a generated text. In early work, schemas were used to generate definitions <ref type="bibr" coords="4,471.70,474.95,79.33,9.82" target="#b8">(McKeown, 1985)</ref>, but the information for the definitional text was found in a knowledge base. In more recent work, information extraction is used to create a top-down approach to summarization <ref type="bibr" coords="4,403.69,529.19,130.98,9.82" target="#b10">(Radev and McKeown, 1998</ref>) by searching for specific types of information which can be extracted from the input texts (e.g., perpetrator in a news article on terrorism). Here, the summary briefs the user on domain-specific information assumed a priori to be of interest.</p><p>Other long-answer QA systems are currently under development as part of the AQUAINT program <ref type="bibr" coords="4,320.40,638.51,73.53,9.82" target="#b17">(Voorhees, 2003)</ref>. Some of these share attributes with DefScriber; Weischedel et al. <ref type="bibr" coords="4,461.73,651.95,55.73,9.82">(ARD, 2003)</ref> explore definitional and biographical questions, using a combination of methods that are largely complementary to those used in DefScriber, namely identification of key linguistic constructions and information extraction (IE) to identify specific types of semantic data.</p><p>Another important contrast between DefScriber and most of the long-answer systems developed under the AQUAINT program has to do with answer format. While these systems mostly produce answers as a ranked list of descriptive phrases or sentences, Def-Scriber uses summarization methods to produce a coherent, multi-sentence, encyclopedia-style definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Previous Evaluations</head><p>An evaluation of DefScriber performed previously used human judgments to measure the performance of DefScriber's definitions over a set of 24 terms from a varied set of domains. We measured five qualities of the definitions: relevance (precision), redundancy, structure, breadth of coverage, and term understanding. Overall, we found that DefScriber achieved the best scores in structure, redundancy, term understanding, and relevance, with statistically significant margins in the first two categories. In coverage, Def-Scriber performed below the baselines, but not at a statistically significant level. The results are reported in detail elsewhere <ref type="bibr" coords="5,156.86,363.71,135.34,9.82" target="#b1">(Blair-Goldensohn et al., 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Modifications for TREC 2003</head><p>Although DefScriber is built specifically to answer definitional questions, several modifications and optimizations were performed before running the system for the definitional questions in the TREC QA task.</p><p>First, the data source to use needed to be fixed: usually, a user of DefScriber's web interface would specify whether to query Internet documents or local collections. For the TREC question set, we hard-coded this value so that only the TREC-11 data sets were searched.</p><p>Secondly, we needed to reconsider our metric for a "good" answer in light of the announced scoring formula. Since each answer for a definition question was to be considered on the basis of its intrinsic information content alone, the statistical cohesion measures described in Section 4 were disabled. Clustering was still used to avoid redundancy, since redundant information nuggets would receive a zero score. But answer sentences were picked from the clusters in a purely importance-based order (as approximated by our TopN ordering), without regard to cohesion.</p><p>Another point of modification was the issue of handling "Who is X?" as opposed to "What is X?" questions, since both types were included in the TREC def-inition question set. Although DefScriber has been designed primarily to provide definitions of objects and concepts<ref type="foot" coords="5,377.76,100.64,3.99,7.17" target="#foot_2">3</ref> , its design allows "Who" questions to be processed easily as well. In fact, a look at the predicates in Table <ref type="table" coords="5,390.09,129.71,5.45,9.82">1</ref> reveals that they can be applied to people, for instance we can and do identify the sentence, "John Glenn was the first astronaut." as a G-S sentence even though its subject is a person. The single difference in DefScriber's processing of "Who" as opposed to "What" questions is that sentences which include certain personal pronouns like "he" or "she" do not have their score reduced as they would for a "What" question.</p><p>Lastly, we needed to decide how many answer sentences to include for each definitional answer. Our current system takes this number as a user-specified parameter, but in this case we needed the system to try to determine an optimal value. Our approach was to use the training data provided by the AQUAINT pilot study <ref type="bibr" coords="5,372.57,332.99,75.09,9.82" target="#b17">(Voorhees, 2003)</ref>, and to optimize a linear combination of a base answer length and an adjustment factor based on the number of relevant documents (i.e. containing one or more NSD sentence) found for a particular answer. We did this by using the assessor nuggets for the 25 pilot definition questions, and calculating what our score would have been if our answer length were determined as a linear function of the number of relevant documents found. We approximated this optimum as: max base,f actor∈1..20 avg q∈1..25 (F (q, base + docs(q)/f actor))</p><p>Where F (q, n) is the TREC F-measure score for Def-Scriber's n-sentence answer on pilot question q, and docs(q) is the number of relevant documents found for question q. The optimum was found at base = 9, f actor = 16. Therefore, the final modification of our system for the TREC task was to set it to produce (9 + docs(q)/16)-sentence answers for each definitional question q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Performance on TREC 2003 Definition Questions</head><p>As mentioned previously, our system is designed specifically to answer definitional questions and as two time world cahmpion Italian Alberto Tomba, three-time Olympic and two-time world champion, came back to win the last World Cup slalom in Schladming, Austria, on Thursday before the world Alpine championships.</p><p>Table <ref type="table" coords="6,99.60,229.79,4.24,9.82">2</ref>: Sentences from DefScriber's output that were judged as non-matching alongside possibly matching nuggets. These non-matches demonstrate the ambiguous nature the judgement matching proces; the first two sentences are arguable matches but require significant inference on the judge's part; the latter two seem to be clearer matches. such was run only for the definition questions in the main QA track. Thus we will focus on our results for these questions. Overall, our system performed well above the median for these questions, achieving an average F score of .338 compared with the median of .192. Examining the evaluation results further, we made a number of observations about the evaluation in general and our performance in particular.</p><p>An initial study of evaluation results showed that some data nuggets present in our response sets were not counted by judges, resulting in degraded recall scores. These judgements may be due in part to the need for higher level inference over response sentences and nuggets to see their connections. The issue of whether such inference is appropriate may have been a source of considerable noise in the evaluation. Table <ref type="table" coords="6,99.12,539.63,5.45,9.82">2</ref> gives several examples of response sentences from our output which were not scored as containing an official nugget, alongside the nugget which they might arguably have been matched with, indicating potential judgment errors. These examples are meant to show the gradient of answer-matching ambiguity from more to less ambiguous. That is, the first two answers would seem to require some level of inference on the part of the judge to be certified a match; the latter two seem more clearly to include the desired nugget.</p><p>A strongly related issue, particularly for a system like DefScriber which produces a multi-sentence answer meant to be read as a whole, is the issue of many-to-one matches. That is, should judges count a nugget as "matching" when its information is not contained in a single answer sentence, but rather in the sum of information provided by several answer sentences? DefScriber also encountered difficulty with certain questions because of its reliance (at the time of the evaluation) the MG search engine, which lacks support for phrasal queries. Lack of phrasal search resulted in low precision, coupled with limitations on the number of documents processed resulted in low recall. This problem became more pronounced in cases where one or more words in term/person to be defined was common, resulting in a large set of documents being returned from MG, which does a boolean OR across all query words. Due to speed limitations, our system truncated such large results sets at a fixed size, and thus found only a subset of the documents which actually contained the term words in a phrase. This resulted in problems, for instance, on questions of the type "Who is X?", where X had a very common first and/or last name (e.g. "Al Sharpton", and "Andrea Bocelli"). Subsequently, updates have been made to DefScriber's IR module so that it now fully supports phrasal search capabilities on locally indexed corpora via the Lucene search engine.</p><p>For questions where this was not an issue, Def-Scriber's sentence selection criteria seem to have performed well, both goal-and data-driven. In some of our higher scoring answers, we see an impact from our goal-driven strategy via the identification of G-S sentences; for instance, we can see in Figure <ref type="figure" coords="6,507.54,719.75,5.45,9.82" target="#fig_2">3</ref> that Def- Scriber achieves the highest score on question 1987, "What is ETA in Spain?", which appears to be one of the harder questions in that its median score was zero. Our high score on this question is in part due to finding and including the G-S sentence, "ETA is an acronym for Basque Homeland and Freedom in the Basque language.", which contains one of the "vital"information nuggets for this question, i.e. the information on what ETA stands for.</p><p>While G-S sentences are clearly helpful, we were also successful when G-S sentences were not found. In these cases, we rely on our robust data-driven methods to statistically guide answer content. These methods allowed us to select high-scoring sentences even when G-S sentences were not found. Such instances included our best and near-best scores on questions 2060 ("Who is Alberto Ghiorso?"), 2082 ("Who was Anthony Blunt?"), 2125 ("Who was Charles Lindbergh?") and 2201 ("What is Bollywood?").</p><p>However, even when IR returned relevant documents, we did see degradation in system performance where high numbers of response sentences were returned. We believe this is due to the precision penalties our system suffered by using the adjustable answer-length threshold explained in Section 7. Since this threshold creates a longer answer when more relevant sentences are found, our lower scores in these cases suggest that the penalties we incurred in precision did not make up for whatever additional recall nuggets we achieved by having longer answers; it would be interesting to see if the answer-length optimization described in the Section 7 would arrive at a smaller length function given the new data from this evaluation.</p><p>As suggested by the zero median F-measure of question 2024 "Who is Andrea Bocceli", few participants in the evaluation have incorporated fuzzy search capabilities to overcome spelling errors in input questions (the singer's name is correctly spelled "Bocelli"). From an IR prespective, this represents a very important advance that most systems should make in order to function adequately with noisy data from source materials and/or search inputs.</p><p>For future evaluations where nuggets of information are to be identified by human judges, it may be useful to perform some error analysis of adjudications made this year. Given the subjective nature of the task, attaining a "perfect" scoring is of course impossible. But an analysis of the kinds of errors or issues seen will be important as we seek to refine the design of the definition question task and the judgement process itself.</p><p>Future work on DefScriber will concentrate on increasing the number of definitional predicates automatically identified by the system, as well as on improving identification performance on such predicates.</p><p>We are currently working to improve our featurebased predicate identification methods by growing our annotated data set while also extracting more and richer features to input into our machine learning methods. To improve the pattern-based methods, we are actively working with IE bootstrapping techniques developed in Snowball (Agichtein and <ref type="bibr" coords="8,287.43,246.11,14.99,9.82;8,72.00,259.67,53.11,9.82">Gravano, 2000)</ref> to automatically learn predicate patterns from manually extracted "seed" examples. Such techniques would allow us to supplement our manuallygenerated patterns and bring new predicates online more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We have presented an overview of DefScriber, a hybrid goal-driven and data-driven system for definitional questions. We explained how the system was modified and applied to answer definitional questions in the TREC 2003 QA track. Finally, we presented DefScriber's results on the definitional questions, which were significantly above median, achieving an average F-score of .338 compared with the median of .192. Finally, we analyzed our scores on certain individual questions, discussing areas where our system performed well and others where it could be improved, as well as noting several issues of the judgement process itself.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,204.60,232.07,218.28,9.82"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: DefScriber answers "What is the Hajj?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,97.08,215.75,435.09,9.82"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pattern extraction and matching for a Genus-Species sentence from an example sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,109.32,344.27,410.46,9.82"><head>FFigure 3 :</head><label>3</label><figDesc>Figure 3: Overlay of evaluation-wide F-measure scores per question with those of DefScriber</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,88.08,711.12,218.61,8.07;1,72.00,721.08,36.12,8.07"><p>We did not produce answers to the other, non-definitional, questions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,88.08,701.16,218.53,8.07;2,72.00,711.12,233.86,8.07;2,72.00,721.08,135.78,8.07"><p>A collection of documents from the Center for Nonproliferation Studies (http://cns.miis.edu) made available to participants in the AQUAINT project.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,336.48,691.20,218.34,8.07;5,320.40,701.16,234.52,8.07;5,320.40,711.12,234.54,8.07;5,320.40,721.08,78.27,8.07"><p>This is in part because a separate, complementary system with greater focus on properties specific to describing individuals, i.e. biographies, is under development at Columbia<ref type="bibr" coords="5,523.56,711.12,31.37,8.07;5,320.40,721.08,78.27,8.07" target="#b3">(Duboue and McKeown, 2003)</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors wish to acknowledge the support of <rs type="funder">ARDA</rs> through AQUAINT contract <rs type="grantNumber">MDA908-02-C-0008</rs>. We are also thankful for the generous and thoughtful contributions of colleagues at <rs type="institution">Columbia University</rs> and at the <rs type="institution">University of Colorado-Boulder</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MkUBYAF">
					<idno type="grant-number">MDA908-02-C-0008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.33,304.02,430.30,10.31;7,306.13,320.12,39.32,7.55;7,80.61,179.63,7.55,40.71;7,407.19,82.82,87.09,4.94;8,72.00,658.43,55.76,10.76;8,72.00,677.80,233.24,8.97;8,82.92,688.84,135.29,8.97" xml:id="b0">
	<monogr>
		<title level="m" coord="7,306.13,320.12,39.32,7.55;7,80.61,179.63,7.55,40.71;7,407.19,82.82,87.09,4.94;8,72.00,658.43,55.76,10.76;8,72.00,677.80,233.24,8.97;8,82.92,688.84,65.56,8.97">Questions F-measure Best DefScriber Median References ARDA and NIST. 2003. AQUAINT R&amp;D Program 18 Month Workshop</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1901">1901 1905 1907 1917 1933 1955 1957 1964 1972 1987 2006 2008 2024 2041 2042 2060 2082 2095 2112 2125 2130 2146 2148 2150 2158 2174 2177 2201 2203 2208 2222 2224 2229 2234 2258 2267 2274 2304 2321 2322 2324 2327 2332 2348 2349 2366 2369 2372 2373 2385</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,709.48,233.09,8.97;8,82.92,720.40,221.90,8.97;8,331.32,76.12,222.12,8.97;8,331.32,87.16,59.81,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,225.62,720.40,79.20,8.97;8,331.32,76.12,143.31,8.97">A hybrid approach for answering definitional questions</title>
		<author>
			<persName coords=""><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Schlaikjer</forename></persName>
		</author>
		<idno>CUCS-006-03</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,320.40,107.08,233.15,8.97;8,331.32,118.00,221.94,8.97;8,331.32,128.92,36.88,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,436.51,107.08,113.05,8.97">Fast effective rule induction</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,342.02,118.00,181.72,8.97">Proc. of 12th Int&apos;l Conf. on Machine Learning</title>
		<meeting>of 12th Int&apos;l Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,148.84,232.82,8.97;8,331.32,159.88,222.27,8.97;8,331.32,170.80,153.96,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,537.17,148.84,16.05,8.97;8,331.32,159.88,222.27,8.97;8,331.32,170.80,78.40,8.97">Statistical acquisition of content selection rules for natural language generation</title>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,427.93,170.80,32.66,8.97">EMNLP</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,190.72,232.75,8.97;8,331.32,201.64,181.84,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,430.48,190.72,122.67,8.97;8,331.32,201.64,85.35,8.97">Information extraction: Techniques and challenges</title>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,435.00,201.64,18.82,8.97">SCIE</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="10" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,221.56,232.55,8.97;8,331.32,232.60,222.23,8.97;8,331.32,243.52,102.71,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,432.90,221.56,120.05,8.97;8,331.32,232.60,153.86,8.97">Automated discourse generation using discourse structure relations</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,493.66,232.60,59.89,8.97;8,331.32,243.52,27.25,8.97">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="341" to="385" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,263.44,232.85,8.97;8,331.32,274.36,221.89,8.97;8,331.32,285.40,132.51,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,523.06,263.44,30.20,8.97;8,331.32,274.36,221.89,8.97;8,331.32,285.40,47.72,8.97">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">C</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,386.61,285.40,14.76,8.97">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,305.32,232.88,8.97;8,331.32,316.24,217.40,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,416.47,305.32,136.81,8.97;8,331.32,316.24,43.73,8.97">Rhetorical parsing of natural language texts</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.46,316.24,81.61,8.97">Proc. ACL-EACL 97</title>
		<meeting>ACL-EACL 97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,336.16,233.18,8.97;8,331.32,347.20,75.11,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,455.74,336.16,63.11,8.97">Text Generation</title>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Cambridge Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,367.12,232.85,8.97;8,331.32,378.04,221.97,8.97;8,331.32,388.96,203.70,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,518.06,367.12,35.19,8.97;8,331.32,378.04,221.97,8.97;8,331.32,388.96,85.64,8.97">Planning text for advisory dialogues: Capturing intentional and rhetorical information</title>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecile</forename><forename type="middle">L</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,424.43,388.96,42.93,8.97">Comp Ling</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="651" to="695" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,408.88,232.89,8.97;8,331.32,419.92,222.01,8.97;8,331.32,430.84,176.82,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,331.32,419.92,222.01,8.97;8,331.32,430.84,58.83,8.97">Generating natural language summaries from multiple on-line sources</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,397.55,430.84,42.93,8.97">Comp Ling</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="500" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,450.76,232.94,8.97;8,331.32,461.68,220.46,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,442.52,450.76,110.82,8.97;8,331.32,461.68,119.31,8.97">Building Natural Language Generation Systems, chapter 4</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,481.60,232.88,8.97;8,331.32,492.64,208.70,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,502.48,481.60,50.80,8.97;8,331.32,492.64,84.16,8.97">A model for definition of concepts</title>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>L'homme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,423.20,492.64,47.97,8.97">Terminology</title>
		<imprint>
			<biblScope unit="page" from="351" to="374" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,512.56,232.83,8.97;8,331.32,523.48,221.95,8.97;8,331.32,534.40,222.01,8.97;8,331.32,545.44,116.98,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,524.76,512.56,28.47,8.97;8,331.32,523.48,221.95,8.97;8,331.32,534.40,24.23,8.97">A new strategy for providing definitions in task oriented dialogues</title>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Sarner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandra</forename><surname>Carberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,375.92,534.40,177.41,8.97;8,331.32,545.44,70.17,8.97">Proc. Int&apos;l Conf. on Computational Linguistics (COLING-88)</title>
		<meeting>Int&apos;l Conf. on Computational Linguistics (COLING-88)</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,565.36,232.70,8.97;8,331.32,576.28,222.12,8.97;8,331.32,587.20,133.78,8.97" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="8,504.93,565.36,48.18,8.97;8,331.32,576.28,194.18,8.97">BoosTexter: A boosting-based system for text categorization</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Machine Learning</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="135" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,607.12,73.60,8.97;8,424.96,607.12,22.42,8.97;8,478.34,607.12,75.35,8.97;8,331.32,618.16,113.25,8.97;8,479.01,618.16,74.68,8.97;8,331.32,629.08,170.58,8.97" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,478.34,607.12,75.35,8.97;8,331.32,618.16,108.76,8.97">Definitions, dictionaries and meanings</title>
		<author>
			<persName coords=""><forename type="first">Norman</forename><surname>Swartz</surname></persName>
		</author>
		<ptr target="http://www.sfu.ca/philosophy/definitn.htm" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,649.00,232.71,8.97;8,331.32,659.92,222.02,8.97;8,331.32,670.96,101.87,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,420.24,649.00,132.87,8.97;8,331.32,659.92,78.72,8.97">Overview of the trec 2002 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,427.47,659.92,101.89,8.97">Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg, MD. NIST</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,320.40,690.88,232.91,8.97;8,331.32,701.80,221.15,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,412.91,690.88,124.36,8.97">Answers to definition questions</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,331.32,701.80,71.29,8.97">HLT-NAACL 2003</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="109" to="111" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
