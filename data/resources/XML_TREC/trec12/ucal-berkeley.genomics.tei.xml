<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.15,95.69,249.69,15.15;1,184.77,117.61,242.46,15.15">BioText Team Report for the TREC 2003 Genomics Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.16,150.11,64.59,10.52;1,218.75,148.53,1.88,6.99"><forename type="first">G</forename><surname>Bhalotia</surname></persName>
							<email>bhalotia@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.22,150.11,55.91,10.52;1,287.14,148.53,1.88,6.99"><forename type="first">P</forename><forename type="middle">I</forename><surname>Nakov</surname></persName>
							<email>nakov@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.62,150.11,74.87,10.52;1,374.48,148.53,1.88,6.99"><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.97,150.11,66.62,10.52;1,453.58,148.53,1.88,6.99"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
							<email>hearst@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.70,162.59,1.88,6.99;1,239.08,164.21,51.38,10.48"><forename type="first">â€ </forename><surname>Computer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.36,164.21,82.81,10.48;1,179.60,176.65,1.88,6.99"><forename type="first">Science</forename><surname>Division</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.15,95.69,249.69,15.15;1,184.77,117.61,242.46,15.15">BioText Team Report for the TREC 2003 Genomics Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01D63F513A66D6A6F8E7D0D0D244F000</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The BioText project team participated in both tasks of the TREC 2003 genomics track. Key to our approach in the primary task was the use of an organism-name recognition module, a module for recognizing gene name variants, and MeSH descriptors. Text classification improved the results slightly. In the secondary task, the key insight was casting it as a classification problem of choosing between the title and the last sentence of the abstract, although MeSH descriptors helped somewhat in this task as well. These approaches yielded results within the top three groups in both tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The paper reports on the work conducted by the Bio-Text project team at UC Berkeley for the TREC 2003 Genomics track. In 2003 this track consists of two tasks and the document collection consists of 525,938 MEDLINE records dating between 4/1/2002 and 4/1/2003. Task 1 was intended to be similar to standard information retrieval queries and was stated as follows:</p><p>For gene X, find all MEDLINE references that focus on the basic biology of the gene or its protein products from the designated organism. Basic biology includes isolation, structure, genetics and function of genes/proteins in normal and disease states.</p><p>The relevance judgements for this task were drawn from GeneRIF references from the National Library of Medicine's LocusLink database. Participants were allowed to make use of the gene name variation information associated with the GeneRIF.</p><p>The secondary task was intended to require more detailed analysis, in order to allow groups to make use of sophisticated language processing technology that is generally considered to be important for genomics and other bioscience text. However, due to limited resources, a specialized annotated collection was not yet available for this task. Instead, the goal of the secondary task was to reproduce the GeneRIF textual description for a given gene/document pair. Because these descriptions were often extracted verbatim from the document's title or abstract, and systems were judged on how closely the extracted text overlapped with the original, the task was better approached as a classification problem than as a language analysis and generation problem.</p><p>There were some commonalities in our approaches for the two tasks: in both cases we made use of classification algorithms and a special module for recognizing gene name variants and identifying MeSH descriptors in the text. Below approaches to both tasks are described in detail. <ref type="bibr" coords="1,310.98,529.86,8.08,12.62" target="#b0">2</ref> The Primary Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The main challenges in the primary task are to improve recall by finding all appropriate variations of the given gene name, to improve precision by removing documents that describe genes that do not pertain to the target organism, and to demote the ranking of documents that mention the target gene but have not been assigned to a GeneRIF.</p><p>To improve recall, we created a special-purpose algorithm for generating and recognizing gene name variants. Our three-fold approach consisted of normalizing gene names by replacing special characters with spaces, developing a set of expansion rules that generate possible variants of the gene names that are not included in LocusLink, and looking in the citations for MeSH terms that pertain to gene names.</p><p>To improve precision we developed a semiautomated method to convert LocusLink organism names to MeSH organism descriptors, and used these to filter out papers that were not relevant to the target organism.</p><p>We submitted two runs, illustrated in Figure <ref type="figure" coords="2,274.59,164.61,3.88,8.74" target="#fig_0">1</ref>. For the first run, the relevance ranking component consisted of a weighted sum over 5 different sub-queries. For the second run, this score was combined with that of a statistical model that was trained to distinguish documents that are referred to by GeneRIFs from those that are not. We used as our backend retrieval system the IBM DB2 Net Search Extender, which allows convenient combination of relational and fulltext queries.</p><p>In the following subsections we describe the modules for gene name recognition, organism filtering, MeSH term mapping, GeneRIF classification, and the method of combining the scores of the sub-queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MeSH Descriptors</head><p>In a number of places in the paper below we make use of MeSH (Medical Subject Heading)<ref type="foot" coords="2,249.92,377.30,3.97,6.12" target="#foot_0">1</ref> lexical hierarchy.</p><p>In MeSH, each concept is assigned a unique identifier (e.g., Eye is D005123) and one or more alphanumeric tree numbers (corresponding to particular positions in the hierarchy). For example, A (Anatomy), A01 (Body Regions), A01.456 (Head ), A01.456.505 (Face), A01.456.505.420 (Eye). Eye is ambiguous according to MeSH and has a second tree number: A09.371 (A09 represents Sense Organs).</p><p>In addition, each MeSH concept is assigned a semantic type (there are over 200): e.g., Enzyme, Gene or Genome, Mammal, Tissue, Virus, etc.</p><p>In some cases in the work below we use MeSH tree numbers and truncate them at period breaks to generalize across sub-hierarchies of the trees. In other cases we use the unique descriptor or the semantic type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Identifying Variations in Gene Names</head><p>In order to capture the variations in gene names we had to expand the original synonym list from Lo-cusLink since using only the gene synonyms available from LocusLink produced relatively less accurate results.</p><p>We created a semi-automated technique to identify such variations. We analyzed a large set of gene names to try to determine rules for converting a given representation into a canonical form. First we used ngram matching to find candidate sets of similar gene names. Then we inspected the results of this matching to make a set of rules for such conversion (see Table <ref type="table" coords="2,339.75,152.51,3.88,8.74" target="#tab_0">2</ref>). Some of these rules were more accurate and so were assigned more weight than the others. The details appear in the next two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-Gram Overlap</head><p>The first step in identifying patterns of variation is to locate the variant form of the gene name in the article text. We automated this step by using an n-gram overlap measure <ref type="bibr" coords="2,417.78,258.81,9.96,8.74" target="#b1">[3]</ref>. "n-grams" are simply nlong strings of continuous characters in a given document/string. The distribution of n-grams between pairs of strings is compared, and a score is computed that represents the similarity between them. The main idea behind using n-grams is that similar words will have a high proportion of n-grams in common. Typical values for n are 2 or 3 corresponding to the use of digrams or trigrams, respectively.</p><p>To compute the similarity of two strings using this method, we first compute the n-gram sets for the strings being compared and then calculate the overlap using the Dice Coefficient <ref type="bibr" coords="2,428.29,402.58,14.61,8.74" target="#b8">[10]</ref>. The Dice coefficient D for two sets A and B of sizes |A| and |B| is given by</p><formula xml:id="formula_0" coords="2,394.29,437.84,61.22,22.31">D = 2|A âˆ© B| |A| + |B|</formula><p>This overlap measure penalizes the presence of extra characters beyond the ones common to the two strings. Thus two strings with the same amount of overlap get a higher score when the non-overlapping regions are smaller in size. We take the abstract and title of the articles associated with the genes and compute the n-gram overlap of all the possible subsequences of words against all known alias forms of the gene. We use character level digrams and trigrams with Dice Coefficient as the overlap measure. The word sequences in the abstracts/title that have high similarity to one of the known alias forms of the query gene are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inspection and Rule Generation</head><p>The procedure outlined above yields high similarity pairs of strings with one of them corresponding to the known alias form of a gene name and the other to the actual variant form of representation found in article text. We process this list to remove the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Known Alias Name</head><p>Best match variant in text HLA-DQB1 hla-dqb DNA synthesis inhibitor inhibitors of dna synthesis phospholipase C, gamma 1 phospholipase c gamma 1 adrenergic receptor, alpha 1d alpha 1d-adrenergic receptor Janus kinase 2 (a protein tyrosine kinase) protein tyrosine kynase golgi protein, 73-kD golgi protein luteinizing hormone/choriogonadotropin luteinizing hormone-choriogonadotropin (lh/hcg) Table <ref type="table" coords="3,179.21,656.12,3.88,8.74">1</ref>: Some selected overlap pairs for gene names and their variants ones that are exactly identical (note that identical strings will receive the highest similarity coefficient of 1). Next we remove those that lie below a threshold, that is obtained using quick inspection of the list (we used a threshold of 0.5). This yields a set of original forms and their variants. Selected overlap matches are shown in Table <ref type="table" coords="4,157.87,140.55,3.88,8.74">1</ref>.</p><p>We inspect the pairs obtained to identify the patterns of variation in gene names. These patterns are used to generate rules to transform the names to obtain a broader set of alias forms for the gene names. The rules that we generated are shown in Table <ref type="table" coords="4,293.27,200.35,3.88,8.74" target="#tab_0">2</ref>. Such rules are syntactic in nature; sometimes there are variations of semantic nature that cannot be captured this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Organism Filtering</head><p>We filtered out documents that do not correspond to the organism that the query gene belongs to (note that each query in the TREC task consists of a gene name and a corresponding organism). Similar genes with the same name can occur in multiple organisms, e.g., the gene named c-myc which stands for "cellular myelocytomatosis oncogene" can be found in different organisms including humans and chickens. In humans it is located on chromosome 8 and is involved in the pathogenesis of Burkitt's lymphoma. In chickens, c-myc activation by avian leukosis virus appears to result in the development of lymphoid leukosis. Most of the time we are interested in documents that talk about the function of the gene corresponding to a given organism. MEDLINE records do not contain an "organism annotation" for the documents. However this information can be inferred from the MeSH terms assigned to the article. For example, a document that talks about the fruitfly "Drosophila melanogaster" contains the MeSH term Drosophila melanogaster. However the organism names used in LocusLink usually do not match the corresponding MeSH term. For example, the term Human is used in MeSH instead of "Homo sapiens" used by LocusLink.</p><p>We used the combined information in LocusLink and MEDLINE to identify the descriptors used to characterize the organisms for MEDLINE documents. We collected the MEDLINE references (as described before, LocusLink has a set of references to MED-LINE documents relevant to the gene) for documents corresponding to each organism in LocusLink. 2 Each query produced a set of documents corresponding to a LocusLink organism. We then ran a query to compute what the top MeSH terms were for each set of 2 LocusLink contains genes from eight different organisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>documents.</head><p>By looking at the most frequent MeSH descriptors for each of the document classes, we can infer the term that is used to denote the organism in MED-LINE. We also checked if the LocusLink organism is used in MEDLINE name in full or partially for the same one; e.g., the terms Drosophila and Caenorhabditis also appear in the MeSH headings contained in MEDLINE. None of the other organism names appears in their original LocusLink form. The terms that we ultimately use to map the documents in the collection to organisms are shown in Table <ref type="table" coords="4,503.19,200.33,3.88,8.74" target="#tab_1">3</ref>. Some of the documents map to multiple organisms and a few map to none.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Mapping Query Terms to MeSH</head><p>In order to further improve the system's performance, we also retrieved documents using their MeSH annotations. Both MeSH Main Headings and MeSH Supplementary Concepts (Chemical List) map to MeSH concepts. Each MeSH concept has one or more textual synonyms that are called MeSH terms. Given a query term the system retrieves all the documents that are annotated with a MeSH concept that has a MeSH term that exactly matches one of the gene names or one of its original synonyms (not including the expanded forms).</p><p>Adding MeSH mappings to the query helped mainly in ranking the retrieved documents. Documents that are retrieved by using the MeSH mapping in addition to the text search are more likely to be relevant, and therefore are given higher scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">GeneRIF Classification Module</head><p>The goal of the classification module is to estimate the probability that a given document has been assigned to a GeneRIF. Our approach is based on the idea that articles which discuss gene function contain a distinct set of features which can be learned using automated techniques. The resulting models can be used to classify new documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Selection</head><p>We experimented with a number of different feature sets; a comparison of their corresponding classification accuracy is presented in Figure <ref type="figure" coords="4,468.91,635.13,3.88,8.74" target="#fig_1">2</ref>. We compared (a) using MeSH descriptors as complete phrases, (b) using MeSH tree numbers, (c) using words in abstracts with stemming, and (d) using MeSH descriptors combined with tree numbers from levels one and two. The best results are obtained for MeSH descriptors used as complete phrases.  Once the feature vectors are obtained, we train a classifier to build a model that can predict whether a document talks about gene function. We use a Naive Bayes classifier <ref type="bibr" coords="6,143.35,123.07,10.51,8.74" target="#b3">[5,</ref><ref type="bibr" coords="6,158.65,123.07,7.01,8.74" target="#b7">9]</ref>. Its fundamental idea is the assumption that the values of the feature variables F = (F 1 , F 2 , ..., F n ) are conditionally independent given the class variable S. The joint probability is given by the expression:</p><formula xml:id="formula_1" coords="5,137.10,80.21,331.60,81.66">A, B â‡’ A B removal of comma B A rearrangement of tokens A([n]) A [n] A-[n] ï£¼ ï£½ ï£¾ â‡’ A [n] where [n] is the set of numerals A[n] â‡’ A [n] addition of spaces (normalization of numerals) A [n] â‡’ A[n] removal</formula><formula xml:id="formula_2" coords="6,130.51,190.90,170.51,30.32">p(S, F ) = p(S) N i=1 p(F i |S)<label>(1)</label></formula><p>The model parameters are given by the probabilities p(S) and p(F i |S), which are usually estimated from the text by means of maximum likelihood estimates (MLE). The classification of a new concept is determined by the most likely category:</p><formula xml:id="formula_3" coords="6,134.56,300.29,166.46,15.25">S M L = arg max S k p(S k |F )<label>(2)</label></formula><p>Naive Bayes classifiers are among the most successful algorithms for document classification. The Naive Bayes classifier is known to be optimal when attributes are independent given the class, but Domingos et al. <ref type="bibr" coords="6,115.15,373.77,10.52,8.74" target="#b2">[4]</ref> show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes even if the independence assumption is not met.</p><p>The implementation of the Naive Bayes classifier we currently use is part of the open source machine learning package WEKA (Waikato Environment for Knowledge Analysis <ref type="bibr" coords="6,164.88,457.46,10.52,8.74">[1,</ref><ref type="bibr" coords="6,179.85,457.46,12.46,8.74" target="#b9">11]</ref>) from the University of Waikato, New Zealand. They provide excellent Java implementation of Naive Bayes and other machine learning algorithms.</p><p>One model was trained for a set of 50 gene names that are not part of the TREC training or test set. We used the retrieval module to extract the relevant documents for each target gene, and the first 1000 documents of each query as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Document Ranking</head><p>As mentioned above, we used IBM DB2 Universal Database to store MEDLINE documents including the abstracts, titles and other annotations. We have built text indexes on these fields using DB2 Net Search Extender, which is then used to search for documents that contain a given set of terms.</p><p>We retrieve all the documents that match one of the known alias forms of the gene or the variations created using the expansion rules (variant forms are generated for all the aliases). The query against the database is composed of five sub-queries combined with the SQL UNION operator, as follows.</p><p>Let G be the various forms of the gene name as computed by the conversion rules shown in Table <ref type="table" coords="6,535.02,105.17,4.98,8.74" target="#tab_0">2</ref> and let LG be other lower-confidence rules for normalizing the gene names that have a higher rate of false positives. For the first run, the score is computed as follows: Score(R) = the aggregated SUM over the result of the UNION operator GROUP BY document id of: As shown above, the scores of the documents in each sub-query are weighted and then aggregated using the UNION operator and the SUM aggregate function. We experimented with using the MAX aggregate function instead, but the results obtained using the SUM function were substantially better. This is due to the fact that documents that are retrieved in multiple sub-queries get a higher total score, and are in fact more likely to be more relevant to the query. We also experimented with giving higher weights to titles over abstracts, but this did not appear to help. Increasing the weights of specific types of aliases (official terms for example) did not improve the system's performance either.</p><p>For the second run, in order to combine the retrieval scores and the classification scores we normalized the weighted scores for each query into values between 0 and 1 by dividing the score by the highest document score of the query. The combined retrieval-classification score is a weighted sum of the two scores. We used 1 and 0.01 as the weights for the retrieval and the classification scores respectively.</p><p>The retrieval score is a value between 0 and 1 for each document and is obtained in part by combining the frequency of occurrence of the term in the document and the relative size of the retrieved document. The exact details of the scoring function are not available as they are part of the DB2 proprietary system. <ref type="foot" coords="6,343.47,638.45,3.97,6.12" target="#foot_1">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Evaluation</head><p>We submitted two runs for the primary task. The first run uses only the retrieval module, and the second run combines it with the classification module.</p><p>When training our system we noticed that some of the topics in the training set did not have any GeneRIFs associated with them. We therefore removed them from the training topic list. Also, some correct GeneRIFs were not listed in the list of qrels. After fixing these errors our system achieved 0.5028 mean average precision (MAP) on the training set with the retrieval module alone, and 0.5101 with the modules combined. The classification module helps but not dramatically.</p><p>On the test set our system achieved 0.3753 MAP with the retrieval module alone, and 0.3912 MAP using both modules combined. Again, the classification module helps but not markedly. In 12 out of 50 queries the retrieval module alone achieves MAP higher than the combined modules.</p><p>The big gap in performance between the test and training sets suggests that the system parameters might be over-fitting the training set. However, an initial sensitivity analysis of the system performance on the test set, shows that only a minor improvement can be achieved by tuning the parameters to fit the test set. Another explanation for the performance gap might be that the test set is inherently harder than the training set. This hypothesis is in agreement with the analysis presented in <ref type="bibr" coords="7,258.24,430.19,9.97,8.74" target="#b4">[6]</ref>, which shows a high degree of variation in MAP across topics in general, and between the training and test sets in particular.</p><p>Both our runs fell within statistical significance of the top performing group <ref type="bibr" coords="7,185.62,491.66,9.97,8.74" target="#b4">[6]</ref>. In 43 out of 50 queries the MAPs of both runs were higher then the median MAP. Analysis of the 7 remaining queries yields some interesting insights about possible improvements of our system. In 3 out of 7 queries the low MAP was a result of a low recall. This is mainly due to some limitations of the current gene name expansion rules. For example, in query 37 the system retrieved only 36 relevant documents out of the possible 61. This is due to the fact that one of the query terms was peroxisome proliferative activated receptor gamma while in many relevant documents it appears as peroxisome proliferative activated receptor gamma isoform 1, and another query term, PPAR gamma, appears in the text as PPARgamma. Additional low-confidence expansion rules could improved the MAP in these cases without affecting the performance of other queries. In many cases the system retrieved much less than the allowed 1000 documents. In these cases, recall could also be improved by retrieving additional documents with low ranking scores like the ones that were filtered out by the organism-filter, or documents that could be retrieved using single query terms instead of full phrases. Of the other 4 sub-par queries, 1 was due to a small bug in the implementation of one of the high-confidence expansion rules that resulted in the addition of an over-generalized term with a high weight into the query, and the other 3 were due to sub-optimal rankings.</p><p>3 The Secondary Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>For the secondary task, our initial intention was to try a linguistically motivated approach but we soon realized that the data was too noisy due to a lack of clear definition of what a GeneRIF is. Instead, we addressed it as a text classification problem.</p><p>Our investigations showed that most of the time the GeneRIF text was pulled verbatim, or with slight modifications, from the abstract text or title. Most of the time the extract came from the title and in the majority of the remaining cases -from the last sentence of the abstract. Thus we assumed the baseline was always choosing the title since it was quite difficult to find an algorithm that performed better than this.</p><p>After much experimentation, we ended up training a Naive Bayes classifier that, given an abstract text, predicts whether the last sentence or the title is a better candidate for GeneRIF text. Our feature set was limited to verbs, MeSH terms (cut at level 2, e.g. G14.330), genes (a single feature for the frequency of all genes), all weighted by TF.IDF, and the appearance of the target gene (a Boolean feature). For training we focused on the abstracts coming from the 5 target journals that were announced on the genomics track Web site (about 6,500 abstracts in total), split them into 10 sets and performed a stratified 10-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GeneRIF Mapping into the Abstract Text</head><p>Looking at the GeneRIFs we found that most of the time the GeneRIF text was pulled verbatim, or with slight modifications, from the abstract text. To quantify this, we investigated 33,662 MEDLINE abstracts that had a GeneRIF assigned and we tried to find a substring in the text that is most similar to the GeneRIF description. Given a particular abstract, we considered all possible sequences, respecting the We accepted a mapping as successful if the score was above some threshold. When the MUD threshold was set to 80%, a successful mapping for 25,590 of the documents (76.02%) could be obtained directly from the title and/or abstract. For 11,847 (35.17%) of them an acceptable match was a substring of the title (and for 6,620 (19.67%), the whole title was taken verbatim: this is 65.10% of the cases when the mapping was found in the title). In 15,421 documents (45.81%), the best match was inside a sentence from the abstract body, and in 1,678 (4.98%) it was found in both the title and the body. In 6,943 of the cases (20.63%) the best match was found in the last sentence of the abstract (this is 50.52% of the cases when it was found in the abstract body). (Note that there is always further opportunity to truncate some unused part of the sentence and improve the score.)</p><p>Given the fact that for most of the abstracts an acceptable matching was found in the title and that in 65.10% of them the best match was the whole title taken verbatim, an obvious baseline was "pick the title". This resulted in a MUD score of 53.39%.</p><p>The last sentence of an abstract usually summarizes its contents, so it was not surprising that it often contained the best match. The title and the last sentence together account for 73.40% of the matches that pass the threshold. Thus, an algorithm that chooses between them would have the potential to perform better than always choosing the title. We calculated that if we limited the choice to title or last sentence and always selected the one that leads to a higher score (we select the whole last sentence or the whole title), this would result in a MUD score of 66.33%. This is the upper bound for any algorithm that relies on whole sentences and chooses between the title and the last abstract sentence only. In practice this algorithm may not perform better than the "pick the title" baseline since it may choose incorrectly. We calculated that if it always made the wrong choice it would end up with a MUD score of 26.62%.</p><p>We performed similar calculations using as similarity measures Classic Dice (CD), Modified Bigram Dice (MBD), Modified Bigram Dice Phrases (MBDP ) as well as a combination of MBU and MBD. The results were all similar, although sometimes the best choices under the different scores differed. We also performed a more general mapping that allowed the target GeneRIF text description to split into two parts, each of which can be mapped to two different parts of the abstract text. Although this way we found better matches for some of the GeneRIFs, the impact was limited: 77.10% matched as compared to 76.02% for the case when a single string was allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Features</head><p>We experimented with a number of different features, including: words/stems, verbs (the most frequent ones only: e.g. bind, block, inhibit, accept, involve etc.; they are stemmed so nominalized verbs are considered as well: e.g. inhibition), genes, genes freq (frequency: how many gene names are mentioned in the sentence), MeSH unique ID (e.g. D005796 ), MeSH tree number cut at a certain level (level 1: G14, or level 2: G14.330 ), MeSH semantic type, journal, publication date (month and year taken together, e.g. <ref type="bibr" coords="8,388.56,426.01,34.21,8.74">10 2003</ref> ). We used also three Boolean features: target gene (is the target gene mentioned?), is title (is the current sentence the title?), is last sentence (is this the last sentence?). The features were weighted according to the TF.IDF measure (except for the Boolean ones). We also experimented without weighting (i.e. using the raw frequency information) as well as treating all features as Boolean.</p><p>The journal and publication date features were introduced in order to account for possible journal-or time-dependent regularities, but these did not prove useful. The MeSH semantic type features were too general. The words and stems lead to a dramatic increase in the vector space dimensionality, which made training some particular classifiers intractable so we did not use them. The same applies to genes freq and MeSH unique ID.</p><p>The best combination of features was (we will refer to it as the standard feature set below): verbs, genes freq, MeSH tree number (cut at level 2), target gene, is title and is last sentence. The three Boolean features were especially important, while the impact of genes freq was minor. The non-Boolean features were weighted using TF.IDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Choosing the Title vs. the Last Sentence</head><p>For the classification experiments we used the WEKA Machine Learning Software in Java again. We used mainly Naive Bayes with kernels <ref type="bibr" coords="9,216.27,135.18,10.52,8.74" target="#b5">[7]</ref> but tried several others classifier as well. Decision trees were helpful to identify the useful features: e.g. the MeSH terms turned up often.</p><p>As mentioned above, we addressed the problem as a text classification task at the sentence level, and ended up stating it as a choice between the title and the last sentence. Using the standard feature set, we trained a Naive Bayes classifier that was able to distinguish between when the best sentence is a title vs. a non-title with an accuracy of 81.22%, as measured on a stratified 10-fold cross-validation on a corpus of 4,000 GeneRIF texts.</p><p>We wanted to extend this idea in the direction of a two-step classification: the first classifier chooses between title and non-title. In case non-title is chosen, then a second classifier chooses the best abstract sentence (here the most important feature is is last sentence). The second classifier is to be trained on non-title sentences only. Unfortunately, comparing the title to the abstract body was problematic due to substantial length differences. We decided to simplify the things further and compare the title to the last sentence only.</p><p>We trained a Naive Bayes classifier with kernels that, given a gene and a document, chooses between the title (class A) and the last sentence (class B ). We used the standard feature set, but without the is title and is last sentence features. In order to label the training examples as belonging to class A/B we compared the MUD overlap of the target GeneRIF text with both the title and the last sentence and assigned the label A or B depending on which get a higher score. We then concatenated the title and the last sentence and extracted the features from the resulting string. So, each abstract produced a single example labeled either A or B.</p><p>We tried marking the features (e.g. MeSH terms) to indicate whether they came from the title or from the last sentence in order to allow the classifier to distinguish between them, but this lead to decreased performance and so was dropped. Finally, we limited the training to the abstracts coming from the 5 target journals that were announced on the genomics track Web site: about 6,500 abstracts in total. We split them into 10 sets and performed a stratified 10-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation</head><p>We performed several experiments for the different formulations of the problem in order to find the best feature set and the best classification algorithm using stratified 10-fold cross-validation and collections of different sizes: 343, 1000, 2000, 5000, 10000, 20000, 33662 etc. Then we fixed the feature set (the standard feature set) and the classifier (Naive Bayes; class A/B ) and concentrated on a set of 6,500 abstracts that have GeneRIFs and come from the 5 journals. We ran series of experiments in order to find the best thresholds for feature selection.</p><p>The baseline results of always choosing the title for the various scoring metrics are shown on line 1 of Table <ref type="table" coords="9,339.26,243.88,3.88,8.74" target="#tab_3">5</ref>. For the training set cross validation runs, the best results were obtained for minimum verb frequency of 5 and minimum MeSH tree number frequency of 12 (see line 2 of Table <ref type="table" coords="9,455.22,279.74,3.88,8.74" target="#tab_3">5</ref>).</p><p>For the TREC run we trained on all the abstracts from the 5 journals, except the 139 ones used for testing. We used the feature thresholds found above (5 for verbs and 12 for MeSH tree numbers) and we obtained the results shown on line 3 of Table <ref type="table" coords="9,510.75,339.96,3.88,8.74" target="#tab_3">5</ref>. Although these are lower than those of line 2, they are still about 3-4% above the baseline shown on line 1.</p><p>We also calculated the best possible score we could have obtained if our algorithm had always made the correct choice between the title and the last sentence (see line 5 of Table <ref type="table" coords="9,391.73,412.13,3.88,8.74" target="#tab_3">5</ref>). If we had tuned the parameters to the test set, we would have used minimum verb frequency of 12 and minimum MeSH frequency of 11 and gotten the results shown in line 4 of Table <ref type="table" coords="9,513.31,447.99,3.88,8.74" target="#tab_3">5</ref>. (Of course, we cannot use the test set to compute the thresholds for the TREC run.) Finally, our scores 57.83%, 59.63%, 46.75%, 49.11% (for CD, MUD, MBD and MBDP) were the second best ones after those of Erasmus University (emc4): 53.04%, 54.65%, 38.62%, 41.17, who used a similar classification technique but managed to successfully train a classifier to choose among all sentences, not just between the title and the abstract. In fact, emc4 was the only other group that was able to beat the baseline. However, according to <ref type="bibr" coords="9,454.85,579.94,9.97,8.74" target="#b4">[6]</ref>, neither of these results represents a statistically significant improvement over just using the titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Primary Task</head><p>It appears that the definition of GeneRIF is quite fuzzy. This limits the potential contribution of our classification module. However, we believe that in cases where the subset of relevant documents could be defined more precisely combining such a classifier with a more traditional IR system could result in a significant boost in performance.</p><p>In order to further improve the performance of our system, semantic information has to be incorporated. In the future we plan to add syntactic and semantic annotations to the text in order to support much more powerful algorithms for information retrieval and extraction from bioscience literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Secondary Task</head><p>Better results could potentially be with a careful feature selection algorithm <ref type="bibr" coords="10,225.27,358.48,14.62,8.74" target="#b10">[12]</ref>: all we do at present is to remove the least frequent features and our algorithm is very sensitive to setting the correct threshold (compare lines 3 and 4 in Table <ref type="table" coords="10,263.30,394.35,3.87,8.74" target="#tab_3">5</ref>). This would allow the introduction of some carefully selected stems as features without a dramatic increase of the vector space dimensionality, and can account for predictive phrases of the kind: our results show that . . . . In addition, although good, our gene and MeSH tagging utilities are not perfect. MeSH ambiguity is another source of problems and the verb nominalization introduces some noise as well.</p><p>A promising improvement would be a more careful truncation of the unnecessary part of the selected sentence. It would be also interesting to try some specialized algorithm that tries to learn a ranking directly (e.g. <ref type="bibr" coords="10,126.40,549.77,10.79,8.74" target="#b0">[2]</ref>) as opposed to the classification approach described above.</p><p>Finally, it would be good to have a similarity measure that takes into account the semantics, e.g., not penalize those cases in which a synonym name for the same gene is used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,421.94,468.00,8.74;3,72.00,433.90,38.78,8.74;3,118.80,114.46,374.39,292.37"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Software architecture for the primary task. Part (a) was used for both runs; part (b) for the second run only.</figDesc><graphic coords="3,118.80,114.46,374.39,292.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,679.83,467.99,8.74;5,72.00,691.78,343.65,8.74;5,118.80,412.90,374.41,251.82"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classification accuracy for different feature sets: (a)descriptors as phrases (b) Whole tree numbers (c) Abstracts cleaned and stemmed (d) descriptors together with tree numbers</figDesc><graphic coords="5,118.80,412.90,374.41,251.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,330.91,198.76,170.30,8.74;6,330.91,210.72,189.17,8.74;6,330.91,222.67,178.60,8.74;6,330.91,234.63,189.17,8.74;6,330.91,246.58,189.17,8.74;6,330.91,258.54,129.77,8.74;6,310.98,279.92,229.02,8.74;6,310.98,291.87,145.07,8.74"><head></head><label></label><figDesc>(a) J * (G compared to terms in titles) (b) J * (LG compared to terms in abstracts) (c) K * (LG compared to terms in titles) (d) K * (LG compared to terms in abstracts) (e) L * (MeSH concepts compared to MeSH terms assigned to documents) where J = 1, K = 0.015, and L = 1.4 (determined experimentally on training data).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,137.10,153.14,337.80,194.79"><head>Table 2 :</head><label>2</label><figDesc>Rules for expanding gene names</figDesc><table coords="5,286.60,153.14,188.30,8.74"><row><cell>of spaces (denormalization of numerals)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,176.18,371.01,259.64,8.74"><head>Table 3 :</head><label>3</label><figDesc>MeSH terms used to map documents to organisms</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,72.00,67.62,229.02,177.33"><head>Table 4 :</head><label>4</label><figDesc>Finding the best mapping of the GeneRIF text against the corresponding abstract.word and sentences boundaries, and for each one we calculated the Modified Unigram Dice (MUD) score.</figDesc><table coords="8,91.27,67.62,190.47,86.84"><row><cell>Run</cell><cell>Modified Unigram Dice</cell></row><row><cell>In title</cell><cell>35.17%</cell></row><row><cell>In abstract</cell><cell>45.81%</cell></row><row><cell>In both</cell><cell>4.98%</cell></row><row><cell>In last sentence</cell><cell>20.63%</cell></row><row><cell>Exact title match</cell><cell>19.67%</cell></row><row><cell>Total matched</cell><cell>76.02%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,87.62,67.59,436.76,102.36"><head>Table 5 :</head><label>5</label><figDesc>TREC run result compared to baseline, best possible result and a better features selection.</figDesc><table coords="10,171.74,67.59,268.52,70.54"><row><cell></cell><cell>Run</cell><cell>CD</cell><cell>MUD MBD MBDP</cell></row><row><cell>1</cell><cell>Baseline</cell><cell cols="2">50.47% 52.60% 34.82% 37.91%</cell></row><row><cell cols="4">2 Cross Validation 58.06% 59.11% 44.74% 47.29%</cell></row><row><cell>3</cell><cell>TREC run</cell><cell cols="2">53.04% 54.65% 38.62% 41.17%</cell></row><row><cell cols="4">4 Tuned thresholds 54.88% 56.66% 40.66% 43.31%</cell></row><row><cell>5</cell><cell>Upper bound</cell><cell cols="2">61.72% 64.19% 50.88% 54.00%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,87.24,708.22,113.66,6.99"><p>http://www.nlm.nih.gov/mesh</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,326.22,660.90,213.77,6.99;6,310.98,670.36,229.02,6.99;6,310.98,679.82,229.02,6.99;6,310.98,689.29,229.02,6.99;6,310.98,698.75,229.02,6.99;6,310.98,708.22,34.58,6.99"><p>However, we have been told via personal communication from James Cooper of IBM, reporting information from Roy Byrd of IBM, that the algorithm is based on the Guru ranking algorithm<ref type="bibr" coords="6,348.77,689.29,8.47,6.99" target="#b6">[8]</ref>, which is a Bayesian computation of a document's probability of being relevant to the query, with lexical affinities mixed in.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,92.48,694.90,208.55,8.74;10,92.48,706.86,208.55,8.74;10,331.46,193.04,208.54,8.74;10,331.46,205.00,208.54,8.74;10,331.46,216.95,149.18,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,238.69,694.90,62.34,8.74;10,92.48,706.86,185.36,8.74">A new family of online algorithms for category ranking</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.46,193.04,208.54,8.74;10,331.46,205.00,208.54,8.74;10,331.46,216.95,56.27,8.74">25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,235.22,208.54,8.74;10,331.46,247.18,208.55,8.74;10,331.46,259.13,171.20,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,397.29,235.22,142.72,8.74;10,331.46,247.18,191.73,8.74">Gauging similarity with n-grams: Language-independent categorization of text</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Damashek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,331.46,259.13,32.08,8.74">Science</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="843" to="848" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,277.40,208.53,8.74;10,331.46,289.36,208.53,8.74;10,331.46,301.32,208.54,8.74;10,331.46,313.27,144.99,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,481.23,277.40,58.76,8.74;10,331.46,289.36,208.53,8.74;10,331.46,301.32,65.96,8.74">Beyond independence: Conditions for the optimality of the simple bayesian</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,417.85,301.32,122.15,8.74;10,331.46,313.27,111.22,8.74">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,331.54,208.55,8.74;10,331.46,343.50,108.38,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,447.25,331.54,92.76,8.74;10,331.46,343.50,78.26,8.74">Pattern classification and scene analysis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,361.77,208.54,8.74;10,331.46,373.72,208.55,8.74;10,331.46,385.68,208.54,8.74;10,331.46,397.63,208.54,8.74;10,331.46,409.59,65.65,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,463.16,385.68,76.83,8.74;10,331.46,397.63,178.24,8.74">Enhancing access to the bibliome: The trec genomics track</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct coords="10,331.46,427.86,208.54,8.74;10,331.46,439.81,208.54,8.74;10,331.46,451.77,208.55,8.74;10,331.46,463.72,204.63,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,443.17,427.86,96.83,8.74;10,331.46,439.81,150.32,8.74">Estimating continuous distributions in bayesian classifiers</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,502.95,439.81,37.05,8.74;10,331.46,451.77,208.55,8.74;10,331.46,463.72,23.42,8.74">Eleventh Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="338" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,481.99,208.55,8.74;10,331.46,493.95,208.54,8.74;10,331.46,505.90,208.54,8.74;10,331.46,517.86,208.54,8.74;10,331.46,529.82,120.28,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,461.50,481.99,78.51,8.74;10,331.46,493.95,108.75,8.74">Full text indexing based on lexical relations</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Smadja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,461.76,493.95,78.24,8.74;10,331.46,505.90,208.54,8.74;10,331.46,517.86,208.54,8.74;10,331.46,529.82,27.38,8.74">Proceedings of the 12th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 12th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,548.09,208.54,8.74;10,331.46,560.04,22.70,8.74" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m" coord="10,391.05,548.09,77.50,8.74">Machine Learning</title>
		<imprint>
			<publisher>McGraw Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,578.31,90.35,8.74;10,438.95,578.31,101.06,8.74;10,331.46,590.27,171.48,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,438.95,578.31,101.06,8.74;10,331.46,590.27,27.86,8.74">Information Retrieval, 2nd ed</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Butterworth-Heinemann</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,608.54,208.54,8.74;10,331.46,620.49,208.55,8.74;10,331.46,632.45,208.54,8.74;10,331.46,644.40,22.70,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,451.22,608.54,88.78,8.74;10,331.46,620.49,208.55,8.74;10,331.46,632.45,114.88,8.74">Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.46,662.67,208.53,8.74;10,331.46,674.63,208.55,8.74;10,331.46,686.58,208.54,8.74;10,331.46,698.54,178.91,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,473.34,662.67,66.65,8.74;10,331.46,674.63,204.30,8.74">A comparative study on feature selection in text categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,345.38,686.58,194.62,8.74;10,331.46,698.54,144.26,8.74">Proceedings of the Fourtheenth Intenational Conference on Machine Learning</title>
		<meeting>the Fourtheenth Intenational Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
