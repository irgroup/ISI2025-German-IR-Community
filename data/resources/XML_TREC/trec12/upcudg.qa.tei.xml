<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,180.12,86.36,251.96,14.88">QA UdG-UPC System at TREC-12</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.90,135.17,67.68,10.46"><forename type="first">Marc</forename><surname>Massot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. Informàtica i Matemàtica Aplicada</orgName>
								<orgName type="institution">Universitat de Girona</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.08,135.17,97.65,10.46"><forename type="first">Horacio</forename><surname>Rodríguez</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">TALP Researh Center Universitat Politècnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,260.94,204.17,69.68,10.46"><forename type="first">Daniel</forename><surname>Ferrés</surname></persName>
							<email>dferres@lsi.upc.es</email>
							<affiliation key="aff2">
								<orgName type="department">TALP Researh Center Universitat Politècnica de Catalunya</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,180.12,86.36,251.96,14.88">QA UdG-UPC System at TREC-12</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9810920D3B00007333CABA5FDF71D8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a prototype multilingual Q&amp;A system that we have designed to participate in the Q&amp;A Track of TREC-12. The system answer concrete responses, then we participate in the Q&amp;A main task for factoid questions. The main areas of our system are:</p><p>(1) Inductive Logic Programming to learn the question type, (2) Clustering of Named Entities to improve Information Retrieval and (3) Semantic relations and EuroWordNet synsets to perform a language-independent answer extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper describes a prototype Q&amp;A system we have designed to participate in the Q&amp;A Track of TREC-12. Our aim has been to build a system as much as possible language independent, where language dependent modules could be changed for allowing the system to be applied to different languages. In this way we have developed in parallel two different Q&amp;A systems, one for English and another for Spanish.</p><p>As our research group has mainly focused in building resources and tools for NLP in Spanish, we have directly applied these tools and resources in our system. For English system we have used, when possible, publicly available resources or adapted our own tools.</p><p>In this paper we present the overall architecture of the system, we describe briefly its main parts, focusing on the language independent ones, and we present a preliminary evaluation of the prototype presented at the TREC-12 competition.</p><p>Our system was designed to participate in the Q&amp;A main task for factoid questions. Thus, we develop a system to answer questions with a concrete response. We structure the remaining part of the paper as follows. In Section 2, we first give an overview of the system and then focusing on every subsystem. Finally, in section 3, we evaluate the results of this participation and we detail some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The system architecture follows the most commonly used schema, splitting the process into three phases that are performed in turn. Several iterations can be carried out within these phases to achieve their goals but once one phase is finished there is no possibility to return to previous phases. There are three main subsystems, one corresponding to each phase:</p><p>1. Question processing (QP) 2. Passage retrieval (PR) 3. Answer extraction (AE) These subsystems are described below. Some pre-processing has been done on the document collection (the Acquaint corpus in this case). We will describe this issue when we present the PR subsystem.</p><p>Language dependent components are included only within the QP and AE subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Processing</head><p>The main goal of this subsystem is to classify the question regarding the kind of expected answer and to attach the information needed for the following subsystems. For PR the information needed is basically lexical (lists of keywords) and for AE lexical, syntactic and semantic. We have tried to represent all these kind of information using a language independent formalism. In particular we use the same semantic primitives and relations for the two languages (English and Spanish) involved in our system. This subsystem uses a great amount of linguistic resources for performing its task. As our goal is processing questions in Spanish and English, both with independent linguistic resources and tools, we need mapping tools for providing information for the following subsystems in an uniform representation.</p><p>The tools used for the Spanish version are those of the group of NLP of the UPC (see <ref type="bibr" coords="2,85.08,620.45,96.53,10.46" target="#b0">[Atseries et al, 1998</ref>] for a description of these tools). The question is analyzed with a pipe including the following processors:</p><p>• ms-analyze, that performs tokenization, morphological analysis (including identification of quantities, dates, multiword terms, etc.), and POS tagging. As a result we obtain a sequence of tokens with POS and lemma. • tacat, a partial parser that obtains nominal, prepositional and verbal phrases.</p><p>• NERC, a Named Entity Recognizer and Classifier that identifies the NE occurring in the question and classifies them in basic categories (person, place, organization,…). See <ref type="bibr" coords="3,226.37,102.53,96.34,10.46" target="#b0">[Carreras et al, 2002</ref>] • Finally we obtain and attach semantic information using EWN<ref type="foot" coords="3,435.30,114.35,3.99,6.96" target="#foot_0">1</ref> . The information obtained and used for further processing consists of the list of synsets (with no attempt to Word Sense Disambiguation), the list of hyperonyms of each synset (up to the top of each hyperonymy chain), the EWN's Top Concept Ontology, TCO class <ref type="bibr" coords="3,148.93,172.37,111.74,10.46">[Rodriguez et al,1998]</ref>,and the Magnini's Domain Code <ref type="bibr" coords="3,429.20,172.37,97.84,10.46;3,121.08,186.17,25.81,10.46">[Magnini, Cavaglià, 2000]</ref>.</p><p>For English version, we have adapted some of our tools or used publicly available ones for getting the same information using the same representation formalism.</p><p>• TnT <ref type="bibr" coords="3,147.18,256.07,67.67,10.46" target="#b0">[Brants, 2000]</ref>, for the morphologic information, As TnT does not provide the lemma we have used the lemmatizer included in Princeton's WordNet software for covering this functionality.</p><p>• MINIPAR <ref type="bibr" coords="3,178.27,298.31,52.77,10.46">[Lin, 1998]</ref>, to perform full parsing. A post-process has been needed for representing the output in a way compatible with tacat's output. • The same NERC used for Spanish has been trained for English.</p><p>• A finer grained classifier for Geographic NE (those of type location) was developed, <ref type="bibr" coords="3,177.19,355.19,96.91,10.46">[Ferres et al, 2003a]</ref>, devising a set of gazetteers with binary classifiers learned using an ILP learner, FOIL, <ref type="bibr" coords="3,295.70,368.99,74.46,10.46" target="#b1">[Quinlan, 1993]</ref>. • A gazetteer of acronyms obtained using a Decision Tree learning approach <ref type="bibr" coords="3,493.12,383.69,33.97,10.46;3,121.08,397.49,52.74,10.46">[Ferres et al, 2003b</ref>]. • A list of relations actor-action obtained through an analysis of the glosses of WordNet.</p><p>The result of applying these linguistic resources and tools, obviously language dependent, to the text of the question is represented in two structures:</p><p>• Sint, composed by two lists, one recording the information related to the syntactic components of the question (basically nominal, prepositional and verbal phrases) and the other collecting the information of dependencies and other relations between these components. • Sent, that provides us with information for each lexical unit: the word form, the lemma, the POS, the semantic class (and subclass if available) of NE, the list of EWN synsets, the information associated to these synsets (TCO and DC) and, finally, whenever possible the verbs associated to the actor and the relations between locations and their gentile.</p><p>Once this information is obtained we are able to get the information relevant to the following tasks:</p><p>• Question type. The most important information we need to extract from the question text is the Question Type, QT, because all the work the system has to perform for searching the answer is based on this issue. A failure on identifying QT practically disables the correct extraction of the answer. Currently we are working with about 50 QT. The QT tries to focus the type of the expected answer providing as well additional constraints on it. For instance, when the expected type of the answer is a person, two types of questions are considered, Who_action, which indicates that we are looking for a person who performs a certain action and Who_person_quality, that indicates that we are looking for a person having the desired quality. In order to determine the QT our system uses an Inductive Logic Programming (ILP) learner that learns, from a set of positive and negative examples, a set of weighted rules. We have used as learner the FOIL system <ref type="bibr" coords="4,406.88,323.33,75.14,10.46" target="#b1">[Quinlan, 1993]</ref>. We use FOIL for learning a different classifier (i.e. a set of rules) for each QT. As training set we have used the set of questions of TREC 8 and 9 (~900 questions) manually tagged and as test set the 500 questions of TREC 11. With these rules we have obtained an overall precision 68.54% and a recall of 60.00%. But the most of the errors are in similar QT categories, i.e. the 50% of errors for a generated when_begins are when_action questions, thus impact of these errors is minimum.</p><p>For [sArtifact,sObject,sAnimal],T3), the_pos(T2,pos("IN")), not(has_term_with_pos(TT,pos("JJS"),_)).</p><p>• Environment. Under this term we collect the semantic relations that hold between the different components identified in the question text. These relations are organized into an ontology of about 100 semantic classes and 25 relations (mostly binary) between them. Both classes and relations are related by taxonomic links.</p><p>The ontology has been manually built and tries to reflect what is needed for an appropriate representation of the semantic environment of the question (and the expected answer). For instance, Action is a class and Human_action another class related to Action by an is_a relation. In the same way, Human is a subclass of Entity. Actor_of_action is a binary relation (between a Human_action and a Human). When a question is classified as Who_action an instance of the class Human_action has to be located in the question text. Later, in the AE phase, an instance of Human_action has to be located in the selected passages and an instance of Human related to it by the Actor_of_action relation has to be extracted as candidate to be the answer.</p><p>The environment of the question is obtained from the syntactic information (sint) and the semantic information included in sent. For performing this task a set of about 150 rules has been manually built.</p><p>For instance, for the question:</p><p>Who won the Nobel Peace Prize in 1991?</p><p>the following environment was obtained: action(A, won), time_of_event(A, T), year(T, 1991), theme_of_event(A, U), neothers(U, Nobel Peace Prize)</p><p>• Semantic Constraints. The environment tries to represent the whole semantic content of the question. Not all the items belonging to the environment are useful, however, for extracting the answer. So, depending on the QT, a subset of the environment has to be extracted. Sometimes additional relations, not present in the environment, are used and sometimes the relations extracted from the environment are extended, refined or modified. We define in this way the set of relations (the semantic constraints) that are supposed to be found in the answer. These relations are classified as mandatory or optional. Following the preceding example:</p><p>o Mandatory Constraints: actor_of_action(A, X) action(A, won) theme_of_event(A, U) neothers(U, Nobel Peace Prize) o Optional Constraints: time_of_event(A, T) year(T, 1991)</p><p>• Question Keywords. The terms extracted from the question text that have to be used for performing the PR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passage retrieval</head><p>In order to perform the PR task we have used MG <ref type="bibr" coords="6,335.25,423.11,93.68,10.46">[Witten et al, 1999]</ref>. Before the TREC-12 competition we indexed into MG the whole Acquaint collection. We built two indexes:</p><p>• Textual, i.e. indexing the documents from their textual content, as simple bag of words, with no pre-process • Named Entities: We carried out a NERC process of the whole collection, we clustered these NE into clusters trying to group the different variants of the same entity, including acronyms for NE of type organization (see <ref type="bibr" coords="6,411.99,521.45,97.83,10.46">[Ferres et al, 2003b]</ref> for details of this process), and we indexed the documents using as key words the terms representative of the clusters.</p><p>At PR phase the process was the following:</p><p>With the Question Keywords obtained in the previous subsystem for each question, we looked for relevant documents in the two collections. We use a ranked retrieval for the textual collection with a threshold of 10% of the first retrieved document and a limit of 200 documents. For the Named Entities collection, we use Boolean retrieval in order to covers all the Named Entities of the query terms. The union of both sets of documents was indexed again into MG, this time only with the textual form but at level of passage. We consider a passage a sequence of 8 consecutive sentences of the original document allowing an overlapping of one sentence. Then, we retrieved the candidate passages with the same keywords, again with a threshold of 10% of the first retrieved passage but this time using a limit of 50 passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer extraction</head><p>The process of extraction of the answer is carried out on the set of passages obtained from the previous subsystem. These passages are segmented into sentences. Each sentence is then scored according to its semantic content regarding the question. We build a general semantic representation of the concepts that occurring in the question in order to overcome the term-based approach limits for the sentence selection. The semantic content of a term is represented using a weighted vector, the weight of each term is computed using the idf of the term, synonyms, hyponyms and hyperonyms. The semantic content of a concept is then built from the semantic content of its terms. <ref type="bibr" coords="7,297.94,269.63,70.50,10.46">[Vicedo, 2002]</ref>.</p><p>The linguistic process of extraction, quite expensive, is carried out on the sentences best scored.</p><p>This process is similar to the process carried out on questions and leads to the construction of the environment of each candidate sentence. The rest is a mapping between the semantic relations contained in this environment and the Semantic Constraints extracted from the question. The mandatory restrictions must be satisfied to take in consideration the sentence, the satisfaction of the optional constraints simply increases the score of the candidate.</p><p>The final extraction process is carried out on the sentences satisfying this filter.</p><p>The Knowledge Source used for this process is a set of extraction rules owning a credibility score. Each QT has its own subset of extraction rules that leads to the selection of the answer.</p><p>The process of application of the rules follows an iterative approach. In the first iteration all the semantic constraints have to be satisfied by at least one of the candidate sentences. If no sentence has satisfied the constraints, the set of semantic constraint is relaxed by means of structural or semantic relaxation rules, using the semantic ontology. If no candidate sentence occurs when all possible relaxations have been performed the question is assumed to have no answer.</p><p>Each candidate to solution comes weighted by diverse factors (sentence score, confidence of the used rules, satisfied optional restrictions, etc.). In the case more than one candidate is detected, a final process of weighted voting is carried out to select the preferred answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation and Conclusions</head><p>As we have said in the introduction, this paper describes the system we have built for our first participation in TREC competitions. Our main goal on attempting to participate in TREC-12 has been to acquire some experience on the kind of problems that have to faced in Q&amp;A tasks. Although some of these problems have been foreseen by analysing other systems and previous competitions it is necessary to face the real problems (in real time) for taking the appropriate conclusions.</p><p>We have participate in TREC-12 with a prototype, not with a complete Q&amp;A system. The different components of the system have got different levels of development (and, obviously, different level of accuracy). The first two subsystems, QP and PR, are the most completed and present the best results but the last one, AE, was only sketched and is currently under construction. Only a few rules for each Question Type were developed and no sufficient experimentation of the performance of these rules was carried out at the time of the competition.</p><p>With these constraints, the results obtained by our system are not good, but we think that they are a good starting point for further improvements of our system. Some initial analysis of the results has been made and some comments follow.</p><p>As has been pointed above our participation was constrained to the factoid questions. So we provided answer to 413 questions. From them we gave the exact answer to only 22 questions (2 other were considered wrong). So the global accuracy of our system was 5.3%. The precision of recognising questions with no answer was of 9.2%, the recall was in this case of 43.3% (this figure was due to the fact that when our system was unable no find an answer the response was NIL, this was the case of 141 questions).</p><p>The classification of the question was rather good. The accuracy of our system was in this issue of 69%, quite close to the scores measured in our tests on previous TREC. Taking into account the fine granularity of our Types of Questions we think that it is a good result.</p><p>There 383 questions for which an answer exists in the collection. From these, only 157 were in the passages (50 per question) retrieved from the PR subsystem. This means that only 38% of the questions could be correctly answered by the AE subsystem.</p><p>As has been noted above, the AE module was only sketched for the competition. The accuracy of these components for questions which the answer occurred in our selected passages was of 8.3%.</p><p>Obviously, considered in isolation, the figures of AE are the worst of the three components. Part of the reason could be the accumulation of errors from previous components but the component itself has to be improved heavily.</p><p>There is of course enough room for improvement in every component and work is currently being done in all these components. In the next future, however, we plan to concentrate on getting better extraction rules by means of applying Machine Learning techniques, in the same line we have applied to the classification of questions problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,142.08,419.93,385.12,10.46;4,121.08,433.73,329.64,10.46;4,139.08,461.27,71.69,11.47;4,139.08,475.07,160.75,11.47;4,139.08,488.87,54.62,11.47;4,139.08,502.67,39.97,11.47;4,139.08,516.47,222.06,11.47;4,139.08,530.27,120.92,11.47;4,139.08,544.07,377.42,11.47;4,139.08,557.87,117.02,11.47;4,139.08,571.67,164.70,11.47;4,139.08,585.47,270.25,11.47;4,121.08,613.07,405.96,10.46;4,121.08,626.87,406.06,10.46;4,121.08,640.67,160.70,10.46;4,139.08,668.27,89.99,11.47;4,166.08,681.63,233.97,8.80;4,178.08,692.97,119.99,8.80;4,178.08,704.31,113.99,8.80;5,178.08,73.65,107.99,8.80;5,178.08,84.93,143.99,8.80;5,139.08,96.71,368.79,11.47;5,166.08,110.07,269.97,8.80;5,178.08,121.41,155.99,8.80;5,178.08,132.75,119.99,8.80;5,178.08,144.09,155.99,8.80;5,139.08,155.81,90.05,11.47;5,166.08,169.17,305.96,8.80;5,178.08,180.51,102.00,8.80;5,178.08,191.85,161.99,8.80;5,178.08,203.19,221.98,8.80"><head></head><label></label><figDesc>each classifier we have used as negative examples the questions belonging to the other classes. The features used for classifying are the following: o Word form o Word position in the question o Lemma o POS o Semantic class of NE, without subclassing o Synsets of the lemma o All hyperonyms for each synset without the information about the distance o TCO for each synset o Domain Codes for each synset o Main syntactic relations, subject and object relations The set of rules for each class has been manually revised and completed with a set of manually built rules (with lower weights) in order to assure a complete coverage. See below a couple of such rules: o A learned rule: regla(non_human_actor_of_action,A,1) :-first_position(A,B), next_position(B,C), is_tco(cObject,C), is_domain(dTransport,C).o The same rule after transformation(performed for the sake of efficiency):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,121.08,199.13,406.07,80.47"><head></head><label></label><figDesc>The action and the quality are the parameter of the corresponding QT. The following are examples of questions classified as Who_action type: o What is the name of the managing director of Apricot Computer? o Who won the Nobel Peace Prize in 1991? o Who is the writer of the book, "The Hobbit"?</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,90.84,712.29,150.28,8.74"><p>http://www.illc.uva.nl/EuroWordNet/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,89.89,201.77,436.37,10.46;9,85.08,215.57,326.95,10.46;9,85.08,229.49,351.35,10.46;9,85.08,243.17,415.15,10.46;9,85.08,256.97,129.62,10.46;9,85.08,284.57,115.32,10.46;9,85.08,298.49,215.37,10.46;9,85.08,312.17,98.32,10.46;9,183.42,309.29,6.21,6.96;9,192.66,312.17,175.26,10.46;9,85.08,339.77,292.86,10.46;9,85.08,353.69,216.34,10.46;9,85.08,367.37,432.48,10.46;9,85.08,381.17,323.94,10.46;9,85.08,408.77,372.25,10.46;9,85.08,422.69,284.37,10.46;9,85.08,436.37,6.00,10.46;9,91.08,433.49,6.21,6.96;9,100.32,436.37,204.75,10.46;9,85.08,463.91,372.91,10.46;9,85.08,477.83,321.88,10.46;9,85.08,491.51,6.00,10.46;9,91.08,488.63,6.21,6.96;9,100.32,491.51,204.75,10.46;9,85.08,519.11,92.05,10.46;9,85.08,533.03,222.17,10.46;9,85.08,546.71,438.23,10.46;9,85.08,560.51,27.35,10.46;9,85.08,588.11,248.37,10.46;9,85.08,602.03,239.99,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,379.44,215.57,32.59,10.46;9,85.08,229.49,346.08,10.46;9,115.77,298.49,179.02,10.46;9,85.08,353.69,216.34,10.46;9,85.08,367.37,432.48,10.46;9,85.08,381.17,25.83,10.46">Named Entity Extraction Using Adaboost. Proceedings of the 6th conference on Computational Natural Language Learning (CoNLL 2002)</title>
		<author>
			<persName coords=""><surname>Atseries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,85.08,243.17,415.15,10.46;9,85.08,256.97,43.45,10.46;9,85.08,312.17,98.32,10.46;9,183.42,309.29,6.21,6.96;9,192.66,312.17,72.39,10.46;9,90.04,339.77,62.10,10.46;9,119.08,381.17,122.07,10.46;9,424.68,408.77,32.66,10.46;9,85.08,422.69,284.37,10.46;9,85.08,436.37,6.00,10.46;9,91.08,433.49,6.21,6.96;9,100.32,436.37,26.97,10.46;9,425.34,463.91,32.66,10.46;9,85.08,477.83,321.88,10.46;9,85.08,491.51,6.00,10.46;9,91.08,488.63,6.21,6.96;9,100.32,491.51,26.97,10.46">Proceedings of First International Conference on Language Resources and Evaluation. LREC-98</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Ferrés</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Massot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Padro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Rodríguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename></persName>
		</editor>
		<meeting>First International Conference on Language Resources and Evaluation. LREC-98<address><addrLine>Granada, Spain; Seattle, USA; Taipei, Taiwan; Lisboa, Portugal; Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Lisboa</publisher>
			<date type="published" when="1998">1998. Brants, 2000. 2000. 2002. September 2002. 2004. 2004</date>
		</imprint>
	</monogr>
	<note>Turmo Automatic Building Gazetteers of Co-referring Named Entities 4 th LREC. Submittted [Lin, 1998] Lin, D. Dependency-based evaluation of MINIPAR Proceedings of the Workshop on the Evaluation of Parsing Systems, LREC 1998, Granada, Spain [Magnini, Cavaglià, 2000] B. Magnini, G. Cavaglià Integrating Subject Field Codes into WordNet</note>
</biblStruct>

<biblStruct coords="9,85.08,615.71,205.34,10.46;9,85.08,643.31,145.99,10.46;9,85.08,657.23,383.68,10.46;9,85.08,670.91,160.29,10.46;10,85.08,74.09,418.93,10.46;10,85.08,87.89,165.39,10.46;10,85.08,101.81,411.72,10.46;10,85.08,115.61,142.67,10.46;10,85.08,129.29,321.34,10.46;10,85.08,156.89,131.30,10.46;10,85.08,170.81,371.68,10.46;10,85.08,184.49,291.52,10.46;10,85.08,212.03,233.01,10.46;10,85.08,225.95,111.97,10.46;10,85.08,239.63,271.93,10.46" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,85.08,657.23,126.23,10.46;10,203.11,87.89,47.36,10.46;10,85.08,101.81,411.72,10.46;10,85.08,115.61,142.67,10.46;10,85.08,129.29,139.61,10.46;10,181.73,156.89,34.66,10.46;10,85.08,170.81,366.26,10.46">Roventini The Top-Down Strategy for Bulding EuroWordNet: Vocabulary Coverage, Base Conceps and Top Ontology. Computer and Humanities 32</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,85.08,615.71,88.62,10.46;9,233.73,657.23,235.03,10.46;9,85.08,670.91,41.19,10.46">Proc. of the sixth European Conf. on Machine Learning</title>
		<meeting>of the sixth European Conf. on Machine Learning<address><addrLine>Athens, Greece; San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kauffman</publisher>
			<date type="published" when="1993">2000. Quinlan, 1993. 1993. 1998. 1998. May 2002. 1999</date>
		</imprint>
		<respStmt>
			<orgName>Dept. LSI. Universidad de Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Vicedo Un modelo semántico aplicado a los sistemas de B´squeda de Respuestas. Witten et al, 1999] I. Witten, A. Moffat, T. Bell Managing Gygabytes. second edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
