<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.36,61.91,306.85,21.52;1,111.84,89.75,401.86,21.52">Task-Specific Query Expansion (MultiText Experiments for TREC 2003)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,78.48,157.88,71.69,9.86"><forename type="first">David</forename><forename type="middle">L</forename><surname>Yeung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.05,157.88,93.98,9.86"><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.91,157.88,90.38,9.86"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.41,157.88,83.45,9.86"><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,477.51,157.88,69.05,9.86"><forename type="first">Egidio</forename><forename type="middle">L</forename><surname>Terra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.36,61.91,306.85,21.52;1,111.84,89.75,401.86,21.52">Task-Specific Query Expansion (MultiText Experiments for TREC 2003)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F87807D1F0F3A7CE2CB0D119F9FB203F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>For TREC 2003 the MultiText Project focused its efforts on the Genomics and Robust tracks. We also submitted passageretrieval runs for the QA track. For the Genomics Track primary task, we used an amalgamation of retrieval and query expansion techniques, including tiering, term re-writing and pseudo-relevance feedback. For the Robust Track, we examined the impact of pseudo-relevance feedback on retrieval effectiveness under the new robustness measures.</p><p>All of our TREC runs were generated by the MultiText System, a collection of tools and techniques for information retrieval, question answering and structured text search. The MultiText Project at the University of Waterloo has been developing this system since 1993 and has participated in TREC annually since TREC-4 in 1995.</p><p>In the next section, we briefly review the retrieval methods used in our TREC 2003 runs. Depending on the track, various combinations of these methods were used to generate our runs. The remaining sections describe our activities for the individual tracks, with the bulk of the report covering our Genomics Track results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RETRIEVAL METHODS</head><p>The MultiText System implements a variety of retrieval methods, three of which were used in our TREC 2003 experiments:</p><p>1) Shortest Substring Ranking (SSR), a ranked retrieval method for extended boolean queries <ref type="bibr" coords="1,235.00,548.56,10.97,8.97" target="#b0">[1]</ref>; 2) Qap, a passage-retrieval technique originally developed for question answering <ref type="bibr" coords="1,176.77,572.56,10.60,8.97" target="#b2">[3]</ref>, <ref type="bibr" coords="1,194.26,572.56,10.97,8.97" target="#b3">[4]</ref>; 3) Okapi BM25 <ref type="bibr" coords="1,136.73,584.56,10.60,8.97" target="#b7">[8]</ref>. Our Genomics runs use a combination of SSR and Okapi methods, generating and executing a number of different term sets and boolean queries, and merging the results to produce a final ranked document set. Our Robust runs use Qap to generate passages for pseudo-relevance feedback and Okapi to evaluate the expanded queries. For our QA track runs, 250byte answer passages are selected from passages generated by Qap.</p><p>In the next few sections, we provide brief overviews of the retrieval methods. Please consult the associated references if further details are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shortest Substring Ranking</head><p>The MultiText project has used variants of the SSR algorithm in TREC experiments since TREC-4 in 1995. The SSR algorithm operates by locating passages that satisfy a boolean query. A passage may consist of any substring of any document in the target corpus. The algorithm identifies all document substrings that satisfy the query and do not contain shorter substrings that also satisfy the query. This shortest substring rule serves to limit the number of passages that must be considered by the SSR algorithm. For ranking purposes, a document's score is computed from the lengths of the passages contained within it.</p><p>For example, along with other queries, our system generates the following query for Genomics topic 23: "c gamma" ˆ("phospholipase" "phospholipases") where the symbol represents boolean OR and the ˆsymbol represents boolean AND. Since the algorithm locates the shortest substrings that satisfy the query, a passage located by the algorithm will begin (or end) with the phrase "c gamma" and end (or begin) with one of the words "phospholipase" or "phospholipases". None of these terms will appear elsewhere in the passage, since otherwise the passage would contain a shorter substring that also satisfies the query.</p><p>In some cases, structural constraints are applied to the query. For example, the query:</p><p>("¡ NameOfSubstance¢ ".."¡ /NameOfSubstance¢ ")¢ "cip1" identifies instances of the NameOfSubstance field that contain the term "cip1", where the ¢ symbol is used to express the CONTAINS relationship.</p><p>Assume that document £ contains passages ¤ ¦¥ ¨ § ©¤ § ¨ § ©¤ , sorted by increasing length, with each passage satisfying the query under the shortest substring rule. We compute a score for £ that rewards shorter passages and documents that contain more passages. For a passage ¤ define:</p><p>¤ "! $# % &amp; ' ( 0) 21 if 3 ¤ "! 54 76 8 if 3 ¤ "! 59 76 <ref type="bibr" coords="1,558.24,678.64,11.75,8.97" target="#b0">(1)</ref> where 3 ¤ @! is the length of ¤ as measured by the number of alphanumeric tokens it contains. For any passage ¤ , we have @¡ ¤ @! 9 8 . The score for £ is then computed by the formula:</p><formula xml:id="formula_0" coords="2,159.80,69.30,147.15,40.40">¡ ¢ £ ¥¤ ¤ ¢ ! §¦<label>(2)</label></formula><p>For our TREC 2003 experiments we use the parameters 6 # 8 ©¨a nd # .</p><p>SSR is most valuable when a boolean query matches a large number of documents. However, in many cases, a boolean query matches few or no documents. To address this case, we often generate tiers of boolean queries, with earlier tiers providing higher precision and later tiers providing higher recall. The tiers are executed in order, with the documents generated by earlier tiers ranked before the documents generated by later tiers. Once a document is generated by a tier, it is eliminated from later tiers. Section III-B describes the tiered boolean queries used in our Genomics Track runs.</p><p>A complete discussion and analysis of the SSR algorithm may be found in Clarke and Cormack <ref type="bibr" coords="2,222.53,277.60,10.60,8.97" target="#b0">[1]</ref>. That paper also provides an efficient algorithm to implement SSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Passage Retrieval</head><p>The Qap passage retrieval algorithm is related to the SSR algorithm, in that it may return any substring of any document. The algorithm locates "hotspots" within the corpus where query terms cluster in close proximity. The score of a hotspot is based on its length and the weights of the terms occurring within it. A hotspot is usually less than 50 words in length, but may be longer. It may start or end at any word and is not constrained by sentence or paragraph boundaries. At most one hotspot is selected from a document, since additional hotspots from the same document may not exhibit the independence properties assumed by our feedback and QA methods.</p><p>Given a query , document substring , and a term set , we compute a score for as follows:</p><p>¡ "! $# &amp;% (' 0) 21 43 ! 65 87 7 # &amp;% 4' 3 ! !</p><p>where 3 is the total number of times 9 appears in the corpus and ) is the total length of all documents in the corpus. In effect, the Qap algorithm considers every substring of every document in the corpus and locates the @ substrings with the highest score. Details of the Qap algorithm, its efficient implementation and its application to question answering may be found in Clarke et al. <ref type="bibr" coords="2,160.45,601.36,10.60,8.97" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Okapi</head><p>Our implementation of Okapi BM25 follows the description of Robertson et al. <ref type="bibr" coords="2,138.87,656.08,11.75,8.97" target="#b7">[8]</ref> with standard parameters: A ¥ # 8 B , C # D E ( , A # F , A (G # DH . Specifically, given a term set , a document £ is assigned the score</p><formula xml:id="formula_2" coords="2,137.90,689.55,169.05,34.35">¡ 4I QP ( ¥ 1 §R A ¥ 8 ! £ S £<label>(4)</label></formula><p>where</p><formula xml:id="formula_3" coords="2,348.90,67.60,129.80,37.60">P ( ¥ 1 # # &amp;% 4' T VU 5 U U W U</formula><p># number of documents in the corpus U # number of documents containing 9 R # frequency that 9 occurs in the topic</p><formula xml:id="formula_4" coords="2,353.20,144.65,158.51,58.70">£ # frequency that 9 occurs in £ S # A ¥ 8 5 C ! C YX 3 a`1 3 avg ! 3 `# length of £ 3 avg # average document length</formula><p>As an extension, our implementation of BM25 allows phrases (and extended boolean queries) to be used as query terms, a facility used in our Genomic runs to allow multitoken gene names and bigrams extracted from gene names to be treated as individual terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENOMICS TRACK -METHODOLOGY</head><p>For the Genomics track we experimented with a number of different retrieval, feedback and fusion techniques. The following sections describe the various aspects of the experimental methodology. Analysis of the results obtained from the training data is found in the Section IV. Section V discusses the results of our official TREC test runs.</p><p>In Section III-A, we investigate the effects of query formulation using the Okapi retrieval model. Section III-B deals with our experiments on using tiers of boolean queries to match against the metadata fields in the MEDLINE records. Then, in Section III-C, we explore the idea of merging the document sets retrieved by Okapi and the query tiering techniques. We describe the use of query expansion and feedback in Section III-D. In Section IV, we assemble the techniques into complete runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Okapi Query Formulation for the Genomics Track</head><p>Two important facts were discovered in preliminary experiments which influenced the design of the Okapi experiments. First, the gene name type did not seem to matter. A document discussing a particular gene was as likely to use an official name as an alternate one. Second, spacing and punctuation had a large effect on performance in some cases. The gene name in the original LocusLink-derived query may differ from the gene or protein name as it actually appears in the corpus only by the addition or removal of spaces or dashes. In a model based on term sets, such as Okapi, these slight variations may significantly affect the results.</p><p>We investigated the effects of query formulation on IR in the Genomics domain by generating multiple term sets from the original query, and comparing the effects of using these term sets to retrieve documents using the Okapi retrieval model. The three rules used to generate the term sets were:</p><p>Okapi 1: Each gene name in the original query, which may consist of multiple alphanumeric tokens, is considered as a phrase and treated as a single term, the only change being the removal of punctuation. Okapi 2: Heuristics were used to split up gene names containing semi-colons, commas, and brackets. Heuristics were also used to guess "plurals" for some of the terms. Okapi 3: First, the gene names were separated into two sets, one containing those gene names which were comprised of a single token, and another containing gene names which were comprised of multiple tokens. The Okapi term set was created from these two sets by first concatenating all pairs of single-token gene names together, and adding all the token-bigrams from the multiple-token gene names.</p><p>The name of the species was also included in each of the term vectors. The three rules are in decreasing order of strictness. Documents retrieved by Okapi 1 will contain the terms exactly as given in the original query (ignoring punctuation), while those retrieved by Okapi 2 will contain terms which are similar to but not exactly like those in the original query. Documents retrieved by Okapi 3 contain the same bigrams as found in the original query.</p><p>Each query formulation has its own advantages and disadvantages. The top documents returned by Okapi 1 are likely to be relevant, since they contain the query exactly, but many relevant documents may be missed because the gene name in the document appears differently than in the query. On the other hand, Okapi 3 retrieves many relevant documents in which the gene name does not appear exactly as in the query. However, it also retrieves many documents that are not relevant. The documents retrieved by Okapi 2 are intermediate between the two.</p><p>We found that the document sets retrieved using the term vectors generated by the three rules were quite different. Therefore, it was decided that the document sets produced by Okapi 1, Okapi 2, and Okapi 3 would be fused together. The fusion was accomplished in the following manner:</p><p>Okapi Fusion: The document sets retrieved by Okapi 1, Okapi 2, and Okapi 3 are combined by taking the intersection of the three result sets. A document's score is taken to be the product of the three scores. This list is then followed by the remainder of Okapi 3, with the scores appropriately scaled.</p><p>The rationale behind the fusion is that a document that scores highly on all three query formulations is very likely to be relevant. Taking the product of the scores allows each of the three document sets to vote on the relative distance between similarity values equally. Since Okapi 3 is the most relaxed of the three query formulations, it retrieves most if not all of the relevant documents retrieved by Okapi 1 and 2. Thus, the intersection of the three document sets likely contains most of the relevant documents in the document sets returned by Okapi 1 and 2, while it might miss relevant documents retrieved by Okapi 3. For that reason, the remainder of the Okapi 3 document set is appended to the end of the combined list.</p><p>While there are other fusion techniques, the above seemed to work very well in preliminary trials, and thus was the only technique used in the final completed runs. The performance of Okapi 1 is considered to be the baseline for comparison purposes in the rest of this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Boolean Query Formulation for the Genomics Track</head><p>Preliminary experiments showed that there was a correlation between some of the metadata fields in the MEDLINE record and the relevance of the document. In particular, there was a strong correspondence between the query terms and the terms that appeared in the RN (registry number) field of the MED-LINE record. The RN field contains a list of the chemicals discussed in the document. Many of these chemical names can be matched to the gene names found in the query. The chemical list is a better indicator of a document's relevance than the document's title, which in turn is a better indicator than the abstract. To capture this hierarchical structure among the metadata fields, we experimented with using a number of query tiers. The final tiering system had the following tiers, in decreasing order of relevance:</p><p>1) Tier 1: The gene name is found in the chemical list, or it is found in the chemical list preceded or followed by the word "protein", optionally followed by the name or description of the species. Spaces and punctuation are ignored for the purposes of comparison. (From training topic 5, "glycine receptor, alpha 1" is considered to be equivalent to "glycine receptor alpha1".) 2) Tier 2: This tier is similar to Tier 1, except that the chemical name is allowed to have additional terms.</p><p>(From training topic 11, "RAC1" retrieves documents in which "rac1 GTP-Binding Protein" appears in the chemical list.) 3) Tier 3: An attempt is made to find the conjunction of the terms from the gene name in the chemical list. If the gene name consists of a class name followed by a sequence of letters and numbers that specifies an object of that class, the name is successively weakened until a match is made. A set of heuristics are also used to recognize plurals. (From training topic 32, "estrogen receptor 1" is weakened until the documents retrieved contain "Receptors, Estrogen" in the chemical list.) 4) Tier 4: The query is converted into a boolean expression by turning each gene name into the conjunction of its terms, and taking the disjunction of all gene names. The boolean expression is applied to the title. 5) Tier 5: The boolean expression is applied to the chemical list. 6) Tier 6: The boolean expression is applied to the abstract. In addition, the documents are restricted to those in which the name of the species appears in the MeSH (Medical Subject Heading) metadata field. This does not completely eliminate documents which are not relevant to the species, since it is possible for the name of the species to appear in the MeSH field even if the focus of the paper is another species. It is quite common for an article about a gene in one species to mention a homologue in a related species. Nevertheless, if the name of the wanted species does not appear in the MeSH heading, then the article is (almost certainly) not relevant. Thus, using species data in the MeSH metadata field may result in false positives but not (or rarely) in false negatives.</p><p>Based on the query tiering model described above, we tested three different ways of retrieving documents:</p><p>All Tiers: Retrieve documents from all the tiers. Documents retrieved by each tier are ranked ahead of all documents retrieved by the next tier. A document that is retrieved in more than one tier is counted towards only its highest tier.</p><p>Best Tier: Retrieve the documents in the first tier that contains a non-zero number of documents. Subsequent tiers are ignored.</p><p>Exact: Retrieve only documents in Tier 1. No documents are retrieved if there are no documents in Tier 1. Note that for some topics, the above techniques may return zero documents. For that reason, the complete runs described in Section IV supplement the document sets retrieved by the tiering techniques with documents retrieved using the Okapi methods.</p><p>While the query tiers have a significant effect on performance, further improvement is possible by using fusion and feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Genomics Track Fusion</head><p>Since the Okapi and tiering experiments retrieved different document sets, we explored merging the results of the two techniques. We tried two different methods of combining the two document sets returned from Okapi and the tiering technique:</p><p>Interweave: The two document sets are combined by taking one document from each set successively. Rank Fusion: First, documents which were retrieved by both methods are merged. The score assigned to a document is a weighted sum of its (reverse) rank in each document set. The combined documents are followed by interweaving the remainder of the two document sets. We also attempted other types of fusion, but these were the only two which were completely implemented and tested due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Genomics Track Feedback</head><p>As explained in Subsection III-B, the similarity of the chemical list in the MEDLINE record to the query is a good indicator of a document's relevance. Because a gene name may have many variants, however, it is not always possible to match the gene name to an item in the chemical list even though one of the chemicals may refer to that gene or its product.</p><p>One possible solution to this problem is to attempt to recognize these name variants. That is not the approach we took. Instead, we attempted to learn the variant name by using feedback. If the gene name was matched in Tier 1 using the tiering technique, then the chemical list in the top retrieved documents already contains the gene name, and so feedback is unnecessary. Otherwise, we assume the top documents retrieved to be relevant, and find the chemical that has the highest correlation with these documents. The chemical names in the top documents were assigned a score using the formula: The highest scoring chemical name is then used to retrieve a set of documents containing that chemical name. The three sets of documents retrieved by the Okapi Fusion, query tiers, and feedback are then merged to produce the final document set. The number of top documents assumed to be relevant and the precise mechanism used to merge the final document sets are discussed in the next subsection.</p><formula xml:id="formula_5" coords="4,328.80,351.25,156.60,58.10">P ¢ # ¢ X T # &amp;% 4' T ) 3 ¢ W W ¡ For a chemical ¢ ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GENOMICS TRACK -TRAINING RUNS</head><p>The parameters of the various runs were optimized for the training data, using the supplied relevance judgments. Thus, the performance of the IR system on the training data is not necessarily reflective of its performance on the test data, especially if the training and test data have different characteristics. In particular, the relative performance of some of the runs that relied on a single retrieval technique may not be necessarily preserved. Nevertheless, the runs involving fusion and feedback do seem to consistently outperform the systems on which they are based. The parameters for these runs were adjusted not only to maximize performance, but to increase stability as well.</p><p>Following the TREC standard procedure, 1000 documents were retrieved for each run. We attempted to test a large variety of techniques, but unfortunately many tests could not be completed due to time constraints. The results for the runs which we conducted on the training data are shown in Table <ref type="table" coords="5,55.68,220.96,2.87,8.97">I</ref>. These were:</p><p>Okapi 1, 2, 3, and Fusion: These are the document sets retrieved by the procedure described in Section III-A.</p><p>All Tiers (AT): This is the set of documents retrieved by using the All Tiers method as described in Section III-B.</p><p>The documents retrieved by Okapi Fusion are appended to the end.</p><p>All Tiers Interweave-fusion (ATI): The set of documents retrieved by All Tiers is interweaved with the document set retrieved by Okapi Fusion.</p><p>All Tiers Rank-fusion (ATR): The set of documents retrieved by All Tiers is merged with the Okapi Fusion documents using the weighted rank fusion. It was experimentally determined that good results can be obtained if the Okapi rank was weighted 4 times as heavily as the tiering rank.</p><p>All Tiers Interweave/Rank-fusion with Feedback (ATIF, ATRF): These are the same as ATI and ATR, respectively, except that the feedback procedure described in Section III-D and further elaborated below was used if no documents were retrieved in Tier 1.</p><p>Best Tier (BT, BTI, BTR, BTIF, BTRF): These are analogous to the above, except that the query tiering subsystem retrieved only documents from the first tier with non-zero documents.</p><p>Exact: Instead of all the tiers or the best tier, only Tier 1 was used to retrieve documents. The Okapi Fusion document set was then appended to the end. (If no documents were retrieved in Tier 1 for a topic, then the final set of retrieved documents is just the set retrieved by Okapi Fusion.) ExactI: The set of documents retrieved by Tier 1 is interweaved with the Okapi Fusion set. Figure <ref type="figure" coords="5,95.73,620.80,5.03,8.97" target="#fig_1">1</ref> shows the combined system for the BTRF (Best Tier, Rank-fusion, Feedback) run. The topic is sent to both the Okapi and query tiers subsystems, each of which returns a set of documents. If the first tier to retrieve a non-zero number of documents is Tier 1, then the two document sets are fused. Otherwise, a third set of documents is retrieved using feedback, and the three sets of documents are fused. The other runs follow a similar logic flow.</p><p>The performance of feedback is dependent on the number of top documents used to determine the most relevant chemical name, and on the type of fusion used to merge the three document sets. These parameters are in turn dependent upon the query tiering technique used. For the All Tiers technique, it was determined that using the top 25-30 documents to determine the most relevant chemical name produced the best performance. (The value of 27 was used in the experiments.)</p><p>The three document sets are fused using rank fusion with equal weights. For the Best Tier technique, the top 42 documents were used, and the three document sets were merged using weighted rank fusion with a weight of 5 for the query tiers document set, 28 for the feedback document set, and 20 for the Okapi Fusion document set. These numbers were determined experimentally.</p><p>The reason for the difference between the feedback parameters of the AT and BT runs is that more of the top documents retrieved by the Best Tier technique are relevant compared to those retrieved by All Tiers. Since feedback is only used when no documents are retrieved in Tier 1, the set of documents retrieved using the top chemical name will be far more likely to be relevant than the documents retrieved by the Best Tier, and slightly more likely to be relevant than those retrieved by Okapi.</p><p>As can be seen from Table <ref type="table" coords="5,449.88,334.24,2.87,8.97">I</ref>, the best average precision belonged to the BTRF run, at 0.4821. This is a 47.3% improvement over the baseline Okapi 1, which had an average precision of 0.3273. The BTIF run had an average precision of 0.4812, a 47.0% improvement, and the ATRF run had an average precision of 0.4598, a 40.5% improvement. The ATRF run retrieved 291 relevant documents, which was the most relevant documents retrieved of all the runs. This is slightly more than the 286 retrieved by BTRF and BTIF, and significantly more than the 224 retrieved by the Okapi 1 run. Some general trends are discernible from the numbers. Feedback and fusion improved performance in every case, and the systems with the best performance made use of both. It isn't clear which fusion method is better, since ATR outperformed ATI, but BTI did better than BTR. However, when fusion is used with feedback, the rank fusion method outperformed the interweave fusion method in both cases.</p><p>There is a high level of correspondence between the metadata fields and the relevance of the documents. This is clear from the fact that retrieval using query tiers based on the information in the metadata fields outperformed the Okapi runs, including the Okapi Fusion run. Before fusion and feedback, the best technique that is based on query tiers is BT, with an average precision of 0.4003, which is a 22% improvement over Okapi 1. The Exact run had an average precision of 0.3981, a 21% improvement, while the AT run had an average precision of 0.3819, which close to 17% over Okapi 1. Note that both Best Tier and Exact had a better average precision than the All Tiers method. It appears that once a match has been found in a tier, it was a better strategy to append the Okapi Fusion list rather than documents from lower tiers. The experimental results suggest that the performance of the Okapi Fusion method was between that of Tier 1 and 2. (("luteinizing"ˆ"hormone"ˆ"choriogonadotropin"ˆ"receptor")+"lhcgr"+"lcgr"+"lhr"+ ("luteinizing"ˆ"hormone"ˆ"receptor")+("lutropin"ˆ"choriogonadotropin"ˆ"receptor")+"lcgrs"+ "lhcgrs"+("luteinizing"ˆ"choriogonadotropin"ˆ"receptor")+"lgr2"+"lhrs"+("lutropin"ˆ"receptor")+ ("choriogonadotropin"ˆ"receptor")) ((("phospholipase"+"phospholipases"))ˆ"c gamma") 24 --3 0 0 0 ((("seven"+"sevens")ˆ("absentia"+"absentias"))) 25 ---3 0 112 ("dntts"+"tdt"+"dntt"+("terminal"ˆ"deoxynucleotidyl"ˆ"transferase")+ ("deoxynucleotidyltransferase"ˆ"terminal")+"tdts") 26 ---1 0 1 (("rho"ˆ"related"ˆ"btb"ˆ"domain"ˆ"containing"ˆ"2")+"rhobtb2"+"kiaa0717"+"dbc2") 27 -----19 (("cholinergic"ˆ"receptor"ˆ"muscarinic"ˆ"3")+"chrm3  Table <ref type="table" coords="7,92.86,346.24,6.71,8.97">II</ref> shows the documents retrieved in each tier for the 50 training topics. The topic number is shown in the first column, followed by six columns showing the number of documents retrieved in each of the six tiers. The last column contains the expression or expressions used in the first tier in which a match was made.</p><p>In 32 out of 50 topics, the best tier was Tier 1. Of the remaining topics, Tier 2 was the best tier in 4 topics, Tier 3 was best in 8, and Tier 4 was best in 4. No documents were retrieved at all in Tier 5, and Tier 6 was the best tier for 1 topic. In the final arrangement of the query tiers, it happened that every document retrieved by Tier 5 had already been retrieved in a higher tier.</p><p>Because Tier 1 had a better performance on its own than Okapi or even feedback, performance can be improved by recognizing relevant chemical names in the chemical list metadata, even in cases where the name of the gene and the relevant chemical name are different.</p><p>Table <ref type="table" coords="7,94.05,561.28,10.07,8.97">III</ref> shows the chemical names produced by the pseudo-relevance feedback for those topics in which no documents were retrieved in Tier 1, for the BTRF run. The first column gives the topic number, and the second column gives a gene name from the query. The third column shows the chemical name that was found using automatic query expansion. The next four columns show the number of documents retrieved, the number retrieved and relevant, the mean average precision, and the interpolated recall-precision, respectively, for that topic without using feedback. The next two columns give the mean average precision and interpolated recall-precision with feedback, and the last column gives the percentage improvement (or degradation) due to using feedback. It is apparent that most of the chemical names are related in some way to the gene name, and a better way of recognizing the relationship between a gene and a chemical name will clearly improve performance.</p><p>For topic 28, the top chemical name "Krox-24 protein" was produced for the "Early growth response 1". In fact, "Krox-24 protein" is another name for "Early growth response 1". By searching on "Krox-24 protein", which does not appear in the original query, the average precision was improved by an incredible 878%. Of course, the original performance for this topic was very poor, but there is clearly a lot of potential for improving performance by recognizing the alternate names of a gene or a substance related to a gene.</p><p>In some cases, this is relatively simple. For topic 14, for example, the chemical name "Tyrosine-rRNA Ligase" was generated for the gene name "tyrosyl-rRNA synthetase". A system that understood the relationship between "tyrosine" and "tyrosyl" and "ligase" and "synthetase" can determine that the two expressions refer to the same thing (or closely related things), and even assign a score for the degree of similarity. In other cases, this is complicated by the fact that more than one chemical name generated by the automatic expansion might be relevant to the query. For topic 27, searching on the gene name "cholinergic receptor, muscarinic 3" resulted in the top chemical name "Receptors, Muscarinic". However, the chemical name "muscarinic receptor M3", which is clearly more relevant, was overlooked. Choosing this chemical name instead of the more general "Receptors, Muscarinic" would have resulted in an improvement of 534%.</p><p>As the table shows, in most cases the performance was improved by using feedback to find the most relevant chemical, though in some cases there was a degradation in performance. Determining the conditions under which feedback improved or degraded performance would allow feedback to be used more effectively.</p><p>The two runs chosen for official submission to TREC were the ATRF and BTRF runs. Even though BTIF had a better mean average precision than ATRF, it was too similar to the BTRF run in that it differed only in the fusion method used. It was found that by adjusting the fusion weights, it was always possible for the rank-fusion to outperform the interweave fusion. It was also suspected that the ATRF run might be more stable, in the sense that the performance would not be too adversely affected by an incorrect match in Tier 1. ATRF also had the most number of relevant documents retrieved, and it would be interesting to examine the tradeoff between retrieving more relevant documents and having a better precision. In addition to the two official runs, we also performed all the various runs using the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GENOMICS TRACK -RESULTS</head><p>The results for various test runs of our system are shown in Table <ref type="table" coords="8,80.13,525.28,8.63,8.97">IV</ref>. ATRF and BTRF are official runs, submitted to NIST under the run tags "uwmtg03atrf" and "uwmtg03btrf". Some similarities and differences between the training and test results may be noted. As with the training data, the BTRF run had the best performance on the test data, with an average precision of 0.3534. This is a 71.5% improvement over the Okapi 1 run, which had an average precision of 0.2060. The ATRF run retrieved the most relevant documents, and had the second best average precision at 0.3479, a 68.9% improvement over the Okapi 1 run. Furthermore, ATRF performed better than BTIF, which had an average precision of 0.3322. The distance between ATRF and BTRF was also smaller. On the training data, BTRF had a 4.8% improvement in average precision over ATRF, but on the test data that difference is only 1.6%. This suggests that with the test data, the gene names in the corpus are less like the queries than with the training data.</p><p>This conjecture is also supported by the performance of the Okapi runs. While the Okapi Fusion run performed better than any individual Okapi run, the Okapi 3 run had the highest average precision, followed by Okapi 2, and then Okapi 1. This is the reverse of the order with the training data. Using bigrams rather than the original query resulted in better performance on the test data. A more thorough analysis is needed to determine if this conjecture is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ROBUST RETRIEVAL TRACK</head><p>For the Robust Track, MultiText examined the impact of pseudo-relevance feedback on retrieval effectiveness under the new robustness measures. There are two unusual aspects to our work on this track: 1) the adaptation of techniques from our question answering system to pseudo-relevance feedback, and 2) the expansion of the corpus with a terabyte of Web data for pseudo-relevance feedback. Previous applications of this "collection enrichment" technique have generally used much smaller corpora <ref type="bibr" coords="8,385.37,272.32,10.60,8.97" target="#b5">[6]</ref>, <ref type="bibr" coords="8,403.10,272.32,10.60,8.97" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robust Track Feedback</head><p>For feedback, we adapted the passage-retrieval and termextraction methods from our QA system, which we have been developing over the past four years. Query processing proceeds as follows:</p><p>1) After stopword elimination and stemming, the terms from the topic field(s) are used by the Qap passageretrieval algorithm (Section II-B) to locate the top @ hotspots. 2) Feedback terms are extracted from the hotspots and the text surrounding them. A score is computed for each extracted term. If two terms stem to the same root, the term with the lowest score is eliminated, since stemming will be applied to the expanded query.</p><p>3) The top A feedback terms are added to the original term set. 4) Terms in the expanded query are stemmed. The result is executed using our implementation of Okapi BM25 to return the top 1000 documents. We treat any non-query term appearing within or near a hotspot as a candidate for feedback. For most of our TREC 2003 runs we extracted only single-word terms for feedback; for one run we we also extracted word bigrams. Our term extraction method assigns a score to each candidate term based on its distance from the hotspot, the number of retrieved passages it which it appears, and its relative frequency within the corpus.</p><p>Let ¥ § § ¨ § ¡ be the hotspots located by Qap. Let 3 ! be the length of hotspot as measured by the number of alphanumeric tokens it contains. We define a function ¢ § 9 ! over hotspots and terms 9 that measures the "length" of a passage that contains both the hotspot and the term. If 9 appears in the hotspot, then ¢ § 9 ! # 3 ! . If 9 appears outside the hotspot, then ¢ § 9 ! is the length in tokens of the shortest passage that contains both 9 and the entire hotspot . If 9 does not appear in proximity to the hotspot -if it </p><note type="other">Old Topics New Topics Old New Topics Run Tag Run Type avgp</note><formula xml:id="formula_6" coords="9,110.80,291.80,196.15,37.15">P # ¡ ¥ ¢¡ ¢ ¡ # &amp;% 4' T ) 3 X ¢ ¢ § 9 ! W<label>(5)</label></formula><p>We generate an expanded query by combining the top A feedback terms with the original topic terms. We adjust the retrieval weights of the added terms with a scaling factor that takes the feedback score P into account, and reflects the fact that terms added through feedback should not be assigned the same importance as the original topic terms.</p><p>Let £ be the score of the top-ranking feedback term (i.e. the term with the largest feedback score). We define the scaling factor for feedback term 9 as:</p><formula xml:id="formula_7" coords="9,152.80,438.90,154.15,31.15">¤ # ¥ X P £<label>(6)</label></formula><p>where ¥ # 8 "1 ¤ in all our experiments. ¤ is used to adjust the retrieval weights in the Okapi BM25 formula, modifying Equation 4 to:</p><formula xml:id="formula_8" coords="9,133.10,507.35,173.85,34.35">¡ 4I ¤ P ( ¥ 1 §R A ¥ 8 ! £ S £<label>(7)</label></formula><p>For original topic terms ¤ # 8 ; for feedback terms ¤ 9 8 "1 ¤ .</p><p>As an example, we examine the end-to-end processing for topic 613, using the description field:</p><p>How were pieces of the Berlin wall disposed of after their removal? After stopword removal, the Qap algorithm is used to generate a set of passages </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robust Track Experiments</head><p>In preparation for the track, we generated a large number of training runs and examined the impact of feedback parameters (e.g. the number of added terms) on average precision and robustness. In general, as feedback parameters are changed, average precision changes slowly in the direction of a single local maxima over a wide range of parameters. In contrast, changes to feedback parameters have small but unpredictable effects on the robustness measures, with many local maxima, making it difficult to tune these parameters specifically for robustness. As a result of our preliminary experiments we used a single set of parameters for all experiments: @ = 20 passages and A # ¤ ( terms. Using our passage-retrieval algorithm, we executed the original queries against three collections: 1) the AQUAINT corpus, used for the QA Track; 2) a terabyte collection of Web data; and 3) TREC disks 4 and 5 minus the CR documents, which is the target collection for the Robust Track. We then retrieved the top 20 passages from each collection, extracted the top 300 terms from each set of passages, and merged these terms into a single ranked list. A term was only included in this list if it appeared in list of terms extracted from the target collection. Finally, we added the top 35 terms from the merged list to the original query and executed this expanded query against the target collection using our version of Okapi BM25.</p><p>Table <ref type="table" coords="9,354.93,633.04,7.19,8.97">V</ref> provides a summary of our Robust track results. Each line provides results for a single run over various topic sets. The "Old Topics" were taken from the adhoc tasks of previous TREC evaluations; the "New Topics" were created for TREC 2003. For each set of topics the table reports values for the three measures used in the Robust track: 1) average precision ("avgp"), 2) the percentage of topics with no relevant documents in the top ten ("norel"), 3) and the mean average precision over the 25% of the topic set on which the run exhibited its worst performance ("bad").</p><p>The first five lines give the results for our five official runs: The pair uwmtCR0 and uwmtCR1 are description-only runs; the pair uwmtCR2 and uwmtCR3 are title-only runs. For the fifth run (uwmtCR4), we extended the feedback process to extract both single words and word bigrams. The last four lines give results for unofficial runs that use the topic title and description. For the sixth and seventh runs, the title and description were merged into a combined query and evaluated using the procedure above. For the last pair of runs, we fused our official title-only and description-only runs with the CombMNZ algorithm <ref type="bibr" coords="10,147.99,202.72,10.60,8.97" target="#b4">[5]</ref>, <ref type="bibr" coords="10,165.49,202.72,10.60,8.97" target="#b6">[7]</ref>.</p><p>As expected, feedback has a substantial positive impact on average precision. The impact is greatest on the old topics, were average precision increases by 31-65%. Over the new topics, performance improves by 11-30%. Overall, feedback has a positive impact on the "bad" robustness measure, but unfortunately it often has a negative impact on the "norel" measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. QUESTION ANSWERING TRACK</head><p>While we continue to develop our QA system, we did not submit runs for the main task of the QA track. The number of new requirements for the task, and our interest in other tasks, precluded our full participation.</p><p>We did submit runs for the passage retrieval task. These runs combine aspects of our TREC 2001 and 2002 QA systems. Using our TREC 2002 system <ref type="bibr" coords="10,180.11,392.08,10.60,8.97" target="#b1">[2]</ref>, we extracted exact answers from passages returned by the Qap algorithm. We then used techniques from our TREC 2001 system to locate 250-byte fragments that contain both the exact answers and related terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>The MultiText system supports a variety of standard and non-standard IR techniques. Depending on the track, we have combined these techniques in different ways to produce competitive runs. For the Genomics Track we merged the results of structured (boolean) and unstructured (term set) queries. Queries were expanded by re-writing terms and through feedback over the chemical names contained in metadata. For the Robust Track, we utilized a new pseudo-relevance feedback method developed from our existing QA system. Queries were expanded through feedback over an expanded collection that included a terabyte of Web data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,412.30,375.50,2.39,11.60;4,419.28,397.36,150.32,8.97;4,318.72,409.36,229.17,8.97;4,551.90,400.80,45.53,11.60;4,556.80,387.50,45.53,11.60;4,563.28,409.36,6.71,8.97;4,318.72,421.12,189.39,8.97;4,512.10,412.50,28.99,11.60;4,525.15,421.12,44.84,8.97;4,318.72,433.12,182.19,8.97;4,505.90,438.90,14.38,11.60;4,513.10,411.30,14.38,11.60;4,520.80,433.12,48.91,8.97;4,318.72,445.12,45.30,8.97;4,367.30,436.45,7.91,20.60;4,370.26,445.12,169.25,8.97;4,542.60,436.45,11.03,20.60;4,552.20,441.55,11.03,13.80;4,552.20,436.45,10.30,20.60;4,567.11,445.12,2.39,8.97"><head>¢</head><label></label><figDesc>is the number of times the chemical name appears in the chemical list of the top documents, 3 ¢ is the number of times it appears in the corpus, ) is the total length of all documents in the corpus, and P ¢ is the score assigned to ¢ . For our TREC 2003 experiments, we set £ # ¥¤ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,151.76,218.65,7.17"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flow diagram for the combined system of the BTRF run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,55.68,64.16,480.56,231.54"><head></head><label></label><figDesc>appears in the corpus. We then compute the feedback score for term 9 as:</figDesc><table coords="9,55.68,64.16,480.56,219.54"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>norel</cell><cell>bad</cell><cell>avgp</cell><cell>norel</cell><cell>bad</cell><cell>avgp</cell><cell>norel</cell><cell>bad</cell></row><row><cell>uwmtCR0</cell><cell cols="2">description only, feedback</cell><cell>0.150</cell><cell>14.0%</cell><cell>0.011</cell><cell>0.403</cell><cell>8.0%</cell><cell>0.052</cell><cell>0.276</cell><cell>11.0%</cell><cell>0.018</cell></row><row><cell>uwmtCR1</cell><cell cols="2">description only, no feedback</cell><cell>0.114</cell><cell>18.0%</cell><cell>0.009</cell><cell>0.355</cell><cell>6.0%</cell><cell>0.035</cell><cell>0.234</cell><cell>12.0%</cell><cell>0.013</cell></row><row><cell>uwmtCR2</cell><cell cols="2">title only, feedback</cell><cell>0.168</cell><cell>22.0%</cell><cell>0.006</cell><cell>0.370</cell><cell>10.0%</cell><cell>0.053</cell><cell>0.269</cell><cell>16.0%</cell><cell>0.015</cell></row><row><cell>uwmtCR3</cell><cell cols="2">title only, no feedback</cell><cell>0.102</cell><cell>16.0%</cell><cell>0.007</cell><cell>0.285</cell><cell>8.0%</cell><cell>0.042</cell><cell>0.194</cell><cell>12.0%</cell><cell>0.013</cell></row><row><cell>uwmtCR4</cell><cell cols="2">description only, feedback, bigrams</cell><cell>0.148</cell><cell>20.0%</cell><cell>0.014</cell><cell>0.404</cell><cell>8.0%</cell><cell>0.054</cell><cell>0.274</cell><cell>14.0%</cell><cell>0.019</cell></row><row><cell>-</cell><cell>title</cell><cell>description, feedback</cell><cell>0.175</cell><cell>16.0%</cell><cell>0.017</cell><cell>0.408</cell><cell>8.0%</cell><cell>0.087</cell><cell>0.292</cell><cell>12.0%</cell><cell>0.029</cell></row><row><cell>-</cell><cell>title</cell><cell>description, no feedback</cell><cell>0.133</cell><cell>10.0%</cell><cell>0.018</cell><cell>0.369</cell><cell>2.0%</cell><cell>0.066</cell><cell>0.251</cell><cell>6.0%</cell><cell>0.026</cell></row><row><cell>-</cell><cell cols="2">combMNZ (uwmtCR0, uwmtCR2)</cell><cell>0.174</cell><cell>10.0%</cell><cell>0.020</cell><cell>0.411</cell><cell>4.0%</cell><cell>0.085</cell><cell>0.292</cell><cell>7.0%</cell><cell>0.033</cell></row><row><cell>-</cell><cell cols="2">combMNZ (uwmtCR1, uwmtCR3)</cell><cell>0.130</cell><cell>10.0%</cell><cell>0.016</cell><cell>0.360</cell><cell>2.0%</cell><cell>0.066</cell><cell>0.245</cell><cell>6.0%</cell><cell>0.024</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">ROBUST TRACK -SUMMARY OF RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">appears outside a large window surrounding the hotspot or if it</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">does not appear inside the document containing the hotspot -then for simplicity we define ¢  § 9 ! # ) 21 43 , where ) is the total length of all documents in the corpus and 3 is the total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">number of times 9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,75.84,226.24,493.95,499.53"><head></head><label></label><figDesc>) precede each term. The average precision for the topic increases from 0.212 to 0.487.</figDesc><table coords="9,75.84,226.24,493.63,499.53"><row><cell>The hotspot is in italics. Feedback over these passages gener-</cell></row><row><cell>ates the expanded term set:</cell></row><row><cell>#1.00 pieces #1.00 berlin #1.00 wall #1.00 dis-</cell></row><row><cell>posed #1.00 removal #0.333 war #0.297 freedom</cell></row><row><cell>#0.293 cold #0.286 dismantling #0.236 souvenirs</cell></row><row><cell>#0.195 display #0.137 selling #0.110 gift ...</cell></row><row><cell>Scaling factors (</cell></row><row><cell>. A typical passage returned by Qap is:</cell></row><row><cell>Edwina Sandys, whose sculptures are installed at</cell></row><row><cell>five United Nations centers around the world. One</cell></row><row><cell>of Winston Churchills 10 grandchildren, her sculp-</cell></row><row><cell>ture Breakthrough made of Berlin Wall pieces has</cell></row><row><cell>been called one of the most important monuments</cell></row><row><cell>constructed on American soil since the Vietnam War</cell></row><row><cell>Memorial.</cell></row></table><note coords="9,385.80,295.05,3.36,8.20"><p>¤</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,70.07,628.40,236.60,7.17;10,70.08,637.52,237.07,7.17;10,70.08,646.40,45.77,7.17" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,221.08,628.40,85.59,7.17;10,70.08,637.52,38.13,7.17">Shortest substring retrieval and ranking</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,117.06,637.52,141.38,7.17">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="78" />
			<date type="published" when="2000-01">January 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.07,655.28,236.60,7.17;10,70.08,664.40,236.39,7.17;10,70.08,673.28,236.15,7.17;10,70.08,682.16,159.37,7.17" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,275.29,664.40,31.18,7.17;10,70.08,673.28,81.86,7.17">Statistical selection of exact answers</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graeme</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kemkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Michae L Laszlo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Egidio</forename><forename type="middle">L</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">L</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tilker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.44,673.28,139.79,7.17;10,70.08,682.16,17.37,7.17">Eleventh Text REtrieval Conference (TREC-2002)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.07,691.28,236.59,7.17;10,70.08,700.16,236.81,7.17;10,70.08,709.28,236.86,7.17;10,70.08,718.16,189.59,7.17" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,295.15,691.28,11.51,7.17;10,70.08,700.16,136.47,7.17">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,222.28,700.16,84.61,7.17;10,70.08,709.28,236.86,7.17;10,70.08,718.16,28.05,7.17">24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.11,60.56,236.84,7.17;10,333.12,69.44,236.59,7.17;10,333.12,78.56,236.83,7.17;10,333.12,87.44,34.03,7.17" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,386.58,69.44,129.98,7.17">Question answering by passage selection</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Egidio</forename><forename type="middle">L</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Terra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,530.88,69.44,38.83,7.17;10,333.12,78.56,112.64,7.17">Advances in Open Domain Question Answering</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="10,333.11,96.32,237.07,7.17;10,333.12,105.44,237.09,7.17;10,333.12,114.32,18.20,7.17" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,422.28,96.32,108.70,7.17">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,546.70,96.32,23.48,7.17;10,333.12,105.44,115.97,7.17">Second Text REtrieval Conference (TREC-2)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.11,123.20,236.61,7.17;10,333.12,132.32,237.11,7.17;10,333.12,141.20,237.05,7.17;10,333.12,150.08,117.68,7.17" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,432.84,123.20,136.88,7.17;10,333.12,132.32,42.16,7.17">Improving two-stage ad-hoc retrieval for short queries</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,397.37,132.32,172.86,7.17;10,333.12,141.20,180.02,7.17">21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.11,159.20,236.82,7.17;10,333.12,168.08,236.61,7.17;10,333.12,176.96,207.08,7.17" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,378.65,159.20,137.90,7.17">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">Joon</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,531.10,159.20,38.83,7.17;10,333.12,168.08,236.61,7.17;10,333.12,176.96,68.80,7.17">20st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.11,186.08,236.63,7.17;10,333.12,194.96,236.86,7.17;10,333.12,204.08,54.40,7.17" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,494.93,186.08,57.27,7.17">Okapi at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,333.12,194.96,148.81,7.17">Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,333.11,212.96,236.83,7.17;10,333.12,221.84,236.18,7.17;10,333.12,230.96,147.39,7.17" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,360.93,221.84,52.24,7.17">At&amp;t at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.00,221.84,139.30,7.17;10,333.12,230.96,5.75,7.17">Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
