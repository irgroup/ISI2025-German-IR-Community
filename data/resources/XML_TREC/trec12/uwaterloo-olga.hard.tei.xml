<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,122.52,75.58,366.93,14.36">Interactive search refinement techniques for HARD tasks</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.68,120.46,80.26,9.94"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Sciences</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.72,120.46,96.96,9.94"><forename type="first">Murat</forename><surname>Karamuftuoglu</surname></persName>
						</author>
						<author>
							<persName coords="1,414.96,120.46,41.35,9.94"><forename type="first">Eric</forename><surname>Lam</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,122.52,75.58,366.93,14.36">Interactive search refinement techniques for HARD tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E8CE72F486CCB31A98BE7A9E481AAF73</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In our entry to the new HARD track, we have investigated two methods of interactively refining user search formulations. One method consists of asking the user to select a number of sentences that may represent relevant documents, and then using the documents, whose sentences were selected for query expansion. The second method is to show to the user a list of noun phrases, extracted from the initial document set, and then expanding the query with the terms from the phrases selected by the user. The results show that the second method is an effective means of interactive query expansion and yields significant performance improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The main goal of the HARD track this year is to explore what techniques could be used to improve search results by using two types of information: (1) extra-linguistic contextual information about the user and the information need, and (2) information dynamically provided by the user in response to topic clarification questions.</p><p>We decided to focus this year on designing techniques for dynamically eliciting additional topic-oriented search criteria from the users, and using their feedback in the second-stage search iterations. We submitted one baseline run, two clarification forms and two final runs. The following subsections describe the system design for each of the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System description 1 Baseline run</head><p>For all of our submitted runs we used Okapi BSS (Basic Search System). For the baseline run UWAThard1 we used keywords only from the title fields of topics, as these proved to be most effective in our preliminary experiments described in section 2.2.3. The topic titles were parsed in Okapi, weighted and searched using BM25 function against the HARD track corpus. We used a GSL file, constructed on the basis of past TREC data, and comprised of 222 stopwords, 254 semi-stopwords (terms that are indexed, but not used in blind or relevance feedback), 58 phrases and 432 synonym groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">2 Query expansion method 1</head><p>According to the track specifications, a clarification form for each topic must fit into a 1152 x 900 screen and the user may spend no more than 3 minutes filling out each form. We have evaluated two clarification forms, the first of which -CF1 -is described in this section.</p><p>In brief, our first approach to eliciting user feedback was to take N top-ranked documents from the baseline run, select one sentence per document, include it in the clarification form CF1, and ask the user to select all sentences that possibly represent relevant documents.</p><p>The goal that we aim to achieve with the aid of the clarification form CF 1 is to have the users judge as many relevant documents as possible on the basis of one sentence per document. The main research questions that we explored here were: RQ1: What is the error rate in selecting relevant documents on the basis of one sentence representation of its content? RQ2: How does sentence-level relevance feedback affect retrieval performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Sentence selection</head><p>In more detail the sentence selection algorithm consists of the following steps: From the baseline run, we take N top-ranked documents. Given the screen space restrictions, we could display only 15 three-line sentences, hence N=15. The full-text of each of the documents is split into sentences. For every sentence that contains one or more query terms, i.e. any term from the title field of the query topic, two scores are calculated: S1 and S2.</p><p>Sentence selection score 1 (S1) is the sum of idf of all query terms present in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence selection score 2 (S2):</head><p>Where: W i -Weight of the term i, see (3); f s -length normalisation factor for sentence s, see (4).</p><p>The weight of each term in the sentence, except stopwords, is calculated as follows:</p><p>Where: idf i -inverse document frequency of term i in the corpus; tf i -frequency of term i in the document; tmax -tf of the term with the highest frequency in the document.</p><formula xml:id="formula_0" coords="2,261.96,449.10,278.56,100.41">= q idf S1 s i f W S = 2 (1)<label>(2)</label></formula><p>)</p><formula xml:id="formula_1" coords="2,224.64,630.87,315.88,29.05">) max * 5 . 0 ( 5 . 0 ( t tf idf W i i i + =<label>(3)</label></formula><p>To normalise the length of the sentence we introduced the sentence length normalisation factor f:</p><p>Where: smax -the length of the longest sentence in the document, measured as a number of terms, excluding stopwords; slen -the length of the current sentence.</p><p>All sentences in the document were ranked by S1 as the primary score and S2 as the secondary score. The rationale of our approach to sentence ranking is to pre-select, first, the sentences that contain more query terms, and therefore are more likely to be related to the user's query formulation, and secondly, from this pool of sentences to select the one which is more content-bearing and central to the topic of the document, i.e. which contains a higher proportion of terms with high tf*idf weights.</p><p>Next, since we are restricted by the screen space, we reject sentences that exceed 250 characters, i.e. three lines. In addition, to avoid displaying very short, and hence insufficiently informative sentences, we reject sentences with less than 6 non-stopwords. If the top-scoring sentence does not satisfy the length criteria, the next sentence in the ranked list is considered to represent the document.</p><p>Finally, since there are a number of almost identical documents in the corpus, we remove the representations of the duplicate documents from the clarification form using pattern matching, and process the necessary number of additional documents from the baseline run sets.</p><p>By selecting the sentence with the query terms and the highest proportion of high-weighted terms in the document, we are showing query term instances in their typical context in this document. Usually, but not always, a term is only used in one sense in the same document. Also, in many cases it is sufficient to establish the linguistic sense of a word by looking at its immediate neighbours in the same syntactic unit (i.e. a sentence or a clause). Based on this, we hypothesise that users will be able to filter out those sentences, where the query terms are used in an unrelated linguistic sense. We, however, recognise that it is more difficult, if not impossible, for users to reliably determine the relevance of the document on the basis of one sentence in those cases where the relevance of the document to the query is due to more subtle aspects of the topic, which are not evident from one sentence.</p><p>We evaluated CF1 on Financial Times and Los Angeles Times corpora from TREC volumes 4 and 5, and ad hoc topics 301-350. Forms were filled in by the authors. The average precision of user-selected sentences, calculated as the number of relevant sentences selected by the user out of the total number of sentences selected by the user, was 0.70 in our experiments. The average recall, calculated as the number of relevant sentences selected by the user out of the total number of relevant sentences shown in the clarification form, was 0.64. The average number of relevant documents shown was 5.36; average number of relevant selected sentences -3.44; non-relevant selected -1.44</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Query expansion algorithm</head><p>The user's feedback to clarification form 1 is used for obtaining query expansion terms for the final run. Our approach to query expansion is to identify collocates of query terms -words co-occurring within a limited span with query terms. Previous query expansion experiments with long-span collocates of query terms obtained from 5 known relevant documents showed 72-74% improvement over the use of title only query terms on the Financial Times (TREC volume 4) corpus with TREC-5 ad hoc topics <ref type="bibr" coords="3,468.37,701.74,11.79,9.94" target="#b3">[4]</ref>.</p><formula xml:id="formula_2" coords="3,271.20,109.61,269.08,29.62">s s slen s f max =<label>(4)</label></formula><p>For the HARD track experiments we slightly modified our collocate extraction and selection algorithm. Instead of using fixed-size windows around instances of query terms to extract collocates, we define a window as one or more sentences surrounding the query term occurrence. The span of the window is measured as the number of sentences to the left and right of the sentence containing the instance of the query term. For example, span 0 means that only terms from the same sentence as the query term are considered as collocates, span 1 means that terms from 1 preceding and 1 following sentences are also considered as collocates.</p><p>In more detail the collocate extraction and ranking algorithm is as follows: Each document judged relevant is split into sentences. For each query term we extract all sentences containing its instance, plus s sentences to the left and right of these sentences, where s is the span size. If s&gt;0 we may have overlapping windows and extract the same sentence several times. To avoid this we keep the record of the sentences already extracted, so that each sentence is only extracted once.</p><p>After all required sentences are selected we extract stems from them, discarding stopwords. For each unique stem we calculate the Z score to measure the significance of its co-occurrence with the query term as follows:</p><p>Where: f r (x,y) -frequency of x and y occurring in the same windows in the known relevant document set (R); f c (y) -frequency of y in the corpus; f r (x) -frequency of x in the relevant documents; v x (R) -average size of windows around x in the known relevant document set (R); N -corpus size in words.</p><p>The joint frequency of x and y -f r (x,y) is calculated as the product of f(x) and f(y) in that window.</p><p>All collocates with an insignificant degree of association: Z&lt;1.65 are discarded, see <ref type="bibr" coords="4,455.17,494.02,11.79,9.94" target="#b1">[2]</ref>. The remaining collocates are sorted by their Z score.</p><p>After we obtain sorted lists of collocates of each query term, we select those collocates for query expansion, which co-occur significantly with two or more query terms. First, for each collocate the collocate score (C1) is calculated:</p><p>Where: n i -rank of the collocate in the z-sorted collocation list for the query term i; W i -weight of the query term i.</p><p>The reason why we use the rank of the collocate in the above formula instead of its Z score is because Z scores of collocates of different terms are not comparable. Finally, we rank collocates by two parameters:</p><formula xml:id="formula_3" coords="4,210.84,302.30,155.21,298.10">) ( ) ( ) ( ) ( ) ( ) ( ) , ( R v x f N y f R v x f N y f y x f Z x r c x r c r - = = i i W n C1</formula><p>(5) (6)</p><p>(1) the number of query terms they co-occur with;</p><p>(2) C1 score.</p><p>Top k collocates in the ranked list are added to the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Evaluation</head><p>We evaluated the algorithm with blind feedback, trying to find the optimal values for R -the size of the pseudo-relevant set, s -the span size, and k -the number of query expansion terms. The results indicate that variations of these parameters do not have significant effect on precision. However, some tendencies were observed, namely: (1) larger R values tend to lead to poorer performance in both Title and Title+Desc. runs; (2) larger span sizes also tend to degrade performance in both Title and Title+Desc runs.</p><p>The Title-only unexpanded run was 10% better than Title+Description. Similarly, the expansion of Titleonly queries performed better than expansion of Title+Description queries. For example, AveP of the worst Title+Desc expansion run (R=50, s=4, k=40) is 23% worse than the baseline, and AveP of the best run (R=5, s=1, k=10) is 8% better than the baseline. AveP of the worst Title expansion run (R=50, s=5, k=20) is 4.5% worse than the baseline, and AveP of the best Title expansion run (R=5, s=1, k=40) is 10.9% better than the baseline.</p><p>Based on this data we decided to use terms only from the Title section of the topics for the official runs, and, given that values k=40 and s=1 contributed to a somewhat better performance, we used these values in all of our official expansion runs. The question of R value is obviously irrelevant here, as we used all documents selected by users in the clarification form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query expansion method 2</head><p>The second user feedback mechanism that we evaluated consists of automatically selecting noun phrases from the top-ranked documents retrieved in the baseline run, and asking the users to select all phrases that contain possibly useful query expansion terms. The research questions explored here were: RQ3: Do noun phrases provide sufficient context for the user to select potentially useful terms for query expansion? RQ4: How does query expansion based on user-selected phrases affect retrieval performance?</p><p>We take top 25 documents from the baseline run, and select 2 sentences per document using the algorithm described above. We then apply Brill's rule-based tagger <ref type="bibr" coords="5,337.69,573.46,12.92,9.94" target="#b0">[1]</ref> and BaseNP noun phrase chunker <ref type="bibr" coords="5,514.21,573.46,12.92,9.94" target="#b2">[3]</ref> to extract noun phrases from these sentences. The phrases are then parsed in Okapi to obtain their term weights, removing all stopwords and phrases consisting entirely of the original query terms. The remaining phrases are ranked by the sum of weights of their constituent terms. Top 78 phrases are then included in the clarification form for the user to select. This is the maximum number of phrases that could fit into the clarification form.</p><p>All user-selected phrases were split into single terms, which were then used to expand the original user query. The expanded query was then searched against the HARD track database using BM25 function for document retrieval and BM250 for passage retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document-level evaluation</head><p>The document-level results of the three official runs are presented in table 1. UWAThard1 is the baseline run using original query terms from the topic titles. UWAThard2 is an experimental run using query expansion method 1 plus the granularity and known relevant documents metadata. UWAThard3 is an experimental run using query expansion method 2 plus the granularity metadata. UWAThard2 did not achieve statistically significant improvement over the baseline. This finding did not correspond to our initial test runs with the FT and LA collections, which showed 21% improvement over the original title-only query run. The analysis of the numbers of relevant and non-relevant sentences selected by the annotators showed slightly better results of 0.73 in average precision and 0.69 in average recall, compared to the selections that we made from clarification forms based on the FT and LA collections (see section 2.2.1). On average 7.14 relevant sentences were included in the forms. The annotators on average selected 4.9 relevant and 1.8 non-relevant sentences. Figure <ref type="figure" coords="6,438.84,441.94,5.52,9.94" target="#fig_0">1</ref> shows the number of relevant/non-relevant selected sentences by topic. It is not clear why query expansion method 1 performed worse in the official UWAThard2 run compared to the test run on FT and LA collection, given very similar numbers of relevant sentences selected. Corpus differences could be one reason -HARD corpus contains a large proportion of governmental documents, and we have only evaluated our algorithm on newswire corpora. More experimentation is needed to determine the effect of the governmental documents on our query expansion algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>In addition to clarification forms, we used the known relevant documents metadata for UWAThard2. We need to conduct more experiments without this metadata to determine how much it contributed to the performance of our query expansion method.</p><p>Our second query expansion run (UWAThard3) was among the best runs in the track, gaining an 18% improvement over the baseline in average precision in soft-doc evaluation and 26.4% in hard-doc evaluation, both of which are statistically significant (using t-test at .05 significance level). These are the highest improvements over the baseline among the top 50% highest-scoring baseline runs submitted to the track. Query expansion method 2 achieved lower performance gains in our training runs on FT and LA collections, which can be explained by the lower number of phrases selected. LDC annotators selected on average 19 phrases, whereas we selected on average 7 phrases in the training runs. This suggests that selecting more phrases leads to a notably better performance. The reason why we selected fewer phrases than the LDC annotators could be due to the fact that on many occasions we were not sufficiently familiar with the topic, and could not determine how an out-of-context phrase is related or not related to the topic. The LDC annotators are more familiar with the topics, which they have formulated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage-level evaluation</head><p>Passage-level evaluation results are given in table 2. UWAThard3 showed 27% improvement in Rprecision over UWAThard1, while UWAThard2 -23%. Such big difference between the expansion runs and the baseline was expected, since we only did document-level retrieval for the baseline run. All our runs were above the median in all passage-level measures </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and future work</head><p>This year we experimented with two user-assisted search refinement techniques:</p><p>(1) inviting the user to select from the clarification form a number of sentences that may represent relevant documents, and then using the documents whose sentences were selected for query expansion.</p><p>(2) showing to the user a list of noun phrases, extracted from the initial document set, and then expanding the query with the terms from the user-selected phrases.</p><p>Comparison with other submissions to the HARD track (88 in total) shows that our submitted runs are above the median in all official evaluation measures. The second query expansion method is more promising than the first, and was among the best runs this year, achieving statistically significant improvement of 18% (soft-rel) and 26% (hard-rel) over the baseline.</p><p>In this year's entry we focused on utilising the user's feedback to clarification forms plus granularity and known relevant documents metadata. For the next year's entry, we plan to address other metadata: genre, familiarity and purpose.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,133.20,390.22,345.48,9.94;7,115.20,110.60,388.80,264.48"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Sentences selected by LDC annotators from the clarification form 1.</figDesc><graphic coords="7,115.20,110.60,388.80,264.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,72.00,197.53,465.69,153.19"><head>Table 1 .</head><label>1</label><figDesc>Document-level evaluation results</figDesc><table coords="6,72.00,197.53,465.69,126.33"><row><cell></cell><cell>Run description</cell><cell cols="2">Soft relevance</cell><cell cols="2">Hard relevance</cell></row><row><cell></cell><cell></cell><cell>Precision</cell><cell>Average</cell><cell>Precision</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell>@ 10</cell><cell>Precision</cell><cell>@ 10</cell><cell>Precision</cell></row><row><cell></cell><cell>Original title-only query</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UWAThard1</cell><cell>terms; BM25 used for all</cell><cell>0.4875</cell><cell>0.3134</cell><cell>0.3875</cell><cell>0.2638</cell></row><row><cell></cell><cell>topics</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Query expansion method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UWAThard2</cell><cell>1; granularity and rel.docs</cell><cell>0.5479</cell><cell>0.3150</cell><cell>0.4354</cell><cell>0.2978</cell></row><row><cell></cell><cell>metadata</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UWAThard3</cell><cell>Query expansion method 2; granularity metadata</cell><cell>0.5958</cell><cell>0.3719</cell><cell>0.4854</cell><cell>0.3335</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,116.16,536.29,348.08,72.19"><head>Table 2 .</head><label>2</label><figDesc>Passage-level evaluation results</figDesc><table coords="7,116.16,536.29,348.08,45.33"><row><cell>Run</cell><cell>Passage P@10</cell><cell>R-Precision</cell><cell>F(30)</cell></row><row><cell>UWAThard1</cell><cell>0.2668</cell><cell>0.1908</cell><cell>0.1255</cell></row><row><cell>UWAThard2</cell><cell>0.3305</cell><cell>0.2359</cell><cell>0.1454</cell></row><row><cell>UWAThard3</cell><cell>0.3617</cell><cell>0.2426</cell><cell>0.1559</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This material is based on work supported in part by <rs type="funder">Natural Sciences and Engineering Research Council of Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,89.88,351.58,450.10,9.94;8,72.00,364.18,384.12,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,132.00,351.58,407.98,9.94;8,72.00,364.18,141.01,9.94">Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging</title>
		<author>
			<persName coords=""><forename type="middle">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,220.68,364.18,115.70,9.94">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="565" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.44,382.90,451.52,9.94;8,72.00,395.50,468.19,9.94;8,72.00,408.10,135.96,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,279.24,382.90,150.43,9.94">Using statistics in lexical analysis</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hindle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,450.24,382.90,89.72,9.94;8,72.00,395.50,202.06,9.94">Lexical Acquisition: Using On-line Resources to Build a Lexicon</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Zernik</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Elbraum Associates</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="115" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.56,426.82,451.42,9.94;8,72.00,439.42,282.36,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,206.64,426.82,241.05,9.94">Text Chunking Using Transformation-Based Learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,456.12,426.82,83.86,9.94;8,72.00,439.42,226.23,9.94">Proceedings of the Third ACL Workshop on Very Large Corpora, MIT</title>
		<meeting>the Third ACL Workshop on Very Large Corpora, MIT</meeting>
		<imprint>
			<date type="published" when="1995-06">June, 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,89.04,458.14,450.96,9.94;8,72.00,470.74,153.12,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,284.16,458.14,194.71,9.94">Query expansion with long-span collocates</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,487.44,458.14,52.56,9.94;8,72.00,470.74,38.88,9.94">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="273" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
