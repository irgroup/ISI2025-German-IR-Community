<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.44,82.79,298.45,13.97">THUIR in TREC2003: HARD Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,191.52,123.43,38.35,9.16"><forename type="first">Liang</forename><surname>Ma</surname></persName>
							<email>maliang00@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.62,123.43,33.04,9.16"><forename type="first">Wei</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.42,123.43,54.16,9.16"><forename type="first">Qunxiu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.76,123.43,56.51,9.16"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab of Intelligent Tech. &amp; Sys</orgName>
								<orgName type="department" key="dep2">CS&amp;T Dept</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">TRS Open Software Laboratory</orgName>
								<orgName type="institution">Beijing Information Technology Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,172.02,139.03,46.76,9.16"><forename type="first">Shuicai</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">TRS Open Software Laboratory</orgName>
								<orgName type="institution">Beijing Information Technology Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,229.14,139.03,48.65,9.16"><forename type="first">Shibin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">TRS Open Software Laboratory</orgName>
								<orgName type="institution">Beijing Information Technology Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.60,139.03,62.75,9.16"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">TRS Open Software Laboratory</orgName>
								<orgName type="institution">Beijing Information Technology Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,362.94,139.03,63.82,9.16"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main" coord="1,148.44,82.79,298.45,13.97">THUIR in TREC2003: HARD Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B08F44C87B0CC2857C61A274C7228663</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe ideas and related experiments of Tsinghua University IR group in TREC-12 HARD Track. In this track, we focus on an automatic delivering mechanism, which combine the existing IR methods and can provide a quick retrieval solution for the practical environment. The final official evaluation show the old ways perform not well, but we think the experiment data will be helpful in evaluating the new ideas developed by other teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Brief Introduction</head><p>As a new evaluation track, HARD is designed to find an effective way to locate the search focus precisely from the data coming from the user, including his/her additional information (such as the he/his background) and an interactive input, so as to provide better retrieval result to the original query request.</p><p>In this track, the key issue is to find the real search focus. There can be two ways to finish it: manually and automatic way. Though the former usually can provide a satisfied performance, we think the automatic way is more useful in practical use and thus try to devote us in this way.</p><p>In following sections, we introduce what we did in our research work and give the final evaluation result. Some further research work done after final TREC submit is also listed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Construct Baseline Run</head><p>We get our baseline run (only with document) using the initial query by a BM25 TF*IDF scoring schema. It is a popular method that is fast and practical. The special treatment is only used for initial query: For each topic, the query is constructed simply by the task description (The detail restriction for none-relevant document are ignored). For the search items, different weights are set according to their location(such as description field) and importance in the task description. Also, there is no positive training documents are used to refine the query, because usually the training resource is unlikely to be provided for various immediate search requirements in Web IR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Focus Probe</head><p>In focus probing, we try to find the search focus of the user input. In this period, there are two missions we did:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Finding potential search items</head><p>In our Clarification Form, all the potential search issues to be confirmed by user are listed with checkbox, together with a text field to fill if he/she finds there are something we missed. These search terms are presented as some keywords or phrases instead of long statements extracted from web pages. Some existing technologies, such as complex passage analysis or do self-learning from related training resources, seems to be good ideas but time-consuming. Here we choose a fast mechanism to extract them automatically. They are got from two ways follow:</p><p>(1) The kernel words/phrases in topic description. We parse the description and get presentive words/phrase set from each fields, then all the set are combined and those words/phrase existing in multi set are thought as kernel words/phrases. (2) Terms with high statistical weight in top-100 ranked documents in search result. But only the terms in the title and the first paragraph(not the whole passage) are calculated, for there should be more focus-related words in these two sections. To keep the search deviation under control, we limit the potential search terms up to 10 issues. Compared to other methods, our idea is efficient in finding the potential search terms, also it doesn't require any training resource, therefore it is feasible when applied in a practical use, but the accuracy of this method has not been proved to be very satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Locate the Desired Focus</head><p>We locate the desired focus from:</p><p>(1) Returned Clarification Form from LDC. Since the returned Clarification Form has been processed by search user, all the words/phrases in selected checkbox and the content filled in the additional text field are thought as desired search focus. (2) The searchitem filed in metadata. Only this field in metadata is used to provide short search terms, other fields are all ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Refine the Query</head><p>Based on the initial queries used in the baseline run, we improve them using the desired focus newly located. But different update styles are used according to how the focuses are located.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Focus from returned Clarification Form</head><p>The words/phrases in each selected checkbox and the filled content in return CF are thought as one search focus. Based on the kernel terms in initial query and the current search item, a sub-query is constructed for a specific search focus. Then the initial query is divided into several queries for different search focus.</p><p>(2) Focus from searchitem field All the search terms in the searchitem field are simply added to the initial query as new weighted terms. They are merged using Rocchio-like feedback mechanism. From the above improvement, we construct the search query for the final run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Refine the Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Return type detection</head><p>For various topics, the user want to receive different search result: document, passage and sentence. We decide the return type by following rules:</p><p>We return document if topic require so.</p><p>For passage and sentence, we usually return the result based on paragraph. For passage, usually there are one or two paragraphs are included. For sentence, it is nearly impossible to present an efficient result in such rough retrieval, therefore a paragraph will be more meaningful.</p><p>If any type is welcomed, we analyze the topic description and decide the result should be passage or document. For example, if there are some words ' the document shouldâ€¦.'in the description, then we think a document should be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Paragraph level indexing</head><p>For test corpus, we built an index based on the document level. Since the paragraph will also be returned, we create a new index based on the paragraph. Each paragraph in the document is taken as a single passage and indexed. For some short paragraphs, we merge them to the neighbor paragraphs until the length of this paragraph to be indexed is large than average paragraph length of the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Result merging for final submission</head><p>All the improved queries are submitted in the document index or paragraph index according to their return type. For topics that return passage and sentence, we also do the retrieval work in the document index. Before getting the final result, we do the following work to the scored list:</p><p>Merge by sub-query: For the topics which have sub-queries presenting different search focus, the final retrieval result is the combination of all of sub-queries, and the scored item is ranked as their order in baseline run(for passage and sentence, they use the order of their host document ). Document detection for passage and sentence: we return paragraph when topic require a passage or sentence. To keep out of the noise paragraphs, for a retrieved relevant paragraph, only its host document is also ranked as the topic-relevant that can we set it to the returned final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Final Submission and Evaluation</head><p>We finally submitted three runs which expand query using different data source ( but with same weighting/scoring parameters for query). For each run, the detail parameters and its evaluation result(also include the baseline run) are listed in table <ref type="table" coords="3,364.86,616.33,5.25,9.16" target="#tab_0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>The evaluation result tell us the clarification form, in lifting the query precision, work better than the metadata. Some of our work later on constructing clarification form using certain cluster algorithm provided us more satisfied result. Also we noticed the result merge seems an effective tool, especially in small amount of documents returned.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,133.56,616.33,321.52,142.66"><head>Table 1 .</head><label>1</label><figDesc>and table 2. Parameters Setup in Each Run</figDesc><table coords="3,133.56,663.61,321.52,95.38"><row><cell>Run</cell><cell>Fields used in task</cell><cell>Use</cell><cell>Use</cell><cell>Merge</cell></row><row><cell></cell><cell>description</cell><cell>Clarification Form</cell><cell>Metadata</cell><cell>Result</cell></row><row><cell>Baseline Run</cell><cell>Title</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TUCSHARD1</cell><cell>Description</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>TUCSHARD2</cell><cell>Narrative</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>TUCSHARD3</cell><cell></cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,133.56,92.37,320.02,137.78"><head>Table 2 .</head><label>2</label><figDesc>The TREC Evaluation Result for Each Run</figDesc><table coords="4,133.56,108.31,320.02,121.84"><row><cell>Run</cell><cell></cell><cell cols="2">Evaluation</cell><cell></cell></row><row><cell></cell><cell cols="2">Passage level</cell><cell cols="2">Document level</cell></row><row><cell></cell><cell>R-Precision</cell><cell>F-score at</cell><cell cols="2">R-Precison</cell></row><row><cell></cell><cell></cell><cell>100 passage</cell><cell>Hard-rel</cell><cell>Soft-rel</cell></row><row><cell>Baseline Run</cell><cell>0.1235</cell><cell>0.1294</cell><cell>0.1960</cell><cell>0.2560</cell></row><row><cell>TUCSHARD1</cell><cell>0.1868</cell><cell>0.1396</cell><cell>0.2148</cell><cell>0.2818</cell></row><row><cell>TUCSHARD2</cell><cell>0.1655</cell><cell>0.1296</cell><cell>0.2012</cell><cell>0.2627</cell></row><row><cell>TUCSHARD3</cell><cell>0.1868</cell><cell>0.1396</cell><cell>0.2138</cell><cell>0.2711</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,94.38,408.14,410.96,9.75;4,111.00,423.74,118.97,9.75" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,303.31,408.14,91.03,9.75">Keenbow at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">/</forename><surname>Okapi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,416.31,408.14,89.03,9.75;4,111.00,423.74,84.66,9.75">Proceedings of the TREC-8. Maryland</title>
		<meeting>the TREC-8. Maryland</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.95,439.34,411.36,9.81;4,111.00,454.94,394.23,9.75;4,111.00,470.54,58.23,9.75" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="4,443.41,439.34,61.91,9.75;4,111.00,454.94,313.96,9.75">HARD Track Overview in TREC 2003 High Accuracy Retrieval from Documents</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<idno>of TREC-2003</idno>
		<imprint>
			<publisher>Microsoft Research Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.95,486.14,411.43,9.81;4,111.00,501.74,75.17,9.75" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,160.71,486.14,193.34,9.75">Using Coreference in Question Answering</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,373.91,486.14,131.47,9.75;4,111.00,501.74,40.90,9.75">Proceedings of the TREC-8. Maryland</title>
		<meeting>the TREC-8. Maryland</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.95,517.34,411.40,9.81;4,111.00,532.94,315.18,9.75" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,311.32,517.34,194.03,9.75;4,111.00,532.94,85.81,9.75">Structuring and Expanding Queries in the Probabilistic Model</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Honma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,216.71,532.94,125.45,9.75">Proceedings of the TREC-9</title>
		<meeting>the TREC-9</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,94.38,548.54,410.80,9.75;4,111.00,564.14,394.21,9.75;4,111.00,579.74,123.10,9.75" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,285.49,548.54,219.69,9.75;4,111.00,564.14,113.00,9.75">An interface for navigating clustered document sets returned by queries</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Obry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,244.72,564.14,260.49,9.75;4,111.00,579.74,91.42,9.75">Proceedings of the ACM Conference on Organizational Computing Systems</title>
		<meeting>the ACM Conference on Organizational Computing Systems</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
