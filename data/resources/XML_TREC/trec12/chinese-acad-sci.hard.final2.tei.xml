<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.80,111.13,300.06,19.27">TREC12 HARD Track at ISCAS</title>
				<funder ref="#_KCnKgN9">
					<orgName type="full">China 863 Project</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Fund of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,219.48,144.47,34.63,9.19"><forename type="first">Zeng</forename><surname>Wu</surname></persName>
							<email>wuzeng01@iscas.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.26,144.47,26.77,9.19"><forename type="first">Lin</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.61,144.47,27.16,9.19"><forename type="first">Le</forename><surname>Sun</surname></persName>
							<email>sunle@iscas.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.96,144.47,41.45,9.19"><forename type="first">Shiwei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.80,111.13,300.06,19.27">TREC12 HARD Track at ISCAS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">75407D9063B6810D7EF085A6395CFA04</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TREC12</term>
					<term>HARD</term>
					<term>Boolean</term>
					<term>statistical model</term>
					<term>gradient decrease</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Statistical model in retrieval has been shown to perform well empirically. Extended Boolean model has been widely used in business system for its easiness to be complemented and not bad results. In this paper, a statistical model and modified Boolean model and natural language processing techniques, shallow query understanding techniques are used and results show that even with very limited training corpus, an appropriate statistical model can greatly improve the performance. .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The HARD, which means high accuracy retrieval from documents, is a new track in TREC12. The goal of HARD is to achieve high accuracy retrieval from documents by leveraging additional information about researcher and/or the search context, through techniques such as passage retrieval, and using very targeted interaction with the searcher.</p><p>The key point of this track is to choose the most relevant text "granularity", the difficulty in choosing which may lead to that Ad-Hoc Track dwindled in the few last years, according to the purpose and genre metadata in the topic. The granularity may be a total document, a passage, or even a sentence. It is different from traditional full text retrieval. If one part, maybe a passage or several sentences, in a document best suit the topic, but the total score of this document is not high, from the point of HARD, this document should be the best document in the list. For every document in the collection, calculating the relevance between a topic and all the granularity parts individually is ideally. But it will cost so much computation. So the process is divided into two steps as Q/A system does.</p><p>The rest of the paper proceeds as follows. Section 2 outlines some background and related work. Section 3 introduces how to generate query words automatically. Section 4 explains the baseline run. Section 5 presents the final run. The evaluation is concluded in section 6. Section 7 is the conclusion and future work part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>The problem of HARD retrieval system design can be thought of as a problem in the combination of the following steps. The first step is to get key words from the topic, the second is to get relevant by using retrieval model and then to rank them, the last is to locate topic information in the documents. This system is built totally by our site.</p><p>The purpose of previous TREC Ad Hoc track is to increase individual topic effectiveness. To generate query more accurately, some ideas can be learned from that community. In William. S. Cooper <ref type="bibr" coords="2,284.01,168.01,12.31,10.50" target="#b1">[2]</ref>, they come up with such model like the following:</p><formula xml:id="formula_0" coords="2,123.72,194.26,108.08,21.59">∑ = + ≈ 6 1 0 ) , | ( log i i i X c c D Q R O</formula><p>They get coefficient by fitting the equation to the empirical data by means of a logistic regression analysis. (Hosmer &amp; Lemesshow 1989) The statistical clues, X i, are all based on the conventional frequency counts in query and document instead of using thesauri, parsing, phrase discovery, disambiguation, and other natural language processing or AI-like approaches. To the contrary, they felt it is a virtue of regression procedures explored here that they are more hospitable than most to the incorporation additional clues. However, an astute use of simple stem and document frequency information lift s one to a high plateau of effectiveness.</p><p>As mentioned above, search with information about searcher and search context can lift the results. So, the relevance feedback is also studied in this experiment. Relevance feedback, despite its long history in information retrieval research, has not been successfully adopted. The closest feature found in some search systems is "find more documents like this". Query expansion techniques have been used in a number of systems to suggest additional search terms, with limited success. There are many reasons for the apparent failure of relevance feedback. The primary one is the difficulty of getting users to provide the relevance information. Simply providing "relevant" and "not relevant" button in the interface does not seem to provide enough incentive for user. For this reason, researchers are investigating techniques to infer relevance through passive measures such as time spent browsing page or number of links followed from the a page. Another reason is that identifying the correct context is not simple. Experiments <ref type="bibr" coords="2,230.43,499.15,19.52,10.50">[13]</ref> have shown that if a user can indicate relevant sections or even phrases in a document, relevance is more accurate.</p><p>To resolve these problems, the sophisticated interface design and good algorithm for inferring context are required.</p><p>In the second step, there is a lot of literature on approaches to information retrieval, we will not survey them all here. The focus, here, is on the modification of extended Boolean model and the statistical model. The modified Boolean model is used in baseline run.</p><p>The standard Boolean retrieval has following limitations 1) It gives counterintuitive results for certain types of queries.</p><p>2) It has no provision for ranking documents.</p><p>3) During the indexing process, it is necessary to decide whether a particular document is either relevant or non relevant with respect to a given index term. 4) It has no provision for assigning importance factors or weights to query terms.</p><p>The P-norm model is proposed as alternative to the Boolean model. P-norm model has the ability to consider weighted query terms and provides a ranking of retrieved documents in order of decreasing relevance.</p><p>However, the statistical model look information retrieval as a problem in the combination of statistical clues. The design objective is to achieve as high a level of retrieval effectiveness as possible, consistent with reasonable theoretical and computational simplicity. In the final run, a statistical model is constructed. Then the compare between two models is evaluated.</p><p>The TREC Q/A track is designed to take a step closer to information retrieval rather than document retrieval. The researchers in this field mostly use statistical method. Abraham Ittycheriah applied Machine Translation ideas to the Q/A <ref type="bibr" coords="3,459.80,240.91,12.39,10.50" target="#b2">[3]</ref>. Because they have sufficient rules and weights, the answers are created from learning their known question and answer pairs in the open domain.</p><p>In getting the answer to a query, researchers usually use tagging or parsing tools to tag the query, then get the critical information including answer concepts, which are identified by categorizing queries using a method similar in spirit to extracting the named entities <ref type="bibr" coords="3,175.65,327.19,12.36,10.50" target="#b7">[8]</ref>, the named focuses <ref type="bibr" coords="3,284.20,327.19,12.36,10.50" target="#b8">[9]</ref>, and question-answer tokens <ref type="bibr" coords="3,436.16,327.19,17.87,10.50">[10]</ref>. Then the same procedure put on the Documents, and it will cost so much time. This process is always off line. Lastly, using different matching methods to generate the answer.</p><p>In retrieval, a query using the phrase such as "white house" is much more likely to be satisfied by a document using those two words in sequence than by one that has them separately. For some words may have distinctive meaning in the context of another word or in a larger phrase. This approach, however, requires that all sentences, whether in documents or in queries be segmented into phrases. This depends on the identity of the previous word generated. David R. H. Miller <ref type="bibr" coords="3,384.03,440.29,13.61,10.50" target="#b4">[5]</ref> bring forwarded three states Hidden Markov Model to identify two words phrase.. All the models mentioned above are built based on the statistical foundations which mean overwhelming majority of documents paired with relevant queries are available. In practices, it is usually difficult to come by.</p><p>As mentioned above, the HARD track has some familiarity with Ad Hoc track, Q/A track, and Interactive track. Some useful ideas and techniques can be learned from these tracks based on both HARD requirements and the resources available in hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Automatic Construct Key Terms</head><p>To take part in the HARD track, the system is built completely by us on the RedHat Linux platform. To make search on disk file more conveniently Berkeley DB-4.2 is introduced in the system.</p><p>In every topic, the sentence is generated from the &lt;title&gt; field, &lt;descr&gt; field and &lt;narr&gt; field. Then use Brill Tagger tool to tag it. In HARD topics, most sentences from &lt;narr&gt; field have such word like "on topic", "off topic". If no such phrases in the sentences, it will have negative words like "neither, nor, no". In this system, the sentences having such words negative sentences is called negative sentences and words extracted from these sentences are called negative words. Others are called positive sentences and words from these sentences are called positive words. After tagging these sentences, the words not tagged as "NNP ", "NN","NNS", "VBD","VBN" are abandoned, except the last word in sentences. For in examining the sample tagged files, the last word, a noun word from human, always is tagged as CD. Some words tagged as ADJ may in fact have some meanings, but limited to our resources, they can not be identified and be thrown off. Now a word list named positive and a negative word list is constructed. In either list, every word is not a stop word and has been stemmed. It has a remark telling it from title field or &lt;narr&gt; field or &lt;descry&gt; field. It is clear that word from title field has more importance in retrieval. As for the same words in the same list or in the two lists, we also include it as if they were different words. The relevance between query q and document d can be calculated according to the following equation freq q is the count of the word occurring in the document. doclength is count of all of the words in document d, n is count of this word occurring in all of the collection , N is the count of all of the document in the collection. To get this equation, the equation is modified according to the document <ref type="bibr" coords="4,435.33,508.57,12.35,10.50" target="#b1">[2]</ref>. W -or w + is the weight of w i in negative list or in the positive list 2) word_location is the offset where the word occur in the document.</p><p>3) positive_c and negative_c, length_c, location_c, qlength_c are our statistical model parameters. But for the limited training corpus by hand, which is only the training topics provided by HARD, and the importance of these five parameters, the last three parameters are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Metadata and Clarification Form</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CF</head><p>The clarification forms contains the following fields. The first field is composed of the title of the topic. The second field is composed of the words extracted from the sentences in the &lt;descr&gt; fields and on-topic words of the &lt;narr&gt; field. The third field is a list of negative words, which are extracted from off-topic section of the &lt;narr&gt; fields. If there is no negative word, this field is empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metadata</head><p>For the tag RELATED-TEXT of the metadata, the relevant words extracted from the documents are added to the queries. If GENRE equals to ADMINSTRATIVE, the documents in HARDGOV corpus is returned. If is I-REACTION, the documents in the HARDGOV is not retrieved. For the metadata PURPOSE, if it is ANSWER, the simple method of Q/A is used. The following section is to do with the circumstance that the GRANULARITY is passage.</p><p>All other tags are not processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.Baseline run</head><p>In our baseline run, assign positive_c = negative_c = 1; For a certain topic, we rank the document according to the followings:</p><p>1) Calculate the weight according equation ( <ref type="formula" coords="5,335.50,287.35,4.53,10.50">1</ref>) 2) Ignore the document whose weight less than 0; 3) Rank the document according weight got in 1) 4) If the count of ranked documents is more than 1000, choose the first 1000 documents 5) For each document from ranked highest to ranked lowest, get the raw document content. For every word in positive list, the first location where the word occurred is the offset value shown in results file. The length is document length minus offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.Final run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">train positive_c, negative_c .</head><p>There is a statistical relation between the topic-document relevance and total positive word weights, negative word weights, words location, the context of words in relevant document, which can be used when the metadata granularity is passage. Because having involved the document length in get words weight, the normalization is not considered in this step. But there is only document and topic No in training relevance document and no other resources are available, so the model is simplified to two parameters as mention above.</p><p>In training relevance document, the document is remarked as 1 or 0.5 or 0, which display the document is hard-relevant, soft-relevant and non-relevant. So document remarked as 1 is more relevant than remarked as 0.5 and 0. It is the same with the document remarked with 0.5 and 0. Then to get such expressions like the following For every certain topic wi &gt; j i wj , ∀ When document i is remarked as 1, as HARD marked, document j is remarked 0.5 or 0.</p><p>When document i is remarked as 0.5, as HARD marked, document j is remarked 0.</p><p>We can simply write W in equation 1 as</p><formula xml:id="formula_1" coords="5,124.92,703.49,270.09,19.55">i i i w w c negative c positive w - + - = * _ * _</formula><p>Because positive_c and negative_c is constant so the constant term is integrated in the two sides of equation, then 0</p><formula xml:id="formula_2" coords="6,160.80,133.54,200.02,19.11">* _ * _ &gt; - - + w w c negative c positive (2)</formula><p>For one topic a list of such equations is got, and for total topics, it consist of a complete list of equations .Now a appropriate value for positive_c and negative_c is set to make expression (2) true in training corpus. We use Gradient Decrease Algorithm, commonly used in numerical calculation to get them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">locate the information</head><p>For all the words in the positive list and the negative list, the place of the first sentence which obtain the positive word is the offset value required in result file. The offset is the file length minus the offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">work done especially for the request of some metadata</head><p>If GENRE equals to ADMINSTRATIVE, the documents in HARDGOV corpus is returned. If is I-REACTION, the documents in the HARDGOV is not retrieved.</p><p>For the tag RELATED-TEXT of the metadata, the words only already in the word list is extracted from the documents are added to the queries ignoring the fact that it has been in the list.</p><p>If the tag GRANULARITY is passage, the retrieval processing is composed of two stages: document retrieval and passage-level ranking. The document retrieval first gets all the relevant documents. Initially, the summary of a document is zero. From top passage to end one, if it contains a word in the positive list, the sum is added with one. If a passage contains a word in the negative list, the score is decreased with one. In the end, the sum of every passage is acquired. Then the maximum of them divide by the doc length is the new score of the doc. Then the doc list is ranked according to the new score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.Evaluation and Results</head><p>The Hard-rel judgment means that the document is relevant and it satisfies the appropriate metadata. The Soft-rel judgment means that document is relevant to the topic but that it does not satisfy the appropriate metadata. It either does not satisfy the PURPOSE, GENRE, or the FAMILIARITY items (the others are not document-level items).</p><p>In constructing the model, we do not count in the idf value in the topic, as the formula * show. For we think our model is a modified Boolean model and the word is noun in the sentences, which has a substantial meaning. The more they occur, the more important they are. And we have a desire to see what is happening without obeying classical theory. But from the tables, this thought does not accord with the fact.</p><p>There is a relation between the first score and last score in the retrieval, but we divide it subjectively.</p><p>In re-scoring the doc, there should be a similar expression with the expression 1. But results are even worse when we manually check them. The reason is that the amount of the sample points is not sufficient. For the same reason, the parameter in form 1 is not precise enough. The model cannot satisfyingly predict the future.</p><p>The training corpus in our site is nothing but the corpus provided by the HARD, so to make parameter trained precise enough, only four topics are chosen for testing the results. It is not sufficient. Document level retrieval results. The following is an operational definition of passage recall and precision as used in the evaluation. For each relevant passage allocate a string representing all of the character positions contained within the relevant passage (i.e., a relevant passage of length 100 has a string of length 100 allocated). Each passage in the retrieved set marks those character positions in the relevant passages that it overlaps with. A character position can be marked at most once, regardless of how many different retrieved passages contain it. (Retrieved passages may overlap, but relevant passages do not overlap.) The passage recall is then defined as the average over all relevant passages of the fraction of the passage that is marked. The passage precision is defined as the total number of marked character positions divided by the total number of characters in the retrieved set. The F score is defined in the same way as for documents, assigning equal weight to recall and precision: F = (2*prec*recall)/(prec+recall) where F is defined to be 0 if prec+recall is 0. We included the F score because set-based recall and precision average extremely poorly but F averages well. R-precision also averages well.</p><p>In all of the above, a document is treated as a (potentially long) passage. That is, for topics where the granularity is "document" the relevant passage starts at the beginning of the document and is as long as the document. (These are represented in the judgment file as passages with -1 offset and -1 length, but are treated as described above.) For any topic, a retrieved document (i.e., where offset and length are -1) is again just a passage with offset 0 and length the length of the document.</p><p>Using the above definition of passage recall, passage recall and standard document level recall are identical when both retrieved and relevant passages are whole documents. That is not true for this definition of passage precision. Passage precision will be greater when a shorter irrelevant document is retrieved as compared to when a longer irrelevant document is retrieved. This makes sense, but is different from standard document level precision.</p><p>The following table is passage level results. From the tables, the statistical elements partly overcome the model default in the baseline run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future work</head><p>The statistical model is effective, for using the same system, the latter results are twice better as much as the former.</p><p>The idf value is important whenever using any kind of retrieval model. It at least does not do any bad to the results.</p><p>Given more time and hands and more corpus, the equation (1) can be expanded in containing such elements as the doc length, query length, the word location occurring in the doc. And we will use Conjunctive Gradient Decrease or use MLP, using simulate anneal to relieve local minimum. From the contrast of the baseline run and final run, we are sure of performing better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,171.00,346.69,29.61,10.66;4,140.40,346.69,26.97,10.66;4,128.94,346.69,5.26,10.66;4,88.74,346.69,38.83,10.66;4,460.86,310.57,38.83,10.66;4,431.52,310.57,24.35,10.66;4,437.76,293.65,46.72,10.66;4,408.78,301.21,5.26,10.66;4,368.58,301.21,38.83,10.66;4,336.18,301.21,29.61,10.66;4,313.56,301.21,17.10,10.66;4,302.10,301.21,5.26,10.66;4,269.40,301.21,29.61,10.66;4,227.82,301.21,5.26,10.66;4,186.78,301.21,40.13,10.66;4,145.02,302.23,5.26,10.66;4,106.44,302.23,36.85,10.66;4,88.92,302.23,7.90,10.66;4,249.90,301.65,10.16,13.71;4,167.04,301.65,10.16,13.71;4,164.34,346.69,5.92,10.66;4,134.40,346.69,5.92,10.66;4,122.10,346.69,5.92,10.66;4,454.14,310.57,5.92,10.66;4,414.24,301.21,5.92,10.66;4,401.94,301.21,5.92,10.66;4,329.46,301.21,5.92,10.66;4,307.62,301.21,5.92,10.66;4,295.26,301.21,5.92,10.66;4,233.34,301.21,5.92,10.66;4,220.98,301.21,5.92,10.66;4,150.48,302.23,5.92,10.66;4,138.12,302.23,5.92,10.66;4,494.58,298.41,6.49,14.50;4,361.74,297.39,6.49,14.50;4,262.56,297.39,6.49,14.50;4,179.76,297.39,6.49,14.50;4,97.56,298.41,6.49,14.50;4,156.42,296.93,10.84,18.65;4,239.22,295.91,10.84,18.65;4,420.84,305.27,10.84,18.65;4,258.00,306.65,3.25,7.25;4,175.20,306.65,3.25,7.25;4,110.88,361.44,20.06,11.66;4,130.92,367.60,4.38,6.98;4,137.22,363.49,188.56,10.50;4,325.74,367.60,2.58,6.98;4,330.18,363.49,177.49,10.50;4,87.54,377.59,17.25,10.50;4,247.02,402.77,3.31,8.95;4,220.08,402.77,3.31,8.95;4,193.56,402.77,4.97,8.95;4,189.12,402.77,3.31,8.95;4,121.38,402.77,4.97,8.95;4,118.92,402.77,2.48,8.95;4,114.00,402.77,4.97,8.95;4,110.64,402.77,3.31,8.95;4,199.44,401.85,16.33,11.50;4,151.44,391.41,6.39,11.50;4,145.38,391.41,6.39,11.50;4,142.20,391.41,3.19,11.50;4,135.90,391.41,6.39,11.50;4,215.88,410.24,2.48,4.47;4,232.80,410.63,4.97,8.95;4,241.32,396.47,4.97,8.95;4,224.76,396.47,6.63,8.95;4,135.96,412.29,50.40,11.50;4,160.26,391.71,20.59,11.50;4,88.92,403.17,8.52,11.50;4,180.90,398.28,4.26,7.67;4,234.12,393.27,5.45,12.17;4,127.74,399.57,5.45,12.17;4,102.84,399.57,5.45,12.17;4,97.32,407.32,2.72,6.08;4,280.50,394.59,14.59,9.73;4,245.52,441.11,3.31,8.95;4,218.82,441.11,3.31,8.95;4,192.66,441.11,4.97,8.95;4,188.34,441.11,3.31,8.95;4,120.84,441.11,4.97,8.95;4,118.44,441.11,2.48,8.95;4,113.52,441.11,4.97,8.95;4,110.16,441.11,3.31,8.95;4,198.36,440.19,16.33,11.50;4,150.72,429.81,6.39,11.50;4,144.72,429.81,6.39,11.50;4,141.60,429.81,3.19,11.50;4,135.30,429.81,6.39,11.50;4,214.74,448.59,2.48,4.47;4,231.42,448.97,4.97,8.95;4,239.76,434.81,4.97,8.95;4,223.50,434.81,6.63,8.95;4,135.24,450.63,50.38,11.50;4,159.42,430.11,20.58,11.50;4,88.92,441.51,8.52,11.50;4,180.06,436.68,4.26,7.67;4,232.74,431.61,5.44,12.17;4,127.14,437.91,5.44,12.17;4,102.54,437.91,5.44,12.17"><head></head><label></label><figDesc>+ is the weight of word in positive list; w -is the weight of word in the negative list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,87.54,238.50,418.92,304.92"><head></head><label></label><figDesc></figDesc><graphic coords="8,87.54,238.50,418.92,304.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,87.54,210.79,309.72,121.08"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="7,87.54,224.21,309.72,107.66"><row><cell>Hard-rel criteria</cell><cell>Baseline run</cell><cell>Final run</cell></row><row><cell>Average precision</cell><cell>0.0324</cell><cell>0.0715</cell></row><row><cell>R-Precision</cell><cell>0.0679</cell><cell>0.1210</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell></row><row><cell>Soft-rel criteria</cell><cell>Baseline run</cell><cell>Final run</cell></row><row><cell>Average precision</cell><cell>0.0368</cell><cell>0.0858</cell></row><row><cell>R-Precision</cell><cell>0.0702</cell><cell>0.1406</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,87.54,135.49,245.89,47.03"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="8,285.78,148.85,47.65,9.19"><row><cell>R-precision</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>9.Acknowledge</head><p>This work is supported by <rs type="funder">China 863 Project</rs>(Grant No. <rs type="grantNumber">2001AA114040</rs>) and the <rs type="funder">National Science Fund of China</rs> under contact 60203007.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KCnKgN9">
					<idno type="grant-number">2001AA114040</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,134.10,188.11,373.57,10.50;9,105.00,201.55,267.07,10.50" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,298.56,188.11,124.94,10.50">Extended Boolean Models</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Betrabet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koushik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,451.79,188.11,55.88,10.50;9,105.00,201.55,182.15,10.50">Information Retrieval Data Structure &amp; Algorithms</title>
		<imprint>
			<biblScope unit="page" from="393" to="418" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,134.57,220.81,373.15,10.50;9,105.00,234.19,223.59,10.50" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,302.96,220.81,204.76,10.50;9,105.00,234.19,217.75,10.50">TREC-3 working note: Experiments in the Probabilistic Retrieval of Full Text Documents</title>
		<author>
			<persName coords=""><forename type="middle">S</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,133.20,253.45,374.32,10.50;9,105.00,266.83,194.66,10.50" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,314.05,253.45,193.46,10.50;9,105.00,266.83,182.89,10.50">TREC-11 working note: IBM&apos;s Statistical Question Answering System -TREC-11</title>
		<author>
			<persName coords=""><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,135.30,286.09,372.21,10.50;9,105.00,299.53,402.62,10.50;9,105.00,312.91,55.76,10.50" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,111.26,299.53,396.36,10.50;9,105.00,312.91,50.70,10.50">TREC-11 working note: TREC 2002 QA at BBN: Answer Selection and Confidence Estimation</title>
		<author>
			<persName coords=""><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>Licuanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,133.20,332.17,374.30,10.50;9,105.00,345.55,402.73,10.50;9,105.00,359.11,402.71,10.50;9,105.00,372.55,402.71,10.50;9,105.00,385.75,250.60,10.50" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,390.74,332.17,116.77,10.50;9,105.00,345.55,147.01,10.50">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,183.52,359.11,324.19,10.50;9,105.00,372.55,398.09,10.50">Information Retrieval Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>Berkeley, California, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.91,405.01,368.70,10.50;9,105.00,418.45,195.12,10.50" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,230.37,405.01,100.79,10.50">Ranking Algorithms</title>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,368.48,405.01,139.13,10.50;9,105.00,418.45,110.21,10.50">Information Retrieval Data Structure &amp; Algorithms</title>
		<imprint>
			<biblScope unit="page" from="363" to="392" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,134.23,437.71,373.45,10.50;9,105.00,451.15,402.75,10.50;9,105.00,464.47,270.19,10.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,279.80,437.71,223.04,10.50">Information Retrieval as Statistical Translation</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,125.60,451.15,382.15,10.50;9,105.00,464.47,156.09,10.50">Proceedings of the 22nd ACM Conference on Research and Development in Information Retrieval((SIGIR&apos;99)</title>
		<meeting>the 22nd ACM Conference on Research and Development in Information Retrieval((SIGIR&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.29,483.73,355.55,10.50;9,95.88,497.17,339.28,10.50" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,301.57,483.73,196.28,10.50;9,95.88,497.17,206.21,10.50">The Use of Dynamic Segment Scoring for Language-Independent Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clifford</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,329.61,497.17,18.82,10.50">HLT</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,141.57,516.43,357.52,10.50;9,95.88,529.93,370.75,10.50;9,95.88,543.31,297.81,10.50" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,279.29,516.43,200.55,10.50">Query-relevant summarization using FAQs</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<ptr target="http://citeseer.nj.nec.com/341776.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,101.25,529.93,365.38,10.50;9,95.88,543.31,51.22,10.50">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,147.40,562.45,307.80,10.50;9,95.88,575.89,411.80,10.50;9,95.88,589.39,377.94,10.50;9,95.88,602.83,43.32,10.50" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,406.11,562.45,49.09,10.50;9,95.88,575.89,333.38,10.50">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName coords=""><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,449.76,575.89,57.93,10.50;9,95.88,589.39,266.22,10.50">Proceedings of the 2nd SIGDIAL workshop on discourse and dialogue</title>
		<meeting>the 2nd SIGDIAL workshop on discourse and dialogue<address><addrLine>Aalborg , Denmark</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,147.42,621.91,345.56,10.50;9,95.88,635.35,398.48,10.50;9,95.88,648.85,396.61,10.50;9,95.88,662.29,142.67,10.50" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,444.55,621.91,48.43,10.50;9,95.88,635.35,308.21,10.50">Relevance Feedback and Personalization: A Language Modeling Perspective</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,424.43,635.35,69.93,10.50;9,95.88,648.85,396.61,10.50;9,95.88,662.29,41.68,10.50">Proceedings of the DELOS-NSF Workshop on Personalization and Recommender Systems in Digital Libraries</title>
		<meeting>the DELOS-NSF Workshop on Personalization and Recommender Systems in Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
