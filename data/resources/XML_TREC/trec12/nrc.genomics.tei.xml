<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.88,81.39,414.21,16.32">Finding Gene Function using LitMiner</title>
				<funder>
					<orgName type="full">NRC Institute for Biological Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.28,126.99,72.85,7.80"><forename type="first">Berry</forename><surname>De Bruijn</surname></persName>
							<email>berry.debruijn@nrc.gc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Information Technology National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.79,126.99,53.81,7.80"><forename type="first">Joel</forename><surname>Martin</surname></persName>
							<email>joel.martin@nrc.gc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Information Technology National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.88,81.39,414.21,16.32">Finding Gene Function using LitMiner</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">13103E85829170500377E8CB439E10D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> NRC (National Research Council, Canada)  <p>submitted 2 sets of results for the primary task in the TREC Genome track. The systems that generated these results were tuned primarily to achieve very high recall (above 90%) and secondarily to minimize the number of documents retrieved. Both submitted sets were the outputs of automatic systems (non-interactive, non-supervised) with a modular architecture.</p><p>The TREC evaluation confirmed that recall for both submissions was extremely high: 543 out of 566 target documents (0.9594) were returned. In addition, these systems returned far fewer documents than were allowed by the genomic track rules. They returned an average of 196 documents per query across the 50 queries, with a median value of only 100 documents.</p><p>For the first submission, the system was entirely based on Information Retrieval techniques, tuned to achieve very high recall and fair precision. Averaged precision was 0.3941 for the first submission. This first submission ranked third out of 49 runs submitted by all participants.</p><p>For the second submission, reranking was done based on the outcome of an information extraction module, tuned towards the task of identifying gene function papers. This module identified 539 documents as highly promising; 121 of these turned out to be target documents, 418 weren't. All in all this caused the averaged precision to drop slightly to 0.3771 -contrary to our expectations. This second submission ranked fifth out of all 49 runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scientists reviewing literature in their field often hope for exhaustive searches that return all the relevant documents. The cost of missing an important document is high, so less than perfect precision is accepted from real-world (less than perfect) systems if close to full recall is still guaranteed. Of course, since the scientist would have to scan every article returned by the search, a system returning 100 results is far better than one returning 1000. This is the type of system we envisioned when taking up the TREC task.</p><p>After a genomics 'pre-track' in 2002, a full genomics track was added to the TREC setup for the 2003 edition. This development is in full agreement with the strong attention that Information Extraction from biomedical literature has recently received (see for instance <ref type="bibr" coords="1,437.62,632.55,102.55,7.80;1,72.00,644.91,26.19,7.80">De Bruijn and Martin, 2003)</ref>. The genomics track presents interesting issues to the text retrieval research community because of the combination of working with large-scale document collections and the specifics of the application field with its esoteric jargon.</p><p>The National Research Council (NRC) is the Government of Canada's premier organization for research and development. For a number of years, researchers at its Institute for Information Technology have been working on language processing technologies, including methods and tools to process biomedical literature. Multiple technologies are now being bundled into an integrated toolbox for literature access and management, named LitMiner. This paper gives an overview of the architecture that we used for our TREC-genomics submissions, including a discussion on how such a high recall was achieved. It includes a further analysis of those queries where performance was disappointing or under par. Design and performance of the information extraction module are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">system architecture</head><p>For storage, a MySQL database was used under Linux/Intel586. The documents were stored in one table, and an index on terms to document identifiers (PMIDs) was created in a separate index table. The index contained lists of PMIDs for all non-stop words, along the position of the word in that text. A title word index was included in a separate column in the same table; other columns contain the word frequencies and document frequencies. Also stored in the index table were entries for the complete RN and MeSH terms which can be multiple word terms.</p><p>The NRC system finally consisted of 7 modules, working in sequence. The modular architecture is sketched in figure <ref type="figure" coords="2,173.05,336.03,4.61,7.80" target="#fig_0">1</ref>. The functions of the various modules is as follows:</p><p>[1] basic retrieval: database/index lookup of all articles containing one or more of the query gene names verbatim [2] morphological term expansion: based on a number of rules, variations of the original gene names are generated and articles containing these expanded query terms are added to the retrieved set [3] acronym disambiguation: this module removes those articles from the result set where the letter combination clearly has an incorrect meaning, for instance, "MVP" should mean "major vault protein" and not "mitral valve prolapse".</p><p>[4] relevance feedback: this is done on the RN field of the Medline records; records are added to the retrieved set if they contain RN terms that are relatively 'overrepresented' in the set so far. Such a relevance feedback (RF) loop is done twice.</p><p>[5] organism matching: since the target organism is given for each query, only articles are returned that actually mention the organism name.</p><p>[6] reranking: articles are reranked based on the occurrence of more gene aliases or more significant ones; aliases occurring in the title give an extra boost.</p><p>[7] function phrase finding: (second submission only) -documents that score high on a high-precision 'function phrase finding' test get an additional boost up.</p><p>Modules 1,2 and 4 are designed to achieve optimal recall, while modules 3, 5, 6 and 7 are designed to improve precision. The various modules are described in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Technical description of each of the modules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">query term retrieval</head><p>Basic document retrieval / index lookup: retrieves the document identifiers for those documents that contain that word (for single-word terms) or that phrase (for multi-word terms). Matching was case-independent and on full, untruncated terms. Since stop words were excluded from indexing, query terms that double as common English stop words (such as AND) would not pose a problem. Stop words were allowed to be part of query phrases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">morphological term expansion</head><p>It was observed by us and our domain experts that authors do not always use the preferred molecule name, or in other words, the list of aliases is not complete in listing all possible molecule names that authors decide to use. Some of the used names are minute variations on one of the official symbol/alias names and can be found by creating a limited number of morphological variations on the official names. This module includes handcrafted rules that make these variations.</p><p>A complete list of the rules is available from the authors; some examples of morphological variations that this module deals with, are: 'ATF2' could be referred to as 'ATF-2', 'ATF 2', 'ATF-II' 'PPARG' could be referred to as 'PPAR Gamma' 'FGFR' could be referred to as 'FGF receptor'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">acronym disambiguation</head><p>Acronyms are fairly likely to be ambiguous. For instance, using the query term "MVP" with the intention retrieve documents on major vault protein will also return irrelevant documents where the same acronym stands for 'mitral valve prolapse', 'microvascular pressure', or 'Midwifery Ventouse Practitioners'. This module disambiguates the meaning by first creating a list of possible meanings for that acronym from Medline documents, in a routine analogous to e.g. <ref type="bibr" coords="3,429.97,682.95,92.15,7.80" target="#b1">Pustejovsky (2001)</ref>, or <ref type="bibr" coords="3,72.00,695.31,137.56,7.80" target="#b2">Schwartz and Hearst (2003)</ref>. For each of the meanings in the list, it is determined whether the meaning is correct or incorrect in the context of the query. Then documents under consideration are tested for the presence of any of the possible acronym meanings, and if only an incorrect meaning is found in a document then it is excluded from the retrieved set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">relevance feedback</head><p>After retrieval with the query terms, and after disambiguation, an unsupervised relevance feedback step is done (twice). For all documents thus far retrieved, the values for the RN field are retrieved.</p><p>The RN field in the Medline data lists Registration Numbers to chemicals and substances involved in the study, including Enzyme Commission numbers and names. Then other (so far unretrieved) documents are added to the retrieved set if they contain RN terms that are strongly represented in the retrieved set. Per feedback iteration, the number of terms used was at most 10 and at least five provided that each added term occurred in at least 20% of the previously retrieved documents.</p><p>The number of feedback iterations was set to two. These parameters were set based on runs with the training material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">organism matching</head><p>This step rejects articles where the correct gene might be discussed but where it relates to another organism. For that, a synonym shortlist is used, containing the synonyms for the most frequently researched organisms (human, drosophila, mouse, rat, cow, yeast, zebrafish, e-coli, c. elegans and xenopus). If the organism from the query is not referred to in the retrieved article then that article is discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6">reranking</head><p>A TF*IDF weighting scheme is used to rerank the retrieved articles, so that articles that contain characteristic query terms will end up higher in the results list. Articles that mention multiple query terms get an additional boost. Articles where the title contains query terms get an additional boost as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.7">phrase matching and boosting</head><p>A phrase matching module was used to identify sentences likely to include descriptions of gene function and boost those documents up in the rankings. The module was trained with a supervised machine learning method on training data, while the test phase took place without supervision. In this method, seed sentences known to be GeneRIFs from the training material were automatically generalized to include similar phrases that appeared in other abstracts. The expansion included identifying words that could be substituted in the sentence and the gradual shortening of the phrase to increase the number of abstracts in which it appeared. The specific algorithm used produced sentence identification with very high precision in the training set. When a sentence or phrase was identified in a title or an abstract, that abstract could be promoted in the results ranking. This act of promotion could lead to large errors. To prevent that, the system further restricted promotion to ensure high precision. This promotion was only used when the phrase was in the title of the article AND clearly contained a reference to the gene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task</head><p>The aim of the competition in the Primary Task of the Genomics track was to retrieve documents that contain a description of the function of a gene, given the gene names and the organism under consideration. Result lists should have the (more) relevant documents at the top and any less/not relevant documents near the tail. Result lists can contain at most 1000 documents per query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Material</head><p>The document set consisted of 525,938 article abstracts, or one year's worth ( <ref type="formula" coords="4,482.67,706.83,19.77,7.80">2002</ref>), from Medline/PubMed. These are article citations from every kind of biomedical discipline. The abstracts or citations or records contain various fields including title, abstract, author names, affiliation, publication info (journal name, issue), category terms from the Medical Subjects Headings (MeSH) hierarchy, and RN entries. The RN field contains Registration Numbers and official names of chemical substances, as well as official Enzyme Commision names and numbers of molecules involved in the study.</p><p>Fifty queries were supplied in a training set and another fifty in a test set. The test set was kept in isolation away from all system development and all developers (conforming to the competition rules). The training set was used to tune system parameters and make other performance decisions. No other data source was used (such as LocusLink, which was specifically off-limits, or Gene Ontology), with the exception of a background collection of older Medline abstracts for the Acronym Disambiguation module (more about that in section 3.3). The training material as well as the test material originated from NCBI's LocusLink database, specifically the GeneRIF field (RIF = Reference in Function) and the nomenclature field (for the query terms). Each query contained a (TREC) query number, a LocusLink identifier (not used), the organism, and a number of gene identifiers, including the official symbol name, alias names, and product names.</p><p>For the training queries, the answers were available, for the test queries, answers were made available after the submission deadline. The number of target documents per query ranged between few (0) to many (53) in the training set, median=4, average=5.83. In the test set, the range turned out to be 2 to 66, median=7, average=11.32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation metrics</head><p>Systems were evaluated with the common Information Retrieval metrics, as well as a metric named Averaged Precision: at each point in the results list where a target document is positioned, the precision is calculated and averaged over the number of target documents. For unretrieved target documents, precision=0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and discussion</head><p>4.1 Evaluation of the entire system Submission 1, overall performance: this was the submission as produced with the system without module 7 (the phrase searching / document boosting module). This system returned 9824 documents over the 50 queries; recall was 543 target documents found out of 566 target documents max, or 95.9% recall. Average precision for all relevant documents (averaged over queries) was .3941. The average number of documents returned per query was 196.5, but the distribution over queries was very skewed and the median number was 100 documents per query.</p><p>Submission 2, overall performance: this system included the phrase searching / document boosting module. The module identified 539 documents as highly promising, 121 of these were indeed target documents while 418 were not. For this system, the submission-1 results were only re-ranked so the returned set remained 9824 documents with 95.9% recall. Since quite some non-target documents got boosted, the average precision score dipped to .3771.</p><p>The results for submissions 1 and 2 are summarized in table <ref type="table" coords="5,370.77,645.03,4.61,7.80" target="#tab_0">1</ref>.</p><p>With a median number of 100 documents per query and a recall of nearly 96%, exhaustive (not exhausting) searches are realistic with this system. Scientists often want to find all the relevant documents without having to discard too many false positives. The NRC system successfully restricts the number of documents returned while maintaining a high recall. The NRC system achieved a very competitive score in the TREC competition, falling just short of matching the highest scores from one other participant.</p><p>One explanation of the lower score for our second submission, is the sparsity of GeneRIF's. The module used for that submission tried to capture characteristics of function descriptions. But not every statement of a gene's function is currently a GeneRIF. Different systems that find the same document will likely favour different valid statements of a gene's function and will produce different measures of precision based on GeneRIFs. Unlike the estimated measure for recall, an estimate of precision based on a small sample of the true documents will not necessarily be a reliable estimate of the precision across the entire population. Only one in four of the documents that were promoted by the Function Phrase finding module in our second system, were in fact GeneRIF's.</p><p>That does not mean that three in four are false positives, only that we do not know their true status.</p><p>The performance of this phrase matching module was strikingly different between the training and the test data sets. Besides the problem of a sparse gold-standard, it is also possible that the learning method overfitted the training data. Only the GeneRIFs in the training set were used to infer the structure of phrases indicating function. As a result, those structures may have worked differentially well when applied to reranking training documents.</p><p>In addition to a solution to the sparsity of the GeneRIFs, we would like to propose another metric to assess the recall. For one example, the average precision could be averaged over the reading cost to give a benefit per unit of effort. Another measure would divide the recall (a measure of benefit), by the number of documents returned (a measure of the cost). Although, the NRC systems would do well on such measures, other systems might also do well by deleting the documents beyond the first 100. This modification would only produce a fair comparison if the system itself is able to determine that the number of documents should be 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of each of the modules</head><p>Additional runs were done with some modules switched on and others switched off, to determine the separate contributions of the modules. The results of these runs can also be found in table 1.</p><p>! Basic retrieval with most modules switched off: only query term retrieval is included, and reranking is applied. This retrieves 25737 articles, but if the retrieved set is capped at 1000 articles per query ('cap-1000', as per TREC guidelines), 13,975 articles remain. Recall is 76.5% (433 target documents out of 566), with .2051 average precision. These measures confirm that basic retrieval alone does not cut it.</p><p>! Morphological query expansion increases the size of the retrieved set to 34188 articles or 17466 after cap-1000. Recall improves to 90.3% (511 documents), average precision is .2355. The strong increase in documents retrieved indicates that many irrelevant documents are also added. In this stage of the process, where high recall is the main objective, that is less important.</p><p>! Relevance feedback (RF) applied as an alternative to morphological query expansion comes close to the same scores but not quite. One RF loop retrieves 28612 documents (16086 after cap-1000); recall is 84.6% (479 documents); average precision is .2198. A second loop increases recall to 87.5% (495 documents) with 17370 retrieved (32616 before cap-1000); average precision .2220. A third RF loop does very liitle more: 88.0% recall (498 documents) with 18540 retrieved (34263 before cap-1000) and average precision of .2198. This confirms the decision that was based on the training material that two RF iterations would be the best setting.</p><p>! Relevance feedback did complement morphological query expansion. With both modules in place, the recall was near to perfect with 96.5% (546 documents) with 20813 retrieved (40692 before cap-1000); average precision here was .2397.</p><p>! Acronym disambiguation did not help much on the test material. While this module did oust 507 documents compared to the previous setting, with no incorrect discardings so no loss in recall, most of these ousted documents were at the tail of the document rankings anyway and wouldn't have survived the cap-1000 step, or wouldn't harm the average precision. While the overall improvement was small, this module did prove very useful in a few cases -including some cases in the training material. Since it this module is intuitively appealing and wasn't hurting performance we decided to leave it in.</p><p>! Organism matching proved a powerful method to drastically limit the number of retrieved items while doing very little harm to recall. This module cut the size of the retrieved set in four -from 40185 to 9837 documents. Among the discarded abstracts were a mere 10 target documents. After cap-1000, the final result set was 9824 with no further loss of precision. This step gives the average precision a terrific boost: from .2399 without the module to .3941 with the module. Of the ten discarded target documents, eight mention a different organism than the target organism and make a dubious or poor GeneRIF. One more document lists a broader organism category ('mammalian') rather than the target document (human). Finally, one article did not list an organism, but the Medline entry for that article has been updated after the TREC data was collected, and currently does list 'human' in the MeSH field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Failure analysis</head><p>In the following section, we analyse a number of queries where our system returned poor results.</p><p>Query 4: "guanine nucleotide binding protein (G protein), alpha activating activity polypeptide, olfactory type". The system retrieved 984 documents with both target documents retrieved, in ranks 18 and 95 (submission 1), or 54 and 125 (submission 2). The results set after module 1 had 2 documents, including one of the two target documents. The morphological query expansion module included a rule that would cause the phrase "G-protein" to be used as a query term in itself, while this would be too general a query term to be appropriate in this case. A system with module 2 disabled would have retrieved five documents with total recall and .5 average precision on this query. Disabling the single rule that caused the over-generalization showed harmful to the performance on other queries.</p><p>Query 22: "arginine vasopressin". This is a fairly frequently researched molecule and many top retrieved articles seem on-topic and relevant, albeit not strictly target documents. This might be a case where the evaluation metrics undervalue the results.</p><p>Query 41: "CASP8 and FADD-like apoptosis regulator". This query includes three query terms that could double as common English words: 'FLAME', 'FLIP' and 'CASH'. These terms contaminate the result set and even though 100% recall was achieved, precision suffered dramatically. An added rule that would prohibit common English words from being used as query terms would not help: on that query, recall dropped to an unacceptable 0.1 (1 retrieved, 10 targets). Allowing English words as long as capitalization of that word is uncommon (a capital letter somewhere after the first letter of the word) did reduce the number of retrieved documents from 323 to 101, kept the recall on 10/10 and raised the average precision from .1502 to .2972. That rule, however, caused a severe degradation on two other queries from the test set (26 and 27) and on one in the training material ( <ref type="formula" coords="8,120.77,336.03,9.13,7.80">14</ref>). With no major improvements on other queries, it would give an overall worse performance.</p><p>Query 49: "T-cell receptor alpha chain". This query gave a failure of a different kind: even though a limited number of articles was retrieved (80) and two target documents were returned in positions 4 and 12 of the result list, five more target documents were not retrieved. That made this query the only one where recall for our system was &lt;50% and it accounted for 5 out of 23 missed target documents over the entire task.</p><p>These failure analyses and other observations gave us no reason to drastically revise the overall architecture or details of the design of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>The LitMiner system used for the primary TREC-genomics task, was almost entirely based on established Information Retrieval techniques. It performed very well in absolute terms as well as in comparison with other systems in this TREC track. The two submissions from LitMiner ranked third and fifth out of 49 submitted runs by all participants.</p><p>We designed our system in such a way that an early stage would return a limited document set with high recall and fair precision. The second stage would be allowed to be more computationally expensive if it would be capable of increasing precision while retaining the high recall. We must admit that the second stage performed less well than we expected. This has to do with the slightly vaguer definition of what a GeneRIF would stand for. Future work will explore the use of additional training examples and hopefully more effective information extraction.</p><p>On the other hand, the first stages succeeded very well in getting the high-recall results while limiting the size of the result set. In fact, searching literature with a &gt;95% recall in a typical result set size of a mere 100 documents provides a very practical tool for biologists.</p><p>While interactive systems are planned to be part of future TREC-genomics tasks, the setting of this year's task was a static environment. It differs from the regular, interactive search environment that characterizes our search toolbox LitMiner. Supervised relevance feedback techniques and result navigation tools are likely to add power to the current design and allow the user to quickly home in on the desired results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,73.32,399.03,260.53,7.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: architecture of the NRC retrieval system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,78.48,422.67,426.70,285.48"><head>Table 1 : performance of the system and its separate components System (or modules included) Retrieved (before capping at 1000)</head><label>1</label><figDesc></figDesc><table coords="6,353.40,465.15,151.78,20.16"><row><cell>Recall</cell><cell>averaged</cell></row><row><cell></cell><cell>precision</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Brandon Smith</rs>, <rs type="person">Roy Walker</rs>, <rs type="person">Hung Fang</rs>, <rs type="person">Maria Moreno</rs>, and <rs type="person">Annie Law</rs> from the <rs type="funder">NRC Institute for Biological Sciences</rs>, as well as <rs type="person">Lynn Wei</rs> and <rs type="person">Jeffrey Demaine</rs> (<rs type="affiliation">NRC-IIT</rs>), for their input during our joint meetings and on-line.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,72.00,262.95,468.13,7.80;9,72.00,275.31,129.85,7.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,192.07,262.95,298.45,7.80">Getting to the (c)ore of knowledge: mining biomedical literature</title>
		<author>
			<persName coords=""><forename type="first">De</forename><surname>Bruijn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,497.01,262.95,43.12,7.80;9,72.00,275.31,12.33,7.80">Int J Med Inf</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="18" />
			<date type="published" when="2002-12">2002 Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,300.03,468.03,7.80;9,72.00,312.39,396.86,7.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,417.85,300.03,122.18,7.80;9,72.00,312.39,244.72,7.80">Automatic extraction of acronym-meaning pairs from MEDLINE databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cochran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kotecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Morrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,325.38,312.39,38.84,7.80">Medinfo</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">Pt 1</biblScope>
			<biblScope unit="page" from="371" to="375" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,337.11,468.04,7.80;9,72.00,349.47,272.24,7.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,234.67,337.11,305.37,7.80;9,72.00,349.47,72.53,7.80">A simple algorithm for identifying abbreviation definitions in biomedical text</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.96,349.47,114.88,7.80">Pac. Symp. Biocomput</title>
		<imprint>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
