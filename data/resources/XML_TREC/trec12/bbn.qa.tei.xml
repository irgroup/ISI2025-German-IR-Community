<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,131.64,75.41,348.54,12.64">TREC2003 QA at BBN: Answering Definitional Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.20,101.88,38.28,10.80"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN</orgName>
								<address>
									<addrLine>Technologies 50 Moulton Street Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.36,101.88,67.32,10.80"><forename type="first">Ana</forename><surname>Licuanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN</orgName>
								<address>
									<addrLine>Technologies 50 Moulton Street Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.08,101.88,87.58,10.80"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN</orgName>
								<address>
									<addrLine>Technologies 50 Moulton Street Cambridge</addrLine>
									<postCode>02138</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,131.64,75.41,348.54,12.64">TREC2003 QA at BBN: Answering Definitional Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD253B3F6C48D40D1B796727057856E7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In TREC 2003, we focused on definitional questions. For factoid and list questions, we simply re-used our TREC 2002 system with some modifications.</p><p>For definitional QA, we adopted a hybrid approach that combines several complementary technology components. Information retrieval (IR) was used to retrieve from the corpus the relevant documents for each question. Various linguistic and extraction tools were used to analyze the retrieved texts and to extract various types of kernel facts from which the answer to the question is generated. These tools include name finding, parsing, co-reference resolution, proposition extraction, relation extraction and extraction of structured patterns. All text analysis functions except structured pattern extraction were carried out by Serif, a state of the art information extraction engine <ref type="bibr" coords="1,217.68,346.32,111.64,10.80" target="#b7">(Ramshaw, et al, 2001)</ref> from BBN.</p><p>Section 2 summarizes our submission for factoid and list qeustion answering (QA). The rest of the paper focuses on defintional questions. Section 4 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FACTOID AND LIST QUESTIONS</head><p>The factoid system is the same as our system for TREC 2002 <ref type="bibr" coords="1,368.76,435.48,76.78,10.80" target="#b8">(Xu, et al, 2003)</ref>, except for a couple of modifications. One modification is to boost the score for answers that occurred multiple times in the corpus. This is similar to previous studies (e.g. <ref type="bibr" coords="1,400.92,462.96,86.84,10.80" target="#b3">Clarke, et al, 2001</ref>) that employed redundancy information to improve QA performance. Assuming the occurrences of an answer a are a 1, a 2, … a n , which are ranked by IR score, then the final score for a is</p><formula xml:id="formula_0" coords="1,110.04,511.60,125.08,25.69">&lt;= &lt; + n i i a score c a score 1 1 ) ( ) (</formula><p>We set c=0.001.</p><p>The other modification is to use additional information to validate answers. Specifically,</p><p>• Validation based on question type: For questions of the form "What X is ...", the system uses WordNet to verify that the question type is a hypernym of the answer. (For example, "Tagalog" is a valid answer for "What language ..." because "language" is a hyperym of "Tagalog".)</p><p>• Validation of answers for questions looking for a date: We required certain constraints on date answers based on the form of the question. For example, "When (was|did) ... " questions are likely to refer to a specific date in the past, and a good answer candidate should contain a year. "When is ..." questions may refer to either a specific date in the past (in which case they should contain a year), or a relative date, such as "the day after X". "What month ..." questions should contain a month, etc.</p><p>• Validation of answers for questions looking for a measurement: We wrote patterns to detect four common measurement questions: dimension, duration, speed and temperature.</p><p>Valid candidate answers are expected to have both a quantity and a unit of the appropriate type.</p><p>• Validation of answers for questions looking for an author: Specific patterns were written to extract "who-wrote" answers from the text.</p><p>• Validation of answers for questions looking for an inventor: Specific patterns were written to extract "who-invented" answers from the text.</p><p>• Validation based on verb-argument: We used WordNet to match verbs (for example, "Who killed X?" = "Y shot X"). In addition, we refined the scoring of slot matching to take into account the number of "filler" words that appear in between the question words.</p><p>List questions were processed like factoid questions, except that answers were ranked and the list of answers was truncated when the score of an answer drops below 90% of that of the best one.</p><p>We submitted three runs, BBN2003A, BBN2003B and BBN2003C. The differences are:</p><p>• BBN2003A. The Web was not used in answer finding.</p><p>• BBN2003B. For factoid questions, answers were found from both the TREC corpus and the Web. For list questions, it is the same as BBN2003A, except the constant c in the score computing formula was increased to 0.1.</p><p>• BBN2003C. For factoid questions, it is the same as BBN2003B, except for one difference.</p><p>If an answer was found from both the TREC corpus and the Web, its score was boosted.</p><p>For list questions, it is the same as BBN2003B.</p><p>Our technique to use the Web for QA is documented in our TREC 2002 work <ref type="bibr" coords="2,447.60,487.80,73.78,10.80" target="#b8">(Xu et al, 2003)</ref>.</p><p>Table <ref type="table" coords="2,102.24,501.60,6.00,10.80" target="#tab_0">1</ref> shows the NIST scores for the three runs. Two observations can be made. First, using the Web improved factoid QA. This is not surprising given previous TREC results by our group as well as by other groups. Compared with our TREC 2002 results, however, the impact of using the Web on QA performance is much smaller, improving accuracy from 0.177 to 0.208, a 3% improvement absolute. In comparison, for TREC 2002, using the Web produced a much larger improvement (10% absolute) in accuracy. More work is needed to determine if the reduced benefit of using the Web is due to changes in the characteristics of the questions or due to the modifications we made to last year's system. Second, using a large c significantly improved the performance of the list questions (from 0.087 to 0.097). This indicates that taking advantage of answer redundancy is more crucial for list questions than for factoid questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>Our system processed a definitional question in a number of steps. First, question classification identified the question type, i.e. whether a question is a who or a what question. The distinction is necessary because some subsequent processing treats the two types of questions differently. Also in this step, the question target was extracted from the question text, by stripping of "What is", "Who is" etc.</p><p>Second, information retrieval pulled documents about the question target from the TREC corpus. This was achieved by treating the question target as an IR query. BBN's HMM IR system <ref type="bibr" coords="3,72.00,325.56,94.24,10.80" target="#b5">(Miller, et al, 1999)</ref> was used for this purpose. For each question, the top 1000 documents were retrieved.</p><p>Third, heuristics were applied to the sentences in the retrieved documents to determine if they mention the question target. Sentences that do not mention the question target were dropped.</p><p>Fourth, kernel facts that mention the question target were extracted from sentences by a variety of linguistic processing and information extraction tools. A kernel fact is usually a phrase extracted from a sentence. The purpose of using kernel facts is twofold: to minimize irrelevant materials in the answer and to facilitate redundancy detection.</p><p>Fifth, all kernel facts were ranked by their type and their similarity to the profile of the question. The question profile is a word centroid that models the importance of different words in answering the question.</p><p>Finally, heuristics were applied to detect redundant kernel facts. Up to a cap on the total answer length, facts that survived redundancy detection were output as the answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Checking if a Sentence Mentions a Question Target</head><p>First, we check if a document mentions a question target at all. If a document does not mention a question target, we drop the whole document from consideration. For who questions, we require a document to contain a word sequence "F…L" (F and L are the first and last names of the question target) and the distance between F and L is less than 3. The purpose is to match "George Bush" with "George Walker Bush". We assume the first and last names are the first and last word of the question target respectively. For what questions, we require the document to contain the exact string of the question target, except that plural forms were converted to singular before string comparison.</p><p>If a sentence contains a noun phrase that either matches the question target directly (via string comparison) or indirectly (through co-reference), we think it contains the question target. We used Serif <ref type="bibr" coords="4,123.24,74.64,108.76,10.80" target="#b7">(Ramshaw et al, 2001)</ref> for co-ref resolution and parsing. For who questions, only the last name was used in string comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extraction of Kernel facts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Appositives and Copula Constructions</head><p>An example appositive is the phrase "George Bush, the US President". An example copula is the sentence "George Bush is the US President." In both cases, the phrase "the US President" is a definition for "George Bush". Appositives and copulas were extracted from the parse trees of the sentences based on simple rules using Serif.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Propositions</head><p>Propositions represent an approximation of predicate-argument structures and take the form: predicate (role 1 : arg 1 , … , role n : arg n ). In the context of this work, the predicate is typically a verb.</p><p>Arguments can be either an entity or another proposition. The most common roles include logical subject, logical object, and object of a prepositional phrase modifying the predicate. For example, "Smith went to Spain" is represented as went(logical subject: Smith, PP-to: Spain).</p><p>Propositions were extracted from parse trees using Serif.</p><p>We classified propositions into special propositions and ordinary ones. We manually created a list of predicate-argument structures that we thought were particularly important in defining an entity. For example, "&lt;PERSON&gt; was born on &lt;DATE&gt;" is one of the predicate -argument structures for persons. Propositions that matched one of such pre-defined structures were classified as special while others were classified as ordinary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Structured Patterns</head><p>We handcrafted over 40 rules to extract structured patterns that are typically used in defining a term. Similar techniques were also used by Columbia University <ref type="bibr" coords="4,385.32,450.00,150.28,10.80">(Blair-Goldensohn, et al, 2004)</ref> and Language Computer Corporation <ref type="bibr" coords="4,254.64,463.80,114.28,10.80" target="#b4">(Harabagiu, et al, 2004)</ref> in their TREC 2003 work. For example, one such rule is "&lt;TERM&gt; ,? (is|was)? also? &lt;RB&gt;? called|named|known+as &lt;NP&gt;". Applied to a parsed sentence, the rule will match the question target (&lt;TERM&gt;), optionally followed by a coma, optionally followed by "is" or "was", optionally followed by "also", optionally by an adverb (&lt;RB&gt;), followed by "called", "named" or "known as" and followed by a noun phrase (&lt;NP&gt;). In the pattern, the "?" denotes optional, "+" concatenation, and "|" alternative. If the question is "What are tsunamis?", the pattern will extract the phrase "Tsunamis, also known as tidal waves" from the sentence "Tsunamis, also known as tidal waves, are caused by earthquakes."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Relations</head><p>As discussed in Section 3.3.2, propositions simply consist of lexical predicates. Since different lexical predicates can represent the same underlying relation, normalizing these propositions into relations that are commonly found in an ontology, where possible, is obviously desirable for definitional QA.</p><p>In this work, relations were extracted by Serif. Serif can extract the 24 binary relations defined in the ACE guidelines <ref type="bibr" coords="4,168.96,697.56,169.92,10.80">(Linguistic Data Consortium, 2002)</ref>. Using lexicalized patterns, Serif extracts those relations from the propositions. For example, the relation role/generalstaff("Gunter Blobel", "Rockefeller University") will be extracted from the sentence "Dr. Gunter Blobel of The Rockefeller University won the Nobel Prize for medicine today for protein research that shed new light on diseases including cystic fibrosis and early development of kidney stones.".</p><p>The QA guidelines require the answer to a definitional question to be a list of textual strings rather relations. We mapped a relation extracted by Serif to a phrase by finding the smallest phrase in the parse tree that contains a mention of the question target and the other argument of the relation. For the above example, the extracted phrase would be "Dr. Gunter Blobel of The Rockefeller University".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Sentences</head><p>In addition to the above types of kernel facts, we used full sentences as fall back facts in order to deal with sentences from which none of the above-mentioned types of kernel facts can be extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ranking the Kernel Facts</head><p>The ranking order of the kernel facts is based on two factors: their type and their similarity to the profile of the question. Appositives and copulas were ranked at the top, then structured patterns, then special propositions, then relations and finally ordinary propositions and sentences. Within each type, kernel facts were ranked based on their similarity to the question profile. The similarity is the tf.idf score where both the kernel fact and the question profile were treated as a bag of words. We used the tf.idf function described by <ref type="bibr" coords="5,336.48,395.40,77.64,10.80" target="#b0">Allan et al, 2000</ref>.</p><p>The question profile was created in three possible ways. First, we searched for existing definitions of the question target from a number of sources. The resources include: WordNet glossaries, Merriam-Webster dictionary (www.m-w.com), the Columbia Encyclopedia (online at www.bartleby.com), Wikipedia (www.wikipedia.com), the biography dictionary at www.s9.com and Google. To search for biographies on Google, we used the person name and the word "biography" as a query (e.g. "George Bush, biography"). A simple rule-based classifier was used to weed out false hits. If definitions were found from these sources, the centroid (i.e,. vector of words and frequencies) of the retrieved definitions was used as the question profile.</p><p>If no definitions were found from the above sources, we considered two options. If the question is a who question, we used the centroid of a collection of 17,000 short biographies from www.s9.com as the question profile. Our hope is that using a large number of human created biographies, we can predict what words are important for biography generation. If the question is a what question, we used the centriod of all kernel facts about the question target as the question profile.</p><p>The assumption here is that the most frequently co-occurring words with the question target are the most informative words for answering the question. Similar techniques have been used in definitional QA <ref type="bibr" coords="5,187.92,633.96,155.08,10.80">(Blair-Goldsenshon, et al, 2003)</ref> and summarization <ref type="bibr" coords="5,440.64,633.96,92.74,10.80" target="#b6">(Radev, et al, 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Redundancy Removal</head><p>The goal of redundancy removal is to determine if the information in a kernel fact f is covered by a set of kernel facts S that are already in the answer. Three methods were used to decide if f is redundant with respect to S:</p><p>• If f is a proposition, we check if one of the facts in S is equivalent to f. Two propositions are considered equivalent if they share the same predicate (e.g., verb) and same head noun for each of the arguments. If such a fact is found, f is considered redundant.</p><p>• If f is a structured pattern, we check how many facts in S were extracted using the same underlying rule. If two or more facts were found, we consider f redundant.</p><p>• Otherwise, we check the percentage of content words in f that have appeared at least once in the facts in S. If the percentage is very high (i.e., &gt;0.7), we consider f redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Results and Discussion</head><p>The algorithm to generate an answer for a definitional question is:</p><p>1. Set the answer set S={} 2. Rank all kernel facts based on their similarity to the question profile regardless of their type. Iterate over all facts: In each iteration, discard a fact if it is redundant with respect to S. Otherwise, add the fact to S. Go to the next step if S has m facts.</p><p>3. Rank all remaining facts by type (the primary field) and then by similarity (the secondary field). Iterate over the ranked facts: In each iteration, add a fact to S if it is not redundant. Go to step 5 if the size (i.e., the number of non-space characters) of S is greater than max_answer_size or the number of sentences and ordinary propositions in S is greater than n.</p><p>4. If S is empty, rank all sentences in the top 1,000 retrieved documents based on the number of shared words between a sentence and the question target. Add the top 20 sentences to S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Return</head><p>S as the answer to the question.</p><p>We submitted three official runs, BBN2003A, BBN2003B and BBN2003C. For all three runs, max_answer_size=4,000 bytes. For BBN2003A, m=0 and n=5. For BBN2003B, m=0 and n=20. For BBN2003C, m=5 and n=10. The parameters were empirically set based on a set of about 79 development questions. Table <ref type="table" coords="6,218.52,511.92,6.00,10.80" target="#tab_1">2</ref> shows the results of the three runs. Overall, our results are satisfactory, given the median and best scores of all runs provided by NIST. In fact, BBN2003C achieved the highest score for definitional QA at TREC 2003.</p><p>Shortly after submitting the official runs and after discussion with NIST, we submitted a baseline run. The goal of the baseline was to give every group (including us) a chance to calibrate the results of their official runs. For each question, the baseline sequentially selected from the top 1,000 documents the sentences that mention the question target. The same heuristic in Section 3.2 was used to check if a sentence mentions the question target. As a fallback, if no sentences were found to mention the question target, all sentences in the top 1,000 documents were selected and ranked by the number of shared words between the question target and the sentences. For answer generation, we iterated over the selected sentences. For each sentence, we checked the percentage of words in the sentence that had occurred in previous sentences in the output answer. If the percentage was greater than 70%, the sentence was considered redundant and was skipped. Otherwise, the sentence was appended to the answer. The iteration continued until all sentences were considered or the answer length (i.e. the number of non-space characters in the answer) is greater than 4,000. Note that we applied a large length threshold because the Fscore favors recall over precision. The baseline run was assessed by NIST in the same way as the official runs.</p><p>As shown in Table <ref type="table" coords="7,165.24,138.84,4.50,10.80" target="#tab_1">2</ref>, the baseline performed surprisingly well, with an F-score 0.49. In fact, it outperformed all runs NIST received except BBN2003A, B&amp;C. Our official runs (BBN2003A, B&amp;C) are higher than the baseline, but the improvements are modest. One possible explanation for the unexpectedly good baseline is that the current state of the art of definitional QA is immature. The other is that with β=5 the F-score is overly recall-oriented and as such was "fooled" by the long answers produced by the baseline.</p><p>BBN2003A BBN2003B BBN2003C Baseline 0.521 0.520 0.555 0.49   Figure <ref type="figure" coords="7,106.20,515.04,6.00,10.80" target="#fig_0">1</ref> shows the score distribution over the 50 definitional questions sorted by F score. Quite a few questions (10) get a score of zero or close to zero. An initial analysis shows that a major source of failures is faulty assumptions we made in interpreting the question target. One example is "What is Ph in biology?". Our system assumed the literal string "Ph in biology" is the question target and tried to find it in text. Understandably, it failed. Another example is "Who is Akbar the Great?". Our system assumed the last name is "Great". These problems can be fixed..</p><p>Another major source of errors is erroneous redundancy removal. For example, for the question "Who is Ari Fleischer?", the inclusion of the kernel fact "Ari Fleischer, Dole's former spokesman who now works for Bush" in the answer masks the fact "Ari Fleischer, a Bush spokesman". The latter was considered to be redundant because all the words in it appeared in the former one. We hope better redundancy detection strategies will overcome such problems. A question by question analysis shows that when a question obtains a bad F score, it is usually due to recall rather than precision. In fact, for BBN2003C, if we assume perfect precision for all questions, it would merely increase the average F score from 0.555 to 0.614. However, if we assume perfect recall for all questions, it would increase the score to 0.797. This imbalance is understandable because the F-metric used for TREC 2003 QA emphasizes recall by a factor of five over precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>In TREC 2003 QA, we focused on definitional questions. Our approach combines a number of complementary technologies, including information retrieval and various linguistic and extraction tools (e.g., parsing, proposition recognition, pattern matching and relation extraction) for analyzing text. Our results for definitional questions are excellent compared with the results of other groups. However, much work remains as our results are only modestly better than a baseline that did little more than sentence selection using IR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,149.88,275.48,312.11,8.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Score distribution over 50 definitional questions for BBN2003C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,81.12,362.48,104.40"><head>Table 1 : Scores for factoid and list questions 3 DEFINITIONAL QUESTIONS</head><label>1</label><figDesc></figDesc><table coords="3,177.36,81.12,257.12,57.36"><row><cell></cell><cell cols="3">BBN2003A BBN2003B BBN2003C</cell></row><row><cell>Factoid</cell><cell>0.177</cell><cell>0.208</cell><cell>0.206</cell></row><row><cell>List</cell><cell>0.087</cell><cell>0.097</cell><cell>0.097</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,228.60,301.16,154.67,8.96"><head>Table 2 : Results for Definitional QA</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,324.84,464.88,154.20"><head></head><label></label><figDesc>Table3shows the scores for who and what questions for BBN2003C. The average score for who questions is somewhat better that that of what questions, but due to the relatively small number of questions, it is hard to determine if the difference is statistically meaningful.</figDesc><table coords="7,169.32,398.40,273.29,80.64"><row><cell>TYPE</cell><cell cols="2">Number of questions NIST F score</cell></row><row><cell>Who questions</cell><cell>30</cell><cell>0.577</cell></row><row><cell>What questions</cell><cell>20</cell><cell>0.522</cell></row><row><cell>Total</cell><cell>50</cell><cell>0.555</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,140.76,491.48,330.39,8.96"><head>Table 3 : BBN2003C score breakdown based on types of definitional questions</head><label>3</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,72.00,543.00,468.12,10.80;8,72.00,556.80,235.80,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,361.52,543.00,106.62,10.80">INQUERY at TREC8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Malin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,504.72,543.00,35.40,10.80;8,72.00,556.80,57.64,10.80">TREC8 Proceedings</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.00,576.60,467.98,10.80;8,72.00,590.40,468.12,10.80;8,72.00,604.20,241.92,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,421.74,576.60,118.24,10.80;8,72.00,590.40,148.65,10.80">Answering Definitional Questions: A Hybrid Approach</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schlaikjer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,405.00,590.40,135.12,10.80;8,72.00,604.20,48.92,10.80">New Directions in Question Answering</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003. 2004</date>
		</imprint>
	</monogr>
	<note>Chapter 13</note>
</biblStruct>

<biblStruct coords="8,72.00,624.00,468.02,10.80;8,72.00,637.80,468.12,10.80;8,72.00,651.60,59.52,10.80" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,400.09,624.00,139.93,10.80;8,72.00,637.80,141.82,10.80">A Hybrid Approach for QA Track Definitional Questions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schlaikjer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>To appear in TREC 2003 Proceedings</note>
</biblStruct>

<biblStruct coords="8,72.00,671.40,467.93,10.80;8,72.00,685.20,151.32,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,311.10,671.40,218.36,10.80">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,84.84,685.20,102.86,10.80">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,74.64,468.12,10.80;9,72.00,88.44,468.00,10.80;9,72.00,102.24,295.20,10.80;9,72.00,122.04,271.66,10.80;9,359.52,122.04,180.36,10.80;9,72.00,135.84,206.76,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,78.07,88.44,387.28,10.80;9,72.00,122.04,141.32,10.80">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
		<ptr target="http://www.ldc.upenn.edu/Projects/ACE2/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,268.68,122.04,74.97,10.80;9,359.52,122.04,170.08,10.80">ACE Phase 2: Information for LDC Annotators</title>
		<imprint>
			<date type="published" when="2002">2004. 2004. 2002</date>
		</imprint>
	</monogr>
	<note>Linguistic Data Consortium</note>
</biblStruct>

<biblStruct coords="9,72.00,155.64,467.98,10.80;9,72.00,169.44,468.00,10.80;9,72.00,183.24,236.88,10.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,312.43,155.64,227.54,10.80;9,72.00,169.44,31.18,10.80">A hidden markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,133.56,169.44,406.44,10.80;9,72.00,183.24,202.19,10.80">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,208.56,448.37,10.80;9,72.00,222.36,446.59,10.80;9,72.00,236.16,237.12,10.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,316.51,208.56,203.86,10.80;9,72.00,222.36,351.26,10.80">Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budzikowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,448.44,222.36,70.15,10.80;9,72.00,236.16,137.02,10.80">ANLP/NAACL Workshop on Summarization</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,255.96,467.88,10.80;9,72.00,269.76,467.88,10.80;9,72.00,283.56,345.48,10.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,108.70,269.76,290.22,10.80">Experiments in Multi-Modal Automatic Content Extraction</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bratus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zamanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,426.96,269.76,112.92,10.80;9,72.00,283.56,162.83,10.80">Proceedings of Human Language Technology Conference</title>
		<meeting>Human Language Technology Conference<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-03-18">2001. March 18-21, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,72.00,303.36,467.98,10.80;9,72.00,317.16,468.00,10.80;9,72.00,330.96,77.52,10.80" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,419.23,303.36,120.75,10.80;9,72.00,317.16,219.90,10.80">TREC2002 QA at BBN: Answer Selection and Confidence Estimation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Licuanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,319.44,317.16,118.59,10.80">TREC 2002 Proceedings</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
