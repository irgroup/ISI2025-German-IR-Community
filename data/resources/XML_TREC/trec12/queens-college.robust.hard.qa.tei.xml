<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,100.32,87.33,411.37,12.64">TREC2003 Robust, HARD and QA Track Experiments using PIRCS</title>
				<funder ref="#_rgJynPy">
					<orgName type="full">U.S. Govt. DST/ATP</orgName>
				</funder>
				<funder ref="#_6UxMSCy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.60,132.40,54.22,10.80"><forename type="first">L</forename><surname>Grunfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.96,132.40,50.58,10.80"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.00,132.40,43.97,10.80"><forename type="first">N</forename><surname>Dinstl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.28,132.40,38.64,10.80"><forename type="first">P</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Queens College</orgName>
								<address>
									<postCode>11367</postCode>
									<settlement>CUNY Flushing</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,100.32,87.33,411.37,12.64">TREC2003 Robust, HARD and QA Track Experiments using PIRCS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B031E7482C9FB7C1F5518490871FF2FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the Robust, HARD and part of the QA tracks in TREC2003. For Robust track, a new way of doing ad-hoc retrieval based on web assistance was introduced. For HARD track, we followed the guideline to generate clarification forms for each topic so as to experiment with user feedback and metadata. In QA, we only did the factoid experiment. The approach to QA was similar to what we have used before, except that WWW searching was added as a front-end processing. These experiments are described in Sections 2, 3 and 4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Robust Track</head><p>Combining the results of a number of different retrieval outcome generally improves the overall performance <ref type="bibr" coords="1,143.61,350.16,10.89,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,154.50,350.16,7.26,8.96" target="#b1">2]</ref>. The intuitive explanation for this phenomenon is that the different retrievals are more likely to rank the same relevant documents early, than the same non-relevant documents. Consequently, combining retrieval methods that differ greatly often yields better results. Paradoxically we can obtain robust retrieval by adding the results of non-robust methods.</p><p>We start with our high performance PIRCS retrieval engine, which is considered robust since it is based on statistical methods, and combine it with retrievals for which the queries were generated based on returned web pages by the Google search engine operating on WWW data. Google queries are of Boolean type and returned results may be less stable. The addition of a single word can dramatically alter retrieval lists, and hence the queries defined by them.</p><p>For each robust task topic, our approach is to employ the 60 best-weighted words (except for common words on a stop list) contained in the top 20 web pages (returned by Google) as a reformulated query for our PIRCS engine. The rationale is that because the web is so huge and rich in content, there is a good chance that relevant pages containing the content terms of the original topic exist in the web. These pages will probably rank near the top by the Google, and may be rich in content terms related to the topic. These terms can therefore define, for our ad-hoc processing, useful alternate queries that can lead to different retrievals, and which could be useful for combining with the original retrieval list that is based on a query generated directly from the topic statement. The next section describes how we form Google queries from the description section of the original topic statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating Google Queries from Topic Statements</head><p>The Google search engine (http://www.google.com) accepts queries in a form similar to simplified Boolean expressions. It allows one to specify conjunctive clauses by having terms placed adjacent to each other, disjunctive clauses by placing the string OR between terms, and negated terms with a '-' prefixing them. Phrase matching is allowed by having words surrounded by double quotes. Un-stemmed words are used.</p><p>We employ three different strategies to create queries for Google retrieval. The queries are formed using only the Description section of a topic. Since previous experience has shown that retrievals combine better if they are dissimilar, our aim is to make the queries as different as possible. The three query formation strategies (identified by the names qds, qdp and qdt) are described below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>qds queries:</head><p>This simplest approach just employs sequentially the first six content words from a topic in a logical AND fashion. Caution is needed to avoid using too many words; otherwise nothing is retrieved. As an example consider the original Query 378 and the generated Google query G378:</p><p>Q378 -Identify documents that discuss opposition to the introduction of the euro, the European currency. G378 -opposition introduction euro, European currency. This method works fairly well except when queries are long. Consider the following: Q610 -Find claims made by U.S. small businesses regarding the adverse impact on their businesses of raising the minimum wage. G610 -claims U.S. small businesses adverse impact The generated query G610 does not include important terms like: "raising", "minimum", "wage", and the returned pages are not satisfactory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>qdp queries:</head><p>This and the following qdt method attempt to create Google queries by identifying important words in the topic based on Dekang Lin's MINIPAR parser <ref type="bibr" coords="2,331.80,317.88,11.72,8.96" target="#b2">[3]</ref> available at http://www.cs.ualberta.ca/ ~lindek/minipar.htm. MINIPAR is a general-purpose parser for an English sentence, identifies phrases, and generates a dependency structure where each word modifies at most one head word. Our strategy is to select the six best nouns based on the following order of priority: nouns appearing in phrases, nouns that designate a person, country, location, corporation, language or title, followed by other nouns. As an example, Q610 above generates the following: G610 -minimum wage U.S. businesses impact claims where "minimum", "wage" come first since it is part of a phrase, followed by "U.S.", a country, then followed by other nouns. The exclusion of verbs and adjectives sometimes harms performance. Consider Query 644: Q644 -Identify documents that discuss exotic species of animals that are imported into the U.S. or animals that are imported into the U.S. or U.K. G644 -U.S. U.K. species animals The resultant G644 misses out the important verb "imported" and adjective "exotic". Another example is: Q362 -Identify incidents of human smuggling. G362 -incidents smuggling which misses the important adjective "human".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>qdt queries:</head><p>This strategy first selects the phrases identified by MINIPAR. If there are none, other phrases defined by patterns (N1 gov N2), (N1 N2) (N2 N3) (if the 3-word phrase N1 N2 N3 are defined), and (A gov N) are then used to select words in this order. If query is &lt; 6 words, nouns and verbs are added (AND'ed) until query has 6 words. For example, Q362 becomes: G362 -"human smuggling" incidents where the quotes tell Google that "human smuggling" needs to be adjacent. Another example is:</p><p>Q643 -What harm have power dams in the Pacific northwest caused to salmon fisheries? G643 -"Pacific northwest" "salmon fisheries" harm dams which includes most of the content terms. A problem with this method is that some queries become too specific and no web pages are returned.</p><p>As an example, we include in the following the output of Q643 after analyzed by MINIPAR: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating PIRCS Queries from Retrieved Web Pages</head><p>Each of the Google queries in the previous sub-section was used to retrieve the top 20 web pages. From these html tags, non-text items and some common words are removed. A query is then created using the 60 best-weighted words. Weight of a word is defined as sum (over all web pages in which it occurs) of its frequency divided by html text length. Exceptionally long pages are skipped. The alternate query for our target retrieval is composed of these 60 words normalized by the least weight. For example, G643 in the previous qds section leads to the following alternative query for our PIRCS retrieval: SALMON 10 RIVER 9 DAMS 8 WATER 6 COLUMBIA 6 FISH 5 POWER 5 DAM 4 NORTHWEST 3 FEDERAL 3 SNAKE 3 BASIN 2 SPECIES 2 OREGON 2 .. + 46 other single terms Note that this alternative query includes important geographical information such as "Columbia River", "Snake River", "Columbia Basin" and "Oregon" that are absent in the original topic description section. This query has good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval based on Data Fusion</head><p>The final robust retrieval submission is based on combination of retrieval lists: using our normal query qd (description) or qa (all sections) obtained from a topic statement, and from alternative queries as discussed in the previous sub-section. All these query types undergo retrieval using our PIRCS engine on the given collection. Experiments have been performed using the description section of a topic only (pircRBd?, where ?=1 means normal PIRCS retrieval with PRF (pseudo-relevance feedback), ?=2 means web-assisted retrieval using combination strategy (i), while ?=3 means combination strategy (ii). The combination strategies are: (i) (qd 0.4) ⊕ (qds 0.2) ⊕ (qdp 0.2) ⊕ (qdt 0.2), and (ii) (qd 0.5) ⊕ (qdt 0.5). The symbol ⊕ is used to denote ranked list combination, and each retrieval list is weighted by the given factors. Experiments using all sections were also submitted pircRBa?: ?=1 means normal PIRCS retrieval, and ?=2 means (qa 0.3) ⊕ (pircRBd1 0.7). Note that pircRBa2 not only make use of web-assistance, but also combine description with all-section query results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Robust Track Results and Discussions</head><p>Results of our submissions are shown in Table <ref type="table" coords="3,278.78,688.56,3.89,8.96" target="#tab_4">1</ref>: split into 50 old topics, 50 new topics and all 100 merged. The 50 old topics may be considered as training topics since their relevant answers are known from previous TREC experiments, and the 50 new as testing set. Evaluation measures shown are the standard ones used in TREC: Rel.Ret = total relevant items in the 1000 retrieved documents, MAP = mean average precision, R-Pre = average precision value at the exact number of available relevant documents for each query, and Pnn = average precision at nn documents retrieved, where nn = 10, 20 or 30. Two new measures for Robust track are: number of topics without relevant documents at 10 retrieved '# no-relv-@10' and the 'area' measure which is a weighted sum of the precision for the worst-25% of the topics. An immediate observation is that all effectiveness values are much lower for the old topics than for the new, showing that the 50 old topics are much more difficult for retrieval with this TREC-8 collection of documents. In particular, the 'area' values are less than .01 for old 'description' queries, while they vary from .05 to .08 for new queries.</p><p>In Table <ref type="table" coords="4,129.14,200.64,3.76,8.96" target="#tab_4">1</ref>, we also show results of our PIRCS retrieval with PRF (pircRBd1 for description query and pircRBa1 for all-section queries) as basis for comparison. Of the two web-assisted description runs pircRBd2 and pircRBd3, the former has better performance. The latter makes use of qdt queries only and is not sufficiently robust, and more combination of retrievals appears useful. Using the 100 query results, one sees that our method of web-assisted retrieval brings substantial improvements for the Robust track measures: reducing the '# no-relv-@10' from 16 in pircRBd1(100) to 8 for pircRBd2(100), while the 'area' value increases from .0122 to .0219, an 80% boost. There are also smaller improvements in the other measures such as MAP, P10, etc. Similar improvements are also observed for the all-section queries.</p><p>Table <ref type="table" coords="4,116.04,304.19,4.98,8.96" target="#tab_5">2</ref> shows percentage improvements of certain measures of the web-assisted runs (for both description and all section queries) compared to their respective basis runs and separated into old and new queries. For the training set (Old-50), the web-assisted retrievals have double-digit percentage improvements compared to basis PIRCS retrieval for the description queries. For the testing (New-50) set, only pircRBd2 has slight improvements. We might have over-trained and the strategy does not carry over to testing set well, or that it is difficult to attain increases for the better performing queries of the New-50. For the long queries, however, except for slight decrease of 1% in two measures, other measures show good improvements over the Basis run in both old training and new testing sets.   Compared to all submissions, our results perform very favorably. Table <ref type="table" coords="5,399.65,423.72,4.98,8.96" target="#tab_6">3</ref> shows the comparison with median AP values. For example, the web-assisted pircRBd2 average precision has 74 topics better than median, 2 equal and 24 worse. One of the 74 has best average precision and none has worst. PircRBa2 is even better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HARD Track</head><p>'HARD' (High Accuracy Retrieval from Documents) is a new 2003 extension to previous ad-hoc retrieval experiments. Its purpose is to study the effects of user feedback and metadata on retrieval effectiveness. After a first round of retrieval by a search engine ('Basis Retrieval'), the system is allowed to solicit user feedback by creating a 'Clarification Form' concerning the topic. Users are allowed three minutes time per topic to answer questions presented in the form. Afterwards, the system is able to make use of the form data, as well as further on-topic metadata that is provided, in order to improve on the Basis Retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basis Retrieval</head><p>We employ our standard PIRCS ad-hoc processing and retrieval to provide first-round results for the user. This involves an initial retrieval plus a pseudo-relevance feedback (PRF) processing using 20 top documents and 60 best terms (20d60t). This 2-stage retrieval is called pircHDBt1, and is our 'Basis Retrieval' results in this HARD track environment. This involves only the title section of a topic as query. Another basis retrieval using both title and description sections as query is also submitted and denoted as: pircHDBtd1. We have also captured the above ad-hoc processing using only the first stage retrieval. Their retrieval results: pircHDBt0 and pircHDBtd0 are not submitted. An alternative is to use the first stage retrieval as basis retrieval. This saves second stage retrieval time, provides data faster for the user, but the basis would generally not be as high since PRF usually brings higher average precision values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clarification Form Design</head><p>After the Basis Retrieval of the previous section, three clarification forms are generated automatically and denoted as: C1 (submitted name QCSU1), C2 (QCSU2) and C3 (QCSU3). The general layout of our clarification form consists of three sections for users to make relevance judgment: i) candidate related terms, and ii) candidate related document titles or first sentences; iii) user keyword input. Each related term or document is associated with a radio button for clicking 'yes ', i.e. relevant. C1 makes use of WordNet <ref type="bibr" coords="6,90.00,167.28,11.72,8.96" target="#b3">[4]</ref> to obtain related terms to a query and display them in the clarification form. Clarification form C1 also does not display any related documents, and hence it does not rely on any retrieval and therefore less costly. C2 needs retrieval processing to define top documents and best terms for display. C2 makes use of the title section of a topic only for the initial retrieval. C3 is similar to C2, but uses both title and description sections of a topic for retrieval. The keyword input section is a scrollable. Since a user has only 3 minutes to complete a form, he or she would not have much time left for keyword input even though the scrollable window allows for the space.</p><p>For C1, we employ the title section of a topic and define each consecutive two-word as a phrase. Each phrase is passed to WordNet to pick up synonyms, and then the single words. The synonyms obtained are displayed in the 'related term' section of C1 for the user to judge. Some topics may fail to pick up synonyms; for these the related term section may be blank or the query words themselves. Users moreover can type in any words they deem important for a topic in the keyword input section of the clarification form. An example is Topic Hard-044 "Amusement Park Safety". The phrase "amusement park" gets Wordnet to return the following synonyms: "amusement park, funfair, pleasure ground". The last two phrases would not be obtained if words were used individually. Thus, the single words "amusement" gets synonyms "entertainment, amusement", and "park" gets "park, parkland, commons, common, green, ballpark, Mungo park, parking lot, car park, parking area". Since a user is present to judge these terms, presence of noise terms is tolerable. For C2 (title) and C3 (title + description), the related document section comes from the title or first sentence of the 10 top-ranked documents of a 2-stage ad-hoc retrieval. (Initially our design was to use the top documents from an initial retrieval. However, after the conference we discovered that an un-intended mix-up of files actually led to the use of the 2 nd stage results instead). 20 top-ranked terms are also displayed in the related term section. An undesirable situation arises because some of the feedback terms are actually stems not regular words. Porter's stemming algorithm has been employed, and the process is irreversible. Some other terms may be a combination of two stems into one string, which are the result of our adjacent two-word phrase indexing. These may be useful as indexing terms but not suitable for user browsing. We hope to remedy these situations in future enhancements. The user can click on whichever term or document they think is useful for a topic. It is possible that, for some difficult queries, none of the suggested terms or documents is related. However, the keyword input section is available for the user to type in additional words so as not to get frustrated. We believe that a user can complete the clarification form -click through 30 items and input some key words --in three minutes time. An example of a C2 form is shown in Fig. <ref type="figure" coords="7,153.07,189.12,3.88,8.96" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Final Retrieval -Document Level</head><p>The system has to decide on how to make use of the clarification form data to do further processing. Our strategy is to employ the 'user-clicked' related terms and the 'keywords' typed by the user to expand on the original query (either the title section or the title + description of each topic). Typed words can have typographic errors. We use Google's spell-check facility to remedy the situation. Afterwards, these keywords have to be stemmed to be compatible with other existing index terms. Repeated mention of the same term is kept to provide higher weight. Each expanded query is used for a fresh full 2-stage retrieval. However, during pseudo-relevance feedback (except for C1 form), the 'user-clicked' documents are guaranteed to be among the 20 feedback documents used. Term expansion is still kept at 60. These procedures provide the submissions pircHDC1t1, pircHDC2t1 and pircHDC3td1. It is to be noted that because of time constraints on the part of the assessors, 4 topics in C3 (036, 048, 053, 105) were not filled in. Results of these queries default back to those of the Basis Retrieval.</p><p>For clarification forms C2 and C3, we have two further submissions: pircHDC2t2 and pircHDC3td2. In many queries, the related document section receives very few or no 'clicks'. We used a threshold of less than 3. This suggests evidence that the Basis Retrieval results are not good (assuming the user can do correct judgment using the title or first sentence of the retrieved document), and may imply that the topic is a difficult one. ), we disable the 2 nd stage retrieval during final retrieval and used the initial retrieval results instead. The idea is that quite often for difficult topics, 2 nd stage retrieval can lead to worse results compared to initial retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Final Retrieval -Phrase Level</head><p>After the clarification data has been filled in, additional information in the form of metadata such as: purpose of retrieval, document genre wanted, user familiarity with topic, granularity of result, sample of relevant texts are also released concerning each topic. We only focus on the granularity metadata which can have values: document, passage, sentence and any. There are 16 (18-2 removed) queries that have the requirement of granularity = passage or sentence. For these, each of their retrieval lists is passed to our PIRCS-QA system (see also Section 3) for further processing to try to isolate a small text extent as answer for the topic. Our QA system can be summarized as follows <ref type="bibr" coords="7,330.63,570.96,10.84,8.96" target="#b4">[5]</ref>:</p><p>1) returning n top-ranked subdocuments from PIRCS retrieval using a query with stemming and stopword removal; 2) scoring and returning top-ranked sentences from the subdocuments with respect to the general context of the question keywords using a set of eleven heuristics --both raw and stemmed words were taken into account; 3) analyzing specific properties of the question to obtain its expected answer types, and assigning one of four functional modules that use keywords, meta-keywords and patterns to detect possible answers and add bonus weights to top-ranked sentences for selection purposes; 4) extracting answer strings of required size from top candidate sentences based on the previous question analysis with rules and heuristics for entity definition or identification.</p><p>Instead of evaluating sentences we evaluate paragraphs. They are detected by the &lt;/p&gt; tag or blank line or an indented line. Since more than one result was submitted and the order is important, we increased the retrieval score bonus. A document-offset bug which was present in our QA track this year was also fixed. The run-id's with the phrase processing are identified with a 'p' at the end like: pircHDC2tp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">HARD Track Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Results of Document-Level Evaluation</head><p>Table <ref type="table" coords="8,117.58,171.00,4.98,8.96" target="#tab_18">4</ref> below shows results of our submitted and some of the un-submitted runs. Each column has "%imp" denoting "% improvement" from the Basis Retrieval. Relevance judgment is of type 'hard' where partially relevant documents are excluded as relevant. 'Soft' type judgment results that include partially relevant documents are shown in Table <ref type="table" coords="8,250.07,205.44,3.75,8.96" target="#tab_19">5</ref>. The following TREC measures are tabulated: Rel.Ret (number of relevant documents within the top-ranked 1000), MAP (mean average precision over the 48 queries), Pnn (average precision at nn retrieved documents where nn = 10, 20 and 30), and RPre (average precision at rr retrieved where rr = the exact number of relevant documents for each query).</p><p>Concerning the short, title only query (average 2.8 terms) results, Table <ref type="table" coords="8,386.87,262.92,4.98,8.96" target="#tab_18">4</ref> shows that our submitted Basis Retrieval (pircHDBt1) is substantially better than the un-submitted first stage retrieval (pircHDBt0) without PRF, and provides a much higher basis to compare with the final retrievals using clarification or metadata.</p><p>The queries using Wordnet-based clarification forms (C1) for expansion have average 11.0 terms, and the final retrieval pircHDC1t1 improves over basis 3 to 11% in various measures except for P30 with a decrease of 1%. Using forms C2, the queries have average 12.6 terms and the pircHDC2t1 result improve over basis from 7 to 15% except for Rel.Ret with a decrease of 2%. Looking at MAP values, pircHDC1t1 employing the less costly C1 forms leads to slightly higher performance compared to pircHDC2t1. However, pircHDC2t1 has better effectiveness in low-recall high-precision retrieval region, achieving double-digit improvements for P10-30 over the basis; pircHDC1t1 has more erratic performance: from 10% increase in P10 to 1% decrease in P30. C1 Wordnet only suggests synonyms of the different senses of a word and for 21 queries did not suggest new word. C2 always have some suggested terms that may be related, not necessarily synonyms. It seems that the three minutes spent by a 'user' can bring out significant precision improvements (&gt;5%) over the basis retrieval.</p><p>Since there are two sources (related terms and keyword input) of user feedback to augment the original title query, we investigate to see which source is more useful and whether both are necessary. The un-submitted runs ~pircHDC1t1term and ~pircHDC1t1key show results using either the clicked related terms or the typed keywords only. Each leads to an average of 4.8 and 9.2 query terms respectively. Similarly for C2 runs ~pirHDC2t1term (7.5 terms) and ~pircHDC2t2key (8.3 terms). Thus, related terms from Wordnet provide on average only 4.8-2.8=2 relevant words while PRF provides 7.5-2.8=4.7. This reinforces previous experience that general purpose thesauri often miss the query words or may not contain the right sense of query terms (for a user to click). In both C1 and C2 cases, use of only one source of feedback data performs worse than using both, and often worse than the basis values. When both sources are used, the original title, input keywords and clicked related terms often overlap. This is equivalent to user weighting some good terms higher, and may contribute to better results. Comparing pircHDC1t1term with pircHDC1t1key rows, the former is uniformly worse, leading us to conclude that Wordnet supplied terms are less useful than typed keywords. On the other hand, comparing pircHDC2t1term and pircHDC2t1key, the former has better results except for Rel.Ret, leading us to believe that PRF supplied terms are more useful than typed keywords. Different forms have different keywords typed, probably by different users.</p><p>Our submitted run pircHDC2t2 that disables PRF when user clicks fewer than 3 relevant documents, was in error due to a use of wrong files. The corrected run is shown as ~pircHDC2t2. It is a bit worse compared to pircHDC2t1 and the procedure is not effective. Of the 15 queries affected, only 5 is better to ignore PRF processing or little change. The additional user-typed keywords may make a query better. It is also qualitatively similar in the case of using title+description queries (pircHDC3td2).</p><p>Overall, results for the longer title+description queries (9.2 terms average) improves slightly over the title only run: e.g. the basis run pircHDBtd1 MAP value of .3277 is better by 2% compared to pircHDBt1 title basis value of .3219. The run with clarification form C3, pircHDC3td1 with average of 18 query terms, improves over the basis pircHDBtd1 between 8 to 15% in all measures except for a 1% decrease in Rel.Ret. As in title runs, the effect of using either the clicked related terms (un-submitted ~pircHDC3td1term, average 15.5 terms) or the typed keywords (~pircHDC3td1key, average 14.3 terms) is to depress performance compared to using both. Also, clicked terms are preferred over user input keywords as in the title only results. This run provides the best overall MAP value of 0.3604 and R-Pre of 0.3875 for all submitted runs.</p><p>Our official title run pircHDC2t1 compares favorably with other submitted runs, with 37 queries (1 best) above median, 10 below median (1 worst) and 1 equal to median average precision. Table <ref type="table" coords="9,478.35,177.60,4.98,8.96" target="#tab_19">5</ref> provides results of soft evaluation, i.e. when partially relevant documents are also treated as relevant. Behavior is similar to that of hard evaluation in Table <ref type="table" coords="9,258.46,200.64,3.76,8.96" target="#tab_18">4</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Results of Phrase-Level Evaluation</head><p>Table <ref type="table" coords="10,116.89,382.56,4.98,8.96" target="#tab_21">6</ref> shows results of our passage retrieval. The official evaluation measures are P10, R-Pre and Fmeasure at 30 documents retrieved (F30). As discussed before, run-id's that end with 'p' undergo special passage processing and return a passage list, i.e. document id with a text extent. Other runs without 'p' return document id lists only. We may consider them as document id with a text extent equal to the whole document length, and each document id contributes one retrieval result only. Errors were also discovered for the pircHDC2tp and pircHDC3tdp runs: there were 18 queries out of 42 that went through our QA </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question-Answering Track</head><p>The Internet is a great storehouse of information and facts. Millions search it daily and use it to solve their information needs. It is not surprising therefore that a number of participants in the QA track make use of it as a source of knowledge. This year we join this trend and extend our QA system to use results of the Google search engine.</p><p>We extract answers from Google retrievals in two different ways. For certain question types we developed reliable patterns, which identify the answer term. For other questions we use the most frequent word from the snippets returned by Google. For questions that require named entity recognition we make use of Minipar's NE capability.</p><p>We submitted three runs for the passage track. pircsQA1 is virtually identical to our QA system for the 2001 QA track. It uses the top 100 documents retrieved by our PIRCS search engine. It combines probabilistic IR methods with search pattern recognition to select the highest-ranking sentence.</p><p>PIRCS does not always return documents with the answer and sometimes the document with the correct answer is ranked very low. To remedy this situation we have merged the original query submitted to PIRCS with possible answer extracted from a Google answer snippets. pircsQA2 uses the top 100 documents created by this retrieval. pircsA3 makes use of a different strategy to utilize suggested answers from Google: extra bonus is added to sentences that contain them.</p><p>The official results are quite low, due to a bug in the system, which caused the document offsets to be calculated incorrectly. Here we report our own unofficial evaluation, for the 382 queries which had answers in the document collection.</p><p>Score % improve pircsQA1 0.249 pircsQA2 0.264 6.32% pircsQA3 0.338 35.79% pircsQA1 performed worse than in 2001, which indicates that the queries are getting harder. pircsQA2 shows that results can be improved by enhancing the query submitted to the front end search engine, but not by much. The greatest improvement comes from searching the test collection for answers found in the Web. This makes the task somewhat unrealistic, sometimes the answer is available, but we don't tell the user, because it is not in the document collection. Our systems performance can be improved by developing improved patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>A method of exploiting the WWW to improve ad-hoc retrieval from a target collection was introduced for the Robust Track. This involves forming Boolean-type Google queries from TREC description queries to perform web retrieval, defining alternate queries from returned web pages, and data fusion to define final results. This approach was successful and has improved the worst-query measures substantially from 30% to 80%.</p><p>For the HARD Track, clarification forms for each query were designed to solicit relevant term and document information from a user. One type makes use of Wordnet to suggest synonyms to query terms and asks the user to 'click' the relevant ones for query expansion purposes. In addition, users can type in more keywords. Data for this form does not rely on a retrieval and is less costly. A second type of form makes use of retrieval results of the original query, with the top retrieved documents and top related terms presented to the user for feedback. In both cases, results seem to indicate that two inputs, user keywords and 'clicked' terms, are necessary to get improvements compared to results not using these forms. Although document-level results show that forms that rely on a retrieval has a slight edge over the Wordnet forms, passage-level results indicate otherwise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,200.16,661.32,211.42,8.96;6,125.88,395.12,360.24,252.24"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: C2 Clarification Form for Query Hard-033</figDesc><graphic coords="6,125.88,395.12,360.24,252.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,94.80,419.69,421.37,66.90"><head>ID Rel.Ret MAP R.Pre P10 P20 P30 # no-relv-@10 area Total No. of Relevant Documents: old50 set=4416, new50 set=1658, all100=6074 Query size: description section only pircRBd1 (old)</head><label></label><figDesc></figDesc><table coords="4,94.80,454.44,421.37,32.16"><row><cell></cell><cell>2216=50% .1526 .1887 .3220 .2810 .2393</cell><cell>14/50=28%</cell><cell>.0045</cell></row><row><cell>pircRBd1 (new)</cell><cell>1534=93% .4022 .3963 .5200 .4230 .3500</cell><cell>2/50=4%</cell><cell>.0804</cell></row><row><cell>pircRBd1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,94.80,478.49,421.37,56.82"><head>(100) 3750=62% .2774 .2925 .4210 .3520 .2947 16/100=16% .0122 Web-assisted runs: (qd 0.4) ⊕ ⊕ ⊕ ⊕ (qds 0.2) ⊕ ⊕ ⊕ ⊕ (qdp 0.2) ⊕ ⊕ ⊕ ⊕ (qdt 0.2) pircRBd2 (old)</head><label></label><figDesc></figDesc><table coords="4,94.80,503.16,421.37,32.16"><row><cell></cell><cell>2377=54% .1772 .2148 .3820 .3240 .2787</cell><cell>6/50=12%</cell><cell>.0091</cell></row><row><cell>pircRBd2 (new)</cell><cell>1565=94% .4029 .3845 .5320 .4170 .3467</cell><cell>2/50=4%</cell><cell>.0819</cell></row><row><cell>pircRBd2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,94.80,527.21,421.37,166.23"><head>(100) 3942=65% .2900 .2996 .4570 .3705 .3127 8/100=8% .0219 Web-assisted runs: (qd 0.5)</head><label></label><figDesc></figDesc><table coords="4,94.80,539.57,421.37,153.87"><row><cell></cell><cell>⊕ ⊕ ⊕ ⊕ (qdt 0.5)</cell><cell></cell><cell></cell></row><row><cell>pircRBd3 (old)</cell><cell>2287=52% .1754 .2106 .3760 .3250 .2727</cell><cell>7/50=14%</cell><cell>.0065</cell></row><row><cell>pircRBd3 (new)</cell><cell>1444=87% .3878 .3781 .5220 .4080 .3273</cell><cell>2/50=4%</cell><cell>.0540</cell></row><row><cell cols="2">pircRBd3 (100) 3731=61% .2816 .2944 .4490 .3665 .3000</cell><cell>9/100=9%</cell><cell>.0165</cell></row><row><cell></cell><cell>Query size: all sections of topic</cell><cell></cell><cell></cell></row><row><cell>pircRBa1 (old)</cell><cell>2562=58% .1796 .2282 .3640 .3230 .2867</cell><cell>6/50=12%</cell><cell>.0136</cell></row><row><cell cols="2">PircRBa1 (new) 1494=90% .4405 .4150 .5440 .4550 .3800</cell><cell>3/50=6%</cell><cell>.0716</cell></row><row><cell>pircRBa1 (100)</cell><cell>4056=67% .3101 .3216 .4540 .3890 .3333</cell><cell>9/100=9%</cell><cell>.0203</cell></row><row><cell></cell><cell>Web-assisted runs: (qa 0.3) ⊕ ⊕ ⊕ ⊕ (pircRBd1 0.7)</cell><cell></cell><cell></cell></row><row><cell>pircRBa2 (old)</cell><cell>2641=60% .1854 .2234 .4000 .3340 .2907</cell><cell>5/50=10%</cell><cell>.0135</cell></row><row><cell>pircRBa2 (new)</cell><cell>1575=95% .4369 .4159 .5760 .4520 .3760</cell><cell>1/50=2%</cell><cell>.1062</cell></row><row><cell>pircRBa2 (100)</cell><cell>4217=69% .3111 .3197 .4880 .3930 .3333</cell><cell>6/100=6%</cell><cell>.0290</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,126.84,696.48,358.37,8.96"><head>Table 1 : Robust Retrieval -Summary for All Submitted Runs --Lenient Evaluation Old-50 training set New-50 testing set MAP %imp P10 %imp area %imp MAP %imp P10 %imp area %imp Query size: description section only pircRBd1</head><label>1</label><figDesc></figDesc><table coords="5,102.48,115.20,401.75,104.48"><row><cell>.1526 *</cell><cell>.3220 *</cell><cell>.0045 *</cell><cell>.4022 *</cell><cell>.5200 *</cell><cell>.0804</cell><cell>*</cell></row><row><cell></cell><cell></cell><cell cols="2">Web-assisted runs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pircRBd2 .1772 +16</cell><cell cols="2">.3820 +19 .0091 +102</cell><cell>.4029 +0</cell><cell>.5320 +2</cell><cell cols="2">.0819 +2</cell></row><row><cell>pircRBd3 .1754 +15</cell><cell cols="2">.3760 +17 .0065 +44</cell><cell>.3878 -4</cell><cell>.5220 +0</cell><cell cols="2">.0540 -33</cell></row><row><cell></cell><cell cols="3">Query size: all sections of topic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pircRBa1 .1796 *</cell><cell>.3640 *</cell><cell>.0136 *</cell><cell>.4405 *</cell><cell>.5440 *</cell><cell cols="2">.0716 *</cell></row><row><cell></cell><cell></cell><cell cols="2">Web-assisted runs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pircRBa2 .1854 +3</cell><cell cols="2">.4000 +10 .0135 -1</cell><cell>.4369 -1</cell><cell>.5760 +6</cell><cell cols="2">.1062 +48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,133.56,235.68,344.73,148.16"><head>Table 2 : Comparing Web-Assisted to Basis Retrieval -Training and Testing Sets</head><label>2</label><figDesc></figDesc><table coords="5,156.12,276.00,308.51,107.84"><row><cell></cell><cell></cell><cell>Median AP</cell><cell></cell><cell></cell><cell>% no-</cell><cell></cell><cell>Worst25%</cell></row><row><cell>Run ID</cell><cell>Best</cell><cell cols="3">(&gt;/=/&lt;) Worst MAP</cell><cell cols="2">relv-@10 area</cell><cell>MAP</cell></row><row><cell>pircRBd1</cell><cell>2</cell><cell>64/3/33</cell><cell>1</cell><cell>0.2774</cell><cell>16%</cell><cell cols="2">0.0122 0.0310</cell></row><row><cell>pircRBd2</cell><cell>1</cell><cell>74/2/24</cell><cell>0</cell><cell>0.2900</cell><cell>8%</cell><cell cols="2">0.0219 0.0478</cell></row><row><cell>pircRBd3</cell><cell>4</cell><cell>73/1/26</cell><cell>2</cell><cell>0.2816</cell><cell>9%</cell><cell cols="2">0.0165 0.0418</cell></row><row><cell>pircRBa1</cell><cell>11</cell><cell>79/0/21</cell><cell>0</cell><cell>0.3101</cell><cell>9%</cell><cell cols="2">0.0203 0.0467</cell></row><row><cell>pircRBa2</cell><cell>7</cell><cell>86/2/12</cell><cell>0</cell><cell>0.3111</cell><cell>6%</cell><cell cols="2">0.0290 0.0622</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,165.12,400.06,281.77,9.94"><head>Table 3 : Comparing PIRCS 100-Topic Results with Median</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,77.40,235.73,440.24,79.95"><head>title No clarification form used (average query size 2.8 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,235.73,440.24,79.95"><row><cell></cell><cell cols="6">Rel.Ret %imp MAP %imp R-Prec %imp P10 %imp</cell><cell>P20 %imp</cell><cell>P30 %imp</cell></row><row><cell></cell><cell cols="7">hard criteria evaluation (48 queries; total 5123 relevant documents)</cell><cell></cell></row><row><cell cols="6">Query size = ~pircHDBt0 3482 -11 .2170 -33 .2558 -26</cell><cell>.3500 -21</cell><cell>.3094 -27</cell><cell>.2917 -29</cell></row><row><cell>pircHDBt1</cell><cell>3893</cell><cell>*</cell><cell>.3219 *</cell><cell>.3460</cell><cell>*</cell><cell>.4417 *</cell><cell>.4229 *</cell><cell>.4132 *</cell></row><row><cell>Clarification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,77.40,306.72,435.81,32.96"><head>form with Wordnet (average query size 11.0 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,318.48,435.81,21.20"><row><cell>pircHDC1t1</cell><cell>3999</cell><cell>+3</cell><cell>.3583 +11</cell><cell>.3740 +8</cell><cell>.4854 +10 .4344 +3</cell><cell>.4111 -1</cell></row><row><cell>Using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="9,77.40,330.72,438.33,32.96"><head>clicked Wordnet terms only (average query size 4.8 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,342.48,438.33,21.20"><row><cell>~pircHDC1t1term 3766</cell><cell>-3</cell><cell>.2995 -7</cell><cell>.3206 -7</cell><cell>.4292 -3</cell><cell>.3792 -10</cell><cell>.3625 -12</cell></row><row><cell>Using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="9,77.40,354.72,435.79,32.96"><head>input keywords only (average query size 9.2 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,366.48,435.79,21.20"><row><cell>~pircHDC1t1key</cell><cell>3802</cell><cell>-2</cell><cell>.3197 -1</cell><cell>.3437 -1</cell><cell>.4604 +4</cell><cell>.4083 -3</cell><cell>.3958 -4</cell></row><row><cell>Clarification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="9,77.40,378.72,440.76,32.96"><head>form with PRF data (average query size 12.6 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,390.48,440.76,21.20"><row><cell>pircHDC2t1</cell><cell>3812</cell><cell>-2</cell><cell>.3536 +10</cell><cell>.3717 +7</cell><cell>.5083 +15 .4802 +14 .4535 +10</cell></row><row><cell>Using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="9,77.40,402.72,435.73,32.96"><head>clicked PRF terms only (average query size 7.5 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,414.48,435.73,21.20"><row><cell>~pircHDC2t1term 3507</cell><cell>-10</cell><cell>.3021 -6</cell><cell>.3242 -6</cell><cell>.5042 +14 .4635 +10 .4236 +3</cell></row><row><cell>Using</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="9,77.40,426.72,442.94,56.72"><head>input keywords only (average query size 8.3 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,438.48,442.94,44.96"><row><cell>~pircHDC2t1key</cell><cell>3564</cell><cell>-8</cell><cell>.2900 -10</cell><cell>.3079 -11</cell><cell>.4437 +0</cell><cell>.4083 -3</cell><cell>.3785 -8</cell></row><row><cell>pircHDC2t2</cell><cell>3589</cell><cell>-8</cell><cell>.3048 -5</cell><cell>.3191 -8</cell><cell>.4542 +3</cell><cell>.4354 +3</cell><cell>.4125 -0</cell></row><row><cell>~pircHDC2t2</cell><cell>3791</cell><cell>-3</cell><cell>.3469 +8</cell><cell>.3648 +5</cell><cell cols="3">.5063 +15 .4812 +14 .4535 +10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="9,77.40,486.72,435.79,56.96"><head>Query size = title + description No clarification form used (average query size 9.2 terms) ~pircHDBtd0</head><label></label><figDesc></figDesc><table coords="9,77.40,510.48,435.79,33.20"><row><cell></cell><cell>3606</cell><cell>-9</cell><cell>.2719 -17</cell><cell cols="2">.3075 -8</cell><cell>.4417 -9</cell><cell>.3875 -7</cell><cell>.3424 -9</cell></row><row><cell>pircHDBtd1</cell><cell>3958</cell><cell>*</cell><cell>.3277 *</cell><cell>.3360</cell><cell>*</cell><cell>.4875 *</cell><cell>.4167 *</cell><cell>.3743 *</cell></row><row><cell>Clarification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="9,77.40,534.72,442.97,32.96"><head>form with PRF data (average query size 18.0 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,546.48,442.97,21.20"><row><cell>pircHDC3td1</cell><cell>3915</cell><cell>-1</cell><cell>.3589 +10</cell><cell>.3813 +13</cell><cell>.5271 +8</cell><cell>.4802 +15 .4306 +15</cell></row><row><cell>Using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" coords="9,77.40,558.72,442.97,87.08"><head>clicked PRF terms only (average query size 15.5 terms)</head><label></label><figDesc></figDesc><table coords="9,77.40,570.48,442.97,75.32"><row><cell>~pircHDC3td1--</cell><cell>3766</cell><cell>-5</cell><cell>.3388 +3</cell><cell>.3574 +6</cell><cell>.4875 +0</cell><cell>.4323 +4</cell><cell>.4049 +9</cell></row><row><cell>term</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Using input keywords only (average query size 14.3 terms)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>~pircHDC3td1--</cell><cell>3699</cell><cell>-7</cell><cell>.3033 -7</cell><cell>.3169 -6</cell><cell>.4604 -6</cell><cell>.3937 -6</cell><cell>.3632 -3</cell></row><row><cell>key</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pircHDC3td2</cell><cell>3901</cell><cell>-2</cell><cell>.3604 +10</cell><cell>.3875 +15</cell><cell>.5146 +6</cell><cell cols="2">.4740 +14 .4236 +13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" coords="9,135.12,660.60,341.77,8.96"><head>Table 4 : 'HARD' Retrieval with Hard Evaluation (~ denotes un-submitted data)</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" coords="10,137.88,335.64,336.19,8.96"><head>Table 5 : 'HARD' Retrieval with Soft Evaluation (~ denotes un-submitted data)</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" coords="10,109.32,475.01,392.83,200.91"><head>-ID P10 %imp R-Prec %imp R30 %imp P30 %imp F(30) %imp hard criteria evaluation (42 queries) Query size = title pircHDBt1</head><label></label><figDesc></figDesc><table coords="10,109.32,510.96,384.98,164.96"><row><cell></cell><cell>.2809</cell><cell>*</cell><cell>.1810</cell><cell>*</cell><cell>.2359 *</cell><cell cols="2">.2491 *</cell><cell>.1491</cell><cell>*</cell></row><row><cell>pircHDC1t1</cell><cell cols="2">.3152 +12</cell><cell cols="5">.2335 +29 .2724 +15 .2369 -5</cell><cell cols="2">.1479 -1</cell></row><row><cell>pircHDC1tp</cell><cell cols="2">.3770 +34</cell><cell cols="5">.3195 +77 .1839 -22 .3081 +24</cell><cell cols="2">.1403 -6</cell></row><row><cell>pircHDC2t1</cell><cell cols="2">.3209 +14</cell><cell cols="3">.2145 +19 .2501 +6</cell><cell cols="2">.2766 +11</cell><cell cols="2">.1549 +4</cell></row><row><cell>pircHDC2tp</cell><cell cols="2">.3754 +34</cell><cell cols="5">.2508 +39 .1426 -40 .3191 +28</cell><cell cols="2">.1269 -15</cell></row><row><cell>~pircHDC2tp</cell><cell cols="2">.3829 +36</cell><cell cols="5">.2595 +43 .1762 -25 .3336 +34</cell><cell cols="2">.1423 -5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Query size = title+description</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pircHDBtd1</cell><cell>.3186</cell><cell>*</cell><cell>.1699</cell><cell>*</cell><cell>.2404 *</cell><cell>.2316</cell><cell>*</cell><cell>.1280</cell><cell>*</cell></row><row><cell>pircHDC3td1</cell><cell cols="2">.3359 +5</cell><cell cols="5">.2141 +26 .2922 +22 .2374 +3</cell><cell cols="2">.1452 +13</cell></row><row><cell>pircHDC3tdp</cell><cell cols="2">.3353 +5</cell><cell cols="5">.2555 +50 .1746 -27 .2772 +20</cell><cell cols="2">.1283 +0</cell></row><row><cell cols="3">~ pircHDC3tdp .3438 +8</cell><cell>.2575</cell><cell cols="4">+52 .1812 -25 .2797 +21</cell><cell>.1294</cell><cell>+1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" coords="10,90.00,74.69,431.26,624.99"><head>Table 6 : 'HARD' Passage Retrieval with Hard Evaluation (~ denotes un-submitted data)</head><label>6</label><figDesc>The rest were supposed to be the document-level retrieval from the basis pircHDC1t1 and pircHDC3td1 respectively; however we erroneously used the first stage retrievals pircHDC2t0 and pircHDC3td0 instead. The corrected runs are shown in Table6as ~pircHDC2tp and ~pircHDC3tdp.It is seen that for all passage runs with run-id 'p', precision values are high, but recall and F(30) values are low compared to the basis. One reason precision values are high is that precision calculation favors shorter passages than whole documents. One reason recall values are low is that relevant documents always cover all relevant materials, especially if it has multiple relevant passages. Comparing pircHDC1t1 (which has Wordnet form C1 feedback) with the basis pircHDBt1, we see P10 improves but F(30) decreases by 1% due to individual precision and recall values at 30 retrieved. Looking at pircHDC1tp where the retrieved list are passage-level, precision values improve over basis probably also because relevant passages are promoted earlier. However, recall values at 30 retrieved are low leading to decreases in F(30).</figDesc><table coords="10,90.00,74.69,431.26,246.15"><row><cell>Run-ID</cell><cell>Rel.Ret</cell><cell></cell><cell>MAP</cell><cell></cell><cell>R-Prec</cell><cell>P10</cell><cell>P20</cell><cell>P30</cell></row><row><cell></cell><cell>%imp</cell><cell></cell><cell>%imp</cell><cell></cell><cell>%imp</cell><cell>%imp</cell><cell>%imp</cell><cell>%imp</cell></row><row><cell></cell><cell cols="7">soft criteria evaluation (48 queries; total 7576 relevant documents)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Query size = title</cell><cell></cell><cell></cell></row><row><cell>~pircHDBt0</cell><cell>4938</cell><cell>-8</cell><cell cols="2">.2548 -30</cell><cell>.2893 -25</cell><cell cols="3">.4460 -17 .4140 -21 .3833 -26</cell></row><row><cell>pircHDBt1</cell><cell>5372</cell><cell>*</cell><cell>.3650</cell><cell>*</cell><cell>.3857 *</cell><cell cols="3">.5396 * .5260 * .5167 *</cell></row><row><cell>pircHDC1t1</cell><cell>5533</cell><cell>+3</cell><cell cols="3">.4069 +11 .4250 +10</cell><cell cols="3">.5979 +11 .5469 +4 .5299 +3</cell></row><row><cell>pircHDC1t1term</cell><cell>5251</cell><cell>-2</cell><cell cols="2">.3449 -6</cell><cell>.3703 -4</cell><cell cols="3">.5375 +0 .4958 -6 .4764 -8</cell></row><row><cell>pircHDC1t1key</cell><cell>5202</cell><cell>-3</cell><cell cols="2">.3585 -2</cell><cell>.3882 +1</cell><cell cols="3">.5646 +5 .5094 -3 .4937 -4</cell></row><row><cell>pircHDC2t1</cell><cell>5215</cell><cell>-3</cell><cell cols="2">.3986 +9</cell><cell>.4242 +10</cell><cell cols="3">.6500 +20 .6104 +16 .5799 +12</cell></row><row><cell>pircHDC2t2term</cell><cell>4726</cell><cell>-12</cell><cell cols="2">.3188 -13</cell><cell>.3512 -9</cell><cell cols="3">.6021 +12 .5479 +4 .5035 -3</cell></row><row><cell>pircHDC2t2key</cell><cell>4857</cell><cell>-10</cell><cell cols="2">.3191 -13</cell><cell>.3501 -9</cell><cell cols="3">.5438 +1 .5208 -1 .4868 -6</cell></row><row><cell>pircHDC2t2</cell><cell>4891</cell><cell>-9</cell><cell cols="2">.3314 -9</cell><cell>.3454 -10</cell><cell cols="3">.5583 +3 .5323 +1 .5062 -2</cell></row><row><cell>~pircHDC2t2</cell><cell>5201</cell><cell>-3</cell><cell cols="2">.3902 +7</cell><cell>.4156 +8</cell><cell cols="3">.6479 +20 .6094 +16 .5771 +12</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Query size = title + description</cell><cell></cell><cell></cell></row><row><cell>~pircHDBtd0</cell><cell>5069</cell><cell>-7</cell><cell cols="2">.3037 -17</cell><cell>.3387 -14</cell><cell cols="3">.5600 -5 .4900 -9 .4473 -7</cell></row><row><cell>pircHDBtd1</cell><cell>5430</cell><cell>*</cell><cell>.3656 *</cell><cell></cell><cell>.3932 *</cell><cell>.5875 *</cell><cell>.5365 *</cell><cell>.4826 *</cell></row><row><cell>pircHDC3td1</cell><cell>5470</cell><cell>+1</cell><cell cols="2">.3934 +8</cell><cell>.4131 +5</cell><cell cols="3">.6271 +7 .5896 +10 .5403 +12</cell></row><row><cell cols="2">PircHDC3td1term 5203</cell><cell>-4</cell><cell cols="2">.3667 +0</cell><cell>.3926 -0</cell><cell cols="3">.5771 -2 .5281 -2 .4979 +3</cell></row><row><cell>PircHDC3td1key</cell><cell>5118</cell><cell>-6</cell><cell cols="2">.3363 -8</cell><cell>.3532 -10</cell><cell cols="3">.5667 -4 .5010 -7 .4708 -2</cell></row><row><cell>pircHDC3td2</cell><cell>5445</cell><cell>+0</cell><cell cols="2">.3937 +8</cell><cell>.4133 +5</cell><cell cols="3">.6167 +5 .5865 +9 .5319 +10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" coords="11,173.88,594.34,264.21,9.94"><head>Table 7 : Unofficial results for 382 queries with answers.</head><label>7</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported the <rs type="institution">Space and Naval Warfare Systems Center San Diego</rs>, under grant No. <rs type="grantNumber">N66001-1-8912</rs>, and by a <rs type="funder">U.S. Govt. DST/ATP</rs> contract <rs type="grantNumber">2003*H532600*000</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rgJynPy">
					<idno type="grant-number">N66001-1-8912</idno>
				</org>
				<org type="funding" xml:id="_6UxMSCy">
					<idno type="grant-number">2003*H532600*000</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,104.85,368.52,417.07,8.96;12,90.00,380.04,55.51,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,259.80,368.52,165.78,8.96">Fusion via a linear combination of scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,432.54,368.52,85.48,8.96">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.43,403.08,416.65,8.96;12,90.00,414.48,432.04,8.96;12,90.00,426.00,381.45,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,298.08,403.08,224.00,8.96;12,90.00,414.48,25.12,8.96">TREC-8 ad-hoc, query and filtering experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,137.93,414.48,303.51,8.96">Information Technology: The Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>US GPO; Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="217" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.23,449.04,416.74,8.96;12,90.00,460.56,61.13,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,168.41,449.04,272.31,8.96">PRINCIPAR -an efficient, broad-coverage, principle-based parser</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,448.08,449.04,73.88,8.96;12,90.00,460.56,8.38,8.96">Proc of COLING-94</title>
		<meeting>of COLING-94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="482" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.21,483.48,396.14,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,175.77,483.48,141.56,8.96">Wordnet: an online lexical database</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,324.00,483.48,157.05,8.96">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,104.49,506.52,417.48,8.96;12,90.00,518.03,431.98,8.96;12,90.00,529.55,431.99,8.96;12,90.00,541.07,145.71,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,329.49,506.52,192.48,8.96;12,90.00,518.03,177.51,8.96">TREC 2001 Question-answering, web and cross language track experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,297.05,518.03,224.93,8.96;12,90.00,529.55,44.01,8.96">Information Technology: The Tenth Text Retrieval Conference</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting><address><addrLine>US GPO; Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="452" to="456" />
		</imprint>
	</monogr>
	<note>TREC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
