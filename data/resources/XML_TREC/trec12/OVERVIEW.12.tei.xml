<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,219.60,112.08,172.72,15.49">Overview of TREC 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,261.48,144.47,88.94,10.76"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ajou University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Inc. OcE Technologies BBN Oregon Health and Science University</orgName>
								<orgName type="institution">NTT Communication Science Laboratories Axontologic</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">California State University San Marcos Queens College</orgName>
								<orgName type="institution" key="instit2">CUNY Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">group)</orgName>
								<orgName type="institution">RMIT University Center for Computing Science &amp; U. Maryland Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">groups) Chinese Information Processing Center Sabir Research</orgName>
								<orgName type="department" key="dep2">Inc. Clairvoyance Corporation State</orgName>
								<orgName type="department" key="dep3">York at Buffalo CL Research StreamSage</orgName>
								<orgName type="department" key="dep4">Inc. Copernic Research Tarragon Consulting Corporation</orgName>
								<orgName type="institution">University of New</orgName>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="institution">CSIRO Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">groups) Dublin City University Universitat Politecnica de Catalunya &amp; Universitat de Girona Erasmus MC Universit√© de Neuchatel Fondazione Ugo Bordoni University Hospital of Geneva Fraunhofer Institute (SCAI)</orgName>
								<orgName type="department" key="dep2">Fairbanks Fudan University</orgName>
								<orgName type="institution">University of Alaska</orgName>
							</affiliation>
							<affiliation key="aff10">
								<orgName type="institution">University of Albany</orgName>
							</affiliation>
							<affiliation key="aff11">
								<orgName type="institution">Hummingbird University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff12">
								<orgName type="institution" key="instit1">IBM Research</orgName>
								<orgName type="institution" key="instit2">Haifa University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff13">
								<orgName type="institution">IBM TJ Watson Research Center</orgName>
							</affiliation>
							<affiliation key="aff14">
								<orgName type="department">Columbia U. Illinois Institute of Technology</orgName>
								<orgName type="institution">University of Colorado</orgName>
							</affiliation>
							<affiliation key="aff15">
								<orgName type="institution">University of Edinburgh Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
								</address>
							</affiliation>
							<affiliation key="aff16">
								<orgName type="institution">University of Edinburgh &amp; Stanford U</orgName>
							</affiliation>
							<affiliation key="aff17">
								<orgName type="department">IRIT</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay University of Glasgow</orgName>
								<address>
									<country>SIG</country>
								</address>
							</affiliation>
							<affiliation key="aff18">
								<orgName type="department">ITC-irst</orgName>
								<orgName type="institution">University of Helsinki</orgName>
							</affiliation>
							<affiliation key="aff19">
								<orgName type="institution">University of Illinois at Chicago Johns Hopkins University</orgName>
								<address>
									<country>APL</country>
								</address>
							</affiliation>
							<affiliation key="aff20">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff21">
								<orgName type="institution">Kasetsart University</orgName>
							</affiliation>
							<affiliation key="aff22">
								<orgName type="institution">University of Iowa</orgName>
							</affiliation>
							<affiliation key="aff23">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
							<affiliation key="aff24">
								<orgName type="institution">University of Limerick Language Computer Corporation University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff25">
								<orgName type="department">Inc. University of Massachusetts Macquarie</orgName>
								<orgName type="institution" key="instit1">Lehigh University University of Maryland Baltimore County LexiClone</orgName>
								<orgName type="institution" key="instit2">University University of Melbourne Massachusetts Institute of Technology University of Michigan</orgName>
							</affiliation>
							<affiliation key="aff26">
								<orgName type="institution">Meiji University</orgName>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="department">Microsoft Research Asia</orgName>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
							<affiliation key="aff28">
								<orgName type="institution">University of Sheffield</orgName>
							</affiliation>
							<affiliation key="aff29">
								<orgName type="institution">Microsoft Research Ltd University of Southern California</orgName>
								<address>
									<country>ISI</country>
								</address>
							</affiliation>
							<affiliation key="aff30">
								<orgName type="institution">MITRE Corp</orgName>
							</affiliation>
							<affiliation key="aff31">
								<orgName type="institution">University of Sunderland National Library of Medicine &amp; U. Maryland University of Tampere National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff32">
								<orgName type="institution">University of Tokyo</orgName>
							</affiliation>
							<affiliation key="aff33">
								<orgName type="institution" key="instit1">National Taiwan University University of Wales</orgName>
								<orgName type="institution" key="instit2">Bangor National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,219.60,112.08,172.72,15.49">Overview of TREC 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5C6D31624D1491F8B151C813C67DF845</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">groups) University of Waterloo (groups) New Mexico State University</head><p>Virginia Tech</p><p>A known-item search is similar to an ad hoc search but the target of the search is a particular document (or a small set of documents) that the searcher knows to exist in the collection and wants to find again. Once again, the retrieval system's response is usually a ranked list of documents, and the system is evaluated by the rank at which the target document is retrieved.</p><p>In a document routing or filtering task, the topic of interest is known and stable, but the document collection is constantly changing <ref type="bibr" coords="2,154.57,697.60,10.60,8.97" target="#b0">[1]</ref>. For example, an analyst who wishes to monitor a news feed for items on a particular subject</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TREC 2003 is the latest in a series of workshops designed to foster research on technologies for information retrieval. The workshop series has four goals:</p><p>to encourage retrieval research based on large test collections; to increase communication among industry, academia, and government by creating an open forum for the exchange of research ideas; to speed the transfer of technology from research labs into commercial products by demonstrating substantial improvements in retrieval methodologies on real-world problems; and to increase the availability of appropriate evaluation techniques for use by industry and academia, including development of new evaluation techniques more applicable to current systems. TREC 2003 contained six areas of focus called "tracks". Three of the tracks, novelty, question answering, and web, were continuations of tracks that had run in earlier TRECs. The remaining three tracks, genomics, High-Accuracy-Retrieval-from-Documents (HARD), and robust retrieval, were new tracks in 2003. The retrieval tasks performed in each of the tracks are summarized in Section 3 below.</p><p>Table <ref type="table" coords="1,112.53,464.80,5.03,8.97">1</ref> lists the 93 groups that participated in TREC 2003. The participating groups come from 22 different countries and include academic, commercial, and government institutions.</p><p>This paper serves as an introduction to the research described in detail in the remainder of the volume. The next section provides a summary of the retrieval background knowledge that is assumed in the other papers. Section 3 presents a short description of each track-a more complete description of a track can be found in that track's overview paper in the proceedings. The final section looks forward to future TREC conferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Information Retrieval</head><p>Information retrieval is concerned with locating information that will satisfy a user's information need. Traditionally, the emphasis has been on text retrieval: providing access to natural language texts where the set of documents to be searched is large and topically diverse. There is increasing interest, however, in finding appropriate information regardless of the medium that happens to contain that information. Thus "document" can be interpreted as any unit of information such as a web page or a MEDLINE record.</p><p>The prototypical retrieval task is a researcher doing a literature search in a library. In this environment the retrieval system knows the set of documents to be searched (the library's holdings), but cannot anticipate the particular topic that will be investigated. We call this an ad hoc retrieval task, reflecting the arbitrary subject of the search and its short duration. Other examples of ad hoc searches are web surfers using Internet search engines, lawyers performing patent searches or looking for precedences in case law, and analysts searching archived news reports for particular events. A retrieval system's response to an ad hoc search is generally a list of documents ranked by decreasing similarity to the query. requires a solution to a filtering task. The filtering task generally requires a retrieval system to make a binary decision whether to retrieve each document in the document stream as the system sees it. The retrieval system's response in the filtering task is therefore an unordered set of documents (accumulated over time) rather than a ranked list. TREC 2003 did not contain an explicit filtering task, though aspects of the filtering task were present in the novelty track tasks.</p><p>Information retrieval has traditionally focused on returning entire documents that contain answers to questions rather than returning the answers themselves. This emphasis is both a reflection of retrieval systems' heritage as library reference systems and an acknowledgement of the difficulty of question answering. However, for certain types of questions, users would much prefer the system to answer the question than be forced to wade through a list of documents looking for the specific answer. To encourage research on systems that return answers instead of document lists, TREC has had a question answering track since 1999. The information extraction task in the genomics track is similar to a question answering task in that the goal was to extract a short segment of a document as a description of a gene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test collections</head><p>Text retrieval has a long history of using retrieval experiments on test collections to advance the state of the art <ref type="bibr" coords="3,518.50,256.60,10.78,8.97" target="#b3">[4,</ref><ref type="bibr" coords="3,532.04,256.60,7.42,8.97" target="#b5">6,</ref><ref type="bibr" coords="3,72.00,268.48,11.86,8.97" target="#b9">10]</ref>, and TREC continues this tradition. A test collection is an abstraction of an operational retrieval environment that provides a means for researchers to explore the relative benefits of different retrieval strategies in a laboratory setting. Test collections consist of three parts: a set of documents, a set of information needs (called topics in TREC), and relevance judgments, an indication of which documents should be retrieved in response to which topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Documents</head><p>The document set of a test collection should be a sample of the kinds of texts that will be encountered in the operational setting of interest. It is important that the document set reflect the diversity of subject matter, word choice, literary styles, document formats, etc. of the operational setting for the retrieval results to be representative of the performance in the real task. Frequently, this means the document set must be large. The primary TREC test collections contain about 2 gigabytes of text (between 500,000 and 1,000,000 documents). The document sets used in various tracks have been smaller and larger depending on the needs of the track and the availability of data.</p><p>The primary TREC document sets consist mostly of newspaper or newswire articles, though there are also some government documents (the Federal Register, patent applications) and computer science abstracts (Computer Selects by Ziff-Davis publishing) included. High-level structures within each document are tagged using SGML, and each document is assigned an unique identifier called the DOCNO. In keeping of the spirit of realism, the text was kept as close to the original as possible. No attempt was made to correct spelling errors, sentence fragments, strange formatting around tables, or similar faults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Topics</head><p>TREC distinguishes between a statement of information need (the topic) and the data structure that is actually given to a retrieval system (the query). The TREC test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. The format of a topic statement has evolved since the earliest TRECs, but it has been stable since <ref type="bibr" coords="3,399.48,562.24,61.61,8.97">TREC-5 (1996)</ref>. A topic statement generally consists of four sections: an identifier, a title, a description, and a narrative. An example topic taken from this year's robust retrieval track is shown in figure <ref type="figure" coords="3,273.14,586.12,3.77,8.97" target="#fig_0">1</ref>.</p><p>The different parts of the TREC topics allow researchers to investigate the effect of different query lengths on retrieval performance. For topics 301 and later, the "title" field was specially designed to allow experiments with very short queries; these title fields consist of up to three words that best describe the topic. The description field is a one sentence description of the topic area. The narrative gives a concise description of what makes a document relevant.</p><p>Participants are free to use any method they wish to create queries from the topic statements. TREC distinguishes among two major categories of query construction techniques, automatic methods and manual methods. An automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. The definition of manual query construction methods is very broad, ranging from simple  tweaks to an automatically derived query, through manual construction of an initial query, to multiple query reformulations based on the document sets retrieved. Since these methods require radically different amounts of (human) effort, care must be taken when comparing manual results to ensure that the runs are truly comparable.</p><p>TREC topic statements are created by the same person who performs the relevance assessments for that topic (the assessor). Usually, each assessor comes to NIST with ideas for topics based on his or her own interests, and searches the document collection using NIST's PRISE system to estimate the likely number of relevant documents per candidate topic. The NIST TREC team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Relevance judgments</head><p>The relevance judgments are what turns a set of documents and topics into a test collection. Given a set of relevance judgments, the retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. TREC almost always uses binary relevance judgments-either a document is relevant to the topic or it is not. To define relevance for the assessors, the assessors are told to assume that they are writing a report on the subject of the topic statement. If they would use any information contained in the document in the report, then the (entire) document should be marked relevant, otherwise it should be marked irrelevant. The assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.</p><p>Relevance is inherently subjective. Relevance judgments are known to differ across judges and for the same judge at different times <ref type="bibr" coords="4,140.81,472.48,10.60,8.97" target="#b6">[7]</ref>. Furthermore, a set of static, binary relevance judgments makes no provision for the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. Despite the idiosyncratic nature of relevance, test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments <ref type="bibr" coords="4,341.08,508.36,15.33,8.97" target="#b10">[11]</ref>.</p><p>The relevance judgments in early retrieval test collections were complete. That is, a relevance decision was made for every document in the collection for every topic. The size of the TREC document sets makes complete judgments utterly infeasible-with 800,000 documents, it would take over 6500 hours to judge the entire document set for one topic, assuming each document could be judged in just 30 seconds. Instead, TREC uses a technique called pooling <ref type="bibr" coords="4,527.77,556.24,11.63,8.97" target="#b8">[9]</ref> to create a subset of the documents (the "pool") to judge for a topic. Each document in the pool for a topic is judged for relevance by the topic author. Documents that are not in the pool are assumed to be irrelevant to that topic.</p><p>The judgment pools are created as follows. When participants submit their retrieval runs to NIST, they rank their runs in the order they prefer them to be judged. NIST chooses a number of runs to be merged into the pools, and selects that many runs from each participant respecting the preferred ordering. For each selected run, the top documents (usually, ¬¢¬° ¬§¬£ ¬¶¬• ¬ß¬• ) per topic are added to the topics' pools. Since the retrieval results are ranked by decreasing similarity to the query, the top documents are the documents most likely to be relevant to the topic. Many documents are retrieved in the top for more than one run, so the pools are generally much smaller than the theoretical maximum of ¬©¬®t he-number-of-selected-runs documents (usually about 1/3 the maximum size). The use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. Critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents.</p><p>Zobel demonstrated that the quality of the pools (the number and diversity of runs contributing to the pools and the depth to which those runs are judged) does affect the quality of the final collection <ref type="bibr" coords="5,425.60,87.16,15.24,8.97" target="#b13">[14]</ref>. He also found that the TREC collections were not biased against unjudged runs. In this test, he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. For the TREC-5 ad hoc collection, he found that using the unique relevant documents increased a run's 11 point average precision score by an average of 0.5 %. The maximum increase for any run was 3.5 %. The average increase for the TREC-3 ad hoc collection was somewhat higher at 2.2 %.</p><p>A similar investigation of the TREC-8 ad hoc collection showed that every automatic run that had a mean average precision score of at least 0.1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents <ref type="bibr" coords="5,262.92,194.68,15.33,8.97" target="#b12">[13]</ref>. That investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs, an effect noted by the organizers of the NTCIR (NACSIS Test Collection for evaluation of Information Retrieval systems) workshop who performed their own manual runs to supplement their pools <ref type="bibr" coords="5,194.97,230.56,10.60,8.97" target="#b4">[5]</ref>.</p><p>While the lack of any appreciable difference in the scores of submitted runs is not a guarantee that all relevant documents have been found, it is very strong evidence that the test collection is reliable for comparative evaluations of retrieval runs. The differences in scores resulting from incomplete pools observed here are smaller than the differences that result from using different relevance assessors <ref type="bibr" coords="5,275.53,278.44,15.24,8.97" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>Retrieval runs on a test collection can be evaluated in a number of ways. In TREC, all ad hoc tasks are evaluated using the trec eval package written by Chris Buckley of Sabir Research <ref type="bibr" coords="5,359.48,340.24,10.60,8.97" target="#b2">[3]</ref>. This package reports about 85 different numbers for a run, including recall and precision at various cut-off levels plus single-valued summary measures that are derived from recall and precision. Precision is the proportion of retrieved documents that are relevant, while recall is the proportion of relevant documents that are retrieved. A cut-off level is a rank that defines the retrieved set; for example, a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. The trec eval program reports the scores as averages over the set of topics where each topic is equally weighted. (The alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. Evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important.)</p><p>Precision reaches its maximal value of 1.0 when only relevant documents are retrieved, and recall reaches its maximal value (also 1.0) when all the relevant documents are retrieved. Note, however, that these theoretical maximum values are not obtainable as an average over a set of topics at a single cut-off level because different topics have different numbers of relevant documents. For example, a topic that has fewer than ten relevant documents will have a precision score less than one after ten documents are retrieved regardless of how the documents are ranked. Similarly, a topic with more than ten relevant documents must have a recall score less than one after ten documents are retrieved. At a single cut-off level, recall and precision reflect the same information, namely the number of relevant documents retrieved. At varying cut-off levels, recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.</p><p>Of all the numbers reported by trec eval, the recall-precision curve and mean (non-interpolated) average precision are the most commonly used measures to describe TREC retrieval results. A recall-precision curve plots precision as a function of recall. Since the actual recall values obtained for a topic depend on the number of relevant documents, the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. The particular interpolation method used is given in Appendix A, which also defines many of the other evaluation measures reported by trec eval. Recall-precision graphs show the behavior of a retrieval run over the entire recall spectrum.</p><p>Mean average precision is the single-valued summary measure used when an entire graph is too cumbersome. The average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved (using zero as the precision for relevant documents that are not retrieved). The mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. The average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents, and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. Geometrically, mean average precision is the area underneath a non-interpolated recallprecision curve.  As TREC has expanded into tasks other than the traditional ad hoc retrieval task, new evaluation measures have had to be devised. Indeed, developing an appropriate evaluation methodology for a new task is one of the primary goals of the TREC tracks. The details of the evaluation methodology used in a track are described in the track overview paper.</p><formula xml:id="formula_0" coords="6,93.12,236.76,425.27,106.71">- - - - 5 4 - - - - - VLC - - - - - - 7 6 - - - - Query - - - - - - 2 5 6 - - - QA - - - - - - - 20 28 36 34 33 Web - - - - - - - 17 23 30 23 27 Video - - - - - - - - - 12 19 a Novelty - - - - - - - - - - 13 14 Genome - - - - - - - - - - - 29 HARD - - - - - - - - - - - 14 Robust - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TREC 200Tracks</head><p>TREC's track structure was begun in <ref type="bibr" coords="6,219.32,496.12,61.13,8.97">TREC-3 (1994)</ref>. The tracks serve several purposes. First, tracks act as incubators for new research areas: the first running of a track often defines what the problem really is, and a track creates the necessary infrastructure (test collections, evaluation methodology, etc.) to support research on its task. The tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. Finally, the tracks make TREC attractive to a broader community by providing tasks that match the research interests of more groups.</p><p>Table <ref type="table" coords="6,111.69,567.88,5.03,8.97" target="#tab_0">2</ref> lists the different tracks that were in each TREC, the number of groups that submitted runs to that track, and the total number of groups that participated in each TREC. The tasks within the tracks offered for a given TREC have diverged as TREC has progressed. This has helped fuel the growth in the number of participants, but has also created a smaller common base of experience among participants since each participant tends to submit runs to fewer tracks.</p><p>This section describes the tasks performed in the TREC 2003 tracks. See the track reports later in these proceedings for a more complete description of each track. Some of the descriptions given here are taken directly from the track overview papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The genomics track</head><p>The genomics track was a new track for TREC 2003. It is the first TREC track devoted to retrieval within a specific domain, and one of the goals of the track is to see how exploiting domain-specific information improves retrieval effectiveness. The track contained two tasks, the primary task that was an ad hoc retrieval task and the secondary task that was an information extraction task.</p><p>The scenario that motivated the primary task was that of a biological researcher or graduate student-that is, someone who already has considerable domain knowledge-confronted with the need to learn about a new gene very quickly. Since NIST assessors do not have the expertise to make judgments for the track, this first track made use of existing data that could serve as surrogate relevance judgments. The document collection consisted of approximately 526,000 MEDLINE records that were indexed between April 1, 2002 and April 1, 2003, and were donated to the track by the U.S. National Library of Medicine. A topic consisted of a gene name and an organism, and was to be interpreted as a request for the basic biology of the gene and its protein products in the designated organism. This is the information given by the Gene Reference into Function (GeneRIF) data in the LocusLink database, a database of biological information created by the National Center for Biotechnology Information. The GeneRIF data were used as the relevance judgments for the track.</p><p>An analysis of the use of GeneRIF data showed that the vast majority of GeneRIF references pointed to relevant documents, but the GeneRIF references were incomplete (i.e., there were many more relevant documents than those included as GeneRIF references). Incompleteness is not necessarily a problem for retrieval system evaluation in that unbiased incomplete judgments allow for fair comparisons. Unfortunately, the GeneRIF data are not unbiased relevance judgments: the Berkeley group was able to build a classifier that could distinguish documents likely to be GeneRIFs <ref type="bibr" coords="7,114.80,324.52,10.69,8.97" target="#b1">[2]</ref>. The track will need to obtain relevance judgments in some other manner in future years.</p><p>Twenty five groups submitted 49 primary task runs to the genomics track. The best performing runs did use domain-specific knowledge as part of the retrieval. Exploiting the Medical Subject Headings (MeSH) and substance name fields of the MEDLINE records and filtering for species were particularly beneficial.</p><p>Part of the GeneRIF data is a text snippet that summarizes the main point of the referred to document with respect to the gene and organism. The secondary task was an information extraction task with the goal of creating this GeneRIF annotation automatically. The test set for the secondary task consisted of 139 GeneRIFs. Effectiveness was measured as a function of the overlap between the words nominated by the system and the actual GeneRIF text.</p><p>Fourteen groups submitted 24 secondary task runs. Since the actual GeneRIF text for many of the annotations is taken directly from the title of the target document, a baseline run consisting of the title of each target document was very hard to beat. The few runs that were able to beat the baseline used classifiers to rank sentences likely to contain GeneRIF text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The HARD track</head><p>The HARD track was another new track in TREC 2003. HARD stands for High Accuracy Retrieval from Documents, and the goal of the track was to improve retrieval performance by targeting retrieval results to the specific user. Of course, to target retrieval results in such a manner the system needs to have some knowledge about the user. The HARD track provided this information in the form of biographical data about the user, information regarding the search context, and a statement of the expected type of a result.</p><p>The underlying task in the HARD track was an ad hoc retrieval task. However, for some topics the expected type of a result was passages rather than documents. Combining document and passage retrieval into a common evaluation methodology was one of the aspects explored in the track. Another aspect was the use of "clarifying forms" to gather information about the searcher. A clarification form was a single web page that solicited information about the query from the user. Any information from the user could be collected by the form subject to the constraints that the user would spend no more than 3 minutes filling out any one form and that the form had to be entirely self-contained HTML.</p><p>The document set used in the track was the set of documents from 1999 from the AQUAINT corpus plus a set of Congressional Record and Federal Register articles also from 1999. This collection consisted of approximately 372,000 documents and 1.7 GB of text. The topics were created by assessors from the Linguistics Data Consortium (LDC). The topics were patterned after standard TREC ad hoc topics, but included a set of metadata elements that described the searcher and/or the context of the search. For example, the PURPOSE metadata field explained why the user was searching for the information (its value could be one of background, details, answer, or any) and the FAMILIARITY field represented how familiar the searcher is with the general subject area of the topic (value between 1 and 5 with 1 meaning no prior knowledge and 5 meaning detailed knowledge of the subject; value could also be unknown). Biographical data such as the age, sex, and occupation of the searcher were also recorded.</p><p>Participants first ran their systems using just the standard TREC portions of the topic and no other information. They then repeated the search using any information from the metadata and/or their clarification forms. The goal was to see if the additional information helped systems to create a more effective retrieved set than the initial baseline result.</p><p>Relevance judgments were made at the LDC by the same assessor who created the topic. Two types of judgments were made, document-level judgments and passage-level judgments. Document-level judgments made without reference to the metadata are the same as standard TREC relevance judgments. Documents that are relevant in the standard TREC sense but do not meet the requirements specified by the metadata are called "SOFT-REL" documents, while relevant documents that also satisfy the metadata are called "HARD-REL". For document-level evaluation, HARD track runs were evaluated using the standard trec eval measures, treating either both SOFT-REL and HARD-REL documents as relevant, or just HARD-REL documents as relevant.</p><p>Passage-level judgments were also made by the LDC assessor. If the metadata for a topic specified that the user wanted something smaller than a full document as a response, the assessor looked at each HARD-REL document in turn and marked the passages within the document that satisfied the topic. Passages were specified by an offset from the beginning of the document and a length. A single document could contain multiple relevant passages, but relevant passages never overlapped (overlapping passage were combined into a single passage if necessary). The relevance judgments were assumed to contain all the relevant passages for the topic.</p><p>The main measure used for passage-based evaluation was R-precision where R is the number of relevant passages for a topic. The passage-based evaluation treated all system responses as passages (i.e., a retrieved document was considered a single long passage). Precision was calculated on the basis of characters: the passage-based precision for a system response at rank was the proportion of characters in the sum of the passages at ranks 1-that were contained in a relevant passage.</p><p>Fourteen groups submitted 88 runs to the HARD track. For most groups, runs based on data obtained from clarification forms improved results as compared to the corresponding baseline run. Evaluation based on passages differs from that based on documents in that systems ranked differently when evaluated by passage-based R-precision than when evaluated by document-based R-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The novelty track</head><p>The goal of the novelty track is to investigate systems' abilities to locate relevant and new (nonredundant) information within an ordered set of documents. This task models an application where the user is skimming a set of documents and the system highlights the new, on-topic information. The track was first introduced in TREC 2002, though this year's track had a number of significant changes from the initial track.</p><p>The basic task in the novelty track is as follows: given a topic and an ordered set of relevant documents segmented into sentences, return sentences that are both relevant to the topic and novel given what has already been seen. To accomplish this task, participants must first identify relevant sentences (a passage retrieval task) and then identify which sentences contain new information (a filtering task). To allow participants to focus on the filtering and passage retrieval aspects separately, four different tasks were included in the track where each task differed by the amount and kind of training data that was provided to the systems.</p><p>Fifty new topics were created for the novelty track by NIST assessors. Half of the topics focused on events and the other half focused on opinions about controversial subjects. For each topic, the assessor created a statement of information need and queried the document collection using the NIST PRISE search engine. The assessor selected 25 relevant documents and labeled the relevant and new sentences in each. The document collection used was the AQUAINT Corpus of English News Text assembled for the TREC 2002 question answering track. This corpus is comprised of documents from three different sources: the AP newswire from 1998-2000, the New York Times newswire from 1998-2000, and the (English portion of the) Xinhua News Agency from 1996-2000. There are approximately 1,033,000 documents and 3 gigabytes of text in the collection. The choice of the collection was motivated by a desire to increase the amount of redundancy in the relevant set as compared to last year's track. The 25 relevant documents for each topic were ordered chronologically for system processing, which is easily accomplished for a newswire collection.</p><p>The four tasks in the track allowed the participants to test their approaches to novelty detection using no, partial, or complete relevance information. Task 1. Given the set of 25 relevant documents for a topic, identify all relevant and novel sentences. Task 2. Given the relevant sentences in all 25 documents, identify all novel sentences. Task 3. Given the relevant and novel sentences in the first 5 documents for the topic, find the relevant and novel sentences in the remaining 20 documents.</p><p>Task 4. Given the relevant sentences in all 25 documents, and the novel sentences in the first 5 documents, find the novel sentences in the remaining 20 documents.</p><p>Given the set of relevant and new sentences selected by the assessor who created the topic, the score for a novelty topic was computed as the F measure where sentence set recall and sentence set precision are equally weighted. Let be the number of matched sentences, i.e., the number of sentences selected by both the assessor and the system, ¬° be the number of sentences selected by the assessor, and ¬¢ be the number of sentences selected by the system. Then sentence set recall is ¬§¬£ ¬° and precision is ¬§¬£ ¬¢ . The ¬• score is then computed as ¬• ¬° ¬ß¬¶ ¬© . Fourteen groups submitted 179 runs to the novelty track. All but one group submitted a run for Task 1, and most groups tried all tasks. The results showed that for the basic task in which systems were given no sentence-level training data, the best systems were more effective than human performance. That is, a second assessor who selected relevant and novel sentences based on the topic statement generally scored lower when evaluated by the author's sentences than did the systems. More data is required to determine if systems are indeed performing at the level of a human at this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The question answering (QA) track</head><p>The question answering track addresses the problem of information overload by encouraging research into systems that return actual answers, as opposed to ranked lists of documents, in response to a question. The track has run since TREC-8 (1999), but has expanded in both scope and difficulty since the initial tracks. The TREC 2003 track contained two tasks, the main task and the passages task. Both tasks used the AQUAINT Corpus of English News Text used in the novelty track as the source of answers.</p><p>In TREC 2002, the QA task was defined such that systems were required to return exact answers, text strings consisting of a complete answer and nothing else. However, pinpointing the precise extent of an answer is a more difficult problem than finding a text segment that contains an answer, and there are applications of QA technology that do not require this extra step. The passages task provided a forum for research groups interested in these applications. A passages task run consisted of exactly one response for each of a set of 413 factoid questions. A response was either a document extract (not longer than 250 characters) believed to contain an answer to the question or the string "NIL" used to indicate the system's belief that there was no correct answer in the collection. Responses were judged as either correct, unsupported, or incorrect by human assessors. The final score for a passages task run was accuracy, the percentage of responses judged correct.</p><p>Twenty-one passages task runs from eleven different groups were submitted to the QA track. As determined by comparing mean accuracy scores, the passages task was not a noticeably easier task than the exact answer task. Accuracy scores for the passages task were in general no better than accuracy scores for the factoid component in the main task that required exact answers. Two of the three groups that submitted runs for both tasks had higher accuracy scores for the exact-answer case.</p><p>The main task was a combination task consisting of three different types of questions: factoids, lists, and definitions. The goal in combining the different question types into a single task was to increase the number of systems that attempted to answer the different question types. Each question was tagged as to its type in the test set. The three question types were evaluated separately, and the final score for a main task run was a combination of the scores for the three question types.</p><p>The factoid component of the main task was identical to the passages task except responses were required to be exact answers rather than document extracts that contained an answer. A fourth value for the judgments, inexact, was added to indicate when an otherwise correct response contained too much information. As in the passages task, the score for the factoid component of the main task was accuracy.</p><p>The list component of the main task required systems to assemble an answer from information located in multiple documents. In TREC, a list question asks for different instances of a particular kind of information to be retrieved, such as List the names of chewing gums. List questions can be thought of as a shorthand for asking the same factoid question multiple times; the set of answers that satisfy the factoid question is the appropriate response for the list question. Unlike the previous two times the list task was run in TREC, this year's list questions did not specify a target number of instances to return. Instead, systems were expected to return all of the correct, distinct answers contained in the document collection. There were 37 list questions in the main task test set.</p><p>Within the response returned for a single question by one system, assessors judged individual items as the factoid responses were judged. In addition, the assessor marked exactly one of a set of equivalent correct items as distinct. The final answer list for a question was created by the assessor based on the answers the assessor found during question development and the set of distinct, correct answers found by the systems. This final answer list was used to compute the instance recall and instance precision of a system's response. Instance recall is the fraction of answers on the final answer list that the system returned. The corresponding instance precision measure is the fraction of instances returned by the system that are on the final answer list. Instance recall and precision were combined using the F measure with recall and precision equally weighted (¬• ¬° ¬¶ ¬®¬° ¬®¬° ¬° ¬° ) as the final score for a list question. The score for the entire list component of the main task was the average of the F scores over the 37 questions.</p><p>Definition questions are questions such as Who is Colin Powell? or What is mold?. This was the first time definition questions were evaluated in TREC. The evaluation was based on a small pilot evaluation of definition questions that was held as part of the ARDA AQUAINT program in the fall of 2002 <ref type="bibr" coords="10,351.42,315.28,15.24,8.97" target="#b11">[12]</ref>. Evaluating systems that answer definition questions is much more difficult than evaluating systems that answer factoid questions because it is no longer useful to judge a system response as simply right or wrong. Assigning partial credit to a system response requires some mechanism for matching the concepts in the desired response to the concepts present in the system's response. The issues are similar to those that arise in the evaluation of machine translation and automatic summarization.</p><p>The following scenario was assumed for definition questions:</p><p>The questioner is an adult, a native speaker of English, and an "average" reader of US newspapers. In reading an article, the user has come across a term that they would like to find out more about. They may have some basic idea of what the term means either from the context of the article (for example, a bandicoot must be a type of animal) or basic background knowledge (Ulysses S. Grant was a US president). They are not experts in the domain of the target, and therefore are not seeking esoteric details (e.g., not a zoologist looking to distinguish the different species in genus Perameles).</p><p>The definition question test set contained 50 questions drawn from search engine logs; the set contained 30 questions for which the target was a (perhaps fictional) person, 10 questions for which the target was an organization, and 10 questions for which the target was some other thing.</p><p>A system response for a definition question was an unordered set of [document-id, answer-string] pairs. Each string was presumed to be a facet in the definition of the target. There were no limits placed on either the length of an individual answer string or on the number of pairs in the list, though systems were penalized for retrieving extraneous information.</p><p>Judging the quality of the systems' responses was done in two steps. In the first step, all of the answer-strings from all of the responses were presented to the assessor in a single (long) list. Using these responses and his own research done during question development, the assessor first created a list of "information nuggets" about the target. An information nugget was defined as a fact for which the assessor could make a binary decision as to whether a response contained the nugget. At the end of this step, the assessor decided which nuggets were vital-nuggets that must appear in a definition for that definition to be good. The assessor went on to the second step once the nugget list was created. In this step the assessor went through each of the system responses in turn and marked where each nugget appeared in the response. If a system returned a particular nugget more than once, it was marked only once. A single item in a system's response may match zero, one, or more than one nuggets.</p><p>Given the judgments as described above, it is straightforward to compute the nugget recall of a response: it is simply the ratio between the number of correctly retrieved nuggets to the number of nuggets on the assessor's list. But the corresponding measure of nugget precision, the ratio between the number of nuggets correctly retrieved to the total number of nuggets retrieved, is problematic since the correct value for the denominator is unknown. A trial evaluation prior to the pilot showed that assessors found enumerating all concepts represented in a response to be so difficult as to be unworkable. Instead, we used length as a crude approximation to precision. The length-based measure captures the intuition that users would prefer the shorter of two definitions that contain the same concepts. The final score for a definition question was the F measure where nugget recall was given five times as much emphasis as nugget precision. The score for the definition component of the main task was the average F over the 50 definition questions.</p><p>The final score for a main task run was computed as a weighted average of the three component scores:</p><formula xml:id="formula_1" coords="11,163.56,134.05,263.69,30.80">FinalScore ¬° ¬£ ¬£ ¬° ¬£¬¢ FactoidScore ¬§ ¬£ ¬£ ¬¶¬• ¬ß¬¢ ListScore ¬§ ¬£ ¬£ ¬¶¬• ¬ß¬¢</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DefScoreS</head><p>ince each of the component scores ranges between 0 and 1, the final score is also in that range. The final score emphasizes the factoid component, which represented the largest number of questions and is the task people are most familiar with. The weight for the other components was made large enough to encourage participation in those subtasks.</p><p>Fifty-four main task runs from 25 different groups were submitted to the track. The results demonstrate that the list and definition tasks are challenging for systems, and that they present challenges for evaluation as well. For the definition task, the difference in evaluation scores required to have confidence in the conclusion that one run is better than another is large relative to the observed scores. This results in a fairly insensitive test since many comparisons are inconclusive. The list task scores are much more stable, but the stability is due in large part to the fact that the scores for the list task are very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The robust retrieval track</head><p>The robust retrieval track was another new track in TREC 2003. The goal of the track was to focus research on improving the consistency of retrieval technology by concentrating on poorly performing topics. In addition, the track brought back a classic ad hoc retrieval task to TREC.</p><p>The topic set used in the track was a set of 100 topics. Fifty of the topics were new, created by NIST assessors using the standard topic development procedure. The other 50 topics were old topics first used in the ad hoc tasks of TRECs 6-8. NIST selected these 50 topics based on the median mean average precision (MAP) score when the topic was first used: the 50 topics all had low median MAP scores with at least one run that did much better than the median to rule out flawed topics.</p><p>Since 50 of the topics were from previous TRECs, the track used the same document set as those years, namely the set of documents on TREC disks 4 and 5 minus the Congressional Record documents. No new relevance judgments were made for these topics. The 50 new topics were judged using pools created from all runs using a depth of 125 documents per topic per run. Evaluation was performed using trec eval on each subset of the topics and on the combined set of 100 topics. Two new measures that focused on the poorly-performing topics were also introduced. The first of these measures was the percentage of topics that returned no relevant documents in the top ten documents retrieved. The second measure is a much more sensitive, but far less intuitive measure. If there are a total of ¬© topics in the test set, plot the MAP score computed over a system's worst topics (as measured by average precision) against for ¬° ¬£ ¬© ¬£ ¬• . The measure is the area underneath this curve. Note that since the measure is computed over the individual system's worst topics, different systems' scores are computed over a different set of topics in general.</p><p>The robust track received a total of 78 runs from 16 participants. All of the runs submitted to the track were automatic runs. The results of the track provide strong confirmation that average values of the traditional effectiveness measures do not reflect poorly performing topics. The new measures do emphasize systems' worst topics, but because they are defined over a subset of the topics, they are much less stable than traditional measures for a given test set size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">The web track</head><p>The goal in the web track is to investigate retrieval behavior when the collection to be searched is a large hyperlinked structure such as the World Wide Web. This year's track focused on finding homepages, the main entry pages to sites. There were two non-interactive tasks and one interactive task in the track.</p><p>All tasks used the .GOV collection created for the TREC 2002 web track and distributed by CSIRO (see http: //www.ted.cmis.csiro.au/TRECWeb/govinfo.html). This collection is based on a January, 2002 crawl of .gov web sites. The documents in the collection contain both page content and the information returned by the http daemon; text extracted from the non-html pages is also included in the collection.</p><p>The two non-interactive tasks in the track were a topic distillation task and a navigational task known as the home/named page finding task. In the topic distillation task, the systems were given a broad information request and were to return a list of relevant home pages. A relevant home page was defined as the entry page to a credible site that is principally devoted to the topic. The emphasis was on returning home pages rather than pages themselves since a result list of homepages provides a better overview of the coverage of a topic in the collection. The primary effectiveness measure used was R-precision (precision after R relevant documents are retrieved) since many of the topics within the set of 50 test topics had fewer than 10 relevant home pages.</p><p>The navigational task was a known-item search task. The queries consisted of a very short description of a page such as "Tennessee Valley Authority", and the systems were to return the target page (in this case, www.tva.gov). The test set consisted of 300 queries, half of which had a home page as the target page. Effectiveness was measured by the mean over the 300 topics of the reciprocal of the rank at which the target page was returned.</p><p>Twenty-seven groups submitted a total of 166 runs to the non-interactive part of the web track. Ninety-three of the runs were topic distillation runs and 73 of the runs were navigational task runs. Results from both tasks showed that exploiting anchor text is an important element of effective homepage finding. Methods that exploited URL syntax and link structure had more mixed results, especially for the navigational task. Attempts to differentiate processing for named pages vs. homepages in the navigational task did not improve effectiveness.</p><p>The interactive task within the web track explored the role of the human searcher in the topic distillation task. Eight of the topics used in the non-interactive version of the task were expanded to include a search scenario to provide context for the searcher. The searchers produced a list of home pages for the topic which were then judged by the assessors along four dimensions: relevance, depth, coverage, and repetition. Each dimension was judged using a 5-point Likert scale.</p><p>Two groups participated in the task. Both groups explored whether a more structured presentation of the search results (rather than a simple ranked list) would better support a searcher in the topic distillation task. The searchers liked the structured result format better, and were somewhat more efficient with it, but there were no significant differences between the list and structured formats in the quality of the homepage lists the searchers assembled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Future</head><p>Since three of the six tracks offered in TREC 2003 were new tracks, the set of tracks to be offered in TREC 2004 will be little changed from this year. Each of the six tracks will continue in TREC 2004. In addition, a new track, currently known as the terabyte track, will be added. The main objective in the terabyte track will be to investigate ad hoc evaluation methodologies for terabyte scale collections <ref type="bibr" coords="12,322.01,447.40,10.60,8.97" target="#b7">[8]</ref>. Of course, the track will also offer participants the opportunity to see how well their retrieval methods scale to significantly larger collections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,72.00,211.48,69.19,8.97;1,72.00,233.80,467.84,8.97;1,72.00,245.68,467.76,8.97;1,72.00,257.68,443.35,8.97"><head>1 Introduction</head><label>1</label><figDesc>The twelfth Text REtrieval Conference, TREC 2003, was held at the National Institute of Standards and Technology (NIST) November 18-21, 2003. The conference was co-sponsored by NIST, the US Department of Defense Advanced Research and Development Activity (ARDA), and the Defense Advanced Research Projects Agency (DARPA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,88.80,75.48,96.81,7.05;4,88.80,86.40,177.65,7.05;4,88.80,108.24,102.32,7.05;4,88.80,119.28,387.34,7.05;4,88.80,130.20,371.26,7.05;4,88.80,152.16,91.52,7.05;4,88.80,163.08,392.62,7.05;4,88.80,174.00,414.21,7.05;4,88.80,185.04,285.14,7.05"><head></head><label></label><figDesc>Retrieve information regarding the process by which the Czech and Slovak republics of Czechoslovakia established separate sovereign countries. &lt;narr&gt; Narrative: A relevant document will provide specific dates and details regarding the separation movement. Documents relating to normal activities of the separate nations, both internal and external are not relevant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,169.08,215.92,273.66,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: A sample TREC 2003 topic from the robust retrieval track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,93.12,82.00,425.42,162.83"><head>Table 2 :</head><label>2</label><figDesc>Number of participants per track and total number of distinct participants in each TREC</figDesc><table coords="6,332.04,93.84,23.03,8.07"><row><cell>TREC</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Special thanks to the track coordinators who make the variety of different tasks addressed in TREC possible.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,93.58,558.40,445.91,8.97;12,93.60,570.28,271.33,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,257.56,558.40,281.94,8.97;12,93.60,570.28,21.69,8.97">Information filtering and information retrieval: Two sides of the same coin?</title>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,119.37,570.28,112.26,8.97">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="1992-12">December 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,589.24,446.12,8.97;12,93.60,601.24,340.17,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,334.24,589.24,205.46,8.97;12,93.60,601.24,18.77,8.97">BioText team report for the TREC 2003 genomics track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bhalotia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">I</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,130.27,601.24,274.37,8.97">Proceedings of the Twelfth Text REtrievalxi Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrievalxi Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,620.20,446.62,8.97;12,93.60,632.73,32.50,7.83" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,161.67,620.20,127.90,8.97">trec eval IR evaluation package</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<ptr target="ftp://ftp.cs.cornell.edu/pub/smart" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.59,651.16,445.88,8.97;12,93.60,663.16,141.27,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,278.65,651.16,232.82,8.97">Factors determining the performance of indexing systems</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Keen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,522.08,651.16,17.38,8.97;12,93.60,663.16,31.66,8.97">Two volumes</title>
		<imprint>
			<date type="published" when="1968">1968</date>
			<pubPlace>Cranfield, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,93.58,682.12,445.97,8.97;12,93.60,694.00,445.93,8.97;12,93.60,706.00,267.00,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,93.60,694.00,198.29,8.97">Overview of IR tasks at the first NTCIR workshop</title>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kazuko</forename><surname>Kuriyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Toshihiko</forename><surname>Nozue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koji</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroyuki</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Souichiro</forename><surname>Hidaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,310.33,694.00,229.19,8.97;12,93.60,706.00,183.10,8.97">Proceedings of the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition</title>
		<meeting>the First NTCIR Workshop on Research in Japanese Text Retrieval and Term Recognition</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="11" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.59,75.16,446.16,8.97;13,93.60,87.16,165.26,8.97" xml:id="b5">
	<monogr>
		<title level="m" coord="13,163.82,75.16,312.97,8.97">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.59,107.08,446.19,8.97;13,93.60,118.96,60.13,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,165.74,107.08,143.31,8.97">Relevance and information behavior</title>
		<author>
			<persName coords=""><forename type="first">Linda</forename><surname>Schamber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,317.34,107.08,218.07,8.97">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.59,138.88,446.09,8.97;13,93.60,150.88,333.53,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,293.14,138.88,246.54,8.97;13,93.60,150.88,191.19,8.97">Summary of the SIGIR 2003 workshop on defining evaluation methodologies for terabyte-scale test collections</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,291.96,150.88,51.20,8.97">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="55" to="58" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.59,170.80,446.09,8.97;13,93.60,182.80,446.00,8.97;13,93.60,194.68,71.77,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,254.78,170.80,284.90,8.97;13,93.60,182.80,55.28,8.97">Report on the need for and provision of an &quot;ideal&quot; information retrieval test collection</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,158.93,182.80,205.21,8.97">British Library Research and Development Report</title>
		<imprint>
			<biblScope unit="volume">5266</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>Computer Laboratory, University of Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.58,214.60,341.21,8.97" xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,178.67,214.60,191.06,8.97">Information Retrieval Experiment. Butterworths</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.58,234.52,446.10,8.97;13,93.60,246.52,213.53,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,174.60,234.52,321.78,8.97">Variations in relevance judgments and the measurement of retrieval effectiveness</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,504.08,234.52,35.60,8.97;13,93.60,246.52,131.19,8.97">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="697" to="716" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.58,266.44,446.22,8.97;13,93.60,278.44,445.90,8.97;13,93.60,290.32,211.03,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,176.64,266.44,169.17,8.97">Evaluating answers to definition questions</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,366.26,266.44,173.53,8.97;13,93.60,278.44,445.90,8.97;13,93.60,290.32,54.52,8.97">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2003)</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2003)</meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="109" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.57,310.24,446.02,8.97;13,93.60,322.24,446.09,8.97;13,93.60,334.24,446.59,8.97;13,93.60,346.65,26.50,7.83" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,256.33,310.24,243.13,8.97">Overview of the eighth Text REtrieval Conference (TREC-8)</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs.html" />
	</analytic>
	<monogr>
		<title level="m" coord="13,238.35,322.24,247.44,8.97">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="500" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,93.57,366.04,446.03,8.97;13,93.60,378.04,446.11,8.97;13,93.60,390.04,446.33,8.97;13,93.60,401.92,239.95,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,150.03,366.04,308.78,8.97">How reliable are the results of large-scale information retrieval experiments?</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,414.10,378.04,125.61,8.97;13,93.60,390.04,376.30,8.97">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Wilkinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Melbourne, Australia; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
