<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,208.32,19.49,179.77,18.66">AnswerFinder in TREC 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,267.12,54.30,62.27,15.55"><forename type="first">Diego</forename><surname>Mollá</surname></persName>
							<email>diego@ics.mq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology Macquarie University Sydney</orgName>
								<address>
									<postCode>2109</postCode>
									<region>NSW</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,208.32,19.49,179.77,18.66">AnswerFinder in TREC 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B7EA93ED59C388EF2AC0CFF182E0602</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this our first participation in TREC we have focused on the passage task of the question answering track. The main aim of our participation was to test the impact of various types of linguistic information in a simple question answering system. In particular, we have tested various combinations of word overlap, grammatical relations overlap, and overlap of minimal logical forms in the final scoring module of the system. The results indicate a small increase of accuracy with respect to a baseline system based on word overlap. Overall, given the short time available for developing the system, the results are satisfactory and equal or surpass the median.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is the first time that the Centre for Language Technology at Macquarie University participates in TREC. Due to strong time restrictions we decided to implement a simple and functional system for the passage task of the Question Answering track. The final estimated time of development was about 55 person-hours (plus execution time) distributed among three months of intermittent work.</p><p>Section 2 describes the general architecture of the system. Our main focus was the exploration of sentence similarity measures for question answering. The measures used were based on word over-lap, grammatical relations, and minimal logical forms. The two latter measures are described in Sections 3 and 4, respectively. Section 5 presents the final results and discussion. Finally, Section 6 describes a post-submission extension that incorporates question classification and named-entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>The system is fairly straightforward (Figure <ref type="figure" coords="1,509.93,330.14,3.91,13.15" target="#fig_0">1</ref>). A document preselection stage returns the documents preselected by NIST. A subsequent sentence preselection stage splits the documents into sentences and ranks the sentences according to word overlap. A final scoring stage analyses the top-ranking sentences returned by the previous stage and re-ranks the sentences according to a similarity measure. The top-ranking sentence is returned as the answer, possibly truncated if it is longer than the limit of 250 characters. Note that all questions are treated the same way. In other words, there is no question classification stage. Also, no attempt to detect NIL answers was made.</p><p>It is worth noting that the complete process was done when the question was processed. All the data structures (except, of course, the documents provided by NIST) were built on the fly.</p><p>To determine the number of documents to preselect, during the development of the system we analysed the questions used in TREC 2002 and the answers available from the TREC web pages. In particular, we used the regular expressions provided by Ken Litkowsky to determine if an arbitrary document contained the answer of a specific question. The results of this analysis are  not necessarily accurate for two reasons. First of all, good answers phrased in unfamiliar terms may not be covered by the regular expressions. Second, some text may happen to match a regular expression by coincidence but still the document may fail to support the answer. Still, the results are indicative for our purposes. The results of our analysis is summarised in Figure <ref type="figure" coords="2,281.11,519.02,4.07,13.15" target="#fig_1">2</ref>, which shows the number of questions that have an answer within the top X documents. We can see that there is little gain by preselecting many documents. For practical reasons we set the threshold of documents to preselect to X = 50. For that threshold, 74% of the questions would find a document that satisfies the regular expression of the answer.</p><p>To split the documents into sentences we used a simple regular expression that detects punctuation characters as end-of-sentence markers: The resulting sentences are ranked according to word overlap. After several experiments we found that the optimal measure of word overlap is obtained by using a list of stop words<ref type="foot" coords="2,469.80,264.58,3.95,9.60" target="#foot_0">1</ref> and ignoring repeated words in the answer candidate. We arbitrarily set a threshold of 100 sentences to be sent to the final scoring stage. Figure <ref type="figure" coords="2,485.05,306.50,5.39,13.15" target="#fig_2">3</ref> shows the relation between the sentences preselected and the number of correct answers. For the threshold of 100 sentences and using the 2002 question set, 59.4% of the questions would have a string that satisfies the regular expression of the answer. This is therefore the expected upper limit of accuracy that the scoring module can achieve.</p><p>The final scoring module combines several types of information. Apart from word overlap we experimented with other types of overlap that use various types of linguistic information. In particular, we used grammatical relations and minimal logical forms, as described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Grammatical Relations</head><p>The grammatical relations by <ref type="bibr" coords="2,306.72,560.18,87.04,13.15" target="#b1">Carroll et al. (1998)</ref> were devised to enable comparative evaluations of parsers. Following their evaluation methodology, the output of the parsers to evaluate is converted into sets of grammatical relations, thus enabling the representation of the parser output in a uniform way. In order Figure <ref type="figure" coords="3,108.65,137.18,4.19,13.15">4</ref>: Hierarchy of grammatical relations <ref type="bibr" coords="3,74.28,150.74,117.60,13.15" target="#b0">(Briscoe and Carroll, 2000)</ref>.</p><p>to be able to accommodate parsers with different granularity in the output, the grammatical relations are classified hierarchically (Figure <ref type="figure" coords="3,277.46,215.90,3.91,13.15">4</ref>). Different types of parser output are represented with different types of grammatical relations and therefore a wide range of parsers can be easily compared.</p><p>Table <ref type="table" coords="3,112.51,283.70,5.39,13.15" target="#tab_0">1</ref> lists the grammatical relations used in the examples of this paper and the final implementation. For further detail about grammatical relations see <ref type="bibr" coords="3,131.15,324.38,117.48,13.15" target="#b0">(Briscoe and Carroll, 2000)</ref>.</p><p>For example, the grammatical relations for the sentence The man that came ate bananas and apples with a fork without asking are: DETMOD( ,man,the), CMOD(that,man,come), SUBJ(come,man, ), SUBJ(eat,man, ), DOBJ(eat,banana, ), DOBJ(eat,apple, ) CONJ(and,banana,apple), NCMOD(fork,eat,with), DETMOD( ,fork,a), XCOMP(without,eat,ask)</p><p>Briscoe and Carroll's grammatical relations are different from the dependency arcs used in dependency grammar formalisms <ref type="bibr" coords="3,214.29,561.98,70.06,13.15" target="#b5">(Mel'čuk, 1988)</ref>. Consider The man that came ate bananas and apples with a fork. Figure <ref type="figure" coords="3,187.34,589.10,5.39,13.15">5</ref> (a) shows the graphical representation of the structure returned by Conexor FDG, a dependency-based parsing system <ref type="bibr" coords="3,94.17,629.78,143.22,13.15" target="#b8">(Tapanainen and Järvinen, 1997)</ref>. For comparison, Figure <ref type="figure" coords="3,142.53,643.34,5.39,13.15">5</ref> (b) shows a simplified graphical representation of the grammatical relations. In dependency grammar a unique head is assigned to each word, thus the head of man is ate. However man is the dependent of more than one grammatical relation, namely SUBJ(eat,man, ) and SUBJ(come,man, ). Furthermore, in dependency grammar a word can have at most one dependent of each argument type, and so ate can have at most one object. But the same is not true for grammatical relations, and we get both OBJ(eat,banana, ) and OBJ(eat,apple, ). Thus, grammatical relations can theoretically provide a sentence representation that is closer to the semantic contents of a sentence than the representation provided by dependency arcs. In practice, the representational power of the grammatical relations depends on the output of the parser used.</p><p>Grammatical relations can be used to introduce parser-independent syntactic information in a question-answering system. Since the grammatical relations are expressed as lists of relations, a score measure can be implemented by simply computing the overlap of grammatical relations between the question and the answer candidate. In theory, to compute the overlap we must use the hierarchical organisation of the grammatical relations to decide if two grammatical relations are related. For example, SUBJ(eat,man, ) can be subsumed by SUB OR DOBJ(eat,man). However, since the same parser was used for both the question and the answer, the granularity of grammatical relations between questions and answer candidates will be practically the same. Thus, each grammatical relation can be seen as an unstructured token and the scoring module can simply count the number of common tokens, very much like counting the overlap of words. This was the approach used in our QA prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Minimal Logical Forms</head><p>Flat logical forms have been used in several NLP systems, including question-answering systems <ref type="bibr" coords="3,306.72,574.22,105.73,13.15" target="#b2">(Harabagiu et al., 2001;</ref><ref type="bibr" coords="3,416.77,574.22,46.22,13.15" target="#b4">Lin, 2001;</ref><ref type="bibr" coords="3,467.31,574.22,54.35,13.15;3,306.72,587.66,80.01,13.15">Mollá et al., 2000, for example)</ref>. The flat logical forms that we use in our QA system are borrowed from <ref type="bibr" coords="3,492.14,601.22,29.44,13.15;3,306.72,614.78,53.18,13.15" target="#b6">(Mollá et al., 2000)</ref>, who uses reification to flatten out nested expressions. For example, the logical form of The cp command will quickly copy files is: The logical form above says that there are two entities x2 and x3 that represent two objects for the compound noun cp command. There is an entity x7 (a file); there is an entity e6, which represents a copying event where the first argument is x3 (the object introduced by the head of the compound noun) and the second argument is x7; there is an entity p5 which states that e6 is done quickly, and the event e6 (the copying) holds.</p><p>The above expression does not aim to express the complete logical form of the sentence. For example, there is no information about quantification, tense, aspect, and plurals. In essence, only the main relations among open words and determiners is expressed. This is why our logical forms are called minimal logical forms: only information that is minimal for the task of questionanswering is encoded.</p><p>An advantage of the use of flat logical forms over nested logical forms is that, again, sentence similarity can be measured as a type of overlap. The only additional complexity is that the question now contains variables. For example, the minimal logical form of Which command copies files? is (the symbols in uppercase indicate variables):</p><formula xml:id="formula_0" coords="4,328.56,583.13,170.60,38.81">object('command',O1,[X1]), evt('copy',E2,[X1,X2]), object('file',O2,[X2])</formula><p>If this logical form is to match that of the sentence The cp command will quickly copy files above, the scoring module needs to instantiate the variable O1 in the question with the constant o3 in the answer candidate, X1 with x3, and so on.</p><p>In our implementation we have used Prolog unification. Basically, the logical form of the answer candidates is stored as Prolog data, and a simple Prolog program computes the overlap of the logical forms of the answer candidates with the logical form of the question. Also, the scoring module ignores the holds term in the question logical form. Otherwise, since almost all questions and candidate answers contain a holds term in the logical form, two completely unrelated sentences would have an overlap of 1 and this is counterintuitive.</p><p>Since there are several plausible combinations of variable instantiations, the scoring module finds the set of instantiations that provides the highest overlap of logical forms.</p><p>Table <ref type="table" coords="5,113.95,364.34,5.39,13.15" target="#tab_2">2</ref> shows the minimal logical forms of questions that differ solely in the argument positions, the minimal logical form of an answer candidate, and the resulting overlaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>To test the impact of grammatical relations and minimal logical forms, we experimented with different scoring modules corresponding to word overlap, grammatical relations, and minimal logical forms, using the TREC 2002 questions. The results (Table <ref type="table" coords="5,141.22,521.78,4.49,13.15">3</ref>) show that, remarkably, word overlap is best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formula</head><p>Accuracy Word overlap 14.8% Grammatical relations 09.0% Minimal logical forms 10.8%</p><p>Table <ref type="table" coords="5,111.31,624.86,4.19,13.15">3</ref>: Experiments with TREC 2002 data.</p><p>Due to the limited time available we decided to postpone any cause analysis. Instead we ran several experiments combining the linguistic information available. Table <ref type="table" coords="5,425.62,146.18,5.39,13.15" target="#tab_3">4</ref> shows the final runs submitted to TREC, the results of our experiments with data from TREC 2003, and the final results returned by NIST.</p><p>The data in the 2002 column show that the runs submitted produce slightly better results than simple word overlap. Interestingly, The evaluation provided by <ref type="bibr" coords="5,365.17,240.98,94.62,13.15">NIST (2003 column)</ref> gives noticeably better results than our home evaluation with the TREC 2002 data (2002 column). This may be due to the fact that the regular expressions provided by Litkowsky do not attempt to cover all possible formulations of correct answers, or it may be indeed the case that the questions asked in TREC 2003 are easier to process. As we will see in next section, the former is more likely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Adding Named Entities</head><p>During the development of the system we tried to integrate the named entity recogniser bundled with GATE. <ref type="foot" coords="5,359.64,419.74,3.95,9.60" target="#foot_2">3</ref>First of all, we implemented a question analyser module that classifies the question into the type of expected answer. The classifier uses 29 regular expressions to allocate one of the following types to the question: person, date, location, money, number, city, date, organization, location, percent,country, state, river, name, and unknown. The regular expressions were based on the questions used in TREC 2002. The final module has an accuracy of 78.6% (393 from a total of 500 questions were correctly classified).</p><p>Since GATE's named entity recogniser can detect a reduced number of named entity types (person, location, date, money, and organization only), a simple mapping was necessary between the question classification and the final list of answer types (Table <ref type="table" coords="5,385.99,651.38,3.91,13.15" target="#tab_4">5</ref>). The information regarding answer type is used during the sentence preselection stage. Thus, the score given to a sentence is the sum of word overlap with the question (as described above) plus a reward of 10 points if the sentence contains an entity of the expected answer type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>We developed a Java interface to GATE's named entity recogniser. However, the steep learning curve required to learn Java, together with unexpected execution errors prevented us from integrating the NE module in the final version. The final system therefore did not use the named entity recogniser and as a consequence the question classifier became useless and therefore it was disabled. Subsequent work on the Java interface enabled us to compute the named entities of the top 50 documents preselected for the TREC 2002 and TREC 2003 questions. With these named entities computed off-line we ran AnswerFinder and obtained the results shown in Table <ref type="table" coords="6,100.87,541.70,4.07,13.15" target="#tab_5">6</ref>.</p><p>The results show a noticeable improvement of accuracy in the runs with the TREC 2002 question set. In contrast, accuracy decreases in the runs with the TREC 2003 question set. A plausible explanation to the results with the TREC 2003 question set is that we used the new regular expressions provided by Litkowsky for the evaluation. The regular expressions are based on the set of answers returned by the systems com- peting in TREC 2003. Possibly, some of the answers returned by the version with named entities are paraphrases of the correct answer that do not match the regular expressions and therefore they are erroneously classified as wrong. In fact, note that the results without named entities reported in Table <ref type="table" coords="6,333.79,516.98,5.39,13.15" target="#tab_5">6</ref> are slightly worse than the ones returned by NIST (Table <ref type="table" coords="6,378.11,530.54,4.49,13.15" target="#tab_3">4</ref>) due to inaccuracies in the regular expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Further Research</head><p>The short time available only allowed us to build a baseline system for the passages task of the Question Answering track. Still, we were pleased to find that the results were better or equal than the median of all 21 submissions to the task. Overall, our experiments suggest that simple word overlap gives better performance than simple overlaps based on grammatical relations or minimal logical forms alone. These findings confirm the work by <ref type="bibr" coords="7,87.58,42.26,57.96,13.15" target="#b7">(Mollá, 2003)</ref>, who used a similar question answering system for the Reading Comprehension corpus <ref type="bibr" coords="7,106.02,69.38,103.95,13.15" target="#b3">(Hirschman et al., 1999)</ref>. Further work will include:</p><p>Error analysis. This will be the first step to do. We will determine if different types of questions are more or less likely to produce good results with the different overlap measures and identify methods of combining word overlap, grammatical relations, and minimal logical forms for each type of question.</p><p>NE integration. We will finalise the integration of the named entity recogniser and identify entity types that are useful for the task of question answering. Besides using the named entities to determine the answer candidates, we will also explore ways to include NE information in the parsing modules and semantic interpreter. This way we hope to obtain more accurate grammatical relations and logical forms.</p><p>Logical forms. We will explore ways to leverage the use of logical forms by using more sophisticated measures. For example, we will look into adding weights to the logical forms. We will also explore the possibility of using weighted abduction methods.</p><p>Extract the exact answer. Logical forms may be useful to determine the exact answer of the question. For example, the original Ex-trAns system <ref type="bibr" coords="7,157.56,496.34,86.80,13.15" target="#b6">(Mollá et al., 2000)</ref> generates a predicate of the form object(_,_,_) that represents the object asked about by the question word. ExtrAns also keeps track of what words produces what predicates in the minimal logical form. All this information can be used to determine the exact part of the sentence that matches the concept being asked about.</p><p>Complex questions. We will also work on methodologies to answer questions that require the fusion of output from several documents, such as list and definition questions.</p><p>By introducing the above and other extensions, in future participations in the question answering track we hope to increase the accuracy of the system and to participate in the main task of the track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,93.60,210.14,176.14,13.15;2,74.38,15.82,214.86,180.78"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of AnswerFinder.</figDesc><graphic coords="2,74.38,15.82,214.86,180.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,74.28,369.86,214.73,13.15;2,74.28,383.42,165.50,13.15;2,74.34,245.54,215.09,110.83"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relation between preselected documents and number of correct answers.</figDesc><graphic coords="2,74.34,245.54,215.09,110.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,306.72,138.02,214.57,13.15;2,306.72,151.58,214.85,13.15;2,306.72,165.14,53.16,13.15;2,306.78,14.56,215.09,109.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relation between preselected sentences and number of correct answers (using the top 50 documents).</figDesc><graphic coords="2,306.78,14.56,215.09,109.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,502.68,640.66,3.95,9.60"><head>Table 1 :</head><label>1</label><figDesc>Grammatical relations used in this paper.</figDesc><table coords="3,502.68,640.66,3.95,9.60"><row><cell>2</cell></row></table><note coords="4,426.97,324.58,19.20,7.00;4,115.56,360.38,364.13,13.15"><p>&gt;det Figure 5: (a) Dependency structure of a sample sentence; (b) grammatical relations.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,101.64,13.79,391.67,71.55"><head>object('mary',O,[X]), evt('see',E,[Y,X]), object('john',O2,[Y])</head><label></label><figDesc></figDesc><table coords="5,101.64,13.79,391.67,71.55"><row><cell>Answer candidate</cell><cell>Minimal Logical Form</cell></row><row><cell>John saw Mary</cell><cell>object('john',o1,[x1]), object('mary',o3,[x3]), evt('see',e2,[x1,x3])</cell></row><row><cell>Question</cell><cell>Minimal Logical Form</cell></row><row><cell cols="2">Did John see Mary? Did Mary see John? object('john',O,[X]), evt('see',E,[Y,X]), object('mary',O2,[Y])</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,129.60,94.34,336.38,13.15"><head>Table 2 :</head><label>2</label><figDesc>Question answering using flat logical forms. Overlap shown in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,74.28,14.18,446.96,201.79"><head>Table 4 :</head><label>4</label><figDesc>Runs submitted to TREC 2003. The 2002 column indicates the results of an automatic selfevaluation with data from TREC 2002. The 2003 column indicates the evaluation results returned by NIST. The formula components are: wo -word overlap; gro -overlap of grammatical relations; mooverlap of minimal logical forms.</figDesc><table coords="6,159.96,14.18,275.75,201.79"><row><cell></cell><cell>Formula</cell><cell>2002</cell><cell>2003</cell></row><row><cell cols="2">answfind1 3wo + gro</cell><cell cols="2">16.8% 19.1%</cell></row><row><cell cols="4">answfind2 9wo + 3gro + mo 16.8% 18.6%</cell></row><row><cell cols="4">answfind3 9wo + 3mo + gro 15.6% 18.2%</cell></row><row><cell>Question Type</cell><cell>Answer Type</cell><cell></cell></row><row><cell cols="2">country, city, state, river location</cell><cell></cell></row><row><cell>percent</cell><cell>number</cell><cell></cell></row><row><cell>name</cell><cell cols="3">person OR organization OR location</cell></row><row><cell cols="3">Any other question type yields same answer type</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,170.88,225.74,254.01,13.15"><head>Table 5 :</head><label>5</label><figDesc>Mapping between question type and answer type.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,306.72,262.82,214.71,147.67"><head>Table 6 :</head><label>6</label><figDesc>Results of integrating the named entity recogniser.</figDesc><table coords="6,332.16,262.82,164.41,111.19"><row><cell>2002</cell><cell cols="2">Without NEs With NEs</cell></row><row><cell cols="2">answfind1 16.8%</cell><cell>19.1%</cell></row><row><cell cols="2">answfind2 16.8%</cell><cell>19.3%</cell></row><row><cell cols="2">answfind3 15.6%</cell><cell>18.4%</cell></row><row><cell>2003</cell><cell cols="2">Without NEs With NEs</cell></row><row><cell cols="2">answfind1 18.2%</cell><cell>16.2%</cell></row><row><cell cols="2">answfind2 17.4%</cell><cell>15.7%</cell></row><row><cell cols="2">answfind3 17.2%</cell><cell>15.5%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,322.80,652.22,199.22,10.80;2,306.72,663.19,199.72,9.61;2,306.72,673.15,70.17,9.61"><p>We used the list of stop words available from http://www-fog.bio.unipd.it/waishelp/ stoplist.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,322.80,662.18,199.21,10.80;3,306.72,672.14,211.89,10.80"><p>For illustration purposes, the logical forms used in this paper are slight variants of the ones shown in the literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,322.80,673.15,97.16,9.61"><p>http://gate.ac.uk/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,306.71,124.37,134.74,12.00;7,457.41,124.37,22.64,12.00;7,495.89,124.37,26.13,12.00;7,317.63,135.41,204.60,12.00;7,317.63,147.46,209.79,10.68;7,317.63,158.38,194.43,10.68" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,495.89,124.37,26.13,12.00;7,317.63,135.41,199.93,12.00">Grammatical relation annotation. On-line document</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<ptr target="http://www.cogs.susx.ac.uk/lab/nlp/carroll/grdescription/index.html" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,177.17,215.39,12.00;7,317.63,188.21,204.47,12.00;7,317.63,199.13,98.64,12.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,346.27,188.21,175.84,12.00;7,317.63,199.13,19.67,12.01">Parser evaluation: a survey and a new proposal</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Sanfilippo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,355.50,199.13,55.50,11.90">Proc. LREC98</title>
		<meeting>LREC98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,219.05,215.27,12.00;7,317.63,229.97,204.23,12.00;7,317.63,241.01,204.43,12.00;7,317.63,251.93,204.35,12.01;7,317.63,262.85,204.39,12.00;7,317.63,273.89,204.34,12.00;7,317.63,284.81,204.38,12.00;7,317.63,295.73,58.74,12.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,317.63,251.93,204.35,12.01;7,317.63,262.85,136.97,12.00">Answering complex, list and context questions with LCC&apos;s question-answering server</title>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pas ¸ca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Gîrju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finley</forename><surname>Lȃcȃtus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rȃzvan</forename><surname>Bunescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,500.53,273.89,21.45,11.90;7,317.63,284.81,35.24,11.90">Proc. TREC-10</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC-10</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>number 500-250 in NIST Special Publication. NIST</note>
</biblStruct>

<biblStruct coords="7,306.71,315.65,215.30,12.00;7,317.63,326.69,204.45,12.00;7,317.63,337.61,204.35,12.00;7,317.63,348.53,41.46,12.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,409.90,326.69,112.19,12.00;7,317.63,337.61,72.14,12.00">Deep Read: A reading comprehension system</title>
		<author>
			<persName coords=""><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,409.19,337.61,52.86,11.90">Proc. ACL&apos;99</title>
		<meeting>ACL&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,368.45,215.37,12.00;7,317.63,379.49,204.35,12.00;7,317.63,390.41,35.60,12.00" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,396.72,368.45,125.36,12.00;7,317.63,379.49,204.35,12.00;7,317.63,390.41,9.71,12.01">Indexing and retrieving natural language using ternary expressions. Master&apos;s thesis</title>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,410.33,215.34,12.00;7,317.63,421.25,183.28,12.00" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,391.56,410.33,130.50,11.90;7,317.63,421.25,32.17,11.90">Dependency Syntax: Theory and Practice</title>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Mel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">'</forename><surname>Čuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>State University of New York Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,441.29,215.31,12.00;7,317.63,452.21,204.46,12.00;7,317.63,463.13,204.48,12.00;7,317.63,474.05,62.30,12.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,415.55,452.21,106.55,12.00;7,317.63,463.13,44.70,12.00">Extrans, an answer extraction system</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rolf</forename><surname>Schwitter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rachel</forename><surname>Fournier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,373.01,463.13,144.55,11.90">Traitment Automatique des Langues</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="495" to="522" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,494.09,215.49,12.00;7,317.63,505.01,204.26,12.00;7,317.63,515.93,112.52,12.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,391.69,494.09,130.52,12.00;7,317.63,505.01,131.78,12.00">Towards semantic-based overlap measures for question answering</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,471.89,505.01,50.00,11.90;7,317.63,515.93,18.11,11.90">Proc. ALTW 2003</title>
		<meeting>ALTW 2003<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,306.71,535.85,215.38,12.00;7,317.63,546.89,204.46,12.00;7,317.63,557.81,22.06,12.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,492.73,535.85,29.36,12.00;7,317.63,546.89,116.54,12.00">A nonprojective dependency parser</title>
		<author>
			<persName coords=""><forename type="first">Pasi</forename><surname>Tapanainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Järvinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,456.77,546.89,65.33,11.90;7,317.63,557.81,16.54,12.01">Proc. ANLP-97. ACL</title>
		<meeting>ANLP-97. ACL</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
