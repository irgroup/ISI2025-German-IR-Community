<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.28,112.08,361.15,15.49">Overview of the TREC 2003 Robust Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,261.48,144.47,88.94,10.76"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Standards and Technology Gaithersburg</orgName>
								<address>
									<postCode>20899</postCode>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.28,112.08,361.15,15.49">Overview of the TREC 2003 Robust Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">25DE8BB83B96518357A8591F0D6CC1BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The robust retrieval track is a new track in TREC 2003. The goal of the track is to improve the consistency of retrieval technology by focusing on poorly performing topics. In addition, the track brings back a classic, ad hoc retrieval task to TREC that provides a natural home for new participants.</p><p>An important component of effectiveness for commercial retrieval systems is the ability of the system to return reasonable results for every topic. Users remember abject failures. A relatively few such failures cause the user to mistrust the system and discontinue use. Yet the standard retrieval evaluation paradigm based on averages over sets of topics does not significantly penalize systems for failed topics. The robust retrieval track looks to improve the consistency of retrieval technology by focusing on poorly performing topics.</p><p>The task within the track was a traditional ad hoc task. An ad hoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. For each topic, participants create a query and submit a ranking of the top 1000 documents for that topic. In addition to the standard evaluation by trec eval, each run was also evaluated using two new effectiveness measures that focus on the effectiveness of the least-well-performing topics.</p><p>This paper presents an overview of the results of the track. The first section provides more details regarding the task and defines the new evaluation measures. The following section presents the systems' retrieval results, while Section 3 examines the new evaluation measures. Systems compare differently when evaluated on the new measures then when evaluated on standard measures such as MAP, suggesting that the new measures capture a different aspect of retrieval behavior. However, the measures are less stable than the traditional measures, and the marigin of error associated with the new measures is large relative to the differences in scores observed in the track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">The Robust Retrieval Task</head><p>As noted above, the task within the robust retrieval track was a traditional ad hoc task. The topic set consisted of a total of 100 topics, 50 old topics taken from TREC topics 301-450 (TRECs 6-8) and 50 new topics. The document collection was the set of documents on TREC disks 4 and 5, minus the Congressional Record, since that is what was used for TRECs 6-8. This document set contains approximately 528,000 documents and 1,904 MB of text.</p><p>Since the focus of the track is on poorly performing topics, we wanted to ensure that there were topics that are generally difficult for systems to answer in the test set. We could not (purposely) construct a difficult topic set using only new topics since it is notoriously hard to predict whether or not a topic will be difficult a priori <ref type="bibr" coords="1,488.45,567.28,10.60,8.97" target="#b4">[5]</ref>. Instead, we used the effectiveness of the retrieval runs in TRECs 6-8 to construct a topic set of known-to-be-difficult topics. For each of topics 301-450, NIST created a box plot of the average precision scores for all runs (both automatic and manual) submitted to the ad hoc task in that topic's TREC. NIST then selected topics with low median average precision scores but with at least one (there was usually more than one) high outlier. The requirement for at least one system doing well on the topic was designed to eliminate flawed topics from the topic set. The set of old topics selected for the robust track is given in Figure <ref type="figure" coords="1,256.83,639.04,3.77,8.97" target="#fig_0">1</ref>.</p><p>While using old topics allowed NIST to construct a test set with certain properties, it also meant that full relevance data for these topics was available to the participants, and that systems were likely developed using those topics. NIST therefore created 50 new topics using the standard topic creation process as a type of control group. The 50 new topics are numbered 601-650. Since we could not control how the old topics had been used in the past, the assumption was that the old topics were fully exploited in any way desired in the construction of a participants <ref type="bibr" coords="1,454.00,698.80,3.84,8.97">'</ref>   other words, participants were allowed to explicitly train on the 50 old topics in the test set if they desired to. The only restriction placed on the use of relevance data for the 50 old topics was that the relevance judgments could not be used during the processing of the submitted runs. This precluded such things as true (rather than pseudo) relevance feedback and computing weights based on the known relevant set.</p><p>The existing relevance judgments were used for the old topics; no new judgments of any kind were made for these topics. The new topics were judged by creating pools from all runs submitted to the track and using the top 125 documents per run. There was an average of 959 documents judged for each new topic. The assessors made three-way judgments of not relevant, relevant, or highly relevant for the new topics. Seven of the 50 new topics had no highly relevant documents, and another 14 topics had fewer than 5 highly relevant documents. All the evaluation results reported for the track consider both relevant and highly relevant documents as the relevant set since there are no highly relevant judgments for the old set. The number of relevant documents per topic for the old topic set ranged from a low of 5 to a high of 361 and an average of 88. For the new topic set, the minimum number of relevant was 4, the maximum was 115, and the average was 33.</p><p>While no new judgments were made for the old topics, we did form pools for those topics (using the top 100 retrieved per run) to examine the coverage of the original judgment set. Across the set of 50 old topics, an average of 61.4 % (minimum 43.2 %, maximum 79.7 %) of the documents in the pools created using robust track runs were judged. A relatively low number of judged documents is to be expected since the old topics were chosen because they were difficult, and there is known to be less overlap among the retrieved sets for difficult topics than for easier topics. Across the 78 runs that were submitted to the track, there was an average of 0.4 unjudged documents in the top 10 documents retrieved and 11.6 unjudged documents in the top 100 retrieved. These averages are inflated by a set of five runs that had very poor effectiveness (a cursory examination confirmed that the poor effectiveness was caused by retrieving documents that were indeed not relevant). Without these five runs, there was an average of 0.2 unjudged documents in the top 10 documents retrieved and 8.7 unjudged documents in the top 100 retrieved. There is still a tendency toward poorer runs having larger numbers of unjudged documents in the retrieved set, but such a bias is expected and is caused by poorer runs retrieving different, really-not-relevant documents.</p><p>Runs were evaluated using trec eval, with average scores computed over the set of 50 old topics, the set of 50 new topics, and the combined set of 100 topics. Two additional measures were computed over the same three topic sets. The first measure was the percentage of topics that retrieved no relevant documents in the top ten retrieved. If one accepts "no relevant documents in the top ten retrieved" as an adequate definition of poorly performing topic, then this is a direct measure of the behavior of interest and is therefore a very intuitive and easily understood measure. It has the drawback of being a very coarse measure. That is, there are relatively few discrete values the measure can assume in theory, and the actual range of values seen in practice is much smaller than the theoretical range.</p><p>The second measure was suggested by Chris Buckley. One of the initial proposals for a measure for the track was to compute the mean of the average precision scores (MAP) for the system's worst topics (as measured by average precision) rather than the entire set of topics as trec eval does. In an attempt to pick a suitable -big enough to make the measure stable but small enough to emphasize the poorly performing topics-the mean average precision over the worst topics, MAP( ), was plotted as a function of for several runs. Chris suggested that instead of picking a single point on the curve to use as the measure, to use the area underneath the MAP( ) vs. curve as the measure. Just as MAP (the area underneath the recall-precision curve) emphasizes high precision but has a recall component, the area under the MAP( ) vs. curve measure emphasizes the worst-performing topics, but also gives a general measure of quality. The measure as implemented for the track computes the area under the MAP( ) vs. curve, but limits to the worst quarter topics. That is, is set from ¡ £¢ ¤¢ ¥¢ ¦¡ ¨ § for the 50-topic sets and ¡ £¢ ¤¢ ¥¢ © § for the combined set. This measure is not exactly intuitive (it doesn't even have a better name than "area underneath the MAP( ) vs. curve" yet), but it incorporates much more information than the percentage of topics with no relevant </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Results</head><p>The robust track received a total of 78 runs from the 16 groups listed in Table <ref type="table" coords="3,382.20,289.24,3.77,8.97" target="#tab_1">1</ref>. All of the runs submitted to the track were automatic runs. Participants were allowed to submit up to 5 runs. One of the runs was required to use only the description portion of the topic statements; the other runs could use any portion of the topic statements. There was a noticeable difference in effectiveness depending on the portion of the topic statement used: runs using all of the topic statement were better than those using selected fields, and runs using only the title field were worse than those using other portions. The retrieval results reported here are restricted to the runs that used just the description portion of the topic since that was the required run. There were 44 description-only runs submitted to the track. Table <ref type="table" coords="3,111.69,372.88,5.03,8.97">2</ref> gives the evaluation scores for one run for each of the groups that submitted a description-only run (one group did not submit such a run by mistake). The table gives the scores for the four main measures used in the track as computed over the old topics only, the new topics only, and the combined set of 100 topics. The four measures are mean average precision (MAP), the average of precision at 10 documents retrieved (P10), the percentage of topics with no relevant in the top 10 retrieved (%no), and the area underneath the MAP( ) vs.</p><p>curve (area). The run shown in the table is the run with the highest MAP score as computed over the combined topic set; the table is sorted by this same value.</p><p>As expected given the way the topic set was constructed, the results show that as a set the 50 old topics are clearly much more difficult than the 50 new topics. The scores for all measures and all runs are better, usually much better, for the new topics than for the old. While all systems score better on the new set than the old, the amount of improvement is not uniform, so the relative ordering of systems is different for the two topic sets. We can quantify how different the relative orderings are by computing the Kendall correlation between system rankings using each of the topics sets in turn. A system ranking is an ordering of the runs by decreasing score of an effectiveness measure. The Kendall correlation measures the similarity between two rankings as a function of the number of pairwise swaps needed to turn one ranking into the other. The ranges between -1.0 and 1.0 where the expected correlation between two randomly generated rankings is 0.0 <ref type="bibr" coords="3,174.46,552.28,10.60,8.97" target="#b1">[2]</ref>. Table <ref type="table" coords="3,216.41,552.28,5.03,8.97" target="#tab_3">3</ref> shows the system rankings for the 44 description-only runs for each of the four evaluation measures of Table <ref type="table" coords="3,188.74,564.16,5.03,8.97">2</ref> for both topics sets. The ranking for the old topic set is given on the top and the ranking for the new topic set on the bottom. Each run is represented by a single character in the rankings. When two runs have a tied score for one measure they are ranked according to their MAP scores for that topic set. The last column in Table <ref type="table" coords="3,96.57,600.04,5.03,8.97" target="#tab_3">3</ref> gives the Kendall correlation between the two rankings. The values confirm that the rankings are different. The precise cause for the differences cannot be determined from this data since there are (at least) two confounded factors: different systems doing different amounts of training on the old topics and different systems being relatively more effective for difficult topics.</p><p>Are current retrieval systems handling the difficult topics better now than when the topics first appeared? We can give an approximate answer to this question by comparing the median and maximum scores obtained for each topic when computed over the set of runs submitted to the TREC in which the topic first appeared and the set of runs submitted to the robust track. Figure <ref type="figure" coords="3,228.07,683.80,5.03,8.97" target="#fig_1">2</ref> shows this comparison using average precision as the evaluation measure. Since there were few description-only runs submitted to the previous ad hoc tasks, the sets of runs used to compute Table <ref type="table" coords="4,96.45,82.00,3.90,8.97">2</ref>: Evaluation results for the best description-only run per group as measured by MAP over the combined topic set. Runs are ordered by MAP over the combined topic set. Values given are the mean average precision (MAP), precision at rank 10 averaged over topics (P10), the percentage of topics with no relevant in the top ten retrieved (%no), and the area underneath the MAP( ) vs. curve (area) as computed for the set of 50 old topics, the set of 50 new topics, and the combined set of 100 topics.  % ¢ ¨( % the median and maximum average precision scores consisted of all automatic runs (i.e., runs using any combination of the fields in the topic statement). In the figure the median scores are plotted using filled symbols while the maximum scores are plotted using hollow symbols. The values computed using the set of runs submitted to the TREC in which the topic first appeared, called the Original TREC, are plotted as ovals; the values computed using robust track runs are plotted as triangles. The topics are sorted by decreasing median average precision score as computed using the robust track submissions. Median effectiveness for the robust track runs is generally better than for the original TREC runs, though for about 10 topics the original runs have a better median. The difference between medians is generally small (with a few notable exceptions). The maximum scores have larger differences and there are more topics for which the original runs had the better maximum score than for the robust runs. There were more different systems contributing to the Original TREC runs set than for the robust track runs set which may account for the better maximum scores. Nonetheless, it is clear that this old topic set remains a difficult set of topics. Many of the participants used the robust track as a place to try new techniques for general ad hoc retrieval, without particularly focusing on the question of poorly performing topics. Both of the two groups with top-scoring runs, Queens College, CUNY and the Multitext group at the University of Waterloo, expanded the query using terms extracted from the Web (and possibly other document sets). Other groups experimented with new retrieval mod-  Almost all groups tried some version of query expansion based on pseudo-feedback. The query expansion improved average effectiveness, but did not help (and frequently hurt) the worst performing topics except when the expansion was done using a different corpus. This is not particularly surprising since the poorly performing topics are unlikely to have relevant documents in the top retrieved documents, and thus the feedback is as likely to harm as to help the results. After the qrels were published, the group from Fondazione Ugo Bordoni ran a series of experiments to see if they could predict when expansion would be beneficial based on an estimate of the MAP score of the initial retrieval result. When expanding only if the prediction determined it would be beneficial, they were able to both increase the MAP score and decrease the number of topics with no relevant retrieved as compared to their baseline.</p><formula xml:id="formula_0" coords="4,83.88,156.55,439.92,168.92">¢¡ ¤£ ¦¥ §¥ ¨¡ © § § £ ¦ ¨¡ § ¨¡ © ¨¡ © § ¨¡ § § ¨¡ § ¢¡ §¥ ¢¡ uwmtCR0 ¢¡ ¤£ ¦ ¨¡ © ¥ £ ¨¡ £ §£ ¨¡ © ¨¡ © ¨¡ ¨¡ ¥ ¢¡ © £ §£ ¢¡ ¨£ aplrob03d ¢¡ ¤£ ! ¨¡ ¨¡ ¨¡ © § © ¨¡ £ ¨¡ ¥ ¨¡ ¥ © ¢¡ © §© § ¢¡ § humR03de ¢¡ ¤£ ¨¡ © ¨¡ ¨¡ © ¥ ¥ ¨¡ £ " ¨¡ © ¨¡ §© ¢¡ ¨£ ! £ ! ¢¡ § VTDokrcgp5 ¢¡ ¤£ !© ¨¡ © ¨£ " £ ¢¡ § ¨¡ © ¨¡ £ ! ¨¡ © ¡ ¨¡ § § £ ¦ ¢¡ § fub03InOLe3 ¢¡ ¤£ !© ¨¡ © §© ¦ ¢¡ § ¥ ¨¡ © §¥ ¨¡ § § £ ¨¡ ¨£ ! ¡ § ¨¡ ¨£ !© £ ¦¥ ¢¡ § UIUC03Rd3 ¢¡ ¤£ ¦ § ¨¡ § ¨¡ §© ¨¡ © ¥ ¨¡ £ ¨¡ ¨£ ¨¡ ¢¡ © § £ ¢¡ § Sel78QE ¢¡ ¤£ ¦ ¨¡ § © ¨¡ §© ¨¡ © § © ¨¡ # £ ¨¡ § §¥ ¨¡ ¦ © ¢¡ © §¥ § § ¢¡ § © THUIRr0305 ¢¡ ¤£ §£ !¥ ¨¡ © ¨£ ! £ ¢¡ § § ¨¡ © §¥ ¨¡ § ¨¡ § ¡ ¦ © ¨¡ ¨£ ! £ ¦ ¢¡ £ ¦ SABIR03BF ¢¡ ¤£ ! § ¨¡ § © ¨¡ §© ¨¡ © § ¨¡ £ ! ¨¡ §© ¨¡ ¢¡ © # © ¢¡ § UAmsT03RDesc ¢¡ ¤£ ! §¥ ¨¡ £ " ¢¡ § § ¨¡ © § ¨¡ # £ ¨¡ ¨£ " ¡ § ¨¡ © § © £ ¦ ¢¡ § oce03noXbmD ¢¡ § § ¨¡ § ¦ ¢¡ § §© ¨¡ © ¨¡ £ ¨¡ ¨£ © ¡ $£ § ¨¡ © © ¢¡ MU03rob01 ¢¡ § § ¨¡ £ ! ¨¡ ¨¡ §¥ ¨¡ § § £ ¨¡ #£ ¨¡ ¤£ §© ¢¡ © § £ ¢¡ § NLPR03vb50 ¢¡ § ¨¡ © §© ¨¡ ¨¡ § ¨¡ ¨¡ ¨£ © ¨¡ ¤£ !¥ §¥ ¢¡ © ¥ ¥ ¢¡ § §¥ rutcor0375 ¢¡ ¨¡ ¤£ ! ¨¡ § ¨¡ ¤£ ¦ ¨¡ © ¨¡ § £ ¨¡ §¥ ¢¡ ¤£ ! # ¢¡ §</formula><p>Other approaches to increasing the effectiveness of the poorly performing topics included per-topic merging of results from different component runs and reordering the similarity-ranked list to maximize the number of retrievedset document clusters with representatives in the top 10 ranks. Johns Hopkins/APL found modest success in decreasing the number of poorly performing topics by merging multiple runs, but also found that their results were far below the optimum theoretically obtainable from merging. Hummingbird tried to increase the diversity of the documents in the top 10 ranks by clustering the retrieved set and reranking the top 100 documents such that the top 10 documents were from different clusters. Unfortunately, the reranking did not lead to a significant increase in the number of topics with a relevant document in the top 10 retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Effectiveness Measures</head><p>One of the common themes of the participants' results was that query expansion improved MAP scores while not improving or even degrading the effectiveness of the worst topics. This demonstrates that MAP scores are essentially unaffected by the poorly performing topics. Mathematically, a poorly performing topic would have to improve dramatically to affect the MAP score since the magnitude of the MAP score is so much larger than an individual poorly </p><formula xml:id="formula_1" coords="6,137.52,179.05,337.28,58.19">¢ ¨( % % ¢ ¡ #&amp; ¡ % ¢ ¡ % ¢ '&amp; £¢ % ¢ ¢ ¡¢ ¨) % ¢ £ ¤ % ¢ ¡¥ § % ¢ ¡ ¦ % % ¢ £ ¨) P10 % ¢ ) §¢ ¡¢ % ¢ ) ¨) ¢) % ¢ ) ¢( ¤¢ % ¢ £¢ % ¢ ¢ ¡¥ &amp; % ¢ ) ¤¥ ¤¢ % no % ¢ ¢ ¡¥ ¤¢ % ¢ ¡ ¨ % ¢ ) &amp; b)</formula><p>Kendall scores computed between rankings for all pairs of measures Figure <ref type="figure" coords="6,179.13,258.16,3.90,8.97">3</ref>: Agreement among system rankings produced by different measures. performing topic's average precision score. This section examines the behavior of the two new measures that were introduced in the track. It shows that the new measures do emphasize poorly performing topics as designed, but because their scores are based on relatively few topics, they are more unstable than traditional measures and the margin of error associated with the new measures is large relative to the differences in scores observed in the track. More reliable measures are needed to support research on developing techniques for consistent retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Agreement among measures</head><p>One way to show that different measures emphasize different factors is to examine whether they rank systems differently. We can produce systems rankings as above (using description-only runs), except that now instead of comparing rankings produced using different topic sets, we compare rankings produced using different evaluation measures. Figure <ref type="figure" coords="6,87.46,435.76,5.03,8.97">3</ref> shows the agreement among system rankings for MAP, the average of precision at 10 documents retrieved, the percentage of topics with no relevant in the top 10 retrieved, and the area under the MAP( ) vs. curve as computed over the set of old topics, the set of new topics, and the combined set of 100 topics. The system rankings themselves as computed over the combined set of topics are given at the top of the figure. The bottom of the figure shows the Kendall score computed between the rankings for each pair of measures.</p><p>The correlations are quite low, providing support for the contention that the measures are affected by different aspects of retrieval behavior. The correlation between MAP and the percentage of topics with no relevant documents in the top 10 documents is only slightly better than chance. While in theory such a low correlation with MAP means only that the two measures are emphasizing different aspects of retrieval, MAP has been shown to be an effective, stable measure <ref type="bibr" coords="6,133.01,543.40,11.63,8.97" target="#b0">[1]</ref> so in practice a low correlation with MAP can be a sign of an unstable measure. The stability of the new measures is investigated below.</p><p>The area under the MAP( ) vs. curve measure depends on the greatest value that assumes. This value reflects the trade-off in emphasis given to the worst-performing topics and the overall effectiveness of the system. The graphs in Figure <ref type="figure" coords="6,142.00,591.16,5.03,8.97" target="#fig_2">4</ref> illustrate how the relative effectiveness among systems changes as changes. The graphs plot MAP( ) vs. using the combined topic set for a subset of the runs shown in Table <ref type="table" coords="6,422.16,603.16,3.77,8.97">2</ref>. The left side of the figure shows the plot for all 100 values of and the right side of the figure shows the same plot restricted to © ¡ £¢ ¤¢ ¥¢ © § so more detail can be seen. The value of the official area measure is the area underneath the curve plotted in the right side of the figure.</p><p>The graphs in Figure <ref type="figure" coords="6,175.70,650.92,5.03,8.97" target="#fig_2">4</ref> make it clear that the relative order of systems ranked by their area scores does change depending on the maximal value of . For example, the THUIRr0305 run has the best area score when ( , and is ranked third until approximately © ( . However, the area measure is not highly sensitive to the maximal value of , provided is greater than about 10. We created system rankings based on the value of the area measure using the combined topic set and all description-only runs as the maximal value of varied from 1 (i.e., the worst topic  </p><formula xml:id="formula_2" coords="7,216.84,372.65,190.32,53.60">¡ ¢ ) % ¢ ¡ #&amp; ¡ P10 § ¢ ( % ¢ § § #) % no ¥ ¢ ¡ % ¢ % ¥ % area ¢ ) % ¢ % ) %</formula><p>determines the score) to 25. The Kendall correlations for ¡ ¡ % are small-in the 0.4 range when ¡ -but this is to be expected since measures based on the effectiveness of very few topics are known to be unstable. For £¢ ¡ % , the values were greater then 0.85, and were generally greater than 0.95 when the values being compared were within 5 of one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stability of measures</head><p>The stability of the evaluation measures for topic sets containing 50 topics can be examined using a procedure similar to the one introduced by Buckley and Voorhees <ref type="bibr" coords="7,263.99,535.96,10.60,8.97" target="#b0">[1]</ref>. This procedure computes an error rate for an evaluation measure by counting how often the measure disagrees with respect to which of two systems being compared is preferred. Larger error rates imply a less stable measure.</p><p>We generated 1000 different test sets of size 50 topics each by randomly selecting 50 topics from the set of 100 topics used in the track. We evaluated all 78 runs submitted to the track on each of the 1000 test sets. For all pairs of runs ¤ and ¥ , we counted the number of test sets for which ¤ evaluated as better than ¥ (¤ ¢ ¥ ), ¥ evaluated as better than ¤ (¥ ¢ ¤ ), and ¤ and ¥ evaluated as equivalent (¤ © ¥ ). Two runs were considered equivalent if the difference in their scores was less than 5 % of the larger score. The error rate is defined as the sum over all run pairs of min¦ §¤ ¢ ¥ ©¥ ¢ ¤ , divided by the total number of comparisons. The proportion of ties, ¤ © ¥ divided by the total number of comparisons is also of interest since it indicates how much discrimination power a measure has. A measure with a low error rate but a high proportion of ties has little power.</p><p>Table <ref type="table" coords="7,112.05,667.48,5.03,8.97" target="#tab_6">4</ref> shows the error rate and proportion of ties computed for the four different measures. The numbers for MAP and P10 are close to the numbers reported by Buckley and Voorhees despite the different collection and slightly different methodology. As suspected, the error rates for the two new measures are substantially greater than for MAP and P10, though the proportion of ties for the new measures is substantially smaller than for the traditional measures.</p><p>The relative instability of the area and topics-with-no-relevant-retrieved measures is not difficult to understand. Numerically, a very low proportion of ties is likely to increase the error rate-the more decisions you make the more likely some of them are wrong, especially since fewer ties implies finer distinctions. In addition, the new measures are defined over a subset of the topics in the test set. For a test set of a given size, the score for the new measures will always be based on fewer topics than for the traditional measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sensitivity of measures</head><p>While the higher error rates for the new measures are understandable, they do mean that there is much more uncertainty associated with a comparison of two systems when using one of these measures. Voorhees and Buckley introduced a procedure to empirically determine the relationship between the number of topics in a test set, the observed difference in scores of a particular measure (called ), and the likelihood that a single comparison of two runs leads to the correct conclusion <ref type="bibr" coords="8,117.54,220.72,10.60,8.97" target="#b3">[4]</ref>. Once established, the relationship can be used to derive the minimum difference in scores required for a certain level of confidence in the conclusion.</p><p>With 100 topics in the robust track test set, we can directly compute the relationship for topic set sizes up to 50 topics. Robust track runs should require somewhat smaller 's for the same level of confidence since they contain 100 topics. Voorhees and Buckley's original procedure used extrapolation to derive minimum differences for topic set sizes larger than those that could be directly computed, but extrapolation is not appropriate for the new measures since their values depend directly on the number of topics in the test set.</p><p>For topic sets of size 50, a run needs at least 11 fewer topics with no relevant in the top 10 retrieved to have 95% confidence that it is better than a second run. Over the 1000 topic sets of size 50 generated to estimate the error rate and comparing all pairs of runs submitted to the track, only 11.0 % of the comparisons had a difference at least this large. This is a small percentage that confirms that the measure is only able to distinguish grossly different systems. The area measure could distinguish even fewer systems. For the area measure, the minimum computed for 95 % confidence was 0.025; only 4.6% of the comparisons across all run pairs and the 1000 test sets had a difference in area score greater than 0.025.</p><p>Note that the best area score (not difference) obtained by a run in the robust track over the old set of 50 topics was 0.0203, so all robust track runs would be considered to be in a single equivalence class if only the old set of topics were used. This set of topics is known to be difficult, and all systems did sufficiently poorly on it that the area measure is not sensitive enough to distinguish one run from another. The best score obtained by a robust track run over the 50 new topics was 0.1062 with 38.6 % of the comparisons between pairs of systems having a difference greater than 0.025, so the measure can distinguish among systems for this topic set. But the new topic set appears to be unusually good: over the 1000 randomly selected 50-topic test sets, the best area score obtained by any run was only 0.043, and as stated above only 4.6 % of the comparisons across all run pairs had a difference greater than 0.025. The topicswith-no-relevant-retrieved measure was much less affected by the particular topic set. For the old topic set, 13.9 % of run pairs had a difference of at least 11 topics; for the new topic set, 11.4 % of run pairs had a difference of at least 11 topics; and over the 1000 randomly selected sets, 11.0 % of run pairs had a difference of at least 11 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The TREC 2003 robust retrieval tracks was an initial effort to improve the consistency of retrieval performance by focusing on poorly performing topics. The results of the track provide strong confirmation that average values of traditional effectiveness measures do not reflect poorly performing topics. New measures introduced in the track do emphasize systems' worst topics as designed. The new measures are defined over a subset of the topics in the test set, however, causing them to be much less stable than traditional measures for a given test set size. In turn, the instability causes the margin of error associated with the measures to be large relative to the differences in scores commonly observed.</p><p>The robust track will continue in TREC 2004. The current plan for the track is to repeat this year's task using the same fifty old topics (they remain difficult topics) and another set of 50 new topics. A new aspect of the evaluation in the track will be to test whether a system can predict which topics it will perform most poorly on. A similar evaluation strategy in the TREC 2002 question answering track demonstrated that accurately predicting whether a correct answer was retrieved is a challenging problem <ref type="bibr" coords="8,227.93,689.08,10.60,8.97" target="#b2">[3]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,196.80,149.56,218.27,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The set of old topics used in the robust track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.00,339.16,467.58,8.97;5,72.00,351.04,467.53,8.97;5,72.00,363.04,25.99,8.97"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Median and maximum per-topic average precision scores for the old set of topics as computed using the runs submitted to the first TREC the topic was used in (Original TREC) and the TREC 2003 robust track runs (TREC 2003).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,72.00,303.16,390.80,8.97;7,477.30,295.60,8.39,10.00;7,488.20,299.95,19.90,13.40;7,512.70,295.65,5.00,10.20;7,522.44,303.16,17.26,8.97"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of MAP( ) vs. . The graph on the left side of the figure shows the entire range of © ¡ £¢ ¥¢ ¤¢ ¦¡ % ¨% ; the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.00,82.00,467.70,166.29"><head>Table 1 :</head><label>1</label><figDesc>Groups participating in the robust track.</figDesc><table coords="3,72.00,96.88,467.70,151.41"><row><cell cols="2">Chinese Academy of Sciences (CAS-NLPR) Tsinghua University (Ma)</cell></row><row><cell>Fondazione Ugo Bordoni</cell><cell>University of Amsterdam</cell></row><row><cell>Hummingbird</cell><cell>University of Glasgow</cell></row><row><cell>Johns Hopkins University/APL</cell><cell>University of Illinois at Chicago</cell></row><row><cell>OcE Technologies</cell><cell>University of Illinois at Urbana-Champaign</cell></row><row><cell>Queens College, CUNY</cell><cell>University of Melbourne</cell></row><row><cell>Rutgers University (Neu)</cell><cell>University of Waterloo (MultiText)</cell></row><row><cell>Sabir Research, Inc.</cell><cell>Virginia Tech</cell></row><row><cell cols="2">in the top 10 retrieved. Note that since the measure is computed over the individual system's worst topics, different</cell></row><row><cell cols="2">systems' scores are computed over a different set of topics in general.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,121.56,359.08,612.69,133.89"><head>Table 3 :</head><label>3</label><figDesc>System rankings and corresponding Kendall scores for the old and new topic sets.</figDesc><table coords="4,132.00,375.76,602.25,117.21"><row><cell>Measure MAP</cell><cell>Rankings Old Set New Set WXCVoDLAqBHIFErhJnimNjpGlkegfMdRUOTQKSPcbZaY</cell><cell>% ¢ '&amp; ¢&amp;  §</cell></row><row><cell>P10</cell><cell>qWoVXCrLnljIEBmiHNADFpGhMJfegdkUORTQKSPcZbaY WXoLqIFERQPHVrjGpTSJhiCNgnDBmAMOlkUdefKcZbaY</cell><cell>% ¢ ¨(  §</cell></row><row><cell>% no</cell><cell>oWXqVjrFnClBImLGENpJMHeRQPifAhOUgkDTSdKZcbaY DRQPTSWXoGkgjArpOKqMJLBHIEmUFhnldCViNefZbcaY</cell><cell>% ¢ )  § ¢&amp;</cell></row><row><cell></cell><cell>WVXpqojGMJgRQPIFfdOTSrlEBAeKLNDCnmHhkUiZcbaY</cell><cell></cell></row><row><cell>area</cell><cell>qojWpBIADXkEHMCgdRrGJFVhLOTfQmnileUSKPNcZabY</cell><cell></cell></row><row><cell></cell><cell>WVqXojBApfDeCdJMGrLlOmFNngIkURKEHTQihPSZcbaY</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,178.56,346.00,254.71,43.17"><head>Table 4 :</head><label>4</label><figDesc>Error rate and proportion of ties for different measures.</figDesc><table coords="7,216.84,367.84,178.08,21.33"><row><cell>Error Rate (%) Proportion of Ties</cell></row><row><cell>MAP</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,88.55,97.48,451.01,8.97;9,88.56,109.36,451.02,8.97;9,88.56,121.36,234.04,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,248.44,97.48,156.12,8.97">Evaluating evaluation measure stability</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,178.44,109.36,361.14,8.97;9,88.56,121.36,150.24,8.97">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Belkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Leong</surname></persName>
		</editor>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.55,141.28,451.00,8.97;9,88.56,153.16,211.73,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,139.73,141.28,51.33,8.97">Kendall&apos;s tau</title>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Stuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,393.93,141.28,141.52,8.97">Encyclopedia of Statistical Sciences</title>
		<editor>
			<persName><forename type="first">Samuel</forename><surname>Kotz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Norman</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="367" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.55,173.20,450.80,8.97;9,88.56,185.08,399.70,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,170.65,173.20,217.98,8.97">Overview of the TREC 2002 question answering track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,408.22,173.20,131.13,8.97;9,88.56,185.08,315.53,8.97">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002), number NIST Special Publication 500-251</title>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002), number NIST Special Publication 500-251</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.55,205.00,451.16,8.97;9,88.56,217.00,451.03,8.97;9,88.56,228.88,87.57,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,247.60,205.00,223.33,8.97">The effect of topic set size on retrieval experiment error</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,490.21,205.00,49.49,8.97;9,88.56,217.00,447.13,8.97">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.55,248.92,451.07,8.97;9,88.56,260.80,451.13,8.97;9,88.56,272.80,440.58,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,255.75,248.92,241.46,8.97">Overview of the sixth Text REtrieval Conference (TREC-6)</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,237.52,260.80,246.61,8.97">Proceedings of the Sixth Text REtrieval Conference (TREC-6)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Sixth Text REtrieval Conference (TREC-6)</meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="500" to="240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
