<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,249.76,71.21,112.53,15.92">Océ at TREC 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,195.40,98.69,46.31,11.29"><forename type="first">Pascha</forename><surname>Iljin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Océ-Technologies</orgName>
								<address>
									<postBox>B.V. P.O. Box 101</postBox>
									<postCode>5900 MA</postCode>
									<settlement>Venlo</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.46,98.69,43.84,11.29"><forename type="first">Roel</forename><surname>Brand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Océ-Technologies</orgName>
								<address>
									<postBox>B.V. P.O. Box 101</postBox>
									<postCode>5900 MA</postCode>
									<settlement>Venlo</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.34,98.69,65.85,11.29"><forename type="first">Samuel</forename><surname>Driessen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Océ-Technologies</orgName>
								<address>
									<postBox>B.V. P.O. Box 101</postBox>
									<postCode>5900 MA</postCode>
									<settlement>Venlo</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,370.90,98.69,45.83,11.29"><forename type="first">Jakob</forename><surname>Klok</surname></persName>
							<email>klok@oce.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Océ-Technologies</orgName>
								<address>
									<postBox>B.V. P.O. Box 101</postBox>
									<postCode>5900 MA</postCode>
									<settlement>Venlo</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,249.76,71.21,112.53,15.92">Océ at TREC 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89DED8BD82CC5C70E8913B7BB6F6D685</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the work done at Océ Research for the TREC 2003. This first participation consists of ad hoc experiments for the Robust track. We used the BM25 model and our new probabilistic model to rank documents. Knowledge Concepts' Content Enabler semantic network was used for stemming and query expansion. Our main goal was to compare the BM25 model and the probabilistic model implemented with and/or without query expansion. The developed generic probabilistic model does not use global statistics of a document collection to rank documents. The relevance of the document to a given query is calculated using term frequencies of the query terms in the document and the length of the document. Furthermore, some theoretical research has been done. We have constructed a model that uses relevance judgements of previous years. However, we did not implement it due to the time constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This is our first participation in the Text REtrieval Conference. We aimed to compare the models we constructed during the last two years. We decided to participate in the Robust track because it allows to evaluate IR systems given a set of topics and relevance judgements of previous years. That is exactly what we did for an internal research using the CLEF Dutch collection. Furthermore, the Robust track is oriented towards the actual practical situation in information retrieval (i.e. good results are expected for every query). Due to the time restrictions we did not manage to retrain our theoretical model for the TREC's collection of documents and queries in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of runs</head><p>The description of the submitted runs is presented in the The information about the query construction is presented in Section 3. The models will be described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query</head><p>A query is constructed automatically from the title and description (in one of the experiments just the description is used, as required by the track guidelines) by splitting on non-alphanumerical characters to obtain terms. All single characters are removed afterwards. Furthermore, all remaining terms are converted to lower case. For the query expansion, the morphological collapse (dictionary based stemming) of Knowledge Concepts' Content Enabler semantic network is used to obtain root forms of query terms. The root forms are then expanded with the semantic network. The morphological variants of the root form (such as plural form, etc.) are added to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expansion of query terms</head><p>All query terms are morphologically expanded using Knowledge Concepts' Content Enabler semantic network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related terms and synonym expansion</head><p>Research was done on using related terms and synonyms. We found that Knowledge Concepts' Content Enabler is not good enough to create related terms and synonyms for our models. A measure of 'similarity' between two terms is needed in order to rank the proposed list of related terms and synonyms. Only terms that are very 'similar' in their meaning to a query term should be added to the expanded query.</p><p>Query consisting of topic + description tags For the experiments with the queries composed of the topic and description tags, the terms from these two tags have been put together without duplicate removal. We assumed that if a term in the query is present more than one time, it is considered to be a more important term than if it occurs once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing</head><p>The index was built by splitting documents on non-alphanumerical characters. Single characters were removed from the index. Stop words were left in the index because it is very difficult to construct a universal set of stop words. If such a set is based on the frequencies within a document collection, it is highly probable that the set of stop words will not be the same for two different document collections. In case it is based on human decisions, a number of important terms from the document collection and/or query will be removed. For example, consider the terms 'new' and 'year' as stop words (they are used in this role quite often). After removing these terms from the document collection and from the queries, it becomes difficult to find a set of relevant documents for the query 'A New Year tree'. In order to show that stop word removal is not always beneficial, consider the query 'Who said "To be or not to be?"'. In this case all terms from the query could be defined as stop words. Nevertheless, the stop words should be treated different than other terms. Therefore, we weight them down. This year the following stop word lists were used:</p><p>-Search Engine World (http://www.searchengineworld.com/spy/stopwords.htm) -Institut interfacultaire d'informatique, University of Neuchatel (http://www.unine.ch/Info/clef/)</p><p>4 Ranking models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BM25 model</head><p>The general description of the BM25 model is as follows: Let q i be a query term in query q Let q i,0 , q i,1 , …, q i,n be the expansion of q i in which q i,0 = q i Let tf (q i,j , d) be the term frequency of expansion term q i,j</p><p>We now calculate the document and term frequency of q i as follows:</p><formula xml:id="formula_0" coords="2,234.16,528.62,301.27,25.01">¦ = j j i i d q tf d q tf ) , ( ) , ( ,<label>(1)</label></formula><formula xml:id="formula_1" coords="2,70.96,574.08,221.85,23.83">df (q i )= j occurs q in which documents of set j i,</formula><p>Then for a document d, and query q, the score is calculated as:</p><formula xml:id="formula_2" coords="2,164.08,618.35,371.36,34.09">Rel (d, q)= ¦ ∈ + ⋅ + - ⋅ + ⋅ ⋅ - q q i i i i d q tf d ndl b b k k d q tf q df N ) , ( ))) ( ( ) 1 (( ) 1 ( ) , ( )) ( log( ) log( 1 1 ,<label>(2)</label></formula><p>in which ndl(d) is the length of the document d, divided by the average document length.</p><p>This model was used for the CLEF 2002 runs and has been described in <ref type="bibr" coords="2,390.21,683.45,10.60,11.29" target="#b0">[1]</ref>. Last year we observed that the performance of the BM25 ranking algorithm depends greatly on the choice of the values of the parameters k1 and b.</p><p>The estimation of those values for the optimal performance is only possible when the document collection, the set of queries and the set of relevance judgements are all available beforehand. Hence, the Robust track for the old queries is a suitable training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Probabilistic model</head><p>The probabilistic model has been selected as the result of theoretical research conducted in 2002 <ref type="bibr" coords="3,459.40,128.33,10.55,11.29" target="#b1">[2]</ref>. It contains some innovations with respect to the standard probabilistic approach. The urn model (i.e. balls in an urn = terms in a document) was selected as a basis for the probabilistic model.</p><p>We calculate the degree of relevancy without using collection statistics (e.g. document frequency). The sparse data problem is commonly solved using the linear interpolation method or other smoothing techniques that are based on collection statistics. Robertson showed that "relevance of a document to a request should not depend on the other documents in the collection" in order to guarantee "optimality of ranking by the probability of relevance" <ref type="bibr" coords="3,527.20,209.04,10.64,11.03" target="#b2">[3]</ref>. Therefore, the selection of a complete document collection as a smoothing element is not strongly motivated and not even supposed to exist according to the basic principle of the probabilistic approach in information retrieval. We found experimentally that under certain distributions of terms over documents in the document collection, the linear interpolation approach will give illogical ranking results. A standard solution to the sparse data problem is to assign non-zero values for query terms that do not exist in a document. The most natural and easy way to solve the sparse data problem is to assign a constant positive value α to the terms that do not exist in the document. We named this 'the α-method'.</p><p>For the query without term expansion:</p><formula xml:id="formula_3" coords="3,211.00,330.34,153.56,30.34">Rel (d, Q)= )] ) , (<label>( 2 1</label></formula><formula xml:id="formula_4" coords="3,258.76,330.34,276.68,32.91">[ α + ⋅ ∏ ∈ d i Q q L d q tf i ,<label>(3)</label></formula><p>where L d is the length (not normalised) of the document d, α should be less than [the length of the longest document in the document collection] -<ref type="foot" coords="3,416.86,381.11,2.70,7.17" target="#foot_0">1</ref> . This guarantees coordination level ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistical model (theoretical results)</head><p>In 2002 we aimed to implement a set of clues (that we defined) in a 'mathematically correct' model, i.e. a model without internal contradictions or violations of axioms. Examples of clues are:</p><p>-presence of terms in the document that are synonyms of the terms in the query; -importance of a topic's tag; -part of speech of the query terms; -query terms of certain document frequency; -presence of proper nouns in the query; -length of a document.</p><p>We found that a set of defined clues could not be entirely incorporated in the currently known information retrieval models while maintaining mathematical correctness. However, we have succeeded to construct a statistical approach that allows incorporation of these clues. For each clue, a value expressing its expected 'significance' is calculated.</p><p>Significance values are based on relevance judgements from previous years for (document, topic) pairs.</p><p>For every clue we test whether its incorporation makes a statistically significant contribution to the overall performance of an information retrieval system.</p><p>Let us select a clue 1 to investigate its contribution to the improvement of the performance. The following procedure is carried out for the whole set of queries. Let us consider a query q:</p><p>• From q we determine those components that can be tested for contribution of the clue with respect to the total performance of an information retrieval system.</p><p>Let us denote by Comp c (q, clue) the c th component in the query q that is tested, where ) , ( , 1 clue q C c = , and C(q, clue) is the total number of components from the query q that can be tested on the clue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>In case the clue is a 'presence of query terms in a document', all query terms are components. Example 2 In case the clue is a 'noun', the components from the query 'Crocodiles living in the lake' are 'crocodiles', and 'lake'.</p><p>• The following notation will be used:</p><p>) , ( q J R -the number of documents from document collection (Dc) that have got values relevant from the relevance judgements for the query q.</p><p>) , ( q J I -the number of documents from Dc that have got values irrelevant from the relevance judgements for the query q.</p><p>) , ( clue q Comp c R -the number of documents from Dc that have got values relevant from the relevance judgements for the query q and that contain Comp c (q, clue).</p><p>) , ( clue q Comp c I -the number of documents from Dc that have got values irrelevant from the relevance judgements for the query q and that contain Comp c (q, clue).</p><p>• Calculate for every component Comp c (q, clue):</p><formula xml:id="formula_5" coords="4,234.88,357.34,300.56,86.38">R c (clue,q) = ) , ( ) , ( q J R R clue q Comp c (4) I c (clue,q) = ) , ( ) , ( q J I I clue q Comp c (5)</formula><p>The pair (R c (clue,q), I c (clue,q)) indicates how often a component Comp c (q, clue) occurs in relevant and irrelevant documents respectively. In case R c (clue,q) &gt; I c (clue,q), the component Comp c (q, clue) occurs more often in relevant documents than in irrelevant ones.</p><p>After (R c (clue,q), I c (clue,q)) is calculated for each component c of each query q, a set of pairs {(R 1 (clue), I 1 (clue)), (R 2 (clue), I 2 (clue)),…, (R t (clue), I t (clue))} is obtained, where t= ¦</p><formula xml:id="formula_6" coords="4,332.68,513.49,60.51,33.36">= Q q clue q C 1 ) , (</formula><p>is the number of all components for clue from all Q queries in the test collection.</p><p>In case</p><formula xml:id="formula_7" coords="4,103.24,581.41,132.53,39.71">¦ ¦ &lt; = &gt; = &gt; t clue I clue R i t clue I clue R i i i i i )} ( ) ( { 1 )} ( ) ( { 1<label>1 1</label></formula><p>, one can state that after incorporating the clue, the components of q appear more often in relevant documents than they appear in irrelevant ones. This statement implies that the incorporated clue is expected to improve the performance of the information retrieval system.</p><p>In order to decide if a clue may improve the performance of the system, the set of pairs {(R 1 (clue), I 1 (clue)), (R 2 (clue), I 2 (clue)),…, (R t (clue), I t (clue))} should be statistically investigated. The statistical method called the Sign Test is used in order to compare two sets of pairs. It is the only method that can be used for our purpose.</p><p>The Sign Test is used to test the hypothesis that there is "no difference" between the two probability distributions (in our case, R(clue) and I(clue)). For the statistical model it tests whether the presence of the clue has influence on the distribution of the query components in relevant and irrelevant documents.</p><p>The theory of the Sign Test requires: 1. The pairs to be mutually independent. 2. Both R i (clue) and I i (clue) should have continuous probability distributions.</p><p>Because of the assumed mutual independence between queries, mutual independence between query terms, and mutual independence between terms in documents, pairs (R 1 (clue), I 1 (clue)) are mutually independent (point 1). A continuous distribution is defined as a distribution for which the variables may take on a continuous range of values. In the considered case, the values of both R i (clue) and I i (clue) take any value from the closed interval [0,1], and so their distributions are continuous (point 2). Hence, the necessary conditions for the Sign Test hold.</p><p>The hypothesis implies that given a pair of measurements (R i (clue), I i (clue)), both R i (clue) and I i (clue) are equally likely to be larger than the other. The zero hypothesis H 0 : P[R i (clue) &gt; I i (clue)] = P[R i (clue) &lt; I i (clue)] = 0.5 is tested for every i= t , 1 . Applying the one-sided Sign Test means that rejecting H 0 , we accept the alternative hypothesis H 1 :</p><p>P[R i (clue) &gt; I i (clue)] &gt; 0.5. A one-sided 95% confidence interval is taken to test the H 0 hypothesis. If H 0 is rejected, the incorporation of the clue is expected to improve the performance of the information retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark</head><p>Using the Sign Test described for a certain clue, we conclude whether its incorporation into an information retrieval system can improve the performance. This conclusion is based on theoretical expectations only.</p><p>Two criteria are defined to estimate the possible contribution of a clue to a system from a practical point of view.</p><p>In case there are t components for all the queries, t i , 1 = ∀ calculate for clue i) #(R(clue)) -the number of components for which R i (clue) &gt; I i (clue) ii) #(I(clue)) -the number of components for which R i (clue) &lt; I i (clue) According to the theoretical issues of the Sign Test, one has to ignore the statistics of the components for which R i (clue) = I i (clue). Thus, when a component of a certain clue is found in both relevant and irrelevant documents, and the relative frequency of R i (clue) = I i (clue), this is neither good nor bad. Such an observation should not influence the total statistics. However, the other theoretical issue will not be taken into account. According to the theory of the Sign Test, when one observes more than one component with the same values of R i (clue) and I i (clue), all but one component should be ignored too. However, this claim cannot be valid in the area of linguistics due to the following reasons:</p><p>1. The influence of each component on the clue has to be calculated. Even in the case the same statistics are obtained for different terms, all terms will make a contribution to the performance of the system. So, every component will be an extra observation for a clue.</p><p>2. If a term is used in more than one query, it has multiple influences on the performance. For each query different statistics should be obtained. Hence, each component should be considered separately for every query.</p><p>3. In case the same component is used more than one time in a query, it is considered multiple times (according to the assumption described in 'Query consisting of topic + description tags', see Section 3.1).</p><p>To estimate the significance of a certain clue, the ratio</p><p>)) ( ( # )) ( ( # clue I clue R is calculated. The larger this ratio, the higher the significance is. After calculating these ratios for all the clues, they can be ranked in a decreasing order, where the top value will correspond with the most significant clue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,76.24,462.13,337.13,83.34"><head>table below</head><label>below</label><figDesc></figDesc><table coords="1,76.24,462.13,337.13,83.34"><row><cell></cell><cell></cell><cell></cell><cell>:</cell></row><row><cell cols="2">Run number Ranking model</cell><cell>Topic's tags used</cell><cell>Expansion of query terms</cell></row><row><cell>1</cell><cell>BM25</cell><cell>Title+Description</cell><cell>yes</cell></row><row><cell>2</cell><cell>BM25</cell><cell>Title+Description</cell><cell>no</cell></row><row><cell>3</cell><cell>BM25</cell><cell>Description</cell><cell>no</cell></row><row><cell>4</cell><cell>probabilistic</cell><cell>Title+Description</cell><cell>yes</cell></row><row><cell>5</cell><cell>probabilistic</cell><cell>Title+Description</cell><cell>no</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,76.72,695.65,234.48,11.03"><p>Taking two or more clues simultaneously is very complex.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Conclusions</head><p>We have compared the BM25 and our probabilistic model on the basis of mono-lingual runs for English. The BM25 model systematically outperforms the probabilistic one. This indicates that striving for mathematical correctness does not imply better retrieval performance. At the same time we have observed that the developed probabilistic model performs satisfactorily. Furthermore, we conclude that the query expansion using the Knowledge Concepts' Content Enabler semantic network does not improve the performance of the IR systems we constructed. The performance of the IR engine using the query consisting of the description tag only, is worse than using the topic and description tags.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• Not all clues have the same contribution to the ranking function. The contribution of a certain clue depends on the level of improvement to the performance of an information retrieval system.</p><p>• Not all clues should be implemented in the statistical model.</p><p>has a value higher than one. Only in this case one can expect that the selected clue can improve the performance of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments with the statistical model</head><p>We have done a number of experiments with the statistical model for the CLEF Dutch document collection, the set of queries and the relevance judgements for 2001 and 2002. Depending on their degree of significance, different statistics have been chosen to obtain better performance for two different sets of queries (using the same document collection). The proper choice of features and their 'gain' values lead to better results. We conclude that this model is strongly dependent on the data collection, queries and relevance judgements. Hence, the results for a set of new documents, new queries and new relevance judgements are difficult to predict. Due to time restrictions we did not retrain the model for the TREC Robust track. Therefore we did not submit the statistical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical results</head><p>The following numerical results were obtained for the runs submitted by Océ at TREC 2003. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Old topics:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.08,202.21,453.23,11.03;7,70.96,213.73,71.94,11.03" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,220.21,202.21,81.91,11.03">Océ at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">Roel</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marvin</forename><surname>Brünner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,311.94,202.21,154.77,11.03">Lecture Notes on Computer Science</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer-Verlag Heidelberg</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.12,225.25,439.22,11.03" xml:id="b1">
	<monogr>
		<title level="m" coord="7,85.12,225.25,359.69,11.03">Pascha Iljin: Modeling Document Relevancy Clues in Information Retrieval Systems. SAI</title>
		<imprint/>
	</monogr>
	<note>to appear in 2004</note>
</biblStruct>

<biblStruct coords="7,86.38,236.65,454.90,11.03;7,70.96,248.17,216.22,11.03" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,160.84,236.65,206.45,11.03">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
