<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.48,72.44,380.30,16.59">UB at TREC-12: HARD and Genomics Tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,100.08,118.02,116.71,11.06"><forename type="first">Munirathnam</forename><surname>Srikanth</surname></persName>
							<email>srikanth@cedar.buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering SUNY at Buffalo Buffalo</orgName>
								<address>
									<postCode>14228</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.28,118.02,76.98,11.06"><forename type="first">Miguel</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
							<email>meruiz@buffalo.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Library and Information Studies SUNY at Buffalo Buffalo</orgName>
								<address>
									<postCode>14228</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,415.08,118.02,71.70,11.06"><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
							<email>rohini@cedar.buffalo.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Engineering SUNY at Buffalo Buffalo</orgName>
								<address>
									<postCode>14228</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.48,72.44,380.30,16.59">UB at TREC-12: HARD and Genomics Tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00E1EA2A969F084B79505FD2DA24E49F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>University at Buffalo (UB) participated in TREC-12 in Genomics and High Accuracy Retrieval from Documents (HARD) tracks. We explored some techniques that combine Information Retrieval and Information Extraction to perform the TREC tasks. We used an Information Extraction engine -InfoXtract <ref type="bibr" coords="1,149.80,277.56,9.59,8.97" target="#b2">[3]</ref> from Cymfony Inc. 1 -to enhance retrieval results.</p><p>For the Genomics primary task, documents retrieved using a vector space model with relevance feedback are reweighted based on the biomedical named entities discovered by InfoXtract. For the secondary task, extracted information along with cue words for text snippets that describe functionality is used for generating GeneRIFs for given Gene name and PubMed abstract. A language modeling approach that incorporates keyword and non-keyword features are used for the HARD task. Features extracted by InfoXtract from the HARD corpus are used to rank documents and/or passages as answers to the HARD queries.</p><p>Cymfony's InfoXtract <ref type="bibr" coords="1,155.92,413.52,9.59,8.97" target="#b2">[3]</ref> is a customizable Information Extraction engine that performs syntactic and semantic parsing of a document to identify features like named entities, relationships and events in them. The baseline InfoXtract engine has been trained for the general English and news domain, It can be customized to recognize new named entities like Gene Names and Gene Function. Biomedical Customization of InfoXtract is briefly presented in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GENOMICS TRACK</head><p>UB participated in both primary and secondary task of TREC 12 Genomics track. Our efforts concentrated on trying to apply Information extraction to improve retrieval performance in task 1 and to combine support vector machines and information extraction for selecting sentences as GeneR-IFs annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevance Feedback Model for Genomics Retrieval</head><p>We used the SMART system as a baseline as the search engine for the Genomics track. Documents are represented using title (TI), abstract (AB), MeSH terms (MH) and EC/RN Numbers (RN). We tried several models using separate ctypes for each of the previously mentioned fields and measure the contribution of each part to retrieval performance. We also experimented with a version that used all the information in a single ctype. Table <ref type="table" coords="1,157.14,693.24,4.55,8.97" target="#tab_0">1</ref> shows the contribution of each 1 www.cymfony.com part to the retrieval performance on the training topics. It is interesting to note that these runs show that the MeSH terms have a very small contribution in the retrieval results of the training topics (0.0326 Avg-Prec). We believe that this is due to two factors: a) low coverage of domain specific Genomics concepts in MeSH, and b) we did not attempt to do a mapping of topics against the MeSH vocabulary (only single word matching is used for this particular run). We were also surprised to see that the contribution of EC/RN numbers does contribute significantly to retrieval performance. The combination of all fields into a single ctype outperforms all runs that use a single field. We also tried two different stemming algorithms since we were not sure whether a simple stemming algorithm that takes care of plurals only would work better for our experiments (as reported by Jacques Savoy in his preliminary experiments on this collection). Additionally we used a heuristic method to try to capture phrases and proper nouns. For this purpose we preprocessed the documents to identify fragments delimited by punctuation symbols and extract bigrams (groups of two consecutive words) that don't include stopwords. The heuristic process takes into account exceptions that allow a limited number of stop words to be part of the bigram term ("of", and "for"), i.e. "alignment of proteins". The best results in the baseline runs were obtained using this heuristic method (0.3200 Avg-Prec).</p><p>Topics were processed by extracting the information available in each field and then representing it in the corresponding ctype. For runs that use bigrams we added the corresponding bigrams and phrases to each training topic using the previously described heuristic. We also tried several weighting schemes (atn, atc, ann, and Lnu for documents and ntc, ltc, lnc, atc, atn, ann, anc, and ltu for queries).</p><p>We also performed pseudo-relevance feedback using the top n documents as relevant and selecting the top m terms to expand each query. The results of our experiments on the training set are presented in Table <ref type="table" coords="1,473.06,588.83,3.54,8.97" target="#tab_1">2</ref>. Pseudo Relevance feedback combined with bigrams is the one that yields the best performance in the training topics (0.3702 Avg-Prec with a 16% improvement over the corresponding baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Extraction in BioMedical Domain</head><p>The InfoXtract engine is customized for the BioMedical domain before using it to process the Genomics document collection. Domain knowledge is essential for an Information Extraction engine to tag documents with named entities in the domain of interest. We used part of UMLS hierarchy weights P@10 Avg. We selected several subtrees from the UMLS that are most related to the Genomics sub domain. For this purpose we selected the following concepts:</p><p>• C1136351: Genetic Phenomena.</p><p>• C1136308: Genetic Processes We use the parent-child relationship in the UMLS metathesaurus to select all terms related (as a narrower concept) for each of these general concepts. This produced a set of 21, 070 concepts and 51, 571 unique terms (after normalization). We also add the related terms to the species of interest in this task ( Homo sapiens, Mus musculus, Rattus norvegicus, and Drosophila melanogaster) for 31 extra terms.</p><p>In addition, InfoXtract is customized to identify and tag named entities of type Gene Name and Gene Functionality. Gene names and their synonyms are collected from Lo-cusLink and assigned a unique name (typically the preferred product name). Automatic candidate selection followed by manual truthing is adopted for generating the Gene Functionality lexicon. The InfoXtract engine is customized to detect Gene Names. Some training documents are processed by InfoXtract and term statistics is used to generate candidate functionality terms. Some heuristics like ignoring terms that appear in a named entity like Gene Name are used to filter out terms and construct a candidate set for Gene  Functionality. Researchers in Biology were asked to manually go through the functionality term list and the context of their usage in the training documents subset to identify Gene functionality terms.</p><p>Genomics document collection is processed by the customized InfoXtract engine. It extracts the 12 different named entities mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Using IE in Genomics IR</head><p>Natural Language Processing (NLP) techniques have been used in document retrieval to select index terms. Prior use of Information Extraction output has been restricted to narrow search problems like question answering. We used Information Extraction as a filter to improve or re-rank the retrieval results. Documents are processed by InfoXtract customized for the biomedical domain. The output of InfoXtract for a document, in addition tagged biomedical named-entities, includes part-of-speech, shallow parsing results and relations between named entities. A subset of the extracted information along with the terms in a document are indexed using the TAPIR toolkit. TAPIR toolkit is a library of software tools that facilitate a number of IR tasks and supports different IR models including language models. The position of the index term in a document is also used as the position of tags representing the extracted information. This index is used to re-rank the document retrieval results corresponding to the run UBgenomRFB1.</p><p>Given a query, each document deemed relevant in the run UBgenomRFB1 to the query is weighted for the cooccurrence of query terms with named entities in the biomedical domain. The co-occurrence frequency of tags with query terms is used to reweight documents. Ad-hoc weights are assigned to different named entity tags with the highest weight given to co-occurrence with gene function words, terms related to genetic process and other gene name. Reweighted documents set is normalized and re-ranked to generate the result named UBgenomBGNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">GeneRIF Extraction</head><p>As noted in the guidelines for this task Mork and Aronson have found that 95% of the GeneRIF snippets contain text from the title and abstract of the articles. For this reason we decided to concentrate our approach on selecting sentences from the title and abstract as a first approach. For GeneRIF extraction we propose a solution that uses text categorization to select the sentence from the MEDLINE document (Title and Abstract) that is the best candidate to be a GeneRIF descriptor.</p><p>Documents are processed using InfoXtract to detect sentences, as well as important information such as the name of the gene of interest, or description of the functionality related to the gene of interest.</p><p>Our baseline system for this task is a trivial procedure that assigns the title as the candidate for GeneRIF description. We also tried to find what would be the upper bound of performance for a method based on sentence selection. For this purpose we found the sentence that would give the highest Dice coefficient value for each GeneRIF.</p><p>The approach that we use for this task uses a support vector machine (SVM-light) to learn the sentences that should be selected as GeneRIF using as input features the vector representation of the document using the smoothed unigram language model. We also included other features such as the position of then sentence in the document, whether the gene of interest is mentioned in the sentence, and whether biomedical terms (that were extracted by InfoXtract) appear in the sentence. For training the SVM we collected 5496 GeneRIFs annotations from LocusLink and gather the respective MEDLINE documents (making sure that these GeneRIFs where not in the test set for the secondary task). This set was randomly divided into 3, 676 documents for training and 1, 820 documents for validation.We also determined the sentence that had the best Dice score in the document to be the "correct GeneRIFs" and mark it as a positive example while the rest of the sentences in the document were marked as negative examples. This process gave us a total of 19, 658 sentences in the training set and 9, 947 sentences in the validation set. The sentence with the highest SVM classification score was selected as the GeneRIF for each document. Table <ref type="table" coords="3,88.43,658.32,4.55,8.97" target="#tab_2">3</ref> shows the results of our experiments on the validation set of 1, 820 GeneRIFs. The upper-bound indicates that the best we can performance of a method that selects full sentences from the MEDLINE article would be at 76.18. This corroborates that most of the GeneRIFs come from "cut and paste" text from the Title and abstract. Our base-line system that selects the title as the GeneRIF annotation performs at 57.02% which indicates that the simplest algorithm for selecting the GeneRIFs annotations is obtaining about 73% of the Upper-bound performance. This is not surprising since a significant number of GeneRIFs are just the title of the article. The first attempt to use SVM only used the features extracted from the unigram language model and performs significantly below our baseline (45.54% Classical Dice and about 20% below the baseline). when we added the cues (gene name, and functionality, words, etc) and the relative position of the sentence in the document the SVM was able to achieve a performance that is about the same as the baseline (56.94% Classical Dice). We were disappointed to realize that after all the training process and information extraction our system wasn't better than our simple baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Upper Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Results and Analysis</head><p>Our official results in task 1 are presented in Table <ref type="table" coords="3,548.62,245.28,3.54,8.97">4</ref>. UBgenomRFB1 uses pseudo-relevance feedback, atn.ltc weighting scheme, and the top 3 retrieved documents from which we get the top 5 terms for query expansion, and α = 32.β = 16.γ = 8 as the parameter in Rocchio's formula. UBgenom-RFB2 uses pseudo-relevance feedback, atn.ltu weighting scheme, and the top 3 retrieved documents from which we get the top 5 terms for query expansion, and α = 8, β = 64, γ = 16 as the parameter in Rocchio's formula. UBgenomeBGNE is the re-ranked output of UBgenomRFB1 using the filtering process explained previously. In general our results are slightly below the average performance. We suspect that this could be a consequence of the way topics were converted into queries in the system but we still need to do a more detailed analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HARD TRACK</head><p>In the HARD track user queries are qualified by metadata that provide additional information on the user's information need such as purpose, genre and granularity of query. Of the three steps in the track -baseline retrieval, clarification forms and final run to use all the information available about the user query. The second step is optional and we did not generate any clarification forms for the queries. We used the metadata provided along with the query in the final run. Language Modeling approach <ref type="bibr" coords="4,199.93,274.32,9.59,8.97" target="#b0">[1]</ref> is adopted for both the baseline and final HARD tasks.</p><p>HARD document corpus is processed by Cymfony's In-foXtract engine which tags part-of-speech, named entities and associated profiles to the entities and events discovered in the document. A subset of information extracted by In-foXtract is used in our HARD solution. However, the text and all extracted information from a document are indexed by TAPIR -an IR toolkit that supports different IR models including Language Modeling approach to IR. A tag of a term in a document is indexed as though it is embedded in the document at the position(offset) corresponding to the term. This method of indexing has been explored for question answering in earlier TREC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline System</head><p>We submitted three different runs for baseline. Our baseline solution is based on the textual part of the user querytitle and description -and perform document retrieval. No effort is made to predict any metadata for the user queries as well as no query expansion or relevance feedback is experimented with. A brief description of the three runs are given below:</p><p>• ub03sugT Run based on smoothed unigram language model that uses Dirichlet smoothing. The Dirichlet parameter is set to 1000. This run used only the title of the query.</p><p>• ub03cugTD This run is based on the Concept Unigram Language Models (CULM) <ref type="bibr" coords="4,189.52,585.12,9.59,8.97" target="#b3">[4]</ref> that have been shown to perform better than smoothed unigram and bigram language models. In Concept Language Model a query is viewed as a sequence of concepts and concepts, in turn, are viewed as a sequence of terms. Consecutive terms typically constitute concepts that can be single terms, bigrams or n-grams. In CULM, concept independence is assumed and query probability is computed as a unigram model on concepts. Concept probability is approximated to smoothed bigram probabilities. The InfoXtract question parser <ref type="bibr" coords="4,264.58,689.64,9.59,8.97" target="#b1">[2]</ref> and its shallow parsing results are used to identify concepts of interest in the query.</p><p>• ub03ugTcugTD This run corresponds to a linear combination of the above two methods. It provided slight improvement in mean average precision on the training set.</p><p>The results of ub03ugTcugTD are used for the final run (ub03smfugTD) as the document retrieval system. Its results are further refined to satisfy the metadata values of the user query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieval</head><p>Except for the queries with granularity value of document, passage retrieval is performed for other queries. Relevant passages are selected based on the query keywords. The granularity of the query determines the length of the passage. Relevant sentences (i.e. passages of length 1) are identified for queries with granularity of sentence or phrase. The granularity of passage resulted in the system selecting text snippets with 3 to 6 sentences. The passages are not overlapped. The coverage of query terms is used as a criteria to determine the passage length. All candidate passages thus selected are considered for final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MetaData Modeling and HARD</head><p>We modeled Purpose and Genre metadata for the HARD task. For a given query, text snippets are short-listed based on their satisfying the user's query and granularity requirements. A number of keyword-and non-keyword based features are used to weight the snippets and rank them. A text snippet is viewed as a sequence of keyword and nonkeyword features and a model is associated with it in the language modeling sense. Text snippets are weighted based on the probability of generating a given feature. Smoothed probabilities are estimated for keyword features and empirical probabilities are used for non-keyword features. Ad-hoc query weights are assigned for these features based on the metadata values.</p><p>Genre metadata is handled as follows: Reaction or I-Reaction typically involve entities that we group as actors. This includes persons, organizations, Government entities. The occurrence of such entities triggers Reaction or I-Reaction genre type. If the origin of such an actor is a US location, the text snippet is most likely to be a Reaction than I-Reaction. InfoXtract engine tags associations or relations that can link entities of one type with another. To determine if the text snippet is Reaction or I-Reaction, entities associated with actors are searched to see if they include location entities. Location names are checked against a lexicon of US cities, counties and states to determine if the origin of the actor. A number of US cities share names with non-US locationse.g. Moscow and China. In such instances, the context of the location name is checked. If the next word/phrase is a US location or the phrase "United States" or its variations, the location is classified as a US location. Document-source based features are used to weight text snippets against genre value of Administration. We did not eliminate all non-government sourced documents but assigned lower weights than government sourced documents.</p><p>The location of the text snippet in a document, presence or absence of details (described below) information and the query term coverage were used as triggers for the different values of purpose metadata. We assumed that a text snippet provides details on a particular topic of interest if it contains numeric, percentage, frequency and time information. Such information, extracted by InfoXtract, are grouped together as details information and snippets are weighted based on the frequency of such information. The absence of such information is also used as a feature. The snippet location feature gives more weights to snippets closer to the center of the document. It is based on the heuristic that details and background information are typically found in the middle rather than at the start or end of the document. Queries with metadata background or details assign more weights to this feature than queries with metadata or answer or any.</p><p>With the absence of significant training data for the different metadata values, we used ad-hoc query weighting. Expectation Maximization or maximum entropy models can be used, in the presence of training data, to weight these features against the metadata values in a query. The ub03ugTcugTD run on training topics performed better than the other two baseline runs. While the combination of language models did improve the average precision values for 28 topics over the CULM, the average performance over all topics of CULM is better than the combination model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results and Analysis</head><p>The performance reduction is significant -as much as 300%for some topics. The ub03smfugTD run that used the metadata to re-rank the results of ub03ugTcugTD did not results in any improvements at document level performance measures. All text snippets from a document were selected and ranked for HARD retrieval. The ad-hoc weighting of the features and no cutoff on the number of snippets selected from a relevant document are possible reasons for the decrease in performance.</p><p>Psg. Prec. @10 R-Prec. F @ 30 ub03sugT 0.2399  <ref type="table" coords="5,351.23,292.08,4.55,8.97" target="#tab_6">9</ref> gives the performance measures for passage relevance. The first three runs were evaluated with the whole document being the retrieved passage. Ub03smfugTD included passage level results corresponding to the granularity metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This section discusses future directions of our work for the two tracks we participated in. Better query representation, the use of all extracted information and incorporating more domain knowledge in document and query processing are some of the avenues of improvement for the Genomics track. For the HARD track, we have only used a subset of information extracted by the InfoXtract engine to represent queries and model metadata. Ad-hoc weights were assigned to extracted features in ranking documents for a given query. This was partly due to the absence of sufficient training data. We plan to explore some formal methods for modeling metadata -specifically identifying and weighting features that satisfy the metadata requirements of queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,67.08,392.64,130.10,8.97;2,67.08,410.76,131.67,8.97;2,67.08,429.00,198.69,8.97;2,67.08,447.12,225.67,8.97;2,76.20,457.56,72.18,8.97;2,67.08,475.80,132.37,8.97;2,67.08,493.92,225.79,8.97;2,76.20,504.36,30.69,8.97"><head>•</head><label></label><figDesc>C1136352: Genetic Structures • C0017398: Science of Genetics • C0002526: Amino acids, Peptides and Proteins • C0019934: Hormones, Hormone Substitutes and Hormone Antagonists • C0018285: Growth Substances • C0014443: Enzymes, Co-enzymes and Enzymes Inhibitors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,53.76,55.80,245.66,260.25"><head>Table 1 :</head><label>1</label><figDesc>Baseline runs on Training Topics</figDesc><table coords="2,53.76,55.80,245.66,260.25"><row><cell></cell><cell></cell><cell cols="2">Prec. R-Prec.</cell></row><row><cell>TI</cell><cell>atn.ntc 0.1040</cell><cell>0.1961</cell><cell>0.1757</cell></row><row><cell>AB</cell><cell>atn.atc 0.1380</cell><cell>0.2604</cell><cell>0.2115</cell></row><row><cell>MH</cell><cell>atn.lnc 0.0180</cell><cell>0.0326</cell><cell>0.0183</cell></row><row><cell>RN</cell><cell>atn.ntc 0.0620</cell><cell>0.1777</cell><cell>0.1615</cell></row><row><cell>TI+AB+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MH+RN</cell><cell>atn.ltu 0.1320</cell><cell>0.3028</cell><cell>0.2683</cell></row><row><cell>(one ctype)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lovins' Stemmer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TI+AB+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MH+RN</cell><cell>atn.ltu 0.1340</cell><cell>0.3044</cell><cell>0.2706</cell></row><row><cell>(one ctype)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rem-s Stemmer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TI+AB+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MH+RN+</cell><cell>atn.ltc 0.1320</cell><cell>0.3200</cell><cell>0.2743</cell></row><row><cell>Bigrams</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(one ctype)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rem-s Stemmer</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">as domain knowledge for InfoXtract. We restricted our cus-</cell></row><row><cell cols="4">tomization effort to identifying domain-specific terminology</cell></row><row><cell>through lexicons.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,316.80,278.72,238.91,18.52"><head>Table 2 :</head><label>2</label><figDesc>Pseudo-Relevance Feedback runs on Training Collection</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,59.16,509.64,229.71,125.05"><head>Table 3 :</head><label>3</label><figDesc>Results in the Validation Set.</figDesc><table coords="3,59.16,509.64,229.71,104.73"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feature+</cell></row><row><cell></cell><cell></cell><cell>bound</cell><cell>only</cell><cell>cues+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>position</cell></row><row><cell>Classic Dice</cell><cell>57.02</cell><cell>76.18</cell><cell>45.54</cell><cell>56.94</cell></row><row><cell>Mod unigram</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dice</cell><cell>57.70</cell><cell>76.51</cell><cell>45.77</cell><cell>57.68</cell></row><row><cell>Mod bigram</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dice</cell><cell>42.79</cell><cell>67.59</cell><cell>30.87</cell><cell>43.23</cell></row><row><cell>Mod bigram</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dice phrases</cell><cell>46.00</cell><cell>69.93</cell><cell>34.01</cell><cell>46.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,316.80,412.20,238.99,262.77"><head>Table 6 :</head><label>6</label><figDesc>Officail results for Secondary Task.</figDesc><table coords="3,316.80,412.20,238.99,262.77"><row><cell></cell><cell cols="3">P@10 Avg. Prec. R-Prec.</cell><cell></cell></row><row><cell>UBgenomRFB1</cell><cell>0.1160</cell><cell>0.1511</cell><cell>0.1232</cell><cell></cell></row><row><cell>UBgenomRFB2</cell><cell>0.1120</cell><cell>0.1493</cell><cell>0.1141</cell><cell></cell></row><row><cell cols="2">UBgenomeBGNE 0.1440</cell><cell>0.1867</cell><cell>0.1603</cell><cell></cell></row><row><cell cols="5">Table 4: Pseudo-Relevance Feedback runs on Train-</cell></row><row><cell>ing Collection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Best ≥Median &lt;Median Worst</cell></row><row><cell>UBgenomRFB1</cell><cell>1</cell><cell>15</cell><cell>34</cell><cell>6</cell></row><row><cell>UBgenomRFB2</cell><cell>1</cell><cell>14</cell><cell>35</cell><cell>7</cell></row><row><cell>UBgenomeBGNE</cell><cell>1</cell><cell>18</cell><cell>31</cell><cell>0</cell></row><row><cell cols="5">Table 5: Relative performance of Official Task 1 Ge-</cell></row><row><cell>nomics runs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Results for the secondary task in Genomics are presented</cell></row><row><cell cols="5">in Table 6. As suspected from our results in the training</cell></row><row><cell cols="5">and validation set the SVM based approach did not perform</cell></row><row><cell cols="5">significantly better than our base line system. All our runs</cell></row><row><cell cols="5">are fairly close to the median system performance (49.31%</cell></row><row><cell cols="3">Classic dice coefficient) in this task.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,53.76,235.56,239.01,390.09"><head>Table 7 :</head><label>7</label><figDesc>Table7presents the performance of the different runs submitted for the HARD track. These performance are judged for soft document relevance. Documents that satisfy the query and not necessarily the metadata requirements are identified to be soft relevant to the query. The Best, Worst and Median values for these measures are also given. Concept unigram language model performs better than the rest of our submissions. The use of syntactic information in query modeling provides around 13% improvement over smoothed unigram language model. Soft Document Relevance Comparison Hard document relevance corresponds to documents that are relevant to the query as well as satisfy the metadata requirements of the query. The Hard-relevance performance comparison is given in Table8.</figDesc><table coords="5,59.16,351.96,229.10,273.69"><row><cell></cell><cell cols="3">Rel. Ret. @10 Avg. Prec. R-Prec.</cell></row><row><cell>ub03sugT</cell><cell>5.375</cell><cell>0.3126</cell><cell>0.3335</cell></row><row><cell>ub03cugTD</cell><cell>5.854</cell><cell>0.3540</cell><cell>0.3784</cell></row><row><cell>ub03ugTcugTD</cell><cell>5.729</cell><cell>0.3495</cell><cell>0.3663</cell></row><row><cell>ub03smfugTD</cell><cell>3.583</cell><cell>0.2073</cell><cell>0.2562</cell></row><row><cell>Best</cell><cell>6.5</cell><cell>0.4069</cell><cell>0.4250</cell></row><row><cell>Median</cell><cell>4.729</cell><cell>0.2841</cell><cell>0.2994</cell></row><row><cell>Worst</cell><cell>0.417</cell><cell>0.0026</cell><cell>0.0038</cell></row><row><cell></cell><cell cols="3">Rel. Ret. @10 Avg. Prec. R-Prec.</cell></row><row><cell>ub03sugT</cell><cell>4.042</cell><cell>0.2543</cell><cell>0.2764</cell></row><row><cell>ub03cugTD</cell><cell>4.542</cell><cell>0.2981</cell><cell>0.3286</cell></row><row><cell>ub03ugTcugTD</cell><cell>4.479</cell><cell>0.2896</cell><cell>0.3075</cell></row><row><cell>ub03smfugTD</cell><cell>2.771</cell><cell>0.1726</cell><cell>0.2078</cell></row><row><cell>Best</cell><cell>5.271</cell><cell>0.3875</cell><cell>0.3604</cell></row><row><cell>Median</cell><cell>3.792</cell><cell>0.2673</cell><cell>0.2490</cell></row><row><cell>Worst</cell><cell>0.312</cell><cell>0.0038</cell><cell>0.0024</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,60.48,637.88,225.51,8.08"><head>Table 8 :</head><label>8</label><figDesc>Hard Document Relevance Comparison</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="5,325.80,173.76,218.07,127.29"><head>Table 9 :</head><label>9</label><figDesc>Passage Relevance Comparison</figDesc><table coords="5,478.38,173.76,65.47,8.97"><row><cell>0.1594</cell><cell>0.1178</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,331.08,529.20,207.46,8.97;5,331.08,539.64,207.65,8.97;5,331.08,550.08,200.49,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,453.75,529.20,84.78,8.97;5,331.08,539.64,132.65,8.97">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,482.03,539.64,56.69,8.96;5,331.08,550.08,36.34,8.96">Proceedings of SIGIR&apos;98</title>
		<meeting>SIGIR&apos;98<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.07,561.60,201.11,8.97;5,331.08,572.04,197.22,8.97;5,331.08,582.48,187.13,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,441.29,561.60,90.90,8.97;5,331.08,572.04,181.35,8.97">A Question Answering System Supported by Information Extraction</title>
		<author>
			<persName coords=""><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,331.08,582.48,95.16,8.96">Proceedings of ANLP&apos;00</title>
		<meeting>ANLP&apos;00</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.08,593.88,188.30,8.97;5,331.08,604.44,182.63,8.97;5,331.08,614.88,208.13,8.97;5,331.08,625.32,217.21,8.96;5,331.08,635.76,181.97,8.96;5,331.08,646.20,152.63,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,331.08,604.44,182.63,8.97;5,331.08,614.88,117.69,8.97">Infoxtract: A customizable intermediate level information extraction engine</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Rohini K Srihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,467.41,614.88,71.80,8.96;5,331.08,625.32,217.21,8.96;5,331.08,635.76,181.97,8.96;5,331.08,646.20,39.96,8.96">Proceedings of the NAACL 2003 Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS)</title>
		<meeting>the NAACL 2003 Workshop on Software Engineering and Architecture of Language Technology Systems (SEALTS)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,331.08,657.72,196.28,8.97;5,331.08,668.16,224.11,8.97;5,331.08,678.60,213.98,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,444.99,657.72,82.37,8.97;5,331.08,668.16,224.11,8.97;5,331.08,678.60,19.62,8.97">Exploiting Syntactic Structure of Queries in a Language Modeling Approach to IR</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,421.11,678.60,95.18,8.96">Proceedings of CIKM&apos;03</title>
		<meeting>CIKM&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
