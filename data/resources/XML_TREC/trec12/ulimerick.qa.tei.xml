<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,82.77,76.73,446.02,18.59">Question Answering using the DLT System at TREC 2003</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.77,109.70,125.06,13.44"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.53,109.70,65.78,13.44"><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.56,109.70,96.26,13.44"><forename type="first">Michael</forename><surname>Mulcahy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.01,109.70,76.03,13.44"><forename type="first">Kieran</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Documents and Linguistic Technology Group Department of Computer Science and Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,82.77,76.73,446.02,18.59">Question Answering using the DLT System at TREC 2003</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">523D3BD6CF88D8547CC5AD325B917942</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article outlines our participation in the Question Answering Track of the Text REtrieval Conference organised by the National Institute of Standards and Technology. This was our second year in the track and we hoped to improve our performance relative to 2002. In the next section we outline the general strategy we adopted, the changes relative to last year and the approaches taken to the three question types, namely factoid, list and definition. Following this the individual system components are described in more detail. Thirdly, the runs we submitted are presented together with the results obtained. Finally, conclusions are drawn based on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Outline of System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Strategy</head><p>In common with most other QA systems we use the following general strategy to answer questions:</p><p>• Question analysis: Process the input query attempting to find its type (e.g. who or colour).</p><p>• Document retrieval: Formulate a search query based on the results of the previous stage. Use this together with a search engine indexed on the document collection to produce a list of candidate documents which are likely to contain answers to the question.</p><p>• Named entity recognition: Based on the query type identified in the first stage, search for appropriate named entities (NEs) in the candidate documents which co-occur with terms derived from the query.</p><p>• Answer selection: Decide which NE (or NEs) should be chosen as the answer.</p><p>We now outline how this strategy was adapted to handle the three types of question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Factoids</head><p>Factoid questions formed the majority (413) of the total of 500 in the collection. They are intended to ask about straightforward pieces of information which can be extracted from free text fairly readily. Our approach to these was based entirely on traditional NEs, i.e. numbers, places, person names and so on. The type of the question as identified in the first stage was directly mapped onto one or more named entities which were then searched for in the text. For example if the question was identified as being of type what_city then the NEs used were nea_x_us_city and nea_x_non_us_city. The answer to the question was defined to be the NE which scored best by one of two measures. The highest_scoring method looked at how many query-derived terms co-occurred with the NE. The highest_google method used the World Wide Web (WWW) to predict the NE most likely to be the correct answer by adopting an algorithm similar to that of <ref type="bibr" coords="3,193.84,100.81,91.99,10.57" target="#b2">Magnini et al. (2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lists</head><p>The approach to answering the X list questions was identical to that for factoids except in the answer selection stage. Here, multiple answers were selected based on their exceeding a simple threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Definitions</head><p>In order to answer the Y definition questions, the NE recognition stage was adapted to find instances of simple phrasal patterns based around terms from the query. These had been developed in another project concerned with scientific definitions. All such phrases were extracted during answer selection. The first two stages of processing were the same as for factoids and lists.</p><p>In the next section we describe the various components of the system in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DLT System Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Summary of Enhancements</head><p>Our 2002 TREC system was constructed in a short time frame and was thus very basic. A number of significant extensions were made to the system for this year which we summarise here. Firstly additional query types were added, taking the total from 19 plus 'unknown' up to 43 plus 'unknown'. Examples of the 29 types actually used this year are shown in Table <ref type="table" coords="3,327.57,393.37,5.52,10.57" target="#tab_0">1</ref> with the remaining fifteen types listed in the caption. Secondly the process of query analysis was completely changed in order to allow a strategy for formulating and subsequently simplifying search expressions to be implemented. Thirdly documents in the Aquaint collection were indexed and searched using a commercial engine <ref type="bibr" coords="3,429.09,432.25,81.08,10.57">(DTSearch, 2000)</ref> rather than relying on the NIST TOPDOCS files. Fourthly recognisers for various NEs were added, including one for recognising general names. Finally, an answer re-ranking component using WWW hit counts was developed. These enhancements are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Query Type Identification</head><p>As before this was accomplished using simple keyword combinations and without carrying out syntactic parsing of the query. 24 additional types were added relative to last year. However, many of these did not in fact come up in the test questions (see Table <ref type="table" coords="3,283.77,546.61,4.00,10.57" target="#tab_0">1</ref>). Once again an 'unknown' type was adopted to allow a default strategy for queries not falling into other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Analysis</head><p>Following type identification the query is subjected to a detailed analysis to assist in the process of search expression formulation as follows:</p><p>• Initial words and phrases are removed;</p><p>• Capitalised word sequences and expressions within quotation marks are recognised;</p><p>• Alternatives for all-capitals words and initial sequences are computed (e.g. ACLU could also be A.C.L.U. and T.E. could be TE); • Alternative formulations for numbers are computed (e.g. Apollo-11 could be Apollo-Eleven);</p><p>• Stopwords are removed;</p><p>• The tense of any remaining verbs is changed to simple past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Search Expression Formulation</head><p>Based on the results of query analysis a search expression is composed. All searches are boolean.</p><p>Remaining query terms (i.e. quotations, capitalised word sequences or individual words, each possibly in disjunction with alternatives) are assigned an importance score using a scheme reminiscent of <ref type="bibr" coords="5,491.61,124.45,48.43,10.57;5,72.09,137.41,43.39,10.57" target="#b2">Magnini et al. (2002)</ref>. Quotations score 10, capitalised word sequences 9, numbers 8, pure nouns and verbs 7, superlative adjectives 6, pure adjectives 2, pure adverbs 1 and every other term 5. A 'pure' noun is one which can not have any other part-of-speech and similarly for pure verbs etc. Terms are then ordered by increasing score and then joined with AND operators to make a single boolean query. This is then used to search for documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Retrieval</head><p>Before the runs, the Aquaint collection was indexed by treating each paragraph (marked by a &lt;p&gt; tag) as a separate document. During retrieval a boolean query as formulated in the previous stage is submitted to the search engine and the first n matching documents (i.e. paragraphs) are returned. n was set to 30 throughout. If no documents are returned the least significant term (i.e. the first) is removed and the search is repeated. This process continues until at least one document is returned or no terms remain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Named Entity Recognition</head><p>NE recognition is similar to last year and uses our own module which is based on grammars. Some extra types were added including nea_x_title and nea_x_general_name. The former recognises quotes expressions while the latter can recognise capitalised expressions with spurious matches being eliminated using simple heuristics. Following <ref type="bibr" coords="5,238.53,379.09,94.64,10.57" target="#b0">Clarke et al. (2003)</ref> queries of unknown type are answered by searching for general names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Answer Selection</head><p>In order to decide which NE candidate (or candidates in the case of list questions) should be returned, two strategies for answer selection were used. The first is highest_scoring, where we return the NE occurring in a context which matches terms in the query best. The second is highest_google which uses a similar algorithm to <ref type="bibr" coords="5,162.56,480.49,91.99,10.57" target="#b2">Magnini et al. (2002)</ref>. Specifically for candidate answer we</p><p>• Submit the candidate answer to Google with search terms and record the number of hits;</p><p>• Submit the candidate answer to Google alone and record the number of hits (it will be many more);</p><p>• Divide the first value by the second.</p><p>During answer selection for factoid questions the candidate with the highest score is chosen. For list questions a threshold of 0.03 was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Runs and Results</head><p>Two runs were submitted. The first used highest_scoring for answer selection while the second used highest_google. The results of Run 2 are shown in Table <ref type="table" coords="5,331.53,639.49,4.14,10.57">2</ref>. The first two columns show the numbers of queries which were classified correctly (C) and incorrectly (NC) broken down by query type. 287 of the 413 factoid questions were classified correctly i.e. 69%. However if unknown and gen_name queries (which are effectively the same) are disregarded the rate of success in recognising known query types is 231 out of 258 which is 90%. As noted before, fifteen query types did not come up at all.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,174.33,95.35,24.49,8.61;4,259.77,95.35,4.50,8.61;4,288.57,95.35,4.50,8.61;4,317.37,95.35,4.50,8.61;4,346.17,95.35,4.50,8.61;4,374.97,95.35,4.50,8.61;4,403.77,95.35,4.50,8.61;4,432.57,95.35,4.50,8.61;4,174.33,108.91,47.99,8.61;4,259.77,108.91,4.50,8.61;4,288.57,108.91,4.50,8.61;4,317.37,108.91,4.50,8.61;4,346.17,108.91,4.50,8.61;4,374.97,108.91,4.50,8.61;4,403.77,108.91,4.50,8.61;4,432.57,108.91,4.50,8.61;4,174.33,123.91,23.00,8.61;4,259.77,123.91,4.50,8.61;4,288.57,123.91,4.50,8.61;4,317.37,123.91,4.50,8.61;4,346.17,123.91,4.50,8.61;4,374.97,123.91,4.50,8.61;4,403.77,123.91,4.50,8.61;4,432.57,123.91,4.50,8.61;4,174.33,137.47,32.99,8.61;4,259.77,137.47,4.50,8.61;4,288.57,137.47,4.50,8.61;4,317.37,137.47,4.50,8.61;4,346.17,137.47,4.50,8.61;4,374.97,137.47,4.50,8.61;4,403.77,137.47,4.50,8.61;4,432.57,137.47,4.50,8.61;4,174.33,152.47,31.48,8.61;4,259.77,152.47,4.50,8.61;4,288.57,152.47,4.50,8.61;4,317.37,152.47,4.50,8.61;4,346.17,152.47,4.50,8.61;4,374.97,152.47,4.50,8.61;4,403.77,152.47,4.50,8.61;4,432.57,152.47,4.50,8.61;4,174.33,167.47,29.49,8.61;4,255.21,167.47,9.00,8.61;4,288.57,167.47,4.50,8.61;4,317.37,167.47,4.50,8.61;4,346.17,167.47,4.50,8.61;4,374.97,167.47,4.50,8.61;4,399.21,167.47,9.00,8.61;4,428.01,167.47,9.00,8.61;4,174.33,182.47,36.99,8.61;4,259.77,182.47,4.50,8.61;4,288.57,182.47,4.50,8.61;4,317.37,182.47,4.50,8.61;4,346.17,182.47,4.50,8.61;4,374.97,182.47,4.50,8.61;4,403.77,182.47,4.50,8.61;4,432.57,182.47,4.50,8.61;4,174.33,197.47,47.00,8.61;4,255.21,197.47,9.00,8.61;4,288.57,197.47,4.50,8.61;4,317.37,197.47,4.50,8.61;4,346.17,197.47,4.50,8.61;4,374.97,197.47,4.50,8.61;4,399.21,197.47,9.00,8.61;4,428.01,197.47,9.00,8.61;4,174.33,212.47,44.50,8.61;4,255.21,212.47,9.00,8.61;4,288.57,212.47,4.50,8.61;4,317.37,212.47,4.50,8.61;4,346.17,212.47,4.50,8.61;4,374.97,212.47,4.50,8.61;4,399.21,212.47,9.00,8.61;4,428.01,212.47,9.00,8.61;4,174.33,226.03,40.00,8.61;4,259.77,226.03,4.50,8.61;4,288.57,226.03,4.50,8.61;4,317.37,226.03,4.50,8.61;4,346.17,226.03,4.50,8.61;4,374.97,226.03,4.50,8.61;4,403.77,226.03,4.50,8.61;4,432.57,226.03,4.50,8.61;4,174.33,241.03,55.00,8.61;4,259.77,241.03,4.50,8.61;4,288.57,241.03,4.50,8.61;4,317.37,241.03,4.50,8.61;4,346.17,241.03,4.50,8.61;4,374.97,241.03,4.50,8.61;4,403.77,241.03,4.50,8.61;4,432.57,241.03,4.50,8.61;4,174.33,254.59,37.99,8.61;4,259.77,254.59,4.50,8.61;4,288.57,254.59,4.50,8.61;4,317.37,254.59,4.50,8.61;4,346.17,254.59,4.50,8.61;4,374.97,254.59,4.50,8.61;4,403.77,254.59,4.50,8.61;4,432.57,254.59,4.50,8.61;4,174.33,269.59,55.99,8.61;4,259.77,269.59,4.50,8.61;4,288.57,269.59,4.50,8.61;4,317.37,269.59,4.50,8.61;4,346.17,269.59,4.50,8.61;4,374.97,269.59,4.50,8.61;4,403.77,269.59,4.50,8.61;4,432.57,269.59,4.50,8.61;4,174.33,283.15,38.50,8.61;4,259.77,283.15,4.50,8.61;4,288.57,283.15,4.50,8.61;4,317.37,283.15,4.50,8.61;4,346.17,283.15,4.50,8.61;4,374.97,283.15,4.50,8.61;4,403.77,283.15,4.50,8.61;4,432.57,283.15,4.50,8.61;4,174.33,296.71,33.99,8.61;4,259.77,296.71,4.50,8.61;4,288.57,296.71,4.50,8.61;4,317.37,296.71,4.50,8.61;4,346.17,296.71,4.50,8.61;4,374.97,296.71,4.50,8.61;4,403.77,296.71,4.50,8.61;4,432.57,296.71,4.50,8.61;4,174.33,311.71,20.49,8.61;4,259.77,311.71,4.50,8.61;4,288.57,311.71,4.50,8.61;4,317.37,311.71,4.50,8.61;4,346.17,311.71,4.50,8.61;4,374.97,311.71,4.50,8.61;4,403.77,311.71,4.50,8.61;4,432.57,311.71,4.50,8.61;4,174.33,326.71,18.00,8.61;4,259.77,326.71,4.50,8.61;4,288.57,326.71,4.50,8.61;4,317.37,326.71,4.50,8.61;4,346.17,326.71,4.50,8.61;4,374.97,326.71,4.50,8.61;4,403.77,326.71,4.50,8.61;4,432.57,326.71,4.50,8.61;4,174.33,341.71,14.00,8.61;4,259.77,341.71,4.50,8.61;4,288.57,341.71,4.50,8.61;4,317.37,341.71,4.50,8.61;4,346.17,341.71,4.50,8.61;4,374.97,341.71,4.50,8.61;4,403.77,341.71,4.50,8.61;4,432.57,341.71,4.50,8.61;4,174.33,356.71,26.50,8.61;4,259.77,356.71,4.50,8.61;4,288.57,356.71,4.50,8.61;4,317.37,356.71,4.50,8.61;4,346.17,356.71,4.50,8.61;4,374.97,356.71,4.50,8.61;4,403.77,356.71,4.50,8.61;4,432.57,356.71,4.50,8.61;4,174.33,371.71,33.50,8.61;4,255.21,371.71,9.00,8.61;4,284.01,371.71,9.00,8.61;4,317.37,371.71,4.50,8.61;4,346.17,371.71,4.50,8.61;4,374.97,371.71,4.50,8.61;4,394.65,371.71,13.50,8.61;4,423.45,371.71,13.50,8.61;4,174.33,386.71,35.50,8.61;4,255.21,386.71,9.00,8.61;4,288.57,386.71,4.50,8.61;4,317.37,386.71,4.50,8.61;4,346.17,386.71,4.50,8.61;4,374.97,386.71,4.50,8.61;4,399.21,386.71,9.00,8.61;4,428.01,386.71,9.00,8.61;4,174.33,401.71,55.49,8.61;4,259.77,401.71,4.50,8.61;4,288.57,401.71,4.50,8.61;4,317.37,401.71,4.50,8.61;4,346.17,401.71,4.50,8.61;4,374.97,401.71,4.50,8.61;4,403.77,401.71,4.50,8.61;4,432.57,401.71,4.50,8.61;4,174.33,416.71,46.49,8.61;4,259.77,416.71,4.50,8.61;4,288.57,416.71,4.50,8.61;4,317.37,416.71,4.50,8.61;4,346.17,416.71,4.50,8.61;4,374.97,416.71,4.50,8.61;4,403.77,416.71,4.50,8.61;4,432.57,416.71,4.50,8.61;4,174.33,431.71,49.49,8.61;4,255.21,431.71,9.00,8.61;4,288.57,431.71,4.50,8.61;4,317.37,431.71,4.50,8.61;4,346.17,431.71,4.50,8.61;4,374.97,431.71,4.50,8.61;4,399.21,431.71,9.00,8.61;4,428.01,431.71,9.00,8.61;4,174.33,446.71,38.49,8.61;4,259.77,446.71,4.50,8.61;4,288.57,446.71,4.50,8.61;4,317.37,446.71,4.50,8.61;4,346.17,446.71,4.50,8.61;4,374.97,446.71,4.50,8.61;4,403.77,446.71,4.50,8.61;4,432.57,446.71,4.50,8.61;4,174.33,461.71,19.49,8.61;4,255.21,461.71,9.00,8.61;4,288.57,461.71,4.50,8.61;4,317.37,461.71,4.50,8.61;4,346.17,461.71,4.50,8.61;4,374.97,461.71,4.50,8.61;4,399.21,461.71,9.00,8.61;4,428.01,461.71,9.00,8.61;4,174.33,475.27,38.99,8.61;4,259.77,475.27,4.50,8.61;4,288.57,475.27,4.50,8.61;4,317.37,475.27,4.50,8.61;4,346.17,475.27,4.50,8.61;4,374.97,475.27,4.50,8.61;4,403.77,475.27,4.50,8.61;4,432.57,475.27,4.50,8.61;4,174.33,490.27,21.99,8.61;4,259.77,490.27,4.50,8.61;4,288.57,490.27,4.50,8.61;4,317.37,490.27,4.50,8.61;4,346.17,490.27,4.50,8.61;4,374.97,490.27,4.50,8.61;4,403.77,490.27,4.50,8.61;4,432.57,490.27,4.50,8.61;4,174.33,505.27,15.50,8.61;4,255.21,505.27,9.00,8.61;4,288.57,505.27,4.50,8.61;4,317.37,505.27,4.50,8.61;4,346.17,505.27,4.50,8.61;4,374.97,505.27,4.50,8.61;4,399.21,505.27,9.00,8.61;4,428.01,505.27,9.00,8.61"><head></head><label></label><figDesc>abbrev 2 0 0 0 0 2 2 airport_name 1 0 0 0 0 1 1 colour 3 0 1 0 0 2 3 company 3 0 0 0 0 3 3 currency 1 0 0 0 0 1 1 distance 17 3 2 0 0 18 20 gen_name 5 2 0 0 0 7 7 how_did_die 23 0 4 0 0 19 23 how_many3 39 6 4 0 0 41 45 how_much 6 1 1 0 0 6 7 length_of_time 3 0 0 0 0 3 3 name_part 2 0 0 0 0 2 2 nickname_state 1 1 0 0 0 2 2 population 2 1 0 0 0 3 3 sci_name 1 0 0 0 0 1 1 speed 4 1 0 0 0 5 5 temp 5 0 0 0 0 5 5 title 3 1 0 0 0 4 4 translat 3 0 1 0 0 2 3 unknown 51 97 6 1 0 141 148 what_city 16 0 2 0 1 13 16 what_continent 4 0 3 0 0 1 4 what_county 1 0 0 0 0 1 1 what_country 22 2 1 0 0 23 24 what_state 0 2 0 0 0 2 2 when 39 0 3 0 0 36 39 when_date 6 0 0 2 0 4 6 where 1 0 0 0 0 1 1 who 23 9 5 0 0 27 32</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,83.73,78.34,437.66,568.41"><head>Table 1 : Question Types used in the DLT system.</head><label>1</label><figDesc>The second column shows a sample question for each type. The third column shows sample answers. These are all of appropriate types for the question but are not necessarily correct. Fifteen question types handled by the system did were not used. They are anthem, atomic_number, atomic_weight, country_religion, element, planet, profession, state_motto, state_nickname (the opposite of nickname_state), what_airport_code, when_interval, when_month, when_week_day, when_year and where_airport.</figDesc><table coords="2,83.73,78.34,437.66,480.81"><row><cell cols="2">Question Type Example Question</cell><cell>Candidate Answer</cell></row><row><cell>abbrev</cell><cell>'What does ACLU stand for?'</cell><cell>American Civil Liberties Union</cell></row><row><cell>airport_name</cell><cell>'What is the name of the airport in Dallas Ft. Worth?'</cell><cell>Dallas Fort Worth International</cell></row><row><cell></cell><cell></cell><cell>Airport</cell></row><row><cell>colour</cell><cell>'What color is the top stripe on the U.S. flag?'</cell><cell>Red</cell></row><row><cell>company</cell><cell>What company manufactures Sinemet?'</cell><cell>Hangzhou MSD Pharmaceutical</cell></row><row><cell>currency</cell><cell>'What is the currency of Denmark?'</cell><cell>Kroner</cell></row><row><cell>distance</cell><cell>'How far is it from Earth to Mars?'</cell><cell>249 million miles</cell></row><row><cell>gen_name</cell><cell>'What is the name of the chart that tells you the</cell><cell>Periodic Table</cell></row><row><cell></cell><cell>symbol for all chemical elements?'</cell><cell></cell></row><row><cell>how_did_die</cell><cell>'How did Cleopatra die?'</cell><cell>asp bite</cell></row><row><cell>how_many3</cell><cell>'How many time zones are there in the world?'</cell><cell>24</cell></row><row><cell>how_much</cell><cell>'What percent of the nation''s cheese does Wisconsin</cell><cell>28 percent</cell></row><row><cell></cell><cell>produce?</cell><cell></cell></row><row><cell cols="2">length_of_time 'How long is a quarter in an NBA game?'</cell><cell>12 minutes</cell></row><row><cell>name_part</cell><cell>'What is Britney Spears'' middle name?'</cell><cell>Jean</cell></row><row><cell cols="2">nickname_state 'What is the Bluegrass state?'</cell><cell>Kentucky</cell></row><row><cell>population</cell><cell>'What is the population of Iceland?'</cell><cell>275000</cell></row><row><cell>sci_name</cell><cell>'What is the scientific name for red ants?'</cell><cell>Solenopsis invicta</cell></row><row><cell>speed</cell><cell>'How fast does light travel through space?'</cell><cell>186,000 miles per second</cell></row><row><cell></cell><cell></cell><cell>April 12th 1861</cell></row><row><cell>where</cell><cell>'Where is Mount Olympus?'</cell><cell>Greece</cell></row><row><cell>who</cell><cell>'Who created the literary character Phineas Fogg?'</cell><cell>Jules Verne</cell></row></table><note coords="2,104.97,369.86,19.92,9.53;2,158.13,369.86,94.26,9.53;2,395.49,369.86,123.10,9.53;2,107.13,384.86,15.50,9.53;2,158.13,384.86,195.23,9.53;2,428.85,384.86,56.17,9.53;2,100.29,399.86,29.32,9.53;2,158.13,399.86,200.30,9.53;2,448.65,399.86,16.59,9.53;2,96.57,414.86,37.07,9.53;2,158.13,414.86,190.79,9.53;2,441.93,414.86,29.88,9.53;2,95.49,429.86,39.28,9.53;2,158.13,429.86,125.80,9.53;2,438.81,429.86,36.51,9.53;2,84.33,444.86,294.67,9.53;2,444.21,444.86,25.44,9.53;2,89.37,459.86,51.45,9.53;2,158.13,459.86,168.95,9.53;2,444.93,459.86,23.78,9.53;2,87.69,474.86,54.77,9.53;2,158.13,474.86,199.09,9.53;2,445.05,474.86,23.79,9.53;2,93.69,489.86,42.60,9.53;2,158.13,489.86,174.46,9.53;2,442.53,489.86,28.76,9.53;2,104.25,504.86,21.57,9.53;2,158.13,504.86,163.24,9.53;2,446.85,504.86,19.92,9.53;2,93.45,519.62,43.15,9.53;2,158.13,519.62,167.56,9.53"><p>temp 'How hot is the sun?' two million degrees centigrade title 'What book did Rachel Carson write in 1962?' Silent Springs translat 'How do you say "cat" in the French language?' chat unknown 'What passage has the Ten Commandments?' Exodus what_city 'What city is Disneyland in?' Anaheim what_continent 'What continent is the world''s largest dessert on?' Africa what_county 'What county is San Antonio Texas in?' Bexar what_country 'What country is Aswan High Dam located in?' Egypt what_state 'What state was Amelia Earhart born in?' Kansas when 'When was "Cold Mountain" written?' 1997 when_date 'What date did the U.S. civil war start?'</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>Many thanks to <rs type="person">Ken Litkowski</rs> for providing answer patterns and supporting documents for the question collection.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The columns marked R, X, U and W show the numbers of answers judged Right, ineXact, Unsupported and Wrong by the NIST assessors. The overall rate of success was thus 33 out of 413 (8%) or 36 out of 413 (9%) including inexact answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>After carrying out a significant number of enhancements to our system relative to last year, the results are still no better. This is a disappointing result. There are several reasons for this. Firstly, the questions appear to be much harder this year. A significant number of the factoid questions could not even in principle be answered by our type of system based mainly on NEs. A selection of hard queries can be seen in Table <ref type="table" coords="7,135.09,153.97,4.14,10.57">3</ref>. Secondly, certain queries are conventional in form but the answer is not stated in a way which is easy to find. An example of this is numbers with unexpected spacing as in the answer to Q-1980 of '250 , 000 miles' . Another example is anaphoric reference as in Q-1939. Thirdly, we are missing a lot of categories for 'normal' forms of TREC query such as what things made of, animals, rock bands and pop groups, baseball teams, musical instruments, how often, how late etc.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,72.09,258.97,468.00,10.57;7,72.09,271.93,468.00,10.57;7,72.09,284.89,468.04,10.57;7,72.09,297.85,468.00,10.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,109.41,271.93,374.20,10.57">Statistical Selection of Exact Answers (MultiText Experiments for TREC 2002)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kemkes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Tilker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,245.37,284.98,294.76,10.36;7,72.09,297.94,23.70,10.36">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2003. November 19-22, 2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.09,310.81,419.76,10.57;7,72.09,336.73,173.22,10.57" xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Gaithersburg</surname></persName>
		</author>
		<ptr target="www.dtsearch.com" />
	</analytic>
	<monogr>
		<title level="j" coord="7,72.09,336.73,44.76,10.57">DTSearch</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Department of Commerce, National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,72.09,362.65,468.00,10.57;7,72.09,375.61,467.97,10.57;7,72.09,388.57,300.32,10.57" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,348.93,362.65,191.16,10.57;7,72.09,375.61,161.92,10.57">Is it the Right Answer? Exploiting Web Redundancy for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,243.45,375.70,296.61,10.36;7,72.09,388.66,115.97,10.36">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-06">2002. July 6-12, 2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
