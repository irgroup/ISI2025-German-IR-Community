<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,236.40,76.37,139.18,13.97;1,90.00,94.79,425.76,13.97">Question Answering By Pattern Matching, Web-Proofing, Semantic Form Proofing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,123.84,125.75,36.51,10.46"><forename type="first">Min</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department SUNY Albany</orgName>
								<orgName type="institution">ILS Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.11,125.75,66.44,10.46"><forename type="first">Xiaoyu</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department SUNY Albany</orgName>
								<orgName type="institution">ILS Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.18,125.75,68.84,10.46"><forename type="first">Michelle</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department SUNY Albany</orgName>
								<orgName type="institution">ILS Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.80,125.75,42.34,10.46"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department SUNY Albany</orgName>
								<orgName type="institution">ILS Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.48,125.75,99.66,10.46"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department SUNY Albany</orgName>
								<orgName type="institution">ILS Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,236.40,76.37,139.18,13.97;1,90.00,94.79,425.76,13.97">Question Answering By Pattern Matching, Web-Proofing, Semantic Form Proofing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54996932B896ACB142EED0BF0FB69159</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Categorization Question Category Subcategory Parsing Question Query Expansion Pattern Generation NP MTV VP started Query: query: #passage350(MTV</term>
					<term>#syn(start</term>
					<term>started</term>
					<term>starts</term>
					<term>starting)) Pattern: (&quot;started&quot; can be replaced by any of it&apos;s synonym) started[ ]+?MTV[ ]+?in[ ]+?&lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt; MTV[ ]+?started[ ]+?in[ ]+?&lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt; Started[ ]+?MTV[^&lt;&gt;]*?&lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date MTV[^&lt;&gt;]*?started[^&lt;&gt;]*?&lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt; started[^&lt;&gt;]*?&lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt;[^&lt;&gt;]*?MTV &lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt;[^&lt;&gt;]*?started[^&lt;&gt;]*?MTV &lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt;[^&lt;&gt;]*?MTV[^&lt;&gt;]*?started Matched Passage: MTV&apos;s parent company</term>
					<term>Viacom</term>
					<term>also owns Nickelodeon and VH1 MTV[ ]+?started[ ]+?in[ ]+?&lt;Date&gt;([^&lt;&gt;]+?)&lt;\/Date&gt;</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce the University at Albany's question answering system, ILQUA. It is developed on the following methods: pattern matching over annotated text, web-proofing and semantic form proofing. These methods are currently used in other QA systems, however, we revised them to work together in our QA system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This is our first time to participate in QA TREC with three runs and we focus on how to build a QA system in a short time (around half a year) with available methods and get a reasonable performance. Our system evaluation results show that it is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>The system components are: question analysis, document retrieval and annotation, pattern matching, web-proofing and semantic form proofing. See Figure <ref type="figure" coords="1,406.33,447.71,4.50,10.46">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Analysis</head><p>There are two main tasks in question analysis: question categorization and query expansion.</p><p>We classify questions according to their answer target. The answer targets of TREC questions usually fall into several categories such as person, location, date, quantity, manner, works, organization and so on. These categories also have subcategories. For example, location contains the subcategories nation, city, mountain, lake, river, etc. So our question is categorized according to the main category and subcategory. It is easy to classify "who", "where" and "when" questions. For "what" and "how" questions, we need the help of dictionary tools like WordNet to identify what is asked. This type is usually identified by the noun or adjective following the question word, e.g., "What country…" or "How long …", etc.</p><p>Once a question falls into some category, the possible answer patterns can be retrieved from our pattern library. These patterns are general to the category and replacing keywords is necessary to get the specific patterns to the question. However, for some definition questions such as "What is golden parachute", it is difficult to find the answer target. We classify these questions as "No-Pattern" questions.</p><p>The first step in the answer finding process is information retrieval (IR), which fetches a number of "relevant" documents from the database. Query modification is usually necessary to increase the recall of IR. It involves both deleting and adding terms to the initial query, which is obtained from the user question through the usual stemming and stopping process. For some questions, we delete some common terms that are not helpful in retrieval. For example, in the question such as "What country is Aswan High Dam located in?", the term "country" is not of much use in retrieving relevant documents. So it is necessary to delete it from our query in order to increase the recall. However, for some nouns and verbs, it may be helpful to get their synonyms and related forms, for example, it is often necessary to find noun and adjective forms for verbs. These keywords and their expansions are kept for future use. WordNet and other dictionaries are used to finish these tasks. Finally, the query is constructed as a Boolean formula that can be processed by the IR component, in our case the UMass' Inquery system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document Retrieval &amp; Annotation</head><p>The IR system we use to pre-fetch documents is the Inquery system developed at Univ. of Mass. at Amherst. ILQUA selects the top 50 documents from the file list returned by Inquery and passes those documents to the annotation component.</p><p>The annotation tool we use is BBN's Identifinder system. It annotates documents according to the answer target of the question. The annotated entity types include ANIMAL, DISEASE, FAC, GAME, EVENT, GPE, LANGUAGE, LAW, LOCATION, NATIONALITY, PERSON, PLANT, PRODUCT, SUBSTANCE, WORK_OF_ART, CARDINAL, MONEY, ORDINAL, PERCENT, QUANTITY, DATE and TIME. We use most of these types in our system.</p><p>The annotated documents are filtered according to the keywords. We cut documents into passages and keep those passages that contain the annotated answer target and keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pattern Matching</head><p>Our pattern matching component consists of two parts, fixed pattern matching and partial pattern matching. For questions with a simple answer pattern, the answer candidates can be found by fixed pattern matching. As for those with complex answer patterns, we try to locate answer candidates via partial pattern matching.</p><p>Fixed pattern matching scans each passage and does pattern matching. Patterns are organized in a list according to their scores. Once a pattern is matched, the answer is extracted from the text and put into the primary answer list along with the score of the pattern which has been used to extract it. After all of the passages are processed, the system merges what appear to be the same answer in the answer list and calculates a final score for each distinct answer. Figure <ref type="figure" coords="4,286.34,116.57,6.00,10.46">2</ref> shows an example of how this fixed pattern matching works. It is very effective for questions such as "When was Adolf Hitler born?", "What does ACLU stand for?", "When was the first camera invented", etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Example of Fixed Pattern Matching</head><p>The idea of partial pattern matching is based on the assumption that the answer is usually surrounded by keywords and their synonyms. Let E k (1 ≤ k ≤ m) denote the kth named entity in the annotated passage, T i denotes the ith query keyword (1</p><formula xml:id="formula_0" coords="4,416.64,537.47,105.28,11.10">≤ i ≤ n), W i (0 ≤ i ≤ 1)</formula><p>denotes the weight of T i and D i denotes the distance from T i to E k . If T i or its synonyms occur in the passage, D i is equal to the count of words between T i and E k , otherwise, D i is equal to the maximum average length of the passages Max. The matching score of E k is:</p><formula xml:id="formula_1" coords="4,199.98,604.70,148.55,48.35">Max n Max W D S i n i i k 1 * 1       × - = ∑ =</formula><p>If the matching score, S k , is above a threshold, the named entity E k is extracted and added to the secondary answer list. Finally, the secondary answer list is merged and sorted. Figure3 and Figure4 show a simple example of how partial matching works.</p><p>ILQUA selects the top 5 answer candidates from the two answer lists and passes them to the web-proofing. It is important that pattern matching gets the correct answer included in the top 5 ranked answers because the final selection is based on relative likelihood among the 5 candidates.</p><p>For definition questions, it is difficult to decide their answer targets and answer patterns in advance. Instead, after the relevant passages are processed, sentences containing a possible answer are passed to the semantic component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Web-Proofing</head><p>Web-proofing is used to select an answer from the answer candidate list. As the most widely used search engine, Google is chosen to search the Web. The query is submitted to Google and the number of occurrences of each answer candidate is calculated. The assumption here is that the correct answer will have more occurrences in the list returned Question: What band did the music for the 1970's film "Saturday Night Fever" Major Target: Organization Minor Target: Band Question noun tokens: band music 1970 film Saturday Night Fever Answer: First to appear will be ``Saturday Night Fever,'' already boasting a $14 million advance and the benefit of a surefire title and score audiences know from the 1977 film that featured John Travolta and music by the Bee Gees. Score: 0.5612</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question: Who created the literary character Phineas Fogg</head><p>Major Target: Person Question verbs: create created creates creating Question verb nouns: creator Question verb synonyms: make made makes making produce produced produces producing Question noun tokens: literary character Phineas Fogg Answer: Jules Verne's Phileas Fogg made literary history when he traveled ``around the world in 80 days'' in 1873. Score: 0.5017 by Google than other answer candidates. This simple method works for factoid questions. For list questions, if any of the answer candidates occur in the top documents returned by Google, we will add it to the final answer list. This web-proofing method needs to be improved to deal properly with cases where the correct answer is not among the candidates being proofed or when a slight nuance in the question requires us to find a less popular answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Semantic Form Proofing</head><p>In our system, the semantic form proofing is only applied to definition questions. We try to find the answer from the sentence list returned (without a match) by the pattern matching step. The system then builds semantic representation for both the question and the selected sentences. The semantic representation is in the form of an acyclic directed graph where each link is assigned a weight. The system then attempts to match the semantic form of the question to the semantic forms of the candidate sentences. For each matching, it assigns a score. The answer is chosen from the highest scoring sentence.</p><p>To build up semantic forms, the head driven parser from M. Collins is adopted. The parser provides the head of phase information for each sentence and semantic form (or say semantic tree) is built up based on this phase head information. Figure <ref type="figure" coords="6,466.03,337.37,6.00,10.46">5</ref> shows an example. The weighted semantic form is built upon form. As we can see from the example that semantic form is an acyclic directed graph and each word is assigned a value to indicate its importance. Since the head of a phase is likely to be more important than other words in the phase, its value is its in-degree plus one (to avoid 0-value links).</p><p>In Figure <ref type="figure" coords="7,137.36,130.37,4.50,10.46">6</ref>, the value of "Tomba" is 2. Also a weight is assigned to each link. The value of a link is the sum of the two nodes' weights in that link. In figure <ref type="figure" coords="7,414.30,144.17,4.50,10.46">6</ref>, the weight of link between "Who" and "Tomba" is 4. Similarly, the weighted semantic forms of sentences containing answer candidate are built. However, for some important links, their weight is doubled. In Figure <ref type="figure" coords="7,459.17,427.85,4.50,10.46" target="#fig_2">7</ref>, the weight of the link between "Champion" and "Tomba" is doubled. After the assignment of weights to each link, we compare links in sentences containing answer candidate with the links in the question. Once matched links are found, the sum of weights of matched links is the matching score of the answer candidate. In Figure <ref type="figure" coords="7,373.01,483.05,4.50,10.46" target="#fig_2">7</ref>, the matching score of word "champion" is 8+3=11. If the score is above some threshold, the answer candidate is extracted and put in the answer list. The answer candidate with the highest matching score will be chosen as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head><p>We submitted three runs for evaluation. The evaluation results are shown in Table <ref type="table" coords="7,487.21,581.93,4.50,10.46" target="#tab_0">1</ref>.</p><p>We try to assign different weights to keywords in each run. In run AlbanyI2, all of the keywords receive the same weight. In runs AlbanyI3 and AlbanyI4, keywords containing digits and proper nouns receive smaller weights. The evaluation results show that the weighting method is more useful to list questions because the accuracy of list questions in Albany03I4 is better than Albany03I2. However, for factoid questions, the weighting method produces a reduction in performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion &amp; Work</head><p>Since ILQUA has been in a short period of time as a semester project for a small group of CS students, some of our ideas and implementations are not yet mature. The system needs to be improved in many ways, and we hope this will happen by the next year TREC meeting.</p><p>Actually, from the analysis of our experiment results, pattern matching can retrieve the correct answer in the top 5 answer candidates for nearly 45 percent of the factoid questions, and in the top 10 answer candidates for 50 percent of factoid questions. Some techniques need to be applied to increase the rank of correct answers while decreasing the rank of incorrect answers. For partial matching, different weighting methods should be applied to different types of questions.</p><p>A more robust and efficient web-proofing method is necessary in our future development, especially a component to deal adequately with junk web content. It is also very difficult for semantic form proofing to handle definition questions whose answer needs to be inferred from the context. Improving ILQUA in this aspect is also in our future plan.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,234.00,399.03,165.03,8.74"><head>Figure</head><label></label><figDesc>Figure 3. Example of Partial Matching</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,243.00,518.13,106.71,8.74"><head>Figure</head><label></label><figDesc>Figure 5. Semantic Form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,234.00,374.49,141.92,8.74"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Answer Semantic Form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,234.00,166.53,113.39,8.74"><head>Table 1 . Evaluation Result</head><label>1</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgements</head><p>Thanks to all the other students in our CSI660(Spring 2003) class. Especially to <rs type="person">Zhenyu Dai</rs> who help us set up Inquery system.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,95.24,533.35,68.53,12.19" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Reference</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,90.00,548.19,431.97,8.74;8,90.00,559.65,415.78,8.74;8,90.00,571.17,432.01,8.74;8,90.00,582.69,360.51,8.74;8,90.00,594.15,432.12,8.74;8,90.00,605.67,31.95,8.74;8,90.00,617.19,337.56,8.74;8,90.00,628.65,432.11,8.74;8,90.00,640.17,386.61,8.74;8,90.00,651.69,432.08,8.74;8,90.00,663.15,239.61,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,406.32,548.19,115.65,8.74;8,90.00,559.65,80.34,8.74;8,320.22,559.65,185.56,8.74">Question Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czuba ; Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergei</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec9/t9_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="8,433.31,571.17,88.70,8.74;8,90.00,582.69,148.49,8.74;8,242.66,594.15,279.46,8.74;8,90.00,605.67,27.39,8.74;8,277.46,628.65,244.65,8.74;8,90.00,640.17,165.38,8.74;8,254.00,651.69,268.08,8.74;8,90.00,663.15,30.44,8.74">TREC-10 Proceedings. 2002. Patterns of Potential Answer Expressions as Clues to the Right Answers</title>
		<editor>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Dan Moldovan</publisher>
			<date type="published" when="2000">2004. 2000. 2000</date>
		</imprint>
	</monogr>
	<note>TREC-9 Proceedings. FALCON: Boosting Knowledge for Answer Engines</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
