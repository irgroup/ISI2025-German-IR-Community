<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.52,80.91,360.82,14.82;1,147.48,96.87,316.57,14.82">The JAVELIN Question-Answering System at TREC 2003: A Multi-Strategy Approach with Dynamic Planning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.76,127.24,48.72,15.55"><forename type="first">E</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.29,127.24,62.18,15.55"><forename type="first">T</forename><surname>Mitamura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.50,127.24,43.38,15.55"><forename type="first">J</forename><surname>Callan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.03,127.24,60.39,15.55"><forename type="first">J</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,392.83,127.24,69.44,15.55"><forename type="first">R</forename><surname>Frederking</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,99.48,141.16,106.74,15.55"><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.94,141.16,74.60,15.55"><forename type="first">L</forename><surname>Hiyakumoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.49,141.16,44.80,15.55"><forename type="first">Y</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.55,141.16,78.01,15.55"><forename type="first">C</forename><surname>Huttenhower</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,439.58,141.16,34.55,15.55"><forename type="first">S</forename><surname>Judy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,482.49,141.16,23.80,15.55"><forename type="first">J</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.32,155.08,44.12,15.55"><forename type="first">A</forename><surname>Kupść</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.34,155.08,46.09,15.55"><forename type="first">L</forename><forename type="middle">V</forename><surname>Lita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.27,155.08,40.41,15.55"><forename type="first">V</forename><surname>Pedro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.17,155.08,58.02,15.55"><forename type="first">D</forename><surname>Svoboda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.50,155.08,72.78,15.55"><forename type="first">B</forename><surname>Van Durme</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.52,80.91,360.82,14.82;1,147.48,96.87,316.57,14.82">The JAVELIN Question-Answering System at TREC 2003: A Multi-Strategy Approach with Dynamic Planning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F86834E592EC7847541201B46B6E197</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The JAVELIN system evaluated at TREC 2003 is an integrated architecture for open-domain question answering. JAVELIN employs a modular approach that addresses individual aspects of the QA task in an abstract manner. The System implements a planner that controls the execution and information flow, as well as a multiple answer seeking strategies used differently depending on the type of question.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">System Overview</head><p>JAVELIN is an object-oriented architecture that separates the processing details of individual operations (e.g. taggers, parsers) from the contexts in which they are used. JAVELIN is a flexible and extensible platform which supports component-level evaluation, so that different strategies can be tested individually and then integrated into the system in a straightforward manner.</p><p>The current system (Figure <ref type="figure" coords="1,195.60,507.51,4.19,12.01" target="#fig_0">1</ref>) brings together a large number of modular components that perform various question-answering tasks, such as: question analysis, document and passage retrieval, answer candidate extraction, answer selection, answer justification, and planning. The Planner module uses abstract interface definitions for each component to select and order the execution of individual components, allowing the system to dynamically generate multiple processing strategies and replan when necessary.</p><p>The details of execution are handled by the Execution Manager, a component which uses a Data Repository for storing session data (process steps, intermediate and final results). JAVELIN incorporates a user interaction component (GUI) for question input and user clarification. The system also includes an Answer Justification module, which provides a browsable view of the process history, intermediate data structures and final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Question Analysis</head><p>The Question Analyzer (QA) produces a programmatic representation of each input question for use by the rest of the JAVELIN system. This representation (referred to as a Request Object or RO) contains three major features: the answer type, keyword sets, and a logical representation of the question. Although many systems generate the first two features through surface analysis of the input question, JAVELIN also attempts to apply natural language processing to obtain a logical representation of the question. The logical representation attempts to capture the semantics (meaning) of the question; a similar representation has been reported for question node templates in <ref type="bibr" coords="1,323.39,415.95,93.21,12.00" target="#b5">(Harabagiu et al., 2000)</ref>.</p><p>In the QA module, natural language processing occurs in two steps: lexical and syntactic parsing. Since the inputs to be analyzed are open-domain questions, most specific dictionaries provide inadequate coverage. Instead, we use several external tools designed for open-domain text:</p><p>• Brill tagger <ref type="bibr" coords="1,381.07,505.59,49.98,12.01" target="#b1">(Brill, 1995)</ref> for part-of-speech tagging;</p><p>• BBN IdentiFinder <ref type="bibr" coords="1,407.44,524.19,74.66,12.00" target="#b0">(Bikel et al., 1999)</ref> for named entity tagging (people, organizations, locations, etc.);</p><p>• WordNet <ref type="bibr" coords="1,371.83,554.79,69.39,12.00" target="#b4">(Fellbaum, 1998)</ref> for use in finding hypernym relationships for semantic categorization and for extracting the root form of a word;</p><p>• KANTOO Lexifier <ref type="bibr" coords="1,414.64,597.27,125.36,12.00" target="#b8">(Nyberg and Mitamura, 2000)</ref> for finding verb valencies.</p><p>The QA module uses the KANTOO parser <ref type="bibr" coords="1,506.81,627.15,33.31,12.00;1,313.20,639.03,87.73,12.00" target="#b8">(Nyberg and Mitamura, 2000)</ref> and hand-built grammar for syntactic analysis. The module extract semantic information produced by KANTOO's lexical processing rules, and specific rule-based patterns are inserted as needed. Specialized grammar rules are used on a per-answertype basis to recognize syntactic and semantic similarities among questions. The grammars of fundamental  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Retrieval Strategist</head><p>The main purpose of the Retrieval Strategist (RS) module is to retrieve likely answer documents from the document repository in response to a query. The RS module also acts as a document server for other parts of JAVELIN which trigger lookup requests for specific documents or passages. The RS module operates on the Request Object produced by the Question Analyzer. The salient subparts of the Request Object are a) the set of keywords produced for the question, and b) a set of constraints associated with the retrieval process.</p><p>The keywords are words or phrases which the QA module has deemed likely to be present in the answer. Each keyword is one of three types: a) a single word; b) a short phrase, e.g. 'electoral college'; c) a proper name, e.g. 'Elvis Presley'. No query expansion is currently performed by the RS module on the keywords. However, the QA module may specify a set of alternates for any given keyword, which are treated as synonyms for retrieval.</p><p>The set of constraints present in the Request Object includes:</p><p>• upper and lower limits on the number of ranked documents to be retrieved;</p><p>• the likely answer type;</p><p>• one or more subtypes for the given answer type;</p><p>• the total time allowed for processing.</p><p>Since TREC 2002, we have switched from using the proprietary Inquery retrieval system to the Lemur 2.0 toolkit <ref type="bibr" coords="2,341.61,399.99,104.48,12.00" target="#b9">(Oglivie and Callan, 2002)</ref>. Lemur is open-source and supports a variety of retrieval models, including support for Inquery-style structured queries.</p><p>Stemming is done at indexing time using the Lemur Porter stemmer. Before indexing, our source documents are preprocessed with the BBN Identifinder (v1.7) named entity tagger <ref type="bibr" coords="2,368.22,471.63,78.99,12.00" target="#b0">(Bikel et al., 1999)</ref> to identify named entities such as 'Organization', 'Time', 'Date', 'Person', 'Place Name', 'Currency Amount', 'Number', 'Percentage', and object types such as 'Animal', 'Plant', 'Product', 'Game', etc. This analysis attempts to focus retrieval on documents containing not only the relevant keywords, but also relevant data types. At indexing time, any terms within a span of text identified as a named entity are stored in the index using a set of corresponding special fields.</p><p>To process a query at runtime, if a likely answer type is specified, it is mapped to a set of named entity types. For example, an answer expected to be of 'temporal' type might map to either a 'Time' or 'Date' named entity field. These named entity fields are treated as special 'keywords' to be included in the terms passed to Lemur.</p><p>The current search algorithm is similar to the algorithm used in TREC 2002, and proceeds using an incremental query relaxation technique; starting from an initial query that is highly constrained, the algorithm searches for all the keyword terms and data types in close proximity to each other. However, last year's algorithm <ref type="bibr" coords="3,243.87,72.75,54.90,12.00">(TREC 2002)</ref> always included all keywords at every relaxation step, while this year's algorithm attempts to be more flexible by assigning a priority to each keyword. This priority is based on a function of the likely answer type, keyword type (word, proper name, or phrase) and the inverse document frequency (idf) value of the keyword term(s). Keywords deemed more likely to exist near the answer are given higher priority. At each iteration, a priority threshold is adjusted which may result in lower-priority keywords being discarded from the query.</p><p>As before, at each relaxation step, the algorithm also relaxes other parameters in the query such as the word proximity window. This assumes that more likely answer documents will have clusters of relevant question keywords and data types in closer proximity. Another new feature of this year's algorithm is the use of a hybrid IR model in which earlier, more constrained query steps use the structured query retrieval model, while later steps switch to a tf.idf model. The algorithm terminates once the requested document list is full, or there are no more relaxation steps possible. The complete set of relaxation parameters includes:</p><p>1. The Inquery-style proximity/belief operator used to combine keywords. At each relaxation step, we either keep the same operator but expand the window size, or start with a new, more general operator. The operator applies to all keywords given in the query. For example, initially all keywords must be found within a proximity of three words. We then relax the operator to consider unordered 20-, 100-, and 250word windows, followed by document-wide probabilistic AND, and so on.</p><p>2. Phrase proximity, for any phrase keywords. This is usually kept at 3 words or less, until later in the relaxation regime, when the window is slightly expanded.</p><p>3. Proper name proximity, for any proper name keywords. Like phrase keywords, this is usually kept at 3 words or less until very late in the relaxation regime, when the window is slightly expanded.</p><p>4. The inclusion or exclusion of the special named entity 'keywords' corresponding to the answer type. This alternates between 'on' and 'off' at every relaxation step.</p><p>5. The keyword priority threshold, which starts low and is slowly increased with further steps.</p><p>The iterative relaxation technique may be considered as an implicit scoring strategy in which the relevance of a document relative to the question is inversely proportional to its window span size, and directly proportional to the tf.idf sum of keywords appearing in the window. The final RS module output is a ranked list of document IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Information Extractor</head><p>The Information Extractor (IX) module extracts candidate answers from relevant documents retrieved by the RS module. JAVELIN implements a multi-strategy approach to answer extraction <ref type="bibr" coords="3,426.70,169.95,77.99,12.00" target="#b3">(Czuba et al., 2003)</ref>, and includes several different implementations of the IX module. The assumption is that optimal performance in extracting different answer types may require a number of separate strategies, ranging from simple finite state transducers to classifiers trained to separate correct answers from incorrect answers. Each information extraction method scores candidate answers (with their corresponding passages) and passes them down the pipeline to the Answer Generator for canonicalization and clustering. Scoring functions differ among the IX modules, and their scores are not normalized individually.</p><p>The first and most straightforward extraction approach (Light IX) implements a non-linear weighted proximity metric that spans a passage of three sentences. This approach identifies candidate answers of the appropriate type and assigns a score based on proximity to question terms and their synonyms. This IX module implements specific strategies for definition questions, relationship questions, and person biography questions.</p><p>Another strategy is to combine statistical features emerging from a passage and an answer, and train a classifier to separate correct answers from incorrect ones for each specific answer type. The classifiers were trained on Trec9 and Trec10 datasets, and include a support vector machine (SVM IX) classifier as well as a k-nearest neighbors (KNN IX) classifier. The positive versus negative data ratio was tuned for best performance.</p><p>The final strategy is based on traditional information extraction and implements a finite state transducer (FST IX). This approach is modeled after each question type/relationship that the Question Analyzer identifies. The FST IX is based on lexical features, Wordnet features, and surface form flags. It is most appropriate for question types where answers can be extracted using a handful of simple patterns <ref type="bibr" coords="3,439.17,589.35,100.77,12.00;3,313.20,601.35,23.48,12.00" target="#b10">(Ravichandran and Hovy, 2002)</ref> <ref type="foot" coords="3,336.72,602.25,3.48,6.67" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Answer Generator</head><p>The Answer Generator (AG) module is responsible for producing a ranked list of answers from the set of answer candidates produced by the IX modules. The AG also makes use of the Request Object produced by the QA module. The AG performs several normalization functions, such as combining similar answer candidates and applying answer type checking to filter out inappropriate answers.</p><p>Type checking is very important when selecting proper answers from a candidate list. Earlier modules can sometimes produce irrelevant answers, leading to candidates which do not match the question. For example, for the question, "What continent is Egypt on?" the AG might receive the candidates, "Middle East", "Arab", "Islam", and "Africa". The answer type is location (continent), so the AG should select "Africa" as the final answer.</p><p>Currently, the AG uses WordNet, the Tipster Gazetteer (TIPSTER, 1992), the Web, and type-specific patterns to support answer type checking. WordNet provides hypernym and meronym relationship information used by the AG to determine the relationships between candidate answers and the target answer type <ref type="bibr" coords="4,211.89,288.75,82.68,12.00" target="#b2">(Cardie et al., 2000)</ref>. The Web is also used as a resource to see how strongly these relationships are supported: the AG creates validation patterns from the answer type and answer candidates, sends a request to the Web, and generates a score from the number of retrieved documents <ref type="bibr" coords="4,204.44,348.51,90.11,12.00" target="#b7">(Magnini et al., 2002)</ref>. For location questions, the AG examines both WordNet and the Gazetteer; the latter provides extensive information on various city, state, and country names.</p><p>When no adequate answer can be found, the AG communicates this information to the Planner. This allows a new strategy to be applied to the question, potentially resulting in a correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Execution Manager and Repository</head><p>The Execution Manager (EM) module coordinates communication between the Planner and all other modules. Additionally, it communicates with the Repository, a centralized SQL database containing all of the information and analysis generated by JAVELIN during question answering. Aside from creating a complete history of system behavior, this allows analysis data to be collected in a manner such that the Answer Justification module can easily create justification results for display to the user. The EM's runtime coordination and communication with the Repository provide fundamental support for JAVELIN's object-oriented, modular architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">Planner</head><p>The Planner module is responsible for controlling the question-answering process. It selects and issues sequences of module calls to maximize the expected utility of the information JAVELIN produces, taking into account the current information state and system resources. This increases the system's flexibility to generate different strategies at run-time, exploit different modules' strengths for specific question types, and more robustly handle module failures as they arise.</p><p>Figure <ref type="figure" coords="4,353.13,97.59,5.03,12.01" target="#fig_1">2</ref> presents a high-level view of the Planner's architecture. The Planner operates as a service for the JAVELIN GUI, and communicates with the rest of the system via the Execution Manager.<ref type="foot" coords="4,458.28,134.37,3.48,6.67" target="#foot_2">2</ref> Upon receiving a new question, the Planner calls the Question Analyzer (via the EM) and uses the resulting question analysis to generate a planning problem describing the initial state (features of the question analysis), and an information goal based on the expected answer type.  The planning and execution process is based on a forward-chaining utility-based algorithm that performs a best-first search across the set of possible information states <ref type="bibr" coords="4,338.62,556.71,129.04,12.00" target="#b6">(Hiyakumoto and Veloso, 2002)</ref>. Beginning with the initial state representation and an empty plan, the Planner evaluates all actions applicable in the current state, selecting the one whose projected outcome states have the highest expected utility. Expected utility is estimated using a weighted combination of the states' information quality metrics and likelihoods. The internal planning state is then projected forward to reflect the pos-sible outcomes of the chosen action, and the selection process repeats. At each step, the algorithm also decides between executing the first unexecuted action in the plan and continuing to plan with the uncertainty of the projected states. If a module call is executed (e.g., documents are retrieved), the results are used to update the internal information state model and a replanning decision is made. The process continues until the goal is satisfied (the system has found an answer with the correct answer type and an expected utility exceeding a predefined threshold), or available resources have been exhausted. Upon terminating, the Planner returns the system's answer or a failure message.</p><p>Critical to this decision-making process is the Planner's domain model of the QA process, which is defined in terms of types, literals, metrics and operators. Table 1 highlights key features of the QA domain model used by the Planner for TREC12. Types define the categories of objects created and transformed by the process, including the system's question and answer type hierarchy (as subtypes of qtype and atype respectively) and intermediate data objects (e.g., a docset). Literals define information features (e.g., the number of question keywords extracted during question analysis), data relationships (e.g., the relationship between a set of answer candidates and the documents used to create them), and system status (e.g., module availability). Metrics represent system resources (e.g., system time) and quality estimates (e.g., docset quality) for the information objects present in the current state, and are used in the calculation of a state's utility. Together, metrics and literals comprise the Planner's internal state representation. Operators (actions) define the QA processing functions that the Planner can control, described in terms of their preconditions (literal and metric conditions that must be true in the current state in order for the action to be applicable), and a set of probabilistic effects (possible changes to the information state that may occur upon execution).</p><p>The domain operators implemented for TREC 12 essentially provide a one-to-one mapping to the QA system components, with the exception of check answers, which triggers an internal sanity check before the Planner terminates (to verify the answer confidence is nonzero). Obviously, these operators are not the only ones we could have defined: we could have chosen to give the Planner finer-grained control of the system (e.g., defining multiple retrieval operators specifying different retrieval methods) assuming we identify state features that reliably indicate appropriate contexts for each operator. However, given the time constraints, we decided to focus solely on the extraction performance. Thus, the effective role of the Planner in TREC12 was to dynamically select amongst the four extraction components.</p><p>The IX selection strategy implemented for factoid  <ref type="table" coords="5,473.93,519.15,4.19,12.01">2</ref>) attempt to exploit differences in their precision and recall to maximize their combined question coverage. Preference is generally given to higher-precision modules that may fail to return an answer more often, with the less-accurate, higherrecall modules used as a fallback method. Two examples of action sequences produced by the Planner are presented in Figure <ref type="figure" coords="5,379.85,602.79,5.03,12.01">3</ref> illustrating the use of this strategy.</p><p>Both definition and list questions were handled with a single fixed strategy by enabling only the extraction operator for the IX module we estimated was "best" for each (namely the Light and KNN modules, respectively). This decision was made because only the Light extractor was capable of generating longer answers, and the KNN extractor had the highest recall of unique answers when evaluated on past TREC list questions. Table <ref type="table" coords="6,96.93,387.39,3.90,12.01">2</ref>: Extraction module preference ordering for factoid questions. Each extractor is called in succession until the system generates an answer or all options have been exhausted. No ordering strategy was used for definition and list questions: in these cases the Planner just invoked the single "best" extractor for each (Light and KNN respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What movie won the Academy Award for best picture in 1989?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Results</head><p>In the main (factoid) task, JAVELIN answered correctly 55 questions, 3 inexact, and 3 unsupported, for an accuracy of 0.133. The system achieved the highest accuracy on location questions: 0.3125 (Table <ref type="table" coords="6,253.70,547.23,3.63,12.01" target="#tab_4">3</ref>). For the 50 definition questions in TREC12, the JAVELIN system achieved an average F score of 2.16. JAVELIN obtained a 0.052 score on list questions (Table <ref type="table" coords="6,222.06,583.11,3.63,12.01">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis</head><p>Table <ref type="table" coords="6,96.21,626.79,5.03,12.01">5</ref> shows the F scores of the list questions according to the answer type. As can be seen in Table <ref type="table" coords="6,245.31,638.67,3.77,12.01">6</ref>, 32% of the questions were misclassified.</p><p>The Question Analyzer's accuracy in assigning answer types has been analyzed using the TREC 9, 10, and 11 corpora. Since grammar development for analysis is based on the TREC 9 and 10 corpora, we have separated our results and provide TREC 11 as an example proved a full 23% because of the additional information available from the Gazetteer. We tested the system with the Planner's IX-ordering strategy on 500 additional questions from TREC 11. As the results in Table <ref type="table" coords="8,154.36,366.75,10.06,12.01" target="#tab_9">11</ref> show, the system's performance with the Planner (PL) was not appreciably better than it was using a fixed sequence with the best single SVM extractor (79 versus 77 questions correctly answered). We attribute this to our use of a relatively small data set for estimating the extractor performance (1193 questions from TREC 9 and 10; particularly problematic for the less-common answer types), coupled with the fact that the questions used to estimate the planning parameters were the same ones used to develop the extractors themselves, leading to possible overestimates of their performance.</p><p>Even if our ordering strategy had perfectly fit the question set (PL+), the Planner would have enabled the system to correctly answer just 8 more questions than the single-extractor fixed sequence version could answer. Thus, although a good operator model is likely to include answer type information, that by itself is not sufficient to correctly determine the context for applying each extraction method. To achieve the best possible performance from the current QA components (PL*) additional predictive features must be identified and incorporated into the Planner's model of the QA domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Current and Future Work</head><p>Our primary focus after TREC 2003 and for the future is on more complex relationship and scenario-based questions. For example, a human interacting with JAVELIN may wish to collect data on a terrorist organization incre-  mentally for use with subsequent question answering. To facilitate this expansion, each module is focusing on enhancements necessary to process more complex question types and content. The QA module is enhancing the depth and accuracy of its language processing output, in addition to creating and improving analysis for scenario-based question types.</p><p>A new NLP-based IX is being developed. This IX is based solely on linguistic analysis of the question and answer data, unifying syntactic and semantic forms of the question against similar analyses of candidate answer passages.</p><p>A Text Processing module is being added to the system to facilitate much of the newly-required natural language processing. This module centralizes textual analysis, providing much of the tagging, hypernym/meronym, syntactic, and semantic processing used by other JAVELIN modules.</p><p>A Linguistic Reasoner module is being created to serve as a more linguistically aware sub-planner. For complex questions not answerable by a "straight shot" through the existing JAVELIN system (e.g. questions containing an implied subquestion), the LR uses a dependency-based mechanism to find missing information and recursively communicate this need to the main JAVELIN Planner. This allows both more sophisticated linguistic reasoning and more intricate use of JAVELIN itself to answer com-plex questions.</p><p>All of these enhancements are being developed to improve the JAVELIN system's capability to answer complex questions dependent on semantics or context. The results from our participation in the first NIST relationship QA pilot evaluation indicate that approaches which are currently common for factoid-style questions are not applicable to relationship and scenario questions. We hope to apply our flexible architecture and linguistic knowledge to areas of question answering which require deeper reasoning about the text. It will be useful to see how these linguistic techniques perform for factoid questions, but we believe that their primary strength will be in addressing other, more complex types of questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,300.75,467.59,12.00;2,72.00,312.63,467.68,12.00;2,72.00,324.63,198.36,12.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The JAVELIN architecture. The Planner operates as a service for the user interface and controls execution of the individual components via the Execution Manager. The Planner selects from a set of 5 different extraction modules according to its domain model of the QA process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,313.20,447.15,226.57,12.00;4,313.20,459.15,226.88,12.00;4,313.20,471.03,226.65,12.00;4,313.20,483.03,226.62,12.00;4,313.20,495.03,73.22,12.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relationship of the Planner to the JAVELIN QA system. Within the Planner module, a server component provides the QA domain-specific functionality, while the Planner component provides domain-independent planning functionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,313.20,73.18,229.11,457.98"><head>Table 1 :</head><label>1</label><figDesc>question, qtype, atype, docset, fillset, anslist, ix-name subtypes of qtype: entity, activity,... subtypes of atype: temporal, location,... Partial listing of the TREC12 QA domain model.</figDesc><table coords="5,313.20,109.18,229.11,421.98"><row><cell>literals:</cell></row><row><cell>(interactive session)</cell></row><row><cell>(satisfies &lt;question&gt; &lt;atype&gt; &lt;anslist&gt;)</cell></row><row><cell>(request &lt;question&gt; &lt;qtype&gt;)</cell></row><row><cell>(retrieved docs &lt;docset&gt; &lt;qtype&gt;)</cell></row><row><cell>(no docs found &lt;qtype&gt;)</cell></row><row><cell>(no more docs &lt;qtype&gt;)</cell></row><row><cell>(candidate fills &lt;fillset&gt; &lt;qtype&gt; &lt;docset&gt; &lt;ix-name&gt;)</cell></row><row><cell>(no fills found &lt;qtype&gt; &lt;docset&gt; &lt;ix-name&gt;)</cell></row><row><cell>(ranked answers &lt;anslist&gt; &lt;qtype&gt; &lt;fillset&gt;)</cell></row><row><cell>(no answers &lt;qtype&gt; &lt;fillset&gt;)</cell></row><row><cell>(displayed &lt;anslist&gt;)</cell></row><row><cell>metrics:</cell></row><row><cell>system time</cell></row><row><cell>request quality</cell></row><row><cell>docset quality</cell></row><row><cell>fillset quality</cell></row><row><cell>answer quality</cell></row><row><cell>operators:</cell></row><row><cell>retrieve documents</cell></row><row><cell>extract KNN candidate fills</cell></row><row><cell>extract FST candidate fills</cell></row><row><cell>extract SVM candidate fills</cell></row><row><cell>extract Light candidate fills</cell></row><row><cell>rank candidates</cell></row><row><cell>check answers</cell></row><row><cell>questions was determined by comparing the performance of each IX module on the TREC 9 and 10 question sets. The sole state feature used in our strategy decisions was the expected answer type, and feedback loops to call multiple IX modules were considered only as a means for failure recovery when prior execution (at either the extraction or answer generation stages) did not identify any potential answers. The resulting extraction module preference orders (summarized in Table</cell></row></table><note coords="5,319.20,73.18,21.34,9.05"><p>types:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.00,96.51,467.70,281.16"><head></head><label></label><figDesc>Sample action sequences generated and executed by the JAVELIN system. The second question shows the Planner's ability to recover from the FST extractor's failure to produce any candidates by invoking the SVM extractor. location FST , SVM , Light , KNN temporal FST , KNN , SVM , Light causal-antecedent Light , KNN , FST , SVM causal-consequence Light , KNN , FST , SVM other 3 SVM , FST , KNN , Light</figDesc><table coords="6,72.00,96.51,449.78,188.88"><row><cell>A: 1. Driving Miss Daisy 2. Chariots of Fire 3. Saving Private Ryan ...</cell><cell>&lt;retrieve documents DS6024 RO6637&gt; &lt;extract SVM candidate fills FS18637 RO6637 DS6024&gt; &lt;rank candidates AL5184 RO6637 FS18637&gt;</cell></row><row><cell></cell><cell>&lt;check answers A5046 AL5184 Q2694&gt;</cell></row><row><cell cols="2">Q: What country is Aswan High Dam located in? &lt;retrieve documents DS5957 RO6570&gt; A: 1. Egypt 2. Saudi Arabia &lt;extract FST candidate fills FS17958 RO6570</cell></row><row><cell></cell><cell>DS5957&gt;</cell></row><row><cell></cell><cell>&lt;extract SVM candidate fills FS17962 RO6570</cell></row><row><cell></cell><cell>DS5957&gt;</cell></row><row><cell></cell><cell>&lt;rank candidates AL5119 RO6570 FS1962&gt;</cell></row><row><cell></cell><cell>&lt;check answers A4983 AL5119 Q2504&gt;</cell></row><row><cell>Figure 3:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,313.20,316.23,226.78,406.57"><head>Table 3 :</head><label>3</label><figDesc>Answer type accuracy for factoid questions.</figDesc><table coords="6,313.20,316.23,226.78,406.57"><row><cell cols="3">Assigned Answer Type #Qs Accuracy</cell></row><row><cell>action</cell><cell>1</cell><cell>0</cell></row><row><cell>definition</cell><cell>1</cell><cell>0</cell></row><row><cell>description</cell><cell>3</cell><cell>0</cell></row><row><cell>lexicon</cell><cell>13</cell><cell>0.1538</cell></row><row><cell>location</cell><cell>64</cell><cell>0.3125</cell></row><row><cell>numeric-expression</cell><cell>97</cell><cell>0.1649</cell></row><row><cell>object</cell><cell>93</cell><cell>0.0430</cell></row><row><cell>organization-name</cell><cell>22</cell><cell>0.0455</cell></row><row><cell>person-name</cell><cell>33</cell><cell>0.1818</cell></row><row><cell>process</cell><cell>34</cell><cell>0</cell></row><row><cell>proper-name</cell><cell>1</cell><cell>0</cell></row><row><cell>regexp</cell><cell>1</cell><cell>0</cell></row><row><cell>relation</cell><cell>1</cell><cell>0</cell></row><row><cell>temporal</cell><cell>46</cell><cell>0.1304</cell></row><row><cell>type-of</cell><cell>1</cell><cell>0</cell></row><row><cell>unknown</cell><cell>2</cell><cell>0</cell></row><row><cell cols="3">of unsupervised accuracy; a breakdown by answer type appears in tables 7 and 8. The average supervised and unsurpervised accuracies are 97.4% and 92.0%, respec-tively. Both are acceptable, but the five percent drop in unsupervised performance indicates that we have room to improve. However, the training data used for the Ques-tion Analyzer has a different question distribution from the TREC 2003 list questions. The low score in list ques-tions reflect this problem through an increase in misclas-sification rate in TREC 2003.</cell></row><row><cell cols="3">Our experiments show that the FST IX seems to bet-ter cover the location question answer space, while the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,151.56,282.63,308.76,211.81"><head>Table 7 :</head><label>7</label><figDesc>Performance of the Question Analyzer on TREC 9 and 10 questions.</figDesc><table coords="7,178.92,305.43,254.27,189.01"><row><cell>Answer Type</cell><cell cols="3">Questions Correct ATypes Accuracy</cell></row><row><cell>temporal</cell><cell>99</cell><cell>99</cell><cell>100%</cell></row><row><cell>object</cell><cell>80</cell><cell>69</cell><cell>86%</cell></row><row><cell>location</cell><cell>111</cell><cell>106</cell><cell>95%</cell></row><row><cell>proper-name</cell><cell>100</cell><cell>91</cell><cell>91%</cell></row><row><cell>numeric-expression</cell><cell>74</cell><cell>65</cell><cell>88%</cell></row><row><cell>lexicon</cell><cell>25</cell><cell>19</cell><cell>76%</cell></row><row><cell>definition</cell><cell>5</cell><cell>5</cell><cell>100%</cell></row><row><cell>person-bio</cell><cell>0</cell><cell>0</cell><cell>-</cell></row><row><cell>regexp</cell><cell>0</cell><cell>0</cell><cell>-</cell></row><row><cell>causal</cell><cell>0</cell><cell>0</cell><cell>-</cell></row><row><cell>procedural</cell><cell>6</cell><cell>6</cell><cell>100%</cell></row><row><cell>action</cell><cell>0</cell><cell>0</cell><cell>-</cell></row><row><cell>relation</cell><cell>0</cell><cell>0</cell><cell>-</cell></row><row><cell>Overall</cell><cell>500</cell><cell>460</cell><cell>92.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,72.00,504.15,468.07,218.64"><head>Table 8 :</head><label>8</label><figDesc>Performance of the Question Analyzer on TREC 11 questions.</figDesc><table coords="7,72.00,534.99,468.07,187.80"><row><cell cols="5">Question Set Best Median Worst JAVELIN</cell><cell>AnswerType</cell><cell cols="2">#Qs Average FScore</cell></row><row><cell>Factoid</cell><cell>0.7</cell><cell>0.177</cell><cell>0.034</cell><cell>0.133</cell><cell>location</cell><cell>12</cell><cell>0.06475</cell></row><row><cell>Definition</cell><cell>0.555</cell><cell>0.192</cell><cell>0.0</cell><cell>0.216</cell><cell>proper-name</cell><cell>1</cell><cell>0.017</cell></row><row><cell>List</cell><cell>0.396</cell><cell>0.069</cell><cell>0.0</cell><cell>0.052</cell><cell>person-name</cell><cell>10</cell><cell>0.0248</cell></row><row><cell cols="5">Table 4: JAVELIN TREC12 scores. JAVELIN scores are compared with the best, median, and worst accuracy scores in the TREC 2003 QA track.</cell><cell cols="3">organization-name object Table 5: TREC12 List Questions Task. 3 0.19767 11 0.02627</cell></row><row><cell cols="5">causal strategy implemented in the proximity based IX handles causal-antecedent/causal-consequence questions from apposition contexts, with reasonable success. seems to extract long and complete definitions, mostly better. The definition question strategy in the IX and AG</cell><cell cols="3">JAVELIN system performance, we ran two tests on the TREC 9 corpus, one with and one without type checking.</cell></row><row><cell cols="5">To examine the impact of answer type checking on</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="7,313.20,674.91,226.68,12.00;7,313.20,686.91,226.67,12.00;7,313.20,698.79,226.76,12.00;7,313.20,710.79,226.65,12.00"><p>The results appear in table 9. Both the TREC score and the MRR score increased approximately 12% with type checking enabled. For location-specific questions from TREC 9, 10, and 11, seen in table 10, these scores im-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,72.00,285.27,226.69,12.00"><head>Table 9 :</head><label>9</label><figDesc>TREC9 results with/without type checking (TC)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,313.20,71.55,226.55,200.24"><head>Table 10 :</head><label>10</label><figDesc>Results on 316 location questions from TREC 9, 10, and 11 with and without type checking (TC).</figDesc><table coords="8,320.76,71.55,211.58,200.24"><row><cell></cell><cell cols="4">Without TC With TC</cell><cell></cell></row><row><cell>Trec Score</cell><cell></cell><cell>0.177</cell><cell></cell><cell>0.218</cell><cell></cell></row><row><cell>MRR</cell><cell></cell><cell>0.207</cell><cell></cell><cell>0.259</cell><cell></cell></row><row><cell>Answer Type</cell><cell cols="3">#Qs SVM PL</cell><cell>PL+</cell><cell>PL*</cell></row><row><cell>location</cell><cell>109</cell><cell>26</cell><cell>27*</cell><cell>26 (S)</cell><cell>33</cell></row><row><cell>temporal</cell><cell>99</cell><cell>28</cell><cell>27</cell><cell>28 (S)</cell><cell>43</cell></row><row><cell>object</cell><cell>90</cell><cell>1</cell><cell>2</cell><cell>3 (K)</cell><cell>5</cell></row><row><cell>num-expression</cell><cell>69</cell><cell>7</cell><cell>8</cell><cell>7 (S)</cell><cell>14</cell></row><row><cell>person-name</cell><cell>50</cell><cell>3</cell><cell>3</cell><cell>7 (K)</cell><cell>10</cell></row><row><cell>proper-name</cell><cell>31</cell><cell>5</cell><cell>5</cell><cell>7 (L)</cell><cell>8</cell></row><row><cell>lexicon</cell><cell>24</cell><cell>3</cell><cell>3</cell><cell>3 (S;L)</cell><cell>3</cell></row><row><cell>org-name</cell><cell>16</cell><cell>4</cell><cell>4</cell><cell>4 (S)</cell><cell>4</cell></row><row><cell>other</cell><cell>12</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>OVERALL</cell><cell>500</cell><cell>77</cell><cell>79</cell><cell>85</cell><cell>120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="8,313.20,281.19,226.79,95.76"><head>Table 11 :</head><label>11</label><figDesc>JAVELIN performance on TREC 11 questions using a fixed sequence with the best single extractor (SVM) and the Planner with answer-type-based IX operator model (PL). The last two columns contain upper bounds for an optimal answer-type-based Planner model (PL+), and the best possible performance for the current QA components by chosing an optimal sequence for each question (PL*).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,329.28,684.06,210.45,8.58;3,313.20,694.02,226.66,8.58;3,313.20,703.98,226.68,8.58;3,313.20,713.94,62.23,8.58"><p>We are also working to add an Information Extractor based on NLP analysis of answer passages (NLP IX); although this module is shown in Figure1, it was not integrated into the system evaluated for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2003" xml:id="foot_1" coords="3,377.71,713.94,45.10,8.58"><p>TREC 2003.   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="4,329.28,664.14,210.60,8.58;4,313.20,674.10,226.66,8.58;4,313.20,684.06,226.64,8.58;4,313.20,694.02,226.66,8.58;4,313.20,703.98,226.66,8.58;4,313.20,713.94,58.64,8.58"><p>It should be noted that the normal operating mode for the GUI and Planner includes an option for the Planner to solicit user-feedback during the QA process. This functionality was disabled during the TREC evaluation, and a batch-mode script was used in place of the standard GUI to issue questions to the Planner module.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,327.34,90.39,212.53,12.00;9,323.16,101.43,216.71,12.00;9,323.16,114.91,128.20,9.34" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,349.40,101.43,165.25,12.00">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,523.20,103.99,16.67,9.34;9,323.16,114.91,58.57,9.34">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,131.31,226.87,12.00;9,323.16,142.23,150.90,12.00" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,375.05,131.31,165.03,12.00;9,323.16,142.23,122.73,12.00">Unsupervised learning of disambiguation rules for part of speech tagging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>VLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,161.19,226.62,12.00;9,323.16,172.11,216.95,12.00;9,323.16,183.03,217.05,12.00;9,323.16,194.07,60.54,12.00" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,525.32,161.19,14.51,12.00;9,323.16,172.11,216.95,12.00;9,323.16,183.03,217.05,12.00;9,323.16,194.07,25.88,12.01">Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>ANLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,213.03,226.62,12.00;9,323.16,223.95,216.60,12.00;9,323.16,237.43,32.97,9.34" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czuba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<title level="m" coord="9,518.13,213.03,21.69,12.00;9,323.16,223.95,216.60,12.00;9,323.16,237.43,27.48,9.35">question answering, two heads are better than one. HLT-NAACL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,253.83,226.69,12.00;9,323.16,264.75,36.90,12.00" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,404.18,253.83,135.70,12.00;9,323.16,264.75,32.80,12.01">Wordnet -an electronic lexical database</title>
		<author>
			<persName coords=""><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,283.71,226.74,12.00;9,323.16,294.63,216.71,12.00;9,323.16,305.67,76.59,12.00" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,323.16,294.63,216.71,12.00;9,323.16,305.67,29.88,12.01">Experiments with open-domain textual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Maiorano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>COLING</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,324.51,226.63,12.00;9,323.16,335.55,216.69,12.00;9,323.16,349.03,216.84,9.35;9,323.16,359.95,20.36,9.35" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,482.54,324.51,57.29,12.00;9,323.16,335.55,173.48,12.00">Towards planning and execution for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Hiyakumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,503.66,338.11,36.19,9.35;9,323.16,349.03,216.84,9.35;9,323.16,359.95,16.29,9.35">Proceedings of AIPS Workshop on Exploring Real-World Planning</title>
		<meeting>AIPS Workshop on Exploring Real-World Planning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,376.35,226.75,12.00;9,323.16,387.27,216.71,12.00;9,323.16,398.31,96.85,12.00" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
		<title level="m" coord="9,323.16,387.27,216.71,12.00;9,323.16,398.31,91.72,12.00">Is it the right answer? exploiting web redundancy for answer validation. ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,417.27,226.75,12.00;9,323.16,428.19,128.49,12.00" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,457.86,417.27,82.09,12.00;9,323.16,428.19,92.91,12.00">The kantoo machine translation environment</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>AMTA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,447.15,226.65,12.00;9,323.16,458.07,84.63,12.00" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Oglivie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<title level="m" coord="9,448.76,447.15,91.09,12.00;9,323.16,458.07,50.71,12.00">Experiments using the lemur toolkit</title>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,313.20,477.03,226.63,12.00;9,323.16,487.95,206.04,12.00;9,313.20,506.91,41.85,12.00;9,385.89,506.91,22.64,12.00;9,439.13,506.91,100.81,12.00;9,323.16,520.39,163.01,9.35" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,471.75,477.03,68.08,12.00;9,323.16,487.95,177.75,12.00">Learning surface text patterns for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hovy</surname></persName>
		</author>
		<ptr target="ftp://crl.nmsu.edu/CLR/lexica/gazetteer/" />
	</analytic>
	<monogr>
		<title level="m" coord="9,508.70,490.51,20.50,9.35;9,313.20,506.91,41.85,12.00;9,385.89,506.91,22.64,12.00;9,439.13,506.91,100.81,12.00">ACL. TIPSTER. 1992. Tipster gazetteer 4.0</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
