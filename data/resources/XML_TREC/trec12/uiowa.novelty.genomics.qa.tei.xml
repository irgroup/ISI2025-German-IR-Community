<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,130.49,71.83,351.02,16.67;1,208.01,101.83,195.98,16.67">Experiments in Novelty, Genes and Questions at The University of Iowa</title>
				<funder>
					<orgName type="full">University of Iowa</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,166.01,135.26,78.25,10.94"><forename type="first">David</forename><surname>Eichmann</surname></persName>
							<email>david-eichmann@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.99,135.26,91.79,10.94"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Management Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.67,135.26,52.49,10.94"><forename type="first">Marc</forename><surname>Light</surname></persName>
							<email>marc-light@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Library and Information Science</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,140.41,156.46,66.99,10.94"><forename type="first">Hudong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.54,156.46,45.00,10.94"><forename type="first">Ying</forename><surname>Xin</surname></persName>
						</author>
						<author>
							<persName coords="1,269.54,156.46,15.75,10.94"><surname>Qiu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Management Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.34,156.46,71.99,10.94"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Arens</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,398.46,156.46,68.33,10.94"><forename type="first">Aditya</forename><surname>Sehgal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,130.49,71.83,351.02,16.67;1,208.01,101.83,195.98,16.67">Experiments in Novelty, Genes and Questions at The University of Iowa</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3ED881DDB143A957AFA372B5AC1A55D4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our system for novelty this year is a refinement of that used for last year. One of the challenges in preparing for the 2002 novelty track was the nature of the training data. Our experiments with using the 2002 evaluation data as training data for this year have shown that the novelty task can in fact be tuned to trade off precision and recall -at least across the range of what a given system can detect as novel. Our tuning involved establishing a similarity threshold for sentence relevance and an new entity threshold for novelty.</p><p>We decided to focus our development experiments for this year on a composite precondition of simple similarity matches between the topic definition and the candidate document and the topic and the candidate sentence. If both measures exceed the declared threshold, a sentence is declared relevant. Additionally, if the number of novel elements present in the sentence is above a declared number, the sentence is declared novel. 'Element' here can be a noun phrase or a named entity. For the available training topics, this proved to be remarkably responsive to tuning between precisionfocused runs and recall-focused runs for novelty as well as the more predictable relevance decision.</p><p>Our official runs involved the following approaches for the defined tasks:</p><p>Task 1 (detect relevance and novelty). Proceed as described above, making a judgement on relevance based upon similarity, and given that as a guard, make a judgement on novelty based upon the existence of new entities.</p><p>Task 2 (given relevance, detect novelty). Load the given relevance judgements, and proceed as per task 1 for novelty.</p><p>Task 3 (given relevance and novelty for first 5, detect relevance and novelty for last 20). Load relevance judgements and entities present in the first five documents, and then proceed as per task 1 for both relevance and novelty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Task 4 (given relevance for all and novelty for first 5, detect novelty for last 20). Load as per task 2 for relevance and task 3 for novelty, run as per task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate Results for All Tasks</head><p>We submitted two sets of runs for tasks 1 and 3, with similarity thresholds of 0.075 and 0.125 and a new entity/noun phrase threshold of 1 and five runs each for tasks 2 nd 4, using new entity/ noun phrase thresholds of 0-4. Table <ref type="table" coords="2,250.97,419.53,6.00,10.94" target="#tab_0">1</ref> shows the full results for all runs. As show in Figure <ref type="figure" coords="2,514.62,419.53,9.55,10.94">1a</ref>, there is a distinct performance differential between relevance and novelty detection, but relative performance among the four threshold/task conditions is comparably positioned for relevance and novelty. As might be expected, increasing the similarity threshold slightly improves precision at a slight cost to recall. More interestingly, precision of the task 1 configurations is similarly higher than their task 3 counterparts. Having the additional information regarding the first five documents for each topic slightly improves recall, but at the cost of precision. In other words, we can achieve better precision in both relevance and novelty by not looking at the initial pool of documents available in task 3.</p><p>As shown in Figure <ref type="figure" coords="2,184.89,560.53,10.00,10.94">1b</ref>, there is a very regular recall/precision trade-off achieved when varying the number of entities and/or noun phrases required to declare a sentence novel, given that it is relevant. It is also interesting to note that the oddity noted for tasks 1 and 3 is still present for tasks 2 and 4. Indeed, in this case, task 4 with relevance and novelty information available for the first five documents uniformly performs less well for both precision and recall for all thresholds. We find this intriguing and plan on further analyzing the cause of these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditioning by Topic Type</head><p>Figures 2 and 3 show the performance per topic for relevance and novelty, broken out by event and opinion topics. There appear to be no major trends to distinguish event topics from opinion topics, although events do seem to edge opinions out in general. It does appear that the additional information available in task 3 results in a 'tightening' of the topic clouds for both relevant and novelty over the topic clouds for task 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of New Entities and Noun Phrases</head><p>Figure <ref type="figure" coords="4,124.33,602.99,6.00,10.94" target="#fig_4">4</ref> shows a topic-level breakout of performance for each run for tasks 2 and 4. For the degenerate condition, n = 0, we see perfect recall generally, but our dual guard of both sentence and document level similarity does lower recall for some topics. Increasing the threshold to n = 1 improves precision overall, as seen in Figure <ref type="figure" coords="4,289.97,647.99,12.00,10.94">1b</ref> in aggregate. Further increases in the threshold generate little benefit with respect to precision and seriously erodes recall. Based upon this we have concluded that a single new entity or noun phrase can serve as an indicator of novelty. Our future work will focus on the analysis of poorly performing topics for n=1. Events </p><formula xml:id="formula_0" coords="4,181.44,78.21,347.73,216.72">✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ Opinions ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Precision Recall (b) Task 3 Events ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ Opinions ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛</formula><formula xml:id="formula_1" coords="4,151.35,325.69,377.83,216.72">n = 0 ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ n = 1 ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ n = 2 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ n = 3 ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ n = 4 ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Precision Recall (b) Task 4, By Topic n = 0 ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ n = 1 ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ n = 2 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ n = 3 ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ n = 4 ▲ ▲ ▲ ▲</formula><p>▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ ▲ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">-Genomics</head><p>We participated in both the primary and secondary tasks in this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary task:</head><p>This is a baseline run where we used SMART as the retrieval system with atc weighting on queries and documents. Queries were generated from the different fields provided to us including gene name, symbols, product names. A few low level experiments were conducted with different weighting schemes and stemming options. We also tried using document classifiers (SVM) to limit the document set, but the results were not good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Secondary task:</head><p>Each abstract sentence was classified to gauge its likelihood as a source of a GeneRIF. A sentence classifier was built using GeneRIF entries in LocusLink excluding those that were in the secondary.txt file and their abstracts. For feature selection an in house tokenizer was used and idf weights computed against a reference subset of 211,457 MEDLINE abstracts selected independent of this track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Set:</head><p>GeneRIF entries (excluding the 'test' set as described above) were used to identify abstracts. 90% of the abstracts were used as training and 10% as testing for model/parameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of positive and negative sentence samples.</head><p>Several methods were tried. But first, sentences in title, last sentence and first sentence are found to be most relevant, thus other sentences are discarded. Our methods involve a measure called pDice. This measures the percentage of words in a sentence that are in the GeneRIF entry corresponding to the abstract in which the sentence occurs.</p><p>Method 1: GeneRIF sentences are positive samples, low pDice sentences are negative samples.</p><p>Method 2: High pDice sentences are positive samples, low pDice ones are negative samples.</p><p>Method 3: GeneRIF and high pDice sentences are positive samples, low pDice ones are negative samples.</p><p>We used SVM classifier technology, specifically LIBSVM java classifier, with most parameters at default value. We also used EPSILON_SVR SVM, RBF kernel function. Positive/Negative class ratio is not used because it doesn't help. The best model found uses GeneRIF statements as positive samples and sentences with pDice&lt;0.25 as negative samples. SVM gives each sentence score, the larger the score the more likely it is to be a GeneRIF. A weighting scheme was also used to emphasize titles, first sentences, and last sentences. The best weighting scheme on the test set was 5:0:1 respectively which is almost the same as saying select titles only. The best model and parameters was selected for use on secondary.txt and corresponding abstracts to generate result, for the official TREC run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">-Question Answering</head><p>This year marks the first evaluation of our complete implementation of an extraction-based QA system. By shifting natural language parsing forward in the process, we can amortize this very expensive step against a number of downstream extraction processes that mine the text for named entities, relationships, etc. Redefinition of extraction specifications hence does not require reparsing of the source text. We have implemented tgrep-like extraction grammar designed for predicatebased extensibility using it in mapping sentence parse trees to relational structure. This overall approach handles not only factoid answers, but definitional answers and those requiring inference across multiple extracted relationships.</p><p>Each document in the corpus is decomposed into doc-id / sentence pairs, with the sentence being the unit of analysis from that point. Each sentence is then POS-tagged and fed to the CMU link grammar parser. The parse tree for the sentence is then attributed with the POS tags for each word. Processing both queries and documents using this scheme allows us to establish both the nature of the query (using a fairly typical taxonomy) and the nature of the needed answer. This is particularly useful with respect to identification of candidate phrases in sentences and scoring of these phrases against the goal of the query. Sentences are then matched against the set of extraction patterns, populating a set of relations used to answer queries derived from the questions.</p><p>The availability of the parse tree for the phrase allows for elision of subordinate clauses that can cause answers to span too long a string and for extraction of likely answers through heuristic matching of, for example, a subordinate clause immediately trailing a mention of a candidate named entity.</p><p>We view our results for this year as very preliminary for two key reasons. The first is operational -a few days before the deadline a database failure cost us the full parse of the corpus and we were only able to reparse the top fifty documents for each question in the time remaining. The second is a developmental one -we have only begun the specification of our extraction pattern framework, and coverage is limited to • persons' titles, ages and a minimal set of interpersonal relationships;</p><p>• location of organizations (e.g., "Seattle-based Microsoft"); and</p><p>• relative location of place names (e.g., "the resort, five miles east of Seattle").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factoid Questions</head><p>The preliminary nature of our extraction patterns is probably most evident for factoid questions. Our pattern sets are insufficiently rich to provide sufficient coverage of potential questions, and hence the number of correct answers we generate is modest. As shown in Table <ref type="table" coords="6,476.81,601.06,4.50,10.94" target="#tab_1">2</ref>, there is interesting potential in the low levels of unsupported and inexact answers relative to correct answers. We also have a comparatively high level of NIL answer recall, particularly given our level of correct answers. This is easily explained when the number of NIL answers returned is considered -~20% of all questions. This is directly attributable to failure to extract sufficient information with the available patterns -we are returning so many NILs that we are catching those questions that actually have no answer in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List Questions</head><p>Our implementation for this year had no support for list identification or extraction. Any coverage of answers in this category was purely accidental...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition Questions</head><p>We do believe that the approach that we are taking with extraction holds good promise for definition questions. As shown in Table <ref type="table" coords="7,249.30,413.06,4.50,10.94" target="#tab_3">4</ref>, performance for this category of question is very different than that for factoids and lists.</p><p>Breaking out performance of individual questions, as shown in Figure <ref type="figure" coords="7,426.88,563.06,4.50,10.94">5</ref>, we see that there is a broad spread of performance, but there are a large number of questions with no answers provided.</p><p>Figure <ref type="figure" coords="7,124.20,599.06,6.00,10.94">6</ref> shows our performance in relation to the number of vital and total facts connected to a question. For questions where our system is performing well, there are a relatively small number (~2-5) of vital facts and a modest number (~10) of total facts.    </p><formula xml:id="formula_2" coords="8,230.39,75.27,177.57,179.01">UIowaQA0301 ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ ✧ UIowaQA0302 ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ ✛ UIowaQA0303 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,227.01,301.53,157.99,11.11"><head></head><label></label><figDesc>Figure 1: Novelty Task Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,183.00,628.03,246.00,11.11"><head></head><label></label><figDesc>Figure 2: Novelty Task, Relevant by Topic Type</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,194.33,299.53,223.33,11.11"><head>Figure</head><label></label><figDesc>Figure 3: Novelty Task, New by Topic Type</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,170.00,550.01,272.00,11.11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Novelty Task, New by Entity/NP Threshold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,202.33,284.71,207.35,11.11"><head>Figure</head><label></label><figDesc>Figure 5: QA Task, Definition Questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.93,71.89,459.63,551.05"><head>Table 1 : Summary Results for Novelty Track</head><label>1</label><figDesc></figDesc><table coords="3,72.93,96.06,459.63,289.11"><row><cell>Run</cell><cell>Task</cell><cell>Sim. Thresh.</cell><cell>Entity/NP Thres.</cell><cell cols="2">Relevant Prec. Recall</cell><cell>F</cell><cell cols="2">New Prec. Recall</cell><cell>F</cell></row><row><cell>UIowa03Nov01</cell><cell></cell><cell>0.075</cell><cell>1</cell><cell>0.64</cell><cell>0.70</cell><cell>0.594</cell><cell>0.47</cell><cell>0.65</cell><cell>0.480</cell></row><row><cell>UIowa03Nov02</cell><cell></cell><cell>0.125</cell><cell>1</cell><cell>0.65</cell><cell>0.64</cell><cell>0.568</cell><cell>0.48</cell><cell>0.59</cell><cell>0.461</cell></row><row><cell>UIowa03Nov03</cell><cell></cell><cell>-</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.65</cell><cell>0.98</cell><cell>0.767</cell></row><row><cell>UIowa03Nov04</cell><cell></cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.73</cell><cell>0.90</cell><cell>0.794</cell></row><row><cell>UIowa03Nov05</cell><cell></cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.76</cell><cell>0.77</cell><cell>0.746</cell></row><row><cell>UIowa03Nov06</cell><cell></cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.78</cell><cell>0.60</cell><cell>0.659</cell></row><row><cell>UIowa03Nov07</cell><cell></cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.80</cell><cell>0.45</cell><cell>0.555</cell></row><row><cell>UIowa03Nov08</cell><cell></cell><cell>0.075</cell><cell>1</cell><cell>0.60</cell><cell>0.78</cell><cell>0.606</cell><cell>0.43</cell><cell>0.71</cell><cell>0.466</cell></row><row><cell>UIowa03Nov09</cell><cell></cell><cell>0.125</cell><cell>1</cell><cell>0.62</cell><cell>0.69</cell><cell>0.585</cell><cell>0.44</cell><cell>0.62</cell><cell>0.448</cell></row><row><cell>UIowa03Nov10</cell><cell></cell><cell>-</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.62</cell><cell>0.98</cell><cell>0.741</cell></row><row><cell>UIowa03Nov11</cell><cell></cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.70</cell><cell>0.89</cell><cell>0.767</cell></row><row><cell>UIowa03Nov12</cell><cell></cell><cell>-</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.73</cell><cell>0.74</cell><cell>0.712</cell></row><row><cell>UIowa03Nov14</cell><cell></cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.75</cell><cell>0.56</cell><cell>0.617</cell></row><row><cell>UIowa03Nov15</cell><cell></cell><cell>-</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.78</cell><cell>0.40</cell><cell>0.505</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,77.73,71.89,448.13,105.28"><head>Table 2 : QA Track, Factoids</head><label>2</label><figDesc></figDesc><table coords="7,77.73,96.06,448.13,81.11"><row><cell>Run</cell><cell>U</cell><cell>X</cell><cell>R</cell><cell>Accuracy</cell><cell># NIL returned</cell><cell>NIL P</cell><cell>NIL R</cell></row><row><cell>UIowaQA0301</cell><cell>3</cell><cell>4</cell><cell>14</cell><cell>0.034</cell><cell>100</cell><cell>0.100</cell><cell>0.333</cell></row><row><cell>UIowaQA0302</cell><cell>2</cell><cell>2</cell><cell>17</cell><cell>0.041</cell><cell>173</cell><cell>0.087</cell><cell>0.500</cell></row><row><cell>UIowaQA0303</cell><cell>3</cell><cell>2</cell><cell>17</cell><cell>0.041</cell><cell>98</cell><cell>0.102</cell><cell>0.333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,243.01,252.89,125.99,105.28"><head>Table 3 : QA Track, Lists</head><label>3</label><figDesc></figDesc><table coords="7,256.79,277.06,97.33,81.11"><row><cell>Run</cell><cell>Ave. F</cell></row><row><cell cols="2">UIowaQA0301 0.002</cell></row><row><cell cols="2">UIowaQA0302 0.002</cell></row><row><cell cols="2">UIowaQA0303 0.004</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,227.67,451.89,156.65,91.28"><head>Table 4 : QA Track, Definitions</head><label>4</label><figDesc></figDesc><table coords="7,252.14,476.06,106.51,67.11"><row><cell>Run</cell><cell>Ave. F</cell></row><row><cell>UIowaQA0301</cell><cell>0.214</cell></row><row><cell>UIowaQA0302</cell><cell>0.231</cell></row><row><cell>UIowaQA0303</cell><cell>0.048</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>The <rs type="funder">University of Iowa</rs> participated in the novelty, genomics and question answering tracks of TREC-2003.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
