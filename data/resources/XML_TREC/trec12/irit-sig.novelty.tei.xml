<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,197.40,66.05,217.28,12.58">TREC NOVELTY TRACK AT IRIT -SIG</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.62,87.05,67.92,11.21"><forename type="first">Taoufiq</forename><surname>Dkaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ISYCOM/GRIMM</orgName>
								<orgName type="institution">Université Toulouse le Mirail</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.35,87.05,66.93,11.21"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique de Toulouse</orgName>
								<address>
									<addrLine>118 Rte de Narbonne</addrLine>
									<postCode>31062</postCode>
									<settlement>Toulouse CEDEX</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Institut Universitaire de Formation des Maîtres Midi-Pyrénées</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,197.40,66.05,217.28,12.58">TREC NOVELTY TRACK AT IRIT -SIG</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">02A122763D1FA7670674A236F3C15B9F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In TREC 2003, IRIT improved the strategy that was introduced in TREC 2002. A sentence is considered as relevant if it matches the topic with a certain level of coverage. This coverage depends on the category of terms used in the texts. Different types of terms have been defined: highly relevant, scarcely relevant, nonrelevant and highly non-relevant. With regard to the novelty part, a sentence is considered as novel if its similarity with previously processed sentences and with the n-best-matching sentences does not exceed certain thresholds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>«The TREC novelty track is designed to investigate systems' abilities to locate relevant and new information within the ranked set of documents retrieved in answer to a TREC topic » <ref type="bibr" coords="1,398.16,316.79,61.69,11.21">[trec.nist.gov]</ref>.</p><p>Retrieving relevant texts is traditionally based on computing a similarity between the representations of the information need (or topic) and the texts. This general statement has been applied to full documents as well as chunks of texts (passage retrieval). Intuitively, the same idea can be applied when sentences retrieval is involved. In TREC 2002 IRIT developed a new strategy in order to detect the relevant sentences. This approach has not been used in a more general context of document retrieval but we did used it previously and partially in document categorization <ref type="bibr" coords="1,241.73,404.33,63.95,11.21">(Mothe, 2002)</ref>. In our approach a sentence is considered as relevant if it matches the topic with a certain level of coverage. This level of coverage depends on the category of the terms used in the texts. Three types of terms were defined for TREC 2002: highly relevant, scarcely relevant and non-relevant. In TREC 2003 we introduced a new class of terms: highly non-relevant terms. Terms from this category are extracted from the narrative parts of the queries that describe what is a non-relevant document. A negative weight can be assigned to these words. With regard to the novelty part, a sentence is considered as novel if its similarity with each previously processed and -selected as novel-sentences does not exceed a certain threshold. In addition, this sentence should not be too similar to a virtual sentence made of the n-best-matching sentences.</p><p>The results we obtained in TREC 2002 were quite good regarding the 'relevant' subtask. Indeed, for 36 topics (73%), the R*P was higher or equal to the average of the 42 runs which were submitted. In TREC 2003, we improved these results as we obtained 46 topics (92%) for which the F-measure (2*R*P/(R+P)) was equal or higher to the average of the 55 runs submitted. With regard to the 'novelty' part, when considering the retrieved sentences, we also obtained 46 topics (92%) for which the F-measure is higher or equal to the average of the 55 runs. However, an interesting result is that our method is better when there is some noise in the sentence set. Indeed the results are better when considering the retrieved sentences than when considering the relevance sentences only, relatively to other participants' methods (i.e. our system ranks better over the submitted runs). We obtained 41 topics (82%) for which the F-measure is higher or equal to the average of the 55 runs. This paper is organized as follows: in section 2 we describe the method we used, including the way documents and topics are represented and the strategies we developed for the two sub-tasks (relevant part and novelty part either considering only relevant sentences or all retrieved sentences). In section 3 we present the results and comment them. We also present results we obtained on runs that were not submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of the method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document and topic representation</head><p>In our method, topics and sentences are considered as chunks of text. Each chunk is pre-processed the same way in order to extract representative terms. Then, terms extracted from a given topic are categorized into different groups: highly relevant terms (HT), scarcely relevant terms (LT) and highly non-relevant terms (IT). Notice that non-relevant terms (iT) correspond to stop words. Each text is finally represented by these sets of terms, weights being associated with each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text processing</head><p>Texts are processed using the following method:</p><p>1. Stop words are removed, 2. The remaining words are normalized using a dictionary that provides a common root for different words. This dictionary contains 21291 entries.</p><p>3. Alternatively phrases are extracted. Phrases correspond to frequent sequences of words or frequent sequences of word roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Topic processing</head><p>A topic is pre-processed in order to mark-up the sentences that describe the information relevancy and the sentences that describe the non-relevancy (see Figure <ref type="figure" coords="2,300.48,363.47,4.13,11.21" target="#fig_0">1</ref>: NarrativeRel and NarrativeNonRel tags).</p><p>Topic: 35 Title: NATO, Poland, Czech Republic, Hungary Type: event Descriptive: Accession of new NATO members: Poland, Czech Republic, Hungary, in 1999.</p><p>NarrativeRel: Identity of current and newly-invited members, statements of support for and opposition to NATO enlargement and steps in the accession process and related special events are relevant. Impact on the new members, i.e., requirements they must satisfy, and their expectations regarding the implications for them are relevant. Progress in the ratification process is relevant. NarrativeNonRel: Future plans for NATO expansion, identification of nations admitted on previous occasions, and comments on future NATO structure or strategy are not relevant. Then it is analyzed in order to extract the representative terms (words or phrases) as explained in the previous section. Each term is then weighted and categorized into 3 groups:</p><p>-Highly relevant terms are terms that get a weight greater than H τ ,</p><p>-Scarcely relevant terms are terms that get a weight equal to L τ , -Highly non-relevant terms are terms that are associated with non-relevancy in the narrative part of the documents.</p><p>More precisely, the formula used to compute the term weights is defined as follows:</p><formula xml:id="formula_0" coords="2,51.06,669.58,328.51,42.67">Given k Q a topic and i t a term, { } word stop a not is t Q t T i k i k / ∈ = T k = TT k U TD k U TNR k U TNN k</formula><p>where TT k corresponds to the set of terms extracted from the Title of the topic, TD k from the Descriptive, TNR k from the NarrativeRel and TNN k is the NarrativeNonRel topic part.</p><formula xml:id="formula_1" coords="3,52.50,52.68,295.64,20.48">P k i tf , , is the frequency of i t in the TP k part, { } NN NP D T P , , , ∈</formula><p>The term weight regarding a topic is computed as follows:</p><formula xml:id="formula_2" coords="3,88.26,106.34,428.16,153.81">{ } otherwise if if if Q t weight otherwise and if f where f tf tf H k i L k i k i H k i k i k i NN k i NN k i NN k i k i NN k i NN k i P k i NP D T P P k i 0 0 0 ) , ( 1 0 0 0 ) , ( ) , ( , , , 1 , , 2 , , , , 1 1 , , 2 1 , ,<label>1 , , , , , 2 , , , , , , 1 =</label></formula><formula xml:id="formula_3" coords="3,51.54,99.27,455.92,186.47">&lt; &lt; = = = ≥ = = &lt; &gt; = ⋅ + = ⋅ = ⋅ = ∑ ∈ τ ω τ ω ω τ ω ω µ ω µ ω ω µ ω ω ω µ ω µ ω L τ and H</formula><p>τ are used in order to obtain a significant difference -in terms of importance-between highly relevant terms and scarcely relevant terms. The weights associated to scarcely relevant terms are set to L τ (1</p><p>in the experiments submitted to TREC). H τ is set to 3 in the TREC runs. This formula is also used in order to take into account highly non-relevant terms.</p><p>The term weight is used to categorized a term into one of the groups defined as follows:</p><formula xml:id="formula_4" coords="3,145.68,355.24,322.33,74.50">{ } { } L k i k k k i i k T t weight and TNR TD TT t t HT τ &gt; ∈ = ) , ( , , / U { } ( ) { } L k i k k k k i i k T t weight and TNR TD TT TNN t t LT τ = - ∈ = ) , ( , , / U { } { } NN NR D T P TP t weight t iT k i i k , , , 0 ) , ( / ∈ ∀ = = { } 0 ) , ( / &lt; ∈ = k i k i i k T t weight and TNN t t IT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Document processing</head><p>Each sentence of a document is considered as a text and the representative terms are extracted as explained in the section 2.1.1. To each term is associated a weight defined as follows:</p><p>Given j S a sentence, i t a term and j i tf , is the frequency of i t in j S .</p><formula xml:id="formula_5" coords="3,407.52,498.53,94.29,16.59">j i j i tf S t weight , ) , (<label>=</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevant sentences</head><p>In order to decide if a sentence is relevant, we associate three components to each sentence:</p><p>-a score that reflect the sentence -topic matching :</p><p>Given a topic k Q and a sentence j S</p><formula xml:id="formula_6" coords="3,181.02,610.09,247.72,25.83">( ) ∑ ⋅ = ) , ( ) , ( ) , ( k i j i k j Q t weight S t weight Q S Score</formula><p>-and two groups of terms:</p><formula xml:id="formula_7" coords="3,70.02,666.62,302.48,64.50">{ } ) ( / k i i j HT Sj t t HS ∩ ∈ = { } ) ( / k i i j LT Sj t t LS ∩ ∈ = j</formula><p>HS corresponds to the highly relevant terms from the topic that also occurs in the sentence, j LS corresponds to the scarcely relevant terms from the topic that also occurs in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that k</head><p>IT and k iT sets are only used to calculate the term weight and it is not used in the sentence selection process.</p><p>A given sentence j S is then considered as relevant iff :</p><formula xml:id="formula_8" coords="4,149.52,146.83,309.53,42.52">k j j j k j j j k j LT HS LS HS g HT HS LS LS f Q S Score ⋅ ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ + + ⋅ ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ + &gt; ) , (</formula><p>where X is the number of elements of X</p><p>In the experiments that correspond to the runs sent to TREC, the function ) ( f and ) ( g have been set to:</p><formula xml:id="formula_9" coords="4,192.60,241.26,228.58,19.37">( ) ( ) x x g and x x f 5 . 0 85 . 0 5 . 1 2 - = - =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Novel sentences</head><p>To decide if a sentence p is to be considered as novel, we compute the similarity between the sentence p and the previous successfully processed sentences i p (novel) and the similarity between the sentence p and a sentence ' P automatically built from the union of the set of i p :</p><formula xml:id="formula_10" coords="4,51.06,357.29,113.89,36.25">Given • { } n p p p , , , 2 1 K = Π</formula><p>a set of sentences labeled as novel and</p><formula xml:id="formula_11" coords="4,359.82,375.55,54.87,27.25">{ } U K n i i p P , , 1 ' ∈ =</formula><p>, ' P is a sentence made of the set of sentences from Π , • ( )</p><formula xml:id="formula_12" coords="4,71.04,426.93,38.86,10.80">y x Sim ,</formula><p>a function that compute a similarity between x and y and</p><p>• p a sentence for which the system has to decide if it brings new information.</p><p>We first compute the following similarities:</p><formula xml:id="formula_13" coords="4,53.04,482.36,73.16,23.81">( ) p P p Sim α = ' ,</formula><p>and for</p><formula xml:id="formula_14" coords="4,173.52,485.66,139.58,21.99">{ } n i , , 1 K ∈ ( ) i p i p p Sim , , ω =</formula><p>We then consider the q best previous sentences:</p><formula xml:id="formula_15" coords="4,53.04,530.12,468.83,56.94">{ } i p P n i for , , , 1 K ∈ is the series of sentences obtained by ordering Π in decreasing order of i p, ω . { } ) , ( , , 1 , ∑ ∈ = q i i p p P p Sim K β where { } 5 , 4 ∈ q</formula><p>in the runs sent to TREC.</p><p>p is considered as redundant (not novel) iff: </p><formula xml:id="formula_16" coords="4,255.72,609.27,36.26,17.27">1 τ α ≥</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section presents the results we obtained with the method we developed and using the parameters as described in section 2. When comparing the results with the other runs, we can notice that our system is better in finding relevant sentences than in detecting novelty in the sentences. The difficulties of our system to detect novelty can be linked to the fact that the system does not take into account the order of the sentences in the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relevant sentences</head><p>Figure <ref type="figure" coords="5,85.93,133.97,5.63,11.21" target="#fig_0">1</ref> indicates the number of topics for which our best system (or run) has been ranked at the X th position among the 55 runs according to the F-Measure. For example, our method obtains the best results for 0 topic, the second position for 1 topics, the third for 1 topics, etc. and has a rank higher than 36 th for only one topic (see figure <ref type="figure" coords="5,153.13,174.47,3.85,11.21" target="#fig_0">1</ref>.a). Figure <ref type="figure" coords="5,210.43,174.47,4.79,11.21" target="#fig_0">1</ref>.b provides a graph that summarizes figure 1.a by grouping together the results obtained for ranges of ranks. Additionally, the cumulative number of topics per range of system position is provided on the same graph. For example, we obtained a rank between 1 to 5 for 3 topics. The system obtains a rank equal or higher than 20 for 38 topics.</p><p>This clearly shows that our method is better than the average of the results. To be more precise, over the 50 topics, we obtained 46 topics (92%) for which the F-measure is higher or equal to the average of the 55 runs. And if we consider the run ranks, we obtained a rank higher or equal to the median (27) for 42 topics (84%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">New sentences</head><p>We present the results obtained in the second subtask the same way (see Figure <ref type="figure" coords="5,426.03,543.05,3.92,11.21" target="#fig_3">2</ref>). We distinguish the results when novel sentences are extracted from the retrieved sentences (TREC task 1 ; figure 2.1) and when they are extracted from the set of known relevant sentences (TREC task 2, figure 2.1).</p><p>Regarding the first case, over the 50 topics, we obtained 46 topics (92%) for which the F-measure is higher or equal to the average of the 55 runs. And if we consider the run ranks, we obtained a rank higher than the median (27) for 41 topics (82%).</p><p>However, when considering the relevant sentences, over the 50 topics, we obtained 41 topics (82%) for which the F-measure is higher or equal to the average of the 55 runs. And if we consider the run ranks, we obtained a rank higher than the median (27) for 30 topics (60%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Other results</head><p>We modified the term weighting function in order to take better into account the query part in which the term occurs. The best results for the relevant subtask we obtained are the following: over the 50 topics, we obtained 46 topics (92%) for which the F-measure is higher or equal to the average of the 55 runs. And if we consider the run ranks, we obtained a rank higher or equal to the median (27) for 45 topics (90%). For one topic we obtained the best rank.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,241.74,526.02,128.63,9.02"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: topic 35 (TREC 2003)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,162.30,495.05,287.42,11.21"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of topics per run rank -relevant sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,159.36,232.55,293.38,11.21"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of topics per run rank -summarized results</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The approach we developed leads to relevant results for the first part of the task (relevant sentences). Over the 50 topics, we obtained 46 topics (92%) for which the F-Measure is higher or equal to the average of the 55 runs. And if we consider the run ranks, we obtained a rank higher than the middle (26) for 42 topics. Our best-submitted run obtains the following results: Average precision 0.64, Average recall 0.58 and Average F 0.526. With regard to the second sub-task (novelty), the submitted results over the 50 topics, we obtained 41 topics (82%) for which the F-measure is higher or equal to the average of the 55 runs. </p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
