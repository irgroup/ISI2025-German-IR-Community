<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.92,100.80,332.02,15.49;1,99.60,120.48,412.72,15.49">Improving the robustness of language models -UIUC TREC-2003 Robust and Genomics Experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.48,151.55,83.34,10.76"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.68,151.55,36.07,10.76"><forename type="first">Tao</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.91,151.55,42.24,10.76"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.49,151.55,58.73,10.76"><forename type="first">Zhidi</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.92,100.80,332.02,15.49;1,99.60,120.48,412.72,15.49">Improving the robustness of language models -UIUC TREC-2003 Robust and Genomics Experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5FD352B704729D48CECCC5D8368C989</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-08-05T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we report our experiments in the TREC 2003 Genomics Track and the Robust Track. A common theme that we explored is the robustness of a basic language modeling retrieval approach. We examine several aspects of robustness, including robustness in handling different types of queries, different types of documents, and optimizing performance for difficult topics. Our basic retrieval method is the KL-divergence retrieval model with the two-stage smoothing method plus a mixture model feedback method. In the Genomics IR track, we propose a new method for modeling semi-structured queries using language models, which is shown to be more robust and effective than the regular query model in handling gene queries. In the Robust track, we experimented with two heuristic approaches to improve the robustness in using language models for pseudo feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work on using language models for information retrieval has shown that probabilistic language models have several advantages over the more traditional retrieval models, including being able to optimize retrieval parameters automatically <ref type="bibr" coords="1,149.67,495.16,11.63,8.97" target="#b6">[7]</ref> and improve retrieval performance through better language models or estimation methods <ref type="bibr" coords="1,72.00,516.64,10.78,8.97" target="#b4">[5,</ref><ref type="bibr" coords="1,85.54,516.64,7.19,8.97" target="#b1">2]</ref>. Language models have also shown promising empirical results in different TREC tasks (e.g., <ref type="bibr" coords="1,252.65,527.44,10.50,8.97" target="#b2">[3]</ref>). In this year's TREC experiments, we focus on the issue of the robustness of language modeling approaches, and examine several aspects of it, including robustness in handling different types of queries, different types of documents, and optimizing performance for difficult topics.</p><p>Our basic retrieval method is the KL-divergence retrieval model with the two-stage smoothing method plus a mixture model feedback method <ref type="bibr" coords="1,224.82,613.48,10.78,8.97" target="#b4">[5,</ref><ref type="bibr" coords="1,240.88,613.48,7.27,8.97" target="#b6">7]</ref>. The KLdivergence retrieval model can be regarded as a generalization of the query likelihood method. It has an additional advantage of supporting model-based feedback <ref type="bibr" coords="1,72.00,656.56,10.60,8.97" target="#b4">[5]</ref>. The two-stage smoothing was originally proposed in the context of query likelihood, but we can adapt it in a straightforward way to handle a query model generated by a feedback method. We test this basic approach in the Genomics IR track to see how robust such an approach is in handling the Medline abstract documents and the gene description queries. In the Genomics IR track, we propose a new method for modeling semi-structured queries using language models, which is shown to be more robust and effective than the regular query model in handling gene queries. In the Robust Track, we explore how we can automatically set parameters and evaluate how well our approach perform on difficult topics. In the Robust track, we experimented with two heuristic approaches to improve the robustness in using language models for pseudo feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval with language models</head><p>In this section, we describe our retrieval approaches. The basic retrieval formula we use is the Kullback-Leibler (KL) divergence between the query language model and the document language model <ref type="bibr" coords="1,437.95,432.28,10.90,8.97" target="#b0">[1,</ref><ref type="bibr" coords="1,452.45,432.28,7.27,8.97" target="#b4">5]</ref>. This method is a generalization of the query likelihood retrieval method proposed in <ref type="bibr" coords="1,361.01,453.88,11.63,8.97" target="#b3">[4]</ref> and can support feedback more naturally than the query likelihood method.</p><p>Suppose that a query q is "generated" by a unigram language model p (q | θ Q ) with θ Q denoting the parameters, and similarly, a document d is generated by a unigram model p (d | θ D ) with θ D denoting the parameters. Let θ Q and θ D be the estimated query and document language models respectively. The KL-divergence retrieval formula scores d with respect to q with the following negative KL-divergence function <ref type="bibr" coords="1,430.65,553.48,10.79,8.97" target="#b4">[5]</ref>:</p><formula xml:id="formula_0" coords="1,317.28,576.69,216.35,47.22">-D( θ Q || θ D ) = w p (w | θ Q ) log p (w | θ D ) +(- w p(w| θ Q )logp(w| θ Q ))</formula><p>Since the second term on the right-hand side of the formula does not depend on d, it can be ignored for the purpose of ranking documents. Thus the scoring is essentially based on the first term, i.e., the cross entropy of the document model and the query model. In general, the computation of this cross entropy involves a sum over all the words that have a non-zero probability according to p(w| θ Q ). However, when θ D is based on the following general smoothing scheme, the computation would only involve a sum over those that both have a non-zero probability according to p(w| θ Q ) and occur in document d. Such a sum can be computed much more efficiently with an inverted index. See <ref type="bibr" coords="2,169.21,155.80,11.75,8.97" target="#b2">[3]</ref> for a detailed explanation of this.</p><p>Clearly, the retrieval performance of the KL-divergence would depend on how we estimate the document model θ D and the query model θ Q . Smoothing of θ D is very important, and an effective smoothing method is the two-stage smoothing method proposed in <ref type="bibr" coords="2,247.95,221.08,10.60,8.97" target="#b6">[7]</ref>, where it has been shown to achieve optimal or near optimal performance with completely automatic parameter setting. However, this smoothing method as presented in <ref type="bibr" coords="2,278.52,253.36,11.63,8.97" target="#b6">[7]</ref> is based on the query likelihood retrieval method, which is unable to incorporate feedback naturally. Although some techniques for tuning retrieval parameters automatically are proposed in <ref type="bibr" coords="2,140.92,296.44,10.60,8.97" target="#b6">[7]</ref>, they are based on without pseudo feedback. This is also why the performance reported in <ref type="bibr" coords="2,83.87,317.92,11.75,8.97" target="#b6">[7]</ref> is not as good as the best official TREC results on the same data, which are usually obtained based on pseudo feedback. The KL-divergence retrieval formula can be shown to be a generalization of the query likelihood method, and can naturally support feedback as query model updating. Two specific methods for performing feedback using language models are proposed in <ref type="bibr" coords="2,72.00,393.28,10.60,8.97" target="#b4">[5]</ref>. In order to automatically tune the retrieval parameters for pseudo feedback, we extend the two-stage smoothing method so that it can work with a query model using KLdivergence. Since maximizing the likelihood is equivalent to minimizing the KL-divergence, this extension is not unnatural. We now explain this extension in detail.</p><p>The basic idea of two-stage smoothing is to decouple the two roles of smoothing (i.e., the estimation role and query modeling role <ref type="bibr" coords="2,158.65,480.04,11.24,8.97" target="#b5">[6]</ref>) so that we can capture the intrinsic connections between the parameter values and the data. Specifically, the document model θ D is smoothed first with Dirichlet prior (to implement the estimation role) and then with a simple linear interpolation with some query background model (to implement the query modeling role). To apply this idea to the KL-divergence retrieval formula, we also use Dirichlet prior as the firststage smoothing method. That is, our estimated document language model θ D is given by Dirichlet prior <ref type="bibr" coords="2,265.81,576.88,10.60,8.97" target="#b5">[6]</ref>. According to this method,</p><formula xml:id="formula_1" coords="2,123.24,609.09,125.20,23.52">p(w| θ D ) = c(w, d) + µp(w|C) |d| + µ</formula><p>where c(w, d) is the count of word w in d, µ is a query independent smoothing parameter, and p(w|C) is reference language model estimated using the whole document collection. µ can be set using leave-one-out cross validation on the document collection <ref type="bibr" coords="2,420.22,76.00,10.60,8.97" target="#b6">[7]</ref>. To implement the secondstage smoothing, we first interpolate the estimated document model θ D with the query background model p(w|U ) to obtain a two-stage smoothed document model θ D , and then compute the KL-divergence</p><formula xml:id="formula_2" coords="2,344.28,125.97,176.72,36.06">D( θ Q || θ D . That is, p(w| θ D ) = (1 -λ)p(w| θ D ) + λp(w|U )</formula><p>where λ can be interpreted as the amount of noise that we believe exists in the query, and p(w|U ) is the user's query background model. Since no information is available about the noise in the query, we simply approximate p(w|U ) by p(w|C). λ can also be estimated in a very much similar way as in <ref type="bibr" coords="2,408.38,227.08,10.60,8.97" target="#b6">[7]</ref>, except that we minimize the KL-divergence between the document mixture model and the query model instead of maximizing the query likelihood given the document mixture model. The collection language model p(w|C) is typically estimated by</p><formula xml:id="formula_3" coords="2,312.12,281.30,228.00,32.20">c(w,C) w c(w ,C) , or a smoothed version c(w,C)+1 V + w c(w ,C)</formula><p>, where V is an estimated vocabulary size (e.g., the total number of distinct words in the collection). One advantage of the smoothed version is that it would never give a zero probability to any term, but in terms of retrieval performance, there will not be any significant difference in these two versions, since w c(w , C) is often significantly larger than V .</p><p>Having discussed how we use two-stage smoothing to estimate a document model, let us now look at the query model. The simplest way to estimate θ Q is to use the maximum likelihood estimator based on the query text, which gives us essentially the empirical query distribution:</p><formula xml:id="formula_4" coords="2,380.40,443.61,88.78,23.52">p ml (w| θ Q ) = c(w, q) |q|</formula><p>Using this estimated value, the KL-divergence scoring formula is essentially the same as the query likelihood retrieval formula, thus we essentially have the two-stage smoothing retrieval method as presented in <ref type="bibr" coords="2,491.31,511.48,10.60,8.97" target="#b6">[7]</ref>. This is what we use for the initial round of retrieval. A more interesting way of computing p(w| θ Q ) is to exploit feedback documents. Specifically, we can interpolate the simple p ml (w| θ Q ) with a feedback model p(w|θ F ) estimated based on feedback documents. That is,</p><formula xml:id="formula_5" coords="2,337.32,590.13,202.78,10.26">p(w| θ Q ) = (1 -α)p ml (w| θ Q ) + αp(w|θ F ) (1)</formula><p>where, α is a parameter that needs to be set empirically.</p><p>To compute θ F , we assume a two component mixture model for the feedback documents, where one component model is p(w|θ F ) and the other is p(w|C). The likelihood of the feedback documents is thus</p><formula xml:id="formula_6" coords="2,393.84,677.37,63.35,10.26">log p(F | θ F ) = k i=1 w c(w; d i ) log((1 -ν)p(w | θ F ) + νp(w | C))</formula><p>where, F = {d 1 , ..., d k } is the set of feedback documents, and ν is yet another parameter that indicates the amount of "background noise" in the feedback documents, and that needs to be set empirically. Given ν, the feedback documents F, and the collection language model p(w|C), we can use the EM algorithm to compute the maximum likelihood estimate of θ F <ref type="bibr" coords="3,162.48,175.00,10.69,8.97" target="#b4">[5]</ref>. For the sake of efficiency, we truncate the model θ F so that we only keep k word with the highest probabilities according to p(w|θ F ). We thus have four parameters to set in performing pseudo feedback: (1) α controls the influence of feedback documents on the new query model; (2) ν indicates the amount of noise that we believe exists in the feedback documents;</p><p>(3) k is the number of terms to select for query model updating; and (4) the number of documents to use for feedback. It is reasonable to assume that the choice of k has a relatively insignificant influence since the words excluded generally have smaller probabilities. But we know that the feedback performance is sensitive to both α and ν <ref type="bibr" coords="3,270.12,304.12,10.60,8.97" target="#b4">[5]</ref>, and no doubt, also to the number of top documents to be used for pseudo feedback. This makes the feedback approach less robust.</p><p>Thus a major research question we address is how we can make this feedback process more robust. We propose two strategies:</p><p>• Weighted pseudo feedback: The basic idea of this strategy is to discount the contribution of documents according to their rank in the results. Specifically, we assume the probability of relevance of a document ranked at rank r to be 1/r. Thus the top document has "full" contribution in feedback, whereas the second and third documents are discounted by a factor of 1/2 and 1/3 respectively. We hoe this strategy would make the performance less sensitive to the choice of the number of documents used for pseudo feedback because if we use more documents, those documents would just be discounted more.</p><p>• Applying two-stage smoothing after feedback:</p><p>The idea of this strategy is to re-estimate the secondstage smoothing parameter λ after we obtain an updated query model using feedback so that we can "compensate" for any non-optimality in the feedback parameter setting by adjusting λ according to the "noise" in the newly updated query model.</p><p>Both strategies have shown some positive effect in our preliminary experiments with the past TREC data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Genomics IR Track</head><p>We participated in the primary task of the genomics IR track, and focused on studying how we may apply lan-guage modeling approaches to this new retrieval task.</p><p>A critical difference between the genomics IR task and a regular ad hoc task is that a query in the genomics IR task has a structure. More specifically, a gene query has several parts, including an official name, which is usually a long noun phrase, several symbols, each being a unique identifier for the gene, and protein product names. Such a query has a clear disjunctive structure in that matching either a gene symbol or the complete gene name would indicate relevance. For example, topic 1 has the following name and symbols:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OFFICIAL_GENE_NAME activating transcription factor 2 OFFICIAL_SYMBOL ATF2 ALIAS_SYMBOL HB16 ALIAS_SYMBOL CREB2 ALIAS_SYMBOL TREB7 ALIAS_SYMBOL CRE-BP1</head><p>These symbols are unique identifiers for the gene, so matching one of the symbols is a sufficiently strong evidence for a document to be relevant, and is as good as matching the whole phrase "activating transcription factor 2". Thus although both "transcription" and "CREB2" are a single word term, "CREB2" should be weighted much higher than "transcription".</p><p>Such a semi-structured query clearly violates the basic assumption made in a language modeling approach that the query can be treated as a sample drawn from a language model. As a result, if we use the basic language modeling approach as is, the gene symbols may be underweighted. Indeed, in our preliminary experiments with the training topics, we found that the basic language modeling approaches did not perform as well as well-tuned vector space models.</p><p>To address this problem, we propose a new way to estimate our query language model θ Q so that it can better model a disjunctive query. Our idea is to first compute the empirical word distribution for each "component query", and then take an average of them. Formally, suppose q = {q 1 , ..., q k } is a disjunctive semi-structured query, where q i is a "component query". Our estimated query model is given by</p><formula xml:id="formula_7" coords="3,373.56,555.14,103.92,30.50">p(w|θ Q ) = 1 k k i=1 p(w|q i )</formula><p>where p(w|q i ) is the empirical word distribution of component query q i , and is computed as c(w,qi) |qi| where c(w, q i ) is the count of word w in the component query q i and |q i | is the length of q i . Note that each gene symbol is one component query so if q i is a gene symbol s, then we have p(s|q i ) = 1, whereas if w is a word in the name, then p(w|q i ) = 1/|q i |, which is usually smaller than 1 because a name phrase almost always has more than one</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No feedback</head><p>Pseudo feedback MAP Pr@0.1 Pr@10doc Rec@1000 MAP Pr@0. word. Thus, in effect, our average query model would favor matching a symbol. Intuitively, this average query model normalizes word counts so that the total contribution from each component query is equal, reflecting the disjunctive semantics of the query. As a baseline method, we also experimented with heuristically duplicating each gene symbol in a query to "boost" its weight.</p><p>Our two official submissions represent these two different approaches to model semi-structured queries. In both official submissions, we used symbols and the official name in the gene query description, and used the retrieval approach as described in Section 2 except that we did not apply two-stage smoothing after pseudo feedback. We used top 10 documents for pseudo feedback, set both α and ν to 0.5, and used top 20 terms for query model updating.</p><p>Since many biology names involve digits and hyphens, we used a slightly different tokenization method than what we normally use to handle the special syntax of gene names. Specifically, we retain all the digits as well as any hyphen that connects at least one digit; in other words, we only remove a hyphen when it connects two letters. This method gives a slight improvement in performance based on the training topics. To test the robustness of our method, we deliberately did not remove any stop word, as we believe that this is best handled through appropriate language modeling. We did not apply stemming either.</p><p>Table <ref type="table" coords="4,107.49,535.84,5.03,8.97" target="#tab_2">3</ref> shows the results of our two official submissions along with some other experiments where we use different types of queries and test them with/without feedback. The results of the two official runs are highlighted in boldface.</p><p>We can make the following observations:</p><p>1. Using gene name alone is least effective, significantly worse than using any other versions of the query. This is probably because the words in a name are too general and thus not discriminative. Indexing the whole name as a phrase may help improve the performance.</p><p>2. Using symbols is more effective than using all the words, which means that we treat the whole gene description as one long text query. This suggests that the symbols alone are the most important information in the query, and there are noise terms in other part of the query (mostly the gene name and protein names).</p><p>3. Using all words but ignoring word frequency (i.e., "All distinct words" in the table) improves performance significantly. Since symbols always have a frequency of 1, this suggests that when pooling all words together, we are overweighting the regular words in those names. The fact that "all distinct words" also performs significantly better than using the symbols alone indicates that the gene name is also very useful.</p><p>4. The two official runs all use names combined with symbols and both put more weight on a symbol term. We see that both perform better than other runs, indicating that treating such a disjunctive query as a regular text query is non-optimal and we need to increase the weight for short component queries (i.e,. symbols).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Comparing the two different methods for modeling disjunctive queries (i.e., ad hoc weighting by duplication and language model averaging), we see that the average query model approach performs slightly better in mean average precision (MAP) and much better in precision at recall level of 0.1 (i.e., the front end precision). However, it has a lower recall at 1,000 documents and a lower precision at 10 documents in feedback runs (i.e., our official runs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Robust Track</head><p>Our goal for the robust track is to evaluate and improve the robustness of the language model based retrieval approach described in Section 2. This approach is a combination of the two-stage smoothing method which has been shown to Topics Query Type MAP Pr@0.1 Pr@10doc Rec@1000 percent(pr@10=0) be quite robust w.r.t., parameter setting <ref type="bibr" coords="5,232.24,346.96,10.60,8.97" target="#b6">[7]</ref>, and the mixture model feedback approach <ref type="bibr" coords="5,196.75,357.76,10.60,8.97" target="#b4">[5]</ref>, which is effective but sensitive to several parameters. As discussed in Section 2, we propose two strategies to reduce the sensitivity of our approach to the setting of parameters: (1) Estimate the probability of relevance for each document to be used for pseudo feedback so as to discount the contribution of documents based on their ranks. (2) Apply the two-stage smoothing method to reestimate the smoothing parameters after query model updating. In some sense, this method is to "bypass" the problem of choosing optimal values for the some of the feedback parameters. In our preliminary experiments with the old topics, these methods perform better than several baseline approaches</p><p>We did minimum preprocessing (only stemming, no stop word removal) to test the robustness of our method, and submitted five runs with one run using title of the query (UIUC03Rt1, three runs using the description part of the query (UIUC03Rd1, UIUC03Rd2, UIUC03Rd3), and one run using both the title and the description (UIUC03Rtd1). In all our submissions we applied the first strategy (i.e., discounting documents based on ranks), and in UIUC03Rd2 and UIUC03Rd3, we also applied the second strategy. UIUC03Rd2 and UIUC03Rd3 differ in the number of top terms selected to update the query model (20 and 50 respectively). In all cases, we used top 10 documents for pseudo feedback, and set α and ν both to 0.5 as in all our experiments.</p><p>In Table <ref type="table" coords="5,119.72,667.36,3.77,8.97">4</ref>, we compare the performance of different versions of the queries on both old topics and new top-ics. It is clear that, in all cases, using the regular precision measures over all the topics, "title+description" performs much better than "description only", which performs better than "title only", though in the case of old topics, the recall of "description only" is worse than that of "title only". However, it is very interesting to see that for both old and new topics, "title only" has the least number of topics with no relevant document in top 10 documents, and its MAP area for the worst 12 topics is also better than the description. "title+description" has the largest MAP area for the worst 12 topics, but, compared with the "title only" run, it actually has more topics which have no relevant document in top 10 documents. These results clearly show that using titles appear to help improve the performance on difficult topics. This may be because the words in the query title are more accurate and when we use more words from other parts they just bring up many non-relevant documents perhaps because the relevant documents do not have a good match in vocabulary with the words in these other parts. Another possibility is that our approach may not be discriminative enough to give title words more weight and due to the fact that we do not remove stop words, the description part just adds too many noisy words. It would thus be interesting to apply our disjunctive query model to model the title and description parts to see if it can improve the performance, which we will explore in the future.</p><p>In Table <ref type="table" coords="5,359.13,645.76,3.77,8.97">4</ref>, we compare the three "description only" runs. One (baseline) run applies two-stage smoothing only for the initial run and after the feedback the same smoothing parameters are used. The other two runs fur-ther apply two-stage smoothing to re-estimate the smoothing parameters after we obtain an updated query model. These two runs differ in the number of top terms to be selected for query model updating. Here we can make the following observations:</p><p>1. Looking at all runs in which we use 20 terms for feedback, we see that for old topics, the re-estimation strategy helps improve performance by all the regular measures averaged on all the topics, but it does not improve the performance on difficult topics. Indeed, it actually hurts the performance for difficult topics. For new topics, the re-estimation strategy not only hurts the performance for difficult topics, it is worse by all measures.</p><p>2. Comparing all the runs involving re-estimation, we see that for old topics, using 50 terms for feedback is better than using 20 terms when averaging over all the topics, but is worse on difficult topics. For new topics, it appears to perform similarly to using 20 terms.</p><p>Thus our re-estimation strategy does not seem to help improve the robustness. We have not yet had more experiments to examine whether the first strategy -discounting feedback documents based on their ranks -is useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we report our experiments in the TREC 2003 Genomics Track and the Robust Track. A common theme that we explored is the robustness of a basic language modeling retrieval approach. We examine several aspects of robustness, including robustness in handling different types of queries, different types of documents, and optimizing performance for difficult topics. Our basic retrieval method is the KL-divergence retrieval model with the two-stage smoothing method plus a mixture model feedback method. In the Genomics IR track, we propose a new method for modeling semi-structured queries using language models, which is shown to be more robust and effective than the regular query model in handling gene queries. In the Robust track, we experimented with two heuristic approaches to improve the robustness in using language models for pseudo feedback. One is the discount feedback documents based on their ranks and the other is to apply two-stage smoothing after feedback to re-estimate the smoothing parameters. Preliminary result analysis appears to suggest that the re-estimation of smoothing parameters does not really help. But we need to do more experiments and analysis in order to make reliable conclusions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,72.00,84.76,467.59,129.57"><head>Table 1 :</head><label>1</label><figDesc>Different query models with/without pseudo feedback. Boldface indicates the results of two official submissions.</figDesc><table coords="4,415.31,84.76,115.87,8.97"><row><cell>1 Pr@10doc Rec@1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,82.68,73.96,446.64,218.61"><head>Table 2 :</head><label>2</label><figDesc>Different types of queries on old and new topics.</figDesc><table coords="5,457.80,73.96,67.36,19.77"><row><cell>MAP area</cell></row><row><cell>(worst 12 topics)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,152.40,304.96,306.95,8.97"><head>Table 3 :</head><label>3</label><figDesc>Effect of two-stage smoothing after feedback on old and new topics.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,327.59,97.12,212.25,8.97;6,327.60,107.80,212.50,8.97;6,327.60,118.60,212.38,8.97;6,327.60,129.40,62.66,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,425.63,97.12,114.20,8.97;6,327.60,107.80,212.50,8.97;6,327.60,118.60,31.82,8.97">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,383.35,118.60,101.40,8.97">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">Sept 2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.59,148.12,212.35,8.97;6,327.60,158.92,212.58,8.97;6,327.60,169.60,42.78,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,434.37,148.12,105.57,8.97;6,327.60,158.92,26.91,8.97">Relevance-based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,372.66,158.92,98.16,8.97">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">Sept 2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.59,188.32,212.27,8.97;6,327.60,199.12,212.38,8.97;6,327.60,209.92,22.64,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,425.75,188.32,114.11,8.97;6,327.60,199.12,25.06,8.97">Experiments using the lemur toolkit</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,369.55,199.12,166.24,8.97">Proceedings of the 2001 TREC conference</title>
		<meeting>the 2001 TREC conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.59,228.64,212.27,8.97;6,327.60,239.32,212.26,8.97;6,327.60,250.12,153.87,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,436.67,228.64,103.19,8.97;6,327.60,239.32,120.75,8.97">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,465.80,239.32,74.07,8.97;6,327.60,250.12,59.58,8.97">Proceedings of the ACM SIGIR&apos;98</title>
		<meeting>the ACM SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.59,268.84,212.26,8.97;6,327.60,279.64,212.50,8.97;6,327.60,290.32,212.37,8.97;6,327.60,301.12,168.24,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,424.19,268.84,115.65,8.97;6,327.60,279.64,119.09,8.97">Model-based feedback in the KL-divergence retrieval model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,463.52,279.64,76.59,8.97;6,327.60,290.32,212.37,8.97;6,327.60,301.12,74.18,8.97">Tenth International Conference on Information and Knowledge Management (CIKM 2001)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.59,319.84,212.27,8.97;6,327.60,330.64,212.50,8.97;6,327.60,341.32,212.55,8.97;6,327.60,352.12,107.71,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,425.99,319.84,113.86,8.97;6,327.60,330.64,212.50,8.97;6,327.60,341.32,52.08,8.97">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,407.09,341.32,128.51,8.97">Proceedings of ACM SIGIR&apos;01</title>
		<meeting>ACM SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">Sept 2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,327.59,370.84,212.26,8.97;6,327.60,381.64,212.29,8.97;6,327.60,392.44,132.04,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,427.79,370.84,112.06,8.97;6,327.60,381.64,95.83,8.97">Two-stage language models for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,442.19,381.64,97.70,8.97;6,327.60,392.44,27.94,8.97">Proceedings of ACM SI-GIR&apos;02</title>
		<meeting>ACM SI-GIR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002-08">Aug 2002</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
